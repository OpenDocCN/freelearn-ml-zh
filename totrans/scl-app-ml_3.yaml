- en: Part III. Module 3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mastering Scala Machine Learning**'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Advance your skills in efficient data analysis and data processing using the
    powerful tools of Scala, Spark, and Hadoop*'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chapter 1. Exploratory Data Analysis
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before I dive into more complex methods to analyze your data later in the book,
    I would like to stop at basic data exploratory tasks on which almost all data
    scientists spend at least 80-90% of their productive time. The data preparation,
    cleansing, transforming, and joining the data alone is estimated to be a $44 billion/year
    industry alone (*Data Preparation in the Big Data Era* by *Federico Castanedo*
    and *Best Practices for Data Integration*, *O''Reilly Media*, *2015*). Given this
    fact, it is surprising that people only recently started spending more time on
    the science of developing best practices and establishing good habits, documentation,
    and teaching materials for the whole process of data preparation (*Beautiful Data:
    The Stories Behind Elegant Data Solutions*, edited by *Toby Segaran* and *Jeff
    Hammerbacher*, *O''Reilly Media*, *2009* and *Advanced Analytics with Spark: Patterns
    for Learning from Data at Scale* by *Sandy Ryza et al.*, *O''Reilly Media*, *2015*).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Few data scientists would agree on specific tools and techniques—and there are
    multiple ways to perform the exploratory data analysis, ranging from Unix command
    line to using very popular open source and commercial ETL and visualization tools.
    The focus of this chapter is how to use Scala and a laptop-based environment to
    benefit from techniques that are commonly referred as a functional paradigm of
    programming. As I will discuss, these techniques can be transferred to exploratory
    analysis over distributed system of machines using Hadoop/Spark.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: What has functional programming to do with it? Spark was developed in Scala
    for a good reason. Many basic principles that lie at the foundation of functional
    programming, such as lazy evaluation, immutability, absence of side effects, list
    comprehensions, and monads go really well with processing data in distributed
    environments, specifically, when performing the data preparation and transformation
    tasks on big data. Thanks to abstractions, these techniques work well on a local
    workstation or a laptop. As mentioned earlier, this does not preclude us from
    processing very large datasets up to dozens of TBs on modern laptops connected
    to distributed clusters of storage/processing nodes. We can do it one topic or
    focus area at the time, but often we even do not have to sample or filter the
    dataset with proper partitioning. We will use Scala as our primary tool, but will
    resort to other tools if required.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: While Scala is complete in the sense that everything that can be implemented
    in other languages can be implemented in Scala, Scala is fundamentally a high-level,
    or even a scripting, language. One does not have to deal with low-level details
    of data structures and algorithm implementations that in their majority have already
    been tested by a plethora of applications and time, in, say, Java or C++—even
    though Scala has its own collections and even some basic algorithm implementations
    today. Specifically, in this chapter, I'll be focusing on using Scala/Spark only
    for high-level tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Installing Scala
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning simple techniques for initial data exploration
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to downsample the original dataset for faster turnover
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing the implementation of basic data transformation and aggregations
    in Scala
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting familiar with big data processing tools such as Spark and Spark Notebook
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting code for some basic visualization of datasets
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with Scala
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have already installed Scala, you can skip this paragraph. One can get
    the latest Scala download from [http://www.scala-lang.org/download/](http://www.scala-lang.org/download/).
    I used Scala version 2.11.7 on Mac OS X El Capitan 10.11.5\. You can use any other
    version you like, but you might face some compatibility problems with other packages
    such as Spark, a common problem in open source software as the technology adoption
    usually lags by a few released versions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In most cases, you should try to maintain precise match between the recommended
    versions as difference in versions can lead to obscure errors and a lengthy debugging
    process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'If you installed Scala correctly, after typing `scala`, you should see something
    similar to the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is a Scala **read-evaluate-print-loop** (**REPL**) prompt. Although Scala
    programs can be compiled, the content of this chapter will be in REPL, as we are
    focusing on interactivity with, maybe, a few exceptions. The `:help` command provides
    a some utility commands available in REPL (note the colon at the start):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting started with Scala](img/image01626.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Distinct values of a categorical field
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you have a dataset and a computer. For convenience, I have provided you
    a small anonymized and obfuscated sample of clickstream data with the book repository
    that you can get at [https://github.com/alexvk/ml-in-scala.git](https://github.com/alexvk/ml-in-scala.git).
    The file in the `chapter01/data/clickstream` directory contains lines with timestamp,
    session ID, and some additional event information such as URL, category information,
    and so on at the time of the call. The first thing one would do is apply transformations
    to find out the distribution of values for different columns in the dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 01-1 shows* screenshot shows the output of the dataset in the terminal
    window of the `gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz | less
    –U` command. The columns are tab (`^I`) separated. One can notice that, as in
    many real-world big data datasets, many values are missing. The first column of
    the dataset is recognizable as the timestamp. The file contains complex data such
    as arrays, structs, and maps, another feature of big data datasets.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Unix provides a few tools to dissect the datasets. Probably, **less**, **cut**,
    **sort**, and **uniq** are the most frequently used tools for text file manipulations.
    **Awk**, **sed**, **perl**, and **tr** can do more complex transformations and
    substitutions. Fortunately, Scala allows you to transparently use command-line
    tools from within Scala REPL, as shown in the following screenshot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Distinct values of a categorical field](img/image01627.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Figure 01-1\. The clickstream file as an output of the less -U Unix command
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, Scala allows you to transparently use command-line tools from
    within Scala REPL:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I used the `scala.sys.process` package to call familiar Unix commands from Scala
    REPL. From the output, we can immediately see the customers of our Webshop are
    mostly interested in men's shoes and running, and that most visitors are using
    the referral code, **KW_0611081618**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One may wonder when we start using complex Scala types and algorithms. Just
    wait, a lot of highly optimized tools were created before Scala and are much more
    efficient for explorative data analysis. In the initial stage, the biggest bottleneck
    is usually just the disk I/O and slow interactivity. Later, we will discuss more
    iterative algorithms, which are usually more memory intensive. Also note that
    the UNIX pipeline operations can be implicitly parallelized on modern multi-core
    computer architectures, as they are in Spark (we will show it in the later chapters).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: It has been shown that using compression, implicit or explicit, on input data
    files can actually save you the I/O time. This is particularly true for (most)
    modern semi-structured datasets with repetitive values and sparse content. Decompression
    can also be implicitly parallelized on modern fast multi-core computer architectures,
    removing the computational bottleneck, except, maybe in cases where compression
    is implemented implicitly in hardware (SSD, where we don't need to compress the
    files explicitly). We also recommend using directories rather than files as a
    paradigm for the dataset, where the insert operation is reduced to dropping the
    data file into a directory. This is how the datasets are presented in big data
    Hadoop tools such as Hive and Impala.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Summarization of a numeric field
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the numeric data, even though most of the columns in the dataset
    are either categorical or complex. The traditional way to summarize the numeric
    data is a five-number-summary, which is a representation of the median or mean,
    interquartile range, and minimum and maximum. I''ll leave the computations of
    the median and interquartile ranges till the Spark DataFrame is introduced, as
    it makes these computations extremely easy; but we can compute mean, min, and
    max in Scala by just applying the corresponding operators:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Grepping across multiple fields
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes one needs to get an idea of how a certain value looks across multiple
    fields—most common are IP/MAC addresses, dates, and formatted messages. For examples,
    if I want to see all IP addresses mentioned throughout a file or a document, I
    need to replace the `cut` command in the previous example by `grep -o -E [1-9][0-9]{0,2}(?:\\.[1-9][0-9]{0,2}){3}`,
    where the `–o` option instructs `grep` to print only the matching parts—a more
    precise regex for the IP address should be `grep –o –E (?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)`,
    but is about 50% slower on my laptop and the original one works in most practical
    cases. I'll leave it as an excursive to run this command on the sample file provided
    with the book.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Basic, stratified, and consistent sampling
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I've met quite a few data practitioners who scorn sampling. Ideally, if one
    can process the whole dataset, the model can only improve. In practice, the tradeoff
    is much more complex. First, one can build more complex models on a sampled set,
    particularly if the time complexity of the model building is non-linear—and in
    most situations, if it is at least *N* log(N)*. A faster model building cycle
    allows you to iterate over models and converge on the best approach faster. In
    many situations, *time to action* is beating the potential improvements in the
    prediction accuracy due to a model built on complete dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Sampling may be combined with appropriate filtering—in many practical situation,
    focusing on a subproblem at a time leads to better understanding of the whole
    problem domain. In many cases, this partitioning is at the foundation of the algorithm,
    like in decision trees, which are considered later. Often the nature of the problem
    requires you to focus on the subset of original data. For example, a cyber security
    analysis is often focused around a specific set of IPs rather than the whole network,
    as it allows to iterate over hypothesis faster. Including the set of all IPs in
    the network may complicate things initially if not throw the modeling off the
    right track.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with rare events, such as clickthroughs in ADTECH, sampling the
    positive and negative cases with different probabilities, which is also sometimes
    called oversampling, often leads to better predictions in short amount of time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Fundamentally, sampling is equivalent to just throwing a coin—or calling a
    random number generator—for each data row. Thus it is very much like a stream
    filter operation, where the filtering is on an augmented column of random numbers.
    Let''s consider the following example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，采样等同于对每一行数据抛硬币——或者调用随机数生成器。因此，它非常类似于流过滤操作，这里的过滤是在随机数增强列上进行的。让我们考虑以下示例：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is all good, but it has the following disadvantages:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很好，但它有以下缺点：
- en: The number of lines in the resulting file is not known beforehand—even though
    on average it should be 5% of the original file
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果文件的行数事先是未知的——尽管平均来说应该是原始文件的5%
- en: The results of the sampling is non-deterministic—it is hard to rerun this process
    for either testing or verification
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采样的结果是非确定的——很难重新运行此过程进行测试或验证
- en: 'To fix the first point, we''ll need to pass a more complex object to the function,
    as we need to maintain the state during the original list traversal, which makes
    the original algorithm less functional and parallelizable (this will be discussed
    later):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第一个问题，我们需要传递一个更复杂的对象给函数，因为我们需要在原始列表遍历期间保持状态，这使得原始算法功能性和并行性降低（这将在稍后讨论）：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will output `numLines` lines. Similarly to reservoir sampling, stratified
    sampling is guaranteed to provide the same ratios of input/output rows for all
    strata defined by levels of another attribute. We can achieve this by splitting
    the original dataset into *N* subsets corresponding to the levels, performing
    the reservoir sampling, and merging the results afterwards. However, MLlib library,
    which will be covered in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib*, already has stratified
    sampling implementation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出`numLines`行。类似于蓄水池采样，分层采样保证为所有由另一个属性的级别定义的层提供相同的输入/输出行比例。我们可以通过将原始数据集分割成与级别相对应的*N*个子集，执行蓄水池采样，然后合并结果来实现这一点。然而，将在[第3章](part0249.xhtml#aid-7DES21
    "第3章。使用Spark和MLlib")中介绍的MLlib库，*使用Spark和MLlib*，已经实现了分层采样的实现：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The other bullet point is more subtle; sometimes we want a consistent subset
    of values across multiple datasets, either for reproducibility or to join with
    another sampled dataset. In general, if we sample two datasets, the results will
    contain random subsets of IDs which might have very little or no intersection.
    The cryptographic hashing functions come to the help here. The result of applying
    a hash function such as MD5 or SHA1 is a sequence of bits that is statistically
    uncorrelated, at least in theory. We will use the `MurmurHash` function, which
    is part of the `scala.util.hashing` package:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要点更为微妙；有时我们希望在多个数据集中保持值的一致子集，无论是为了可重复性还是为了与其他采样数据集连接。一般来说，如果我们采样两个数据集，结果将包含随机子集的ID，这些ID可能几乎没有交集或完全没有交集。密码学哈希函数在这里提供了帮助。应用MD5或SHA1等哈希函数的结果是一系列在理论上至少是统计上不相关的位序列。我们将使用`MurmurHash`函数，它是`scala.util.hashing`包的一部分：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This function is guaranteed to return exactly the same subset of records based
    on the value of the first field—it is either all records where the first field
    equals a certain value or none—and will come up with approximately one-sixteenth
    of the original sample; the range of `hash` is `0` to `65,535`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数保证基于第一个字段的值返回完全相同的记录子集——要么是第一个字段等于某个特定值的所有记录，要么是没有任何记录——并且将产生大约原始样本的六分之一；`hash`的范围是`0`到`65,535`。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: MurmurHash? It is not a cryptographic hash!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: MurmurHash？它不是一个密码学哈希！
- en: Unlike cryptographic hash functions, such as MD5 and SHA1, MurmurHash is not
    specifically designed to be hard to find an inverse of a hash. It is, however,
    really fast and efficient. This is what really matters in our use case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与MD5和SHA1等密码学哈希函数不同，MurmurHash并不是专门设计成难以找到哈希的逆函数。然而，它确实非常快且高效。在我们的用例中，这真正是重要的。
- en: Working with Scala and Spark Notebooks
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scala和Spark笔记本工作
- en: Often the most frequent values or five-number summary are not sufficient to
    get the first understanding of the data. The term **descriptive statistics** is
    very generic and may refer to very complex ways to describe the data. Quantiles,
    a **Paretto** chart or, when more than one attribute is analyzed, correlations
    are also examples of descriptive statistics. When sharing all these ways to look
    at the data aggregates, in many cases, it is also important to share the specific
    computations to get to them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Scala or Spark Notebook [https://github.com/Bridgewater/scala-notebook](https://github.com/Bridgewater/scala-notebook),
    [https://github.com/andypetrella/spark-notebook](https://github.com/andypetrella/spark-notebook)
    record the whole transformation path and the results can be shared as a JSON-based
    `*.snb` file. The Spark Notebook project can be downloaded from [http://spark-notebook.io](http://spark-notebook.io),
    and I will provide a sample `Chapter01.snb` file with the book. I will use Spark,
    which I will cover in more detail in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: For this particular example, Spark will run in the local mode. Even in the local
    mode Spark can utilize parallelism on your workstation, but it is limited to the
    number of cores and hyperthreads that can run on your laptop or workstation. With
    a simple configuration change, however, Spark can be pointed to a distributed
    set of machines and use resources across a distributed set of nodes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the set of commands to download the Spark Notebook and copy the necessary
    files from the code repository:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now you can open the notebook at `http://localhost:9000` in your browser, as
    shown in the following screenshot:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![Working with Scala and Spark Notebooks](img/image01628.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Figure 01-2\. The first page of the Spark Notebook with the list of notebooks
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `Chapter01` notebook by clicking on it. The statements are organized
    into cells and can be executed by clicking on the small right arrow at the top,
    as shown in the following screenshot, or run all cells at once by navigating to
    **Cell** | **Run All**:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '![Working with Scala and Spark Notebooks](img/image01629.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
- en: Figure 01-3\. Executing the first few cells in the notebook
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will look at the values of all or some of discrete variables. For
    example, to get the distribution of the labels, issue the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The first time I read the dataset, it took about a minute on MacBook Pro, but
    Spark caches the data in memory and the subsequent aggregation runs take only
    about a second. Spark Notebook provides you the distribution of the values, as
    shown in the following screenshot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![Working with Scala and Spark Notebooks](img/image01630.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: Figure 01-4\. Computing the distribution of values for a categorical field
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'I can also look at crosstab counts for pairs of discrete variables, which gives
    me an idea of interdependencies between the variables using [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions)—the
    object does not support computing correlation measures such as chi-square yet:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![Working with Scala and Spark Notebooks](img/image01631.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: Figure 01-5\. Contingency table or crosstab
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: However, we can see that the most popular service is private and it correlates
    well with the `SF` flag. Another way to analyze dependencies is to look at `0`
    entries. For example, the `S2` and `S3` flags are clearly related to the SMTP
    and FTP traffic since all other entries are `0`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the most interesting correlations are with the target variable, but
    these are better discovered by supervised learning algorithms that I will cover
    in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working with Spark and MLlib"),
    *Working with Spark and MLlib,* and [Chapter 5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression
    and Classification"), *Regression and Classification*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Working with Scala and Spark Notebooks](img/image01632.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Figure 01-6\. Computing simple aggregations using org.apache.spark.sql.DataFrameStatFunctions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Analogously, we can compute correlations for numerical variables with the `dataFrame.stat.corr()`
    and `dataFrame.stat.cov()` functions (refer to *Figure 01-6)*. In this case, the
    class supports the **Pearson correlation coefficient**. Alternatively, we can
    use the standard SQL syntax on the parquet file directly:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, I promised you to compute percentiles. Computing percentiles usually
    involves sorting the whole dataset, which is expensive; however, if the tile is
    one of the first or the last ones, usually it is possible to optimize the computation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Computing the exact percentiles for a more generic case is more computationally
    expensive and is provided as a part of the Spark Notebook example code.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Basic correlations
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You probably noticed that detecting correlations from contingency tables is
    hard. Detecting patterns takes practice, but many people are much better at recognizing
    the patterns visually. Detecting actionable patterns is one of the primary goals
    of machine learning. While advanced supervised machine learning techniques that
    will be covered in [Chapter 4](part0256.xhtml#aid-7K4G02 "Chapter 4. Supervised
    and Unsupervised Learning"), *Supervised and Unsupervised Learning* and [Chapter
    5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression and Classification"), *Regression
    and Classification* exist, initial analysis of interdependencies between variables
    can help with the right transformation of variables or selection of the best inference
    technique.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Multiple well-established visualization tools exist and there are multiple sites,
    such as [http://www.kdnuggets.com](http://www.kdnuggets.com), which specialize
    on ranking and providing recommendations on data analysis, data explorations,
    and visualization software. I am not going to question the validity and accuracy
    of such rankings in this book, and very few sites actually mention Scala as a
    specific way to visualize the data, even if this is possible with, say, a `D3.js`
    package. A good visualization is a great way to deliver your findings to a larger
    audience. One look is worth a thousand words.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purposes of this chapter, I will use **Grapher** that is present on
    every Mac OS notebook. To open **Grapher**, go to Utilities (*shift* + *command*
    + *U* in Finder) and click on the **Grapher** icon (or search by name by pressing
    *command* + *space*). Grapher presents many options, including the following **Log-Log**
    and **Polar** coordinates:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic correlations](img/image01633.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: Figure 01-7\. The Grapher window
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, the amount of information that can be delivered through visualization
    is limited by the number of pixels on the screen, which, for most modern computers,
    is in millions and color variations, which arguably can also be in millions (*Judd*,
    *Deane B.*; *Wyszecki*, *Günter* (*1975*). *Color in Business, Science and Industry*.
    *Wiley Series in Pure and Applied Optics (3rd ed.)*. New York). If I am working
    on a multidimensional TB dataset, the dataset first needs to be summarized, processed,
    and reduced to a size that can be viewed on a computer screen.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of illustration, I will use the Iris UCI dataset that can be
    found at [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris).
    To bring the dataset into the tool, type the following code (on Mac OS):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Open the new **Point Set** in the **Grapher** (*command* + *alt* + *P*), press
    **Edit Points…** and paste the data by pressing *command* + *V*. The tools has
    line-fitting capabilities with basic linear, polynomial, and exponential families
    and provides the popular chi-squared metric to estimate the goodness of the fit
    with respect to the number of free parameters:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic correlations](img/image01634.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: Figure 01-8\. Fitting the Iris dataset using Grapher on Mac OS X
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: We will cover how to estimate the goodness of model fit in the following chapters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I've tried to establish a common ground to perform a more complex data science
    later in the book. Don't expect these to be a complete set of exploratory techniques,
    as the exploratory techniques can extend to running very complex modes. However,
    we covered simple aggregations, sampling, file operations such as read and write,
    working with tools such as notebooks and Spark DataFrames, which brings familiar
    SQL constructs into the arsenal of an analyst working with Spark/Scala.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'The next chapter will take a completely different turn by looking at the data
    pipelines as a part of a data-driven enterprise and cover the data discovery process
    from the business perspective: what are the ultimate goals we are trying to accomplish
    by doing the data analysis. I will cover a few traditional topics of ML, such
    as supervised and unsupervised learning, after this before delving into more complex
    representations of the data, where Scala really shows it''s advantage over SQL.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2. Data Pipelines and Modeling
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have looked at basic hands-on tools for exploring the data in the previous
    chapter, thus we now can delve into more complex topics of statistical model building
    and optimal control or science-driven tools and problems. I will go ahead and
    say that we will only touch on some topics in optimal control since this book
    really is just about ML in Scala and not the theory of data-driven business management,
    which might be an exciting topic for a book on its own.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, I will stay away from specific implementations in Scala and
    discuss the problem of building a data-driven enterprise at a high level. Later
    chapters will address how to solve these smaller pieces of the puzzle. A special
    emphasis will be given to handing uncertainty. Uncertainty usually comes in several
    flavors: first, there can be noise in the information we are provided with. Secondly,
    the information can be incomplete. The system may have some degree of freedom
    in filling the missing pieces, which results in uncertainty. Finally, there may
    be variations in the interpretation of the models and the resulting metrics. The
    final point is subtle, as most classic textbooks assume that we can measure things
    directly. Not only the measurements may be noisy, but the definition of the measure
    may change in time—try measuring satisfaction or happiness. Certainly, we can
    avoid the ambiguity by saying that we can optimize only measurable metrics, as
    people usually do, but it will significantly limit the application domain in practice.
    Nothing prevents the scientific machinery from handling the uncertainty in the
    interpretation into account as well.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The predictive models are often built just for data understanding. From the
    linguistic derivation, model is a simplified representation of the actual complex
    buildings or processes for exactly the purpose of making a point and convincing
    people, one or another way. The ultimate goal for predictive modeling, the modeling
    I am concerned about in this book and this chapter specifically, is to optimize
    the business processes by taking the most important factors into account in order
    to make the world a better place. This was certainly a sentence with a lot of
    uncertainty entrenched, but at least it looks like a much better goal than optimizing
    a click-through rate.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a traditional business decision-making process: a traditional
    business might involve a set of C-level executives making decisions based on information
    that is usually obtained from a set of dashboards with graphical representation
    of the data in one or several DBs. The promise of an automated data-driven business
    is to be able to automatically make most of the decisions provided the uncertainties
    eliminating human bias. This is not to say that we no longer need C-level executives,
    but the C-level executives will be busy helping the machines to make the decisions
    instead of the other way around.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Going through the basics of influence diagrams as a tool for decision making
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at variations of the pure decision making optimization in the context
    of adaptive **Markov Decision** making process and **Kelly Criterion**
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting familiar with at least three different practical strategies for exploration-exploitation
    trade-off
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing the architecture of a data-driven enterprise
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing major architectural components of a decision-making pipeline
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting familiar with standard tools for building data pipelines
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Influence diagrams
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the decision making process can have multiple facets, a book about decision
    making under uncertainty would be incomplete without mentioning influence diagrams
    (*Influence Diagrams for Team Decision Analysis*, Decision Analysis 2 (4): 207–228),
    which help the analysis and understanding of the decision-making process. The
    decision may be as mundane as selection of the next news article to show to a
    user in a personalized environment or a complex one as detecting malware on an
    enterprise network or selecting the next research project.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the weather she can try and go on a boat trip. We can represent
    the decision-making process as a diagram. Let''s decide whether to take a river
    boat tour during her stay in Portland, Oregon:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Influence diagrams](img/image01635.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: Figure 02-1\. A simple vacation influence diagram to represent a simple decision-making
    process. The diagram contains decision nodes such as Vacation Activity, observable
    and unobservable information nodes such as Weather Forecast and Weather, and finally
    the value node such as Satisfaction
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram represents this situation. The decision whether to participate
    in the activity is clearly driven by the potential to get certain satisfaction,
    which is a function of the decision itself and the weather at the time of the
    activity. While the actual weather conditions are unknown at the time of the trip
    planning, we believe there is a certain correlation between the weather forecast
    and the actual weather experienced during the trip, which is represented by the
    edge between the **Weather** and **Weather Forecast** nodes. The **Vacation Activity**
    node is the decision node, it has only one parent as the decision is made solely
    based on **Weather Forecast**. The final node in the DAG is **Satisfaction**,
    which is a function of the actual whether and the decision we made during the
    trip planning—obviously, *yes + good weather* and *no + bad weather* are likely
    to have the highest scores. The *yes + bad weather* and *no + good weather* would
    be a bad outcome—the latter case is probably just a missed opportunity, but not
    necessarily a bad decision, provided an inaccurate weather forecast.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The absence of an edge carries an independence assumption. For example, we believe
    that **Satisfaction** should not depend on **Weather Forecast**, as the latter
    becomes irrelevant once we are on the boat. Once the vacation plan is finalized,
    the actual weather during the boating activity can no longer affect the decision,
    which was made solely based on the weather forecast; at least in our simplified
    model, where we exclude the option of buying a trip insurance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph shows different stages of decision making and the flow of information
    (we will provide an actual graph implementation in Scala in [Chapter 7](part0283.xhtml#aid-8DSF61
    "Chapter 7. Working with Graph Algorithms"), *Working with Graph Algorithms*).
    There is only one piece of information required to make the decision in our simplified
    diagram: the weather forecast. Once the decision is made, we can no longer change
    it, even if we have information about the actual weather at the time of the trip.
    The weather and the decision data can be used to model her satisfaction with the
    decision she has made.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s map this approach to an advertising problem as an illustration: the
    ultimate goal is to get user satisfaction with the targeted ads, which results
    in additional revenue for an advertiser. The satisfaction is the function of user-specific
    environmental state, which is unknown at the time of decision making. Using machine
    learning algorithms, however, we can forecast this state based on the user''s
    recent Web visit history and other information that we can gather, such as geolocation,
    browser-agent string, time of day, category of the ad, and so on (refer to *Figure
    02-2*).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'While we are unlikely to measure the level of dopamine in the user''s brain,
    which will certainly fall under the realm of measurable metrics and probably reduce
    the uncertainty, we can measure the user satisfaction indirectly by the user''s
    actions, either the fact that they responded to the ad or even the measure of
    time the user spent between the clicks to browse relevant information, which can
    be used to estimate the effectiveness of our modeling and algorithms. Here is
    an influence diagram, similar to the one for "vacation", adjusted for the advertising
    decision-making process:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![Influence diagrams](img/image01636.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Figure 02-2\. The vacation influence diagram adjusted to the online advertising
    decision-making case. The decisions for online advertising can be made thousand
    times per second
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: The actual process might be more complex, representing a chain of decisions,
    each one depending on a few previous time slices. For example, the so-called **Markov
    Chain Decision Process**. In this case, the diagram might be repeated over multiple
    time slices.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Yet another example might be Enterprise Network Internet malware analytics system.
    In this case, we try to detect network connections indicative of either **command
    and control** (**C2**), lateral movement, or data exfiltration based on the analysis
    of network packets flowing through the enterprise switches. The goal is to minimize
    the potential impact of an outbreak with minimum impact on the functioning systems.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: One of the decisions we might take is to reimage a subset of nodes or to at
    least isolate them. The data we collect may contain uncertainty—many benign software
    packages may send traffic in suspicious ways, and the models need to differentiate
    between them based on the risk and potential impact. One of the decisions in this
    specific case may be to collect additional information.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: I will leave it to the reader to map this and other potential business cases
    to the corresponding diagram as an exercise. Let's consider a more complex optimization
    problem now.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Sequential trials and dealing with risk
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What if my preferences for making an extra few dollars outweigh the risk of
    losing the same amount? I will stop on why one''s preferences might be asymmetric
    in a little while in this section, and there is scientific evidence that this
    asymmetry is ingrained in our minds for evolutionary reasons, but you are right,
    I have to optimize the expected value of the asymmetric function of the parameterized
    utility now, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/image01637.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'Why would an asymmetric function surface in the analysis? One example is repeated
    bets or re-investments, also known as the Kelly Criterion problem. Although originally,
    the Kelly Criterion was developed for a specific case of binary outcome as in
    a gambling machine and the optimization of the fraction of money to bet in each
    round (*A New Interpretation of Information Rate*, Bell System Technical Journal
    35 (4): 917–926, 1956), a more generic formulation as an re-investment problem
    involves a probabilistic distribution of possible returns.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'The return over multiple bets is a product of individual return rates on each
    of the bets—the return rate is the ratio between the bankroll after the bet to
    the original bankroll before each individual bet, as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/image01638.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: 'This does not help us much to optimize the total return as we don''t know how
    to optimize the product of *i.i.d*. random variables. However, we can convert
    the product to a sum using log transformation and apply the **central limit theorem**
    (**CLT**) to approximate the sum of *i.i.d*. variables (provided that the distribution
    of *r* *[i]* is subect to CLT conditions, for example, has a finite mean and variance),
    as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/image01639.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Thus, the cumulative result of making *N* bets would look like the result of
    making *N* bets with expected return of ![Sequential trials and dealing with risk](img/image01640.jpeg),
    and not ![Sequential trials and dealing with risk](img/image01641.jpeg)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'As I mentioned before, the problem is most often applied for the case of binary
    bidding, although it can be easily generalized, in which case there is an additional
    parameter: *x*, the amount of money to bid in each round. Let''s say I make a
    profit of *W* with probability *p* or completely lose my bet otherwise with the
    probability *(1-p)*. Optimizing the expected return with respect to the following
    additional parameter:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/image01642.jpeg)![Sequential
    trials and dealing with risk](img/image01643.jpeg)![Sequential trials and dealing
    with risk](img/image01644.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: The last equation is the Kelly Criterion ratio and gives you the optimal amount
    to bet.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason that one might bet less than the total amount is that even if the
    average return is positive, there is still a possibility to lose the whole bankroll,
    particularly, in highly skewed situations. For example, even if the probability
    of making *10 x* on your bet is *0.105* (*W = 10*, the expected return is *5%)*,
    the combinatorial analysis show that even after *60* bets, there is roughly a
    *50%* chance that the overall return will be negative, and there is an *11%* chance,
    in particular, of losing *(57 - 10 x 3) = 27* times your bet or more:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note that to recover the *27 x* amount, one would need to play only ![Sequential
    trials and dealing with risk](img/image01645.jpeg) additional rounds on average
    with these favourable odds, but one must have something to bet to start with.
    The Kelly Criterion provides that the optimal is to bet only *1.55%* of our bankroll.
    Note that if I bet the whole bankroll, I would lose all my money with 89.5% certainty
    in the first round (the probability of a win is only *0.105*). If I bet only a
    fraction of the bankroll, the chances of staying in the game are infinitely better,
    but the overall returns are smaller. The plot of expected log of return is shown
    in *Figure 02-3* as a function of the portions of the bankroll to bet, *x*, and
    possible distribution of outcomes in 60 bets that I just computed. In 24% of the
    games we''ll do worse than the lower curve, in 39% worse than the next curve,
    in about half—44%—a gambler we''ll do the same or better than the black curve
    in the middle, and in 30% of cases better than the top one. The optimal Kelly
    Criterion value for *x* is *0.0155*, which will eventually optimize the overall
    return over infinitely many rounds:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/image01646.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: Figure 02-3\. The expected log of return as a function of the bet amount and
    possible outcomes in 60 rounds (see equation (2.2))
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The Kelly Criterion has been criticized for being both too aggressive (gamblers
    tend to overestimate their winning potential/ratio and underestimate the probability
    of a ruin), as well as for being too conservative (the value at risk should be
    the total available capital, not just the bankroll), but it demonstrates one of
    the examples where we need to compensate our intuitive understanding of the "benefit"
    with some additional transformations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'From the financial point of view, the Kelly Criterion is a much better description
    of risk than the standard definition as volatility or variance of the returns.
    For a generic parametrized payoff distribution, *y(z)*, with a probability distribution
    function, *f(z)*, the equation (2.3) can be reformulated as follows. after the
    substitution *r(x) = 1 + x y(z)*, where *x* is still the amount to bet:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/image01647.jpeg)![Sequential
    trials and dealing with risk](img/image01648.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'It can also be written in the following manner in the discrete case:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/image01649.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: Here, the denominator emphasizes the contributions from the regions with negative
    payoffs. Specifically, the possibility of losing all your bankroll is exactly
    where the denominator ![Sequential trials and dealing with risk](img/image01650.jpeg)
    is zero.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned before, interestingly, risk aversion is engrained in our intuitions
    and there seems to be a natural risk-aversion system of preferences encoded in
    both humans and primates (*A Monkey Economy as Irrational as Ours* by Laurie Santos,
    TED talk, 2010). Now enough about monkeys and risk, let's get into another rather
    controversial subject—the exploration-exploitation trade-off, where one might
    not even know the payoff trade-offs initially.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Exploration and exploitation
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The exploration-exploitation trade-off is another problem that has its apparent
    origin within gambling, even though the real applications range from allocation
    of funding to research projects to self-driving cars. The traditional formulation
    is a multi-armed bandit problem, which refers to an imaginary slot machine with
    one or more arms. Sequential plays of each arm generate *i.i.d* `.` returns with
    unknown probabilities for each arm; the successive plays are independent in the
    simplified models. The rewards are assumed to be independent across the arms.
    The goal is to maximize the reward—for example, the amount of money won, and to
    minimize the learning loss, or the amount spend on the arms with less than optimal
    winning rate, provided an agreed upon arm selection policy. The obvious trade-off
    is between the **exploration** in search of an arm that produces the best return
    and **exploitation** of the best-known arm with optimal return:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploration and exploitation](img/image01651.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: 'The **pseudo-regret** is then the difference:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploration and exploitation](img/image01652.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: Here, ![Exploration and exploitation](img/image01653.jpeg) is the *i^(th)* arm
    selection out of *N* trials. The multi-armed bandit problem was extensively studied
    in the 1930s and again during the early 2000s, with the application in finance
    and ADTECH. While in general, due to stochastic nature of the problem, it is not
    possible to provide a bound on the expected regret better than the square root
    of *N*, the pseudo-regret can be controlled so that we are able to bound it by
    a log of *N* (*Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit
    Problems* by Sebastien Bubeck and Nicolo Cesa-Bianchi, [http://arxiv.org/pdf/1204.5721.pdf](http://arxiv.org/pdf/1204.5721.pdf)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common strategies used in practice is epsilon strategies, where
    the optimal arm is chosen with the probability of ![Exploration and exploitation](img/image01654.jpeg)
    and one of the other arms with the remaining probability. The drawback of this
    approach is that we might spend a lot of exploration resources on the arms that
    are never going to provide any rewards. The UCB strategy improves the epsilon
    strategy by choosing an arm with the largest estimate of the return, plus some
    multiple or fraction of the standard deviation of the return estimates. The approach
    needs the recomputation of the best arm to pull at each round and suffers from
    approximations made to estimate the mean and standard deviation. Besides, UCB
    requires the recomputation of the estimates for each successive pull, which might
    be a scalability problem.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the Thompson sampling strategy uses a fixed random sample from Beta-Bernoulli
    posterior estimates and assigns the next arm to the one that gives the minimal
    expected regret, for which real data can be used to avoid parameter recomputation.
    Although the specific numbers may depend on the assumptions, one available comparison
    for these model performances is provided in the following diagram:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploration and exploitation](img/image01655.jpeg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Figure 02-3\. The simulation results for different exploration exploitation
    strategies for K = 5, one-armed bandits, and different strategies.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 02-3* shows simulation results for different strategies (taken from
    the Rich Relevance website at [http://engineering.richrelevance.com/recommendations-thompson-sampling](http://engineering.richrelevance.com/recommendations-thompson-sampling)).
    The **Random** strategy just allocates the arms at random and corresponds to pure
    exploration. The **Naive** strategy is random up to a certain threshold and than
    switches to pure Exxploitation mode. **Upper Confidence Bound** (**UCB**) with
    95% confidence level. UCB1 is a modification of UCB to take into account the log-normality
    of the distributions. Finally the Thompson sampling strategy makes a random sample
    from actual posterior distribution to optimize the regret.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Exploration/exploitation models are known to be very sensitive to the initial
    conditions and outliers, particularly on the low-response side. One can spend
    enormous amount of trials on the arms that are essentially dead.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Other improvements on the strategies are possible by estimating better priors
    based on additional information, such as location, or limiting the set of arms
    to explore—*K*—due to such additional information, but these aspects are more
    domain-specific (such as personalization or online advertising).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Unknown unknowns
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unknown unknowns have been largely made famous due to a phrase from a response
    the United States Secretary of Defense, Donald Rumsfeld, gave to a question at
    a United States **Department of Defense** (**DoD**) news briefing on February
    12, 2002 about the lack of evidence linking the government of Iraq with the supply
    of weapons of mass destruction to terrorist groups, and books by Nassim Taleb
    (*The Black Swan: The Impact of the Highly Improbable* by Nassim Taleb, Random
    House, 2007).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Turkey paradox**'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, the unknown unknown is better explained by the turkey paradox. Suppose
    you have a family of turkeys playing in the backyard and enjoying protection and
    free food. Across the fence, there is another family of turkeys. This all works
    day after day, and month after month, until Thanksgiving comes—Thanksgiving Day
    is a national holiday celebrated in Canada and the United States, where it's customary
    to roast the turkeys in an oven. The turkeys are very likely to be harvested and
    consumed at this point, although from the turkey's point of view, there is no
    discernable signal that anything will happen on the second Monday of October in
    Canada and the fourth Thursday of November in the United States. No amount of
    modeling on the within-the-year data can fix this prediction problem from the
    turkey's point of view besides the additional year-over-year information.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The unknown unknown is something that is not in the model and cannot be anticipated
    to be in the model. In reality, the only unknown unknowns that are of interest
    are the ones that affect the model so significantly that the results that were
    previously virtually impossible, or possible with infinitesimal probability, now
    become the reality. Given that most of the practical distributions are from exponential
    family with really thin tails, the deviation from normal does not have to be more
    than a few sigmas to have devastating results on the standard model assumptions.
    While one has still to come up with an actionable strategy of how to include the
    unknown factors in the model—a few ways have been proposed, including fractals,
    but few if any are actionable—the practitioners have to be aware of the risks,
    and here the definition of the risk is exactly the possibility of delivering the
    models useless. Of course, the difference between the known unknown and unknown
    unknown is exactly that we understand the risks and what needs to be explored.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: As we looked at the basic scope of problems that the decision-making systems
    are facing, let's look at the data pipelines, the software systems that provide
    information for making the decisions, and more practical aspects of designing
    the data pipeline for a data-driven system.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Basic components of a data-driven system
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In short, a data-driven architecture contains the following components—at least
    all the systems I''ve seen have them—or can be reduced to these components:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingest**: We need to collect the data from systems and devices. Most
    of the systems have logs, or at least an option to write files into a local filesystem.
    Some can have capabilities to report information to network-based interfaces such
    as syslog, but the absence of persistence layer usually means potential data loss,
    if not absence of audit information.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation layer**: It was also historically called **extract, transform,
    and load** (**ETL**). Today the data transformation layer can also be used to
    have real-time processing, where the aggregates are computed on the most recent
    data. The data transformation layer is also traditionally used to reformat and
    index the data to be efficiently accessed by a UI component of algorithms down
    the pipeline.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data analytics and machine learning engine**: The reason this is not part
    of the standard data transformation layer is usually that this layer requires
    quite different skills. The mindset of people who build reasonable statistical
    models is usually different from people who make terabytes of data move fast,
    even though occasionally I can find people with both skills. Usually, these unicorns
    are called data scientists, but the skills in any specific field are usually inferior
    to ones who specialize in a particular field. We need more of either, though.
    Another reason is that machine learning, and to a certain extent, data analysis,
    requires multiple aggregations and passes over the same data, which as opposed
    to a more stream-like ETL transformations, requires a different engine.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UI component**: Yes, UI stands for user interface, which most often is a
    set of components that allow you to communicate with the system via a browser
    (it used to be a native GUI, but these days the web-based JavaScript or Scala-based
    frameworks are much more powerful and portable). From the data pipeline and modeling
    perspective, this component offers an API to access internal representation of
    data and models.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions engine**: This is usually a configurable rules engine to optimize
    the provided metrics based on insights. The actions may be either real-time, like
    in online advertising, in which case the engine should be able to supply real-time
    scoring information, or a recommendation for a user action, which can take the
    form of an e-mail alert.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlation engine**: This is an emerging component that may analyze the
    output of data analysis and machine learning engine to infer additional insights
    into data or model behavior. The actions might also be triggered by an output
    from this layer.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**: This is a complex system will be incomplete without logging,
    monitoring, and some way to change system parameters. The purpose of monitoring
    is to have a nested decision-making system regarding the optimal health of the
    system and either to mitigate the problem(*s*) automatically or to alert the system
    administrators about the problem(*s*).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's discuss each of the components in detail in the following sections.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Data ingest
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the proliferation of smart devices, information gathering has become less
    of a problem and more of a necessity for any business that does more than a type-written
    text. For the purpose of this chapter, I will assume that the device or devices
    are connected to the Internet or have some way of passing this information via
    home dialing or direct network connection.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'The major purpose of this component is to collect all relevant information
    that can be relevant for further data-driven decision making. The following table
    provides details on the most common implementations of the data ingest:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | When used | Comments |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| **Syslog** | Syslog is one of the most common standards to pass messages
    between the machines on Unix. Syslog usually listens on port 514 and the transport
    protocol can be configured either with UDP (unreliable) or with TCP. The latest
    enhanced implementation on CentOS and Red Hat Linux is rsyslog, which includes
    many advanced options such as regex-based filtering that is useful for system-performance
    tuning and debugging. Apart from slightly inefficient raw message representation—plain
    text, which might be inefficient for long messages with repeated strings—the syslog
    system can support tens of thousands of messages per second. | Syslog is one of
    the oldest protocols developed in the 1980s by Eric Allman as part of Sendmail.
    While it does not guarantee delivery or durability, particularly for distributed
    systems, it is one of the most widespread protocols for message passing. Some
    of the later frameworks, such as Flume and Kafka, have syslog interfaces as well.
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| **Rsync** | Rsync is a younger framework developed in the 1990s. If the data
    is put in the flat files on a local filesystem, rsync might be an option. While
    rsync is more traditionally used to synchronize two directories, it also can be
    run periodically to transfer log data in batches. Rsync uses a recursive algorithm
    invented by an Australian computer programmer, Andrew Tridgell, for efficiently
    detecting the differences and transmitting a structure (such as a file) across
    a communication link when the receiving computer already has a similar, but not
    identical, version of the same structure. While it incurs extra communication,
    it is better from the point of durability, as the original copy can always be
    retrieved. It is particularly appropriate if the log data is known to arrive in
    batches in the first place (such as uploads or downloads). | Rsync has been known
    to be hampered by network bottlenecks, as it ultimately passes more information
    over the network when comparing the directory structures. However, the transferred
    files may be compressed when passed over the network. The network bandwidth can
    be limited per command-line flags. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| **Flume** | Flume is one of the youngest frameworks developed by Cloudera
    in 2009-2011 and open sourced. Flume—we refer to the more popular flume-ng implementation
    as Flume as opposed to an older regular Flume—consists of sources, pipes, and
    sinks that may be configured on multiple nodes for high availability and redundancy
    purposes. Flume was designed to err on the reliability side at the expense of
    possible duplication of data. Flume passes the messages in the **Avro** format,
    which is also open sourced and the transfer protocol, as well as messages can
    be encoded and compressed. | While Flume originally was developed just to ship
    records from a file or a set of files, it can also be configured to listen to
    a port, or even grab the records from a database. Flume has multiple adapters
    including the preceding syslog. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| **Kafka** | Kafka is the latest addition to the log-processing framework
    developed by LinkedIn and is open sourced. Kafka, compared to the previous frameworks,
    is more like a distributed reliable message queue. Kafka keeps a partitioned,
    potentially between multiple distributed machines; buffer and one can subscribe
    to or unsubscribe from getting messages for a particular topic. Kafka was built
    with strong reliability guarantees in mind, which is achieved through replication
    and consensus protocol. | Kafka might not be appropriate for small systems (<
    five nodes) as the benefits of the fully distributed system might be evident only
    at larger scales. Kafka is commercially supported by Confluent. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: The transfer of information usually occurs in batches, or micro batches if the
    requirements are close to real time. Usually the information first ends up in
    a file, traditionally called log, in a device's local filesystem, and then is
    transferred to a central location. Recently developed Kafka and Flume are often
    used to manage these transfers, together with a more traditional syslog, rsync,
    or netcat. Finally, the data can be placed into a local or distributed storage
    such as HDFS, Cassandra, or Amazon S3.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation layer
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the data ends up in HDFS or other storage, the data needs to be made
    available for processing. Traditionally, the data is processed on a schedule and
    ends up partitioned by time-based buckets. The processing can happen daily or
    hourly, or even on a sub-minute basis with the new Scala streaming framework,
    depending on the latency requirements. The processing may involve some preliminary
    feature construction or vectorization, even though it is traditionally considered
    a machine-learning task. The following table summarizes some available frameworks:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | When used | Comments |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| **Oozie** | This is one of the oldest open source frameworks developed by
    Yahoo. This has good integration with big data Hadoop tools. It has limited UI
    that lists the job history. | The whole workflow is put into one big XML file,
    which might be considered a disadvantage from the modularity point of view. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| **Azkaban** | This is an alternative open source workflow-scheduling framework
    developed by LinkedIn. Compared to Oozie, this arguably has a better UI. The disadvantage
    is that all high-level tasks are executed locally, which might present a scalability
    problem. | The idea behind Azkaban is to create a fully modularized drop-in architecture
    where the new jobs/tasks can be added with as few modifications as possible. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| **StreamSets** | StreamSets is the latest addition build by the former Informix
    and Cloudera developers. It has a very developed UI and supports a much richer
    set of input sources and output destinations. | This is a fully UI-driven tool
    with an emphasis on data curation, for example, constantly monitoring the data
    stream for problems and abnormalities. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: Separate attention should be given to stream-processing frameworks, where the
    latency requirements are reduced to one or a few records at a time. First, stream
    processing usually requires much more resources dedicated to processing, as it
    is more expensive to process individual records at a time as opposed to batches
    of records, even if it is tens or hundreds of records. So, the architect needs
    to justify the additional costs based on the value of more recent result, which
    is not always warranted. Second, stream processing requires a few adjustments
    to the architecture as handling the more recent data becomes a priority; for example,
    a delta architecture where the more recent data is handled by a separate substream
    or a set of nodes became very popular recently with systems such as **Druid**
    ([http://druid.io](http://druid.io)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Data analytics and machine learning
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the purpose of this chapter, **Machine Learning** (**ML**) is any algorithm
    that can compute aggregates or summaries that are actionable. We will cover more
    complex algorithms from [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib* to [Chapter 6](part0273.xhtml#aid-84B9I2
    "Chapter 6. Working with Unstructured Data"), *Working with Unstructured Data*,
    but in some cases, a simple sliding-window average and deviation from the average
    may be sufficient signal for taking an action. In the past few years, it just
    works in A/B testing somehow became a convincing argument for model building and
    deployment. I am not speculating that solid scientific principles might or might
    not apply, but many fundamental assumptions such as *i.i.d.*, balanced designs,
    and the thinness of the tail just fail to hold for many big data situation. Simpler
    models tend to be faster and to have better performance and stability.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: For example, in online advertising, one might just track average performance
    of a set of ads over a certain similar properties over times to make a decision
    whether to have this ad displayed. The information about anomalies, or deviation
    from the previous behavior, may signal a new unknown unknown, which signals that
    the old data no longer applies, in which case, the system has no choice but to
    start the new exploration cycle.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: I will talk about more complex non-structured, graph, and pattern mining later
    in [Chapter 6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working with Unstructured
    Data"), *Working with Unstructured Data*, [Chapter 8](part0288.xhtml#aid-8IL202
    "Chapter 8. Integrating Scala with R and Python"), *Integrating Scala with R and
    Python* and [Chapter 9](part0291.xhtml#aid-8LGJM2 "Chapter 9. NLP in Scala"),
    *NLP in Scala*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: UI component
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Well, UI is for wimps! Just joking...maybe it's too harsh, but in reality, UI
    usually presents a syntactic sugar that is necessary to convince the population
    beyond the data scientists. A good analyst should probably be able to figure out
    t-test probabilities by just looking at a table with numbers.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: However, one should probably apply the same methodologies we used at the beginning
    of the chapter, assessing the usefulness of different components and the amount
    of cycles put into them. The presence of a good UI is often justified, but depends
    on the target audience.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'First, there are a number of existing UIs and reporting frameworks. Unfortunately,
    most of them are not aligned with the functional programming methodologies. Also,
    the presence of complex/semi-structured data, which I will describe in [Chapter
    6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working with Unstructured Data"), *Working
    with Unstructured Data* in more detail, presents a new twist that many frameworks
    are not ready to deal with without implementing some kind of DSL. Here are a few
    frameworks for building the UI in a Scala project that I find particularly worthwhile:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | When used | Comments |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| **Scala Swing** | If you used Swing components in Java and are proficient
    with them, Scala Swing is a good choice for you. Swing component is arguably the
    least portable component of Java, so your mileage can vary on different platforms.
    | The `Scala.swing` package uses the standard Java Swing library under the hood,
    but it has some nice additions. Most notably, as it''s made for Scala, it can
    be used in a much more concise way than the standard Swing. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| **Lift** | Lift is a secure, developer-centric, scalable, and interactive
    framework written in Scala. Lift is open sourced under Apache 2.0 license. | The
    open source Lift framework was launched in 2007 by David Polak, who was dissatisfied
    with certain aspects of the Ruby on Rails framework. Any existing Java library
    and web container can be used in running Lift applications. Lift web applications
    are thus packaged as WAR files and deployed on any servlet 2.4 engine (for example,
    Tomcat 5.5.xx, Jetty 6.0, and so on). Lift programmers may use the standard Scala/Java
    development toolchain, including IDEs such as Eclipse, NetBeans, and IDEA. Dynamic
    web content is authored via templates using standard HTML5 or XHTML editors. Lift
    applications also benefit from native support for advanced web development techniques,
    such as Comet and Ajax. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| **Play** | Play is arguably better aligned with Scala as a functional language
    than any other platform—it is officially supported by Typesafe, the commercial
    company behind Scala. The Play framework 2.0 builds on Scala, Akka, and sbt to
    deliver superior asynchronous request handling, fast and reliable. Typesafe templates,
    and a powerful build system with flexible deployment options. Play is open sourced
    under Apache 2.0 license. | The open source Play framework was created in 2007
    by Guillaume Bort, who sought to bring a fresh web development experience inspired
    by modern web frameworks like Ruby on Rails to the long-suffering Java web development
    community. Play follows a familiar stateless model-view-controller architectural
    pattern, with a philosophy of convention-over-configuration and an emphasis on
    developer productivity. Unlike traditional Java web frameworks with their tedious
    compile-package-deploy-restart cycles, updates to Play applications are instantly
    visible with a simple browser refresh. |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| **Dropwizard** | The dropwizard ([www.dropwizard.io](http://www.dropwizard.io))
    project is an attempt to build a generic RESTful framework in both Java and Scala,
    even though one might end up using more Java than Scala. What is nice about this
    framework is that it is flexible enough to be used with arbitrary complex data
    (including semi-structured).This is licensed under Apache License 2.0. | RESTful
    API assumes state, while functional languages shy away from using state. Unless
    you are flexible enough to deviate from a pure functional approach, this framework
    is probably not good enough for you. |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| **Slick** | While Slick is not a UI component, it is Typesafe''s modern database
    query and access library for Scala, which can serve as a UI backend. It allows
    you to work with the stored data almost as if you were using Scala collections,
    while at the same time, giving you full control over when a database access occurs
    and what data is transferred. You can also use SQL directly. Use it if all of
    your data is purely relational. This is open sourced under BSD-Style license.
    | Slick was started in 2012 by Stefan Zeiger and maintained mainly by Typesafe.
    It is useful for mostly relational data. |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| **NodeJS** | Node.js is a JavaScript runtime, built on Chrome''s V8 JavaScript
    engine. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight
    and efficient. Node.js'' package ecosystem, npm, is the largest ecosystem of open
    source libraries in the world. It is open sourced under MIT License. | Node.js
    was first introduced in 2009 by Ryan Dahl and other developers working at Joyent.
    Originally Node.js supported only Linux, but now it runs on OS X and Windows.
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| **AngularJS** | AngularJS ([https://angularjs.org](https://angularjs.org))
    is a frontend development framework, built to simplify development of one-page
    web applications. This is open sourced under MIT License. | AngularJS was originally
    developed in 2009 by Misko Hevery at Brat Tech LLC. AngularJS is mainly maintained
    by Google and by a community of individual developers and corporations, and thus
    is specifically for Android platform (support for IE8 is dropped in versions 1.3
    and later). |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: Actions engine
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this is the heart of the data-oriented system pipeline, it is also arguably
    the easiest one. Once the system of metrics and values is known, the system decides,
    based on the known equations, whether to take a certain set of actions or not,
    based on the information provided. While the triggers based on a threshold is
    the most common implementation, the significance of probabilistic approaches that
    present the user with a set of possibilities and associated probabilities is emerging—or
    just presenting the user with the top *N* relevant choices like a search engine
    does.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The management of the rules might become pretty involved. It used to be that
    managing the rules with a rule engine, such as **Drools** ([http://www.drools.org](http://www.drools.org)),
    was sufficient. However, managing complex rules becomes an issue that often requires
    development of a DSL (*Domain-Specific Languages* by Martin Fowler, Addison-Wesley,
    2010). Scala is particularly fitting language for the development of such an actions
    engine.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Correlation engine
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The more complex the decision-making system is, the more it requires a secondary
    decision-making system to optimize its management. DevOps is turning into DataOps
    (*Getting Data Right* by Michael Stonebraker et al., Tamr, 2015). Data collected
    about the performance of a data-driven system are used to detect anomalies and
    semi-automated maintenance.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Models are often subject to time drift, where the performance might deteriorate
    either due to the changes in the data collection layers or the behavioral changes
    in the population (I will cover model drift in [Chapter 10](part0297.xhtml#aid-8R7N21
    "Chapter 10. Advanced Model Monitoring"), *Advanced Model Monitoring*). Another
    aspect of model management is to track model performance, and in some cases, use
    "collective" intelligence of the models by various consensus schemes.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring a system involves collecting information about system performance
    either for audit, diagnostic, or performance-tuning purposes. While it is related
    to the issues raised in the previous sections, monitoring solution often incorporates
    diagnostic and historical storage solutions and persistence of critical data,
    such as a black box on an airplane. In the Java and, thus, Scala world, a popular
    tool of choice is Java performance beans, which can be monitored in the Java Console.
    While Java natively supports MBean for exposing JVM information over JMX, **Kamon**
    ([http://kamon.io](http://kamon.io)) is an open source library that uses this
    mechanism to specifically expose Scala and Akka metrics.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Some other popular monitoring open source solutions are **Ganglia** ([http://ganglia.sourceforge.net/](http://ganglia.sourceforge.net/))
    and **Graphite** ([http://graphite.wikidot.com](http://graphite.wikidot.com)).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: I will stop here, as I will address system and model monitoring in more detail
    in [Chapter 10](part0297.xhtml#aid-8R7N21 "Chapter 10. Advanced Model Monitoring"),
    *Advanced Model Monitoring*.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Optimization and interactivity
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the data collected can be just used for understanding the business, the
    final goal of any data-driven business is to optimize the business behavior by
    automatically making data-based and model-based decisions. We want to reduce human
    intervention to minimum. The following simplified diagram can be depicted as a
    cycle:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '![Optimization and interactivity](img/image01656.jpeg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: Figure 02-4\. The predictive model life cycle
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The cycle is repeated over and over for new information coming into the system.
    The parameters of the system may be tuned to improve the overall system performance.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Feedback loops
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While humans are still likely to be kept in the loop for most of the systems,
    last few years saw an emergence of systems that can manage the complete feedback
    loop on their own—ranging from advertisement systems to self-driving cars.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: The classical formulation of this problem is the optimal control theory, which
    is also an optimization problem to minimize cost functional, given a set of differential
    equations describing the system. An optimal control is a set of control policies
    to minimize the cost functional given constraints. For example, the problem might
    be to find a way to drive the car to minimize its fuel consumption, given that
    it must complete a given course in a time not exceeding some amount. Another control
    problem is to maximize profit for showing ads on a website, provided the inventory
    and time constraints. Most software packages for optimal control are written in
    other languages such as C or MATLAB (PROPT, SNOPT, RIOTS, DIDO, DIRECT, and GPOPS),
    but can be interfaced with Scala.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: However, in many cases, the parameters for the optimization or the state transition,
    or differential equations, are not known with certainty. **Markov Decision Processes**
    (**MDPs**) provide a mathematical framework to model decision making in situations
    where outcomes are partly random and partly under the control of the decision
    maker. In MDPs, we deal with a discrete set of possible states and a set of actions.
    The "rewards" and state transitions depend both on the state and actions. MDPs
    are useful for studying a wide range of optimization problems solved via dynamic
    programming and reinforcement learning.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I described a high-level architecture and approach to design
    a data-driven enterprise. I also introduced you to influence diagrams, a tool
    for understanding how the decisions are made in traditional and data-driven enterprises.
    I stopped on a few key models, such as Kelly Criterion and multi-armed bandit,
    essential to demonstrate the issues from the mathematical point of view. I built
    on top of this to introduce some Markov decision process approaches where we deal
    with decision policies based on the results of the previous decisions and observations.
    I delved into more practical aspects of building a data pipeline for decision-making,
    describing major components and frameworks that can be used to built them. I also
    discussed the issues of communicating the data and modeling results between different
    stages and nodes, presenting the results to the user, feedback loop, and monitoring.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, I will describe MLlib, a library for machine learning over
    distributed set of nodes written in Scala.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3. Working with Spark and MLlib
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are powered with the knowledge of where and how statistics and machine
    learning fits in the global data-driven enterprise architecture, let's stop at
    the specific implementations in Spark and MLlib, a machine learning library on
    top of Spark. Spark is a relatively new member of the big data ecosystem that
    is optimized for memory usage as opposed to disk. The data can be still spilled
    to disk as necessary, but Spark does the spill only if instructed to do so explicitly,
    or if the active dataset does not fit into the memory. Spark stores lineage information
    to recompute the active dataset if a node goes down or the information is erased
    from memory for some other reason. This is in contrast to the traditional MapReduce
    approach, where the data is persisted to the disk after each map or reduce task.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Spark is particularly suited for iterative or statistical machine learning algorithms
    over a distributed set of nodes and can scale out of core. The only limitation
    is the total memory and disk space available across all Spark nodes and the network
    speed. I will cover the basics of Spark architecture and implementation in this
    chapter.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: One can direct Spark to execute data pipelines either on a single node or across
    a set of nodes with a simple change in the configuration parameters. Of course,
    this flexibility comes at a cost of slightly heavier framework and longer setup
    times, but the framework is very parallelizable and as most of modern laptops
    are already multithreaded and sufficiently powerful, this usually does not present
    a big issue.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring Spark if you haven't done so yet
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning the basics of Spark architecture and why it is inherently tied to the
    Scala language
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning why Spark is the next technology after sequential implementations and
    Hadoop MapReduce
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about Spark components
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the simple implementation of word count in Scala and Spark
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at the streaming word count implementation
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing how to create Spark DataFrames from either a distributed file or a distributed
    database
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about Spark performance tuning
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Spark
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you haven''t done so yet, you can download the pre-build Spark package from
    [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    The latest release at the time of writing is **1.6.1**:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up Spark](img/image01657.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: Figure 03-1\. The download site at http://spark.apache.org with recommended
    selections for this chapter
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can build the Spark by downloading the full source distribution
    from [https://github.com/apache/spark](https://github.com/apache/spark):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The command will download the necessary dependencies and create the `spark-2.0.0-SNAPSHOT-bin-alex-spark-build-2.6-yarn.tgz`
    file in the Spark directory; the version is 2.0.0, as it is the next release version
    at the time of writing. In general, you do not want to build from trunk unless
    you are interested in the latest features. If you want a released version, you
    can checkout the corresponding tag. Full list of available versions is available
    via the `git branch –r` command. The `spark*.tgz` file is all you need to run
    Spark on any machine that has Java JRE.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: The distribution comes with the `docs/building-spark.md` document that describes
    other options for building Spark and their descriptions, including incremental
    Scala compiler, zinc. Full Scala 2.11 support is in the works for the next Spark
    2.0.0 release.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spark architecture
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A parallel execution involves splitting the workload into subtasks that are
    executed in different threads or on different nodes. Let's see how Spark does
    this and how it manages execution and communication between the subtasks.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Task scheduling
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark workload splitting is determined by the number of partitions for **Resilient
    Distributed Dataset** (**RDD**), the basic abstraction in Spark, and the pipeline
    structure. An RDD represents an immutable, partitioned collection of elements
    that can be operated on in parallel. While the specifics might depend on the mode
    in which Spark runs, the following diagram captures the Spark task/resource scheduling:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![Task scheduling](img/image01658.jpeg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: Figure 03-2\. A generic Spark task scheduling diagram. While not shown explicitly
    in the figure, Spark Context opens an HTTP UI, usually on port 4040 (the concurrent
    contexts will open 4041, 4042, and so on), which is present during a task execution.
    Spark Master UI is usually 8080 (although it is changed to 18080 in CDH) and Worker
    UI is usually 7078\. Each node can run multiple executors, and each executor can
    run multiple tasks.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You will find that Spark, as well as Hadoop, has a lot of parameters. Some of
    them are specified as environment variables (refer to the `$SPARK_HOME/conf/spark-env.sh`
    file), and yet some can be given as a command-line parameter. Moreover, some files
    with pre-defined names can contain parameters that will change the Spark behavior,
    such as `core-site.xml`. This might be confusing, and I will cover as much as
    possible in this and the following chapters. If you are working with **Hadoop
    Distributed File System** (**HDFS**), then the `core-site.xml` and `hdfs-site.xml`
    files will contain the pointer and specifications for the HDFS master. The requirement
    for picking this file is that it has to be on `CLASSPATH` Java process, which,
    again, may be set by either specifying `HADOOP_CONF_DIR` or `SPARK_CLASSPATH`
    environment variables. As is usual with open source, you need to grep the code
    sometimes to understand how various parameters work, so having a copy of the source
    tree on your laptop is a good idea.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Each node in the cluster can run one or more executors, and each executor can
    schedule a sequence of tasks to perform the Spark operations. Spark driver is
    responsible for scheduling the execution and works with the cluster scheduler,
    such as Mesos or YARN to schedule the available resources. Spark driver usually
    runs on the client machine, but in the latest release, it can also run in the
    cluster under the cluster manager. YARN and Mesos have the ability to dynamically
    manage the number of executors that run concurrently on each node, provided the
    resource constraints.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Standalone mode, **Spark Master** does the work of the cluster scheduler—it
    might be less efficient in allocating resources, but it''s better than nothing
    in the absence of preconfigured Mesos or YARN. Spark standard distribution contains
    shell scripts to start Spark in Standalone mode in the `sbin` directory. Spark
    Master and driver communicate directly with one or several Spark workers that
    run on individual nodes. Once the master is running, you can start Spark shell
    with the following command:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tip
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that you can always run Spark in local mode, which means that all tasks
    will be executed in a single JVM, by specifying `--master local[2]`, where `2`
    is the number of threads that have to be at least `2`. In fact, we will be using
    the local mode very often in this book for running small examples.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark shell is an application from the Spark point of view. Once you start
    a Spark application, you will see it under **Running Applications** in the Spark
    Master UI (or in the corresponding cluster manager), which can redirect you to
    the Spark application HTTP UI at port 4040, where one can see the subtask execution
    timeline and other important properties such as environment setting, classpath,
    parameters passed to the JVM, and information on resource usage (refer to *Figure
    3-3*):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '![Task scheduling](img/image01659.jpeg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: Figure 03-3\. Spark Driver UI in Standalone mode with time decomposition
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw, with Spark, one can easily switch between local and cluster mode
    by providing the `--master` command-line option, setting a `MASTER` environment
    variable, or modifying `spark-defaults.conf`, which should be on the classpath
    during the execution, or even set explicitly using the `setters` method on the
    `SparkConf` object directly in Scala, which will be covered later:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '| Cluster Manager | MASTER env variable | Comments |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| Local (single node, multiple threads) | `local[n]` | *n* is the number of
    threads to use, should be greater than or equal to *2*. If you want Spark to communicate
    with other Hadoop tools such as Hive, you still need to point it to the cluster
    by either setting the `HADOOP_CONF_DIR` environment variable or copying the Hadoop
    `*-site.xml` configuration files into the `conf` subdirectory. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| Standalone (Daemons running on the nodes) | `spark:// master-address>:7077`
    | This has a set of start/stop scripts in the `$SPARK_HOME/sbin` directory. This
    also supports the HA mode. More details can be found at [https://spark.apache.org/docs/latest/spark-standalone.html](https://spark.apache.org/docs/latest/spark-standalone.html).
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| Mesos | `mesos://host:5050` or `mesos://zk://host:2181`(multimaster) | Here,
    you need to set `MESOS_NATIVE_JAVA_LIBRARY=<path to libmesos.so>` and `SPARK_EXECUTOR_URI=<URL
    of spark-1.5.0.tar.gz>`. The default is fine-grained mode, where each Spark task
    runs as a separate Mesos task. Alternatively, the user can specify the coarse-grained
    mode, where the Mesos tasks persists for the duration of the application. The
    advantage is lower total start-up costs. This can use dynamic allocation (refer
    to the following URL) in coarse-grained mode. More details can be found at [https://spark.apache.org/docs/latest/running-on-mesos.html](https://spark.apache.org/docs/latest/running-on-mesos.html).
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| YARN | `yarn` | Spark driver can run either in the cluster or on the client
    node, which is managed by the `--deploy-mode` parameter (cluster or client, shell
    can only run in the client mode). Set `HADOOP_CONF_DIR` or `YARN_CONF_DIR` to
    point to the YARN config files. Use the `--num-executors` flag or `spark.executor.instances`
    property to set a fixed number of executors (default).Set `spark.dynamicAllocation.enabled`
    to `true` to dynamically create/kill executors depending on the application demand.
    More details are available at [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html).
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: 'The most common ports are 8080, the master UI, and 4040, the application UI.
    Other Spark ports are summarized in the following table:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '| Standalone ports |   |   |   |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| From | To | Default Port | Purpose | Configuration Setting |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| Browser | Standalone Master | 8080 | Web UI | `spark.master.ui.port /SPARK_MASTER_WEBUI_PORT`
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| Browser | Standalone worker | 8081 | Web UI | `spark.worker.ui.port /SPARK_WORKER_WEBUI_PORT`
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| Driver / Standalone worker | Standalone Master | 7077 | Submit job to cluster
    / Join cluster | `SPARK_MASTER_PORT` |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| Standalone master | Standalone worker | (random) | Schedule executors | `SPARK_WORKER_PORT`
    |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| Executor / Standalone master | Driver | (random) | Connect to application
    / Notify executor state changes | `spark.driver.port` |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| **Other ports** |   |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| **From** | **To** | **Default Port** | **Purpose** | **Configuration Setting**
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| Browser | Application | 4040 | Web UI | `spark.ui.port` |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| Browser | History server | 18080 | Web UI | `spark.history.ui.port` |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| Driver | Executor | (random) | Schedule tasks | `spark.executor.port` |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| Executor | Driver | (random) | File server for files and jars | `spark.fileserver.port`
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| Executor | Driver | (random) | HTTP broadcast | `spark.broadcast.port` |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: Also, some of the documentation is available with the source distribution in
    the `docs` subdirectory, but may be out of date.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Spark components
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the emergence of Spark, multiple applications that benefit from Spark''s
    ability to cache RDDs have been written: Shark, Spork (Pig on Spark), graph libraries
    (GraphX, GraphFrames), streaming, MLlib, and so on; some of these will be covered
    here and in later chapters.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, I will cover major architecture components to collect, store,
    and analyze the data in Spark. While I will cover a more complete data life cycle
    architecture in [Chapter 2](part0242.xhtml#aid-76P842 "Chapter 2. Data Pipelines
    and Modeling"), *Data Pipelines and Modeling*, here are Spark-specific components:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark components](img/image01660.jpeg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: Figure 03-4\. Spark architecture and components.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: MQTT, ZeroMQ, Flume, and Kafka
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All of these are different ways to reliably move data from one place to another
    without loss and duplication. They usually implement a publish-subscribe model,
    where multiple writers and readers can write and read from the same queues with
    different guarantees. Flume stands out as a first distributed log and event management
    implementation, but it is slowly replaced by Kafka, a fully functional publish-subscribe
    distributed message queue optionally persistent across a distributed set of nodes
    developed at LinkedIn. We covered Flume and Kafka briefly in the previous chapter.
    Flume configuration is file-based and is traditionally used to deliver messages
    from a Flume source to one or several Flume sinks. One of the popular sources
    is `netcat`—listening on raw data over a port. For example, the following configuration
    describes an agent receiving data and then writing them to HDFS every 30 seconds
    (default):'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This file is included as part of the code provided with this book in the `chapter03/conf`
    directory. Let''s download and start Flume agent (check the MD5 sum with one provided
    at [http://flume.apache.org/download.html](http://flume.apache.org/download.html)):'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, in a separate window, you can type a `netcat` command to send text to
    the Flume agent:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The Flume agent will first create a `*.tmp` file and then rename it to a file
    without extension (the file extension can be used to filter out files being written
    to):'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, each row is a Unix time in milliseconds and data received. In this case,
    we put the data into HDFS, from where they can be analyzed by a Spark/Scala program,
    we can exclude the files being written to by the `*.tmp` filename pattern. However,
    if you are really interested in up-to-the-last-minute values, Spark, as well as
    some other platforms, supports streaming, which I will cover in a few sections.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: HDFS, Cassandra, S3, and Tachyon
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'HDFS, Cassandra, S3, and Tachyon are the different ways to get the data into
    persistent storage and compute nodes as necessary with different guarantees. HDFS
    is a distributed storage implemented as a part of Hadoop, which serves as the
    backend for many products in the Hadoop ecosystem. HDFS divides each file into
    blocks, which are 128 MB in size by default, and stores each block on at least
    three nodes. Although HDFS is reliable and supports HA, a general complaint about
    HDFS storage is that it is slow, particularly for machine learning purposes. Cassandra
    is a general-purpose key/value storage that also stores multiple copies of a row
    and can be configured to support different levels of consistency to optimize read
    or write speeds. The advantage that Cassandra over HDFS model is that it does
    not have a central master node; the reads and writes are completed based on the
    consensus algorithm. This, however, may sometimes reflect on the Cassandra stability.
    S3 is the Amazon storage: The data is stored off-cluster, which affects the I/O
    speed. Finally, the recently developed Tachyon claims to utilize node''s memory
    to optimize access to working sets across the nodes.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, new backends are being constantly developed, for example, Kudu
    from Cloudera ([http://getkudu.io/kudu.pdf](http://getkudu.io/kudu.pdf)) and **Ignite
    File System** (**IGFS**) from GridGain ([http://apacheignite.gridgain.org/v1.0/docs/igfs)](http://apacheignite.gridgain.org/v1.0/docs/igfs).
    Both are open source and Apache-licensed.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Mesos, YARN, and Standalone
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned before, Spark can run under different cluster resource schedulers.
    These are various implementations to schedule Spark''s containers and tasks on
    the cluster. The schedulers can be viewed as cluster kernels, performing functions
    similar to the operating system kernel: resource allocation, scheduling, I/O optimization,
    application services, and UI.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Mesos is one of the original cluster managers and is built using the same principles
    as the Linux kernel, only at a different level of abstraction. A Mesos slave runs
    on every machine and provides API's for resource management and scheduling across
    entire datacenter and cloud environments. Mesos is written in C++.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: YARN is a more recent cluster manager developed by Yahoo. Each node in YARN
    runs a **Node Manager**, which communicates with the **Resource Manager** which
    may run on a separate node. The resource manager schedules the task to satisfy
    memory and CPU constraints. The Spark driver itself can run either in the cluster,
    which is called the cluster mode for YARN. Otherwise, in the client mode, only
    Spark executors run in the cluster and the driver that schedules Spark pipelines
    runs on the same machine that runs Spark shell or submit program. The Spark executors
    will talk to the local host over a random open port in this case. YARN is written
    in Java with the consequences of unpredictable GC pauses, which might make latency's
    long tail fatter.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if none of these resource schedulers are available, the standalone
    deployment mode starts a `org.apache.spark.deploy.worker.Worker` process on each
    node that communicates with the Spark Master process run as `org.apache.spark.deploy.master.Master`.
    The worker process is completely managed by the master and can run multiple executors
    and tasks (refer to *Figure 3-2*).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: In practical implementations, it is advised to track the program parallelism
    and required resources through driver's UI and adjust the parallelism and available
    memory, increasing the parallelism if necessary. In the following section, we
    will start looking at how Scala and Scala in Spark address different problems.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's consider a few practical examples and libraries in Spark/Scala starting
    with a very traditional problem of word counting.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Word count
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most modern machine learning algorithms require multiple passes over data. If
    the data fits in the memory of a single machine, the data is readily available
    and this does not present a performance bottleneck. However, if the data becomes
    too large to fit into RAM, one has a choice of either dumping pieces of the data
    on disk (or database), which is about 100 times slower, but has a much larger
    capacity, or splitting the dataset between multiple machines across the network
    and transferring the results. While there are still ongoing debates, for most
    practical systems, analysis shows that storing the data over a set of network
    connected nodes has a slight advantage over repeatedly storing and reading it
    from hard disks on a single node, particularly if we can split the workload effectively
    between multiple CPUs.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An average disk has bandwidth of about 100 MB/sec and transfers with a few mms
    latency, depending on the rotation speed and caching. This is about 100 times
    slower than reading the data from memory, depending on the data size and caching
    implementation again. Modern data bus can transfer data at over 10 GB/sec. While
    the network speed still lags behind the direct memory access, particularly with
    standard TCP/IP kernel networking layer overhead, specialized hardware can reach
    tens of GB/sec and if run in parallel, it can be potentially as fast as reading
    from the memory. In practice, the network-transfer speeds are somewhere between
    1 to 10 GB/sec, but still faster than the disk in most practical systems. Thus,
    we can potentially fit the data into combined memory of all the cluster nodes
    and perform iterative machine learning algorithms across a system of them.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'One problem with memory, however, is that it is does not persist across node
    failures and reboots. A popular big data framework, Hadoop, made possible with
    the help of the original Dean/Ghemawat paper (Jeff Dean and Sanjay Ghemawat, *MapReduce:
    Simplified Data Processing on Large Clusters*, OSDI, 2004.), is using exactly
    the disk layer persistence to guarantee fault tolerance and store intermediate
    results. A Hadoop MapReduce program would first run a `map` function on each row
    of a dataset, emitting one or more key/value pairs. These key/value pairs then
    would be sorted, grouped, and aggregated by key so that the records with the same
    key would end up being processed together on the same reducer, which might be
    running on same or another node. The reducer applies a `reduce` function that
    traverses all the values that were emitted for the same key and aggregates them
    accordingly. The persistence of intermediate results would guarantee that if a
    reducer fails for one or another reason, the partial computations can be discarded
    and the reduce computation can be restarted from the checkpoint-saved results.
    Many simple ETL-like applications traverse the dataset only once with very little
    information preserved as state from one record to another.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, one of the traditional applications of MapReduce is word count.
    The program needs to count the number of occurrences of each word in a document
    consisting of lines of text. In Scala, the word count is readily expressed as
    an application of the `foldLeft` method on a sorted list of words:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If I run this program, the output will be a list of (word, count) tuples. The
    program splits the lines into words, sorts the words, and then matches each word
    with the latest entry in the list of (word, count) tuples. The same computation
    in MapReduce would be expressed as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: First, we need to process each line of the text by splitting the line into words
    and generation `(word, 1)` pairs. This task is easily parallelized. Then, to parallelize
    the global count, we need to split the counting part by assigning a task to do
    the count for a subset of words. In Hadoop, we compute the hash of the word and
    divide the work based on the value of the hash.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Once the map task finds all the entries for a given hash, it can send the key/value
    pairs to the reducer, the sending part is usually called shuffle in MapReduce
    vernacular. A reducer waits until it receives all the key/value pairs from all
    the mappers, combines the values—a partial combine can also happen on the mapper,
    if possible—and computes the overall aggregate, which in this case is just sum.
    A single reducer will see all the values for a given word.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the log output of the word count operation in Spark (Spark is
    very verbose by default, you can manage the verbosity level by modifying the `conf/log4j.properties`
    file by replacing `INFO` with `ERROR` or `FATAL`):'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'At this stage, the only thing that happened is metadata manipulations, Spark
    has not touched the data itself. Spark estimates that the size of the dataset
    and the number of partitions. By default, this is the number of HDFS blocks, but
    we can specify the minimum number of partitions explicitly with the `minPartitions`
    parameter:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We just defined another RDD derived from the original `linesRdd`:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Word count over 2 GB of text data—40,291 lines and 353,087 words—took under
    a second to read, split, and group by words.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'With extended logging, you could see the following:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Spark opens a few ports to communicate with the executors and users
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark UI runs on port 4040 on `http://localhost:4040`
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can read the file either from local or distributed storage (HDFS, Cassandra,
    and S3)
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark will connect to Hive if Spark is built with Hive support
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark uses lazy evaluation and executes the pipeline only when necessary or
    when output is required
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark uses internal scheduler to split the job into tasks, optimize the execution,
    and execute the tasks
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results are stored into RDDs, which can either be saved or brought into
    RAM of the node executing the shell with collect method
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The art of parallel performance tuning is to split the workload between different
    nodes or threads so that the overhead is relatively small and the workload is
    balanced.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Streaming word count
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark supports listening on incoming streams, partitioning it, and computing
    aggregates close to real-time. Currently supported sources are Kafka, Flume, HDFS/S3,
    Kinesis, Twitter, as well as the traditional MQs such as ZeroMQ and MQTT. In Spark,
    streaming is implemented as micro-batches. Internally, Spark divides input data
    into micro-batches, usually from subseconds to minutes in size and performs RDD
    aggregation operations on these micro-batches.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s extend the Flume example that we covered earlier. We''ll
    need to modify the Flume configuration file to create a Spark polling sink. Instead
    of HDFS, replace the sink section:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, instead of writing to HDFS, Flume will wait for Spark to poll for data:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To run the program, start the Flume agent in one window:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then run the `FlumeWordCount` object in another:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, any text typed to the `netcat` connection will be split into words and
    counted every two seconds for a six second sliding window:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Spark/Scala allows to seamlessly switch between the streaming sources. For
    example, the same program for Kafka publish/subscribe topic model looks similar
    to the following:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To start the Kafka broker, first download the latest binary distribution and
    start ZooKeeper. ZooKeeper is a distributed-services coordinator and is required
    by Kafka even in a single-node deployment:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In another window, start the Kafka server:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Run the `KafkaWordCount` object:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, publishing the stream of words into the Kafka topic will produce the window
    counts:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you see, the programs output every two seconds. Spark streaming is sometimes
    called **micro-batch processing**. Streaming has many other applications (and
    frameworks), but this is too big of a topic to be entirely considered here and
    needs to be covered separately. I'll cover some ML on streams of data in [Chapter
    5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression and Classification"), *Regression
    and Classification*. Now, let's get back to more traditional SQL-like interfaces.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL and DataFrame
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DataFrame was a relatively recent addition to Spark, introduced in version 1.3,
    allowing one to use the standard SQL language for data analysis. We already used
    some SQL commands in [Chapter 1](part0235.xhtml#aid-703K61 "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis* for the exploratory data analysis.
    SQL is really great for simple exploratory analysis and data aggregations.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: According to the latest poll results, about 70% of Spark users use DataFrame.
    Although DataFrame recently became the most popular framework for working with
    tabular data, it is a relatively heavyweight object. The pipelines that use DataFrames
    may execute much slower than the ones that are based on Scala's vector or LabeledPoint,
    which will be discussed in the next chapter. The evidence from different developers
    is that the response times can be driven to tens or hundreds of milliseconds depending
    on the query, from submillisecond on simpler objects.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark implements its own shell for SQL, which can be invoked in addition to
    the standard Scala REPL shell: `./bin/spark-sql` can be used to access the existing
    Hive/Impala or relational DB tables:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In standard Spark''s REPL, the same query can be performed by running the following
    command:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ML libraries
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark, particularly with memory-based storage systems, claims to substantially
    improve the speed of data access within and between nodes. ML seems to be a natural
    fit, as many algorithms require multiple passes over the data, or repartitioning.
    MLlib is the open source library of choice, although private companies are catching,
    up with their own proprietary implementations.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'As I will chow in [Chapter 5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression
    and Classification"), *Regression and Classification*, most of the standard machine
    learning algorithms can be expressed as an optimization problem. For example,
    classical linear regression minimizes the sum of squares of *y* distance between
    the regression line and the actual value of *y*:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '![ML libraries](img/image01661.jpeg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![ML libraries](img/image01662.jpeg) are the predicted values according
    to the linear expression:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '![ML libraries](img/image01663.jpeg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
- en: '*A* is commonly called the slope, and *B* the intercept. In a more generalized
    formulation, a linear optimization problem is to minimize an additive function:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '![ML libraries](img/image01664.jpeg)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![ML libraries](img/image01665.jpeg) is a loss function and ![ML libraries](img/image01666.jpeg)
    is a regularization function. The regularization function is an increasing function
    of model complexity, for example, the number of parameters (or a natural logarithm
    thereof). Most common loss functions are given in the following table:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Loss function L | Gradient |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| Linear | ![ML libraries](img/image01667.jpeg) | ![ML libraries](img/image01668.jpeg)
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| Logistic | ![ML libraries](img/image01669.jpeg) | ![ML libraries](img/image01670.jpeg)
    |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| Hinge | ![ML libraries](img/image01671.jpeg) | ![ML libraries](img/image01672.jpeg)
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: 'The purpose of the regularizer is to penalize more complex models to avoid
    overfitting and improve generalization error: more MLlib currently supports the
    following regularizers:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Regularizer R | Gradient |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| L2 | ![ML libraries](img/image01673.jpeg) | ![ML libraries](img/image01674.jpeg)
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| L1 | ![ML libraries](img/image01675.jpeg) | ![ML libraries](img/image01676.jpeg)
    |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| Elastic net | ![ML libraries](img/image01677.jpeg) | ![ML libraries](img/image01678.jpeg)
    |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: Here, *sign(w)* is the vector of the signs of all entries of *w*.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, MLlib includes implementation of the following algorithms:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic statistics:'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary statistics
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlations
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stratified sampling
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming significance testing
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Random data generation
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classification and regression:'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear models (SVMs, logistic regression, and linear regression)
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembles of trees (Random Forests and Gradient-Boosted Trees)
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Isotonic regression
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Collaborative filtering:'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternating least squares** (**ALS**)'
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clustering:'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian mixture
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power Iteration Clustering** (**PIC**)'
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Dirichlet allocation** (**LDA**)'
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting k-means
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming k-means
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimensionality reduction:'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**)'
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal Component Analysis** (**PCA**)'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction and transformation
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frequent pattern mining:'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FP-growth?Association rules
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PrefixSpan
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimization:'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent** (**SGD**)'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited-Memory BFGS** (**L-BFGS**)'
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: I will go over some of the algorithms in [Chapter 5](part0260.xhtml#aid-7NUI81
    "Chapter 5. Regression and Classification"), *Regression and Classification*.
    More complex non-structured machine learning methods will be considered in [Chapter
    6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working with Unstructured Data"), *Working
    with Unstructured Data*.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: SparkR
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: R is an implementation of popular S programming language created by John Chambers
    while working at Bell Labs. R is currently supported by the **R Foundation for
    Statistical Computing**. R's popularity has increased in recent years according
    to polls. SparkR provides a lightweight frontend to use Apache Spark from R. Starting
    with Spark 1.6.0, SparkR provides a distributed DataFrame implementation that
    supports operations such as selection, filtering, aggregation, and so on, which
    is similar to R DataFrames, dplyr, but on very large datasets. SparkR also supports
    distributed machine learning using MLlib.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: SparkR required R version 3 or higher, and can be invoked via the `./bin/sparkR`
    shell. I will cover SparkR in [Chapter 8](part0288.xhtml#aid-8IL202 "Chapter 8. Integrating
    Scala with R and Python"), *Integrating Scala with R and Python*.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Graph algorithms – GraphX and GraphFrames
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graph algorithms are one of the hardest to correctly distribute between nodes,
    unless the graph itself is naturally partitioned, that is, it can be represented
    by a set of disconnected subgraphs. Since the social networking analysis on a
    multi-million node scale became popular due to companies such as Facebook, Google,
    and LinkedIn, researches have been coming up with new approaches to formalize
    the graph representations, algorithms, and types of questions asked.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: 'GraphX is a modern framework for graph computations described in a 2013 paper
    (*GraphX: A Resilient Distributed Graph System on Spark* by Reynold Xin, Joseph
    Gonzalez, Michael Franklin, and Ion Stoica, GRADES (SIGMOD workshop), 2013). It
    has graph-parallel frameworks such as Pregel, and PowerGraph as predecessors.
    The graph is represented by two RDDs: one for vertices and another one for edges.
    Once the RDDs are joined, GraphX supports either Pregel-like API or MapReduce-like
    API, where the map function is applied to the node''s neighbors and reduce is
    the aggregation step on top of the map results.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, GraphX includes the implementation for the following
    graph algorithms:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: PageRank
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connected components
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triangle counting
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label propagation
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD++ (collaborative filtering)
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strongly connected components
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As GraphX is an open source library, changes to the list are expected. GraphFrames
    is a new implementation from Databricks that fully supports the following three
    languages: Scala, Java, and Python, and is build on top of DataFrames. I''ll discuss
    specific implementations in [Chapter 7](part0283.xhtml#aid-8DSF61 "Chapter 7. Working
    with Graph Algorithms"), *Working with Graph Algorithms*.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Spark performance tuning
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While efficient execution of the data pipeline is prerogative of the task scheduler,
    which is part of the Spark driver, sometimes Spark needs hints. Spark scheduling
    is primarily driven by the two parameters: CPU and memory. Other resources, such
    as disk and network I/O, of course, play an important part in Spark performance
    as well, but neither Spark, Mesos or YARN can currently do anything to actively
    manage them.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter to watch is the number of RDD partitions, which can be specified
    explicitly when reading the RDD from a file. Spark usually errs on the side of
    too many partitions as it provides more parallelism, and it does work in many
    cases as the task setup/teardown times are relatively small. However, one might
    experiment with decreasing the number of partitions, especially if one does aggregations.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: The default number of partitions per RDD and the level of parallelism is determined
    by the `spark.default.parallelism` parameter, defined in the `$SPARK_HOME/conf/spark-defaults.conf`
    configuration file. The number of partitions for a specific RDD can also be explicitly
    changed by the `coalesce()` or `repartition()` methods.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: The total number of cores and available memory is often the reason for deadlocks
    as the tasks cannot proceed further. One can specify the number of cores for each
    executor with the `--executor-cores` flag when invoking spark-submit, spark-shell,
    or PySpark from the command line. Alternatively, one can set the corresponding
    parameters in the `spark-defaults.conf` file discussed earlier. If the number
    of cores is set too high, the scheduler will not be able to allocate resources
    on the nodes and will deadlock.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: In a similar way, `--executor-memory` (or the `spark.executor.memory` property)
    specifies the requested heap size for all the tasks (the default is 1g). If the
    executor memory is specified too high, again, the scheduler may be deadlocked
    or will be able to schedule only a limited number of executors on a node.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: 'The implicit assumption in Standalone mode when counting the number of cores
    and memory is that Spark is the only running application—which may or may not
    be true. When running under Mesos or YARN, it is important to configure the cluster
    scheduler that it has the resources available to schedule the executors requested
    by the Spark Driver. The relevant YARN properties are: `yarn.nodemanager.resource.cpu-vcores`
    and `yarn.nodemanager.resource.memory-mb`. YARN may round the requested memory
    up a little. YARN''s `yarn.scheduler.minimum-allocation-mb` and `yarn.scheduler.increment-allocation-mb`
    properties control the minimum and increment request values respectively.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: JVMs can also use some memory off heap, for example, for interned strings and
    direct byte buffers. The value of the `spark.yarn.executor.memoryOverhead` property
    is added to the executor memory to determine the full memory request to YARN for
    each executor. It defaults to max (*384, .07 * spark.executor.memory*).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: Since Spark can internally transfer the data between executors and client node,
    efficient serialization is very important. I will consider different serialization
    frameworks in [Chapter 6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working with Unstructured
    Data"), *Working with Unstructured Data*, but Spark uses Kryo serialization by
    default, which requires the classes to be registered explicitly in a static method.
    If you see a serialization error in your code, it is likely because the corresponding
    class has not been registered or Kryo does not support it, as it happens with
    too nested and complex data types. In general, it is recommended to avoid complex
    objects to be passed between the executors unless the object serialization can
    be done very efficiently.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: 'Driver has similar parameters: `spark.driver.cores`, `spark.driver.memory`,
    and `spark.driver.maxResultSize`. The latter one sets the limit for the results
    collected from all the executors with the `collect` method. It is important to
    protect the driver process from out-of-memory exceptions. The other way to avoid
    out-of-memory exceptions and consequent problems are to either modify the pipeline
    to return aggregated or filtered results or use the `take` method instead.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: Running Hadoop HDFS
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A distributed processing framework wouldn't be complete without distributed
    storage. One of them is HDFS. Even if Spark is run on local mode, it can still
    use a distributed file system at the backend. Like Spark breaks computations into
    subtasks, HDFS breaks a file into blocks and stores them across a set of machines.
    For HA, HDFS stores multiple copies of each block, the number of copies is called
    replication level, three by default (refer to *Figure 3-5*).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '**NameNode** is managing the HDFS storage by remembering the block locations
    and other metadata such as owner, file permissions, and block size, which are
    file-specific. **Secondary Namenode** is a slight misnomer: its function is to
    merge the metadata modifications, edits, into fsimage, or a file that serves as
    a metadata database. The merge is required, as it is more practical to write modifications
    of fsimage to a separate file instead of applying each modification to the disk
    image of the fsimage directly (in addition to applying the corresponding changes
    in memory). Secondary **Namenode** cannot serve as a second copy of the **Namenode**.
    A **Balancer** is run to move the blocks to maintain approximately equal disk
    usage across the servers—the initial block assignment to the nodes is supposed
    to be random, if enough space is available and the client is not run within the
    cluster. Finally, the **Client** communicates with the **Namenode** to get the
    metadata and block locations, but after that, either reads or writes the data
    directly to the node, where a copy of the block resides. The client is the only
    component that can be run outside the HDFS cluster, but it needs network connectivity
    with all the nodes in the cluster.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: 'If any of the node dies or disconnects from the network, the **Namenode** notices
    the change, as it constantly maintains the contact with the nodes via heartbeats.
    If the node does not reconnect to the **Namenode** within 10 minutes (by default),
    the **Namenode** will start replicating the blocks in order to achieve the required
    replication level for the blocks that were lost on the node. A separate block
    scanner thread in the **Namenode** will scan the blocks for possible bit rot—each
    block maintains a checksum—and will delete corrupted and orphaned blocks:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '![Running Hadoop HDFS](img/image01679.jpeg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
- en: Figure 03-5\. This is the HDFS architecture. Each block is stored in three separate
    locations (the replication level).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: 'To start HDFS on your machine (with replication level 1), download a Hadoop
    distribution, for example, from [http://hadoop.apache.org](http://hadoop.apache.org):'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To get the minimal HDFS configuration, modify the `core-site.xml` and `hdfs-site.xml`
    files, as follows:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This will put the Hadoop HDFS metadata and data directories under the `/tmp/hadoop-$USER`
    directories. To make this more permanent, we can add the `dfs.namenode.name.dir`,
    `dfs.namenode.edits.dir`, and `dfs.datanode.data.dir` parameters, but we will
    leave these out for now. For a more customized distribution, one can download
    a Cloudera version from [http://archive.cloudera.com/cdh](http://archive.cloudera.com/cdh).
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, we need to write an empty metadata:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then start the `namenode`, `secondarynamenode`, and `datanode` Java processes
    (I usually open three different command-line windows to see the logs, but in a
    production environment, these are usually daemonized):'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We are now ready to create the first HDFS file:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Of course, in this particular case, the actual file is stored only on one node,
    which is the same node we run `datanode` on (localhost). In my case, it is the
    following:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The Namenode UI can be found at `http://localhost:50070` and displays a host
    of information, including the HDFS usage and the list of DataNodes, the slaves
    of the HDFS Master node as follows:![Running Hadoop HDFS](img/image01680.jpeg)
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 03-6\. A snapshot of HDFS NameNode UI.
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding figure shows HDFS Namenode HTTP UI in a single node deployment
    (usually, `http://<namenode-address>:50070`). The **Utilities** | **Browse the
    file system** tab allows you to browse and download the files from HDFS. Nodes
    can be added by starting DataNodes on a different node and pointing to the Namenode
    with the `fs.defaultFS=<namenode-address>:8020` parameter. The Secondary Namenode
    HTTP UI is usually at `http:<secondarynamenode-address>:50090`.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Scala/Spark by default will use the local file system. However, if the `core-site/xml`
    file is on the classpath or placed in the `$SPARK_HOME/conf` directory, Spark
    will use HDFS as the default.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I covered the Spark/Hadoop and their relationship with Scala
    and functional programming at a very high level. I considered a classic word count
    example and it's implementation in Scala and Spark. I also provided high-level
    components of Spark ecosystem with specific examples of word count and streaming.
    I now have all the components to start looking at the specific implementation
    of classic machine learning algorithms in Scala/Spark. In the next chapter, I
    will start by covering supervised and unsupervised learning—a traditional division
    of learning algorithms for structured data.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4. Supervised and Unsupervised Learning
  id: totrans-493
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I covered the basics of the MLlib library in the previous chapter, but MLlib,
    at least at the time of writing this book, is more like a fast-moving target that
    is gaining the lead rather than a well-structured implementation that everyone
    uses in production or even has a consistent and tested documentation. In this
    situation, as people say, rather than giving you the fish, I will try to focus
    on well-established concepts behind the libraries and teach the process of fishing
    in this book in order to avoid the need to drastically modify the chapters with
    each new MLlib release. For better or worse, this increasingly seems to be a skill
    that a data scientist needs to possess.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: Statistics and machine learning inherently deal with uncertainty, due to one
    or another reason we covered in [Chapter 2](part0242.xhtml#aid-76P842 "Chapter 2. Data
    Pipelines and Modeling"), *Data Pipelines and Modeling*. While some datasets might
    be completely random, the goal here is to find trends, structure, and patterns
    beyond what a random number generator will provide you. The fundamental value
    of ML is that we can generalize these patterns and improve on at least some metrics.
    Let's see what basic tools are available within Scala/Spark.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I am covering supervised and unsupervised leaning, the two
    historically different approaches. Supervised learning is traditionally used when
    we have a specific goal to predict a label, or a specific attribute of a dataset.
    Unsupervised learning can be used to understand internal structure and dependencies
    between any attributes of a dataset, and is often used to group the records or
    attributes in meaningful clusters. In practice, both methods can be used to complement
    and aid each other.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: Learning standard models for supervised learning – decision trees and logistic
    regression
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing the staple of unsupervised learning – k-means clustering and its
    derivatives
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding metrics and methods to evaluate the effectiveness of the above
    algorithms
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having a glimpse of extending the above methods on special cases of streaming
    data, sparse data, and non-structured data
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Records and supervised learning
  id: totrans-502
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the purpose of this chapter, a record is an observation or measurement
    of one or several attributes. We assume that the observations might contain noise
    ![Records and supervised learning](img/image01681.jpeg) (or be inaccurate for
    one or other reason):'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: '![Records and supervised learning](img/image01682.jpeg)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
- en: While we believe that there is some pattern or correlation between the attributes,
    the one that we are after and want to uncover, the noise is uncorrelated across
    either the attributes or the records. In statistical terms, we say that the values
    for each record are drawn from the same distribution and are independent (or *i.i.d*.
    in statistical terms). The order of records does not matter. One of the attributes,
    usually the first, might be designated to be the label.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning is when the goal is to predict the label *yi*:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '![Records and supervised learning](img/image01683.jpeg)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
- en: Here, *N* is the number of remaining attributes. In other words, the goal is
    to generalize the patterns so that we can predict the label by just knowing the
    other attributes, whether because we cannot physically get the measurement or
    just want to explore the structure of the dataset without having the immediate
    goal to predict the label.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: The unsupervised learning is when we don't use the label—we just try to explore
    the structure and correlations to understand the dataset to, potentially, predict
    the label better. The number of problems in this latter category has increased
    recently with the emergence of learning for unstructured data and streams, each
    of which, I'll be covering later in the book in separate chapters.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: Iris dataset
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I will demonstrate the concept of records and labels based on one of the most
    famous datasets in machine learning, the Iris dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).
    The Iris dataset contains 50 records for each of the three types of Iris flower,
    150 lines of total five fields. Each line is a measurement of the following:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the final field being the type of the flower (*setosa*, *versicolor*,
    or *virginica*). The classic problem is to predict the label, which, in this case,
    is a categorical attribute with three possible values as a function of the first
    four attributes:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '![Iris dataset](img/image01684.jpeg)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
- en: 'One option would be to draw a plane in the four-dimensional space that separates
    all four labels. Unfortunately, as one can find out, while one of the classes
    is clearly separable, the remaining two are not, as shown in the following multidimensional
    scatterplot (we have used Data Desk software to create it):'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '![Iris dataset](img/image01685.jpeg)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
- en: Figure 04-1\. The Iris dataset as a three-dimensional plot. The Iris setosa
    records, shown by crosses, can be separated from the other two types based on
    petal length and width.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 'The colors and shapes are assigned according to the following table:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: '| Label | Color | Shape |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
- en: '| *Iris setosa* | Blue | x |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
- en: '| *Iris versicolor* | Green | Vertical bar |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
- en: '| *Iris virginica* | Purple | Horizontal bar |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
- en: The *Iris setosa* is separable because it happens to have a very short petal
    length and width compared to the two other types.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how we can use MLlib to find that separating multidimensional plane.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: Labeled point
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The labeled datasets used to have a very special place in ML—we will discuss
    unsupervised learning later in the chapter, where we do not need a label, so MLlib
    has a special data type to represent a record with a `org.apache.spark.mllib.regression.LabeledPoint`
    label (refer to [https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point](https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point)).
    To read the Iris dataset from a text file, we need to transform the original UCI
    repository file into the so-called LIBSVM text format. While there are plenty
    of converters from CSV to LIBSVM format, I''d like to use a simple AWK script
    to do the job:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note
  id: totrans-532
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Why do we need LIBSVM format?**'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: LIBSVM is the format that many libraries use. First, LIBSVM takes only continuous
    attributes. While a lot of datasets in the real world contain discrete or categorical
    attributes, internally they are always converted to a numerical representation
    for efficiency reasons, even if the L1 or L2 metrics on the resulting numerical
    attribute does not make much sense in the unordered discrete values. Second, the
    LIBSVM format allows for efficient sparse data representation. While the Iris
    dataset is not sparse, almost all of the modern big data sources are sparse, and
    the format allows for efficient storage by only storing the provided values. Many
    modern big data key-value and traditional RDBMS databases actually do the same
    for efficiency reasons.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: The code might be more complex for missing values, but we know that the Iris
    dataset is not sparse—otherwise we'd complement our code with a bunch of if statements.
    We mapped the last two labels to 1 for our purpose now.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: SVMWithSGD
  id: totrans-536
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s run the **Linear Support Vector Machine** (**SVM**) SVMWithSGD
    code from MLlib:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'So, you just run one of the most complex algorithms in the machine learning
    toolbox: SVM. The result is a separating plane that distinguishes *Iris setosa*
    flowers from the other two types. The model in this case is exactly the intercept
    and the coefficients of the plane that best separates the labels:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'If one looks under the hood, the model is stored in a `parquet` file, which
    can be dumped using `parquet-tool`:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The **Receiver Operating Characteristic** (**ROC**) is a common measure of the
    classifier to be able to correctly rank the records according to their numeric
    label. We will consider precision metrics in more detail in [Chapter 9](part0291.xhtml#aid-8LGJM2
    "Chapter 9. NLP in Scala"), *NLP in Scala*.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**What is ROC?**'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: 'ROC has emerged in signal processing with the first application to measure
    the accuracy of analog radars. The common measure of accuracy is area under ROC,
    which, shortly, is the probability of two randomly chosen points to be ranked
    correctly according to their labels (the *0* label should always have a lower
    rank than the *1* label). AUROC has a number of attractive characteristics:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: The value, at least theoretically, does not depend on the oversampling rate,
    that is, the rate at which we see *0* labels as opposed to *1* labels.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value does not depend on the sample size, excluding the expected variance
    due to the limited sample size.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a constant to the final score does not change the ROC, thus the intercept
    can always be set to *0*. Computing the ROC requires a sort with respect to the
    generated score.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, separating the remaining two labels is a harder problem since the
    plane that separated *Iris versicolor* from *Iris virginica* does not exist: the
    AUROC score will be less than *1.0*. However, the SVM method will find the plane
    that best differentiates between the latter two classes.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  id: totrans-551
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is one of the oldest classification methods. The outcome
    of the logistic regression is also a set of weights, which define the hyperplane,
    but the loss function is logistic loss instead of *L2*:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/image01686.jpeg)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
- en: 'Logit function is a frequent choice when the label is binary (as *y = +/- 1*
    in the above equation):'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The labels in this case can be any integer in the range *[0, k)*, where *k*
    is the total number of classes (the correct class will be determined by building
    multiple binary logistic regression models against the pivot class, which in this
    case, is the class with the *0* label) (*The Elements of Statistical Learning*
    by *Trevor Hastie*, *Robert Tibshirani*, *Jerome Friedman*, *Springer Series in
    Statistics*).
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy metric is precision, or the percentage of records predicted correctly
    (which is 95% in our case).
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree
  id: totrans-558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding two methods describe linear models. Unfortunately, the linear
    approach does not always work for complex interactions between attributes. Assume
    that the label looks like an exclusive *OR: 0* if *X ? Y* and *1* if *X = Y*:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '| X | Y | Label |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 1 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 1 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
- en: 'There is no hyperplane that can differentiate between the two labels in the
    *XY* space. Recursive split solution, where the split on each level is made on
    only one variable or a linear combination thereof might work a bit better in these
    case. Decision trees are also known to work well with sparse and interaction-rich
    datasets:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As you can see, the error (misprediction) rate on hold-out 30% sample is only
    2.6%. The 30% sample of 150 is only 45 records, which means we missed only 1 record
    from the whole test set. Certainly, the result might and will change with a different
    seed, and we need a more rigorous cross-validation technique to prove the accuracy
    of the model, but this is enough for a rough estimate of model performance.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree generalizes on regression case, that is, when the label is continuous
    in nature. In this case, the splitting criterion is minimization of weighted variance,
    as opposed to entropy gain or gini in the case of classification. I will talk
    more about the differences in [Chapter 5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression
    and Classification"), *Regression and Classification*.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of parameters, which can be tuned to improve the performance:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Description | Recommended value |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
- en: '| `maxDepth` | This is the maximum depth of the tree. Deep trees are costly
    and usually are more likely to overfit. Shallow trees are more efficient and better
    for bagging/boosting algorithms such as AdaBoost. | This depends on the size of
    the original dataset. It is worth experimenting and plotting the accuracy of the
    resulting tree versus the parameter to find out the optimum. |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
- en: '| `minInstancesPerNode` | This also limits the size of the tree: once the number
    of instances falls under this threshold, no further splitting occurs. | The value
    is usually 10-100, depending on the complexity of the original dataset and the
    number of potential labels. |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
- en: '| `maxBins` | This is used only for continuous attributes: the number of bins
    to split the original range. | Large number of bins increase computation and communication
    cost. One can also consider the option of pre-discretizing the attribute based
    on domain knowledge. |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
- en: '| `minInfoGain` | This is the amount of information gain (entropy), impurity
    (gini), or variance (regression) gain to split a node. | The default is *0*, but
    you can increase the default to limit the tree size and reduce the risk of overfitting.
    |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
- en: '| `maxMemoryInMB` | This is the amount of memory to be used for collecting
    sufficient statistics. | The default value is conservatively chosen to be 256
    MB to allow the decision algorithm to work in most scenarios. Increasing `maxMemoryInMB`
    can lead to faster training (if the memory is available) by allowing fewer passes
    over the data. However, there may be decreasing returns as `maxMemoryInMB` grows,
    as the amount of communication on each iteration can be proportional to `maxMemoryInMB`.
    |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
- en: '| `subsamplingRate` | This is the fraction of the training data used for learning
    the decision tree. | This parameter is most relevant for training ensembles of
    trees (using `RandomForest` and `GradientBoostedTrees`), where it can be useful
    to subsample the original data. For training a single decision tree, this parameter
    is less useful since the number of training instances is generally not the main
    constraint. |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
- en: '| `useNodeIdCache` | If this is set to true, the algorithm will avoid passing
    the current model (tree or trees) to executors on each iteration. | This can be
    useful with deep trees (speeding up computation on workers) and for large random
    forests (reducing communication on each iteration). |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
- en: '| `checkpointDir:` | This is the directory for checkpointing the node ID cache
    RDDs. | This is an optimization to save intermediate results to avoid recomputation
    in case of node failure. Set it in large clusters or with unreliable nodes. |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
- en: '| `checkpointInterval` | This is the frequency for checkpointing the node ID
    cache RDDs. | Setting this too low will cause extra overhead from writing to HDFS
    and setting this too high can cause problems if executors fail and the RDD needs
    to be recomputed. |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
- en: Bagging and boosting – ensemble learning methods
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a portfolio of stocks has better characteristics compared to individual equities,
    models can be combined to produce better classifiers. Usually, these methods work
    really well with decision trees as the training technique can be modified to produce
    models with large variations. One way is to train the model on random subsets
    of the original data or random subsets of attributes, which is called random forest.
    Another way is to generate a sequence of models, where misclassified instances
    are reweighted to get a larger weight in each subsequent iteration. It has been
    shown that this method has a relation to gradient descent methods in the model
    parameter space. While these are valid and interesting techniques, they usually
    require much more space in terms of model storage and are less interpretable compared
    to bare decision tree models. For Spark, the ensemble models are currently under
    development—the umbrella issue is SPARK-3703 ([https://issues.apache.org/jira/browse/SPARK-3703](https://issues.apache.org/jira/browse/SPARK-3703)).
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  id: totrans-584
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we get rid of the label in the Iris dataset, it would be nice if some algorithm
    could recover the original grouping, maybe without the exact label names—*setosa*,
    *versicolor*, and *virginica*. Unsupervised learning has multiple applications
    in compression and encoding, CRM, recommendation engines, and security to uncover
    internal structure without actually having the exact labels. The labels sometimes
    can be given base on the singularity in attribute value distributions. For example,
    *Iris setosa* can be described as a *Flower with Small Leaves*.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: While a supervised learning problem can always be cast as unsupervised by disregarding
    the label, the reverse is also true. A clustering algorithm can be cast as a density-estimation
    problem by assigning label *1* to all vectors and generating random vectors with
    label *0* (*The Elements of Statistical Learning* by *Trevor Hastie*, *Robert
    Tibshirani*, *Jerome Friedman*, *Springer Series in Statistics*). The difference
    between the two is formal and it's even fuzzier with non-structured and nested
    data. Often, running unsupervised algorithms in labeled datasets leads to a better
    understanding of the dependencies and thus a better selection and performance
    of the supervised algorithm.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular algorithms for clustering and unsupervised learning
    in k-means (and its variants, k-median and k-center, will be described later):'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: One can see that the first center, the one with index `0`, has petal length
    and width of `1.464` and `0.244`, which is much shorter than the other two—`5.715`
    and `2.054`, `4.389` and `1.434`). The prediction completely matches the first
    cluster, corresponding to *Iris setosa*, but has a few mispredictions for the
    other two.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: 'The measure of cluster quality might depend on the (desired) labels if we want
    to achieve a desired classification result, but since the algorithm has no information
    about the labeling, a more common measure is the sum of distances from centroids
    to the points in each of the clusters. Here is a graph of `WSSSE`, depending on
    the number of clusters:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As expected, the average distance is decreasing as more clusters are configured.
    A common method to determine the optimal number of clusters—in our example, we
    know that there are three types of flowers—is to add a penalty function. A common
    penalty is the log of the number of clusters as we expect a convex function. What
    would be the coefficient in front of log? If each vector is associated with its
    own cluster, the sum of all distances will be zero, so if we would like a metric
    that achieves approximately the same value at both ends of the set of possible
    values, `1` to `150`, the coefficient should be `680.8244/log(150)`:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here is how the sum of the squared distances with penalty looks as a graph:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning](img/image01687.jpeg)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
- en: Figure 04-2\. The measure of the clustering quality as a function of the number
    of clusters
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides k-means clustering, MLlib also has implementations of the following:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian mixture
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power Iteration Clustering** (**PIC**)'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Dirichlet Allocation** (**LDA**)'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming k-means
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Gaussian mixture is another classical mechanism, particularly known for
    spectral analysis. Gaussian mixture decomposition is appropriate, where the attributes
    are continuous and we know that they are likely to come from a set of Gaussian
    distributions. For example, while the potential groups of points corresponding
    to clusters may have the average for all attributes, say **Var1** and **Var2**,
    the points might be centered around two intersecting hyperplanes, as shown in
    the following diagram:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning](img/image01688.jpeg)'
  id: totrans-603
  prefs: []
  type: TYPE_IMG
- en: Figure 04-3\. A mixture of two Gaussians that cannot be properly described by
    k-means clustering
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: This renders the k-means algorithm ineffective as it will not be able to distinguish
    between the two (of course a simple non-linear transformation such as a distance
    to one of the hyperplanes will solve the problem, but this is where domain knowledge
    and expertise as a data scientist are handy).
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: PIC is using clustering vertices of a graph provided pairwise similarity measures
    given as edge properties. It computes a pseudo-eigenvector of the normalized affinity
    matrix of the graph via power iteration and uses it to cluster vertices. MLlib
    includes an implementation of PIC using GraphX as its backend. It takes an RDD
    of (`srcId`, `dstId`, similarity) tuples and outputs a model with the clustering
    assignments. The similarities must be non-negative. PIC assumes that the similarity
    measure is symmetric. A pair (`srcId`, `dstId`) regardless of the ordering should
    appear at most once in the input data. If a pair is missing from the input, their
    similarity is treated as zero.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: LDA can be used for clustering documents based on keyword frequencies. Rather
    than estimating a clustering using a traditional distance, LDA uses a function
    based on a statistical model of how text documents are generated.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, streaming k-means is a modification of the k-means algorithm, where
    the clusters can be adjusted with new batches of data. For each batch of data,
    we assign all points to their nearest cluster, compute new cluster centers based
    on the assignment, and then update each cluster parameters using the equations:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised learning](img/image01689.jpeg)![Unsupervised learning](img/image01690.jpeg)'
  id: totrans-609
  prefs: []
  type: TYPE_IMG
- en: Here, *c* *t* and *c'* *t* are the centers of from the old model and the ones
    computed for the new batch and *n* *t* and *n'* *t* are the number of vectors
    from the old model and for the new batch. By changing the *a* parameter, we can
    control how much information from the old runs can influence the clustering—*0*
    means the new cluster centers are totally based on the points in the new batch,
    while *1* means that we accommodate for all points that we have seen so far.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering has many modifications. For example, k-medians computes the
    cluster centers as medians of the attribute values, not mean, which works much
    better for some distributions and with *L1* target distance metric (absolute value
    of the difference) as opposed to *L2* (the sum of squares). K-medians centers
    are not necessarily present as a specific point in the dataset. K-medoids is another
    algorithm from the same family, where the resulting cluster center has to be an
    actual instance in the input set and we actually do not need to have the global
    sort, only the pairwise distances between the points. Many variations of the techniques
    exist on how to choose the original seed cluster centers and converge on the optimal
    number of clusters (besides the simple log trick I have shown).
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: Another big class of clustering algorithms is hierarchical clustering. Hierarchical
    clustering is either done from the top—akin to the decision tree algorithms—or
    from the bottom; we first find the closest neighbors, pair them, and continue
    the pairing process up the hierarchy until all records are merged. The advantage
    of hierarchical clustering is that it can be made deterministic and relatively
    fast, even though the cost of one iteration in k-means is probably going to be
    better. However, as mentioned, the unsupervised problem can actually be converted
    to a density-estimation supervised problem, with all the supervised learning techniques
    available. So have fun understanding the data!
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: Problem dimensionality
  id: totrans-613
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The larger the attribute space or the number of dimensions, the harder it is
    to usually predict the label for a given combination of attribute values. This
    is mostly due to the fact that the total number of possible distinct combinations
    of attributes increases exponentially with the dimensionality of the attribute
    space—at least in the case of discrete variables (in case of continuous variables,
    the situation is more complex and depends on the metrics used), and it is becoming
    harder to generalize.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: The effective dimensionality of the problem might be different from the dimensionality
    of the input space. For example, if the label depends only on the linear combination
    of the (continuous) input attributes, the problem is called linearly separable
    and its internal dimensionality is one—we still have to find the coefficients
    for this linear combination like in logistic regression though.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: This idea is also sometimes referred to as a **Vapnik–Chervonenkis** (**VC**)
    dimension of a problem, model, or algorithm—the expressive power of the model
    depending on how complex the dependencies that it can solve, or shatter, might
    be. More complex problems require algorithms with higher VC dimensions and larger
    training sets. However, using an algorithm with higher VC dimension on a simple
    problem can lead to overfitting and worse generalization to new data.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: 'If the units of input attributes are comparable, say all of them are meters
    or units of time, PCA, or more generally, kernel methods, can be used to reduce
    the dimensionality of the input space:'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-618
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Here, we reduced the original four-dimensional problem to two-dimensional. Like
    averaging, computing linear combinations of input attributes and selecting only
    those that describe most of the variance helps to reduce noise.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-620
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at supervised and unsupervised learning and a few
    examples of how to run them in Spark/Scala. We considered SVM, logistic regression,
    decision tree, and k-means in the example of UCI Iris dataset. This is in no way
    a complete guide, and many other libraries either exist or are being made as we
    speak, but I would bet that you can solve 99% of the immediate data analysis problems
    just with these tools.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: This will give you a very fast shortcut on how to start being productive with
    a new dataset. There are many other ways to look at the datasets, but before we
    get into more advanced topics, let's discuss regression and classification in
    the next chapter, that is, how to predict continuous and discrete labels.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5. Regression and Classification
  id: totrans-623
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we got familiar with supervised and unsupervised learning.
    Another standard taxonomy of the machine learning methods is based on the label
    is from continuous or discrete space. Even if the discrete labels are ordered,
    there is a significant difference, particularly how the goodness of fit metrics
    is evaluated.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the origin of the word regression
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning metrics for evaluating the goodness of fit in continuous and discrete
    space
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing how to write simple code in Scala for linear and logistic regression
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about advanced concepts such as regularization, multiclass predictions,
    and heteroscedasticity
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing an example of MLlib application for regression tree analysis
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about the different ways of evaluating classification models
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What regression stands for?
  id: totrans-632
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the word classification is intuitively clear, the word regression does
    not seem to imply a predictor of a continuous label. According to the Webster
    dictionary, regression is:'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: '*"a return to a former or less developed state."*'
  id: totrans-634
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It does also mention a special definition for statistics as *a measure of the
    relation between the mean value of one variable (for example, output) and corresponding
    values of other variables (for example, time and cost)*, which is actually correct
    these days. However, historically, the regression coefficient was meant to signify
    the hereditability of certain characteristics, such as weight and size, from one
    generation to another, with the hint of planned gene selection, including humans
    ([http://www.amstat.org/publications/jse/v9n3/stanton.html](http://www.amstat.org/publications/jse/v9n3/stanton.html)).
    More specifically, in 1875, Galton, a cousin of Charles Darwin and an accomplished
    19th-century scientist in his own right, which was also widely criticized for
    the promotion of eugenics, had distributed packets of sweet pea seeds to seven
    friends. Each friend received seeds of uniform weight, but with substantial variation
    across the seven packets. Galton''s friends were supposed to harvest the next
    generation seeds and ship them back to him. Galton then proceeded to analyze the
    statistical properties of the seeds within each group, and one of the analysis
    was to plot the regression line, which always appeared to have the slope less
    than one—the specific number cited was 0.33 (Galton, F. (1894), Natural Inheritance
    (5th ed.), New York: Macmillan and Company), as opposed to either *0*, in the
    case of no correlation and no inheritance; or *1*, in the case the total replication
    of the parent''s characteristics in the descendants. We will discuss why the coefficient
    of the regression line should always be less than *1* in the presence of noise
    in the data, even if the correlation is perfect. However, beyond the discussion
    and details, the origin of the term regression is partly due to planned breeding
    of plants and humans. Of course, Galton did not have access to PCA, Scala, or
    any other computing machinery at the time, which might shed more light on the
    differences between correlation and the slope of the regression line.'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: Continuous space and metrics
  id: totrans-636
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As most of this chapter''s content will be dealing with trying to predict or
    optimize continuous variables, let''s first understand how to measure the difference
    in a continuous space. Unless a drastically new discovery is made pretty soon,
    the space we live in is a three-dimensional Euclidian space. Whether we like it
    or not, this is the world we are mostly comfortable with today. We can completely
    specify our location with three continuous numbers. The difference in locations
    is usually measured by distance, or a metric, which is a function of a two arguments
    that returns a single positive real number. Naturally, the distance, ![Continuous
    space and metrics](img/image01691.jpeg), between *X* and *Y* should always be
    equal or smaller than the sum of distances between *X* and *Z* and *Y* and *Z*:'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/image01692.jpeg)'
  id: totrans-638
  prefs: []
  type: TYPE_IMG
- en: 'For any *X*, *Y*, and *Z*, which is also called triangle inequality. The two
    other properties of a metric is symmetry:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/image01693.jpeg)'
  id: totrans-640
  prefs: []
  type: TYPE_IMG
- en: 'Non-negativity of distance:'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/image01694.jpeg)![Continuous space and
    metrics](img/image01695.jpeg)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
- en: 'Here, the metric is `0` if, and only if, *X=Y*. The ![Continuous space and
    metrics](img/image01696.jpeg) distance is the distance as we understand it in
    everyday life, the square root of the sum of the squared differences along each
    of the dimensions. A generalization of our physical distance is p-norm (*p = 2*
    for the ![Continuous space and metrics](img/image01696.jpeg) distance):'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/image01697.jpeg)'
  id: totrans-644
  prefs: []
  type: TYPE_IMG
- en: 'Here, the sum is the overall components of the *X* and *Y* vectors. If *p=1*,
    the 1-norm is the sum of absolute differences, or Manhattan distance, as if the
    only path from point *X* to point *Y* would be to move only along one of the components.
    This distance is also often referred to as ![Continuous space and metrics](img/image01698.jpeg)
    distance:'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/image01699.jpeg)'
  id: totrans-646
  prefs: []
  type: TYPE_IMG
- en: Figure 05-1\. The ![Continuous space and metrics](img/image01698.jpeg) circle
    in two-dimensional space (the set of points exactly one unit from the origin (0,
    0))
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a representation of a circle in a two-dimensional space:'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/image01700.jpeg)'
  id: totrans-649
  prefs: []
  type: TYPE_IMG
- en: Figure 05-2\. ![Continuous space and metrics](img/image01696.jpeg) circle in
    two-dimensional space (the set of points equidistant from the origin (0, 0)),
    which actually looks like a circle in our everyday understanding of distance.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: 'Another frequently used special case is ![Continuous space and metrics](img/image01701.jpeg),
    the limit when ![Continuous space and metrics](img/image01702.jpeg), which is
    the maximum deviation along any of the components, as follows:'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/image01703.jpeg)'
  id: totrans-652
  prefs: []
  type: TYPE_IMG
- en: 'The equidistant circle for the ![Continuous space and metrics](img/image01701.jpeg)
    distance is shown in *Figure 05-3*:'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/image01704.jpeg)'
  id: totrans-654
  prefs: []
  type: TYPE_IMG
- en: Figure 05-3\. ![Continuous space and metrics](img/image01701.jpeg) circle in
    two-dimensional space (the set of points equidistant from the origin (0, 0)).
    This is a square as the ![Continuous space and metrics](img/image01701.jpeg) metric
    is the maximum distance along any of the components.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: I'll consider the **Kullback-Leibler** (**KL**) distance later when I talk about
    classification, which measures the difference between two probability distributions,
    but it is an example of distance that is not symmetric and thus it is not a metric.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: The metric properties make it easier to decompose the problem. Due to the triangle
    inequality, one can potentially reduce a difficult problem of optimizing a goal
    by substituting it by a set of problems by optimizing along a number of dimensional
    components of the problem separately.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  id: totrans-658
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained in [Chapter 2](part0242.xhtml#aid-76P842 "Chapter 2. Data Pipelines
    and Modeling"), *Data Pipelines and Modeling*, most complex machine learning problems
    can be reduced to optimization as our final goal is to optimize the whole process
    where the machine is involved as an intermediary or the complete solution. The
    metric can be explicit, such as error rate, or more indirect, such as **Monthly
    Active Users** (**MAU**), but the effectiveness of an algorithm is finally judged
    by how it improves some metrics and processes in our lives. Sometimes, the goals
    may consist of multiple subgoals, or other metrics such as maintainability and
    stability might eventually be considered, but essentially, we need to either maximize
    or minimize a continuous metric in one or other way.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: 'For the rigor of the flow, let''s show how the linear regression can be formulated
    as an optimization problem. The classical linear regression needs to optimize
    the cumulative ![Linear regression](img/image01696.jpeg) error rate:'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image01705.jpeg)'
  id: totrans-661
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Linear regression](img/image01706.jpeg) is the estimate given by a
    model, which, in the case of linear regression, is as follows:'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image01707.jpeg)'
  id: totrans-663
  prefs: []
  type: TYPE_IMG
- en: '(Other potential **loss functions** have been enumerated in [Chapter 3](part0249.xhtml#aid-7DES21
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*). As
    the ![Linear regression](img/image01696.jpeg) metric is a differentiable convex
    function of *a*, *b*, the extreme value can be found by equating the derivative
    of the cumulative error rate to `0`:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image01708.jpeg)'
  id: totrans-665
  prefs: []
  type: TYPE_IMG
- en: 'Computing the derivatives is straightforward in this case and leads to the
    following equation:'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image01709.jpeg)![Linear regression](img/image01710.jpeg)'
  id: totrans-667
  prefs: []
  type: TYPE_IMG
- en: 'This can be solved to give:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image01711.jpeg)![Linear regression](img/image01712.jpeg)'
  id: totrans-669
  prefs: []
  type: TYPE_IMG
- en: 'Here, *avg()* denotes the average overall input records. Note that if *avg(x)=0*
    the preceding equation is reduced to the following:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image01713.jpeg)![Linear regression](img/image01714.jpeg)'
  id: totrans-671
  prefs: []
  type: TYPE_IMG
- en: 'So, we can quickly compute the linear regression coefficients using basic Scala
    operators (we can always make *avg(x)* to be zero by performing a ![Linear regression](img/image01715.jpeg)):'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Didn't I inform you previously that Scala is a very concise language? We just
    did linear regression with five lines of code, three of which were just data-generation
    statements.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: Although there are libraries written in Scala for performing (multivariate)
    linear regression, such as Breeze ([https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)),
    which provides a more extensive functionality, it is nice to be able to use pure
    Scala functionality to get some simple statistical results.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the problem of Mr. Galton, where he found that the regression
    line always has the slope of less than one, which implies that we should always
    regress to some predefined mean. I will generate the same points as earlier, but
    they will be distributed along the horizontal line with some predefined noise.
    Then, I will rotate the line by *45* degrees by doing a linear rotation transformation
    in the *xy*-space. Intuitively, it should be clear that if anything, *y* is strongly
    correlated with x and absent, the *y* noise should be nothing else but *x*:'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-677
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The slope is only `0.81`! Note that if one runs PCA on the `x1` and `y1` data,
    the first principal component is correctly along the diagonal.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness, I am giving a plot of (*x1, y1*) zipped here:'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/image01716.jpeg)'
  id: totrans-680
  prefs: []
  type: TYPE_IMG
- en: Figure 05-4\. The regression curve slope of a seemingly perfectly correlated
    dataset is less than one. This has to do with the metric the regression problem
    optimizes (y-distance).
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: I will leave it to the reader to find the reason why the slope is less than
    one, but it has to do with the specific question the regression problem is supposed
    to answer and the metric it optimizes.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  id: totrans-683
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression optimizes the logit loss function with respect to *w*:'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/image01717.jpeg)'
  id: totrans-685
  prefs: []
  type: TYPE_IMG
- en: Here, *y* is binary (in this case plus or minus one). While there is no closed-form
    solution for the error minimization problem like there was in the previous case
    of linear regression, logistic function is differentiable and allows iterative
    algorithms that converge very fast.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient is as follows:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/image01718.jpeg)'
  id: totrans-688
  prefs: []
  type: TYPE_IMG
- en: 'Again, we can quickly concoct a Scala program that uses the gradient to converge
    to the value, where ![Logistic regression](img/image01719.jpeg) (we use the MLlib
    `LabeledPoint` data structure only for convenience of reading the data):'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The logistic regression was reduced to only one line of Scala code! The last
    line was to normalize the weights—only the relative values are important to define
    the separating plane—to compare them to the one obtained with the MLlib in previous
    chapter.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Stochastic Gradient Descent** (**SGD**) algorithm used in the actual
    implementation is essentially the same gradient descent, but optimized in the
    following ways:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
- en: The actual gradient is computed on a subsample of records, which may lead to
    faster conversion due to less rounding noise and avoid local minima.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The step—a fixed *0.1* in our case—is a monotonically decreasing function of
    the iteration as ![Logistic regression](img/image01720.jpeg), which might also
    lead to better conversion.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It incorporates regularization; instead of minimizing just the loss function,
    you minimize the sum of the loss function, plus some penalty metric, which is
    a function of model complexity. I will discuss this in the following section.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-696
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The regularization was originally developed to cope with ill-poised problems,
    where the problem was underconstrained—allowed multiple solutions given the data—or
    the data and the solution that contained too much noise (*A.N. Tikhonov*, *A.S.
    Leonov*, *A.G. Yagola. Nonlinear Ill-Posed Problems*, *Chapman and Hall*, *London*,
    *Weinhe*). Adding additional penalty function that skews a solution if it does
    not have a desired property, such as the smoothness in curve fitting or spectral
    analysis, usually solves the problem.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the penalty function is somewhat arbitrary, but it should reflect
    a desired skew in the solution. If the penalty function is differentiable, it
    can be incorporated into the gradient descent process; ridge regression is an
    example where the penalty is the ![Regularization](img/image01696.jpeg)metric
    for the weights or the sum of squares of the coefficients.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: MLlib currently implements ![Regularization](img/image01696.jpeg), ![Regularization](img/image01698.jpeg),
    and a mixture thereof called **Elastic Net**, as was shown in [Chapter 3](part0249.xhtml#aid-7DES21
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*. The
    ![Regularization](img/image01698.jpeg) regularization effectively penalizes for
    the number of non-zero entries in the regression weights, but has been known to
    have slower convergence. **Least Absolute Shrinkage and Selection Operator** (**LASSO**)
    uses the ![Regularization](img/image01698.jpeg) regularization.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: Another way to reduce the uncertainty in underconstrained problems is to take
    the prior information that may be coming from domain experts into account. This
    can be done using Bayesian analysis and introducing additional factors into the
    posterior probability—the probabilistic rules are generally expressed as multiplication
    rather than sum. However, since the goal is often minimizing the log likelihood,
    the Bayesian correction can often be expressed as standard regularizer as well.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate regression
  id: totrans-701
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is possible to minimize multiple metrics at the same time. While Spark only
    has a few multivariate analysis tools, other more traditional well-established
    packages come with **Multivariate Analysis of Variance** (**MANOVA**), a generalization
    of **Analysis of Variance** (**ANOVA**) method. I will cover ANOVA and MANOVA
    in [Chapter 7](part0283.xhtml#aid-8DSF61 "Chapter 7. Working with Graph Algorithms"),
    *Working with Graph Algorithms*.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: For a practical analysis, we first need to understand if the target variables
    are correlated, for which we can use the PCA Spark implementation covered in [Chapter
    3](part0249.xhtml#aid-7DES21 "Chapter 3. Working with Spark and MLlib"), *Working
    with Spark and MLlib*. If the dependent variables are strongly correlated, maximizing
    one leads to maximizing the other, and we can just maximize the first principal
    component (and potentially build a regression model on the second component to
    understand what drives the difference).
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: If the targets are uncorrelated, building a separate model for each of them
    can pinpoint the important variables that drive either and whether these two sets
    are disjoint. In the latter case, we could build two separate models to predict
    each of the targets independently.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: Heteroscedasticity
  id: totrans-705
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the fundamental assumptions in regression approach is that the target
    variance is not correlated with either independent (attributes) or dependent (target)
    variables. An example where this assumption might break is counting data, which
    is generally described by **Poisson distribution**. For Poisson distribution,
    the variance is proportional to the expected value, and the higher values can
    contribute more to the final variance of the weights.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: 'While heteroscedasticity may or may not significantly skew the resulting weights,
    one practical way to compensate for heteroscedasticity is to perform a log transformation,
    which will compensate for it in the case of Poisson distribution:'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: '![Heteroscedasticity](img/image01721.jpeg)![Heteroscedasticity](img/image01722.jpeg)'
  id: totrans-708
  prefs: []
  type: TYPE_IMG
- en: 'Some other (parametrized) transformations are the **Box-Cox transformation**:'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: '![Heteroscedasticity](img/image01723.jpeg)'
  id: totrans-710
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Heteroscedasticity](img/image01724.jpeg) is a parameter (the log transformation
    is a partial case, where ![Heteroscedasticity](img/image01725.jpeg)) and Tuckey''s
    lambda transformation (for attributes between *0* and *1*):'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: '![Heteroscedasticity](img/image01726.jpeg)'
  id: totrans-712
  prefs: []
  type: TYPE_IMG
- en: These compensate for Poisson binomial distributed attributes or the estimates
    of the probability of success in a sequence of trails with potentially a mix of
    *n* Bernoulli distributions.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: Heteroscedasticity is one of the main reasons that logistic function minimization
    works better than linear regression with ![Heteroscedasticity](img/image01696.jpeg)
    minimization in a binary prediction problem. Let's consider discrete labels in
    more details.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: Regression trees
  id: totrans-715
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen classification trees in the previous chapter. One can build a
    recursive split-and-concur structure for a regression problem, where a split is
    chosen to minimize the remaining variance. Regression trees are less popular than
    decision trees or classical ANOVA analysis; however, let''s provide an example
    of a regression tree here as a part of MLlib:'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-717
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The splits at each level are made to minimize the variance, as follows:'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression trees](img/image01727.jpeg)'
  id: totrans-719
  prefs: []
  type: TYPE_IMG
- en: which is equivalent to minimizing the ![Regression trees](img/image01696.jpeg)
    distances between the label values and their mean within each leaf summed over
    all the leaves of the node.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
- en: Classification metrics
  id: totrans-721
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the label is discrete, the prediction problem is called classification. In
    general, the target can take only one of the values for each record (even though
    multivalued targets are possible, particularly for text classification problems
    to be considered in [Chapter 6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working
    with Unstructured Data"), *Working with Unstructured Data*).
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: If the discrete values are ordered and the ordering makes sense, such as *Bad*,
    *Worse*, *Good*, the discrete labels can be cast into integer or double, and the
    problem is reduced to regression (we believe if you are between *Bad* and *Worse*,
    you are definitely farther away from being *Good* than *Worse*).
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: 'A generic metric to optimize is the misclassification rate is as follows:'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification metrics](img/image01728.jpeg)'
  id: totrans-725
  prefs: []
  type: TYPE_IMG
- en: However, if the algorithm can predict the distribution of possible values for
    the target, a more general metric such as the KL divergence or Manhattan can be
    used.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
- en: 'KL divergence is a measure of information loss when probability distribution
    ![Classification metrics](img/image01729.jpeg) is used to approximate probability
    distribution ![Classification metrics](img/image01730.jpeg):'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification metrics](img/image01731.jpeg)'
  id: totrans-728
  prefs: []
  type: TYPE_IMG
- en: It is closely related to entropy gain split criteria used in the decision tree
    induction, as the latter is the sum of KL divergences of the node probability
    distribution to the leaf probability distribution over all leaf nodes.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass problems
  id: totrans-730
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the number of possible outcomes for target is larger than two, in general,
    we have to predict either the expected probability distribution of the target
    values or at least the list of ordered values—hopefully augmented by a rank variable,
    which can be used for additional analysis.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
- en: While some algorithms, such as decision trees, can natively predict multivalued
    attributes. A common technique is to reduce the prediction of one of the *K* target
    values to *(K-1)* binary classification problems by choosing one of the values
    as the base and building *(K-1)* binary classifiers. It is usually a good idea
    to select the most populated level as the base.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  id: totrans-733
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the early days of machine learning, researchers were trying to imitate the
    functionality of the human brain. At the beginning of the 20th century, people
    thought that the human brain consisted entirely of cells that are called neurons—cells
    with long appendages called axons that were able to transmit signals by means
    of electric impulses. The AI researchers were trying to replicate the functionality
    of neurons by a perceptron, which is a function that is firing, based on a linearly-weighted
    sum of its input values:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptron](img/image01732.jpeg)'
  id: totrans-735
  prefs: []
  type: TYPE_IMG
- en: This is a very simplistic representation of the processes in the human brain—biologists
    have since then discovered other ways in which information is transferred besides
    electric impulses such as chemical ones. Moreover, they have found over 300 different
    types of cells that may be classified as neurons ([http://neurolex.org/wiki/Category:Neuron](http://neurolex.org/wiki/Category:Neuron)).
    Also, the process of neuron firing is more complex than just linear transmission
    of voltages as it involves complex time patterns as well. Nevertheless, the concept
    turned out to be very productive, and multiple algorithms and techniques were
    developed for neural nets, or the sets of perceptions connected to each other
    in layers. Specifically, it can be shown that the neural network, with certain
    modification, where the step function is replaced by a logistic function in the
    firing equation, can approximate an arbitrary differentiable function with any
    desired precision.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
- en: 'MLlib implements **Multilayer Perceptron Classifier** (**MLCP**) as an `org.apache.spark.ml.classification.MultilayerPerceptronClassifier`
    class:'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-738
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Generalization error and overfitting
  id: totrans-739
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, how do we know that the model we have discussed is good? One obvious and
    ultimate criterion is its performance in practice.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
- en: One common problem that plagues the more complex models, such as decision trees
    and neural nets, is overfitting. The model can minimize the desired metric on
    the provided data, but does a very poor job on a slightly different dataset in
    practical deployments, Even a standard technique, when we split the dataset into
    training and test, the training for deriving the model and test for validating
    that the model works well on a hold-out data, may not capture all the changes
    that are in the deployments. For example, linear models such as ANOVA, logistic,
    and linear regression are usually relatively stable and less of a subject to overfitting.
    However, you might find that any particular technique either works or doesn't
    work for your specific domain.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
- en: Another case when generalization may fail is time-drift. The data may change
    over time significantly so that the model trained on the old data no longer generalizes
    on the new data in a deployment. In practice, it is always a good idea to have
    several models in production and constantly monitor their relative performance.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: I will consider standard ways to avoid overfitting such as hold out datasets
    and cross-validation in [Chapter 7](part0283.xhtml#aid-8DSF61 "Chapter 7. Working
    with Graph Algorithms"), *Working with Graph Algorithms* and model monitoring
    in [Chapter 9](part0291.xhtml#aid-8LGJM2 "Chapter 9. NLP in Scala"), *NLP in Scala*.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-744
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have all the necessary tools to look at more complex problems that are
    more commonly called the big data problems. Armed with standard statistical algorithms—I
    understand that I have not covered many details and I am completely ready to accept
    the criticism—there is an entirely new ground to explore where we do not have
    clearly defined records, the variables in the datasets may be sparse and nested,
    and we have to cover a lot of ground and do a lot of preparatory work just to
    get to the stage where we can apply the standard statistical models. This is where
    Scala shines best.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look more at working with unstructured data.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6. Working with Unstructured Data
  id: totrans-747
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I am very excited to introduce you to this chapter. Unstructured data is what,
    in reality, makes big data different from the old data, it also makes Scala to
    be the new paradigm for processing the data. To start with, unstructured data
    at first sight seems a lot like a derogatory term. Notwithstanding, every sentence
    in this book is unstructured data: it does not have the traditional record / row
    / column semantics. For most people, however, this is the easiest thing to read
    rather than the book being presented as a table or spreadsheet.'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the unstructured data means nested and complex data. An XML document
    or a photograph are good examples of unstructured data, which have very rich structure
    to them. My guess is that the originators of the term meant that the new data,
    the data that engineers at social interaction companies such as Google, Facebook,
    and Twitter saw, had a different structure to it as opposed to a traditional flat
    table that everyone used to see. These indeed did not fit the traditional RDBMS
    paradigm. Some of them can be flattened, but the underlying storage would be too
    inefficient as the RDBMSs were not optimized to handle them and also be hard to
    parse not only for humans, but for the machines as well.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: A lot of techniques introduced in this chapter were created as an emergency
    Band-Aid to deal with the need to just process the data.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: Learning about the serialization, popular serialization frameworks, and language
    in which the machines talk to each other
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about Avro-Parquet encoding for nested data
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how RDBMs try to incorporate nested structures in modern SQL-like languages
    to work with them
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how you can start working with nested structures in Scala
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing a practical example of sessionization—one of the most frequent use cases
    for unstructured data
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing how Scala traits and match/case statements can simplify path analysis
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning where the nested structures can benefit your analysis
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nested data
  id: totrans-759
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You already saw unstructured data in the previous chapters, the data was an
    array of **LabeledPoint**, which is a tuple **(label: Double, features: Vector)**.
    The label is just a number of type **Double**. **Vector** is a sealed trait with
    two subclasses: **SparseVector** and **DenseVector**. The class diagram is as
    follows:'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
- en: '![Nested data](img/image01733.jpeg)'
  id: totrans-761
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The LabeledPoint class structure is a tuple of label and features,
    where features is a trait with two inherited subclasses {Dense,Sparse}Vector.
    DenseVector is an array of double, while SparseVector stores only size and non-default
    elements by index and value.'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: 'Each observation is a tuple of label and features, and features can be sparse.
    Definitely, if there are no missing values, the whole row can be represented as
    vector. A dense vector representation requires (*8 x size + 8*) bytes. If most
    of the elements are missing—or equal to some default value—we can store only the
    non-default elements. In this case, we would require (*12 x non_missing_size +
    20*) bytes, with small variations depending on the JVM implementation. So, the
    threshold for switching between one or another, from the storage point of view,
    is when the size is greater than *1.5 x* ( *non_missing_size + 1* ), or if roughly
    at least 30% of elements are non-default. While the computer languages are good
    at representing the complex structures via pointers, we need some convenient form
    to exchange these data between JVMs or machines. First, let''s see first how Spark/Scala
    does it, specifically persisting the data in the Parquet format:'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: What we did was create a new RDD dataset from command line, or we could use
    `org.apache.spark.mllib.util.MLUtils` to load a text file, converted it to a DataFrames
    and create a serialized representation of it in the Parquet file under the `points`
    directory.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-766
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**What Parquet stands for?**'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: Apache Parquet is a columnar storage format, jointly developed by Cloudera and
    Twitter for big data. Columnar storage allows for better compression of values
    in the datasets and is more efficient if only a subset of columns need to be retrieved
    from the disk. Parquet was built from the ground up with complex nested data structures
    in mind and uses the record shredding and assembly algorithm described in the
    Dremel paper ([https://blog.twitter.com/2013/dremel-made-simple-with-parquet](https://blog.twitter.com/2013/dremel-made-simple-with-parquet)).
    Dremel/Parquet encoding uses definition/repetition fields to denote the level
    in the hierarchy the data is coming from, which covers most of the immediate encoding
    needs, as it is sufficient to store optional fields, nested arrays, and maps.
    Parquet stores the data by chunks, thus probably the name Parquet, which means
    flooring composed of wooden blocks arranged in a geometric pattern. Parquet can
    be optimized for reading only a subset of blocks from disk, depending on the subset
    of columns to be read and the index used (although it very much depends on whether
    the specific implementation is aware of these features). The values in the columns
    can use dictionary and **Run-Length Encoding** (**RLE**), which provides exceptionally
    good compression for columns with many duplicate entries, a frequent use case
    in big data.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: 'Parquet file is a binary format, but you might look at the information in it
    using `parquet-tools`, which are downloadable from [http://archive.cloudera.com/cdh5/cdh/5](http://archive.cloudera.com/cdh5/cdh/5):'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-770
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s look at the schema, which is very close to the structure depicted in
    *Figure 1*: first member is the label of type double and the second and last one
    is features of composite type. The keyword optional is another way of saying that
    the value can be null (absent) in the record for one or another reason. The lists
    or arrays are encoded as a repeated field. As the whole array may be absent (it
    is possible for all features to be absent), it is wrapped into optional groups
    (indices and values). Finally, the type encodes whether it is a sparse or dense
    representation:'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-772
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'You are probably a bit confused about the `R`: and `D`: in the output. These
    are the repetition and definition levels as described in the Dremel paper and
    they are necessary to efficiently encode the values in the nested structures.
    Only repeated fields increment the repetition level and only non-required fields
    increment the definition level. Drop in `R` signifies the end of the list(array).
    For every non-required level in the hierarchy tree, one needs a new definition
    level. Repetition and definition level values are small by design and can be efficiently
    stored in a serialized form.'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
- en: What is best, if there are many duplicate entries, they will all be placed together.
    The case for which the compression algorithm (by default, it is gzip) are optimized.
    Parquet also implements other algorithms exploiting repeated values such as dictionary
    encoding or RLE compression.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: This is a simple and efficient serialization out of the box. We have been able
    to write a set of complex objects to a file, each column stored in a separate
    block, representing all values in the records and nested structures.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now read the file and recover RDD. The Parquet format does not know
    anything about the `LabeledPoint` class, so we''ll have to do some typecasting
    and trickery here. When we read the file, we''ll see a collection of `org.apache.spark.sql.Row`:'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-777
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Personally, I think that this is pretty cool: without any compilation, we can
    encode and decide complex objects. One can easily create their own objects in
    REPL. Let''s consider that we want to track user''s behavior on the web:'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-779
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'As a matter of good practice, we need to register the newly created classes
    with the `Kryo` `serializer`—Spark will use another serialization mechanism to
    pass the objects between tasks and executors. If the class is not registered,
    Spark will use default Java serialization, which might be up to *10 x* slower:'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: If you are deploying the code on a cluster, the recommendation is to put this
    code in a jar on the classpath.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
- en: I've certainly seen examples of up to 10 level deep nesting in production. Although
    this might be an overkill for performance reasons, nesting is required in more
    and more production business use cases. Before we go into the specifics of constructing
    a nested object in the example of sessionization, let's get an overview of serialization
    in general.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
- en: Other serialization formats
  id: totrans-784
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I do recommend the Parquet format for storing the data. However, for completeness,
    I need to at least mention other serialization formats, some of them like Kryo
    will be used implicitly for you during Spark computations without your knowledge
    and there is obviously a default Java serialization.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-786
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Object-oriented approach versus functional approach**'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
- en: Objects in object-oriented approach are characterized by state and behavior.
    Objects are the cornerstone of object-oriented programming. A class is a template
    for objects with fields that represent the state, and methods that may represent
    the behavior. Abstract method implementation may depend on the instance of the
    class. In functional approach, the state is usually frowned upon; in pure programming
    languages, there should be no state, no side effects, and every invocation should
    return the same result. The behaviors may be expressed though additional function
    parameters and higher order functions (functions over functions, such as currying),
    but should be explicit unlike the abstract methods. Since Scala is a mix of object-oriented
    and functional language, some of the preceding constraints are violated, but this
    does not mean that you have to use them unless absolutely necessary. It is best
    practice to store the code in jar packages while storing the data, particularly
    for the big data, separate from code in data files (in a serialized form); but
    again, people often store data/configurations in jar files, and it is less common,
    but possible to store code in the data files.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
- en: The serialization has been an issue since the need to persist data on disk or
    transfer object from one JVM or machine to another over network appeared. Really,
    the purpose of serialization is to make complex nested objects be represented
    as a series of bytes, understandable by machines, and as you can imagine, this
    might be language-dependent. Luckily, serialization frameworks converge on a set
    of common data structures they can handle.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most popular serialization mechanisms, but not the most efficient,
    is to dump an object in an ASCII file: CSV, XML, JSON, YAML, and so on. They do
    work for more complex nested data like structures, arrays, and maps, but are inefficient
    from the storage space perspective. For example, a Double represents a continuous
    number with 15-17 significant digits that will, without rounding or trivial ratios,
    take 15-17 bytes to represent in US ASCII, while the binary representation takes
    only 8 bytes. Integers may be stored even more efficiently, particularly if they
    are small, as we can compress/remove zeroes.'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of text encoding is that they are much easier to visualize with
    simple command-line tools, but any advanced serialization framework now comes
    with a set of tools to work with raw records such as `avro` *-* or `parquet-tools`.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table provides an overview for most common serialization frameworks:'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
- en: '| Serialization Format | When developed | Comments |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
- en: '| XML, JSON, YAML | This was a direct response to the necessity to encode nested
    structures and exchange the data between machines. | While grossly inefficient,
    these are still used in many places, particularly in web services. The only advantage
    is that they are relatively easy to parse without machines. |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
- en: '| Protobuf | Developed by Google in the early 2000s. This implements the Dremel
    encoding scheme and supports multiple languages (Scala is not officially supported
    yet, even though some code exists). | The main advantage is that Protobuf can
    generate native classes in many languages. C++, Java, and Python are officially
    supported. There are ongoing projects in C, C#, Haskell, Perl, Ruby, Scala, and
    more. Run-time can call native code to inspect/serialize/deserialize the objects
    and binary representations. |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
- en: '| Avro | Avro was developed by Doug Cutting while he was working at Cloudera.
    The main objective was to separate the encoding from a specific implementation
    and language, allowing better schema evolution. | While the arguments whether
    Protobuf or Avro are more efficient are still ongoing, Avro supports a larger
    number of complex structures, say unions and maps out of the box, compared to
    Protobuf. Scala support is still to be strengthened to the production level. Avro
    files have schema encoded with every file, which has its pros and cons. |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
- en: '| Thrift | The Apache Thrift was developed at Facebook for the same purpose
    Protobuf was developed. It probably has the widest selection of supported languages:
    C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js,
    Smalltalk, OCaml, Delphi, and other languages. Again, Twitter is hard at work
    for making the Thrift code generation in Scala ([https://twitter.github.io/scrooge/](https://twitter.github.io/scrooge/)).
    | Apache Thrift is often described as a framework for cross-language services
    development and is most frequently used as **Remote Procedure Call** (**RPC**).
    Even though it can be used directly for serialization/deserialization, other frameworks
    just happen to be more popular. |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
- en: '| Parquet | Parquet was developed in a joint effort between Twitter and Cloudera.
    Compared to the Avro format, which is row-oriented, Parquet is columnar storage
    that results in better compression and performance if only a few columns are to
    be selected. The interval encoding is Dremel or Protobuf-based, even though the
    records are presented as Avro records; thus, it is often called **AvroParquet**.
    | Advances features such as indices, dictionary encoding, and RLE compression
    potentially make it very efficient for pure disk storage. Writing the files may
    be slower as Parquet requires some preprocessing and index building before it
    can be committed to the disk. |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
- en: '| Kryo | This is a framework for encoding arbitrary classes in Java. However,
    not all built-in Java collection classes can be serialized. | If one avoids non-serializable
    exceptions, such as priority queues, Kryo can be very efficient. Direct support
    in Scala is also under way. |'
  id: totrans-800
  prefs: []
  type: TYPE_TB
- en: Certainly, Java has a built-in serialization framework, but as it has to support
    all Java cases, and therefore is overly general, the Java serialization is far
    less efficient than any of the preceding methods. I have certainly seen other
    companies implement their own proprietary serialization earlier, which would beat
    any of the preceding serialization for the specific cases. Nowadays, it is no
    longer necessary, as the maintenance costs definitely overshadow the converging
    inefficiency of the existing frameworks.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: Hive and Impala
  id: totrans-802
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the design considerations for a new framework is always the compatibility
    with the old frameworks. For better or worse, most data analysts still work with
    SQL. The roots of the SQL go to an influential relational modeling paper (*Codd,
    Edgar F* (June 1970). *A Relational Model of Data for Large Shared Data Banks*.
    *Communications of the ACM (Association for Computing Machinery) 13 (6): 377–87*).
    All modern databases implement one or another version of SQL.'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: 'While the relational model was influential and important for bringing the database
    performance, particularly for **Online Transaction Processing** (**OLTP**) to
    the competitive levels, the significance of normalization for analytic workloads,
    where one needs to perform aggregations, and for situations where relations themselves
    change and are subject to analysis, is less critical. This section will cover
    the extensions of standard SQL language for analysis engines traditionally used
    for big data analytics: Hive and Impala. Both of them are currently Apache licensed
    projects. The following table summarizes the complex types:'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Hive support since version | Impala support since version | Comments
    |'
  id: totrans-805
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
- en: '| `ARRAY` | This is supported since 0.1.0, but the use of non-constant index
    expressions is allowed only as of 0.14. | This is supported since 2.3.0 (only
    for Parquet tables). | This can be an array of any type, including complex. The
    index is `int` in Hive (`bigint` in Impala) and access is via array notation,
    for example, `element[1]` only in Hive (`array.pos` and `item pseudocolumns` in
    Impala). |'
  id: totrans-807
  prefs: []
  type: TYPE_TB
- en: '| `MAP` | This is supported since 0.1.0, but the use of non-constant index
    expressions is allowed only as of 0.14. | This is supported since 2.3.0 (only
    for Parquet tables). | The key should be of primitive type. Some libraries support
    keys of the string type only. Fields are accessed using array notation, for example,
    `map["key"]` only in Hive (map key and value pseudocolumns in Impala). |'
  id: totrans-808
  prefs: []
  type: TYPE_TB
- en: '| `STRUCT` | This is supported since 0.5.0. | This is supported since 2.3.0
    (only for Parquet tables). | Access is using dot notation, for example, `struct.element`.
    |'
  id: totrans-809
  prefs: []
  type: TYPE_TB
- en: '| `UNIONTYPE` | This is supported since 0.7.0. | This is not supported in Impala.
    | Support is incomplete: queries that reference `UNIONTYPE` fields in `JOIN` (HIVE-2508),
    `WHERE`, and `GROUP BY` clauses will fail, and Hive does not define the syntax
    to extract the tag or value fields of `UNIONTYPE`. This means that `UNIONTYPEs`
    are effectively look-at-only. |'
  id: totrans-810
  prefs: []
  type: TYPE_TB
- en: 'While Hive/Impala tables can be created on top of many underlying file formats
    (Text, Sequence, ORC, Avro, Parquet, and even custom format) and multiple serializations,
    in most practical instances, Hive is used to read lines of text in ASCII files.
    The underlying serialization/deserialization format is `LazySimpleSerDe` (**Serialization**/**Deserialization**
    (**SerDe**)). The format defines several levels of separators, as follows:'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-812
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The default for separators are `''\001''` or `^A`, `''\002''` or `^B`, and
    `''\003''` or `^B`. In other words, it''s using the new separator at each level
    of the hierarchy as opposed to the definition/repetition indicator in the Dremel
    encoding. For example, to encode the `LabeledPoint` table that we used before,
    we need to create a file, as follows:'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Download Hive from [http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz)
    and perform the follow:'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-816
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: In Spark, select from a relational table is supported via the `sqlContext.sql`
    method, but unfortunately the Hive union types are not directly supported as of
    Spark 1.6.1; it does support maps and arrays though. The supportability of complex
    objects in other BI and data analysis tools still remains the biggest obstacle
    to their adoption. Supporting everything as a rich data structure in Scala is
    one of the options to converge on nested data representation.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: Sessionization
  id: totrans-818
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I will demonstrate the use of the complex or nested structures in the example
    of sessionization. In sessionization, we want to find the behavior of an entity,
    identified by some ID over a period of time. While the original records may come
    in any order, we want to summarize the behavior over time to derive trends.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
- en: We already analyzed web server logs in [Chapter 1](part0235.xhtml#aid-703K61
    "Chapter 1. Exploratory Data Analysis"), *Exploratory Data Analysis*. We found
    out how often different web pages are accessed over a period of time. We could
    dice and slice this information, but without analyzing the sequence of pages visited,
    it would be hard to understand each individual user interaction with the website.
    In this chapter, I would like to give this analysis more individual flavor by
    tracking the user navigation throughout the website. Sessionization is a common
    tool for website personalization and advertising, IoT tracking, telemetry, and
    enterprise security, in fact anything to do with entity behavior.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume the data comes as tuples of three elements (fields `1`, `5`,
    `11` in the original dataset in [Chapter 1](part0235.xhtml#aid-703K61 "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*):'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-822
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here, `id` is a unique entity ID, timestamp is an event `timestamp` (in any
    sortable format: Unix timestamp or an ISO8601 date format), and `path` is some
    indication of the location on the web server page hierarchy.'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: 'For people familiar with SQL, sessionization, or at least a subset of it, is
    better known as a windowing analytics function:'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Here `ANALYTIC_FUNCTION` is some transformation on the sequence of paths for
    a given `id`. While this approach works for a relatively simple function, such
    as first, last, lag, average, expressing a complex function over a sequence of
    paths is usually very convoluted (for example, nPath from Aster Data ([https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf](https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf))).
    Besides, without additional preprocessing and partitioning, these approaches usually
    result in big data transfers across multiple nodes in a distributed setting.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
- en: While in a pure functional approach, one would just have to design a function—or
    a sequence of function applications—to produce the desired answers from the original
    set of tuples, I will create two helper objects that will help us to simplify
    working with the concept of a user session. As an additional benefit, the new
    nested structures can be persisted on a disk to speed up getting answers on additional
    questions.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how it''s done in Spark/Scala using case classes:'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-829
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The first class will represent a single page view with a timestamp, which, in
    this case, is an ISO8601 `String`, while the second a sequence of page views.
    Could we do it by encoding both members as a `String` with a object separator?
    Absolutely, but representing the fields as members of a class gives us nice access
    semantics, together with offloading some of the work that we need to perform on
    the compiler, which is always nice.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s read the previously described log files and construct the objects:'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Bingo! We have an RDD of Sessions, one per each unique IP address. The IP `189.248.74.238`
    has a session that lasted from `23:09:16` to `23:15:10`, and seemingly ended after
    browsing for men''s shoes. The session for IP `82.166.130.148` contains only one
    hit. The last session concentrated on sports watch and lasted for over three minutes
    from `2015-08-23 22:36:10` to `2015-08-23 22:39:26`. Now, we can easily ask questions
    involving specific navigation path patterns. For example, we want analyze all
    the sessions that resulted in checkout (the path contains `checkout`) and see
    the number of hits and the distribution of times after the last hit on homepage:'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-834
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The sessions last from 1 to 121 hits with a mode at 8 hits and from 15 to 2653
    seconds (or about 45 minutes). Why would you be interested in this information?
    Long sessions might indicate that there was a problem somewhere in the middle
    of the session: a long delay or non-responsive call. It does not have to be: the
    person might just have taken a long lunch break or a call to discuss his potential
    purchase, but there might be something of interest here. At least one should agree
    that this is an outlier and needs to be carefully analyzed.'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
- en: Let's talk about persisting this data to the disk. As you've seen, our transformation
    is written as a long pipeline, so there is nothing in the result that one could
    not compute from the raw data. This is a functional approach, the data is immutable.
    Moreover, if there is an error in our processing, let's say I want to change the
    homepage to some other anchor page, I can always modify the function as opposed
    to data. You may be content or not with this fact, but there is absolutely no
    additional piece of information in the result—transformations only increase the
    disorder and entropy. They might make it more palatable for humans, but this is
    only because humans are a very inefficient data-processing apparatus.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Why rearranging the data makes the analysis faster?**'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: Sessionization seems just a simple rearranging of data—we just put the pages
    that were accessed in sequence together. Yet, in many cases, it makes practical
    data analysis run 10 to 100 times faster. The reason is data locality. The analysis,
    like filtering or path matching, most often tends to happen on the pages in one
    session at a time. Deriving user features requires all page views or interactions
    of the user to be in one place on disk and memory. This often beats other inefficiencies
    such as the overhead of encoding/decoding the nested structures as this can happen
    in local L1/L2 cache as opposed to data transfers from RAM or disk, which are
    much more expensive in modern multithreaded CPUs. This very much depends on the
    complexity of the analysis, of course.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
- en: There is a reason to persist the new data to the disk, and we can do it with
    either CSV, Avro, or Parquet format. The reason is that we do not want to reprocess
    the data if we want to look at them again. The new representation might be more
    compact and more efficient to retrieve and show to my manager. Really, humans
    like side effects and, fortunately, Scala/Spark allows you to do this as was described
    in the previous section.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, well, well...will say the people familiar with sessionization. This is
    only a part of the story. We want to split the path sequence into multiple sessions,
    run path analysis, compute conditional probabilities for page transitions, and
    so on. This is exactly where the functional paradigm shines. Write the following
    function:'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-842
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Then run the following code:'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-844
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Bingo! The result is the session's split. I intentionally left the implementation
    out; it's the implementation that is user-dependent, not the data, and every analyst
    might have it's own way to split the sequence of page visits into sessions.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: 'Another use case to apply the function is feature generation for applying machine
    learning…well, this is already hinting at the side effect: we want to modify the
    state of the world to make it more personalized and user-friendly. I guess one
    cannot avoid it after all.'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
- en: Working with traits
  id: totrans-847
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw, case classes significantly simplify handling of new nested data structures
    that we want to construct. The case class definition is probably the most convincing
    reason to move from Java (and SQL) to Scala. Now, what about the methods? How
    do we quickly add methods to a class without expensive recompilation? Scala allows
    you to do this transparently with traits!
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental feature of functional programming is that functions are a first
    class citizen on par with objects. In the previous section, we defined the two
    `EpochSeconds` functions that transform the ISO8601 format to epoch time in seconds.
    We also suggested the `splitSession` function that provides a multi-session view
    for a given IP. How do we associate this or other behavior with a given class?
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define a desired behavior:'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-851
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'This basically creates a `PageView`-specific function that converts a string
    representation for datetime to epoch time in seconds. Now, if we just make the
    following transformation:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-853
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We now have a new RDD of page views with additional behavior. For example,
    if we want to find out what is the time spent on each individual page in a session
    is, we will run a pipeline, as follows:'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-855
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Multiple traits can be added at the same time without affecting either the original
    class definitions or original data. No recompilation is required.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: Working with pattern matching
  id: totrans-857
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'No Scala book would be complete without mentioning the match/case statements.
    Scala has a very rich pattern-matching mechanism. For instance, let''s say we
    want to find all instances of a sequence of page views that start with a homepage
    followed by a products page—we really want to filter out the determined buyers.
    This may be accomplished with a new function, as follows:'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-859
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Note that we explicitly put `PageView` constructors in the case statement!
    Scala will traverse the `visits` sequence and generate new sessions that match
    the specified two `PageViews`, as follows:'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-861
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: I leave it to the reader to write a function that also filters only those sessions
    where the user spent less than 10 seconds before going to the products page. The
    epoch trait or the previously defined to the `EpochSeconds` function may be useful.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
- en: The match/case function can be also used for feature generation and return a
    vector of features over a session.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
- en: Other uses of unstructured data
  id: totrans-864
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The personalization and device diagnostic obviously are not the only uses of
    unstructured data. The preceding case is a good example as we started from structured
    record and quickly converged on the need to construct an unstructured data structure
    to simplify the analysis.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
- en: In fact, there are many more unstructured data than there are structured; it
    is just the convenience of having the flat structure for the traditional statistical
    analysis that makes us to present the data as a set of records. Text, images,
    and music are the examples of semi-structured data.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
- en: One example of non-structured data is denormalized data. Traditionally the record
    data are normalized mostly for performance reasons as the RDBMSs have been optimized
    to work with structured data. This leads to foreign key and lookup tables, but
    these are very hard to maintain if the dimensions change. Denormalized data does
    not have this problem as the lookup table can be stored with each record—it is
    just an additional table object associated with a row, but may be less storage-efficient.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic structures
  id: totrans-868
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another use case is the probabilistic structures. Usually people assume that
    answering a question is deterministic. As I showed in [Chapter 2](part0242.xhtml#aid-76P842
    "Chapter 2. Data Pipelines and Modeling"), *Data Pipelines and Modeling*, in many
    cases, the true answer has some uncertainty associated with it. One of the most
    popular ways to encode uncertainty is probability, which is a frequentist approach,
    meaning that the simple count of when the answer does happen to be the true answer,
    divided by the total number of attempts—the probability also can encode our beliefs.
    I will touch on probabilistic analysis and models in the following chapters, but
    probabilistic analysis requires storing each possible outcome with some measure
    of probability, which happens to be a nested structure.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
- en: Projections
  id: totrans-870
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One way to deal with high dimensionality is projections on a lower dimensional
    space. The fundamental basis for why projections might work is Johnson-Lindenstrauss
    lemma. The lemma states that a small set of points in a high-dimensional space
    can be embedded into a space of much lower dimension in such a way that distances
    between the points are nearly preserved. We will touch on random and other projections
    when we talk about NLP in [Chapter 9](part0291.xhtml#aid-8LGJM2 "Chapter 9. NLP
    in Scala"), *NLP in Scala*, but the random projections work well for nested structures
    and functional programming language, as in many cases, generating a random projection
    is the question of applying a function to a compactly encoded data rather than
    flattening the data explicitly. In other words, the Scala definition for a random
    projection may look like functional paradigm shines. Write the following function:'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-872
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Here, `Vector` is in low dimensional space.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
- en: The map used for embedding is at least Lipschitz, and can even be taken to be
    an orthogonal projection.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-875
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw examples of how to represent and work with complex and
    nested data in Scala. Obviously, it would be hard to cover all the cases as the
    world of unstructured data is much larger than the nice niche of structured row-by-row
    simplification of the real world and is still under construction. Pictures, music,
    and spoken and written language have a lot of nuances that are hard to capture
    in a flat representation.
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
- en: While for ultimate data analysis, we eventually convert the datasets to the
    record-oriented flat representation, at least at the time of collection, one needs
    to be careful to store that data as it is and not throw away useful information
    that might be contained in data or metadata. Extending the databases and storage
    with a way to record this useful information is the first step. The next one is
    to use languages that can effectively analyze this information; which is definitely
    Scala.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we'll look at somewhat related topic of working with graphs,
    a specific example of non-structured data.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7. Working with Graph Algorithms
  id: totrans-879
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I'll delve into graph libraries and algorithm implementations
    in Scala. In particular, I will introduce Graph for Scala ([http://www.scala-graph.org](http://www.scala-graph.org)),
    an open source project that was started in 2011 in the EPFL Scala incubator. Graph
    for Scala does not support distributed computing yet—the distributed computing
    aspects of popular graph algorithms is available in GraphX, which is a part of
    MLlib library that is part of Spark project ([http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)).
    Both, Spark and MLlib were started as class projects at UC Berkeley around or
    after 2009\. I considered Spark in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib* and introduced an RDD.
    In GraphX, a graph is a pair of RDDs, each of which is partitioned among executors
    and tasks, represents vertices and edges in a graph.
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
- en: Configuring **Simple Build Tool** (**SBT**) to use the material in this chapter
    interactively
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning basic operations on graphs supported by Graph for Scala
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to enforce graph constraints
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to import/export graphs in JSON
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing connected components, triangle count, and strongly connected components
    running on Enron e-mail data
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing PageRank computations on Enron e-mail data
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to use SVD++
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick introduction to graphs
  id: totrans-889
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is a graph? A graph is a set of **vertices** where some pairs of these
    vertices are linked with **edges**. If every vertex is linked with every other
    vertex, we say the graph is a complete graph. On the contrary, if it has no edges,
    the graph is said to be empty. These are, of course, extremes that are rarely
    encountered in practice, as graphs have varying degrees of density; the more edges
    it has proportional to the number of vertices, the more dense we say it is.
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: Depending on what algorithms we intend to run on a graph and how dense is it
    expected to be, we can choose how to appropriately represent the graph in memory.
    If the graph is really dense, it pays off to store it as a square *N x N* matrix,
    where *0* in the *n*th row and *m*th column means that the *n* vertex is not connected
    to the *m* vertex. A diagonal entry expresses a node connection to itself. This
    representation is called the adjacency matrix.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
- en: If there are not many edges and we need to traverse the whole edge set without
    distinction, often it pays off to store it as a simple container of pairs. This
    structure is called an **edge list**.
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we can model many real-life situations and events as graphs. We
    could imagine cities as vertices and plane routes as edges. If there is no flight
    between two cities, there is no edge between them. Moreover, if we add the numerical
    costs of plane tickets to the edges, we say that the graph is **weighted**. If
    there are some edges where only travels in one direction exist, we can represent
    that by making a graph directed as opposed to an undirected graph. So, for an
    undirected graph, it is true that the graph is symmetric, that is, if *A* is connected
    to *B*, then *B* is also connected to *A*—that is not necessarily true for a directed
    graph.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
- en: Graphs without cycles are called acyclic. Multigraph can contain multiple edges,
    potentially of different type, between the nodes. Hyperedges can connect arbitrary
    number of nodes.
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
- en: The most popular algorithm on the undirected graphs is probably **connected
    components**, or partitioning of a graph into subgraph, in which any two vertices
    are connected to each other by paths. Partitioning is important to parallelize
    the operations on the graphs.
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
- en: Google and other search engines made PageRank popular. According to Google,
    PageRank estimates of how important the website is by counting the number and
    quality of links to a page. The underlying assumption is that more important websites
    are likely to receive more links from other websites, especially more highly ranked
    ones. PageRank can be applied to many problems outside of websites ranking and
    is equivalent to finding eigenvectors and the most significant eigenvalue of the
    connectivity matrix.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
- en: The most basic, nontrivial subgraph, consists of three nodes. Triangle counting
    finds all the possible fully connected (or complete) triples of nodes and is another
    well-known algorithm used in community detection and CAD.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
- en: 'A **clique** is a fully connected subgraph. A strongly connected component
    is an analogous notion for a directed graph: every vertex in a subgraph is reachable
    from every other vertex. GraphX provides an implementation for both.'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a recommender graph is a graph connecting two types of nodes: users
    and items. The edges can additionally contain the strength of a recommendation
    or a measure of satisfaction. The goal of a recommender is to predict the satisfaction
    for potentially missing edges. Multiple algorithms have been developed for a recommendation
    engine, such as SVD and SVD++, which are considered at the end of this chapter.'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
- en: SBT
  id: totrans-900
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everyone likes Scala REPL. REPL is the command line for Scala. It allows you
    to type Scala expressions that are evaluated immediately and try and explore things.
    As you saw in the previous chapters, one can simply type `scala` at the command
    prompt and start developing complex data pipelines. What is even more convenient
    is that one can press *tab* to have auto-completion, a required feature of any
    fully developed modern IDE (such as Eclipse or IntelliJ, *Ctrl +*. or *Ctrl +
    Space*) by keeping track of the namespace and using reflection mechanisms. Why
    would we need one extra tool or framework for builds, particularly that other
    builds management frameworks such as Ant, Maven, and Gradle exist in addition
    to IDEs? As the SBT authors argue, even though one might compile Scala using the
    preceding tools, all of them have inefficiencies, as it comes to interactivity
    and reproducibility of Scala builds (*SBT in Action* by *Joshua Suereth* and *Matthew
    Farwell*, Nov 2015).
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
- en: One of the main SBT features for me is interactivity and the ability to seamlessly
    work with multiple versions of Scala and dependent libraries. In the end, what
    is critical for software development is the speed with which one can prototype
    and test new ideas. I used to work on mainframes using punch cards, where the
    programmers were waiting to execute their programs and ideas, sometimes for hours
    and days. The efficiency of the computers mattered more, as this was the bottleneck.
    These days are gone, and a personal laptop is probably having more computing power
    than rooms full of servers a few decades back. To take advantage of this efficiency,
    we need to utilize human time more efficiently by speeding up the program development
    cycle, which also means interactivity and more versions in the repositories.
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the ability to handle multiple versions and REPL, SBT''s main features
    are as follows:'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
- en: Native support for compiling Scala code and integrating with many test frameworks,
    including JUnit, ScalaTest, and Selenium
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build descriptions written in Scala using a DSL
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependency management using Ivy (which also supports Maven-format repositories)
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous execution, compilation, testing, and deployment
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with the Scala interpreter for rapid iteration and debugging
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for mixed Java/Scala projects
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for testing and deployment frameworks
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to complement the tool with custom plugins
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel execution of tasks
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SBT is written in Scala and uses SBT to build itself (bootstrapping or dogfooding).
    SBT became the de facto build tool for the Scala community, and is used by the
    **Lift** and **Play** frameworks.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
- en: 'While you can download SBT directly from [http://www.scala-sbt.org/download](http://www.scala-sbt.org/download),
    the easiest way to install SBT on Mac is to run MacPorts:'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-915
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'You can also run Homebrew:'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'While other tools exist to create SBT projects, the most straightforward way
    is to run the `bin/create_project.sh` script in the GitHub book project repository
    provided for each chapter:'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'This will create main and test source subdirectories (but not the code). The
    project directory contains project-wide settings (refer to `project/build.properties`).
    The target will contain compiled classes and build packages (the directory will
    contain different subdirectories for different versions of Scala, for example,
    2.10 and 2.11). Finally, any jars or libraries put into the `lib` directory will
    be available across the project (I personally recommend using the `libraryDependencies`
    mechanism in the `build.sbt` file, but not all libraries are available via centralized
    repositories). This is the minimal setup, and the directory structure may potentially
    contain multiple subprojects. The Scalastyle plugin will even check the syntax
    for you ([http://www.scalastyle.org/sbt.html](http://www.scalastyle.org/sbt.html)).
    Just add `project/plugin.sbt`:'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-921
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Finally, the SBT creates Scaladoc documentation with the `sdbt doc` command.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-923
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Blank lines and other settings in build.sbt**'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
- en: 'Probably most of the `build.sbt` files out there are double spaced: this is
    a remnant of old versions. You no longer need them. As of version 0.13.7, the
    definitions do not require extra lines.'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
- en: There are many other settings that you can use on `build.sbt` or `build.properties`,
    the up-to-date documentation is available at [http://www.scala-sbt.org/documentation.html](http://www.scala-sbt.org/documentation.html).
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
- en: When run from the command line, the tool will automatically download and use
    the dependencies, in this case, `graph-{core,constrained,json}` and `lift-json`.
    In order to run the project, simply type `sbt run`.
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
- en: In continuous mode, SBT will automatically detect changes to the source file
    and rerun the command(s). In order to continuously compile and run the code, type
    `~~ run` after starting REPL with `sbt`.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
- en: 'To get help on the commands, run the following command:'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-930
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: While SBT will be sufficient for our use even with a simple editor such as **vi**
    or **Emacs**, the `sbteclipse` project at [https://github.com/typesafehub/sbteclipse](https://github.com/typesafehub/sbteclipse)
    will create the necessary project files to work with your Eclipse IDE.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
- en: Graph for Scala
  id: totrans-932
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this project, I will create a `src/main/scala/InfluenceDiagram.scala` file.
    For demo purpose, I will just recreate the graph from [Chapter 2](part0242.xhtml#aid-76P842
    "Chapter 2. Data Pipelines and Modeling"), *Data Pipelines and Modeling*:'
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-934
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The `~+>` operator is used to create a directed labeled edge between two nodes
    defined in `scalax/collection/edge/Implicits.scala`, which, in our case, are of
    the `String` type. The list of other edge types and operators is provided in the
    following table:'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
- en: '*The following table shows* graph edges from `scalax.collection.edge.Implicits`
    (from [http://www.scala-graph.org/guides/core-initializing.html](http://www.scala-graph.org/guides/core-initializing.html))'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
- en: '| Edge Class | Shortcut/Operator | Description |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-938
  prefs: []
  type: TYPE_TB
- en: '| **Hyperedges** |'
  id: totrans-939
  prefs: []
  type: TYPE_TB
- en: '| `HyperEdge` | `~` | hyperedge |'
  id: totrans-940
  prefs: []
  type: TYPE_TB
- en: '| `WHyperEdge` | `~%` | weighted hyperedge |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
- en: '| `WkHyperEdge` | `~%#` | key-weighted hyperedge |'
  id: totrans-942
  prefs: []
  type: TYPE_TB
- en: '| `LHyperEdge` | `~+` | labeled hyperedge |'
  id: totrans-943
  prefs: []
  type: TYPE_TB
- en: '| `LkHyperEdge` | `~+#` | key-labeled hyperedge |'
  id: totrans-944
  prefs: []
  type: TYPE_TB
- en: '| `WLHyperEdge` | `~%+` | weighted labeled hyperedge |'
  id: totrans-945
  prefs: []
  type: TYPE_TB
- en: '| `WkLHyperEdge` | `~%#+` | key-weighted labeled hyperedge |'
  id: totrans-946
  prefs: []
  type: TYPE_TB
- en: '| `WLkHyperEdge` | `~%+#` | weighted key-labeled hyperedge |'
  id: totrans-947
  prefs: []
  type: TYPE_TB
- en: '| `WkLkHyperEdge` | `~%#+#` | key-weighted key-labeled hyperedge |'
  id: totrans-948
  prefs: []
  type: TYPE_TB
- en: '| **Directed hyperedges** |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
- en: '| `DiHyperEdge` | `~>` | directed hyperedge |'
  id: totrans-950
  prefs: []
  type: TYPE_TB
- en: '| `WDiHyperEdge` | `~%>` | weighted directed hyperedge |'
  id: totrans-951
  prefs: []
  type: TYPE_TB
- en: '| `WkDiHyperEdge` | `~%#>` | key-weighted directed hyperedge |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
- en: '| `LDiHyperEdge` | `~+>` | labeled directed hyperedge |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
- en: '| `LkDiHyperEdge` | `~+#>` | key-labeled directed hyperedge |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
- en: '| `WLDiHyperEdge` | `~%+>` | weighted labeled directed hyperedge |'
  id: totrans-955
  prefs: []
  type: TYPE_TB
- en: '| `WkLDiHyperEdge` | `~%#+>` | key-weighted labeled directed hyperedge |'
  id: totrans-956
  prefs: []
  type: TYPE_TB
- en: '| `WLkDiHyperEdge` | `~%+#>` | weighted key-labeled directed hyperedge |'
  id: totrans-957
  prefs: []
  type: TYPE_TB
- en: '| `WkLkDiHyperEdge` | `~%#+#>` | key-weighted key-labeled directed hyperedge
    |'
  id: totrans-958
  prefs: []
  type: TYPE_TB
- en: '| **Undirected edges** |'
  id: totrans-959
  prefs: []
  type: TYPE_TB
- en: '| `UnDiEdge` | `~` | undirected edge |'
  id: totrans-960
  prefs: []
  type: TYPE_TB
- en: '| `WUnDiEdge` | `~%` | weighted undirected edge |'
  id: totrans-961
  prefs: []
  type: TYPE_TB
- en: '| `WkUnDiEdge` | `~%#` | key-weighted undirected edge |'
  id: totrans-962
  prefs: []
  type: TYPE_TB
- en: '| `LUnDiEdge` | `~+` | labeled undirected edge |'
  id: totrans-963
  prefs: []
  type: TYPE_TB
- en: '| `LkUnDiEdge` | `~+#` | key-labeled undirected edge |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
- en: '| `WLUnDiEdge` | `~%+` | weighted labeled undirected edge |'
  id: totrans-965
  prefs: []
  type: TYPE_TB
- en: '| `WkLUnDiEdge` | `~%#+` | key-weighted labeled undirected edge |'
  id: totrans-966
  prefs: []
  type: TYPE_TB
- en: '| `WLkUnDiEdge` | `~%+#` | weighted key-labeled undirected edge |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
- en: '| `WkLkUnDiEdge` | `~%#+#` | key-weighted key-labeled undirected edge |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
- en: '| **Directed edges** |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
- en: '| `DiEdge` | `~>` | directed edge |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
- en: '| `WDiEdge` | `~%>` | weighted directed edge |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
- en: '| `WkDiEdge` | `~%#>` | key-weighted directed edge |'
  id: totrans-972
  prefs: []
  type: TYPE_TB
- en: '| `LDiEdge` | `~+>` | labeled directed edge |'
  id: totrans-973
  prefs: []
  type: TYPE_TB
- en: '| `LkDiEdge` | `~+#>` | key-labeled directed edge |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
- en: '| `WLDiEdge` | `~%+>` | weighted labeled directed edge |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
- en: '| `WkLDiEdge` | `~%#+>` | key-weighted labeled directed edge |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
- en: '| `WLkDiEdge` | `~%+#>` | weighted key-labeled directed edge |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
- en: '| `WkLkDiEdge` | `~%#+#>` | key-weighted key-labeled directed edge |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
- en: 'You saw the power of graph for Scala: the edges can be weighted and we may
    potentially construct a multigraph (key-labeled edges allow multiple edges for
    a pair of source and destination nodes).'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run SBT on the preceding project with the Scala file in the `src/main/scala`
    directory, the output will be as follows:'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: If continuous compilation is enabled, the main method will be run as soon as
    SBT detects that the file has changed (in the case of multiple classes having
    the main method, SBT will ask you which one to run, which is not great for interactivity;
    so you might want to limit the number of executable classes).
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
- en: I will cover different output formats in a short while, but let's first see
    how to perform simple operations on the graph.
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
- en: Adding nodes and edges
  id: totrans-984
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we already know that the graph is directed and acyclic, which is a required
    property for all decision diagrams so that we know we did not make a mistake.
    Let''s say that I want to make the graph more complex and add a node that will
    indicate the likelihood of me recommending a vacation in Portland, Oregon to another
    person. The only thing I need to add is the following line:'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-986
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'If you have continuous compilation/run enabled, you will immediately see the
    changes after pressing the **Save File** button:'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-988
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now, if we want to know the parents of the newly introduced node, we can simply
    run the following code:'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-990
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'This will give us a set of parents for a specific node—and thus drive the decision
    making process. If we add a cycle, the acyclic method will automatically detect
    it:'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-992
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Note that you can create the graphs completely programmatically:'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-994
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Here, the element computation provided as the second parameter to the fill method
    is repeated `45` times (the first parameter). The graph connects every node to
    all of its predecessors, which is also known as a clique in the graph theory.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
- en: Graph constraints
  id: totrans-996
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Graph for Scala enables us to set constraints that cannot be violated by any
    future graph update. This comes in handy when we want to preserve some detail
    in the graph structure. For example, a **Directed Acyclic Graph** (**DAG**) should
    not contain cycles. Two constraints are currently implemented as a part of the
    `scalax.collection.constrained.constraints` package—connected and acyclic, as
    follows:'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-998
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Here is the command to run the program that tries to add or remove nodes that
    violate the constraints:'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-1000
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Adding or subtracting nodes that violate one of the constraints is rejected.
    The programmer can also specify a side effect if an attempt to add or subtract
    a node that violates the condition is made.
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  id: totrans-1002
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Graph for Scala supports importing/exporting graphs to JSON, as follows:'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-1004
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'To produce a JSON representation for a sample graph, run:'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-1006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: For more complex structures, one might need to write custom descriptors, serializers,
    and deserializers (refer to [http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package](http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package)).
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
- en: GraphX
  id: totrans-1008
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While graph for Scala may be considered a DSL for graph operations and querying,
    one should go to GraphX for scalability. GraphX is build on top of a powerful
    Spark framework. As an example of Spark/GraphX operations, I''ll use the CMU Enron
    e-mail dataset (about 2 GB). The actual semantic analysis of the e-mail content
    is not going to be important to us until the next chapters. The dataset can be
    downloaded from the CMU site. It has e-mail from mailboxes of 150 users, primarily
    Enron managers, and about 517,401 e-mails between them. The e-mails may be considered
    as an indication of a relation (edge) between two people: Each email is an edge
    between a source (`From:`) and a destination (`To:`) vertices.'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
- en: 'Since GraphX requires the data in RDD format, I''ll have to do some preprocessing.
    Luckily, it is extremely easy with Scala—this is why Scala is the perfect language
    for semi-structured data. Here is the code:'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-1011
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: First, we use the `MurmurHash3` class to generate node IDs, which are of type
    `Long`, as they are required for each node in GraphX. The `emailRe` and `messageRe`
    are used to match the file content to find the required content. Scala allows
    you to parallelize the programs without much work.
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
- en: Note the `par` call on line 50, `getFileTree(new File(args(0))).par.map`. This
    will make the loop parallel. If processing the whole Enron dataset can take up
    to an hour even on 3 GHz processor, adding parallelization reduces it by about
    8 minutes on a 32-core Intel Xeon E5-2630 2.4 GHz CPU Linux machine (it took 15
    minutes on an Apple MacBook Pro with 2.3 GHz Intel Core i7).
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the code will produce a set of JSON records that can be loaded into
    Spark (to run it, you''ll need to put **joda-time** and **lift-json** library
    jars on the classpath), as follows:'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-1015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Nice! Spark was able to figure out the fields and types on it''s own. If Spark
    was not able to parse all the records, one would have a `_corrupt_record` field
    containing the unparsed records (one of them is the `[success]` line at the end
    of the dataset, which can be filtered out with a `grep -Fv [success]`). You can
    see them with the following command:'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-1017
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The nodes (people) and edges (relations) datasets can be extracted with the
    following commands:'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-1019
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Note
  id: totrans-1020
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Node IDs in GraphX**'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in Graph for Scala, specifying the edges is sufficient for defining
    the nodes and the graph. In Spark/GraphX, nodes need to be extracted explicitly,
    and each node needs to be associated with *n* id of the `Long` type. While this
    potentially limits the flexibility and the number of unique nodes, it enhances
    the efficiency. In this particular example, generating node ID as a hash of the
    e-mail string was sufficient as no collisions were detected, but the generation
    of unique IDs is usually a hard problem to parallelize.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
- en: 'The first GraphX graph is ready!! It took a bit more work than Scala for Graph,
    but now it''s totally ready for distributed processing. A few things to note:
    first, we needed to explicitly convert the fields to `Long` and `String` as the
    `Edge` constructor needed help in figuring out the types. Second, Spark might
    need to optimize the number of partitions (likely, it created too many):'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-1024
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'To repartition, there are two calls: repartition and coalesce. The latter tries
    to avoid shuffle, as follows:'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  id: totrans-1026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'However, this might limit parallelism if one performs computations over a large
    cluster. Finally, it''s a good idea to use `cache` method that pins the data structure
    in memory:'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  id: totrans-1028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'It took a few more commands to construct a graph in Spark, but four is not
    too bad. Let''s compute some statistics (and show the power of Spark/GraphX, in
    the following table:'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
- en: Computing basic statistics on Enron e-mail graph.
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
- en: '| Statistics | Spark command | Value for Enron |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
- en: '| Total # of relations (pairwise communications) | `graph.numEdges` | 3,035,021
    |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
- en: '| Number of e-mails (message IDs) | `graph.edges.map( e => e.attr._1 ).distinct.count`
    | 371,135 |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
- en: '| Number of connected pairs | `graph.edges.flatMap( e => List((e.srcId, e.dstId),
    (e.dstId, e.srcId))).distinct.count / 2` | 217,867 |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
- en: '| Number of one-way communications | `graph.edges.flatMap( e => List((e.srcId,
    e.dstId), (e.dstId, e.srcId))).distinct.count - graph.edges.map( e => (e.srcId,
    e.dstId)).distinct.count` | 193,183 |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
- en: '| Number of distinct subject lines | `graph.edges.map( e => e.attr._2 ).distinct.count`
    | 110,273 |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
- en: '| Total # of nodes | `graph.numVertices` | 23,607 |'
  id: totrans-1038
  prefs: []
  type: TYPE_TB
- en: '| Number of destination-only nodes | `graph. numVertices - graph.edges.map(
    e => e.srcId).distinct.count` | 17,264 |'
  id: totrans-1039
  prefs: []
  type: TYPE_TB
- en: '| Number of source-only nodes | `graph. numVertices - graph.edges.map( e =>
    e.dstId).distinct.count` | 611 |'
  id: totrans-1040
  prefs: []
  type: TYPE_TB
- en: Who is getting e-mails?
  id: totrans-1041
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most straightforward ways to estimate people''s importance in an
    organization is to look at the number of connections or the number of incoming
    and outgoing communicates. The GraphX graph has built-in `inDegrees` and `outDegrees`
    methods. To rank the emails with respect to the number of incoming emails, run:'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  id: totrans-1043
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'To rank the emails with respect to the number of egressing emails, run:'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  id: totrans-1045
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Let's apply some more complex algorithms to the Enron dataset.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
- en: Connected components
  id: totrans-1047
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Connected components determine whether the graph is naturally partitioned into
    several parts. In the Enron relationship graph, this would mean that two or several
    groups communicate mostly between each other:'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  id: totrans-1049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We see 18 groups. Each one of the groups can be counted and extracted by filtering
    the ID. For instance, the group associated with `<[etc.survey@enron.com](mailto:etc.survey@enron.com)>`
    can be found by running a SQL query on DataFrame:'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  id: totrans-1051
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: This group is based on a single e-mail sent on September 19, 2000, from `<[survey.test@enron.com](mailto:survey.test@enron.com)>`
    to `<[etc.survey@enron](mailto:etc.survey@enron)>`. The e-mail is listed twice,
    only because it ended up in two different folders (and has two distinct message
    IDs). Only the first group, the largest subgraph, contains more than two e-mail
    addresses in the organization.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
- en: Triangle counting
  id: totrans-1053
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The triangle counting algorithm is relatively straightforward and can be computed
    in the following three steps:'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
- en: Compute the set of neighbors for each vertex.
  id: totrans-1055
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each edge, compute the intersection of the sets and send the count to both
    vertices.
  id: totrans-1056
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the sum at each vertex and divide by two, as each triangle is counted
    twice.
  id: totrans-1057
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to convert the multigraph to an undirected graph with `srcId < dstId`,
    which is a precondition for the algorithm:'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  id: totrans-1059
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: While there is no direct relationship between the triangle count and the importance
    of people in the organization, the people with higher triangle count arguably
    are more social—even though a clique or a strongly connected component count might
    be a better measure.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
- en: Strongly connected components
  id: totrans-1061
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the mathematical theory of directed graphs, a subgraph is said to be strongly
    connected if every vertex is reachable from every other vertex. It could happen
    that the whole graph is just one strongly connected component, but on the other
    end of the spectrum, each vertex could be its own connected component.
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
- en: If you contract each connected component to a single vertex, you get a new directed
    graph that has a property to be without cycles—acyclic.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm for SCC detection is already built into GraphX:'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  id: totrans-1065
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: There are 18,200 strongly connected components with only an average 23,787/18,200
    = 1.3 users per group.
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
- en: PageRank
  id: totrans-1067
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The PageRank algorithm gives us an estimate of how important a person by analysing
    the links, which are the emails in this case. For example, let''s run PageRank
    on Enron email graph:'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  id: totrans-1069
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Ostensibly, these are the go-to people. PageRank tends to emphasize the incoming
    edges, and Tana Jones returns to the top of the list compared to the 9th place
    in the triangle counting.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
- en: SVD++
  id: totrans-1071
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SVD++ is a recommendation engine algorithm, developed specifically for Netflix
    competition by Yahuda Koren and team in 2008—the original paper is still out there
    in the public domain and can be Googled as `kdd08koren.pdf`. The specific implementation
    comes from the .NET *MyMediaLite* library by ZenoGarther ([https://github.com/zenogantner/MyMediaLite](https://github.com/zenogantner/MyMediaLite)),
    who granted Apache 2 license to the Apache Foundation. Let''s assume I have a
    set of users (on the left) and items (on the right):'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/image01734.jpeg)'
  id: totrans-1073
  prefs: []
  type: TYPE_IMG
- en: Figure 07-1\. A graphical representation of a recommendation problem as a bipartite
    graph.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram is a graphical representation of the recommendation problem.
    The nodes on the left represent users. The nodes on the right represent items.
    User **1** recommends items **A** and **C**, while users **2** and **3** recommend
    only a single item **A**. The rest of the edges are missing. The common question
    is to find recommendation ranking of the rest of the items, the edges may also
    have a weight or recommendation strength attached to them. The graph is usually
    sparse. Such graph is also often called bipartite, as the edges only go from one
    set of nodes to another set of nodes (the user does not recommend another user).
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
- en: 'For the recommendation engine, we typically need two types of nodes—users and
    items. The recommendations are based on the rating matrix of (user, item, and
    rating) tuples. One of the implementation of the recommendation algorithm is based
    on **Singular Value Decomposition** (**SVD**) of the preceding matrix. The final
    scoring has four components: the baseline, which is the sum of average for the
    whole matrix, average for the users, and average for the items, as follows:'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/image01735.jpeg)'
  id: totrans-1077
  prefs: []
  type: TYPE_IMG
- en: 'Here, the ![SVD++](img/image01736.jpeg), ![SVD++](img/image01737.jpeg), and
    ![SVD++](img/image01738.jpeg) can be understood as the averages for the whole
    population, user (among all user recommendations), and item (among all the users).
    The final part is the Cartesian product of two rows:'
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/image01739.jpeg)'
  id: totrans-1079
  prefs: []
  type: TYPE_IMG
- en: 'The problem is posed as a minimization problem (refer to [Chapter 4](part0256.xhtml#aid-7K4G02
    "Chapter 4. Supervised and Unsupervised Learning"), *Supervised and Unsupervised
    Learning*):'
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/image01740.jpeg)'
  id: totrans-1081
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![SVD++](img/image01741.jpeg) is a regularization coefficient also discussed
    in [Chapter 4](part0256.xhtml#aid-7K4G02 "Chapter 4. Supervised and Unsupervised
    Learning"), *Supervised and Unsupervised Learning*. So, each user is associated
    with a set of numbers (![SVD++](img/image01742.jpeg), and each item with ![SVD++](img/image01743.jpeg),
    ![SVD++](img/image01744.jpeg). In this particlar implementation, the optimal coefficients
    are found by gradient descent. This is the basic of SVD optimization. In linear
    algebra, SVD takes an arbitrary ![SVD++](img/image01745.jpeg) matrix *A* and represents
    it as a product of an orthogonal ![SVD++](img/image01745.jpeg) matrix *U*, a diagonal
    ![SVD++](img/image01745.jpeg) matrix ![SVD++](img/image01746.jpeg), and a ![SVD++](img/image01745.jpeg)
    unitary matrix *V*, for example, the columns are mutually orthogonal. Arguably,
    if one takes the largest ![SVD++](img/image01747.jpeg) entries of the ![SVD++](img/image01746.jpeg)
    matrix, the product is reduced to the product of a very tall ![SVD++](img/image01748.jpeg)
    matrix and a wide ![SVD++](img/image01749.jpeg) matric, where ![SVD++](img/image01747.jpeg)
    is called the rank of decomposition. If the remaining values are small, the new
    ![SVD++](img/image01750.jpeg) numbers approximate the original ![SVD++](img/image01745.jpeg)
    numbers for the relation, *A*. If *m* and *n* are large to start with, and in
    practical online shopping situations, *m* is the items and can be in hundreds
    of thousands, and *n* is the users and can be hundreds of millions, the saving
    can be substantial. For example, for *r=10*, *m=100,000*, and *n=100,000,000*,
    the savings are as follows:'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
- en: '![SVD++](img/image01751.jpeg)'
  id: totrans-1083
  prefs: []
  type: TYPE_IMG
- en: 'SVD can also be viewed as PCA for matrices with ![SVD++](img/image01752.jpeg).
    In the Enron case, we can treat senders as users and recipients as items (we''ll
    need to reassign the node IDs), as follows:'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  id: totrans-1085
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'The `svdRanks` is the user-part of the ![SVD++](img/image01753.jpeg) prediction.
    The distribution lists take a priority as this is usually used for mass e-mailing.
    To get the user-specific part, we need to provide the user ID:'
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  id: totrans-1087
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Here, we computed the top five recommended e-mail-to list for top in-degree
    and out-degree users.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
- en: SVD has only 159 lines of code in Scala and can be the basis for some further
    improvements. SVD++ includes a part based on implicit user feedback and item similarity
    information. Finally, the Netflix winning solution had also taken into consideration
    the fact that user preferences are time-dependent, but this part has not been
    implemented in GraphX yet.
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1090
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While one can easily create their own data structures for graph problems, Scala's
    support for graphs comes from both semantic layer—Graph for Scala is effectively
    a convenient, interactive, and expressive language for working with graphs—and
    scalability via Spark and distributed computing. I hope that some of the material
    exposed in this chapter will be useful for implementing algorithms on top of Scala,
    Spark, and GraphX. It is worth mentioning that bot libraries are still under active
    development.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll step down from from our flight in the the skies and
    look at Scala integration with traditional data analysis frameworks such as statistical
    language R and Python, which are often used for data munching. Later, in [Chapter
    9](part0291.xhtml#aid-8LGJM2 "Chapter 9. NLP in Scala"), *NLP in Scala*. I'll
    look at NLP Scala tools, which leverage complex data structures extensively.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8. Integrating Scala with R and Python
  id: totrans-1093
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Spark provides MLlib as a library for machine learning, in many practical
    situations, R or Python present a more familiar and time-tested interface for
    statistical computations. In particular, R's extensive statistical library includes
    very popular ANOVA/MANOVA methods of analyzing variance and variable dependencies/independencies,
    sets of statistical tests, and random number generators that are not currently
    present in MLlib. The interface from R to Spark is available under SparkR project.
    Finally, data analysts know Python's NumPy and SciPy linear algebra implementations
    for their efficiency as well as other time-series, optimization, and signal processing
    packages. With R/Python integration, all these familiar functionalities can be
    exposed to Scala/Spark users until the Spark/MLlib interfaces stabilize and the
    libraries make their way into the new framework while benefiting the users with
    Spark's ability to execute workflows in a distributed way across multiple machines.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
- en: When people program in R or Python, or with any statistical or linear algebra
    packages for this matter, they are usually not specifically focusing on the functional
    programming aspects. As I mentioned in [Chapter 1](part0235.xhtml#aid-703K61 "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*, Scala should be treated as a high-level
    language and this is where it shines. Integration with highly efficient C and
    Fortran implementations, for example, of the freely available **Basic Linear Algebra
    Subprograms** (**BLAS**), **Linear Algebra** **Package** (**LAPACK**), and **Arnoldi
    Package** (**ARPACK**), is known to find its way into Java and thus Scala ([http://www.netlib.org](http://www.netlib.org),
    [https://github.com/fommil/netlib-java](https://github.com/fommil/netlib-java)).
    I would like to leave Scala at what it's doing best. In this chapter, however,
    I will focus on how to use these languages with Scala/Spark.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
- en: I will use the publicly available United States Department of Transportation
    flights dataset for this chapter ([http://www.transtats.bts.gov](http://www.transtats.bts.gov)).
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
- en: Installing R and configuring SparkR if you haven't done so yet
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about R (and Spark) DataFrames
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing linear regression and ANOVA analysis with R
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing **Generalized Linear Model** (**GLM**) modeling with SparkR
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Python if you haven't done so yet
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to use PySpark and call Python from Scala
  id: totrans-1103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating with R
  id: totrans-1104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with many advanced and carefully designed technologies, people usually either
    love or hate R as a language. One of the reason being that R was one of the first
    language implementations that tries to manipulate complex objects, even though
    most of them turn out to be just a list as opposed to struct or map as in more
    mature modern implementations. R was originally created at the University of Auckland
    by Ross Ihaka and Robert Gentleman around 1993, and had its roots in the S language
    developed at Bell Labs around 1976, when most of the commercial programming was
    still done in Fortran. While R incorporates some functional features such as passing
    functions as a parameter and map/apply, it conspicuously misses some others such
    as lazy evaluation and list comprehensions. With all this said, R has a very good
    help system, and if someone says that they never had to go back to the `help(…)`
    command to figure out how to run a certain data transformation or model better,
    they are either lying or just starting in R.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
- en: Setting up R and SparkR
  id: totrans-1106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run SparkR, you'll need R version 3.0 or later. Follow the given instructions
    for the installation, depending on you operating system.
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
- en: Linux
  id: totrans-1108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On a Linux system, detailed installation documentation is available at [https://cran.r-project.org/bin/linux](https://cran.r-project.org/bin/linux).
    However, for example, on a Debian system, one installs it by running the following
    command:'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  id: totrans-1110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'To list installed/available packages on the Linux repository site, perform
    the following command:'
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  id: totrans-1112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'R packages, which are a part of `r-base` and `r-recommended`, are installed
    into the `/usr/lib/R/library` directory. These can be updated using the usual
    package maintenance tools such as `apt-get` or aptitude. The other R packages
    available as precompiled Debian packages, `r-cran-*` and `r-bioc-*`, are installed
    into `/usr/lib/R/site-library`. The following command shows all packages that
    depend on `r-base-core`:'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  id: totrans-1114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'This comprises of a large number of contributed packages from CRAN and other
    repositories. If you want to install R packages that are not provided as package,
    or if you want to use newer versions, you need to build them from source that
    requires the `r-base-dev` development package that can be installed by the following
    command:'
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  id: totrans-1116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'This pulls in the basic requirements to compile R packages, such as the development
    tools group install. R packages may then be installed by the local user/admin
    from the CRAN source packages, typically from inside R using the `R> install.packages()`
    function or `R CMD INSTALL`. For example, to install the R `ggplot2` package,
    run the following command:'
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  id: totrans-1118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'This will download and optionally compile the package and its dependencies
    from one of the available sites. Sometime R is confused about the repositories;
    in this case, I recommend creating a `~/.Rprofile` file in the home directory
    pointing to the closest CRAN repository:'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  id: totrans-1120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '`~/.Rprofile` contains commands to customize your sessions. One of the commands
    I recommend to put in there is `options (prompt="R> ")` to be able to distinguish
    the shell you are working in by the prompt, following the tradition of most tools
    in this book. The list of known mirrors is available at [https://cran.r-project.org/mirrors.html](https://cran.r-project.org/mirrors.html).'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, it is good practice to specify the directory to install `system/site/user`
    packages via the following command, unless your OS setup does it already by putting
    these commands into `~/.bashrc` or system `/etc/profile`:'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  id: totrans-1123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Mac OS
  id: totrans-1124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'R for Mac OS can be downloaded, for example, from [http://cran.r-project.org/bin/macosx](http://cran.r-project.org/bin/macosx).
    The latest version at the time of the writing is 3.2.3\. Always check the consistency
    of the downloaded package. To do so, run the following command:'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  id: totrans-1126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: The environment settings in the preceding subsection also apply to the Mac OS
    setup.
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
- en: Windows
  id: totrans-1128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: R for Windows can be downloaded from [https://cran.r-project.org/bin/windows/](https://cran.r-project.org/bin/windows/)
    as an exe installer. Run this executable as an administrator to install R.
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
- en: One can usually edit the environment setting for **System/User** by following
    the **Control Panel** | **System and Security** | **System** | **Advanced system
    settings** | **Environment Variables** path from the Windows menu.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
- en: Running SparkR via scripts
  id: totrans-1131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To run SparkR, one needs to run install the `R/install-dev.sh` script that
    comes with the Spark git tree. In fact, one only needs the shell script and the
    content of the `R/pkg` directory, which is not always included with the compiled
    Spark distributions:'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  id: totrans-1133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Running Spark via R's command line
  id: totrans-1134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alternatively, we can also initialize Spark from the R command line directly
    (or from RStudio at [http://rstudio.org/](http://rstudio.org/)) using the following
    commands:'
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  id: totrans-1136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: As described previously in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib*, the `SPARK_HOME` environment
    variable needs to point to your local Spark installation directory and `SPARK_MASTER`
    and `YARN_CONF_DIR` to the desired cluster manager (local, standalone, mesos,
    and YARN) and YARN configuration directory if one is using Spark with the YARN
    cluster manager.
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
- en: Although most all of the distributions come with a UI, in the tradition of this
    book and for the purpose of this chapter I'll use the command line.
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames
  id: totrans-1139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DataFrames originally came from R and Python, so it is natural to see them
    in SparkR.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that the implementation of DataFrames in SparkR is on top of RDDs,
    so they work differently than the R DataFrames.
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
- en: The question of when and where to store and apply the schema and other metadata
    like types has been a topic of active debate recently. On one hand, providing
    the schema early with the data enables thorough data validation and potentially
    optimization. On the other hand, it may be too restrictive for the original data
    ingest, whose goal is just to capture as much data as possible and perform data
    formatting/cleansing later on, the approach often referred as schema on read.
    The latter approach recently won more ground with the tools to work with evolving
    schemas such as Avro and automatic schema discovery tools, but for the purpose
    of this chapter, I'll assume that we have done the schema discovery part and can
    start working with a DataFrames.
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first download and extract a flight delay dataset from the United States
    Department of Transportation, as follows:'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  id: totrans-1145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'If you have Spark running on the cluster, you want to copy the file in HDFS:'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  id: totrans-1147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'The `flights/readme.html` files gives you detailed metadata information, as
    shown in the following image:'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
- en: '![DataFrames](img/image01754.jpeg)'
  id: totrans-1149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 08-1: Metadata provided with the On-Time Performance dataset released
    by the US Department of Transportation (for demo purposes only)'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, I want you to analyze the delays of `SFO` returning flights and possibly
    find the factors contributing to the delay. Let''s start with the R `data.frame`:'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  id: totrans-1152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: If you were flying from Salt Lake City on Sunday with Alaska Airlines in July
    2015, consider yourself unlucky (we have only done simple analysis so far, so
    one shouldn't attach too much significance to this result). There may be multiple
    other random factors contributing to the delay.
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though we ran the example in SparkR, we still used the R `data.frame`.
    If we want to analyze data across multiple months, we will need to distribute
    the load across multiple nodes. This is where the SparkR distributed DataFrame
    comes into play, as it can be distributed across multiple threads even on a single
    node. There is a direct way to convert the R DataFrame to SparkR DataFrame (and
    thus to RDD):'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  id: totrans-1155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'If I run it on a laptop, I will run out of memory. The overhead is large due
    to the fact that I need to transfer the data between multiple threads/nodes, we
    want to filter it as soon as possible:'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  id: totrans-1157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'This will run even on my laptop. There is, of course, a reverse conversion
    from Spark''s DataFrame to R''s `data.frame`:'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  id: totrans-1159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'Alternatively, I can use the `spark-csv` package to read it from the `.csv`
    file, which, if the original `.csv` file is in a distributed filesystem such as
    HDFS, will avoid shuffling the data over network in a cluster setting. The only
    drawback, at least currently, is that Spark cannot read from the `.zip` files
    directly:'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  id: totrans-1161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'Note that we loaded the additional `com.databricks:spark-csv_2.10:1.3.0` package
    by supplying the `--package` flag on the command line; we can easily go distributed
    by using a Spark instance over a cluster of nodes or even analyze a larger dataset:'
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  id: totrans-1163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'This will download and put the on-time performance data in the flight''s directory
    (remember, as we discussed in [Chapter 1](part0235.xhtml#aid-703K61 "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*, we would like to treat directories
    as big data datasets). We can now run the same analysis over the whole period
    of 2015 (for the available data):'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  id: totrans-1165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Note that we used a `cache()` call to pin the dataset to the memory as we will
    use it again later. This time it''s Minneapolis/United on Saturday! However, you
    probably already know why: there is only one record for this combination of `DayOfWeek`,
    `Origin`, and `UniqueCarrier`; it''s most likely an outlier. The average over
    about `30` flights for the previous outlier was reduced to `30` minutes:'
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  id: totrans-1167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Sunday still remains a problem in terms of delays. The limit to the amount of
    data we can analyze now is only the number of cores on the laptop and nodes in
    the cluster. Let's look at more complex machine learning models now.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
- en: Linear models
  id: totrans-1169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear methods play an important role in statistical modeling. As the name
    suggests, linear model assumes that the dependent variable is a weighted combination
    of independent variables. In R, the `lm` function performs a linear regression
    and reports the coefficients, as follows:'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE131]'
  id: totrans-1171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'The `summary` function provides even more information:'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  id: totrans-1173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: While we considered generalized linear models in [Chapter 3](part0249.xhtml#aid-7DES21
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*, and
    we will also consider the `glm` implementation in R and SparkR shortly, linear
    models provide more information in general and are an excellent tool for working
    with noisy data and selecting the relevant attribute for further analysis.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Data analysis life cycle**'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
- en: While most of the statistical books focus on the analysis and best use of available
    data, the results of statistical analysis in general should also affect the search
    for the new sources of information. In the complete data life cycle, discussed
    at the end of [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working with Spark
    and MLlib"), *Working with Spark and MLlib*, a data scientist should always transform
    the latest variable importance results into the theories of how to collect data.
    For example, if the ink usage analysis for home printers points to an increase
    in ink usage for photos, one could potentially collect more information about
    the format of the pictures, sources of digital images, and paper the user prefers
    to use. This approach turned out to be very productive in a real business situation
    even though not fully automated.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, here is a short description of the output that linear models
    provide:'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
- en: '**Residuals**: These are statistics for the difference between the actual and
    predicted values. A lot of techniques exist to detect the problems with the models
    on patterns of the residual distribution, but this is out of scope of this book.
    A detailed residual table can be obtained with the `resid(model)` function.'
  id: totrans-1179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coefficients**: These are the actual linear combination coefficients; the
    t-value represents the ratio of the value of the coefficient to the estimate of
    the standard error: higher values mean a higher likelihood that this coefficient
    has a non-trivial effect on the dependent variable. The coefficients can also
    be obtained with `coef(model)` functions.'
  id: totrans-1180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual standard error**: This reports the standard mean square error, the
    metric that is the target of optimization in a straightforward linear regression.'
  id: totrans-1181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple R-squared**: This is the fraction of the dependent variable variance
    that is explained by the model. The adjusted value accounts for the number of
    parameters in your model and is considered to be a better metric to avoid overfitting
    if the number of observations does not justify the complexity of the models, which
    happens even for big data problems.'
  id: totrans-1182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-statistic**: The measure of model quality. In plain terms, it measures
    how all the parameters in the model explain the dependent variable. The p-value
    provides the probability that the model explains the dependent variable just due
    to random chance. The values under 0.05 (or 5%) are, in general, considered satisfactory.
    While in general, a high value probably means that the model is probably not statistically
    valid and "nothing else matters, the low F-statistic does not always mean that
    the model will work well in practice, so it cannot be directly applied as a model
    acceptance criterion.'
  id: totrans-1183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the linear models are applied, usually more complex `glm` or recursive
    models, such as decision trees and the `rpart` function, are applied to find interesting
    variable interactions. Linear models are good for establishing baseline on the
    other models that can improve.
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, ANOVA is a standard technique to study the variance if the independent
    variables are discrete:'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  id: totrans-1186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: The measure of the model quality is F-statistics. While one can always run R
    algorithms with RDD using the pipe mechanism with `Rscript`, I will partially
    cover this functionality with respect to **Java Specification Request** (**JSR**)
    223 Python integration later. In this section, I would like to explore specifically
    a generalized linear regression `glm` function that is implemented both in R and
    SparkR natively.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
- en: Generalized linear model
  id: totrans-1188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once again, you can run either R `glm` or SparkR `glm`. The list of possible
    link and optimization functions for R implementation is provided in the following
    table:'
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list shows possible options for R `glm` implementation:'
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
- en: '| Family | Variance | Link |'
  id: totrans-1191
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1192
  prefs: []
  type: TYPE_TB
- en: '| gaussian | gaussian | identity |'
  id: totrans-1193
  prefs: []
  type: TYPE_TB
- en: '| binomial | binomial | logit, probit or cloglog |'
  id: totrans-1194
  prefs: []
  type: TYPE_TB
- en: '| poisson | poisson | log, identity or sqrt |'
  id: totrans-1195
  prefs: []
  type: TYPE_TB
- en: '| Gamma | Gamma | inverse, identity or log |'
  id: totrans-1196
  prefs: []
  type: TYPE_TB
- en: '| inverse.gaussian | inverse.gaussian | 1/mu^2 |'
  id: totrans-1197
  prefs: []
  type: TYPE_TB
- en: '| quasi | user-defined | user-defined |'
  id: totrans-1198
  prefs: []
  type: TYPE_TB
- en: 'I will use a binary target, `ArrDel15`, which indicates whether the plane was
    more than 15 minutes late for the arrival. The independent variables will be `DepDel15`,
    `DayOfWeek`, `Month`, `UniqueCarrier`, `Origin`, and `Dest`:'
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  id: totrans-1200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'While you wait for the results, open another shell and run `glm` in the `SparkR`
    mode on the full seven months of data:'
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  id: totrans-1202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: Here we try to build a model explaining delays as an effect of carrier, day
    of week, and origin on destination airports, which is captured by the formular
    construct `ArrDel15 ~ UniqueCarrier + DayOfWeek + Origin + Dest`.
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Nulls, big data, and Scala**'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the SparkR case of `glm`, I had to explicitly filter out the non-cancelled
    flights and removed the NA—or nulls in the C/Java lingo. While R does this for
    you by default, NAs in big data are very common as the datasets are typically
    sparse and shouldn't be treated lightly. The fact that we have to deal with nulls
    explicitly in MLlib warns us about some additional information in the dataset
    and is definitely a welcome feature. The presence of an NA can carry information
    about the way the data was collected. Ideally, each NA should be accompanied by
    a small `get_na_info` method as to why this particular value was not available
    or collected, which leads us to the `Either` type in Scala.
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
- en: Even though nulls are inherited from Java and a part of Scala, the `Option`
    and `Either` types are new and more robust mechanism to deal with special cases
    where nulls were traditionally used. Specifically, `Either` can provide a value
    or exception message as to why it was not computed; while `Option` can either
    provide a value or be `None`, which can be readily captured by the Scala pattern-matching
    framework.
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing you will notice is that SparkR will run multiple threads, and even
    on a single node, it will consume CPU time from multiple cores and returns much
    faster even with a larger size of data. In my experiment on a 32-core machine,
    it was able to finish in under a minute (as opposed to 35 minutes for R `glm`).
    To get the results, as in the R model case, we need to run the `summary()` method:'
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-1209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'The worst performer is `NK` (Spirit Airlines). Internally, SparkR uses limited-memory
    BFGS, which is a limited-memory quasi-Newton optimization method that is similar
    to the results obtained with R `glm` on the July data:'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  id: totrans-1211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'Other parameters of SparkR `glm` implementation are provided in the following
    table:'
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows a list of parameters for SparkR `glm` implementation:'
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Possible Values | Comments |'
  id: totrans-1214
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1215
  prefs: []
  type: TYPE_TB
- en: '| `formula` | A symbolic description like in R | Currently only a subset of
    formula operators are supported: ''`~`'', ''`.`'', ''`:`'', ''`+`'', and ''`-`''
    |'
  id: totrans-1216
  prefs: []
  type: TYPE_TB
- en: '| `family` | gaussian or binomial | Needs to be in quotes: gaussian -> linear
    regression, binomial -> logistic regression |'
  id: totrans-1217
  prefs: []
  type: TYPE_TB
- en: '| `data` | DataFrame | Needs to be SparkR DataFrame, not `data.frame` |'
  id: totrans-1218
  prefs: []
  type: TYPE_TB
- en: '| `lambda` | positive | Regularization coefficient |'
  id: totrans-1219
  prefs: []
  type: TYPE_TB
- en: '| `alpha` | positive | Elastic-net mixing parameter (refer to glmnet''s documentation
    for details) |'
  id: totrans-1220
  prefs: []
  type: TYPE_TB
- en: '| `standardize` | TRUE or FALSE | User-defined |'
  id: totrans-1221
  prefs: []
  type: TYPE_TB
- en: '| `solver` | l-bfgs, normal or auto | auto will choose the algorithm automatically,
    l-bfgs means limited-memory BFGS, normal means using normal equation as an analytical
    solution to the linear regression problem |'
  id: totrans-1222
  prefs: []
  type: TYPE_TB
- en: Reading JSON files in SparkR
  id: totrans-1223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Schema on Read is one of the convenient features of big data. The DataFrame
    class has the ability to figure out the schema of a text file containing a JSON
    record per line:'
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  id: totrans-1225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: Writing Parquet files in SparkR
  id: totrans-1226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned in the previous chapter, the Parquet format is an efficient
    storage format, particularly for low cardinality columns. Parquet files can be
    read/written directly from R:'
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  id: totrans-1228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'You can see that the new Parquet file is 66 times smaller that the original
    zip file downloaded from the DoT:'
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  id: totrans-1230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: Invoking Scala from R
  id: totrans-1231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's assume that one has an exceptional implementation of a numeric method
    in Scala that we want to call from R. One way of doing this would be to use the
    R `system()` function that invokes `/bin/sh` on Unix-like systems. However, the
    `rscala` package is a more efficient way that starts a Scala interpreter and maintains
    communication over TCP/IP network connection.
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the Scala interpreter maintains the state (memoization) between the calls.
    Similarly, one can define functions, as follows:'
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  id: totrans-1234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'R from Scala can be invoked using the `!` or `!!` Scala operators and `Rscript`
    command:'
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  id: totrans-1236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: Using Rserve
  id: totrans-1237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A more efficient way is to use the similar TCP/IP binary transport protocol
    to communicate with R with `Rsclient/Rserve` ([http://www.rforge.net/Rserve](http://www.rforge.net/Rserve)).
    To start `Rserve` on a node that has R installed, perform the following action:'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  id: totrans-1239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: By default, `Rserv` opens a connection on `localhost:6311`. The advantage of
    the binary network protocol is that it is platform-independent and multiple clients
    can communicate with the server. The clients can connect to `Rserve`.
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
- en: Note that, while passing the results as a binary object has its advantages,
    you have to be careful with the type mappings between R and Scala. `Rserve` supports
    other clients, including Python, but I will also cover JSR 223-compliant scripting
    at the end of this chapter.
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with Python
  id: totrans-1242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python has slowly established ground as a de-facto tool for data science. It
    has a command-line interface and decent visualization via matplotlib and ggplot,
    which is based on R's ggplot2\. Recently, Wes McKinney, the creator of Pandas,
    the time series data-analysis package, has joined Cloudera to pave way for Python
    in big data.
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Python
  id: totrans-1244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python is usually part of the default installation. Spark requires version 2.7.0+.
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don''t have Python on Mac OS, I recommend installing the Homebrew package
    manager from [http://brew.sh](http://brew.sh):'
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  id: totrans-1247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'Otherwise, on a Unix-like system, Python can be compiled from the source distribution:'
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  id: totrans-1249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'It is good practice to place it in a directory different from the default Python
    installation. It is normal to have multiple versions of Python on a single system,
    which usually does not lead to problems as Python separates the installation directories.
    For the purpose of this chapter, as for many machine learning takes, I''ll also
    need a few packages. The packages and specific versions may differ across installations:'
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  id: totrans-1251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: If everything compiles—SciPy uses a Fortran compiler and libraries for linear
    algebra—we are ready to use Python 2.7.11!
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that if one wants to use Python with the `pipe` command in a distributed
    environment, Python needs to be installed on every node in the network.
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
- en: PySpark
  id: totrans-1255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As `bin/sparkR` launches R with preloaded Spark context, `bin/pyspark` launches
    Python shell with preloaded Spark context and Spark driver running. The `PYSPARK_PYTHON`
    environment variable can be used to point to a specific Python version:'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  id: totrans-1257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'PySpark directly supports most of MLlib functionality on Spark RDDs ([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)),
    but it is known to lag a few releases behind the Scala API ([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)).
    As of the 1.6.0+ release, it also supports DataFrames ([http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)):'
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  id: totrans-1259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Calling Python from Java/Scala
  id: totrans-1260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As this is really a book about Scala, we should also mention that one can call
    Python code and its interpreter directly from Scala (or Java). There are a few
    options available that will be discussed in this chapter.
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
- en: Using sys.process._
  id: totrans-1262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Scala, as well as Java, can call OS processes via spawning a separate thread,
    which we already used for interactive analysis in [Chapter 1](part0235.xhtml#aid-703K61
    "Chapter 1. Exploratory Data Analysis"), *Exploratory Data Analysis*: the `.!`
    method will start the process and return the exit code, while `.!!` will return
    the string that contains the output:'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  id: totrans-1264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'Let''s try a more complex SVD computation (similar to the one we used in SVD++
    recommendation engine, but this time, it invokes BLAS C-libraries at the backend).
    I created a Python executable that takes a string representing a matrix and the
    required rank as an input and outputs an SVD approximation with the provided rank:'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  id: totrans-1266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Let''s call it `svd.py` and put in in the current directory. Given a matrix
    and rank as an input, it produces an approximation of a given rank:'
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  id: totrans-1268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'To call it from Scala, let''s define the following `#<<<` method in our DSL:'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  id: totrans-1270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'Now, we can use the `#<<<` operator to call Python''s SVD method:'
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  id: totrans-1272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'Note that as we requested the resulting matrix rank to be one, all rows and
    columns are linearly dependent. We can even pass several lines of input at a time,
    as follows:'
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  id: totrans-1274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: Spark pipe
  id: totrans-1275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SVD decomposition is usually a pretty heavy operation, so the relative overhead
    of calling Python in this case is small. We can avoid this overhead if we keep
    the process running and supply several lines at a time, like we did in the last
    example. Both Hadoop MR and Spark implement this approach. For example, in Spark,
    the whole computation will take only one line, as shown in the following:'
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  id: totrans-1277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: The whole pipeline is ready to be distributed across a cluster of multicore
    workstations! I think you will be in love with Scala/Spark already.
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
- en: Note that debugging the pipelined executions might be tricky as the data is
    passed from one process to another using OS pipes.
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
- en: Jython and JSR 223
  id: totrans-1280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For completeness, we need to mention Jython, a Java implementation of Python
    (as opposed to a more familiar C implementation, also called CPython). Jython
    avoids the problem of passing input/output via OS pipelines by allowing the users
    to compile Python source code to Java byte codes, and running the resulting bytecodes
    on any Java virtual machine. As Scala also runs in Java virtual machine, it can
    use the Jython classes directly, although the reverse is not true in general;
    Scala classes sometimes are not compatible to be used by Java/Jython.
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**JSR 223**'
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
- en: In this particular case, the request is for "Scripting for the JavaTM Platform"
    and was originally filed on Nov 15th 2004 ([https://www.jcp.org/en/jsr/detail?id=223](https://www.jcp.org/en/jsr/detail?id=223)).
    At the beginning, it was targeted towards the ability of the Java servlet to work
    with multiple scripting languages. The specification requires the scripting language
    maintainers to provide a Java JAR with corresponding implementations. Portability
    issues hindered practical implementations, particularly when platforms require
    complex interaction with OS, such as dynamic linking in C or Fortran. Currently,
    only a handful languages are supported, with R and Python being supported, but
    in incomplete form.
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Java 6, JSR 223: Scripting for Java added the `javax.script` package
    that allows multiple scripting languages to be called through the same API as
    long as the language provides a script engine. To add the Jython scripting language,
    download the latest Jython JAR from the Jython site at [http://www.jython.org/downloads.html](http://www.jython.org/downloads.html):'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  id: totrans-1286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'Now, I can use the Jython/Python scripting engine:'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  id: totrans-1288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: It is worth giving a disclaimer here that not all Python modules are available
    in Jython. Modules that require a C/Fortran dynamic linkage for the library that
    doesn't exist in Java are not likely to work in Jython. Specifically, NumPy and
    SciPy are not supported in Jython as they rely on C/Fortran. If you discover some
    other missing modules, you can try copying the `.py` file from a Python distribution
    to a `sys.path` Jython directory—if this works, consider yourself lucky.
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
- en: 'Jython has the advantage of accessing Python-rich modules without the necessity
    of starting the Python runtime on each call, which might result in a significant
    performance saving:'
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  id: totrans-1291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: Jython JSR 223 call is 10 times faster!
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R and Python are like bread and butter for a data scientist. Modern frameworks
    tend to be interoperable and borrow from each other's strength. In this chapter,
    I went over the plumbing of interoperability with R and Python. Both of them have
    packages (R) and modules (Python) that became very popular and extend the current
    Scala/Spark functionality. Many consider R and Python existing libraries to be
    crucial for their implementations.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
- en: This chapter demonstrated a few ways to integrate these packages and provide
    the tradeoffs of using these integrations so that we can proceed on to the next
    chapter, looking at the NLP, where functional programming has been traditionally
    used from the start.
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9. NLP in Scala
  id: totrans-1296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter describes a few common techniques of **Natural Language Processing**
    (**NLP**), specifically, the ones that can benefit from Scala. There are some
    NLP packages in the open source out there. The most famous of them is probably
    NLTK ([http://www.nltk.org](http://www.nltk.org)), which is written in Python,
    and ostensibly even a larger number of proprietary software solutions emphasizing
    different aspects of NLP. It is worth mentioning Wolf ([https://github.com/wolfe-pack](https://github.com/wolfe-pack)),
    FACTORIE ([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)), and ScalaNLP
    ([http://www.scalanlp.org](http://www.scalanlp.org)), and skymind ([http://www.skymind.io](http://www.skymind.io)),
    which is partly proprietary. However, few open source projects in this area remain
    active for a long period of time for one or another reason. Most projects are
    being eclipsed by Spark and MLlib capabilities, particularly, in the scalability
    aspect.
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
- en: Instead of giving a detailed description of each of the NLP projects, which
    also might include speech-to-text, text-to-speech, and language translators, I
    will provide a few basic techniques focused on leveraging Spark MLlib in this
    chapter. The chapter comes very naturally as the last analytics chapter in this
    book. Scala is a very natural-language looking computer language and this chapter
    will leverage the techniques I developed earlier.
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
- en: NLP arguably is the core of AI. Originally, the AI was created to mimic the
    humans, and natural language parsing and understanding is an indispensable part
    of it. Big data techniques has started to penetrate NLP, even though traditionally,
    NLP is very computationally intensive and is regarded as a small data problem.
    NLP often requires extensive deep learning techniques, and the volume of data
    of all written texts appears to be not so large compared to the logs volumes generated
    by all the machines today and analyzed by the big data machinery.
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
- en: Even though the Library of Congress counts millions of documents, most of them
    can be digitized in PBs of actual digital data, a volume that any social websites
    is able to collect, store, and analyze within a few seconds. Complete works of
    most prolific authors can be stored within a few MBs of files (refer to *Table
    09-1*). Nonetheless, the social network and ADTECH companies parse text from millions
    of users and in hundreds of contexts every day.
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
- en: '| The complete works of | When lived | Size |'
  id: totrans-1301
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1302
  prefs: []
  type: TYPE_TB
- en: '| *Plato* | 428/427 (or 424/423) - 348/347 BC | 2.1 MB |'
  id: totrans-1303
  prefs: []
  type: TYPE_TB
- en: '| *William Shakespeare* | 26 April 1564 (baptized) - 23 April 1616 | 3.8 MB
    |'
  id: totrans-1304
  prefs: []
  type: TYPE_TB
- en: '| *Fyodor Dostoevsky* | 11 November 1821 - 9 February 1881 | 5.9 MB |'
  id: totrans-1305
  prefs: []
  type: TYPE_TB
- en: '| *Leo Tolstoy* | 9 September 1828 - 20 November 1910 | 6.9 MB |'
  id: totrans-1306
  prefs: []
  type: TYPE_TB
- en: '| *Mark Twain* | November 30, 1835 - April 21, 1910 | 13 MB |'
  id: totrans-1307
  prefs: []
  type: TYPE_TB
- en: Table 09-1\. Complete Works collections of some famous writers (most can be
    acquired on Amazon.com today for a few dollars, later authors, although readily
    digitized, are more expensive)
  id: totrans-1308
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The natural language is a dynamic concept that changes over time, technology,
    and generations. We saw the appearance of emoticons, three-letter abbreviations,
    and so on. Foreign languages tend to borrow from each other; describing this dynamic
    ecosystem is a challenge on itself.
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous chapters, I will focus on how to use Scala as a tool to orchestrate
    the language analysis rather than rewriting the tools in Scala. As the topic is
    so large, I will not claim to cover all aspects of NLP here.
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
- en: Discussing NLP with the example of text processing pipeline and stages
  id: totrans-1312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning techniques for simple text analysis in terms of bags
  id: totrans-1313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about **Term Frequency Inverse Document Frequency** (**TF-IDF**) technique
    that goes beyond simple bag analysis and de facto the standard in **Information
    Retrieval** (**IR**)
  id: totrans-1314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about document clustering with the example of the **Latent Dirichlet
    Allocation** (**LDA**) approach
  id: totrans-1315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing semantic analysis using word2vec n-gram-based algorithms
  id: totrans-1316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text analysis pipeline
  id: totrans-1317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we proceed to detailed algorithms, let's look at a generic text-processing
    pipeline depicted in *Figure 9-1*. In text analysis, the input is usually presented
    as a stream of characters (depending on the specific language).
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
- en: 'Lexical analysis has to do with breaking this stream into a sequence of words
    (or lexemes in linguistic analysis). Often it is also called tokenization (and
    the words called the tokens). **ANother Tool for Language Recognition** (**ANTLR**)
    ([http://www.antlr.org/](http://www.antlr.org/)) and Flex ([http://flex.sourceforge.net](http://flex.sourceforge.net))
    are probably the most famous in the open source community. One of the classical
    examples of ambiguity is lexical ambiguity. For example, in the phrase *I saw
    a bat.* *bat* can mean either an animal or a baseball bat. We usually need context
    to figure this out, which we will discuss next:'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
- en: '![Text analysis pipeline](img/image01755.jpeg)'
  id: totrans-1320
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. Typical stages of an NLP process.
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
- en: Syntactic analysis, or parsing, traditionally deals with matching the structure
    of the text with grammar rules. This is relatively more important for computer
    languages that do not allow any ambiguity. In natural languages, this process
    is usually called chunking and tagging. In many cases, the meaning of the word
    in human language can be subject to context, intonation, or even body language
    or facial expression. The value of such analysis, as opposed to the big data approach,
    where the volume of data trumps complexity is still a contentious topic—one example
    of the latter is the word2vec approach, which will be described later.
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic analysis is the process of extracting language-independent meaning
    from the syntactic structures. As much as possible, it also involves removing
    features specific to particular cultural and linguistic contexts, to the extent
    that such a project is possible. The sources of ambiguity at this stage are: phrase
    attachment, conjunction, noun group structure, semantic ambiguity, anaphoric non-literal
    speech, and so on. Again, word2vec partially deals with these issues.'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
- en: 'Disclosure integration partially deals with the issue of the context: the meaning
    of a sentence or an idiom can depend on the sentences or paragraphs before that.
    Syntactic analysis and cultural background play an important role here.'
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
- en: Finally, pragmatic analysis is yet another layer of complexity trying to reinterpret
    what is said in terms of what the intention was. How does this change the state
    of the world? Is it actionable?
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
- en: Simple text analysis
  id: totrans-1326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The straightforward representation of the document is a bag of words. Scala,
    and Spark, provides an excellent paradigm to perform analysis on the word distributions.
    First, we read the whole collection of texts, and then count the unique words:'
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  id: totrans-1328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: 'This gives us just an estimate of the number of distinct words in the repertoire
    of quite different authors. The simplest way to find intersection between the
    two corpuses is to find the common vocabulary (which will be quite different as
    *Leo Tolstoy* wrote in Russian and French, while *Shakespeare* was an English-writing
    author):'
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  id: totrans-1330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'A few thousands word indices are manageable with the current implementations.
    For any new story, we can determine whether it is more likely to be written by
    Leo Tolstoy or *William Shakespeare*. Let''s take a look at *The King James Version
    of the Bible*, which also can be downloaded from Project Gutenberg ([https://www.gutenberg.org/files/10/10-h/10-h.htm](https://www.gutenberg.org/files/10/10-h/10-h.htm)):'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE161]'
  id: totrans-1332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'This seems reasonable as the religious language was popular during the Shakespearean
    time. On the other hand, plays by *Anton Chekhov* have a larger intersection with
    the *Leo Tolstoy* vocabulary:'
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  id: totrans-1334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: This is a very simple approach that works, but there are a number of commonly
    known improvements we can make. First, a common technique is to stem the words.
    In many languages, words have a common part, often called root, and a changeable
    prefix or suffix, which may depend on the context, gender, time, and so on. Stemming
    is the process of improving the distinct count and intersection by approximating
    this flexible word form to the root, base, or a stem form in general. The stem
    form does not need to be identical to the morphological root of the word, it is
    usually sufficient that related words map to the same stem, even if this stem
    is not in itself a valid grammatical root. Secondly, we probably should account
    for the frequency of the words—while we will describe more elaborate methods in
    the next section, for the purpose of this exercise, we'll exclude the words with
    very high count, that usually are present in any document such as articles and
    possessive pronouns, which are usually called stop words, and the words with very
    low count. Specifically, I'll use the optimized **Porter Stemmer** implementation
    that I described in more detail at the end of the chapter.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [http://tartarus.org/martin/PorterStemmer/](http://tartarus.org/martin/PorterStemmer/)
    site contains some of the Porter Stemmer implementations in Scala and other languages,
    including a highly optimized ANSI C, which may be more efficient, but here I will
    provide another optimized Scala version that can be used immediately with Spark.
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
- en: 'The Stemmer example will stem the words and count the relative intersections
    between them, removing the stop words:'
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  id: totrans-1339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'When one runs the main class example from the command line, it outputs the
    stemmed bag sizes and intersection for datasets specified as parameters (these
    are directories in the home filesystem with documents):'
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  id: totrans-1341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: This, in this case, just confirms the hypothesis that Bible's vocabulary is
    closer to *William Shakespeare* than to Leo Tolstoy and other sources. Interestingly,
    modern vocabularies of *NY Times* articles and Enron's e-mails from the previous
    chapters are much closer to *Leo Tolstoy's*, which is probably more an indication
    of the translation quality.
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to notice is that the pretty complex analysis took about *40*
    lines of Scala code (not counting the libraries, specifically the Porter Stemmer,
    which is about ~ *100* lines) and about 12 seconds. The power of Scala is that
    it can leverage other libraries very efficiently to write concise code.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Serialization**'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
- en: We already talked about serialization in [Chapter 6](part0273.xhtml#aid-84B9I2
    "Chapter 6. Working with Unstructured Data"), *Working with Unstructured Data*.
    As Spark's tasks are executed in different threads and potentially JVMs, Spark
    does a lot of serialization/deserialization when passing the objects. Potentially,
    I could use `map { val stemmer = new Stemmer; stemmer.stem(_) }` instead of `map
    { stemmer.stem(_) }`, but the latter reuses the object for multiple iterations
    and seems to be linguistically more appealing. One suggested performance optimization
    is to use *Kryo serializer*, which is less flexible than the Java serializer,
    but more performant. However, for integrative purpose, it is much easier to just
    make every object in the pipeline serializable and use default Java serialization.
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
- en: 'As another example, let''s compute the distribution of word frequencies, as
    follows:'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  id: totrans-1348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'The distribution of relative frequencies on the log-log scale is presented
    in the following diagram. With the exception of the first few tokens, the dependency
    of frequency on rank is almost linear:'
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple text analysis](img/image01756.jpeg)'
  id: totrans-1350
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. A typical distribution of word relative frequencies on log-log
    scale (Zipf's Law)
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
- en: MLlib algorithms in Spark
  id: totrans-1352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's halt at MLlib that complements other NLP libraries written in Scala. MLlib
    is primarily important because of scalability, and thus supports a few of the
    data preparation and text processing algorithms, particularly in the area of feature
    construction ([http://spark.apache.org/docs/latest/ml-features.html](http://spark.apache.org/docs/latest/ml-features.html)).
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  id: totrans-1354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the preceding analysis can already give a powerful insight, the piece
    of information that is missing from the analysis is term frequency information.
    The term frequencies are relatively more important in information retrieval, where
    the collection of documents need to be searched and ranked in relation to a few
    terms. The top documents are usually returned to the user.
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF is a standard technique where term frequencies are offset by the frequencies
    of the terms in the corpus. Spark has an implementation of the TF-IDF. Spark uses
    a hash function to identify the terms. This approach avoids the need to compute
    a global term-to-index map, but can be subject to potential hash collisions, the
    probability of which is determined by the number of buckets of the hash table.
    The default feature dimension is *2^20=1,048,576*.
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Spark implementation, each document is a line in the dataset. We can
    convert it into to an RDD of iterables and compute the hashing by the following
    code:'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  id: totrans-1358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'When computing `hashingTF`, we only need a single pass over the data, applying
    IDF needs two passes: first to compute the IDF vector and second to scale the
    term frequencies by IDF:'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  id: totrans-1360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: Here we see each document represented by a set of terms and their scores.
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
- en: LDA
  id: totrans-1362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LDA in Spark MLlib is a clustering mechanism, where the feature vectors represent
    the counts of words in a document. The model maximizes the probability of observing
    the word counts, given the assumption that each document is a mixture of topics
    and the words in the documents are generated based on **Dirichlet distribution**
    (a generalization of beta distribution on multinomial case) for each of the topic
    independently. The goal is to derive the (latent) distribution of the topics and
    the parameters of the words generation statistical model.
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
- en: The MLlib implementation is based on 2009 LDA paper ([http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf](http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf))
    and uses GraphX to implement a distributed **Expectation Maximization** (**EM**)
    algorithm for assigning topics to the documents.
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the Enron e-mail corpus discussed in [Chapter 7](part0283.xhtml#aid-8DSF61
    "Chapter 7. Working with Graph Algorithms"), *Working with Graph Algorithms*,
    where we tried to figure out communications graph. For e-mail clustering, we need
    to extract the body of the e-mail and place is as a single line in the training
    file:'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  id: totrans-1366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'Now, let''s use Scala/Spark to construct a corpus dataset containing the document
    ID, followed by a dense array of word counts in the bag:'
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  id: totrans-1368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'We can also list the words and their relative importance for the topic in the
    descending order:'
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  id: totrans-1370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: 'To find out the top documents per topic or top topics per document, we need
    to convert this model to `DistributedLDA` or `LocalLDAModel`, which extend `LDAModel`:'
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  id: totrans-1372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: Segmentation, annotation, and chunking
  id: totrans-1373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the text is presented in digital form, it is relatively easy to find words
    as we can split the stream on non-word characters. This becomes more complex in
    spoken language analysis. In this case, segmenters try to optimize a metric, for
    example, to minimize the number of distinct words in the lexicon and the length
    or complexity of the phrase (*Natural Language Processing with Python* by *Steven
    Bird et al*, *O'Reilly Media Inc*, 2009).
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
- en: Annotation usually refers to parts-of-speech tagging. In English, these are
    nouns, pronouns, verbs, adjectives, adverbs, articles, prepositions, conjunctions,
    and interjections. For example, in the phrase *we saw the yellow dog*, *we* is
    a pronoun, *saw* is a verb, *the* is an article, *yellow* is an adjective, and
    *dog* is a noun.
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
- en: In some languages, the chunking and annotation depends on context. For example,
    in Chinese, *????* literally translates to *love country person* and can mean
    either *country-loving person* or *love country-person*. In Russian, *???????
    ?????? ??????????*, literally translating to *execute not pardon*, can mean *execute,
    don't pardon*, or *don't execute, pardon*. While in written language, this can
    be disambiguated using commas, in a spoken language this is usually it is very
    hard to recognize the difference, even though sometimes the intonation can help
    to segment the phrase properly.
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
- en: For techniques based on word frequencies in the bags, some extremely common
    words, which are of little value in helping select documents, are explicitly excluded
    from the vocabulary. These words are called stop words. There is no good general
    strategy for determining a stop list, but in many cases, this is to exclude very
    frequent words that appear in almost every document and do not help to differentiate
    between them for classification or information retrieval purposes.
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
- en: POS tagging
  id: totrans-1378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'POS tagging probabilistically annotates each word with it''s grammatical function—noun,
    verb, adjective, and so on. Usually, POS tagging serves as an input to syntactic
    and semantic analysis. Let''s demonstrate POS tagging on the FACTORIE toolkit
    example, a software library written in Scala ([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)).
    To start, you need to download the binary image or source files from [https://github.com/factorie/factorie.git](https://github.com/factorie/factorie.git)
    and build it:'
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  id: totrans-1380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'After the build, which also includes model training, the following command
    will start a network server on `port 3228`:'
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  id: totrans-1382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: 'Now, all traffic to `port 3228` will be interpreted (as text), and the output
    will be tokenized and annotated:'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  id: totrans-1384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: This POS is a single-path left-right tagger that can process the text as a stream.
    Internally, the algorithm uses probabilistic techniques to find the most probable
    assignment. Let's also look at other techniques that do not use grammatical analysis
    and yet proved to be very useful for language understanding and interpretation.
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
- en: Using word2vec to find word relationships
  id: totrans-1386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Word2vec has been developed by Tomas Mikolov at Google, around 2012\. The original
    idea behind word2vec was to demonstrate that one might improve efficiency by trading
    the model''s complexity for efficiency. Instead of representing a document as
    bags of words, word2vec takes each word context into account by trying to analyze
    n-grams or skip-grams (a set of surrounding tokens with potential the token in
    question skipped). The words and word contexts themselves are represented by an
    array of floats/doubles ![Using word2vec to find word relationships](img/image01757.jpeg).
    The objective function is to maximize log likelihood:'
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
- en: '![Using word2vec to find word relationships](img/image01758.jpeg)'
  id: totrans-1388
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
- en: '![Using word2vec to find word relationships](img/image01759.jpeg)'
  id: totrans-1390
  prefs: []
  type: TYPE_IMG
- en: By choosing the optimal ![Using word2vec to find word relationships](img/image01757.jpeg)
    and to get a comprehensive word representation (also called **map optimization**).
    Similar words are found based on cosine similarity metric (dot product) of ![Using
    word2vec to find word relationships](img/image01757.jpeg). Spark implementation
    uses hierarchical softmax, which reduces the complexity of computing the conditional
    probability to ![Using word2vec to find word relationships](img/image01760.jpeg),
    or log of the vocabulary size *V*, as opposed to ![Using word2vec to find word
    relationships](img/image01761.jpeg), or proportional to *V*. The training is still
    linear in the dataset size, but is amenable to big data parallelization techniques.
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
- en: '`Word2vec` is traditionally used to predict the most likely word given context
    or find similar words with a similar meaning (synonyms). The following code trains
    in `word2vec` model on *Leo Tolstoy''s Wars and Peace*, and finds synonyms for
    the word *circle*. I had to convert the Gutenberg''s representation of *War and
    Peace* to a single-line format by running the `cat 2600.txt | tr "\n\r" " " >
    warandpeace.txt` command:'
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  id: totrans-1393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: While in general, it is hard to some with an objective function, and `freedom`
    is not listed as a synonym to `life` in the English Thesaurus, the results do
    make sense.
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
- en: 'Each word in the word2vec model is represented as an array of doubles. Another
    interesting application is to find associations *a to b is the same as c to ?*
    by performing subtraction *vector(a) - vector(b) + vector(c)*:'
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  id: totrans-1396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: This can be used to find relationships in the language.
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
- en: A Porter Stemmer implementation of the code
  id: totrans-1398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Porter Stemmer was first developed around the 1980s and there are many implementations.
    The detailed steps and original reference are provided at [http://tartarus.org/martin/PorterStemmer/def.txt](http://tartarus.org/martin/PorterStemmer/def.txt).
    It consists of roughly 6-9 steps of suffix/endings replacements, some of which
    are conditional on prefix or stem. I will provide a Scala-optimized version with
    the book code repository. For example, step 1 covers the majority of stemming
    cases and consists of 12 substitutions: the last 8 of which are conditional on
    the number of syllables and the presence of vowels in the stem:'
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  id: totrans-1400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: The complete code is available at [https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala](https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala).
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I described basic NLP concepts and demonstrated a few basic
    techniques. I hoped to demonstrate that pretty complex NLP concepts could be expressed
    and tested in a few lines of Scala code. This is definitely just the tip of the
    iceberg as a lot of NLP techniques are being developed now, including the ones
    based on in-CPU parallelization as part of GPUs. (refer to, for example, **Puck**
    at [https://github.com/dlwh/puck](https://github.com/dlwh/puck)). I also gave
    a flavor of major Spark MLlib NLP implementations.
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, which will be the final chapter of this book, I'll cover
    systems and model monitoring.
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10. Advanced Model Monitoring
  id: totrans-1405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though this is the last chapter of the book, it can hardly be an afterthought
    even though monitoring in general often is in practical situations, quite unfortunately.
    Monitoring is a vital deployment component for any long execution cycle component
    and thus is part of the finished product. Monitoring can significantly enhance
    product experience and define future success as it improves problem diagnostic
    and is essential to determine the improvement path.
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary rules of successful software engineering is to create systems
    as if they were targeted for personal use when possible, which fully applies to
    monitoring, diagnostic, and debugging—quite hapless name for fixing existing issues
    in software products. Diagnostic and debugging of complex systems, particularly
    distributed systems, is hard, as the events often can be arbitrary interleaved
    and program executions subject to race conditions. While there is a lot of research
    going in the area of distributed system devops and maintainability, this chapter
    will scratch the service and provide guiding principle to design a maintainable
    complex distributed system.
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
- en: To start with, a pure functional approach, which Scala claims to follow, spends
    a lot of time avoiding side effects. While this idea is useful in a number of
    aspects, it is hard to imagine a useful program that has no effect on the outside
    world, the whole idea of a data-driven application is to have a positive effect
    on the way the business is conducted, a well-defined side effect.
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring clearly falls in the side effect category. Execution needs to leave
    a trace that the user can later parse in order to understand where the design
    or implementation went awry. The trace of the execution can be left by either
    writing something on a console or into a file, usually called a log, or returning
    an object that contains the trace of the program execution, and the intermediate
    results. The latter approach, which is actually more in line with functional programming
    and monadic philosophy, is actually more appropriate for the distributed programming
    but often overlooked. This would have been an interesting topic for research,
    but unfortunately the space is limited and I have to discuss the practical aspects
    of monitoring in contemporary systems that is almost always done by logging. Having
    the monadic approach of carrying an object with the execution trace on each call
    can certainly increase the overhead of the interprocess or inter-machine communication,
    but saves a lot of time in stitching different pieces of information together.
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s list the naive approaches to debugging that everyone who needed to find
    a bug in the code tried:'
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing program output, particularly logs produced by simple print statements
    or built-in logback, java.util.logging, log4j, or the slf4j façade
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attaching a (remote) debugger
  id: totrans-1412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring CPU, disk I/O, memory (to resolve higher level resource-utilization
    issues)
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More or less, all these approaches fail if we have a multithreaded or distributed
    system—and Scala is inherently multithreaded as Spark is inherently distributed.
    Collecting logs over a set of nodes is not scalable (even though a few successful
    commercial systems exist that do this). Attaching a remote debugger is not always
    possible due to security and network restrictions. Remote debugging can also induce
    substantial overhead and interfere with the program execution, particularly for
    ones that use synchronization. Setting the debug level to the `DEBUG` or `TRACE`
    level helps sometimes, but leaves you at the mercy of the developer who may or
    may not have thought of a particular corner case you are dealing with right at
    the moment. The approach we take in this book is to open a servlet with enough
    information to glean into program execution and application methods real-time,
    as much as it is possible with the current state of Scala and Scalatra.
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
- en: 'Enough about the overall issues of debugging the program execution. Monitoring
    is somewhat different, as it is concerned with only high-level issue identification.
    Intersection with issue investigation or resolution happens, but usually is outside
    of monitoring. In this chapter, we will cover the following topics:'
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
- en: Understanding major areas for monitoring and monitoring goals
  id: totrans-1416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning OS tools for Scala/Java monitoring to support issue identification
    and debugging
  id: totrans-1417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about MBeans and MXBeans
  id: totrans-1418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding model performance drift
  id: totrans-1419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding A/B testing
  id: totrans-1420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System monitoring
  id: totrans-1421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While there are other types of monitoring dealing specifically with ML-targeted
    tasks, such as monitoring the performance of the models, let me start with basic
    system monitoring. Traditionally, system monitoring is a subject of operating
    system maintenance, but it is becoming a vital component of any complex application,
    specifically running over a set of distributed workstations. The primary components
    of the OS are CPU, disk, memory, network, and energy on battery-powered machines.
    The traditional OS-like tools for monitoring system performance are provided in
    the following table. We limit them to Linux tools as this is the platform for
    most Scala applications, even though other OS vendors provide OS monitoring tools
    such as **Activity Monitor**. As Scala runs in Java JVM, I also added Java-specific
    monitoring tools that are specific to JVMs:'
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
- en: '| Area | Programs | Comments |'
  id: totrans-1423
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-1424
  prefs: []
  type: TYPE_TB
- en: '| CPU | `htop,` `top`, `sar-u` | `top` has been the most often used performance
    diagnostic tool, as CPU and memory have been the most constraint resources. With
    the advent of distributed programming, network and disk tend to be the most constraint.
    |'
  id: totrans-1425
  prefs: []
  type: TYPE_TB
- en: '| Disk | `iostat`, `sar -d`, `lsof` | The number of open files, provided by
    `lsof`, is often a constraining resource as many big data applications and daemons
    tend to keep multiple files open. |'
  id: totrans-1426
  prefs: []
  type: TYPE_TB
- en: '| Memory | `top`, `free`, `vmstat`, `sar -r` | Memory is used by OS in multiple
    ways, for example to maintain disk I/O buffers so that having extra buffered and
    cached memory helps performance. |'
  id: totrans-1427
  prefs: []
  type: TYPE_TB
- en: '| Network | `ifconfig`, `netstat`, `tcpdump`, `nettop`, `iftop`, `nmap` | Network
    is how the distributed systems talk and is an important OS component. From the
    application point of view, watch for errors, collisions, and dropped packets as
    an indicator of problems. |'
  id: totrans-1428
  prefs: []
  type: TYPE_TB
- en: '| Energy | `powerstat` | While power consumption is traditionally not a part
    of OS monitoring, it is nevertheless a shared resource, which recently became
    one of the major costs for maintaining a working system. |'
  id: totrans-1429
  prefs: []
  type: TYPE_TB
- en: '| Java | `jconsole`, `jinfo`, `jcmd`, `jmc` | All these tools allow you to
    examine configuration and run-time properties of an application. **Java Mission
    Control** (**JMC**) is shipped with JDK starting with version 7u40. |'
  id: totrans-1430
  prefs: []
  type: TYPE_TB
- en: Table 10.1\. Common Linux OS monitoring tools
  id: totrans-1431
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In many cases, the tools are redundant. For example, the CPU and memory information
    can be obtained with `top`, `sar`, and `jmc` commands.
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
- en: There are a few tools for collecting this information over a set of distributed
    nodes. Ganglia is a BSD-licensed scalable distributed monitoring system ([http://ganglia.info](http://ganglia.info)).
    It is based on a hierarchical design and is very careful about data structure
    and algorithm designs. It is known to scale to 10,000s of nodes. It consists of
    a gmetad daemon that is collects information from multiple hosts and presents
    it in a web interface, and gmond daemons running on each individual host. The
    communication happens on the 8649 port by default, which spells Unix. By default,
    gmond sends information about CPU, memory, and network, but multiple plugins exist
    for other metrics (or can be created). Gmetad can aggregate the information and
    pass it up the hierarchy chain to another gmetad daemon. Finally, the data is
    presented in a Ganglia web interface.
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
- en: Graphite is another monitoring tool that stores numeric time-series data and
    renders graphs of this data on demand. The web app provides a /render endpoint
    to generate graphs and retrieve raw data via a RESTful API. Graphite has a pluggable
    backend (although it has it's own default implementation). Most of the modern
    metrics implementations, including scala-metrics used in this chapter, support
    sending data to Graphite.
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
- en: Process monitoring
  id: totrans-1435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tools described in the previous section are not application-specific. For
    a long-running process, it often necessary to provide information about the internal
    state to either a monitoring a graphing solution such as Ganglia or Graphite,
    or just display it in a servlet. Most of these solutions are read-only, but in
    some cases, the commands give the control to the users to modify the state, such
    as log levels, or to trigger garbage collection.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
- en: 'Monitoring, in general is supposed to do the following:'
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
- en: Provide high-level information about program execution and application-specific
    metrics
  id: totrans-1438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potentially, perform health-checks for critical components
  id: totrans-1439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Might incorporate alerting and thresholding on some critical metrics
  id: totrans-1440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have also seen monitoring to include update operations to either update the
    logging parameters or test components, such as trigger model scoring with predefined
    parameters. The latter can be considered as a part of parameterized health check.
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how it works on the example of a simple `Hello World` web application
    that accepts REST-like requests and assigns a unique ID for different users written
    in the Scalatra framework ([http://scalatra.org](http://scalatra.org)), a lightweight
    web-application development framework in Scala. The application is supposed to
    respond to CRUD HTTP requests to create a unique numeric ID for a user. To implement
    the service in Scalatra, we need just to provide a `Scalate` template. The full
    documentation can be found at [http://scalatra.org/2.4/guides/views/scalate.html](http://scalatra.org/2.4/guides/views/scalate.html),
    the source code is provided with the book and can be found in `chapter10` subdirectory:'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  id: totrans-1443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: 'First, the code gets the `name` parameter from the request (REST-like parameter
    parsing is also supported). Then, it checks the internal HashMap for existing
    entries, and if the entry does not exist, it creates a new index using a synchronized
    call to increment `hwCounter` (in a real-world application, this information should
    be persistent in a database such as HBase, but I''ll skip this layer in this section
    for the purpose of simplicity). To run the application, one needs to download
    the code, start `sbt`, and type `~;jetty:stop;jetty:start` to enable continuous
    run/compilation as in [Chapter 7](part0283.xhtml#aid-8DSF61 "Chapter 7. Working
    with Graph Algorithms"), *Working with Graph Algorithms*. The modifications to
    the file will be immediately picked up by the build tool and the jetty server
    will restart:'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  id: totrans-1445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: 'When the servlet is started on port 8080, issue a browser request:'
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-1447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I pre-created the project for this book, but if you want to create a Scalatra
    project from scratch, there is a `gitter` command in `chapter10/bin/create_project.sh`.
    Gitter will create a `project/build.scala` file with a Scala object, extending
    build that will set project parameters and enable the Jetty plugin for the SBT.
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
- en: '`http://localhost:8080/hw/Joe`.'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
- en: 'The output should look similar to the following screenshot:'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
- en: '![Process monitoring](img/image01762.jpeg)'
  id: totrans-1451
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-1: The servlet web page.'
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
- en: If you call the servlet with a different name, it will assign a distinct ID,
    which will be persistent across the lifetime of the application.
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
- en: 'As we also enabled console logging, you will also see something similar to
    the following command on the console:'
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  id: totrans-1455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: While retrieving and analyzing logs, which can be redirected to a file, is an
    option and there are multiple systems to collect, search, and analyze logs from
    a set of distributed servers, it is often also important to have a simple way
    to introspect the running code. One way to accomplish this is to create a separate
    template with metrics, however, Scalatra provides metrics and health support to
    enable basic implementations for counts, histograms, rates, and so on.
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
- en: I will use the Scalatra metrics support. The `ScalatraBootstrap` class has to
    implement the `MetricsBootstrap` trait. The `org.scalatra.metrics.MetricsSupport`
    and `org.scalatra.metrics.HealthChecksSupport` traits provide templating similar
    to the Scalate templates, as shown in the following code.
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the content of the `ScalatraTemplate.scala` file:'
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  id: totrans-1459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'The following is the content of the `ServletWithMetrics.scala` file:'
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE182]'
  id: totrans-1461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'If you run the server again, the `http://localhost:8080/admin` page will show
    a set of links for operational information, as shown in the following screenshot:'
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
- en: '![Process monitoring](img/image01763.jpeg)'
  id: totrans-1463
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-2: The admin servlet web page'
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Metrics** link will lead to the metrics servlet depicted in *Figure 10-3*.
    The `org.akozlov.exampes.ServletWithMetrics.counter` will have a global count
    of requests, and `org.akozlov.exampes.ServletWithMetrics.histogram` will show
    the distribution of accumulated values, in this case, the name lengths. More importantly,
    it will compute `50`, `75`, `95`, `98`, `99`, and `99.9` percentiles. The meter
    counter will show rates for the last `1`, `5`, and `15` minutes:'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
- en: '![Process monitoring](img/image01764.jpeg)'
  id: totrans-1466
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-3: The metrics servlet web page'
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, one can write health checks. In this case, I will just check whether
    the result of the response function contains the string that it has been passed
    as a parameter. Refer to the following *Figure 10.4*:'
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
- en: '![Process monitoring](img/image01765.jpeg)'
  id: totrans-1469
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-4: The health check servlet web page.'
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
- en: The metrics can be configured to report to Ganglia or Graphite data collection
    servers or periodically dump information into a log file.
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
- en: 'Endpoints do not have to be read-only. One of the pre-configured components
    is the timer, which measures the time to complete a task—which can be used for
    measuring scoring performance. Let''s put the code in the `ServletWithMetrics`
    class:'
  id: totrans-1472
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  id: totrans-1473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: Accessing `http://localhost:8080/time` will trigger code execution, which will
    be timed with a timer in metrics.
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
- en: Analogously, the put operation, which can be created with the `put()` template,
    can be used to either adjust the run-time parameters or execute the code in-situ—which,
    depending on the code, might need to be secured in production environments.
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**JSR 110**'
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
- en: JSR 110 is another **Java Specification Request** (**JSR**), commonly known
    as **Java Management Extensions** (**JMX**). JSR 110 specifies a number of APIs
    and protocols in order to be able to monitor the JVM executions remotely. A common
    way to access JMX Services is via the `jconsole` command that will connect to
    one of the local processes by default. To connect to a remote host, you need to
    provide the `-Dcom.sun.management.jmxremote.port=portNum` property on the Java
    command line. It is also advisable to enable security (SSL or password-based authentication).
    In practice, other monitoring tools use JMX for monitoring, as well as managing
    the JVM, as JMX allows callbacks to manage the system state.
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
- en: You can provide your own metrics that are exposed via JMX. While Scala runs
    in JVM, the implementation of JMX (via MBeans) is very Java-specific, and it is
    not clear how well the mechanism will play with Scala. JMX Beans can certainly
    be exposed as a servlet in Scala though.
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
- en: The JMX MBeans can usually be examined in JConsole, but we can also expose it
    as `/jmx servlet`, the code provided in the book repository ([https://github.com/alexvk/ml-in-scala](https://github.com/alexvk/ml-in-scala)).
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
- en: Model monitoring
  id: totrans-1481
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered basic system and application metrics. Lately, a new direction
    evolved for using monitoring components to monitor statistical model performance.
    The statistical model performance covers the following:'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
- en: How the model performance evolved over time
  id: totrans-1483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When is the time to retire the model
  id: totrans-1484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model health check
  id: totrans-1485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance over time
  id: totrans-1486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ML models deteriorate with time, or ''age'': While this process is not still
    well understood, the model performance tends to change with time, if even due
    to concept drift, where the definition of the attributes change, or the changes
    in the underlying dependencies. Unfortunately, model performance rarely improves,
    at least in my practice. Thus, it is imperative to keep track of models. One way
    to do this is by monitoring the metrics that the model is intended to optimize,
    as in many cases, we do not have a ready-labeled set of data.'
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, the model performance deterioration is not related directly to
    the quality of the statistical modeling, even though simpler models such as linear
    and logistic regression tend to be more stable than more complex models such as
    decision trees. Schema evolution or unnoticed renaming of attributes may cause
    the model to not perform well.
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
- en: Part of model monitoring should be running the health check, where a model periodically
    scores either a few records or a known scored set of data.
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
- en: Criteria for model retiring
  id: totrans-1490
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A very common case in practical deployments is that data scientists come with
    better sets of models every few weeks. However, if this does not happen, one needs
    come up with a set of criteria to retire a model. As real-world traffic rarely
    comes with the scored data, for example, the data that is already scored, the
    usual way to measure model performance is via a proxy, which is the metric that
    the model is supposed to improve.
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing
  id: totrans-1492
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A/B testing is a specific case of controlled experiment in e-commerce setting.
    A/B testing is usually applied to versions of a web page where we direct completely
    independent subset of users to each of the versions. The dependent variable to
    test is usually the response rate. Unless any specific information is available
    about users, and in many cases, it is not unless a cookie is placed in the computer,
    the split is random. Often the split is based on unique userID, but this is known
    not to work too well across multiple devices. A/B testing is subject to the same
    assumptions the controlled experiments are subject to: the tests should be completely
    independent and the distribution of the dependent variable should be `i.i.d.`.
    Even though it is hard to imagine that all people are truly `i.i.d.`, the A/B
    test has been shown to work for practical problems.'
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
- en: In modeling, we split the traffic to be scored into two or multiple channels
    to be scored by two or multiple models. Further, we need to measure the cumulative
    performance metric for each of the channels together with estimated variance.
    Usually, one of the models is treated as a baseline and is associated with the
    null hypothesis, and for the rest of the models, we run a t-test, comparing the
    ratio of the difference to the standard deviation.
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-1495
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter described system, application, and model monitoring goals together
    with the existing monitoring solutions for Scala, and specifically Scalatra. Many
    metrics overlap with standard OS or Java monitoring, but we also discussed how
    to create application-specific metrics and health checks. We talked about a new
    emerging field of model monitoring in an ML application, where statistical models
    are subject to deterioration, health, and performance monitoring. I also touched
    on monitoring distributed systems, a topic that really deserves much more space,
    which unfortunately, I did not have.
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
- en: This is the end of the book, but in no way is it the end of the journey. I am
    sure, new frameworks and applications are being written as we speak. Scala has
    been a pretty awesome and succinct development tool in my practice, with which
    I've been able to achieve results in hours instead of days, which is the case
    with more traditional tools, but it is yet to win the popular support, which I
    am pretty sure it. We just need to emphasize its advantages in the modern world
    of interactive analysis, complex data, and distributed processing.
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
