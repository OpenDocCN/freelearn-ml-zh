- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Additional AI/ML Tools, Frameworks, and Considerations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的 AI/ML 工具、框架和考虑因素
- en: At this point, we have covered all of the major steps and considerations in
    a typical **machine learning** (**ML**) project. Considering that AI/ML is one
    of the fastest-developing areas of research in the technology industry, new tools,
    methodologies, and frameworks emerge every day.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了典型**机器学习**（**ML**）项目中所有的主要步骤和考虑因素。考虑到人工智能/机器学习是技术行业中发展最快的研究领域之一，每天都有新的工具、方法和框架出现。
- en: In this chapter, we will discuss additional tools and frameworks that are popular
    in the data science industry that we haven’t covered so far. This includes important
    topics such as **BigQuery ML** (**BQML**), various types of hardware that we can
    use for AI/ML workloads, and the use of open source libraries and frameworks such
    as PyTorch, Ray, and Spark MLlib. We will also discuss some tips on how to implement
    large-scale distributed training on Google Cloud.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论数据科学行业中流行的其他工具和框架，这些我们之前还没有涉及。这包括重要主题，如**BigQuery ML**（**BQML**）、我们可以用于
    AI/ML 工作负载的各种硬件，以及使用 PyTorch、Ray 和 Spark MLlib 等开源库和框架。我们还将讨论一些关于如何在 Google Cloud
    上实现大规模分布式训练的技巧。
- en: At the end of this chapter, I will provide some additional context to help transition
    the focus of the remainder of this book to Generative AI. This will include diving
    a bit deeper into some of the commonly used neural network architectures that
    I described at a high level earlier in this book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我将提供一些额外的背景信息，以帮助将本书剩余部分的重点转向生成式人工智能。这包括对我在本书前面以高层次描述的一些常用神经网络架构进行更深入的探讨。
- en: 'For example, in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245), we covered
    the basics of neural networks and introduced common types of neural network architectures,
    such as **convolutional neural networks** (**CNNs**), **recurrent neural networks**
    (**RNNs**), and transformers. In this chapter, we will dive into those use cases
    in more detail to build some foundational knowledge for discussing Generative
    AI in the remaining chapters. Specifically, this chapter includes the following
    main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[*第 9 章*](B18143_09.xhtml#_idTextAnchor245)中，我们介绍了神经网络的基础知识，并介绍了常见的神经网络架构类型，如**卷积神经网络**（**CNNs**）、**循环神经网络**（**RNNs**）和转换器。在本章中，我们将更详细地探讨这些用例，为在剩余章节中讨论生成式人工智能建立一些基础知识。具体来说，本章包括以下主要主题：
- en: Custom Jupyter kernels
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义 Jupyter 内核
- en: BQML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BQML
- en: Hardware considerations for AI/ML workloads
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI/ML 工作负载的硬件考虑
- en: Additional popular open source tools and frameworks – Spark MLlib, Ray, and
    PyTorch on Google Cloud
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他流行的开源工具和框架 - Spark MLlib、Ray 和 PyTorch 在 Google Cloud 上
- en: Large-scale distributed model training
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模分布式模型训练
- en: Transitioning to Generative AI
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转向生成式人工智能
- en: The first topic in the list is also associated with some prerequisite steps
    that we need to cover to set up our environment for the practical activities in
    this chapter. These will be described in the next section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的第一个主题也与一些我们需要覆盖的先决步骤相关，以便为本章中的实际活动设置我们的环境。这些将在下一节中描述。
- en: Prerequisite topics and steps
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先决主题和步骤
- en: This section describes the prerequisite topics and steps for setting up our
    Vertex AI Workbench environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了设置我们的 Vertex AI 工作台环境的先决主题和步骤。
- en: Custom Jupyter kernels and package dependency management
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义 Jupyter 内核和包依赖管理
- en: When we run our code in a Jupyter Notebook (such as a Vertex AI Workbench Notebook),
    the environment in which our code executes is referred to as a kernel. Vertex
    AI Workbench instances come with various kernels already installed for popular
    tools and frameworks such as TensorFlow and PyTorch, which we will cover in more
    depth in this chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 Jupyter Notebook（例如 Vertex AI 工作台笔记本）中运行我们的代码时，我们的代码执行的运行环境被称为内核。Vertex
    AI 工作台实例已经预装了各种内核，用于流行的工具和框架，如 TensorFlow 和 PyTorch，我们将在本章中更深入地介绍这些内容。
- en: However, we can also create custom kernels if we want to define isolated environments
    with specific packages installed. This is a good practice to follow when using
    packages that are in preview mode, for example, as they may have very specific
    dependency requirements. We will use one such library, called `bigframes`, which
    I will describe in detail in this chapter. As a prerequisite, I will outline how
    to create a custom Jupyter kernel and explain some important concepts related
    to that process. Let’s begin with the concept of virtual environments.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想定义具有特定已安装包的隔离环境，我们也可以创建自定义内核。当使用处于预览模式的包时，这是一个很好的做法，例如，因为它们可能有非常具体的依赖要求。我们将使用一个名为`bigframes`的库，我将在本章中详细描述。作为先决条件，我将概述如何创建自定义Jupyter内核，并解释与该过程相关的一些重要概念。让我们从虚拟环境的概念开始。
- en: Virtual environments
  id: totrans-18
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 虚拟环境
- en: 'When our code executes, it runs within an environment, and this environment
    contains all of the dependencies that our code requires, which are usually other
    software packages. Managing the dependencies for various packages can be complicated,
    especially if we have two or more pieces of software that depend on different
    versions of a particular package. For example, imagine the following scenario:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的代码执行时，它在一个环境中运行，这个环境包含我们的代码所需的全部依赖项，这些依赖项通常是其他软件包。管理各种包的依赖项可能很复杂，特别是如果我们有两件或更多软件依赖于特定包的不同版本。例如，想象以下场景：
- en: Software package X depends on version 1.0.3 of software package A
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件包X依赖于软件包A的版本1.0.3
- en: Software package Y depends on version 2.2.1 of software package A
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件包Y依赖于软件包A的版本2.2.1
- en: If we installed software package Y and all of its dependencies in our environment,
    then our environment would contain version 2.2.1 of software package A.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在环境中安装了软件包Y及其所有依赖项，那么我们的环境将包含软件包A的版本2.2.1。
- en: Now, if we try to run software package X in our environment, it may fail because
    it specifically requires a different version (that is, version 1.0.3) of software
    package A to be installed in the environment. This problem is referred to as a
    **dependency conflict**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们尝试在我们的环境中运行软件包X，它可能会失败，因为它特别需要安装不同版本（即版本1.0.3）的软件包A。这个问题被称为**依赖冲突**。
- en: Virtual environments can help us avoid this problem because, as the name suggests,
    they provide a virtual execution environment in which we can run our code. When
    we create a virtual environment, it’s almost like creating a dedicated machine
    on which to execute our code because that environment, and everything in it, is
    isolated from other execution environments, but the isolation is virtual because
    it is simply a logical separation from other environments that can run on the
    same machine.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟环境可以帮助我们避免这个问题，因为正如其名所示，它们提供了一个虚拟执行环境，在其中我们可以运行我们的代码。当我们创建一个虚拟环境时，几乎就像创建了一个专门用于执行代码的机器，因为该环境以及其中的所有内容都与其他执行环境隔离，但这种隔离是虚拟的，因为它只是与其他可以在同一台机器上运行的环境的逻辑分离。
- en: 'When we use Vertex AI Notebook instances, there are two main types of virtual
    environments that we can create:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用Vertex AI笔记本实例时，我们可以创建两种主要的虚拟环境：
- en: '`venv` module, and which are therefore specific to Python packages. This option
    uses `pip` for package management.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`venv`模块，因此它们是特定于Python包的。此选项使用`pip`进行包管理。'
- en: '**Conda environments**, which use the Conda package and environment management
    system that goes beyond Python and can also manage packages for various other
    languages, such as R, Ruby, and others.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Conda环境**，它使用超越Python的Conda包和环境管理系统，可以管理各种其他语言的包，例如R、Ruby等。'
- en: When determining which option to use, bear in mind that Python virtual environments
    are simpler and more lightweight, but Conda offers more features and handles more
    complex scenarios (as a result, Conda environments can be larger and slower to
    set up than Python virtual environments). We will use Conda for our use case.
    I will describe this next.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定要使用哪个选项时，请记住，Python虚拟环境更简单、更轻量级，但Conda提供了更多功能，并处理更复杂的场景（因此，Conda环境可能比Python虚拟环境更大、设置速度更慢）。我们将使用Conda来处理我们的用例。我将在下面描述这一点。
- en: Creating a Conda virtual environment and a custom Jupyter kernel
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建Conda虚拟环境和自定义Jupyter内核
- en: 'Perform the following steps to create the Python virtual environment and custom
    Jupyter kernel that we will use later in this chapter:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以创建我们将在此章后面部分使用的Python虚拟环境和自定义Jupyter内核：
- en: Open JupyterLab on the Vertex AI Notebook instance you created in [*Chapter
    5*](B18143_05.xhtml#_idTextAnchor168).
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您在 [*第 5 章*](B18143_05.xhtml#_idTextAnchor168) 中创建的 Vertex AI Notebook 实例上打开
    JupyterLab。
- en: Select **File** | **New** | **Terminal** and perform the following steps on
    the terminal screen.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **文件** | **新建** | **终端**，并在终端屏幕上执行以下步骤。
- en: 'Create a Conda environment:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 Conda 环境：
- en: '[PRE0]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Activate the environment:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活环境：
- en: '[PRE1]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Install `bigframes` (we could also install any other packages we want, but
    we’ll keep it simple for now):'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `bigframes`（我们也可以安装我们想要的任何其他包，但现在我们将保持简单）：
- en: '[PRE2]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Install JupyterLab in the new environment (this will make our new Conda environment
    available as a kernel in JupyterLab via JupyterLab’s autodiscovery feature):'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新环境中安装 JupyterLab（这将使我们的新 Conda 环境通过 JupyterLab 的自动发现功能作为内核可用）：
- en: '[PRE3]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Change the display name of the kernel (it can take a few minutes for this update
    to appear in the Vertex AI Workbench JupyterLab interface):'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改内核的显示名称（此更新可能需要几分钟才能在 Vertex AI Workbench JupyterLab 界面中显示）：
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, our Conda environment and custom Jupyter kernel are ready to be used in
    the hands-on exercises in this chapter. We just have one more prerequisite step
    to perform before we dive into the remaining topics in this chapter, at that is
    to stage the required files for our serverless Spark MLlib activities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的 Conda 环境和自定义 Jupyter 内核已经准备好用于本章的动手练习。在我们深入本章剩余主题之前，我们只需执行一个先决步骤，那就是为我们的无服务器
    Spark MLlib 活动准备所需的文件。
- en: Staging files for serverless Spark MLlib activities
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为无服务器 Spark MLlib 活动准备文件
- en: 'In this section, we will stage some files in Google Cloud Storage to be used
    in the serverless Spark MLlib activities later in this chapter. To do this, perform
    the following steps:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把一些文件存放在 Google Cloud Storage 中，以便在本章后面的无服务器 Spark MLlib 活动中使用。为此，请执行以下步骤：
- en: 'Go to the Google Cloud console and open Cloud Shell by clicking the **Cloud
    Shell** icon, as shown in *Figure 14*. This icon looks like a “greater than” symbol,
    followed by an underscore – that is, **>_**:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 Google Cloud 控制台，通过点击 **Cloud Shell** 图标打开 Cloud Shell，如图 *图 14* 所示。此图标看起来像是一个“大于”符号，后面跟着一个下划线——即
    **>_**：
- en: '![Figure 14.1: Activating Cloud Shell](img/B18143_14_002.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.1：激活 Cloud Shell](img/B18143_14_002.jpg)'
- en: 'Figure 14.1: Activating Cloud Shell'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1：激活 Cloud Shell
- en: 'Run the following commands in Cloud Shell to download the required files from
    our GitHub repository:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Cloud Shell 中运行以下命令以从我们的 GitHub 仓库下载所需的文件：
- en: '[PRE5]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Run the following commands in Cloud Shell to upload the required files to Google
    Cloud Storage (**important: replace [YOUR-BUCKET-NAME] with the name of a bucket
    you created in** **earlier chapters**):'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Cloud Shell 中运行以下命令以将所需的文件上传到 Google Cloud Storage（**重要：请将 [YOUR-BUCKET-NAME]
    替换为在** **早期章节** **中创建的存储桶名称**）：
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, everything is ready for our hands-on exercises later in this chapter. Next,
    we’ll dive into the important topic of BQML.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，本章后面的动手练习准备工作已经完成。接下来，我们将深入探讨 BQML 的重要主题。
- en: BQML
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BQML
- en: We first introduced BigQuery in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059),
    and we’ve used it in various chapters of this book for its data management and
    processing functionality. However, given the close relationship between large-scale
    data processing and ML, Google Cloud has built ML functionality directly into
    BigQuery, as well as native integrations with Google Cloud Vertex AI. This functionality
    is referred to as BQML, and this section explores this service in detail.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [*第 3 章*](B18143_03.xhtml#_idTextAnchor059) 中首先介绍了 BigQuery，并在本书的各个章节中使用了它进行数据管理和处理。然而，鉴于大规模数据处理与机器学习的紧密关系，Google
    Cloud 已经将机器学习功能直接集成到 BigQuery 中，以及与 Google Cloud Vertex AI 的原生集成。这个功能被称为 BQML，本节将详细介绍这项服务。
- en: BQML enables us to create and execute ML models using standard SQL queries in
    BigQuery. Considering that many companies already store large amounts of data
    in BigQuery, BQML makes it easy for data scientists in those companies to train
    models on large datasets and make predictions directly in the database system,
    without needing to move the data around between different storage systems.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: BQML 允许我们使用 BigQuery 中的标准 SQL 查询创建和执行机器学习模型。考虑到许多公司已经在 BigQuery 中存储了大量的数据，BQML
    使得这些公司的数据科学家能够在大型数据集上训练模型，并在数据库系统中直接进行预测，而无需在不同存储系统之间移动数据。
- en: It supports a variety of ML algorithms, including linear regression, logistic
    regression, k-means clustering, and deep neural networks, as well as forecasting
    use cases based on time series data stored in BigQuery.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 它支持各种 ML 算法，包括线性回归、逻辑回归、k-means 聚类和深度神经网络，以及基于存储在 BigQuery 中的时间序列数据的预测用例。
- en: In addition to training and prediction, we can use BQML to perform many of the
    steps in the model development life cycle, such as feature engineering, model
    performance evaluation, and hyperparameter tuning. Considering that all of this
    can be done using standard SQL, data scientists can easily start using BQML without
    needing to go through a steep learning curve in terms of tooling. This is referred
    to as lowering the barrier of entry, where the barrier of entry represents how
    easy or difficult it is for people to start performing an activity. For instance,
    an activity with a high barrier of entry presents a lot of initial difficulty.
    An example of this would be a system that requires extensive training or complex
    prerequisites, such as provisioning infrastructure, before somebody could start
    using that system. Some technology systems require months of training and effort
    before somebody can start using them effectively. BQML, on the other hand, can
    easily be used by anybody who understands standard SQL syntax, and it does not
    require any complex infrastructure provisioning. Just like other managed services
    provided by Google Cloud, BigQuery (and, by extension, BQML) manages the infrastructure
    for us, and it can automatically scale up and down based on demand.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练和预测，我们还可以使用 BQML 执行模型开发生命周期中的许多步骤，例如特征工程、模型性能评估和超参数调整。考虑到所有这些都可以使用标准 SQL
    完成，数据科学家可以轻松开始使用 BQML，而无需在工具方面经历陡峭的学习曲线。这被称为降低入门门槛，其中入门门槛代表人们开始进行一项活动有多容易或有多难。例如，入门门槛高的活动会带来很多初始困难。一个例子是，在某人能够有效使用该系统之前，需要对其进行大量培训或复杂的先决条件，例如提供基础设施。一些技术系统需要数月培训和努力，某人才能有效使用它们。另一方面，BQML
    可以被任何理解标准 SQL 语法的人轻松使用，并且不需要任何复杂的设施配置。就像 Google Cloud 提供的其他托管服务一样，BigQuery（以及由此扩展的
    BQML）为我们管理基础设施，并且可以根据需求自动扩展和缩减。
- en: With this in mind, let’s take a look at how we can use BQML to develop and use
    ML models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，让我们来看看如何使用 BQML 开发和使用 ML 模型。
- en: Using BQML
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BQML
- en: 'While the Jupyter Notebook that accompanies this chapter provides hands-on
    instructions on how to use BQML for various ML tasks, I will summarize some of
    the main features here. For context, let’s recall our ML model development life
    cycle, as depicted in *Figure 14**.3*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章附带的 Jupyter Notebook 提供了如何使用 BQML 进行各种 ML 任务的动手操作指南，但我将在这里总结一些主要功能。为了了解背景，让我们回顾一下我们的
    ML 模型开发生命周期，如图 *14**.3* 所示：
- en: '![Figure 14.2: ML model development life cycle](img/B18143_14_003.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.2：ML 模型开发生命周期](img/B18143_14_003.jpg)'
- en: 'Figure 14.2: ML model development life cycle'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2：ML 模型开发生命周期
- en: The following subsections dive into each step in more detail.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将更详细地探讨每个步骤。
- en: Data preparation
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'We will combine the following steps shown in *Figure 14**.3* into this section:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把 *图 14**.3* 中显示的以下步骤组合到本节中：
- en: Ingest data
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摄入数据
- en: Store input data
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储输入数据
- en: Explore data
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据
- en: Process/transform data
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理/转换数据
- en: Store processed data
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储处理后的数据
- en: Starting with data ingestion and storage, there are many ways in which we can
    get our data into BigQuery. For example, we can directly upload files via the
    BigQuery console UI or the CLI. For larger datasets, we can stage them in Google
    Cloud Storage and import them to BigQuery from there. We can also use the BigQuery
    Data Transfer Service to automate data movement into BigQuery, either as a one-off
    transfer or on a scheduled cadence. We can stream data into BigQuery by integrating
    with other services, such as Google Cloud Pub/Sub and Dataflow. There are also
    third-party ETL tools that can be used to transfer data to BigQuery.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据摄入和存储开始，我们有多种方式可以将数据导入 BigQuery。例如，我们可以直接通过 BigQuery 控制台 UI 或 CLI 上传文件。对于大型数据集，我们可以将它们存放在
    Google Cloud Storage 中，然后从那里导入到 BigQuery。我们还可以使用 BigQuery 数据传输服务自动将数据移动到 BigQuery，无论是作为一次性传输还是按预定的时间表进行。我们可以通过与其他服务集成，如
    Google Cloud Pub/Sub 和 Dataflow，将数据流式传输到 BigQuery。还有第三方 ETL 工具可以用于将数据传输到 BigQuery。
- en: 'Once we have stored our data in BigQuery, we can use various tools to explore
    and transform the data, in addition to the BigQuery console and standard SQL.
    We introduced and used the `pandas` library in previous chapters, and we know
    that it’s a very widely used library in data science, particularly for data exploration
    and manipulation. For this reason, many data scientists like to use pandas directly
    with their data stored in BigQuery. Fortunately, there are additional libraries
    that make it easy to do this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据存储在BigQuery中，我们就可以使用各种工具来探索和转换数据，除了BigQuery控制台和标准SQL之外。我们在前面的章节中介绍了并使用了`pandas`库，我们知道它是一个在数据科学中非常广泛使用的库，尤其是在数据探索和处理方面。因此，许多数据科学家喜欢直接使用pandas与存储在BigQuery中的数据。幸运的是，有一些额外的库使得这样做变得容易：
- en: The BigQuery DataFrames Python API
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigQuery DataFrame Python API
- en: The `pandas_gbq` library
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas_gbq`库'
- en: Let’s take a look at each of these in more detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些内容。
- en: The BigQuery DataFrames Python API
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BigQuery DataFrame Python API
- en: 'The BigQuery DataFrames Python API enables us to use Python to analyze and
    manipulate data in BigQuery and perform various ML tasks. It’s a relatively new,
    open source option that was launched and is maintained by Google Cloud for using
    DataFrames to interact with BigQuery (at the time of writing this in December
    2023, it is currently in Preview status). We can access it by using the `bigframes`
    Python library, which consists of two main parts:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery DataFrame Python API使我们能够使用Python在BigQuery中分析和处理数据，并执行各种机器学习任务。这是一个相对较新的开源选项，由Google
    Cloud推出并维护，用于使用DataFrame与BigQuery交互（在撰写本文的2023年12月时，它目前处于预览状态）。我们可以通过使用`bigframes`
    Python库来访问它，该库由两个主要部分组成：
- en: '`bigframes.pandas`, which implements a pandas-like API on top of BigQuery'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bigframes.pandas`，它在大查询（BigQuery）之上实现了类似pandas的API'
- en: '`bigframes.ml`, which implements a `scikit-learn`-like API on top of BigQuery
    ML'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bigframes.ml`，它在大查询机器学习（BigQuery ML）之上实现了类似`scikit-learn`的API'
- en: The Jupyter Notebook that accompanies this chapter provides instructions on
    how to use `bigframes.pandas` in more detail. We will dive into those steps shortly,
    but first, I’ll briefly cover `pandas_gbq`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 伴随本章的Jupyter Notebook提供了如何更详细地使用`bigframes.pandas`的说明。我们将在稍后深入这些步骤，但首先，我将简要介绍`pandas_gbq`。
- en: pandas_gbq
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: pandas_gbq
- en: If I were writing this chapter a few months ago, the `pandas_gbq` library would
    have been the main or only option that I would include in this section because,
    at the time of writing, it has been the primary option available for using pandas
    to interact with BigQuery. It’s an open source library that is maintained by PyData
    and volunteer contributors and has been around for quite some time (since 2017),
    so it has become broadly used in the industry.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我几个月前写这一章，`pandas_gbq`库将是本节中包含的主要或唯一选项，因为在撰写本文时，它一直是使用pandas与BigQuery交互的主要选项。这是一个由PyData和志愿者维护的开源库，已经存在一段时间了（自2017年以来），因此它在行业中得到了广泛的应用。
- en: Essentially, it’s a thin wrapper around the BigQuery client library (`google-cloud-bigquery`)
    that provides a simple interface for running SQL queries and uploading pandas
    DataFrames to BigQuery. The results from these queries are parsed into a `pandas.DataFrame`
    object in which the shape and data types are derived from the source table.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，它是对BigQuery客户端库（`google-cloud-bigquery`）的一个薄包装，提供了一个简单的接口来运行SQL查询并将pandas
    DataFrame上传到BigQuery。这些查询的结果被解析成一个`pandas.DataFrame`对象，其形状和数据类型来自源表。
- en: 'The Jupyter Notebook that accompanies this chapter also provides instructions
    on how to use this library in more detail. Now would be a good time to dive into
    those steps. Open JupyterLab on the Vertex AI Workbench instance you created in
    [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168) and perform the following steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 伴随本章的Jupyter Notebook也提供了如何更详细地使用此库的说明。现在是深入研究这些步骤的好时机。打开您在[*第5章*](B18143_05.xhtml#_idTextAnchor168)中创建的Vertex
    AI Workbench实例上的JupyterLab，并执行以下步骤：
- en: In the navigation panel on the left-hand side of the screen, navigate to the
    `Chapter-14` directory within the `Google-Machine-Learning-for-Solutions-Architects`
    folder.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕左侧的导航面板中，导航到`Google-Machine-Learning-for-Solutions-Architects`文件夹内的`Chapter-14`目录。
- en: Double-click on the `pandas-gbq.ipynb` notebook file to open it. When prompted
    to select a kernel, you can use the default **Python 3 (****ipykernel)** kernel.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击`pandas-gbq.ipynb`笔记本文件以打开它。当提示选择内核时，您可以使用默认的**Python 3 (****ipykernel**)内核。
- en: Double-click on the `bigframes.ipynb` notebook file to open it. When prompted
    to select a kernel, you can use the default **Python 3 (bigframes)** kernel that
    we created in the *Prerequisite topics and steps* section of this chapter.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击`bigframes.ipynb`笔记本文件以打开它。当提示选择内核时，您可以使用我们在本章的“先决主题和步骤”部分创建的默认**Python 3
    (bigframes)**内核。
- en: In each of the notebooks you’ve opened, press *Shift* + *Enter* to execute each
    cell.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您打开的每个笔记本中，按*Shift* + *Enter*来执行每个单元格。
- en: The notebooks contain comments and Markdown text that describe what the code
    in each cell is doing.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 笔记本中包含注释和Markdown文本，描述了每个单元格中代码的功能。
- en: As you can see, there are multiple options for interacting with BigQuery data
    in Vertex AI.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在Vertex AI中有多种与BigQuery数据交互的选项。
- en: Given the broad adoption of the `pandas_gbq` library in the industry, and now
    that Google has launched the official BigQuery DataFrames Python API described
    previously, it’s likely that both options will continue to be popular among data
    scientists. A key thing to bear in mind is that `pandas-gbq` downloads data to
    your local environment, whereas the BigQuery DataFrames Python API is used to
    run your data operations on Google Cloud distributed infrastructure.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`pandas_gbq`库在业界的广泛应用，以及谷歌已经推出了之前描述的官方BigQuery DataFrames Python API，因此这两种选项很可能在数据科学家中继续流行。需要记住的关键一点是，`pandas-gbq`将数据下载到您的本地环境，而BigQuery
    DataFrames Python API用于在谷歌云分布式基础设施上运行您的数据操作。
- en: Next, let’s discuss how to create, use, and manage ML models with BQML. The
    Jupyter Notebook file that accompanies this chapter can be used to implement the
    following steps, but let’s discuss them before diving into the notebook file.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论如何使用BQML创建、使用和管理机器学习模型。本章附带的Jupyter Notebook文件可以用来实施以下步骤，但在深入到笔记本文件之前，让我们先讨论它们。
- en: Creating ML models
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建机器学习模型
- en: 'We can use the `CREATE MODEL` statement in BigQuery to define and train a model.
    For example, the following code excerpt creates a linear regression model named
    `my_model_name`, which is trained on the data in `my_dataset.my_table`, using
    the values in `target_colum` as the labels:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用BigQuery中的`CREATE MODEL`语句来定义和训练模型。例如，以下代码片段创建了一个名为`my_model_name`的线性回归模型，该模型在`my_dataset.my_table`中的数据上训练，使用`target_column`中的值作为标签：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we’ve already discussed, BQML supports many types of commonly used algorithms.
    We can also import and export our models from/to Google Cloud Storage to interact
    with other tools and frameworks, and we can even perform hyperparameter tuning
    during the model development process.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经讨论过的，BQML支持许多常用的算法类型。我们还可以从/到Google Cloud Storage导入和导出我们的模型，以与其他工具和框架交互，甚至在模型开发过程中进行超参数调整。
- en: Evaluating ML models
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估机器学习模型
- en: 'Once our model has been trained, we can evaluate its performance using SQL
    code such as the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的模型经过训练，我们可以使用如下SQL代码来评估其性能：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This returns evaluation metrics such as accuracy, precision, recall, and more,
    depending on the model type.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回评估指标，如准确率、精确率、召回率等，具体取决于模型类型。
- en: Generating predictions
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成预测
- en: 'Next, we can use the model to make predictions using SQL code such as the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用以下SQL代码之类的模型来生成预测：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This statement will feed the data from `my_dataset.my_input_table` into our
    trained model to generate predictions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此语句将数据从`my_dataset.my_input_table`输入到我们的训练模型中，以生成预测。
- en: 'Now, we can use the Jupyter Notebook file that accompanies this chapter to
    implement these steps. To do that, open JupyterLab on the Vertex AI Workbench
    instance you created in Chapter 5 and perform the following steps:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用本章附带的Jupyter Notebook文件来实施这些步骤。为此，打开您在第五章中创建的Vertex AI Workbench实例上的JupyterLab，并执行以下步骤：
- en: In the navigation panel on the left-hand side of the screen, navigate to the
    `Chapter-14` directory within the `Google-Machine-Learning-for-Solutions-Architects`
    folder.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕左侧的导航面板中，导航到`Google-Machine-Learning-for-Solutions-Architects`文件夹内的`Chapter-14`目录。
- en: Double-click on the `BQML.ipynb` notebook file to open it. When prompted, select
    the default **Python 3 (****ipykernel)** kernel.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击`BQML.ipynb`笔记本文件以打开它。当提示时，选择默认的**Python 3 (ipykernel)**内核。
- en: Press *Shift* + *Enter* to execute each cell.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按*Shift* + *Enter*来执行每个单元格。
- en: The notebook contains comments and Markdown text that describe what the code
    in each cell is doing.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 笔记本中包含注释和Markdown文本，描述了每个单元格中代码的功能。
- en: Next, we’ll discuss how to set up model monitoring and continuous training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何设置模型监控和持续训练。
- en: Model monitoring and continuous training
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型监控和持续训练
- en: To implement ongoing model monitoring and continuous training, we could set
    up scheduled jobs to execute the evaluation and training queries. This is a solution
    that we would need to architect using additional Google Cloud services, such as
    Google Cloud Scheduler and Google Cloud Functions, in which we could use Google
    Cloud Scheduler to periodically invoke Google Cloud Functions to run the training
    and evaluation queries. Having said that, for complex MLOps pipelines, Vertex
    AI offers more functionality for customization. This point brings us to our next
    topic, which is determining when to use BQML versus other tools for AI/ML use
    cases.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现持续模型监控和持续训练，我们可以设置计划任务来执行评估和训练查询。这是一个我们需要使用额外的 Google Cloud 服务来架构的解决方案，例如
    Google Cloud Scheduler 和 Google Cloud Functions，其中我们可以使用 Google Cloud Scheduler
    定期调用 Google Cloud Functions 来运行训练和评估查询。话虽如此，对于复杂的 MLOps 管道，Vertex AI 提供了更多用于定制的功能。这一点引出了我们的下一个话题，即确定何时使用
    BQML 与其他 AI/ML 用例工具。
- en: When to use BQML versus other tools for AI/ML use cases
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用 BQML 与其他 AI/ML 用例工具
- en: We’ve already covered that BQML is a great fit for data analysts and data scientists
    who want to use familiar SQL syntax to implement ML use cases on data stored in
    BigQuery, and how simplicity is one of its main benefits in that context. Bear
    in mind that there can often be a tradeoff between simplicity and customization.
    If you need to build highly advanced and customized models, then you may find
    that the simplicity of SQL does not provide the same level of customization that
    can be achieved when using specialized AI/ML frameworks such as TensorFlow, PyTorch,
    Ray, and Spark MLlib.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过，BQML 对于希望使用熟悉的 SQL 语法在 BigQuery 中实现 ML 用例的数据分析师和数据科学家来说是一个很好的选择，并且简单性是其在此环境中的主要优势之一。请记住，简单性和定制性之间往往存在权衡。如果您需要构建高度先进和定制的模型，那么您可能会发现
    SQL 的简单性无法提供使用 TensorFlow、PyTorch、Ray 和 Spark MLlib 等专用 AI/ML 框架时所能达到的相同级别的定制性。
- en: 'You can also have “the best of both worlds” by interacting with models hosted
    in Vertex AI from BigQuery by using **BigQuery remote functions**, which provide
    integrations with Cloud Functions and Cloud Run. With this approach, you can write
    code that will be executed on Cloud Functions or Cloud Run, and you can invoke
    that code from a query in BigQuery. The code could send an inference request to
    a model that has been trained and hosted in Vertex AI, and then also send the
    response back to BigQuery. You could even implement transformations in both the
    request and the response, if needed, to ensure compatibility between your query’s
    expected data types and your model’s expected data types. You can find out more
    about BigQuery remote functions in the Google Cloud documentation at the following
    link, which describes known limitations and best practices: [https://cloud.google.com/bigquery/docs/remote-functions](https://cloud.google.com/bigquery/docs/remote-functions).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过与 Vertex AI 中托管模型交互来“兼得两者之优”，使用 **BigQuery 远程函数**，这些函数提供了与 Cloud Functions
    和 Cloud Run 的集成。采用这种方法，您可以编写将在 Cloud Functions 或 Cloud Run 上执行代码，您可以从 BigQuery
    中的查询调用该代码。该代码可以向在 Vertex AI 中训练和托管的模型发送推理请求，并将响应发送回 BigQuery。如果需要，您甚至可以在请求和响应中实现转换，以确保您的查询期望的数据类型与模型期望的数据类型之间的兼容性。您可以在以下链接的
    Google Cloud 文档中了解更多关于 BigQuery 远程函数的信息，该链接描述了已知限制和最佳实践：[https://cloud.google.com/bigquery/docs/remote-functions](https://cloud.google.com/bigquery/docs/remote-functions)。
- en: Note
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While BigQuery is designed for large-scale analytics workloads, Google Cloud
    Spanner is another highly scalable database service in Google Cloud. Spanner is
    designed for distributed and strongly consistent transactional use cases, and
    it also supports SQL syntax. It recently added integrations with Vertex AI, providing
    somewhat similar functionality as BQML (that is, the ability to access ML models
    hosted on Vertex AI through a SQL interface), but intended for transactional rather
    than analytical workloads.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 BigQuery 是为大规模分析工作负载而设计的，但 Google Cloud Spanner 是 Google Cloud 中另一个高度可扩展的数据库服务。Spanner
    是为分布式和强一致性事务性用例而设计的，并且也支持 SQL 语法。它最近增加了与 Vertex AI 的集成，提供与 BQML 类似的功能（即通过 SQL
    接口访问 Vertex AI 上托管的 ML 模型），但旨在用于事务性工作负载，而不是分析性工作负载。
- en: Next, I will end this section with a brief discussion of BigQuery Studio, which
    is a convenient way to manage all of our BigQuery tasks and workloads.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将用简短的讨论结束本节，讨论 BigQuery Studio，这是一个方便管理我们所有 BigQuery 任务和工作负载的方式。
- en: BigQuery Studio
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BigQuery Studio
- en: BigQuery Studio, as the name suggests, provides a single-pane-of-glass experience
    that allows us to perform many different kinds of activities in BigQuery. It includes
    a SQL editor interface that enables us to write and run queries directly in the
    Google Cloud console, with intelligent code development assistance via integration
    with Duet, a Google Cloud Generative AI real-time code assistant that we will
    cover in more detail later in this book. BigQuery Studio also enables us to schedule
    SQL queries to run on a periodic cadence (for example, every day) for use cases
    that require repeated query executions, such as generating a daily report.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，BigQuery Studio 提供了一种单窗口体验，使我们能够在 BigQuery 中执行许多不同类型的活动。它包括一个 SQL 编辑器界面，使我们能够直接在
    Google Cloud 控制台中编写和运行查询，并通过与 Duet 集成获得智能代码开发辅助，Duet 是一个 Google Cloud 生成式 AI 实时代码助手，我们将在本书稍后部分更详细地介绍。BigQuery
    Studio 还使我们能够安排 SQL 查询以定期的时间间隔（例如，每天）运行，用于需要重复查询执行的场景，如生成每日报告。
- en: Via BigQuery Studio, we can access Dataform to define and run data processing
    workflows within BigQuery and utilize BigQuery Studio’s integrations with Dataplex
    for data discovery, data profiling, and data quality management. BigQuery Studio
    also integrates with Colab Enterprise, enabling us to use Jupyter Notebooks directly
    within the BigQuery console.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 BigQuery Studio，我们可以访问 Dataform 来在 BigQuery 中定义和运行数据处理工作流程，并利用 BigQuery Studio
    与 Dataplex 的集成进行数据发现、数据分析和数据质量管理。BigQuery Studio 还与 Colab Enterprise 集成，使我们能够直接在
    BigQuery 控制台中使用 Jupyter Notebook。
- en: I encourage you to visit the BigQuery Studio interface within your Google Cloud
    console and explore its various capabilities and integrations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你访问 Google Cloud 控制台中的 BigQuery Studio 界面，并探索其各种功能和集成。
- en: We will revisit BigQuery in later chapters, but for now, let’s discuss some
    hardware considerations for AI/ML workloads.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中重新讨论 BigQuery，但就目前而言，让我们讨论一些 AI/ML 工作负载的硬件考虑因素。
- en: Hardware considerations for AI/ML workloads
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI/ML 工作负载的硬件考虑因素
- en: The majority of the topics in this book focus on the software and service-level
    functionalities that are available on Google Cloud. Advanced practitioners will
    also be interested in what kind of hardware capabilities exist. If your use cases
    require extreme performance, then selecting the right hardware components on which
    to run your workloads is an important decision. The selection and efficient usage
    of underlying hardware also affect the costs, which are, of course, another important
    factor in your solution architecture. In this section, we’ll shift the discussion
    so that it focuses on some of the hardware considerations for running AI/ML workloads
    in Google Cloud, beginning with an overview of **central processing unit** (**CPU**),
    **graphics processing unit** (**GPU**), and **tensor processing unit** (**TPU**)
    capabilities.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大部分内容都集中在 Google Cloud 上可用的软件和服务级功能。高级实践者也会对存在的硬件能力感兴趣。如果你的用例需要极端性能，那么选择运行工作负载的正确硬件组件是一个重要的决定。底层硬件的选择和高效使用也会影响成本，当然，这也是你解决方案架构中的另一个重要因素。在本节中，我们将转换讨论焦点，重点关注在
    Google Cloud 中运行 AI/ML 工作负载的一些硬件考虑因素，从对**中央处理器**（**CPU**）、**图形处理器**（**GPU**）和**张量处理器**（**TPU**）能力的概述开始。
- en: CPUs, GPUs, and TPUs
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU、GPU 和 TPU
- en: You will probably already be familiar with CPUs and GPUs, but TPUs are more
    specific to Google Cloud. As a brief overview, CPUs are what power the vast majority
    of consumer devices, such as laptops and mobile phones, as well as general-purpose
    servers in data centers. They are effective at multi-tasking across a broad range
    of tasks, but the processing they perform is somewhat sequential.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经熟悉 CPU 和 GPU，但 TPU 更是 Google Cloud 的专有技术。简要概述一下，CPU 是为大多数消费设备提供动力的，例如笔记本电脑和移动电话，以及数据中心中的通用服务器。它们在执行广泛任务的多任务处理中非常有效，但它们执行的处理是某种程度的顺序的。
- en: GPUs, on the other hand, are built with an architecture that optimizes parallel
    (rather than sequential) processing. If you can take a process and break it down
    into similar tasks that can run in parallel, then you’ll be able to complete that
    process much more quickly on a GPU than on a CPU. Although, as the name suggests,
    GPUs were originally designed for handling graphics, which involves processing
    large blocks of pixels and vertices in parallel, it turns out that the kinds of
    matrix manipulation tasks that are inherent to many AI/ML workloads can also be
    sped up by the parallel architecture of GPUs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GPU采用了一种优化并行（而不是顺序）处理的架构。如果你能将一个过程分解成可以并行运行的类似任务，那么你将能够在GPU上比在CPU上更快地完成该过程。尽管，正如其名称所暗示的，GPU最初是为处理图形而设计的，这涉及到并行处理大量像素和顶点，但结果证明，许多AI/ML工作负载固有的矩阵操作任务也可以通过GPU的并行架构来加速。
- en: TPUs were designed by Google to accelerate TensorFlow operations and they are
    optimized for both training and running models more efficiently than CPUs and
    GPUs for some types of workloads. Although they were created specifically for
    TensorFlow, they can now also be used with other frameworks, such as PyTorch,
    by using libraries such as PyTorch/XLA, which is a Python package that uses the
    **Accelerated Linear Algebra** (**XLA**) deep learning compiler to enable PyTorch
    to connect to TPUs and use TPU cores as devices.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud设计的TPU旨在加速TensorFlow操作，并且它们在训练和运行模型方面比CPU和GPU更高效，适用于某些类型的工作负载。尽管它们最初是为TensorFlow而创建的，但现在也可以通过使用如PyTorch/XLA之类的库来与其他框架一起使用，PyTorch/XLA是一个Python包，它使用**加速线性代数**（**XLA**）深度学习编译器，使PyTorch能够连接到TPU并将TPU核心作为设备使用。
- en: Google Cloud provides many different types of hardware servers that we can choose
    for running our ML workloads, and these servers provide various amounts of CPU,
    GPU, and TPU power.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌云提供了许多不同类型的硬件服务器，我们可以选择用于运行我们的ML工作负载，这些服务器提供各种数量的CPU、GPU和TPU功率。
- en: At the time of writing this in December 2023, Google Cloud has recently launched
    its most powerful TPU (**Cloud TPU v5p**) in conjunction with their new **AI Hypercomputer**
    offering, which provides highly optimized resources (that is, storage, compute,
    networking, and more) for AI/ML workloads.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文的2023年12月时，谷歌云最近推出了其最强大的TPU（**Cloud TPU v5p**），并与他们新的**AI超级计算机**产品一起推出，为AI/ML工作负载提供高度优化的资源（即存储、计算、网络等）。
- en: 'Given the broad variety of computing options on Google Cloud, I will not list
    all of them here. Google Cloud is constantly launching additional options, so
    I encourage you to review the Google Cloud documentation to find the latest details:
    [https://cloud.google.com/compute/docs/machine-resource](https://cloud.google.com/compute/docs/machine-resource).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于谷歌云提供了广泛的计算选项，我这里不会列出所有选项。谷歌云不断推出更多选项，因此我鼓励您查阅谷歌云文档以获取最新细节：[https://cloud.google.com/compute/docs/machine-resource](https://cloud.google.com/compute/docs/machine-resource)。
- en: Next, let’s switch the discussion back to the software level and explore some
    popular open source tools and frameworks in the ML and data science industry that
    are supported in Google Cloud.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将讨论切换回软件层面，并探索一些在谷歌云上得到支持的ML和数据科学行业的流行开源工具和框架。
- en: Additional open source tools and frameworks –Spark MLlib, Ray, and PyTorch on
    Google Cloud
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他开源工具和框架——Spark MLlib、Ray和PyTorch在谷歌云上
- en: In this section, I’ll introduce additional open source tools and frameworks,
    such as PyTorch, Ray, and Spark **Machine Learning Library** (**MLlib**), and
    demonstrate how they can be used to implement AI/ML workloads on Google Cloud.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍其他开源工具和框架，例如PyTorch、Ray和Spark **机器学习库**（**MLlib**），并演示如何使用它们在谷歌云上实现AI/ML工作负载。
- en: Spark MLlib
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark MLlib
- en: We introduced Apache Spark in previous chapters and used it for data processing
    to perform feature engineering in [*Chapter 6*](B18143_06.xhtml#_idTextAnchor187).
    Apache Spark MLlib is a component of Apache Spark that provides ML tools and algorithms
    that are optimized for parallel processing with large datasets. In addition to
    feature engineering, we can use the tools in MLlib to implement various stages
    in our ML model development life cycle, such as model training, model evaluation,
    hyperparameter tuning, and prediction, as well as assembling stages into a pipeline
    that can be executed end to end.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中介绍了Apache Spark，并在[*第6章*](B18143_06.xhtml#_idTextAnchor187)中使用它进行数据处理以执行特征工程。Apache
    Spark MLlib是Apache Spark的一个组件，它提供了针对并行处理大数据集优化的ML工具和算法。除了特征工程外，我们还可以使用MLlib中的工具来实现我们ML模型开发生命周期中的各个阶段，例如模型训练、模型评估、超参数调整和预测，以及将这些阶段组装成可以端到端执行的管道。
- en: Just as we discussed in the context of data processing, one of the main advantages
    of Apache Spark (including MLlib) is its ability to execute large-scale computing
    workloads. While libraries such as scikit-learn work well for performing ML workloads
    on single machines, MLlib can distribute the computation work across many machines,
    which enables us to handle larger datasets more efficiently.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在数据处理上下文中讨论的那样，Apache Spark（包括MLlib）的一个主要优势是它执行大规模计算工作负载的能力。虽然像scikit-learn这样的库在单机上执行ML工作负载时表现良好，但MLlib可以将计算工作分布到多台机器上，这使得我们能够更有效地处理更大的数据集。
- en: 'In the hands-on activities that accompany this chapter, we will use Spark MLlib
    to train a model on Google Cloud Dataproc by using the Serverless Spark features
    within Vertex AI. This is important to understand from a solution architecture
    perspective: we will perform the steps in Vertex AI, but it will run the Spark
    job on Dataproc in the background. There are multiple ways in which we can implement
    this workload, and we will cover the following two methods in our hands-on activities:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章伴随的实践活动中，我们将使用Spark MLlib通过Vertex AI中的无服务器Spark功能在Google Cloud Dataproc上训练一个模型。从解决方案架构的角度来看，这一点很重要：我们将在Vertex
    AI中执行步骤，但它将在后台在Dataproc上运行Spark作业。我们可以以多种方式实现这项工作负载，在我们的实践活动中，我们将介绍以下两种方法：
- en: Using an MLOps pipeline, similar to the activities we performed in [*Chapter
    11*](B18143_11.xhtml#_idTextAnchor288).
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与我们在[*第11章*](B18143_11.xhtml#_idTextAnchor288)中执行的活动类似的MLOps管道。
- en: Using the Serverless Spark user interface in Vertex AI.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Vertex AI中的无服务器Spark用户界面。
- en: 'Let’s begin with the first method: using an MLOps pipeline.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一种方法开始：使用MLOps管道。
- en: Serverless Spark via Kubeflow Pipelines on Vertex AI
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过Vertex AI上的Kubeflow Pipelines实现的无服务器Spark
- en: 'In [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288), we used the `DataprocPySparkBatchOp`
    operator in Kubeflow Pipelines to automate the execution of a serverless Spark
    job in an MLOps pipeline. We will use the same operator again in this chapter,
    but this time, we will use it to run a model training and evaluation job using
    Spark MLlib. To do so, open JupyterLab on the Vertex AI Workbench instance you
    created in Chapter 5 and perform the following steps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第11章*](B18143_11.xhtml#_idTextAnchor288)中，我们使用了Kubeflow Pipelines中的`DataprocPySparkBatchOp`操作符来自动化MLOps管道中无服务器Spark作业的执行。在本章中，我们将再次使用该操作符，但这次我们将使用它来运行使用Spark
    MLlib进行模型训练和评估的作业。为此，打开你在第5章中创建的Vertex AI Workbench实例上的JupyterLab，并执行以下步骤：
- en: In the navigation panel on the left-hand side of the screen, navigate to the
    `Chapter-14` directory within the `Google-Machine-Learning-for-Solutions-Architects`
    folder.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕左侧的导航面板中，导航到`Google-Machine-Learning-for-Solutions-Architects`文件夹中的`Chapter-14`目录。
- en: Double-click on the `spark-ml.ipynb` notebook file to open it. When you’re prompted
    to select a kernel, select the **Python 3 (****ipykernel)** kernel.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击`spark-ml.ipynb`笔记本文件以打开它。当你被提示选择内核时，选择**Python 3 (****ipykernel)**内核。
- en: In each of the notebooks you’ve opened, press *Shift* + *Enter* to execute each
    cell.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你打开的每个笔记本中，按*Shift* + *Enter*来执行每个单元格。
- en: The notebooks contain comments and Markdown text that describe what the code
    in each cell is doing.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 笔记本中包含注释和Markdown文本，描述了每个单元格中代码的功能。
- en: As we can see, it’s quite easy to extend the activities we performed in [*Chapter
    11*](B18143_11.xhtml#_idTextAnchor288) to use Kubeflow Pipelines to automate a
    Serverless Spark job for model training purposes with Spark MLlib. Let’s take
    a look at another option for running our Serverless Spark job directly via the
    Serverless Spark user interface in Vertex AI.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，将我们在[*第11章*](B18143_11.xhtml#_idTextAnchor288)中执行的活动扩展到使用Kubeflow Pipelines来自动化用于模型训练的无服务器Spark作业，Spark
    MLlib非常简单。让我们看看运行我们的无服务器Spark作业的另一种选项，即通过Vertex AI的无服务器Spark用户界面直接运行。
- en: The Serverless Spark user interface in Vertex AI
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vertex AI中的无服务器Spark用户界面
- en: 'To access the Serverless Spark user interface in Vertex AI, open JupyterLab
    on the Vertex AI Workbench instance you created in Chapter 5 and perform the following
    steps:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问Vertex AI中的无服务器Spark用户界面，打开你在第5章中创建的Vertex AI工作台实例上的JupyterLab，并执行以下步骤：
- en: From the **File** menu, select **New Launcher**.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**文件**菜单中选择**新建启动器**。
- en: Scroll down to the **Dataproc Jobs and Sessions** section and select **Serverless**.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到**Dataproc作业和会话**部分，并选择**无服务器**。
- en: Select **Create Batch**.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**创建批量**。
- en: 'On the screen that appears (see *Figure 14**.4* for reference), enter the following
    details:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在出现的屏幕上（参见*图14.4*以供参考），输入以下详细信息：
- en: '`pyspark-ml.py` file. It should be in the following format `gs://YOUR-BUCKET-NAME/code/additional-use-cases-chapter/pyspark-ml.py`.'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pyspark-ml.py`文件。它应该具有以下格式`gs://YOUR-BUCKET-NAME/code/additional-use-cases-chapter/pyspark-ml.py`。'
- en: '`--``processed_data_path=gs://YOUR-BUCKET-NAME/data/processed/mlops-titanic`'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`--``processed_data_path=gs://YOUR-BUCKET-NAME/data/processed/mlops-titanic`'
- en: '`--``model_path=gs://YOUR-BUCKET-NAME/models/additional-use-cases-chapter/`'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`--``model_path=gs://YOUR-BUCKET-NAME/models/additional-use-cases-chapter/`'
- en: 'Leave all other fields at their default values and click **Submit** at the
    bottom of the screen:![Figure 14.3: The Serverless Spark user interface in Vertex
    AI](img/B18143_14_004.jpg)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有其他字段保留在默认值，并在屏幕底部点击**提交**：![图14.3：Vertex AI中的无服务器Spark用户界面](img/B18143_14_004.jpg)
- en: 'Figure 14.3: The Serverless Spark user interface in Vertex AI'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：Vertex AI中的无服务器Spark用户界面
- en: After clicking the **Submit** button, a screen will appear showing a list of
    serverless Spark jobs; your newly submitted job will appear at the top of the
    list (you may need to refresh your browser page to update the list). Wait until
    the status of your job says **Succeeded**.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**提交**按钮后，将出现一个屏幕，显示一系列无服务器Spark作业的列表；你新提交的作业将出现在列表的顶部（你可能需要刷新浏览器页面来更新列表）。等待直到作业状态显示为**成功**。
- en: If the job fails for any reason, click on the name of the job to display its
    details, and then click **View Cloud Logs** at the top of the screen.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果作业因任何原因失败，点击作业名称以显示其详细信息，然后点击屏幕顶部的**查看云日志**。
- en: It may take some time for the logs to populate. You can click the **Run query**
    button periodically to refresh the logs.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 日志填充可能需要一些时间。你可以定期点击**运行查询**按钮来刷新日志。
- en: 'When the job has finished executing, the model artifacts will be saved in two
    directories named `metadata` and `stages` at the location you specified for the
    `--model-path` argument. Verify that those directories have been created and populated
    by performing the following steps in the Google Cloud console:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当作业执行完毕后，模型工件将被保存在两个目录中，分别命名为`metadata`和`stages`，位于你为`--model-path`参数指定的位置。通过在Google
    Cloud控制台中执行以下步骤来验证这些目录是否已创建并填充：
- en: Go to the Google Cloud services menu and choose `--model-path` argument by clicking
    on each successive component of the path – for example, `YOUR-BUCKET-NAME/models/additional-use-cases-chapter/`.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往Google Cloud服务菜单，通过点击路径的每个后续组件来选择`--model-path`参数 – 例如，`YOUR-BUCKET-NAME/models/additional-use-cases-chapter/`。
- en: 'Great job! You have just used Spark MLlib to implement a serverless Spark workload
    to train a model on Dataproc via Vertex AI. Next, we’ll briefly discuss another
    distributed computing framework that has become increasingly broadly used in the
    data science industry: Ray.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！你刚刚使用Spark MLlib实现了一个无服务器Spark工作负载，通过Vertex AI在Dataproc上训练模型。接下来，我们将简要讨论另一个在数据科学行业中越来越广泛使用的分布式计算框架：Ray。
- en: Ray
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ray
- en: I won’t spend much time on Ray in this book, but I want to mention it for completeness.
    The developers of Ray describe it as an “*open source unified compute framework
    that makes it easy to scale AI and Python workloads.*” Ray is another type of
    distributed execution framework that has been gaining popularity in recent years,
    especially in AI/ML applications.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我不会花太多时间讨论Ray，但我想为了完整性而提及它。Ray的开发者将其描述为“*一个开源的统一计算框架，使扩展AI和Python工作负载变得容易。*”Ray是另一种近年来越来越受欢迎的分布式执行框架，尤其是在AI/ML应用中。
- en: Like Spark, Ray enables parallelization of code and distribution of tasks across
    a cluster of machines. It also includes components that can help with specific
    model development steps, such as Ray Tune for hyperparameter tuning, and Ray RLlib
    for reinforcement learning. Google Cloud Vertex AI now directly supports Ray by
    enabling us to create Ray clusters.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与Spark类似，Ray能够并行化代码并在机器集群中分配任务。它还包括可以帮助特定模型开发步骤的组件，例如用于超参数调整的Ray Tune和用于强化学习的Ray
    RLlib。Google Cloud Vertex AI现在通过允许我们创建Ray集群来直接支持Ray。
- en: PyTorch
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch
- en: Like TensorFlow and Spark MLlib, PyTorch is an open source framework that includes
    a set of tools and libraries for ML and deep learning use cases. It was originally
    developed by Facebook’s AI Research lab, and it evolved from another AI/ML framework
    named Torch, which is based on a programming language named Lua. PyTorch, as the
    name suggests, has a Pythonic interface, and it has gained popularity in recent
    years for its ease of use and the flexibility provided by its various components,
    such as TorchVision for computer vision, TorchAudio for audio processing, and
    TorchText for natural language processing.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与TensorFlow和Spark MLlib类似，PyTorch是一个开源框架，包括一套用于机器学习和深度学习用例的工具和库。它最初由Facebook的人工智能研究实验室开发，并从另一个名为Torch的AI/ML框架演变而来，该框架基于Lua编程语言。正如其名所示，PyTorch具有Pythonic接口，近年来因其易用性和由其各种组件（如用于计算机视觉的TorchVision、用于音频处理的TorchAudio和用于自然语言处理的TorchText）提供的灵活性而受到欢迎。
- en: PyTorch uses a dynamic (or “imperative”) computational graph, referred to as
    a **define-by-run** approach, where the graph is built on the fly as operations
    are performed, which enables more intuitive and flexible model development compared
    to static graphs used in some other frameworks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch使用动态（或“命令式”）计算图，称为**定义即运行**方法，其中图在执行操作时即时构建，这使得与某些其他框架中使用的静态图相比，模型开发更加直观和灵活。
- en: Note
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: TensorFlow used to only provide the option of using a static computation graph,
    but in recent years, it has introduced an option named “Eager mode,” which offers
    dynamic graph functionality.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow过去只提供使用静态计算图的选择，但近年来，它引入了一个名为“即时模式”的选项，该选项提供了动态图功能。
- en: Also like TensorFlow, PyTorch supports GPU acceleration (via NVIDIA’s CUDA framework)
    for tensor computations, which significantly speeds up model training and inference.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，与TensorFlow一样，PyTorch支持通过NVIDIA的CUDA框架进行张量计算的GPU加速，这显著加快了模型训练和推理的速度。
- en: On the topic of speeding up model training, it’s a common practice in computing
    to get more work done faster by performing operations in parallel across multiple
    devices. In the next section, we will discuss the practice of achieving this goal
    via distributed training.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在加快模型训练的话题上，在计算中，通过在多个设备上并行执行操作以更快地完成更多工作是一种常见的做法。在下一节中，我们将讨论通过分布式训练实现这一目标的做法。
- en: Large-scale distributed model training
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模分布式模型训练
- en: Think back to the discussion we had in the *AI/ML and cloud computing* section
    of [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), in which I described the process
    of scaling our models to larger sizes, starting with small models that we could
    train on our laptop, progressing to larger models trained on a powerful, high-end
    server, and eventually getting to the scale at which a single computer (even the
    most powerful server on the market) couldn’t handle either the size of the model
    or the dataset on which the model is trained. In this section, we’ll look at what
    it means to train such large-scale models in more detail.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 想起我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)的“AI/ML和云计算”部分中进行的讨论，我在其中描述了将我们的模型扩展到更大尺寸的过程，从我们可以在笔记本电脑上训练的小型模型开始，逐步过渡到在强大、高端服务器上训练的大型模型，最终达到单台计算机（即使是市场上最强大的服务器）无法处理模型的大小或训练模型的数据集的规模。在本节中，我们将更详细地探讨训练这样的大型模型意味着什么。
- en: We’ve covered the model training process in great detail throughout this book,
    but I’ll briefly summarize the process here as a knowledge refresher because these
    concepts are important when discussing large-scale distributed model training.
    For this discussion, I will focus on supervised training of neural networks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中详细介绍了模型训练过程，但为了知识复习，我将在下面简要总结这个过程，因为这些概念在讨论大规模分布式模型训练时很重要。对于这次讨论，我将专注于神经网络的监督训练。
- en: 'The supervised training process, at a high level, works as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 监督训练过程在高级别上工作如下：
- en: Instances from our training dataset are fed into the model.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的训练数据集的实例被输入到模型中。
- en: The model algorithm or network processes the training instances, and for each
    instance, it tries to predict the target variable. In the case of neural networks,
    this is referred to as forward propagation.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型算法或网络处理训练实例，并且对于每个实例，它试图预测目标变量。在神经网络的情况下，这被称为前向传播。
- en: After making a prediction, the model calculates the error or loss using a loss
    function (for example, **mean squared error** (**MSE**) for regression or cross-entropy
    for classification), which measures the difference between the model’s prediction
    and the actual label (the ground truth).
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在做出预测后，模型使用损失函数（例如，回归中的**均方误差**（MSE）或分类中的交叉熵）计算错误或损失，该函数衡量模型预测与实际标签（真实值）之间的差异。
- en: The backpropagation process then begins, which applies the chain rule from calculus
    to compute gradients of the loss function concerning each weight in the network.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后开始反向传播过程，它将微积分中的链式法则应用于计算关于网络中每个权重的损失函数的梯度。
- en: An optimization algorithm such as gradient descent makes use of the gradients
    that were calculated during backpropagation to update the weights in the network.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化算法，如梯度下降，利用在反向传播期间计算的梯度来更新网络中的权重。
- en: Training is usually performed by looping through the preceding process multiple
    times in a series of epochs, where an epoch is one complete pass through the entire
    training dataset. However, when using large datasets, the training process can
    divide the overall dataset into smaller batches, and each pass through a batch
    is considered a training step. Also, some frameworks, such as TensorFlow, allow
    us to specify the number of training steps per epoch. Over time, if the model
    is learning effectively, its predictions will become more accurate after each
    epoch, and the loss will decrease (hopefully to an acceptable threshold).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 训练通常通过在一系列的epoch中多次循环执行前面的过程来完成，其中epoch是整个训练数据集的一次完整遍历。然而，当使用大型数据集时，训练过程可以将整体数据集划分为更小的批次，并且每个批次的遍历被视为一个训练步骤。此外，一些框架，如TensorFlow，允许我们指定每个epoch的训练步骤数。随着时间的推移，如果模型学习有效，那么在每个epoch之后，其预测将变得更加准确，损失将减少（希望达到可接受的阈值）。
- en: Note
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The term **processor** in these discussions refers to CPUs, GPUs, and TPUs.
    Also, the concepts we will discuss in this section can apply to training workloads
    that are distributed across multiple processors in a single machine, or across
    multiple processors on multiple machines.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些讨论中，术语**处理器**指的是CPU、GPU和TPU。此外，本节中我们将讨论的概念可以应用于单个机器上多个处理器或多个机器上的多个处理器上分布的训练工作负载。
- en: In distributed training use cases, some communication and data sharing is required
    among the processors. When using a single machine, the processors usually share
    the same system memory (**RAM**), which simplifies data sharing, but when using
    multiple machines, communication needs to happen over a network (including routers,
    switches, and more), which can introduce latency and additional complexity.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式训练用例中，处理器之间需要一些通信和数据共享。当使用单个机器时，处理器通常共享相同的系统内存（**RAM**），这简化了数据共享，但当使用多台机器时，通信需要在网络（包括路由器、交换机等）上发生，这可能会引入延迟和额外的复杂性。
- en: We’ve implemented the training process many times previously in this book. Next,
    we’ll discuss how this works when we’re using multiple processors.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中已经多次实现了训练过程。接下来，我们将讨论当我们使用多个处理器时它是如何工作的。
- en: Data parallelism and model parallelism
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行性和模型并行性
- en: 'There are generally two main reasons we would need to implement distributed
    training workloads:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通常有两个主要原因我们需要实现分布式训练工作负载：
- en: We want to use a very large dataset
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望使用一个非常大的数据集
- en: We want to train a very large model
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望训练一个非常大的模型
- en: Note that both scenarios may exist at the same time (that is, we may need to
    train a very large model on a very large dataset). In this section, I’ll explain
    each of these scenarios in detail, starting with the case of using large datasets.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这两种场景可能同时存在（也就是说，我们可能需要在非常大的数据集上训练一个非常大的模型）。在本节中，我将详细解释这些场景，从使用大型数据集的情况开始。
- en: Data parallelism
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据并行
- en: 'Sometimes, we need to train our models on huge datasets that would take a long
    time to process on just one processor. To make the training process more efficient,
    we can split the dataset into smaller subsets or batches and process them in parallel
    across multiple processors. In this case, we can have a copy of our model running
    on each processor, which works through the subset of data that is loaded onto
    that processor. A simple example would be if we had a dataset that contained 10,000
    data points and we ran 10 processors that each worked on 1,000 data points from
    our dataset. This approach is referred to as **data parallelism**, and it is depicted
    in *Figure 14**.5*:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要在巨大的数据集上训练我们的模型，这些数据集仅在一个处理器上处理需要很长时间。为了使训练过程更高效，我们可以将数据集分成更小的子集或批次，并在多个处理器上并行处理它们。在这种情况下，我们可以在每个处理器上运行我们模型的副本，该副本处理加载到该处理器上的数据子集。一个简单的例子是，如果我们有一个包含
    10,000 个数据点的数据集，并且我们运行了 10 个处理器，每个处理器处理数据集中的 1,000 个数据点。这种方法被称为**数据并行**，并在 *图
    14.5* 中表示：
- en: '![Figure 14.4: Data parallelism](img/B18143_14_005.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.4：数据并行](img/B18143_14_005.jpg)'
- en: 'Figure 14.4: Data parallelism'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：数据并行
- en: In *Figure 14**.5*, each purple box represents a batch of our data, and each
    batch is sent to a separate processor in parallel. Each batch in the diagram contains
    only a few data points, but in reality, the batches would be much larger (this
    diagram intends to illustrate the concept at a high level). Next, we’ll discuss
    the other main scenario in which we may need to implement a distributed training
    workload. This is referred to as model parallelism.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 14.5* 中，每个紫色方块代表我们的数据的一个批次，每个批次都并行发送到单独的处理器。图中的每个批次只包含几个数据点，但在现实中，批次会大得多（此图旨在从高层次上说明概念）。接下来，我们将讨论我们可能需要实现分布式训练工作负载的另一个主要场景。这被称为模型并行。
- en: Model parallelism
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型并行
- en: 'Sometimes, the model itself is so large that it cannot fit in memory on a single
    processor. In these cases, we need to spread the model across multiple processors,
    where each processor handles a different portion or segment of the model. A segment
    could be a layer in the network, or pieces of layers in the case of some very
    complex models. *Figure 14**.6* shows a simple example of model parallelism, in
    which each layer of our neural network runs on a separate processor:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，模型本身可能非常大，以至于无法在一个处理器上放入内存。在这些情况下，我们需要将模型分散到多个处理器上，每个处理器处理模型的不同部分或段。一个段可以是网络中的一层，或者在一些非常复杂的模型中是层的部分。*图
    14.6* 展示了一个简单的模型并行示例，其中我们神经网络中的每一层都在一个单独的处理器上运行：
- en: '![Figure 14.5: Model parallelism](img/B18143_14_006.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.5：模型并行](img/B18143_14_006.jpg)'
- en: 'Figure 14.5: Model parallelism'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：模型并行
- en: There are different methods we can use to break our model down into segments,
    and we generally want to choose an approach that minimizes communication overhead
    between the processors. This is because that part of the process can introduce
    the most latency and complexity, especially when we’re using multiple machines
    where communication needs to happen over a physical network.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的方法将我们的模型分解成段，我们通常希望选择一种方法，以最小化处理器之间的通信开销。这是因为这个过程可能会引入最大的延迟和复杂性，尤其是在我们使用多台机器，通信需要在物理网络上发生时。
- en: In addition to optimizing communication between the processors, we also need
    to design our training procedure in a way that maximizes the utilization of each
    processor. Remember that the outputs from one layer in our neural network become
    inputs to other layers in our network, so there are **sequential dependencies**
    among the layers. When our layers are distributed across different processors,
    some processors could be idle while they wait for inputs to be propagated from
    the previous layer in the network. This, of course, would be an inefficient use
    of our processing resources, and we can use an approach called **pipelining**
    to improve the training efficiency by breaking the input data up into micro-batches.
    In that case, each segment or stage of the model processes its micro-batch and
    passes the outputs to the next stage. Then, while the next stage begins to process
    the outputs, the previous stage can start processing the next micro-batch, and
    so on. In this way, data flows through the network in a more streamlined manner,
    rather than processors for later layers being idle while waiting for an earlier
    layer to process an entire large dataset.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 除了优化处理器之间的通信外，我们还需要以最大化每个处理器利用率的方式设计我们的训练程序。记住，我们神经网络中某一层的输出成为我们网络中其他层的输入，因此层之间存在**顺序依赖性**。当我们的层分布在不同的处理器上时，一些处理器可能会空闲，等待从网络中前一层传播输入。这当然是对我们处理资源的低效使用，我们可以使用一种称为**流水线**的方法，通过将输入数据分成微批来提高训练效率。在这种情况下，每个段或阶段处理其微批，并将输出传递到下一阶段。然后，当下一阶段开始处理输出时，前一阶段可以开始处理下一个微批，依此类推。这样，数据以更流畅的方式在网络中流动，而不是后续层的处理器在等待早期层处理整个大型数据集时处于空闲状态。
- en: So far, I’ve been mainly talking about how to parallelize the training procedure
    in terms of how the models process the inputs (that is, the forward pass in the
    training process). Remember that the training procedure also includes a feedback
    loop to check the outputs of the model, compute the loss against the ground truth,
    then compute the gradients of the loss and update the weights. Next, we will dive
    into how those steps can be implemented in a distributed manner.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我主要在谈论如何从模型处理输入的角度（即在训练过程中的前向传递）来并行化训练过程。记住，训练过程还包括一个反馈循环来检查模型的输出，计算与真实值的损失，然后计算损失的梯度并更新权重。接下来，我们将深入探讨这些步骤如何以分布式的方式进行实现。
- en: Distributed training update process
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式训练更新过程
- en: In the case of model parallelism, data is still processed through the network
    sequentially – that is, each layer (or segment) in the network produces outputs,
    and those outputs become the inputs for the next layer (even though the layers
    or segments are being processed on different processors). Because of this, the
    backpropagation process can also happen sequentially, where gradients are computed
    in each segment starting from the last to the first, similar to how it’s done
    in a non-distributed setting. Each segment independently updates its portion of
    the model weights based on the computed gradients.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型并行的情况下，数据仍然按顺序通过网络进行处理——也就是说，网络中的每一层（或段）都会产生输出，这些输出成为下一层的输入（即使这些层或段在不同的处理器上被处理）。因此，反向传播过程也可以按顺序进行，其中梯度是从最后一个段开始计算到第一个段，类似于在非分布式设置中的做法。每个段独立地根据计算出的梯度更新其模型权重的部分。
- en: In the case of data parallelism, however, subsets of the training data are processed
    in parallel across different processors, so the overall workflow is not sequential.
    Each processor has an identical copy of the model, but each copy works on a different
    subset of the data. For this reason, the individual replicas of the model on each
    processor are unaware of what’s being done on the other processors. Therefore,
    some additional coordination is needed when computing the loss, the gradients,
    and the weights.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在数据并行的情况下，训练数据的子集会在不同的处理器上并行处理，因此整体工作流程不是顺序的。每个处理器都有一个模型的相同副本，但每个副本处理不同的数据子集。因此，每个处理器上的模型副本对其他处理器正在做什么一无所知。因此，在计算损失、梯度和权重时需要额外的协调。
- en: 'We can implement this coordination in various ways, but we should generally
    pick one of the following two options:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过各种方式实现这种协调，但我们应该通常选择以下两种选项之一：
- en: Use a centralized **parameter server** to do the coordination
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集中式的**参数服务器**进行协调
- en: Implement a protocol that allows the individual training servers to act as a
    community (I will refer to this as “collaborative computing”)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个协议，允许单个训练服务器作为一个社区行动（我将将其称为“协作计算”）
- en: Let’s discuss each of these approaches in more detail, starting with the parameter
    server approach.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论这些方法，从参数服务器方法开始。
- en: The parameter server approach
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数服务器方法
- en: In the parameter server approach, one or more nodes in the distributed system
    are designated as parameter servers, and these nodes hold the global model parameters.
    This means that they have a global view of the gradients and parameters for all
    of the model replicas across all nodes in the distributed training system or cluster.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数服务器方法中，分布式系统中的一个或多个节点被指定为参数服务器，这些节点持有全局模型参数。这意味着它们对所有节点在分布式训练系统或集群中的所有模型副本的梯度参数具有全局视图。
- en: 'In this case, the individual training nodes (let’s call them “worker nodes”)
    process different subsets of data and compute their respective gradients, and
    then each worker node sends its gradients to the centralized parameter server(s).
    The parameter server(s) then update the global model parameters based on these
    gradients and send the updated parameters back to the worker nodes. A simplified
    architecture diagram for this approach is shown in *Figure 14**.7*:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，单个训练节点（让我们称它们为“工作节点”）处理不同的数据子集并计算各自的梯度，然后每个工作节点将其梯度发送到集中的参数服务器（们）。然后参数服务器（们）根据这些梯度更新全局模型参数，并将更新的参数发送回工作节点。这种方法的简化架构图如图**14.7**所示：
- en: '![Figure 14.6: The parameter server approach](img/B18143_14_007.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图14.6：参数服务器方法](img/B18143_14_007.jpg)'
- en: 'Figure 14.6: The parameter server approach'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：参数服务器方法
- en: In *Figure 14**.7*, the arrows represent the interchange of gradients and weight
    updates between the workers and the parameters server(s). Dashed lines and borders
    represent that there could be one or more parameter servers because a single parameter
    server could become a bottleneck in the solution. There also needs to be a coordinating
    entity in this architecture to manage all of the resources and their communication.
    This could be co-located in the parameter server or could be implemented as a
    separate server.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图14.7**中，箭头表示工作节点和参数服务器（们）之间梯度和工作更新之间的交换。虚线和边框表示可能存在一个或多个参数服务器，因为单个参数服务器可能会成为解决方案的瓶颈。此外，在这个架构中还需要有一个协调实体来管理所有资源和它们的通信。这可以与参数服务器共址，也可以作为一个单独的服务器实现。
- en: Next, we’ll discuss the other option for implementing model training with data
    parallelism where, instead of using centralized servers for performing the coordination
    steps, all of the workers collaborate directly with each other.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论使用数据并行性实现模型训练的另一种选项，在这种选项中，不是使用集中式服务器来执行协调步骤，而是所有工作节点直接相互协作。
- en: The collaborative computing approach using Ring All-Reduce
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用环全归约的协作计算方法
- en: In the collaborative computing approach, there is no central server or set of
    servers that are dedicated to the purpose of performing coordination among the
    independent worker nodes in the training cluster. Instead, a distributed computing
    protocol is used for coordination among the nodes. This means that the nodes get
    updates (in this case, the computed gradients) from each other. Numerous communication
    protocols exist for coordination across distributed computing systems, and a pretty
    popular one is known as **All-Reduce**. These kinds of protocols are generally
    designed to aggregate data across all nodes in a distributed system and then share
    the result back to all nodes. This means that, even though each node may process
    a different subset of our dataset, and the model instance on each node may therefore
    compute different gradients and weights, each node eventually ends up with an
    aggregated representation of the gradients and weights that is consistent across
    all nodes.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在协作计算方法中，没有中央服务器或一组服务器专门用于在训练集群中的独立工作节点之间执行协调。相反，使用分布式计算协议在节点之间进行协调。这意味着节点从彼此那里获取更新（在这种情况下，计算出的梯度）。存在许多用于分布式计算系统之间协调的通信协议，其中一种相当流行的协议被称为**全归约**。这类协议通常设计用于聚合分布式系统中的所有节点的数据，然后将结果返回给所有节点。这意味着，尽管每个节点可能处理我们数据集的不同子集，并且每个节点上的模型实例因此可能计算不同的梯度和权重，但每个节点最终都会得到一个梯度权重的聚合表示，该表示在所有节点上是一致的。
- en: 'When using All-Reduce, we can connect the processors in various ways, such
    as by implementing a fully connected mesh, as depicted in *14.8*, a ring architecture,
    as depicted in *Figure 14**.9*, or other approaches, such as tree and butterfly
    architectures, which we will not cover here:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用全归约时，我们可以以各种方式连接处理器，例如通过实现完全连接的网格，如图*14.8*所示，环形架构，如图*图14**.9*所示，或其他方法，例如树形和蝴蝶架构，我们在这里不会涉及：
- en: '![Figure 14.7: Fully connected mesh](img/B18143_14_008.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图14.7：完全连接的网格](img/B18143_14_008.jpg)'
- en: 'Figure 14.7: Fully connected mesh'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.7：完全连接的网格
- en: 'As shown in *Figure 14**.8*, the fully connected mesh architecture, in which
    all nodes are connected to all other nodes, would require a lot of complex coordination
    among all of the nodes. For this reason, the simpler and more effective ring architecture
    is often chosen:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图14**.8*所示，完全连接的网格架构，其中所有节点都与其他所有节点相连，将需要所有节点之间进行大量的复杂协调。因此，更简单、更有效的环形架构通常被选择：
- en: '![Figure 14.8: Ring architecture](img/B18143_14_009.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图14.8：环形架构](img/B18143_14_009.jpg)'
- en: 'Figure 14.8: Ring architecture'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8：环形架构
- en: In the ring architecture, as the name implies, the nodes are logically arranged
    in a ring. One of the important aspects of this architecture, referred to as **Ring
    All-Reduce**, is that each node only needs to send its computed gradients to one
    of its neighbors. The neighbor then combines (aggregates) the received gradients
    with its own gradients, passes the aggregated gradients onto the next node in
    the ring, and so on. This aggregation is often as simple as adding the gradients
    together. After *N-1* steps (where *N* is the number of nodes), each node will
    have a copy of the aggregated gradients for all nodes. They can then simply perform
    a division operation to calculate the average gradients, and use those values
    in the optimization step.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在环形架构中，正如其名所示，节点在逻辑上排列成一个环。这个架构的一个重要方面，称为**环形全归约**，是每个节点只需将其计算出的梯度发送给其邻居之一。然后邻居将接收到的梯度与其自己的梯度合并（聚合），将聚合后的梯度传递给环形中的下一个节点，依此类推。这种聚合通常只是简单地将梯度相加。经过*N-1*步（其中*N*是节点数），每个节点都将拥有所有节点的聚合梯度的副本。然后它们可以简单地执行除法运算来计算平均梯度，并在优化步骤中使用这些值。
- en: Note
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To be more specific, the Ring All-Reduce algorithm is usually broken down into
    two phases, referred to as the “reduce-scatter” phase and the “all-gather” phase,
    but that’s a level of detail that’s not necessary for understanding how the process
    works at a high level, as described in this section.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，环形全归约算法通常分为两个阶段，称为“归约-散射”阶段和“全收集”阶段，但这是理解本节所述的高层次过程工作原理时不需要的细节。
- en: At this point, you might be wondering which approach is better – that is, using
    a centralized parameter server or using a collaborative approach such as Ring
    All-Reduce. The answer, as is the case in many solution architecture decisions,
    is. “It depends.” Let’s discuss some additional factors that play into this decision,
    such as whether we want to implement synchronous versus asynchronous data parallelism.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能想知道哪种方法更好——也就是说，使用集中式参数服务器或使用如环形全归约之类的协作方法。答案，正如许多解决方案架构决策的情况一样，是“这取决于。”让我们讨论一些影响这个决策的额外因素，例如我们是否想实现同步与异步数据并行。
- en: Synchronous versus asynchronous data parallelism
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同步与异步数据并行
- en: In the case of synchronous data parallelism, all the nodes need to update their
    parameters at the same time, or at least in a coordinated way. A benefit of this
    approach is that it provides consistency in parameter updates across all of the
    nodes. However, it can result in bottlenecks that slow down the overall training
    process, because all nodes must wait for the slowest one.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在同步数据并行的案例中，所有节点需要同时更新其参数，或者至少以一种协调的方式进行。这种方法的一个好处是，它为所有节点的参数更新提供了一致性。然而，它可能导致瓶颈，从而减慢整体训练过程，因为所有节点必须等待最慢的那个节点。
- en: In the case of asynchronous data parallelism, on the other hand, the nodes update
    their parameters independently without waiting for others, which can lead to faster
    training. However, this can sometimes cause problems with model convergence and
    stability because the nodes can go out of sync if the procedure is not implemented
    correctly.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，在异步数据并行的案例中，节点独立更新其参数，无需等待其他节点，这可以导致训练速度更快。然而，这有时会导致模型收敛和稳定性问题，因为如果程序没有正确实施，节点可能会失去同步。
- en: Asynchronous implementations can also be more fault tolerant because the nodes
    are not dependent on each other, so if a node goes out of service for some reason,
    the rest of the cluster can continue to function, so long as we have set up the
    environment in a fault-tolerant manner.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 异步实现也可以具有更高的容错性，因为节点之间不相互依赖，所以如果某个节点因某种原因停止服务，只要我们以容错方式设置了环境，集群的其余部分可以继续运行。
- en: All-Reduce is generally implemented synchronously because the nodes need to
    share their gradients, whereas the parameter server approach can be implemented
    either synchronously or asynchronously, with asynchronous usually being the more
    common choice.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: All-Reduce 通常以同步方式实现，因为节点需要共享它们的梯度，而参数服务器方法可以同步或异步实现，其中异步通常是更常见的选项。
- en: Next, we’ll learn about how Google Cloud Vertex AI provides an optimized All-Reduce
    mechanism via the Vertex AI Reduction Server.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解 Google Cloud Vertex AI 如何通过 Vertex AI Reduction Server 提供优化的 All-Reduce
    机制。
- en: The Vertex AI Reduction Server
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vertex AI Reduction Server
- en: While the Ring All-Reduce approach is pretty well established and popular in
    the industry, it also has some limitations. For example, in practice, it is found
    that latency tends to scale linearly with the number of workers in the ring (that
    is, the more workers we add, the more latency we get). Also, since each worker
    needs to wait on all other workers in the ring, a single slow worker can slow
    down the ring overall. Additionally, the ring architecture can have a single point
    of failure, whereby if one node fails, it can break the entire ring.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Ring All-Reduce 方法在业界已经相当成熟且受欢迎，但它也有一些局限性。例如，在实践中，发现延迟往往与环中工作节点的数量成线性关系（也就是说，我们添加的工作节点越多，延迟就越大）。此外，由于每个工作节点都需要等待环中所有其他节点，单个慢速节点可能会减慢整个环的速度。此外，环架构可能存在单点故障，如果其中一个节点失败，可能会破坏整个环。
- en: 'For these reasons, Google created a product called the Reduction Server, which
    provides a faster All-Reduce algorithm. In addition to the worker nodes, the Vertex
    AI Reduction Server also provides **reducer nodes**. The workers host and execute
    the model replicas, compute the gradients, and apply the optimization steps, while
    the reducers have a relatively simple job, which is just to aggregate the gradients
    from workers. *Figure 14**.10* shows an example of the Reduction Server architecture:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，谷歌创建了一个名为 Reduction Server 的产品，它提供了一个更快的 All-Reduce 算法。除了工作节点外，Vertex AI
    Reduction Server 还提供了 **reducer 节点**。工作节点托管和执行模型副本，计算梯度并应用优化步骤，而 reducers 执行相对简单的任务，即只是从工作节点中汇总梯度。*图
    14**.10* 展示了 Reduction Server 架构的示例：
- en: '![Figure 14.9: Reduction Server implementation](img/B18143_14_010.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.9：Reduction Server 实现](img/B18143_14_010.jpg)'
- en: 'Figure 14.9: Reduction Server implementation'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：Reduction Server 实现
- en: In *Figure 14**.10*, you’ll notice a similarity to the parameter server architecture,
    but bear in mind that the reducers perform much fewer tasks than parameter servers,
    and this simplicity provides some benefits. For example, the reducers are just
    lightweight instances that can use CPUs, which, in this case, can be significantly
    cheaper than GPUs. Also, unlike the ring architecture, the latency does not scale
    linearly as we add more reducer nodes, and there is no single point of failure
    that can break the overall architecture.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 14**.10* 中，你会注意到它与参数服务器架构的相似之处，但请记住，reducers 执行的任务比参数服务器少得多，这种简单性带来了一些好处。例如，reducers
    只是轻量级的实例，可以使用 CPU，在这种情况下，可能比 GPU 显著便宜。此外，与环架构不同，随着我们添加更多的 reducer 节点，延迟不会线性增长，并且没有单点故障可以破坏整体架构。
- en: Based on the improvements provided by the Vertex AI Reduction Server architecture,
    I recommend using this approach when performing distributed training on Vertex
    AI.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Vertex AI Reduction Server架构提供的改进，我建议在Vertex AI上进行分布式训练时使用这种方法。
- en: Distributed training jobs generally involve using a lot of resources, which
    could incur expenses, so we will not perform a hands-on activity for this topic
    in this chapter. If you would like to learn more about how to implement an ML
    project using distributed training on Vertex AI (optionally, also including the
    Reduction Server), I recommend referencing the documentation at [https://cloud.google.com/vertex-ai/docs/training/distributed-training](https://cloud.google.com/vertex-ai/docs/training/distributed-training).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练作业通常需要使用大量资源，这可能会产生费用，因此我们不会在本章中为此主题进行动手实践。如果您想了解如何在Vertex AI上使用分布式训练实现机器学习项目（可选，包括Reduction
    Server），我建议参考[https://cloud.google.com/vertex-ai/docs/training/distributed-training](https://cloud.google.com/vertex-ai/docs/training/distributed-training)的文档。
- en: Next, we’ll look at some other important factors for distributed training.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨分布式训练的其他一些重要因素。
- en: Other important factors for distributed training
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式训练的其他重要因素
- en: In distributed training, the most complex and error-prone part of the process
    is usually managing communication among the nodes. If this is not implemented
    efficiently, then it can cause bottlenecks and impact training performance. We’ve
    already discussed how we generally want to minimize how much data needs to be
    sent between the nodes, and fortunately, there are some tricks we can use to help
    in this regard, such as compressing the gradients before transmitting them.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式训练中，过程中最复杂且容易出错的部分通常是管理节点间的通信。如果这部分没有高效实现，可能会导致瓶颈并影响训练性能。我们之前已经讨论了如何尽量减少节点间需要传输的数据量，幸运的是，有一些技巧可以帮助我们在这方面，例如在传输之前压缩梯度。
- en: Also, considering that distributed training jobs usually need to process a lot
    of data, they can often run for long periods, even when the workload is being
    parallelized across multiple nodes. For this reason, we should put mechanisms
    in place to help us recover from failures that could occur during the training
    process. For example, if our job has been running for many hours, we wouldn’t
    want to have to start it from the beginning again if something fails before the
    job finishes. A common and important mechanism to implement for this purpose is
    **checkpointing**, in which we regularly save the state of the model during training
    so that the process can continue from a recent state if failure occurs. This is
    just like periodically saving our work while we’re working on a document just
    in case our laptop crashes unexpectedly.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑到分布式训练作业通常需要处理大量数据，它们可能需要运行很长时间，即使在多个节点上并行化工作负载时也是如此。因此，我们应该建立机制来帮助我们从训练过程中可能发生的故障中恢复。例如，如果我们的作业已经运行了几个小时，我们不想在作业完成前发生故障时不得不从头开始。为此目的，一个常见且重要的机制是**检查点**，在训练过程中我们定期保存模型的状态，以便在发生故障时可以从最近的状态继续。这就像在我们处理文档时定期保存我们的工作，以防笔记本电脑意外崩溃。
- en: All of the distributed training concepts we’ve discussed in this section so
    far assume that we have all of the required data stored somewhere in our environment,
    and we simply need to distribute that data (or the models) across multiple nodes.
    However, there’s another popular type of distributed model training referred to
    as **federated learning**, which I’ll briefly describe next.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中讨论的所有分布式训练概念都假设我们已经在我们的环境中某个地方存储了所有所需的数据，我们只需要将数据（或模型）分布到多个节点上。然而，还有一种流行的分布式模型训练类型被称为**联邦学习**，我将在下面简要描述。
- en: Federated learning
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 联邦学习
- en: In previous chapters, we discussed deploying ML models to edge devices such
    as mobile phones or IoT devices, usually to provide low-latency inference on those
    devices. We also discussed how those edge devices generally have much less computing
    power and data storage resources than servers have. Therefore, our ability to
    train models on those devices is limited.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了将机器学习模型部署到边缘设备，如手机或物联网设备，通常是为了在这些设备上提供低延迟的推理。我们也讨论了这些边缘设备通常比服务器拥有更少的计算能力和数据存储资源。因此，我们在这些设备上训练模型的能力是有限的。
- en: Let’s imagine that the models that are deployed to the devices can be improved
    by being updated with data that becomes available to the devices from their local
    environment, such as from sensors. If we want to provide the best possible experience
    for users of those devices, then we will want to ensure that the models are periodically
    updated to account for new data that becomes available. However, in some cases,
    it may be inefficient or otherwise undesirable to constantly send data from the
    edge devices to a central location for larger model training jobs.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，部署到设备上的模型可以通过更新从其本地环境（例如传感器）获得的数据来改进。如果我们想为这些设备的用户提供最佳体验，那么我们希望确保模型定期更新以适应新的数据。然而，在某些情况下，不断从边缘设备向集中位置发送数据以进行大型模型训练工作可能既低效又不可取。
- en: Federated learning is a technique that enables us to periodically update the
    models without needing to send data from those devices back to a centralized location.
    Instead, copies of the model on each device are trained locally by the data that
    becomes available to the device. The model training process on each device goes
    through the familiar steps of calculating the loss and the gradients, and then
    updating the model parameters or weights accordingly. Let’s imagine that this
    kind of training is being performed on millions of devices, where each device
    is making updates to the model based on small pieces of data that become available
    to it. Given the limited training capabilities on each device, this process alone
    will not lead to a lot of model improvements in the long term. However, in the
    case of federated learning, the updated weights can be sent back to a central
    location to be combined to create a more powerful model that can learn from the
    small training processes on millions of devices. This means that all of the weights
    from the millions of devices can be averaged (that is, aggregated) to form a more
    advanced model. This updated model (or an optimized version of it) can then be
    sent out to each device, and the process can be repeated on an ongoing basis to
    keep improving the models over time.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习是一种技术，它使我们能够定期更新模型，而无需将这些设备上的数据发送回集中位置。相反，每个设备上的模型副本通过设备可用的数据进行本地训练。每个设备上的模型训练过程经过熟悉的步骤，包括计算损失和梯度，然后相应地更新模型参数或权重。让我们想象一下，这种训练正在数百万台设备上执行，每台设备根据其可用的少量数据进行模型更新。鉴于每台设备有限的训练能力，这个过程本身在长期内不会导致模型有太多改进。然而，在联邦学习的情况下，更新的权重可以被发送回集中位置以合并，创建一个更强大的模型，该模型可以从数百万台设备上的小型训练过程中学习。这意味着数百万台设备上的所有权重都可以平均（即聚合）形成一个更高级的模型。然后，这个更新后的模型（或其优化版本）可以被发送到每个设备，并且这个过程可以持续进行，以随着时间的推移不断提高模型。
- en: 'The effective thing about this process is that only the model weights need
    to be communicated back to the centralized infrastructure, and none of the actual
    data on the devices ever needs to be transmitted. This is important for two main
    reasons:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的有效之处在于，只需要将模型权重发送回集中式基础设施，而设备上的实际数据根本不需要传输。这有两个主要原因：
- en: The weights are much smaller (in terms of data size) than the training data,
    so sending only the weights is much more efficient than sending training data
    from the devices to the central infrastructure
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重（从数据大小方面来说）远小于训练数据，因此只发送权重比从设备发送训练数据到集中式基础设施要高效得多。
- en: Since only the weights are being transmitted (and no actual user data), this
    helps keep user data safe and preserves privacy
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于只有权重被传输（并且没有实际的用户数据），这有助于保护用户数据的安全并保护隐私。
- en: There are, of course, other methods for implementing distributed model training,
    but we’ve covered many of the more popular ones here.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有其他实现分布式模型训练的方法，但在这里我们已经介绍了很多更受欢迎的方案。
- en: The remainder of this book will focus on Generative AI, so this is a good point
    for us to revisit some important topics that we covered at a high level earlier
    in this book that will help us understand various developments that led to the
    Generative AI era.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 本书剩余部分将专注于生成式AI，因此这是一个很好的时机，让我们回顾一下本书早期以高层次讨论的一些重要主题，这些主题将帮助我们理解导致生成式AI时代到来的各种发展。
- en: Transitioning to Generative AI
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转向生成式AI
- en: 'Before we start diving into Generative AI, I’ll provide additional details
    on some of the important neural network architectures we discussed earlier in
    this book. For example, in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245), we
    briefly introduced some common neural network architectures, such as CNNs, RNNs,
    **long short-term memory** (**LSTM**), and transformers. In this section, we will
    dive deeper into how some of these notable architectures work, for two reasons:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始深入探讨生成式AI之前，我将提供一些关于我们在这本书中早期讨论的一些重要神经网络架构的额外细节。例如，在[*第9章*](B18143_09.xhtml#_idTextAnchor245)中，我们简要介绍了某些常见的神经网络架构，如CNN、RNN、**长短期记忆**（**LSTM**）和转换器。在本节中，我们将深入探讨这些显著架构的工作原理，原因如下：
- en: The practical exercises accompanying this chapter include building a CNN for
    a computer vision use case, so it’s important to describe the inner workings of
    CNNs in more detail
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伴随这一章的实践练习包括构建一个用于计算机视觉用例的CNN，因此详细描述CNN的内部工作原理是很重要的。
- en: The rest of the neural network architectures mentioned here can be seen as milestones
    in the journey toward developing Generative AI technologies
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里提到的其余神经网络架构可以被视为开发生成式AI技术的旅程中的里程碑。
- en: Let’s begin with a deeper dive into CNNs and computer vision.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从对CNN和计算机视觉的深入探讨开始。
- en: CNNs and computer vision
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN和计算机视觉
- en: As we discussed in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245), CNNs are
    often used in computer vision use cases such as object recognition and visual
    categorization, and their trick is to break pictures down into smaller components,
    or “features,” and learn each component separately. In this way, the network learns
    smaller details in the picture and then combines them to identify larger shapes
    or objects hierarchically.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第9章*](B18143_09.xhtml#_idTextAnchor245)中讨论的那样，CNN通常用于计算机视觉用例，如物体识别和视觉分类，它们的技巧是将图片分解成更小的组件，或“特征”，并分别学习每个组件。通过这种方式，网络学习图片中的较小细节，然后将它们组合起来以分层识别更大的形状或对象。
- en: The architecture of CNNs
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN的架构
- en: We could probably write an entire book on CNN architecture, but that level of
    detail is not required in this book. I’ll cover some of the important concepts
    at a high level, and the Jupyter Notebook that accompanies this chapter provides
    code examples on how to build a relatively simple CNN for a computer vision use
    case.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能可以写一本关于CNN架构的整本书，但在这个书中我们不需要那么详细的程度。我将从高层次上介绍一些重要的概念，并且伴随这一章的Jupyter Notebook提供了如何构建一个相对简单的CNN用于计算机视觉用例的代码示例。
- en: 'As a brief refresher of what we discussed in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    the most basic form of neural network, as depicted in *Figure 14**.11*, is referred
    to as a **feed-forward neural network** (**FFNN**), in which the information that
    is being fed into the network follows a simple forward direction as it passes
    through the network:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对[*第9章*](B18143_09.xhtml#_idTextAnchor245)中我们讨论内容的简要回顾，如图*图14.11*所示，最基本形式的神经网络被称为**前馈神经网络**（**FFNN**），其中被输入到网络中的信息随着它通过网络而遵循一个简单的正向方向：
- en: '![Figure 14.10: A simple neural network](img/B18143_14_011.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图14.10：一个简单的神经网络](img/B18143_14_011.jpg)'
- en: 'Figure 14.10: A simple neural network'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.10：一个简单的神经网络
- en: A CNN, on the other hand, adds some specific types of layers into the architecture
    that help with implementing the hierarchical processing we mentioned in the introduction
    to this section. These types of layers will be explained at a high level in the
    following sub-sections.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，CNN在架构中添加了一些特定的层，这些层有助于实现我们在本节引言中提到的分层处理。这些类型的层将在以下子节中从高层次上进行解释。
- en: Convolutional layers
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 卷积层
- en: These layers perform a mathematical operation called **convolution**, which
    involves sliding a filter (or kernel) over the input image to produce a feature
    map. The filter is a small matrix that’s used to detect specific features, such
    as edges, corners, or textures, where each filter is **convolved** across the
    input image, computing the dot product between the filter and input, resulting
    in a feature map.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层执行一种称为**卷积**的数学运算，它涉及将一个滤波器（或核）在输入图像上滑动以生成一个特征图。滤波器是一个用于检测特定特征（如边缘、角或纹理）的小矩阵，每个滤波器在输入图像上**卷积**，计算滤波器和输入之间的点积，从而生成一个特征图。
- en: The output of a convolutional layer, therefore, is a set of feature maps that
    represent specific features that are detected by the filters at different locations
    in the input image.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，卷积层的输出是一组特征图，这些特征图代表了在输入图像的不同位置由过滤器检测到的特定特征。
- en: Pooling layers
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 池化层
- en: Pooling layers are mainly used for dimensionality reduction by downsampling
    the feature maps and reducing the number of parameters. We covered the concept
    of dimensionality reduction in detail in [*Chapter 7*](B18143_07.xhtml#_idTextAnchor215)
    and discussed how it is often necessary to implement dimensionality reduction
    to reduce the amount of computation required during training and serving ML models,
    as well as to reduce the likelihood of overfitting the training data. Different
    pooling approaches can be used, but the most common approach is called “max pooling,”
    which takes the maximum value from a set of pixels in a region (defined by the
    pool size). Another approach, called “average pooling,” simply takes the average
    of the values from a set of pixels in a region.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层主要用于通过下采样特征图和减少参数数量来进行降维。我们在[*第7章*](B18143_07.xhtml#_idTextAnchor215)中详细介绍了降维的概念，并讨论了在训练和部署机器学习模型时，通常有必要实现降维以减少所需的计算量，以及降低过拟合训练数据的可能性。可以使用不同的池化方法，但最常见的方法称为“最大池化”，它从一个区域（由池大小定义）的像素集中取最大值。另一种方法，称为“平均池化”，简单地取一个区域内像素值的平均值。
- en: Fully connected layers
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全连接层
- en: Combining the convolutional and pooling layers results in the features being
    extracted from the input images. After the input data is passed through the convolutional
    and pooling layers, the high-level reasoning in the network is then performed
    by using fully connected layers. These are the kinds of layers we covered when
    we discussed FFNNs, where the neurons in each layer have connections to all activations
    in the previous layer. Just as in the case of FFNNs, the fully connected layers
    are often used for the final classification or regression task that the network
    is performing.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 将卷积层和池化层结合使用，可以得到从输入图像中提取的特征。在输入数据通过卷积层和池化层后，网络中的高级推理是通过使用全连接层来完成的。这些就是我们讨论前馈神经网络（FFNNs）时提到的层，其中每一层的神经元都与前一层的所有激活连接。正如FFNNs的情况一样，全连接层通常用于网络执行的最终分类或回归任务。
- en: The final output layer often uses a softmax activation function to output a
    probability distribution over the classes that the model is trying to identify.
    Alternatively, for binary classification, a sigmoid function could be used.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出层通常使用softmax激活函数来输出模型试图识别的类别的概率分布。对于二分类，也可以使用sigmoid函数。
- en: Residual networks
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络
- en: Residual networks, or ResNets, are a type of CNN that was developed to address
    the problem of vanishing and exploding gradients, something we discussed in [*Chapter
    9*](B18143_09.xhtml#_idTextAnchor245). They do this by introducing “skip connections,”
    which are shortcuts that “skip” over layers within the network, and directly connect
    the output of one layer to the input of a later layer.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络，或称ResNets，是一种为了解决梯度消失和梯度爆炸问题而开发的CNN，我们曾在[*第9章*](B18143_09.xhtml#_idTextAnchor245)中讨论过这个问题。它们通过引入“跳过连接”来实现这一点，这些跳过连接“跳过”网络中的某些层，并将一层输出直接连接到后续层的输入。
- en: That’s about as much detail as we’re going to cover regarding the inner workings
    of CNNs. Many research papers go into great detail on how CNNs work, but that’s
    a level of detail beyond the scope of this book. Next, we’ll dive in and work
    on some code examples to build our very first CNN for a computer vision use case!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 关于CNN内部工作原理的细节，我们就介绍到这里。许多研究论文都详细介绍了CNN的工作原理，但这超出了本书的范围。接下来，我们将深入探讨，并通过一些代码示例来构建我们第一个用于计算机视觉用例的CNN！
- en: In the Jupyter Notebooks that accompany this chapter, we will use Keras to build
    a simple CNN. This is where you will also start learning how to use PyTorch, which
    we’ll then use to perform the same task.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章附带的Jupyter Notebooks中，我们将使用Keras构建一个简单的CNN。这也是你开始学习如何使用PyTorch的地方，然后我们将使用PyTorch来完成相同的任务。
- en: Building a CNN
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建卷积神经网络（CNN）
- en: 'To build our CNNs, open JupyterLab on the Vertex AI Workbench instance you
    created in Chapter 5 and perform the following steps:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建我们的CNN，请在第5章中创建的Vertex AI Workbench实例上打开JupyterLab，并执行以下步骤：
- en: In the navigation panel on the left-hand side of the screen, navigate to the
    `Chapter-14` directory within the `Google-Machine-Learning-for-Solutions-Architects`
    folder.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕左侧的导航面板中，导航到`Google-Machine-Learning-for-Solutions-Architects`文件夹中的`Chapter-14`目录。
- en: Double-click on the `Keras-CV.ipynb` notebook file to open it. When you’re prompted
    to select a kernel, select the **TensorFlow** kernel.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击`Keras-CV.ipynb`笔记本文件以打开它。当提示选择内核时，选择**TensorFlow**内核。
- en: Double-click on the `PyTorch-CV.ipynb` notebook file to open it. When you’re
    prompted to select a kernel, select **PyTorch** kernel.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击`PyTorch-CV.ipynb`笔记本文件以打开它。当提示选择内核时，选择**PyTorch**内核。
- en: In each of the notebooks you’ve opened, press *Shift* + *Enter* to execute each
    cell.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你打开的每个笔记本中，按*Shift* + *Enter*执行每个单元格。
- en: The notebooks contain comments and Markdown text that describe what the code
    in each cell is doing.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 笔记本中包含注释和Markdown文本，描述了每个单元格中的代码所执行的操作。
- en: Take a moment to savor the fact that you have just used both Keras and PyTorch
    to build CNNs for a computer vision use case on Google Cloud Vertex AI. This is
    some pretty advanced stuff!
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 请花点时间品味一下这个事实：你刚刚已经使用了Keras和PyTorch在Google Cloud Vertex AI上为计算机视觉用例构建了CNN。这是一些相当高级的内容！
- en: Next, we’ll take a look at some types of neural network architectures that are
    typically useful for sequential data and use cases such as **natural language**
    **processing** (**NLP**).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨一些通常用于序列数据和用例（如**自然语言处理**（NLP））的神经网络架构类型。
- en: RNNs, LSTMs, and transformers
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNNs，LSTMs和transformers
- en: Again, we already introduced these concepts at a high level in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245).
    In this section, we will dive a little bit deeper into these types of architectures.
    We won’t build these networks in this chapter, nor go into excessive detail; I’m
    covering these topics here mainly for historical context to pave the way for concepts
    and activities that I’ll introduce in the Generative AI chapters in this book.
    This is because all of the neural network architectures in this section can be
    seen as stepping stones toward the development of the Generative AI models we
    use today, in the order shown in *Table 14.1:*
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们已经在[*第9章*](B18143_09.xhtml#_idTextAnchor245)中从高层次介绍了这些概念。在本节中，我们将对这些类型的架构进行更深入的探讨。我们不会在本章中构建这些网络，也不会过于详细地介绍；我在这里主要介绍这些主题，主要是为了历史背景，为我在本书生成式AI章节中将要介绍的概念和活动铺路。这是因为本节中所有的神经网络架构都可以看作是我们今天使用的生成式AI模型发展的垫脚石，顺序如*表14.1:*所示：
- en: '| **Development** | **Timeframe** | **References** |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| **开发** | **时间框架** | **参考文献** |'
- en: '| RNN | 1990 | Elman, J. L. (1990). *Finding structure in time*. Cognitive
    Science, 14(2), 179-211.https://doi.org/10.1207/s15516709cog1402_1. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 1990 | Elman, J. L. (1990). *Finding structure in time*. Cognitive
    Science, 14(2), 179-211.https://doi.org/10.1207/s15516709cog1402_1. |'
- en: '| LSTM | 1997 | Hochreiter, Sepp & Schmidhuber, Jürgen. (1997). *Long Short-term
    Memory*. Neural computation. 9\. 1735-80\. 10.1162/neco.1997.9.8.1735.https://doi.org/10.1162/neco.1997.9.8.1735.
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 1997 | Hochreiter, Sepp & Schmidhuber, Jürgen. (1997). *Long Short-term
    Memory*. Neural computation. 9\. 1735-80\. 10.1162/neco.1997.9.8.1735.https://doi.org/10.1162/neco.1997.9.8.1735.
    |'
- en: '| Transformer | 2017 | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. *Attention Is
    All You Need*. [2017, June 12]. arXiv preprint arXiv:1706.03762\. http://arxiv.org/abs/1706.03762.
    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Transformer | 2017 | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. *Attention Is
    All You Need*. [2017, June 12]. arXiv preprint arXiv:1706.03762\. http://arxiv.org/abs/1706.03762.
    |'
- en: 'Table 14.1: The stepping stones in sequential model development'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 表14.1：序列模型开发中的垫脚石
- en: Let’s begin by diving into RNNs.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从深入研究RNNs开始。
- en: RNNs
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNNs
- en: 'As you read each word in this sentence, you need to remember the words that
    came before it so that you can build an understanding of what the overall sentence
    is saying. This is an application of **natural language understanding** (**NLU**),
    which is a sub-field of NLP. In a more generic sense, the sentence represents
    sequential data because the data points (in this case, words) in the sentence
    relate to other words in the sentence and the order in which they are processed
    matters. *Figure 14**.12* shows an example of sequence data being fed into a neural
    network:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读这个句子中的每个单词时，你需要记住之前的单词，以便你可以理解整个句子的含义。这是**自然语言理解**（NLU）的应用，它是NLP的一个子领域。在更通用的意义上，这个句子代表了序列数据，因为句子中的数据点（在这种情况下，是单词）与句子中的其他单词相关，并且它们被处理的顺序很重要。*图14.12*展示了序列数据被输入到神经网络中的例子：
- en: '![Figure 14.11: Sequence data being fed into a neural network](img/B18143_14_012.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图14.11：序列数据被输入到神经网络中](img/B18143_14_012.jpg)'
- en: 'Figure 14.11: Sequence data being fed into a neural network'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：序列数据被输入到神经网络中
- en: In *Figure 14**.12*, we can see that data point 1 will be processed first by
    the neural network, and then data point 2 will be processed, and so on, where
    each data point is processed one at a time by the network.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图14.12**中，我们可以看到数据点1将首先被神经网络处理，然后是数据点2，依此类推，其中每个数据点都是一次由网络处理的。
- en: In simple FFNNs, each data point is passed independently through the network,
    so the network does not associate data points with previous data points. As we
    briefly mentioned in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245), RNNs introduce
    a type of looping mechanism into the network architecture, and this looping mechanism
    is what enables the network to “remember” previous data points. Let’s look at
    this in a bit more detail to understand what this means.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的FFNNs（前馈神经网络）中，每个数据点独立地通过网络传递，因此网络不会将数据点与之前的数据点关联起来。正如我们在[*第9章*](B18143_09.xhtml#_idTextAnchor245)中简要提到的，RNNs将一种循环机制引入网络架构中，这种循环机制使得网络能够“记住”之前的数据点。让我们更详细地看看这意味着什么。
- en: In RNNs, the concept of each data point being processed by the network is referred
    to as a **time step**. The important thing to note is that, with RNNs, the outputs
    from neurons in each time step can be saved for future reference. Let’s call this
    the **current state** of our network; this is often referred to as the **hidden
    state** of the network because it is generally not exposed externally.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNNs（循环神经网络）中，每个数据点被网络处理的概念被称为**时间步**。需要注意的是，在RNNs中，每个时间步中神经元的输出可以被保存以供将来参考。我们称这个为网络的**当前状态**；这通常被称为网络的**隐藏状态**，因为它通常不会对外暴露。
- en: 'At each time step, neurons in the network receive two sets of inputs:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，网络中的神经元接收两组输入：
- en: The output from neurons in the previous layer of the network (or inputs from
    the input layer if we’re referring to the first hidden layer in the network).
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络前一层神经元的输出（或者如果我们指的是网络中的第一个隐藏层，则是输入层的输入）。
- en: The output from the same neuron (that is, itself) that was saved from the previous
    time step. This is the hidden state.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从上一个时间步保存的相同神经元（即自身）的输出。这是隐藏状态。
- en: Looking at our diagram in *Figure 14**.12*, in the first time step, when the
    first data point in the sequence (that is, data point 1) passes through the network,
    the process will be much like a standard FFNN, as we described in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    because there is no previous state to remember.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 观察我们的*图14.12*中的图，在第一个时间步，当序列中的第一个数据点（即数据点1）通过网络时，过程将非常类似于我们在[*第9章*](B18143_09.xhtml#_idTextAnchor245)中描述的标准FFNN，因为没有之前的状态需要记住。
- en: However, when data point 2 is being processed through the network, each neuron
    can combine this new data with the “current state” of the network (which is the
    state that was created after processing data point 1). This process repeats for
    each new data point that passes through the network, and by doing this, the network
    maintains a kind of memory of the data points it processed previously.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当数据点2通过网络处理时，每个神经元都可以将这个新数据与网络的“当前状态”结合起来（即处理数据点1后创建的状态）。这个过程对每个通过网络的新数据点重复进行，通过这样做，网络保持了对之前处理的数据点的某种记忆。
- en: Backpropagation through time (BPTT)
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 时间反向传播（BPTT）
- en: We discussed the backpropagation process in neural network learning in [*Chapter
    9*](B18143_09.xhtml#_idTextAnchor245). Considering that RNNs use the concept of
    time steps and combine the outputs of neurons over time, the backpropagation process
    in RNNs needs to be modified accordingly. This modified version of backpropagation
    is called **BPTT**.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: It involves “unrolling” the RNN in time, meaning that the time steps are treated
    as separate layers of a deep neural network. Once the network has been unrolled,
    the standard backpropagation algorithm is applied – that is, the network’s error
    is calculated at each time step, the gradients are computed, and then these gradients
    are used to update the weights in the network.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of RNNs – gradients and short-term memory
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In an RNN, the unrolling that is performed as part of the BPTT process can lead
    to very deep networks for long sequences. Remember that vanishing and exploding
    gradients happen because of the chain rule in calculus. As the gradients are backpropagated
    through the deep networks associated with long sequences, this can make the problem
    of vanishing and exploding gradients more pronounced.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: This also affects the “memory” of the RNN. As sequences get longer, the ability
    for backpropagation to update the gradients of earlier time steps gets smaller
    and smaller (in the case of vanishing gradients). This has the effect of making
    the network “forget” earlier time steps in longer sequences, which means that
    RNNs are generally limited to short sequences.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: The vanishing and exploding gradient problems and short-term memory limitations
    have led to the development of more advanced RNN architectures such as LSTMs,
    which can address these issues. We’ll discuss this in more detail next.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LSTM is a type of RNN that was designed specifically to overcome the limitations
    of traditional RNNs, especially the vanishing and exploding gradient problems
    when processing long sequences, as we discussed in the previous section.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the hidden state mechanism we described in the *RNNs* section,
    LSTMs add a concept called **cell state**. This is often likened to a kind of
    conveyor belt that runs straight through the entire chain of an LSTM network,
    which allows information to be easily transported across many steps in the process.
    The hidden state still acts as the network’s short-term memory, and the cell state
    acts as a longer-term memory.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs also introduce the concepts of **gates**, which can be seen as checkpoints
    along the conveyor belt that decide what information should be maintained, dropped,
    or added. In this way, the gates regulate the flow of information through the
    network. There are generally three types of gates:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '**Input gates**, which update the cell state with new information'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output gates**, which use the cell state to determine what the next hidden
    state should be'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forget gates**, which decide what information to discard from the cell state'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’d like to learn more about how these gates are used in combination with
    the hidden state and the cell state, I recommend reading the original paper (Hochreiter
    and Schmidhuber, 1997) at [https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory](https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of gates, a variation on LSTMs called **gated recurrent unit** (**GRU**)
    was more recently introduced (Cho et al., 2014). It combines the forget and input
    gates into a single **update gate** and merges the cell state and hidden state,
    which results in a simpler and more efficient model.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll discuss one of the most important breakthroughs in the AI/ML industry
    in recent years: the transformer architecture.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As is the case in most fields of research, the pace of advancement generally
    proceeds at a somewhat linear rate, with occasional transformative breakthroughs
    along the timeline. The linear advancement in any field represents the step-by-step
    progress that occurs on an ongoing basis, and the transformative breakthroughs
    represent the sudden, giant leaps forward that completely change everything. When
    Google invented the transformer architecture (Vaswani, A., et al., 2017), this
    was one such giant leap forward in AI/ML research, and it has formed the basis
    of the Generative AI revolution that we are currently experiencing in the world.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: The progression from RNNs to LSTMs is what I would consider to be a linear development.
    However, the progression from LSTMs to the transformer architecture was an enormous
    jump forward in the development of AI/ML technologies.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, unlike RNNs and LSTMs, which process sequences step by step, transformers
    can handle entire sequences simultaneously (in parallel), and this parallel processing
    allows them to manage long-range dependencies in our data much more efficiently.
    This also allows them to achieve larger scales than previous model architectures,
    resulting in what are now the largest and most powerful language models in the
    industry. However, parallel processing is not the most significant feature of
    the transformer architecture. As suggested in the title of the paper that first
    introduced the transformer architecture to the world – *Attention Is All You Need*
    (arXiv:1706.03762 [cs.CL]) – the main innovation in transformers is the self-attention
    mechanism, which allows the model to weigh the importance of different parts of
    the input data. For example, if we feed a sentence (or sequence) into a transformer
    model, the self-attention mechanism allows the model to understand the entire
    sentence and its internal relationships (for example, the relationships between
    the words).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Transformers run multiple self-attention **heads** in parallel on the sequence,
    something that’s referred to as **multi-head attention** in the paper. Hopefully,
    you already read through the paper when we linked it in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    but if not, I highly recommend reading the original paper to dive into the intricate
    details of how transformers work, given their pivotal importance in the AI/ML
    and Generative AI industry. Transformers are the basis of the current state-of-the-art
    Generative AI models.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve recapped the evolutionary steps that led to their development,
    we are ready to move on to the next chapter. But first, let’s take a moment to
    reflect on what we’ve learned.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered important AI/ML concepts that are typically used
    by more advanced practitioners who have specific needs or preferences in terms
    of how they want to implement their ML workloads, such as by using specific frameworks,
    or by parallelizing their workloads across multiple processors.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: We started by discussing BQML, focusing on the close relationship between data
    processing, data analytics, and ML. We learned how to train and evaluate a model
    using BQML, and how to get predictions from that model, all by using SQL query
    syntax.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Then, we discussed different types of hardware that we can use for our AI/ML
    workloads, and popular tools and frameworks that we had not previously covered
    in this book, such as Spark MLlib, Ray, and PyTorch.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Next, we dived into CNNs and their use in computer vision before moving on to
    discuss neural network architectures that are particularly useful for sequential
    data and use cases such as NLP, such as RNNs, LSTMs, and transformers. We dived
    into the inner workings of these architectures, mainly to understand the evolutionary
    steps that have led to the development of the Generative AI technologies that
    we will outline in the remaining chapters of this book.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed distributed model training in AI/ML, including why it’s
    needed, and some of the mechanisms we can use to implement it, such as data parallelism
    and model parallelism. We also dived into different approaches for implementing
    data parallelism, such as using centralized parameter servers, or distributed
    coordination mechanisms such as Ring All-Reduce. We also explored the topic of
    federated learning, and how it can help to preserve privacy by avoiding the process
    of transmitting data from devices.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will embark on an exciting journey as we dive into the world of Generative
    AI. Join me in the next chapter to get started!
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
