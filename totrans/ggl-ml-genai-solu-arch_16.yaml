- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Additional AI/ML Tools, Frameworks, and Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have covered all of the major steps and considerations in
    a typical **machine learning** (**ML**) project. Considering that AI/ML is one
    of the fastest-developing areas of research in the technology industry, new tools,
    methodologies, and frameworks emerge every day.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss additional tools and frameworks that are popular
    in the data science industry that we haven’t covered so far. This includes important
    topics such as **BigQuery ML** (**BQML**), various types of hardware that we can
    use for AI/ML workloads, and the use of open source libraries and frameworks such
    as PyTorch, Ray, and Spark MLlib. We will also discuss some tips on how to implement
    large-scale distributed training on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, I will provide some additional context to help transition
    the focus of the remainder of this book to Generative AI. This will include diving
    a bit deeper into some of the commonly used neural network architectures that
    I described at a high level earlier in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245), we covered
    the basics of neural networks and introduced common types of neural network architectures,
    such as **convolutional neural networks** (**CNNs**), **recurrent neural networks**
    (**RNNs**), and transformers. In this chapter, we will dive into those use cases
    in more detail to build some foundational knowledge for discussing Generative
    AI in the remaining chapters. Specifically, this chapter includes the following
    main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Custom Jupyter kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BQML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware considerations for AI/ML workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional popular open source tools and frameworks – Spark MLlib, Ray, and
    PyTorch on Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale distributed model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transitioning to Generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first topic in the list is also associated with some prerequisite steps
    that we need to cover to set up our environment for the practical activities in
    this chapter. These will be described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisite topics and steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes the prerequisite topics and steps for setting up our
    Vertex AI Workbench environment.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Jupyter kernels and package dependency management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we run our code in a Jupyter Notebook (such as a Vertex AI Workbench Notebook),
    the environment in which our code executes is referred to as a kernel. Vertex
    AI Workbench instances come with various kernels already installed for popular
    tools and frameworks such as TensorFlow and PyTorch, which we will cover in more
    depth in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can also create custom kernels if we want to define isolated environments
    with specific packages installed. This is a good practice to follow when using
    packages that are in preview mode, for example, as they may have very specific
    dependency requirements. We will use one such library, called `bigframes`, which
    I will describe in detail in this chapter. As a prerequisite, I will outline how
    to create a custom Jupyter kernel and explain some important concepts related
    to that process. Let’s begin with the concept of virtual environments.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual environments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When our code executes, it runs within an environment, and this environment
    contains all of the dependencies that our code requires, which are usually other
    software packages. Managing the dependencies for various packages can be complicated,
    especially if we have two or more pieces of software that depend on different
    versions of a particular package. For example, imagine the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: Software package X depends on version 1.0.3 of software package A
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software package Y depends on version 2.2.1 of software package A
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we installed software package Y and all of its dependencies in our environment,
    then our environment would contain version 2.2.1 of software package A.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we try to run software package X in our environment, it may fail because
    it specifically requires a different version (that is, version 1.0.3) of software
    package A to be installed in the environment. This problem is referred to as a
    **dependency conflict**.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual environments can help us avoid this problem because, as the name suggests,
    they provide a virtual execution environment in which we can run our code. When
    we create a virtual environment, it’s almost like creating a dedicated machine
    on which to execute our code because that environment, and everything in it, is
    isolated from other execution environments, but the isolation is virtual because
    it is simply a logical separation from other environments that can run on the
    same machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use Vertex AI Notebook instances, there are two main types of virtual
    environments that we can create:'
  prefs: []
  type: TYPE_NORMAL
- en: '`venv` module, and which are therefore specific to Python packages. This option
    uses `pip` for package management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conda environments**, which use the Conda package and environment management
    system that goes beyond Python and can also manage packages for various other
    languages, such as R, Ruby, and others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When determining which option to use, bear in mind that Python virtual environments
    are simpler and more lightweight, but Conda offers more features and handles more
    complex scenarios (as a result, Conda environments can be larger and slower to
    set up than Python virtual environments). We will use Conda for our use case.
    I will describe this next.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Conda virtual environment and a custom Jupyter kernel
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Perform the following steps to create the Python virtual environment and custom
    Jupyter kernel that we will use later in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Open JupyterLab on the Vertex AI Notebook instance you created in [*Chapter
    5*](B18143_05.xhtml#_idTextAnchor168).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **File** | **New** | **Terminal** and perform the following steps on
    the terminal screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a Conda environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Activate the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install `bigframes` (we could also install any other packages we want, but
    we’ll keep it simple for now):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install JupyterLab in the new environment (this will make our new Conda environment
    available as a kernel in JupyterLab via JupyterLab’s autodiscovery feature):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change the display name of the kernel (it can take a few minutes for this update
    to appear in the Vertex AI Workbench JupyterLab interface):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, our Conda environment and custom Jupyter kernel are ready to be used in
    the hands-on exercises in this chapter. We just have one more prerequisite step
    to perform before we dive into the remaining topics in this chapter, at that is
    to stage the required files for our serverless Spark MLlib activities.
  prefs: []
  type: TYPE_NORMAL
- en: Staging files for serverless Spark MLlib activities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will stage some files in Google Cloud Storage to be used
    in the serverless Spark MLlib activities later in this chapter. To do this, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the Google Cloud console and open Cloud Shell by clicking the **Cloud
    Shell** icon, as shown in *Figure 14*. This icon looks like a “greater than” symbol,
    followed by an underscore – that is, **>_**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.1: Activating Cloud Shell](img/B18143_14_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: Activating Cloud Shell'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands in Cloud Shell to download the required files from
    our GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following commands in Cloud Shell to upload the required files to Google
    Cloud Storage (**important: replace [YOUR-BUCKET-NAME] with the name of a bucket
    you created in** **earlier chapters**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, everything is ready for our hands-on exercises later in this chapter. Next,
    we’ll dive into the important topic of BQML.
  prefs: []
  type: TYPE_NORMAL
- en: BQML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We first introduced BigQuery in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059),
    and we’ve used it in various chapters of this book for its data management and
    processing functionality. However, given the close relationship between large-scale
    data processing and ML, Google Cloud has built ML functionality directly into
    BigQuery, as well as native integrations with Google Cloud Vertex AI. This functionality
    is referred to as BQML, and this section explores this service in detail.
  prefs: []
  type: TYPE_NORMAL
- en: BQML enables us to create and execute ML models using standard SQL queries in
    BigQuery. Considering that many companies already store large amounts of data
    in BigQuery, BQML makes it easy for data scientists in those companies to train
    models on large datasets and make predictions directly in the database system,
    without needing to move the data around between different storage systems.
  prefs: []
  type: TYPE_NORMAL
- en: It supports a variety of ML algorithms, including linear regression, logistic
    regression, k-means clustering, and deep neural networks, as well as forecasting
    use cases based on time series data stored in BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to training and prediction, we can use BQML to perform many of the
    steps in the model development life cycle, such as feature engineering, model
    performance evaluation, and hyperparameter tuning. Considering that all of this
    can be done using standard SQL, data scientists can easily start using BQML without
    needing to go through a steep learning curve in terms of tooling. This is referred
    to as lowering the barrier of entry, where the barrier of entry represents how
    easy or difficult it is for people to start performing an activity. For instance,
    an activity with a high barrier of entry presents a lot of initial difficulty.
    An example of this would be a system that requires extensive training or complex
    prerequisites, such as provisioning infrastructure, before somebody could start
    using that system. Some technology systems require months of training and effort
    before somebody can start using them effectively. BQML, on the other hand, can
    easily be used by anybody who understands standard SQL syntax, and it does not
    require any complex infrastructure provisioning. Just like other managed services
    provided by Google Cloud, BigQuery (and, by extension, BQML) manages the infrastructure
    for us, and it can automatically scale up and down based on demand.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let’s take a look at how we can use BQML to develop and use
    ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Using BQML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the Jupyter Notebook that accompanies this chapter provides hands-on
    instructions on how to use BQML for various ML tasks, I will summarize some of
    the main features here. For context, let’s recall our ML model development life
    cycle, as depicted in *Figure 14**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2: ML model development life cycle](img/B18143_14_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: ML model development life cycle'
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections dive into each step in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will combine the following steps shown in *Figure 14**.3* into this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Ingest data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store input data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process/transform data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store processed data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting with data ingestion and storage, there are many ways in which we can
    get our data into BigQuery. For example, we can directly upload files via the
    BigQuery console UI or the CLI. For larger datasets, we can stage them in Google
    Cloud Storage and import them to BigQuery from there. We can also use the BigQuery
    Data Transfer Service to automate data movement into BigQuery, either as a one-off
    transfer or on a scheduled cadence. We can stream data into BigQuery by integrating
    with other services, such as Google Cloud Pub/Sub and Dataflow. There are also
    third-party ETL tools that can be used to transfer data to BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have stored our data in BigQuery, we can use various tools to explore
    and transform the data, in addition to the BigQuery console and standard SQL.
    We introduced and used the `pandas` library in previous chapters, and we know
    that it’s a very widely used library in data science, particularly for data exploration
    and manipulation. For this reason, many data scientists like to use pandas directly
    with their data stored in BigQuery. Fortunately, there are additional libraries
    that make it easy to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: The BigQuery DataFrames Python API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pandas_gbq` library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at each of these in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The BigQuery DataFrames Python API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The BigQuery DataFrames Python API enables us to use Python to analyze and
    manipulate data in BigQuery and perform various ML tasks. It’s a relatively new,
    open source option that was launched and is maintained by Google Cloud for using
    DataFrames to interact with BigQuery (at the time of writing this in December
    2023, it is currently in Preview status). We can access it by using the `bigframes`
    Python library, which consists of two main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bigframes.pandas`, which implements a pandas-like API on top of BigQuery'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bigframes.ml`, which implements a `scikit-learn`-like API on top of BigQuery
    ML'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Jupyter Notebook that accompanies this chapter provides instructions on
    how to use `bigframes.pandas` in more detail. We will dive into those steps shortly,
    but first, I’ll briefly cover `pandas_gbq`.
  prefs: []
  type: TYPE_NORMAL
- en: pandas_gbq
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If I were writing this chapter a few months ago, the `pandas_gbq` library would
    have been the main or only option that I would include in this section because,
    at the time of writing, it has been the primary option available for using pandas
    to interact with BigQuery. It’s an open source library that is maintained by PyData
    and volunteer contributors and has been around for quite some time (since 2017),
    so it has become broadly used in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, it’s a thin wrapper around the BigQuery client library (`google-cloud-bigquery`)
    that provides a simple interface for running SQL queries and uploading pandas
    DataFrames to BigQuery. The results from these queries are parsed into a `pandas.DataFrame`
    object in which the shape and data types are derived from the source table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jupyter Notebook that accompanies this chapter also provides instructions
    on how to use this library in more detail. Now would be a good time to dive into
    those steps. Open JupyterLab on the Vertex AI Workbench instance you created in
    [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168) and perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the navigation panel on the left-hand side of the screen, navigate to the
    `Chapter-14` directory within the `Google-Machine-Learning-for-Solutions-Architects`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click on the `pandas-gbq.ipynb` notebook file to open it. When prompted
    to select a kernel, you can use the default **Python 3 (****ipykernel)** kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click on the `bigframes.ipynb` notebook file to open it. When prompted
    to select a kernel, you can use the default **Python 3 (bigframes)** kernel that
    we created in the *Prerequisite topics and steps* section of this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each of the notebooks you’ve opened, press *Shift* + *Enter* to execute each
    cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The notebooks contain comments and Markdown text that describe what the code
    in each cell is doing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, there are multiple options for interacting with BigQuery data
    in Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Given the broad adoption of the `pandas_gbq` library in the industry, and now
    that Google has launched the official BigQuery DataFrames Python API described
    previously, it’s likely that both options will continue to be popular among data
    scientists. A key thing to bear in mind is that `pandas-gbq` downloads data to
    your local environment, whereas the BigQuery DataFrames Python API is used to
    run your data operations on Google Cloud distributed infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss how to create, use, and manage ML models with BQML. The
    Jupyter Notebook file that accompanies this chapter can be used to implement the
    following steps, but let’s discuss them before diving into the notebook file.
  prefs: []
  type: TYPE_NORMAL
- en: Creating ML models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the `CREATE MODEL` statement in BigQuery to define and train a model.
    For example, the following code excerpt creates a linear regression model named
    `my_model_name`, which is trained on the data in `my_dataset.my_table`, using
    the values in `target_colum` as the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As we’ve already discussed, BQML supports many types of commonly used algorithms.
    We can also import and export our models from/to Google Cloud Storage to interact
    with other tools and frameworks, and we can even perform hyperparameter tuning
    during the model development process.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating ML models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once our model has been trained, we can evaluate its performance using SQL
    code such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This returns evaluation metrics such as accuracy, precision, recall, and more,
    depending on the model type.
  prefs: []
  type: TYPE_NORMAL
- en: Generating predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we can use the model to make predictions using SQL code such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This statement will feed the data from `my_dataset.my_input_table` into our
    trained model to generate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use the Jupyter Notebook file that accompanies this chapter to
    implement these steps. To do that, open JupyterLab on the Vertex AI Workbench
    instance you created in Chapter 5 and perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the navigation panel on the left-hand side of the screen, navigate to the
    `Chapter-14` directory within the `Google-Machine-Learning-for-Solutions-Architects`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click on the `BQML.ipynb` notebook file to open it. When prompted, select
    the default **Python 3 (****ipykernel)** kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Shift* + *Enter* to execute each cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The notebook contains comments and Markdown text that describe what the code
    in each cell is doing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we’ll discuss how to set up model monitoring and continuous training.
  prefs: []
  type: TYPE_NORMAL
- en: Model monitoring and continuous training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To implement ongoing model monitoring and continuous training, we could set
    up scheduled jobs to execute the evaluation and training queries. This is a solution
    that we would need to architect using additional Google Cloud services, such as
    Google Cloud Scheduler and Google Cloud Functions, in which we could use Google
    Cloud Scheduler to periodically invoke Google Cloud Functions to run the training
    and evaluation queries. Having said that, for complex MLOps pipelines, Vertex
    AI offers more functionality for customization. This point brings us to our next
    topic, which is determining when to use BQML versus other tools for AI/ML use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: When to use BQML versus other tools for AI/ML use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve already covered that BQML is a great fit for data analysts and data scientists
    who want to use familiar SQL syntax to implement ML use cases on data stored in
    BigQuery, and how simplicity is one of its main benefits in that context. Bear
    in mind that there can often be a tradeoff between simplicity and customization.
    If you need to build highly advanced and customized models, then you may find
    that the simplicity of SQL does not provide the same level of customization that
    can be achieved when using specialized AI/ML frameworks such as TensorFlow, PyTorch,
    Ray, and Spark MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also have “the best of both worlds” by interacting with models hosted
    in Vertex AI from BigQuery by using **BigQuery remote functions**, which provide
    integrations with Cloud Functions and Cloud Run. With this approach, you can write
    code that will be executed on Cloud Functions or Cloud Run, and you can invoke
    that code from a query in BigQuery. The code could send an inference request to
    a model that has been trained and hosted in Vertex AI, and then also send the
    response back to BigQuery. You could even implement transformations in both the
    request and the response, if needed, to ensure compatibility between your query’s
    expected data types and your model’s expected data types. You can find out more
    about BigQuery remote functions in the Google Cloud documentation at the following
    link, which describes known limitations and best practices: [https://cloud.google.com/bigquery/docs/remote-functions](https://cloud.google.com/bigquery/docs/remote-functions).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While BigQuery is designed for large-scale analytics workloads, Google Cloud
    Spanner is another highly scalable database service in Google Cloud. Spanner is
    designed for distributed and strongly consistent transactional use cases, and
    it also supports SQL syntax. It recently added integrations with Vertex AI, providing
    somewhat similar functionality as BQML (that is, the ability to access ML models
    hosted on Vertex AI through a SQL interface), but intended for transactional rather
    than analytical workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I will end this section with a brief discussion of BigQuery Studio, which
    is a convenient way to manage all of our BigQuery tasks and workloads.
  prefs: []
  type: TYPE_NORMAL
- en: BigQuery Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BigQuery Studio, as the name suggests, provides a single-pane-of-glass experience
    that allows us to perform many different kinds of activities in BigQuery. It includes
    a SQL editor interface that enables us to write and run queries directly in the
    Google Cloud console, with intelligent code development assistance via integration
    with Duet, a Google Cloud Generative AI real-time code assistant that we will
    cover in more detail later in this book. BigQuery Studio also enables us to schedule
    SQL queries to run on a periodic cadence (for example, every day) for use cases
    that require repeated query executions, such as generating a daily report.
  prefs: []
  type: TYPE_NORMAL
- en: Via BigQuery Studio, we can access Dataform to define and run data processing
    workflows within BigQuery and utilize BigQuery Studio’s integrations with Dataplex
    for data discovery, data profiling, and data quality management. BigQuery Studio
    also integrates with Colab Enterprise, enabling us to use Jupyter Notebooks directly
    within the BigQuery console.
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to visit the BigQuery Studio interface within your Google Cloud
    console and explore its various capabilities and integrations.
  prefs: []
  type: TYPE_NORMAL
- en: We will revisit BigQuery in later chapters, but for now, let’s discuss some
    hardware considerations for AI/ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware considerations for AI/ML workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of the topics in this book focus on the software and service-level
    functionalities that are available on Google Cloud. Advanced practitioners will
    also be interested in what kind of hardware capabilities exist. If your use cases
    require extreme performance, then selecting the right hardware components on which
    to run your workloads is an important decision. The selection and efficient usage
    of underlying hardware also affect the costs, which are, of course, another important
    factor in your solution architecture. In this section, we’ll shift the discussion
    so that it focuses on some of the hardware considerations for running AI/ML workloads
    in Google Cloud, beginning with an overview of **central processing unit** (**CPU**),
    **graphics processing unit** (**GPU**), and **tensor processing unit** (**TPU**)
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: CPUs, GPUs, and TPUs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will probably already be familiar with CPUs and GPUs, but TPUs are more
    specific to Google Cloud. As a brief overview, CPUs are what power the vast majority
    of consumer devices, such as laptops and mobile phones, as well as general-purpose
    servers in data centers. They are effective at multi-tasking across a broad range
    of tasks, but the processing they perform is somewhat sequential.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs, on the other hand, are built with an architecture that optimizes parallel
    (rather than sequential) processing. If you can take a process and break it down
    into similar tasks that can run in parallel, then you’ll be able to complete that
    process much more quickly on a GPU than on a CPU. Although, as the name suggests,
    GPUs were originally designed for handling graphics, which involves processing
    large blocks of pixels and vertices in parallel, it turns out that the kinds of
    matrix manipulation tasks that are inherent to many AI/ML workloads can also be
    sped up by the parallel architecture of GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: TPUs were designed by Google to accelerate TensorFlow operations and they are
    optimized for both training and running models more efficiently than CPUs and
    GPUs for some types of workloads. Although they were created specifically for
    TensorFlow, they can now also be used with other frameworks, such as PyTorch,
    by using libraries such as PyTorch/XLA, which is a Python package that uses the
    **Accelerated Linear Algebra** (**XLA**) deep learning compiler to enable PyTorch
    to connect to TPUs and use TPU cores as devices.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud provides many different types of hardware servers that we can choose
    for running our ML workloads, and these servers provide various amounts of CPU,
    GPU, and TPU power.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this in December 2023, Google Cloud has recently launched
    its most powerful TPU (**Cloud TPU v5p**) in conjunction with their new **AI Hypercomputer**
    offering, which provides highly optimized resources (that is, storage, compute,
    networking, and more) for AI/ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the broad variety of computing options on Google Cloud, I will not list
    all of them here. Google Cloud is constantly launching additional options, so
    I encourage you to review the Google Cloud documentation to find the latest details:
    [https://cloud.google.com/compute/docs/machine-resource](https://cloud.google.com/compute/docs/machine-resource).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s switch the discussion back to the software level and explore some
    popular open source tools and frameworks in the ML and data science industry that
    are supported in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Additional open source tools and frameworks –Spark MLlib, Ray, and PyTorch on
    Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I’ll introduce additional open source tools and frameworks,
    such as PyTorch, Ray, and Spark **Machine Learning Library** (**MLlib**), and
    demonstrate how they can be used to implement AI/ML workloads on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduced Apache Spark in previous chapters and used it for data processing
    to perform feature engineering in [*Chapter 6*](B18143_06.xhtml#_idTextAnchor187).
    Apache Spark MLlib is a component of Apache Spark that provides ML tools and algorithms
    that are optimized for parallel processing with large datasets. In addition to
    feature engineering, we can use the tools in MLlib to implement various stages
    in our ML model development life cycle, such as model training, model evaluation,
    hyperparameter tuning, and prediction, as well as assembling stages into a pipeline
    that can be executed end to end.
  prefs: []
  type: TYPE_NORMAL
- en: Just as we discussed in the context of data processing, one of the main advantages
    of Apache Spark (including MLlib) is its ability to execute large-scale computing
    workloads. While libraries such as scikit-learn work well for performing ML workloads
    on single machines, MLlib can distribute the computation work across many machines,
    which enables us to handle larger datasets more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the hands-on activities that accompany this chapter, we will use Spark MLlib
    to train a model on Google Cloud Dataproc by using the Serverless Spark features
    within Vertex AI. This is important to understand from a solution architecture
    perspective: we will perform the steps in Vertex AI, but it will run the Spark
    job on Dataproc in the background. There are multiple ways in which we can implement
    this workload, and we will cover the following two methods in our hands-on activities:'
  prefs: []
  type: TYPE_NORMAL
- en: Using an MLOps pipeline, similar to the activities we performed in [*Chapter
    11*](B18143_11.xhtml#_idTextAnchor288).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the Serverless Spark user interface in Vertex AI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s begin with the first method: using an MLOps pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Spark via Kubeflow Pipelines on Vertex AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [*Chapter 11*](B18143_11.xhtml#_idTextAnchor288), we used the `DataprocPySparkBatchOp`
    operator in Kubeflow Pipelines to automate the execution of a serverless Spark
    job in an MLOps pipeline. We will use the same operator again in this chapter,
    but this time, we will use it to run a model training and evaluation job using
    Spark MLlib. To do so, open JupyterLab on the Vertex AI Workbench instance you
    created in Chapter 5 and perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the navigation panel on the left-hand side of the screen, navigate to the
    `Chapter-14` directory within the `Google-Machine-Learning-for-Solutions-Architects`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click on the `spark-ml.ipynb` notebook file to open it. When you’re prompted
    to select a kernel, select the **Python 3 (****ipykernel)** kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each of the notebooks you’ve opened, press *Shift* + *Enter* to execute each
    cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The notebooks contain comments and Markdown text that describe what the code
    in each cell is doing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we can see, it’s quite easy to extend the activities we performed in [*Chapter
    11*](B18143_11.xhtml#_idTextAnchor288) to use Kubeflow Pipelines to automate a
    Serverless Spark job for model training purposes with Spark MLlib. Let’s take
    a look at another option for running our Serverless Spark job directly via the
    Serverless Spark user interface in Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: The Serverless Spark user interface in Vertex AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To access the Serverless Spark user interface in Vertex AI, open JupyterLab
    on the Vertex AI Workbench instance you created in Chapter 5 and perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: From the **File** menu, select **New Launcher**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to the **Dataproc Jobs and Sessions** section and select **Serverless**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create Batch**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the screen that appears (see *Figure 14**.4* for reference), enter the following
    details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pyspark-ml.py` file. It should be in the following format `gs://YOUR-BUCKET-NAME/code/additional-use-cases-chapter/pyspark-ml.py`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--``processed_data_path=gs://YOUR-BUCKET-NAME/data/processed/mlops-titanic`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`--``model_path=gs://YOUR-BUCKET-NAME/models/additional-use-cases-chapter/`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Leave all other fields at their default values and click **Submit** at the
    bottom of the screen:![Figure 14.3: The Serverless Spark user interface in Vertex
    AI](img/B18143_14_004.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 14.3: The Serverless Spark user interface in Vertex AI'
  prefs: []
  type: TYPE_NORMAL
- en: After clicking the **Submit** button, a screen will appear showing a list of
    serverless Spark jobs; your newly submitted job will appear at the top of the
    list (you may need to refresh your browser page to update the list). Wait until
    the status of your job says **Succeeded**.
  prefs: []
  type: TYPE_NORMAL
- en: If the job fails for any reason, click on the name of the job to display its
    details, and then click **View Cloud Logs** at the top of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It may take some time for the logs to populate. You can click the **Run query**
    button periodically to refresh the logs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When the job has finished executing, the model artifacts will be saved in two
    directories named `metadata` and `stages` at the location you specified for the
    `--model-path` argument. Verify that those directories have been created and populated
    by performing the following steps in the Google Cloud console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the Google Cloud services menu and choose `--model-path` argument by clicking
    on each successive component of the path – for example, `YOUR-BUCKET-NAME/models/additional-use-cases-chapter/`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Great job! You have just used Spark MLlib to implement a serverless Spark workload
    to train a model on Dataproc via Vertex AI. Next, we’ll briefly discuss another
    distributed computing framework that has become increasingly broadly used in the
    data science industry: Ray.'
  prefs: []
  type: TYPE_NORMAL
- en: Ray
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I won’t spend much time on Ray in this book, but I want to mention it for completeness.
    The developers of Ray describe it as an “*open source unified compute framework
    that makes it easy to scale AI and Python workloads.*” Ray is another type of
    distributed execution framework that has been gaining popularity in recent years,
    especially in AI/ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: Like Spark, Ray enables parallelization of code and distribution of tasks across
    a cluster of machines. It also includes components that can help with specific
    model development steps, such as Ray Tune for hyperparameter tuning, and Ray RLlib
    for reinforcement learning. Google Cloud Vertex AI now directly supports Ray by
    enabling us to create Ray clusters.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like TensorFlow and Spark MLlib, PyTorch is an open source framework that includes
    a set of tools and libraries for ML and deep learning use cases. It was originally
    developed by Facebook’s AI Research lab, and it evolved from another AI/ML framework
    named Torch, which is based on a programming language named Lua. PyTorch, as the
    name suggests, has a Pythonic interface, and it has gained popularity in recent
    years for its ease of use and the flexibility provided by its various components,
    such as TorchVision for computer vision, TorchAudio for audio processing, and
    TorchText for natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch uses a dynamic (or “imperative”) computational graph, referred to as
    a **define-by-run** approach, where the graph is built on the fly as operations
    are performed, which enables more intuitive and flexible model development compared
    to static graphs used in some other frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow used to only provide the option of using a static computation graph,
    but in recent years, it has introduced an option named “Eager mode,” which offers
    dynamic graph functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Also like TensorFlow, PyTorch supports GPU acceleration (via NVIDIA’s CUDA framework)
    for tensor computations, which significantly speeds up model training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: On the topic of speeding up model training, it’s a common practice in computing
    to get more work done faster by performing operations in parallel across multiple
    devices. In the next section, we will discuss the practice of achieving this goal
    via distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale distributed model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Think back to the discussion we had in the *AI/ML and cloud computing* section
    of [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), in which I described the process
    of scaling our models to larger sizes, starting with small models that we could
    train on our laptop, progressing to larger models trained on a powerful, high-end
    server, and eventually getting to the scale at which a single computer (even the
    most powerful server on the market) couldn’t handle either the size of the model
    or the dataset on which the model is trained. In this section, we’ll look at what
    it means to train such large-scale models in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered the model training process in great detail throughout this book,
    but I’ll briefly summarize the process here as a knowledge refresher because these
    concepts are important when discussing large-scale distributed model training.
    For this discussion, I will focus on supervised training of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The supervised training process, at a high level, works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Instances from our training dataset are fed into the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model algorithm or network processes the training instances, and for each
    instance, it tries to predict the target variable. In the case of neural networks,
    this is referred to as forward propagation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After making a prediction, the model calculates the error or loss using a loss
    function (for example, **mean squared error** (**MSE**) for regression or cross-entropy
    for classification), which measures the difference between the model’s prediction
    and the actual label (the ground truth).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The backpropagation process then begins, which applies the chain rule from calculus
    to compute gradients of the loss function concerning each weight in the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An optimization algorithm such as gradient descent makes use of the gradients
    that were calculated during backpropagation to update the weights in the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training is usually performed by looping through the preceding process multiple
    times in a series of epochs, where an epoch is one complete pass through the entire
    training dataset. However, when using large datasets, the training process can
    divide the overall dataset into smaller batches, and each pass through a batch
    is considered a training step. Also, some frameworks, such as TensorFlow, allow
    us to specify the number of training steps per epoch. Over time, if the model
    is learning effectively, its predictions will become more accurate after each
    epoch, and the loss will decrease (hopefully to an acceptable threshold).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The term **processor** in these discussions refers to CPUs, GPUs, and TPUs.
    Also, the concepts we will discuss in this section can apply to training workloads
    that are distributed across multiple processors in a single machine, or across
    multiple processors on multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: In distributed training use cases, some communication and data sharing is required
    among the processors. When using a single machine, the processors usually share
    the same system memory (**RAM**), which simplifies data sharing, but when using
    multiple machines, communication needs to happen over a network (including routers,
    switches, and more), which can introduce latency and additional complexity.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve implemented the training process many times previously in this book. Next,
    we’ll discuss how this works when we’re using multiple processors.
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism and model parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are generally two main reasons we would need to implement distributed
    training workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: We want to use a very large dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to train a very large model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that both scenarios may exist at the same time (that is, we may need to
    train a very large model on a very large dataset). In this section, I’ll explain
    each of these scenarios in detail, starting with the case of using large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sometimes, we need to train our models on huge datasets that would take a long
    time to process on just one processor. To make the training process more efficient,
    we can split the dataset into smaller subsets or batches and process them in parallel
    across multiple processors. In this case, we can have a copy of our model running
    on each processor, which works through the subset of data that is loaded onto
    that processor. A simple example would be if we had a dataset that contained 10,000
    data points and we ran 10 processors that each worked on 1,000 data points from
    our dataset. This approach is referred to as **data parallelism**, and it is depicted
    in *Figure 14**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4: Data parallelism](img/B18143_14_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: Data parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14**.5*, each purple box represents a batch of our data, and each
    batch is sent to a separate processor in parallel. Each batch in the diagram contains
    only a few data points, but in reality, the batches would be much larger (this
    diagram intends to illustrate the concept at a high level). Next, we’ll discuss
    the other main scenario in which we may need to implement a distributed training
    workload. This is referred to as model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sometimes, the model itself is so large that it cannot fit in memory on a single
    processor. In these cases, we need to spread the model across multiple processors,
    where each processor handles a different portion or segment of the model. A segment
    could be a layer in the network, or pieces of layers in the case of some very
    complex models. *Figure 14**.6* shows a simple example of model parallelism, in
    which each layer of our neural network runs on a separate processor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5: Model parallelism](img/B18143_14_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: Model parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: There are different methods we can use to break our model down into segments,
    and we generally want to choose an approach that minimizes communication overhead
    between the processors. This is because that part of the process can introduce
    the most latency and complexity, especially when we’re using multiple machines
    where communication needs to happen over a physical network.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to optimizing communication between the processors, we also need
    to design our training procedure in a way that maximizes the utilization of each
    processor. Remember that the outputs from one layer in our neural network become
    inputs to other layers in our network, so there are **sequential dependencies**
    among the layers. When our layers are distributed across different processors,
    some processors could be idle while they wait for inputs to be propagated from
    the previous layer in the network. This, of course, would be an inefficient use
    of our processing resources, and we can use an approach called **pipelining**
    to improve the training efficiency by breaking the input data up into micro-batches.
    In that case, each segment or stage of the model processes its micro-batch and
    passes the outputs to the next stage. Then, while the next stage begins to process
    the outputs, the previous stage can start processing the next micro-batch, and
    so on. In this way, data flows through the network in a more streamlined manner,
    rather than processors for later layers being idle while waiting for an earlier
    layer to process an entire large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: So far, I’ve been mainly talking about how to parallelize the training procedure
    in terms of how the models process the inputs (that is, the forward pass in the
    training process). Remember that the training procedure also includes a feedback
    loop to check the outputs of the model, compute the loss against the ground truth,
    then compute the gradients of the loss and update the weights. Next, we will dive
    into how those steps can be implemented in a distributed manner.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training update process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the case of model parallelism, data is still processed through the network
    sequentially – that is, each layer (or segment) in the network produces outputs,
    and those outputs become the inputs for the next layer (even though the layers
    or segments are being processed on different processors). Because of this, the
    backpropagation process can also happen sequentially, where gradients are computed
    in each segment starting from the last to the first, similar to how it’s done
    in a non-distributed setting. Each segment independently updates its portion of
    the model weights based on the computed gradients.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of data parallelism, however, subsets of the training data are processed
    in parallel across different processors, so the overall workflow is not sequential.
    Each processor has an identical copy of the model, but each copy works on a different
    subset of the data. For this reason, the individual replicas of the model on each
    processor are unaware of what’s being done on the other processors. Therefore,
    some additional coordination is needed when computing the loss, the gradients,
    and the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement this coordination in various ways, but we should generally
    pick one of the following two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a centralized **parameter server** to do the coordination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a protocol that allows the individual training servers to act as a
    community (I will refer to this as “collaborative computing”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss each of these approaches in more detail, starting with the parameter
    server approach.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter server approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the parameter server approach, one or more nodes in the distributed system
    are designated as parameter servers, and these nodes hold the global model parameters.
    This means that they have a global view of the gradients and parameters for all
    of the model replicas across all nodes in the distributed training system or cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the individual training nodes (let’s call them “worker nodes”)
    process different subsets of data and compute their respective gradients, and
    then each worker node sends its gradients to the centralized parameter server(s).
    The parameter server(s) then update the global model parameters based on these
    gradients and send the updated parameters back to the worker nodes. A simplified
    architecture diagram for this approach is shown in *Figure 14**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6: The parameter server approach](img/B18143_14_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: The parameter server approach'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14**.7*, the arrows represent the interchange of gradients and weight
    updates between the workers and the parameters server(s). Dashed lines and borders
    represent that there could be one or more parameter servers because a single parameter
    server could become a bottleneck in the solution. There also needs to be a coordinating
    entity in this architecture to manage all of the resources and their communication.
    This could be co-located in the parameter server or could be implemented as a
    separate server.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll discuss the other option for implementing model training with data
    parallelism where, instead of using centralized servers for performing the coordination
    steps, all of the workers collaborate directly with each other.
  prefs: []
  type: TYPE_NORMAL
- en: The collaborative computing approach using Ring All-Reduce
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the collaborative computing approach, there is no central server or set of
    servers that are dedicated to the purpose of performing coordination among the
    independent worker nodes in the training cluster. Instead, a distributed computing
    protocol is used for coordination among the nodes. This means that the nodes get
    updates (in this case, the computed gradients) from each other. Numerous communication
    protocols exist for coordination across distributed computing systems, and a pretty
    popular one is known as **All-Reduce**. These kinds of protocols are generally
    designed to aggregate data across all nodes in a distributed system and then share
    the result back to all nodes. This means that, even though each node may process
    a different subset of our dataset, and the model instance on each node may therefore
    compute different gradients and weights, each node eventually ends up with an
    aggregated representation of the gradients and weights that is consistent across
    all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using All-Reduce, we can connect the processors in various ways, such
    as by implementing a fully connected mesh, as depicted in *14.8*, a ring architecture,
    as depicted in *Figure 14**.9*, or other approaches, such as tree and butterfly
    architectures, which we will not cover here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7: Fully connected mesh](img/B18143_14_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Fully connected mesh'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 14**.8*, the fully connected mesh architecture, in which
    all nodes are connected to all other nodes, would require a lot of complex coordination
    among all of the nodes. For this reason, the simpler and more effective ring architecture
    is often chosen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8: Ring architecture](img/B18143_14_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: Ring architecture'
  prefs: []
  type: TYPE_NORMAL
- en: In the ring architecture, as the name implies, the nodes are logically arranged
    in a ring. One of the important aspects of this architecture, referred to as **Ring
    All-Reduce**, is that each node only needs to send its computed gradients to one
    of its neighbors. The neighbor then combines (aggregates) the received gradients
    with its own gradients, passes the aggregated gradients onto the next node in
    the ring, and so on. This aggregation is often as simple as adding the gradients
    together. After *N-1* steps (where *N* is the number of nodes), each node will
    have a copy of the aggregated gradients for all nodes. They can then simply perform
    a division operation to calculate the average gradients, and use those values
    in the optimization step.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To be more specific, the Ring All-Reduce algorithm is usually broken down into
    two phases, referred to as the “reduce-scatter” phase and the “all-gather” phase,
    but that’s a level of detail that’s not necessary for understanding how the process
    works at a high level, as described in this section.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might be wondering which approach is better – that is, using
    a centralized parameter server or using a collaborative approach such as Ring
    All-Reduce. The answer, as is the case in many solution architecture decisions,
    is. “It depends.” Let’s discuss some additional factors that play into this decision,
    such as whether we want to implement synchronous versus asynchronous data parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous versus asynchronous data parallelism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of synchronous data parallelism, all the nodes need to update their
    parameters at the same time, or at least in a coordinated way. A benefit of this
    approach is that it provides consistency in parameter updates across all of the
    nodes. However, it can result in bottlenecks that slow down the overall training
    process, because all nodes must wait for the slowest one.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of asynchronous data parallelism, on the other hand, the nodes update
    their parameters independently without waiting for others, which can lead to faster
    training. However, this can sometimes cause problems with model convergence and
    stability because the nodes can go out of sync if the procedure is not implemented
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous implementations can also be more fault tolerant because the nodes
    are not dependent on each other, so if a node goes out of service for some reason,
    the rest of the cluster can continue to function, so long as we have set up the
    environment in a fault-tolerant manner.
  prefs: []
  type: TYPE_NORMAL
- en: All-Reduce is generally implemented synchronously because the nodes need to
    share their gradients, whereas the parameter server approach can be implemented
    either synchronously or asynchronously, with asynchronous usually being the more
    common choice.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll learn about how Google Cloud Vertex AI provides an optimized All-Reduce
    mechanism via the Vertex AI Reduction Server.
  prefs: []
  type: TYPE_NORMAL
- en: The Vertex AI Reduction Server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the Ring All-Reduce approach is pretty well established and popular in
    the industry, it also has some limitations. For example, in practice, it is found
    that latency tends to scale linearly with the number of workers in the ring (that
    is, the more workers we add, the more latency we get). Also, since each worker
    needs to wait on all other workers in the ring, a single slow worker can slow
    down the ring overall. Additionally, the ring architecture can have a single point
    of failure, whereby if one node fails, it can break the entire ring.
  prefs: []
  type: TYPE_NORMAL
- en: 'For these reasons, Google created a product called the Reduction Server, which
    provides a faster All-Reduce algorithm. In addition to the worker nodes, the Vertex
    AI Reduction Server also provides **reducer nodes**. The workers host and execute
    the model replicas, compute the gradients, and apply the optimization steps, while
    the reducers have a relatively simple job, which is just to aggregate the gradients
    from workers. *Figure 14**.10* shows an example of the Reduction Server architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9: Reduction Server implementation](img/B18143_14_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Reduction Server implementation'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14**.10*, you’ll notice a similarity to the parameter server architecture,
    but bear in mind that the reducers perform much fewer tasks than parameter servers,
    and this simplicity provides some benefits. For example, the reducers are just
    lightweight instances that can use CPUs, which, in this case, can be significantly
    cheaper than GPUs. Also, unlike the ring architecture, the latency does not scale
    linearly as we add more reducer nodes, and there is no single point of failure
    that can break the overall architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the improvements provided by the Vertex AI Reduction Server architecture,
    I recommend using this approach when performing distributed training on Vertex
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training jobs generally involve using a lot of resources, which
    could incur expenses, so we will not perform a hands-on activity for this topic
    in this chapter. If you would like to learn more about how to implement an ML
    project using distributed training on Vertex AI (optionally, also including the
    Reduction Server), I recommend referencing the documentation at [https://cloud.google.com/vertex-ai/docs/training/distributed-training](https://cloud.google.com/vertex-ai/docs/training/distributed-training).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at some other important factors for distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: Other important factors for distributed training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In distributed training, the most complex and error-prone part of the process
    is usually managing communication among the nodes. If this is not implemented
    efficiently, then it can cause bottlenecks and impact training performance. We’ve
    already discussed how we generally want to minimize how much data needs to be
    sent between the nodes, and fortunately, there are some tricks we can use to help
    in this regard, such as compressing the gradients before transmitting them.
  prefs: []
  type: TYPE_NORMAL
- en: Also, considering that distributed training jobs usually need to process a lot
    of data, they can often run for long periods, even when the workload is being
    parallelized across multiple nodes. For this reason, we should put mechanisms
    in place to help us recover from failures that could occur during the training
    process. For example, if our job has been running for many hours, we wouldn’t
    want to have to start it from the beginning again if something fails before the
    job finishes. A common and important mechanism to implement for this purpose is
    **checkpointing**, in which we regularly save the state of the model during training
    so that the process can continue from a recent state if failure occurs. This is
    just like periodically saving our work while we’re working on a document just
    in case our laptop crashes unexpectedly.
  prefs: []
  type: TYPE_NORMAL
- en: All of the distributed training concepts we’ve discussed in this section so
    far assume that we have all of the required data stored somewhere in our environment,
    and we simply need to distribute that data (or the models) across multiple nodes.
    However, there’s another popular type of distributed model training referred to
    as **federated learning**, which I’ll briefly describe next.
  prefs: []
  type: TYPE_NORMAL
- en: Federated learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous chapters, we discussed deploying ML models to edge devices such
    as mobile phones or IoT devices, usually to provide low-latency inference on those
    devices. We also discussed how those edge devices generally have much less computing
    power and data storage resources than servers have. Therefore, our ability to
    train models on those devices is limited.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine that the models that are deployed to the devices can be improved
    by being updated with data that becomes available to the devices from their local
    environment, such as from sensors. If we want to provide the best possible experience
    for users of those devices, then we will want to ensure that the models are periodically
    updated to account for new data that becomes available. However, in some cases,
    it may be inefficient or otherwise undesirable to constantly send data from the
    edge devices to a central location for larger model training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Federated learning is a technique that enables us to periodically update the
    models without needing to send data from those devices back to a centralized location.
    Instead, copies of the model on each device are trained locally by the data that
    becomes available to the device. The model training process on each device goes
    through the familiar steps of calculating the loss and the gradients, and then
    updating the model parameters or weights accordingly. Let’s imagine that this
    kind of training is being performed on millions of devices, where each device
    is making updates to the model based on small pieces of data that become available
    to it. Given the limited training capabilities on each device, this process alone
    will not lead to a lot of model improvements in the long term. However, in the
    case of federated learning, the updated weights can be sent back to a central
    location to be combined to create a more powerful model that can learn from the
    small training processes on millions of devices. This means that all of the weights
    from the millions of devices can be averaged (that is, aggregated) to form a more
    advanced model. This updated model (or an optimized version of it) can then be
    sent out to each device, and the process can be repeated on an ongoing basis to
    keep improving the models over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The effective thing about this process is that only the model weights need
    to be communicated back to the centralized infrastructure, and none of the actual
    data on the devices ever needs to be transmitted. This is important for two main
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The weights are much smaller (in terms of data size) than the training data,
    so sending only the weights is much more efficient than sending training data
    from the devices to the central infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since only the weights are being transmitted (and no actual user data), this
    helps keep user data safe and preserves privacy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are, of course, other methods for implementing distributed model training,
    but we’ve covered many of the more popular ones here.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of this book will focus on Generative AI, so this is a good point
    for us to revisit some important topics that we covered at a high level earlier
    in this book that will help us understand various developments that led to the
    Generative AI era.
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning to Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start diving into Generative AI, I’ll provide additional details
    on some of the important neural network architectures we discussed earlier in
    this book. For example, in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245), we
    briefly introduced some common neural network architectures, such as CNNs, RNNs,
    **long short-term memory** (**LSTM**), and transformers. In this section, we will
    dive deeper into how some of these notable architectures work, for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The practical exercises accompanying this chapter include building a CNN for
    a computer vision use case, so it’s important to describe the inner workings of
    CNNs in more detail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of the neural network architectures mentioned here can be seen as milestones
    in the journey toward developing Generative AI technologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with a deeper dive into CNNs and computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs and computer vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245), CNNs are
    often used in computer vision use cases such as object recognition and visual
    categorization, and their trick is to break pictures down into smaller components,
    or “features,” and learn each component separately. In this way, the network learns
    smaller details in the picture and then combines them to identify larger shapes
    or objects hierarchically.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We could probably write an entire book on CNN architecture, but that level of
    detail is not required in this book. I’ll cover some of the important concepts
    at a high level, and the Jupyter Notebook that accompanies this chapter provides
    code examples on how to build a relatively simple CNN for a computer vision use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a brief refresher of what we discussed in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    the most basic form of neural network, as depicted in *Figure 14**.11*, is referred
    to as a **feed-forward neural network** (**FFNN**), in which the information that
    is being fed into the network follows a simple forward direction as it passes
    through the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.10: A simple neural network](img/B18143_14_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: A simple neural network'
  prefs: []
  type: TYPE_NORMAL
- en: A CNN, on the other hand, adds some specific types of layers into the architecture
    that help with implementing the hierarchical processing we mentioned in the introduction
    to this section. These types of layers will be explained at a high level in the
    following sub-sections.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These layers perform a mathematical operation called **convolution**, which
    involves sliding a filter (or kernel) over the input image to produce a feature
    map. The filter is a small matrix that’s used to detect specific features, such
    as edges, corners, or textures, where each filter is **convolved** across the
    input image, computing the dot product between the filter and input, resulting
    in a feature map.
  prefs: []
  type: TYPE_NORMAL
- en: The output of a convolutional layer, therefore, is a set of feature maps that
    represent specific features that are detected by the filters at different locations
    in the input image.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pooling layers are mainly used for dimensionality reduction by downsampling
    the feature maps and reducing the number of parameters. We covered the concept
    of dimensionality reduction in detail in [*Chapter 7*](B18143_07.xhtml#_idTextAnchor215)
    and discussed how it is often necessary to implement dimensionality reduction
    to reduce the amount of computation required during training and serving ML models,
    as well as to reduce the likelihood of overfitting the training data. Different
    pooling approaches can be used, but the most common approach is called “max pooling,”
    which takes the maximum value from a set of pixels in a region (defined by the
    pool size). Another approach, called “average pooling,” simply takes the average
    of the values from a set of pixels in a region.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Combining the convolutional and pooling layers results in the features being
    extracted from the input images. After the input data is passed through the convolutional
    and pooling layers, the high-level reasoning in the network is then performed
    by using fully connected layers. These are the kinds of layers we covered when
    we discussed FFNNs, where the neurons in each layer have connections to all activations
    in the previous layer. Just as in the case of FFNNs, the fully connected layers
    are often used for the final classification or regression task that the network
    is performing.
  prefs: []
  type: TYPE_NORMAL
- en: The final output layer often uses a softmax activation function to output a
    probability distribution over the classes that the model is trying to identify.
    Alternatively, for binary classification, a sigmoid function could be used.
  prefs: []
  type: TYPE_NORMAL
- en: Residual networks
  prefs: []
  type: TYPE_NORMAL
- en: Residual networks, or ResNets, are a type of CNN that was developed to address
    the problem of vanishing and exploding gradients, something we discussed in [*Chapter
    9*](B18143_09.xhtml#_idTextAnchor245). They do this by introducing “skip connections,”
    which are shortcuts that “skip” over layers within the network, and directly connect
    the output of one layer to the input of a later layer.
  prefs: []
  type: TYPE_NORMAL
- en: That’s about as much detail as we’re going to cover regarding the inner workings
    of CNNs. Many research papers go into great detail on how CNNs work, but that’s
    a level of detail beyond the scope of this book. Next, we’ll dive in and work
    on some code examples to build our very first CNN for a computer vision use case!
  prefs: []
  type: TYPE_NORMAL
- en: In the Jupyter Notebooks that accompany this chapter, we will use Keras to build
    a simple CNN. This is where you will also start learning how to use PyTorch, which
    we’ll then use to perform the same task.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To build our CNNs, open JupyterLab on the Vertex AI Workbench instance you
    created in Chapter 5 and perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the navigation panel on the left-hand side of the screen, navigate to the
    `Chapter-14` directory within the `Google-Machine-Learning-for-Solutions-Architects`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click on the `Keras-CV.ipynb` notebook file to open it. When you’re prompted
    to select a kernel, select the **TensorFlow** kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click on the `PyTorch-CV.ipynb` notebook file to open it. When you’re
    prompted to select a kernel, select **PyTorch** kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each of the notebooks you’ve opened, press *Shift* + *Enter* to execute each
    cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The notebooks contain comments and Markdown text that describe what the code
    in each cell is doing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Take a moment to savor the fact that you have just used both Keras and PyTorch
    to build CNNs for a computer vision use case on Google Cloud Vertex AI. This is
    some pretty advanced stuff!
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll take a look at some types of neural network architectures that are
    typically useful for sequential data and use cases such as **natural language**
    **processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: RNNs, LSTMs, and transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Again, we already introduced these concepts at a high level in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245).
    In this section, we will dive a little bit deeper into these types of architectures.
    We won’t build these networks in this chapter, nor go into excessive detail; I’m
    covering these topics here mainly for historical context to pave the way for concepts
    and activities that I’ll introduce in the Generative AI chapters in this book.
    This is because all of the neural network architectures in this section can be
    seen as stepping stones toward the development of the Generative AI models we
    use today, in the order shown in *Table 14.1:*
  prefs: []
  type: TYPE_NORMAL
- en: '| **Development** | **Timeframe** | **References** |'
  prefs: []
  type: TYPE_TB
- en: '| RNN | 1990 | Elman, J. L. (1990). *Finding structure in time*. Cognitive
    Science, 14(2), 179-211.https://doi.org/10.1207/s15516709cog1402_1. |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM | 1997 | Hochreiter, Sepp & Schmidhuber, Jürgen. (1997). *Long Short-term
    Memory*. Neural computation. 9\. 1735-80\. 10.1162/neco.1997.9.8.1735.https://doi.org/10.1162/neco.1997.9.8.1735.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Transformer | 2017 | Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. *Attention Is
    All You Need*. [2017, June 12]. arXiv preprint arXiv:1706.03762\. http://arxiv.org/abs/1706.03762.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14.1: The stepping stones in sequential model development'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by diving into RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you read each word in this sentence, you need to remember the words that
    came before it so that you can build an understanding of what the overall sentence
    is saying. This is an application of **natural language understanding** (**NLU**),
    which is a sub-field of NLP. In a more generic sense, the sentence represents
    sequential data because the data points (in this case, words) in the sentence
    relate to other words in the sentence and the order in which they are processed
    matters. *Figure 14**.12* shows an example of sequence data being fed into a neural
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.11: Sequence data being fed into a neural network](img/B18143_14_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: Sequence data being fed into a neural network'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 14**.12*, we can see that data point 1 will be processed first by
    the neural network, and then data point 2 will be processed, and so on, where
    each data point is processed one at a time by the network.
  prefs: []
  type: TYPE_NORMAL
- en: In simple FFNNs, each data point is passed independently through the network,
    so the network does not associate data points with previous data points. As we
    briefly mentioned in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245), RNNs introduce
    a type of looping mechanism into the network architecture, and this looping mechanism
    is what enables the network to “remember” previous data points. Let’s look at
    this in a bit more detail to understand what this means.
  prefs: []
  type: TYPE_NORMAL
- en: In RNNs, the concept of each data point being processed by the network is referred
    to as a **time step**. The important thing to note is that, with RNNs, the outputs
    from neurons in each time step can be saved for future reference. Let’s call this
    the **current state** of our network; this is often referred to as the **hidden
    state** of the network because it is generally not exposed externally.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each time step, neurons in the network receive two sets of inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The output from neurons in the previous layer of the network (or inputs from
    the input layer if we’re referring to the first hidden layer in the network).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output from the same neuron (that is, itself) that was saved from the previous
    time step. This is the hidden state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at our diagram in *Figure 14**.12*, in the first time step, when the
    first data point in the sequence (that is, data point 1) passes through the network,
    the process will be much like a standard FFNN, as we described in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    because there is no previous state to remember.
  prefs: []
  type: TYPE_NORMAL
- en: However, when data point 2 is being processed through the network, each neuron
    can combine this new data with the “current state” of the network (which is the
    state that was created after processing data point 1). This process repeats for
    each new data point that passes through the network, and by doing this, the network
    maintains a kind of memory of the data points it processed previously.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through time (BPTT)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We discussed the backpropagation process in neural network learning in [*Chapter
    9*](B18143_09.xhtml#_idTextAnchor245). Considering that RNNs use the concept of
    time steps and combine the outputs of neurons over time, the backpropagation process
    in RNNs needs to be modified accordingly. This modified version of backpropagation
    is called **BPTT**.
  prefs: []
  type: TYPE_NORMAL
- en: It involves “unrolling” the RNN in time, meaning that the time steps are treated
    as separate layers of a deep neural network. Once the network has been unrolled,
    the standard backpropagation algorithm is applied – that is, the network’s error
    is calculated at each time step, the gradients are computed, and then these gradients
    are used to update the weights in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of RNNs – gradients and short-term memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In an RNN, the unrolling that is performed as part of the BPTT process can lead
    to very deep networks for long sequences. Remember that vanishing and exploding
    gradients happen because of the chain rule in calculus. As the gradients are backpropagated
    through the deep networks associated with long sequences, this can make the problem
    of vanishing and exploding gradients more pronounced.
  prefs: []
  type: TYPE_NORMAL
- en: This also affects the “memory” of the RNN. As sequences get longer, the ability
    for backpropagation to update the gradients of earlier time steps gets smaller
    and smaller (in the case of vanishing gradients). This has the effect of making
    the network “forget” earlier time steps in longer sequences, which means that
    RNNs are generally limited to short sequences.
  prefs: []
  type: TYPE_NORMAL
- en: The vanishing and exploding gradient problems and short-term memory limitations
    have led to the development of more advanced RNN architectures such as LSTMs,
    which can address these issues. We’ll discuss this in more detail next.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LSTM is a type of RNN that was designed specifically to overcome the limitations
    of traditional RNNs, especially the vanishing and exploding gradient problems
    when processing long sequences, as we discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the hidden state mechanism we described in the *RNNs* section,
    LSTMs add a concept called **cell state**. This is often likened to a kind of
    conveyor belt that runs straight through the entire chain of an LSTM network,
    which allows information to be easily transported across many steps in the process.
    The hidden state still acts as the network’s short-term memory, and the cell state
    acts as a longer-term memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs also introduce the concepts of **gates**, which can be seen as checkpoints
    along the conveyor belt that decide what information should be maintained, dropped,
    or added. In this way, the gates regulate the flow of information through the
    network. There are generally three types of gates:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input gates**, which update the cell state with new information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output gates**, which use the cell state to determine what the next hidden
    state should be'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forget gates**, which decide what information to discard from the cell state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’d like to learn more about how these gates are used in combination with
    the hidden state and the cell state, I recommend reading the original paper (Hochreiter
    and Schmidhuber, 1997) at [https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory](https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory).
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of gates, a variation on LSTMs called **gated recurrent unit** (**GRU**)
    was more recently introduced (Cho et al., 2014). It combines the forget and input
    gates into a single **update gate** and merges the cell state and hidden state,
    which results in a simpler and more efficient model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll discuss one of the most important breakthroughs in the AI/ML industry
    in recent years: the transformer architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As is the case in most fields of research, the pace of advancement generally
    proceeds at a somewhat linear rate, with occasional transformative breakthroughs
    along the timeline. The linear advancement in any field represents the step-by-step
    progress that occurs on an ongoing basis, and the transformative breakthroughs
    represent the sudden, giant leaps forward that completely change everything. When
    Google invented the transformer architecture (Vaswani, A., et al., 2017), this
    was one such giant leap forward in AI/ML research, and it has formed the basis
    of the Generative AI revolution that we are currently experiencing in the world.
  prefs: []
  type: TYPE_NORMAL
- en: The progression from RNNs to LSTMs is what I would consider to be a linear development.
    However, the progression from LSTMs to the transformer architecture was an enormous
    jump forward in the development of AI/ML technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, unlike RNNs and LSTMs, which process sequences step by step, transformers
    can handle entire sequences simultaneously (in parallel), and this parallel processing
    allows them to manage long-range dependencies in our data much more efficiently.
    This also allows them to achieve larger scales than previous model architectures,
    resulting in what are now the largest and most powerful language models in the
    industry. However, parallel processing is not the most significant feature of
    the transformer architecture. As suggested in the title of the paper that first
    introduced the transformer architecture to the world – *Attention Is All You Need*
    (arXiv:1706.03762 [cs.CL]) – the main innovation in transformers is the self-attention
    mechanism, which allows the model to weigh the importance of different parts of
    the input data. For example, if we feed a sentence (or sequence) into a transformer
    model, the self-attention mechanism allows the model to understand the entire
    sentence and its internal relationships (for example, the relationships between
    the words).
  prefs: []
  type: TYPE_NORMAL
- en: Transformers run multiple self-attention **heads** in parallel on the sequence,
    something that’s referred to as **multi-head attention** in the paper. Hopefully,
    you already read through the paper when we linked it in [*Chapter 9*](B18143_09.xhtml#_idTextAnchor245),
    but if not, I highly recommend reading the original paper to dive into the intricate
    details of how transformers work, given their pivotal importance in the AI/ML
    and Generative AI industry. Transformers are the basis of the current state-of-the-art
    Generative AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve recapped the evolutionary steps that led to their development,
    we are ready to move on to the next chapter. But first, let’s take a moment to
    reflect on what we’ve learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered important AI/ML concepts that are typically used
    by more advanced practitioners who have specific needs or preferences in terms
    of how they want to implement their ML workloads, such as by using specific frameworks,
    or by parallelizing their workloads across multiple processors.
  prefs: []
  type: TYPE_NORMAL
- en: We started by discussing BQML, focusing on the close relationship between data
    processing, data analytics, and ML. We learned how to train and evaluate a model
    using BQML, and how to get predictions from that model, all by using SQL query
    syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we discussed different types of hardware that we can use for our AI/ML
    workloads, and popular tools and frameworks that we had not previously covered
    in this book, such as Spark MLlib, Ray, and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we dived into CNNs and their use in computer vision before moving on to
    discuss neural network architectures that are particularly useful for sequential
    data and use cases such as NLP, such as RNNs, LSTMs, and transformers. We dived
    into the inner workings of these architectures, mainly to understand the evolutionary
    steps that have led to the development of the Generative AI technologies that
    we will outline in the remaining chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discussed distributed model training in AI/ML, including why it’s
    needed, and some of the mechanisms we can use to implement it, such as data parallelism
    and model parallelism. We also dived into different approaches for implementing
    data parallelism, such as using centralized parameter servers, or distributed
    coordination mechanisms such as Ring All-Reduce. We also explored the topic of
    federated learning, and how it can help to preserve privacy by avoiding the process
    of transmitting data from devices.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will embark on an exciting journey as we dive into the world of Generative
    AI. Join me in the next chapter to get started!
  prefs: []
  type: TYPE_NORMAL
