- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Unsupervised Models with K-Means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned about building **machine learning** (**ML**) models
    where data is supplied with labels. In this chapter, we will learn about building
    ML models on a dataset without any labels by using the **K-means clustering algorithm**.
    Unlike **supervised models**, where predictions are made at the observation level,
    K-means clustering groups observations into clusters where they share a commonality
    – for example, similar demographics or reading habits.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will provide detailed examples of business problems that can be
    solved with these modeling techniques. By the end of this chapter, you will be
    in a position to identify a business problem that an **unsupervised modeling technique**
    can be applied to. You will also learn how to build, train, and evaluate K-means
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Grouping data through cluster analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a K-means ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the results of K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires a web browser and access to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Amazon Redshift Serverless endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Redshift Query Editor v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complete the *Getting started with Amazon Redshift Serverless* section in [*Chapter
    1*](B19071_01.xhtml#_idTextAnchor015)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the code used in this chapter here: [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter8/chapter8.sql](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter8/chapter8.sql).'
  prefs: []
  type: TYPE_NORMAL
- en: Grouping data through cluster analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have explored datasets that contained input and target variables,
    and we trained a model with a set of input variables and a target variable. This
    is called supervised learning. However, how do you address a dataset that does
    not contain a label to supervise the training? **Amazon Redshift ML** supports
    unsupervised learning using the cluster analysis method, also known as the K-means
    algorithm. In **cluster analysis**, the ML algorithm automatically discovers the
    grouping of data points. For example, if you have a population of 1,000 people,
    a clustering algorithm can group them based on height, weight, or age.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike supervised learning, where an ML model predicts an outcome based on
    a label, unsupervised models use unlabeled data. One type of unsupervised learning
    is clustering, where unlabeled data is grouped based on its similarity or differences.
    From a dataset with demographic information about individuals, you can create
    clusters based on young, adult, and elderly populations, underweight, normal weight,
    and overweight populations, and so on. These groups are calculated based on values
    – for example, if two people are young, then they are grouped together. These
    groups are called **clusters**. In the following diagram, you can see that input
    variables (**Age**, **Height**, and **Weight**) are grouped into **young**, **adult**,
    and **elder**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A simple cluster example](img/B19071_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – A simple cluster example
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, each individual data point is placed in a cluster,
    based on the distance from the center of the cluster, called the **centroid**.
    The distance from the centroid for each data point is calculated using the **Euclidean
    distance formula**. Data points that are closest to a given centroid have similarities
    and belong to the same group. In real-world situations, it is very common to find
    data points with overlapping clusters and too many of them. When you encounter
    too many clusters, then it is a challenge to identify the right number of clusters
    for your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common use cases for a K-means cluster include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E-commerce**: Grouping customers by purchase history'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare**: Detecting patterns of diseases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Finance**: Grouping purchases into abnormal versus normal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will show you one of the common methods to help you determine how many
    clusters you should use.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the optimal number of clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One popular method that is frequently adopted is the **Elbow method**. The idea
    of the Elbow method is to run K-means algorithms with different values of K –
    for example, from 1 cluster all the way to 10 – and for each value of K, calculate
    the sum of squared errors. Then, plot a chart of the **sum of squared deviation**
    (**SSD**) values. SSD is the sum of the squared difference and is used to measure
    variance. If the line chart looks like an arm, then the *elbow* on the arm is
    the value of K that is the best among the various K values. The method behind
    this approach is that SSD usually tends to decrease as the value of K is increased,
    and the goal of the evaluation method is also to aim for lower SSD or **mean squared
    deviation** (**MSD**) values. The elbow represents a starting point, where SSD
    starts to have diminishing returns when the K value increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following chart, you can see that the MSD value, when charted over different
    K values, represents an arm, and the *elbow* is at value **6**. After **6**, there
    is no significant decrease in the MSD value, so we can pick *6* as the best cluster
    value in the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 8.2 – MSD values when charted over different \uFEFFK values](img/B19071_08_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – MSD values when charted over different K values
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see how we can create a K-means clustering model with Amazon Redshift
    ML.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a K-means ML model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will walk through the process with the help of a use case.
    In this use case, assume you are a data analyst for an e-commerce company specializing
    in home improvement goods. You have been tasked with classifying economic segments
    in different regions, based on income, so that you can better target customers,
    based on various factors, such as median home value. We will use this dataset
    from Kaggle: [https://www.kaggle.com/datasets/camnugent/california-housing-prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices).'
  prefs: []
  type: TYPE_NORMAL
- en: From this dataset, you will use the `median_income`, `latitude`, and `longitude`
    attributes so that you can create clusters based on `location` and `income`.
  prefs: []
  type: TYPE_NORMAL
- en: The syntax to create a K-means model is slightly different from what you will
    have used up to this point, so let’s dive into that.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model syntax overview for K-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the basic syntax to create a K-means model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A couple of key things to note in the preceding code snippet are the lines
    in bold, as they are required when creating K-means models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AUTO OFF`: This must be turned off, since Amazon SageMaker Autopilot is not
    used for K-means'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MODEL_TYPE` `KMEANS`: You must set `MODEL_TYPE`, as there is no auto-discovery
    for K-means'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HYPERPARAMETERS DEFAULT EXCEPT (K ''2'')`: This tells SageMaker how many clusters
    to create in this model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, note that there are three optional preprocessors available with K-means.
    We will explore that in more detail when we create the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to this link for more details on the K-means parameters available:
    [https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_k-means-create-model-parameters](https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_k-means-create-model-parameters).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will load our dataset in preparation for creating our model.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading and analyzing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this use case, we will use a file that contains housing price information
    and summary stats, based on census data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Data is stored in the following S3 location: `s3://packt-serverless-ml-redshift/chapter08/housinghousing_prices.csv`.'
  prefs: []
  type: TYPE_NORMAL
- en: After successfully connecting to Redshift as an admin or database developer,
    load data into Amazon Redshift and follow the steps outlined here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to Redshift **query editor** **v****2**, connect to the **Serverless:
    default** endpoint, and then connect to the **dev** database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 8.3 – Connecting via \uFEFFRedshift query editor v2](img/B19071_08_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Connecting via Redshift query editor v2
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following steps to create the schema and customer table and load
    the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This dataset contains 2,064,020,640 records. We will use `longitude`, `latitude`,
    and `median_income` in our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following query to examine some sample data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Housing prices data](img/B19071_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Housing prices data
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data is loaded, we are ready to create the model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the K-means model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s create our model and cluster based on `median_income`, `longitude`, and
    `latitude`.
  prefs: []
  type: TYPE_NORMAL
- en: We will create a few models and then use the elbow method to determine the optimal
    number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, let’s create our first model with two clusters using the following
    SQL. You can then experiment by creating different models by changing the K value,
    and then you can learn how the MSD value diminishes over different K values.
  prefs: []
  type: TYPE_NORMAL
- en: Creating two clusters with a K value of 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s run the following SQL in Query Editor v2 to create a model with two clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You can see in this model that we supply values for the `preprocessors` parameter.
    We chose to do this because K-means is sensitive to scale, so we can normalize
    with the `standardscaler` transformer. `standardscalar` moves the mean and scale
    to unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `hyperparameters` parameter is where we specify `(K ''2'')` to create two
    clusters. Remember to add your S3 bucket, where the created model artifacts are
    stored. You will find the model artifacts in `s3: s3://<your-s3-bucket>/redshift-ml/housing_segments_k2/`.
    Redshift ML will automatically append `''redshift-ml''/''your model name''` to
    your S3 bucket. Now, check the status of the model, using the `SHOW MODEL` command
    in Query Editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Fi\uFEFFgure 8.5 – Two clusters](img/B19071_08_05.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Two clusters
  prefs: []
  type: TYPE_NORMAL
- en: The key things to note are **Model State**, which indicates that the model is
    ready, and **train:msd**, which is the objective metric. This represents the mean
    squared distances between each record in our input dataset and the closest center
    of the model. The **MSD** value is **1.088200**, which is a good score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run a query to get the number of data points in each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – The data points](img/B19071_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – The data points
  prefs: []
  type: TYPE_NORMAL
- en: Clusters are numbered from `0` to `n`. Our first cluster has **8719** data points,
    and the second cluster has **11921** data points.
  prefs: []
  type: TYPE_NORMAL
- en: In our use case, we want to further segment our customers. Let’s create a few
    more models with different numbers of clusters. We can then evaluate all the SSD
    values and apply the Elbow method to help us choose the optimal number of clusters
    to use for our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Creating three clusters with a K value of 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s run the following SQL in Query Editor v2 to create a model with three
    clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Creating the remaining models with clusters 4, 5, and 6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Repeat the preceding code 3 more times to create models with 4, 5, and 6 clusters,
    respectively. You will find the code at [https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter8/chapter8.sql](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter8/chapter8.sql).
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take ~15 minutes for all the models to finish training. Then, run the
    `SHOW MODEL` command, including the one for the model where `K = 2`, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s find the elbow!
  prefs: []
  type: TYPE_NORMAL
- en: Gathering inputs to chart the elbow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, from the output of each `SHOW MODEL` command, note the value for `test:msd`
    and build a `Select` statement, as shown in the following code snippet. Change
    the value for MSD using the `test:mds` value for each model.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we will use the value `1.088200,` which we saw earlier for `train:msd`,
    for the model with two clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our other output from `train:mds` from the `SHOW MODEL` output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two clusters: `train:msd` – `1.088200`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Three clusters: `train:msd` – `0.775993`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Four clusters: `train:msd` – `0.532355`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Five clusters: `train:msd` – `0.437294`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Six clusters: `train:msd` – `0.373781`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that your numbers may be slightly different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Run the preceding SQL command in Query Editor v2.
  prefs: []
  type: TYPE_NORMAL
- en: 'By observing the output, we can see that the MSD value is highest for two clusters
    and gradually decreases as the number of clusters increases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – msd](img/B19071_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – msd
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Result** window, click on the **Chart** option, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Creating a chart](img/B19071_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Creating a chart
  prefs: []
  type: TYPE_NORMAL
- en: 'By choosing `k` as the `X` value and `msd` as the `Y` value, you will get the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – The elbow method chart](img/B19071_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The elbow method chart
  prefs: []
  type: TYPE_NORMAL
- en: From the chart, we can see that when MSD is charted over a line graph, an arm
    is formed, and the elbow is at **3**. This means that there is little difference
    in the MSD value with **4** clusters compared to **3** clusters . We can see that
    after **3**, the curve is very smooth, and the difference between the MSD value
    does not drastically change compared to the beginning of the line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how data points are clustered when we use a function deployed for
    our model with three clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the following output from Query Editor v2\. The counts represent
    the number of data points assigned to each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Three clusters](img/B19071_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Three clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also chart this by clicking on the **Chart** button and observing the
    cluster counts represented visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – The cluster data points](img/B19071_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – The cluster data points
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how we can use our model to help make business decisions based
    on the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the results of the K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have segmented your clusters with the K-means algorithm, you are
    ready to perform various analyses using the model you created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example query you can run to get the average median house value
    by cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Average median house values](img/B19071_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Average median house values
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also run a query to see whether higher median incomes correspond to
    the same clusters with higher home values. Run the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – median_income](img/B19071_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – median_income
  prefs: []
  type: TYPE_NORMAL
- en: When we established our use case, we said this was for an e-commerce retailer
    specializing in home improvement products. Another way you could use this information
    is to create different marketing campaigns and tailor your product offerings,
    based on home values in a given cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how to do unsupervised learning with the K-means
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: You are now able to explain what the K-means algorithm is and what use cases
    it is appropriate for. Also, you can use Amazon Redshift ML to create a K-means
    model, determine the appropriate number of clusters, and draw conclusions by analyzing
    the clusters to help make business decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will show you how to use the multi-layer perceptron
    algorithm to perform deep learning with Amazon Redshift ML.
  prefs: []
  type: TYPE_NORMAL
- en: Part 3:Deploying Models with Redshift ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Part 3* introduces you to more ways to leverage Amazon Redshift ML. You will
    learn about deep learning algorithms, how to train a customized model, and how
    you can use models trained outside of Amazon Redshift to run inference queries
    in your data warehouse.'
  prefs: []
  type: TYPE_NORMAL
- en: This part closes with an introduction to time-series forecasting, how to use
    it with Amazon Redshift ML, and how you can optimize and easily re-train your
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19071_09.xhtml#_idTextAnchor157), *Deep Learning with Redshift
    ML*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19071_10.xhtml#_idTextAnchor178), *Creating Custom ML Models
    with XGBoost*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B19071_11.xhtml#_idTextAnchor192), *Bring Your Own Models for
    In-Database Inference*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B19071_12.xhtml#_idTextAnchor217), *Time-Series Forecasting
    in Your Data Warehouse*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B19071_13.xhtml#_idTextAnchor233), *Operationalizing and Optimizing
    Amazon Redshift ML Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
