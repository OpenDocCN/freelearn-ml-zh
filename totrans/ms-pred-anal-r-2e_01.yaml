- en: Chapter 1. Gearing Up for Predictive Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章：为预测建模做准备
- en: In this first chapter, we'll start by establishing a common language for models
    and taking a deep view of the predictive modeling process. Much of predictive
    modeling involves the key concepts of statistics and machine learning, and this
    chapter will provide a brief tour of the core features of these fields that are
    essential knowledge for a predictive modeler. In particular, we'll emphasize the
    importance of knowing how to evaluate a model that is appropriate to the type
    of problem we are trying to solve. Finally, we will showcase our first model,
    the k-nearest neighbors model, as well as `caret`, a very useful R package for
    predictive modelers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一章中，我们将首先建立一个用于模型的标准语言，并对预测建模过程进行深入探讨。预测建模的大部分内容涉及统计学和机器学习的关键概念，本章将简要介绍这些领域的核心特征，这些特征对于预测模型师来说是必备的知识。特别是，我们将强调了解如何评估适合我们试图解决的问题类型的模型的重要性。最后，我们将展示我们的第一个模型，即k近邻模型，以及`caret`，这是一个对预测模型师非常有用的R包。
- en: Models
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: Models are at the heart of predictive analytics and, for this reason, we'll
    begin our journey by talking about models and what they look like. In simple terms,
    a model is a representation of a state, process, or system that we want to understand
    and reason about. We make models so that we can draw inferences from them and,
    more importantly for us in this book, make predictions about the world. Models
    come in a multitude of different formats and flavors, and we will explore some
    of this diversity in this book. Models can be equations linking quantities that
    we can observe or measure; they can also be a set of rules. A simple model with
    which most of us are familiar from school is Newton's Second Law of Motion. This
    states that the net sum of force acting on an object causes the object to accelerate
    in the direction of the force applied and at a rate proportional to the resulting
    magnitude of the force and inversely proportional to the object's mass.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是预测分析的核心，因此，我们将从讨论模型及其外观开始我们的旅程。简单来说，模型是我们想要理解和推理的状态、过程或系统的表示。我们建立模型是为了从中得出推论，并且对我们这本书来说更重要的是，对世界做出预测。模型有多种不同的格式和风味，本书将探讨其中的一些多样性。模型可以是连接我们可以观察或测量的数量的方程；它们也可以是一组规则。我们大多数人从学校时代就熟悉的一个简单模型是牛顿的第二运动定律。该定律表明，作用在物体上的合力使物体沿着力的方向加速，加速度与力的结果大小成正比，与物体的质量成反比。
- en: 'We often summarize this information via an equation using the letters *F*,
    *m*, and *a* for the quantities involved. We also use the capital Greek letter
    sigma (*Σ*) to indicate that we are summing over the force and arrows above the
    letters that are vector quantities (that is, quantities that have both magnitude
    and direction):'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常通过使用字母 *F*、*m* 和 *a* 来表示涉及的数量，通过方程来总结这些信息。我们还使用大写希腊字母sigma (*Σ*) 来表示我们对力进行求和，并在字母上方使用箭头来表示矢量量（即具有大小和方向的量）：
- en: '![Models](img/00002.jpeg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![模型](img/00002.jpeg)'
- en: This simple but powerful model allows us to make some predictions about the
    world. For example, if we apply a known force to an object with a known mass,
    we can use the model to predict how much it will accelerate. Like most models,
    this model makes some assumptions and generalizations. For example, it assumes
    that the color of the object, the temperature of the environment it is in, and
    its precise coordinates in space are all irrelevant to how the three quantities
    specified by the model interact with each other. Thus, models abstract away the
    myriad of details of a specific instance of a process or system in question, in
    this case the particular object in whose motion we are interested, and limit our
    focus only to properties that matter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单但强大的模型使我们能够对世界做出一些预测。例如，如果我们对一个已知质量的物体施加一个已知的力，我们可以使用这个模型来预测它将加速多少。像大多数模型一样，这个模型做出了一些假设和概括。例如，它假设物体的颜色、环境的温度以及它在空间中的精确坐标都与模型所指定的三个数量如何相互作用无关。因此，模型抽象掉了特定过程或系统实例的众多细节，在这种情况下，是我们感兴趣的特定物体的运动，并且只关注那些重要的属性。
- en: Newton's second law is not the only possible model to describe the motion of
    objects. Students of physics soon discover other more complex models, such as
    those taking into account relativistic mass. In general, models are considered
    more complex if they take a larger number of quantities into account or if their
    structure is more complex. For example, nonlinear models are generally more complex
    than linear models. Determining which model to use in practice isn't as simple
    as picking a more complex model over a simpler model. In fact, this is a central
    theme that we will revisit time and again as we progress through the many different
    models in this book. To build our intuition as to why this is so, consider the
    case where our instruments that measure the mass of the object and the applied
    force are very noisy. Under these circumstances, it might not make sense to invest
    in using a more complicated model, as we know that the additional accuracy in
    the prediction won't make a difference because of the noise in the inputs. Another
    situation where we may want to use the simpler model is when, in our application,
    we simply don't need the extra accuracy. A third situation arises where a more
    complex model involves a quantity that we have no way of measuring. Finally, we
    might not want to use a more complex model if it turns out that it takes too long
    to train or make a prediction because of its complexity.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 牛顿第二定律并不是描述物体运动的唯一可能模型。物理学学生很快会发现其他更复杂的模型，例如考虑相对论质量的模型。一般来说，如果模型考虑了更多的量或其结构更复杂，则认为模型更复杂。例如，非线性模型通常比线性模型更复杂。确定在实际情况中应使用哪个模型并不像简单地选择一个更复杂的模型而不是一个简单的模型那样简单。事实上，这是我们将在本书中探讨的许多不同模型中反复出现的核心主题。为了构建我们对为什么是这样的直觉，考虑这样一种情况，即我们测量物体质量和施加力的仪器非常嘈杂。在这种情况下，投资使用更复杂的模型可能没有意义，因为我们知道由于输入的噪声，预测的额外准确性不会产生影响。另一种情况下，我们可能想要使用更简单的模型，因为我们应用中根本不需要额外的准确性。第三种情况是，一个更复杂的模型涉及一个我们无法测量的量。最后，如果由于复杂性，训练或做出预测需要太长时间，我们可能不想使用更复杂的模型。
- en: Learning from data
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据中学习
- en: In this book, the models we will study have two important and defining characteristics.
    The first of these is that we will not use mathematical reasoning or logical induction
    to produce a model from known facts, nor will we build models from technical specifications
    or business rules; instead, the field of predictive analytics builds models from
    data. More specifically, we will assume that for any given predictive task that
    we want to accomplish, we will start with some data that is in some way related
    to (or derived from) the task at hand. For example, if we want to build a model
    to predict annual rainfall in various parts of a country, we might have collected
    (or have the means to collect) data on rainfall at different locations, while
    measuring potential quantities of interest, such as the height above sea level,
    latitude, and longitude. The power of building a model to perform our predictive
    task stems from the fact that we will use examples of rainfall measurements at
    a finite list of locations to predict the rainfall in places where we did not
    collect any data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将要研究的模型有两个重要且定义性的特征。第一个特征是，我们不会使用数学推理或逻辑归纳从已知事实中产生模型，也不会从技术规范或业务规则中构建模型；相反，预测分析领域是从数据中构建模型的。更具体地说，我们将假设对于任何我们想要完成的预测任务，我们都会从与（或从）当前任务以某种方式相关（或派生）的数据开始。例如，如果我们想要构建一个模型来预测一个国家不同地区的年降雨量，我们可能已经收集（或拥有收集）了不同地点的降雨数据，同时测量潜在的感兴趣量，如海拔高度、纬度和经度。构建模型以执行我们的预测任务的力量源于这样一个事实，即我们将使用有限列表地点的降雨测量示例来预测我们没有收集任何数据的地方的降雨量。
- en: 'The second important characteristic of the problems for which we will build
    models is that, during the process of building a model from some data to describe
    a particular phenomenon, we are bound to encounter some source of randomness.
    We will refer to this as the **stochastic** or **nondeterministic component**
    of the model. It may be the case that the system itself that we are trying to
    model doesn''t have any inherent randomness in it, but it is the data that contains
    a random component. A good example of a source of randomness in data is the measurement
    of errors from readings taken for quantities such as temperature. A model that
    contains no inherent stochastic component is known as a **deterministic model**,
    Newton''s second law being a good example of this. A stochastic model is one that
    assumes that there is an intrinsic source of randomness to the process being modeled.
    Sometimes, the source of this randomness arises from the fact that it is impossible
    to measure all the variables that are most likely impacting a system, and we simply
    choose to model this using probability. A well-known example of a purely stochastic
    model is rolling an unbiased six-sided die. Recall that, in probability, we use
    the term **random variable** to describe the value of a particular outcome of
    an experiment or of a random process. In our die example, we can define the random
    variable, *Y*, as the number of dots on the side that lands face up after a single
    roll of the die, resulting in the following model:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要构建的模型的问题的第二个重要特征是，在从某些数据构建模型以描述特定现象的过程中，我们必然会遇到一些随机性的来源。我们将称之为模型的**随机**或**非确定性**成分。可能的情况是，我们试图模拟的系统本身并不具有固有的随机性，但数据中包含随机成分。数据中随机性的一个很好的例子是从温度等数量读取的测量误差。不包含固有随机成分的模型被称为**确定性模型**，牛顿第二定律就是这样一个很好的例子。一个随机模型是假设被模拟的过程具有内在随机性的模型。有时，这种随机性的来源可能是无法测量可能影响系统的所有变量，我们只是选择用概率来模拟这种情况。一个纯随机模型的例子是掷一个公平的六面骰子。回想一下，在概率论中，我们使用**随机变量**这个术语来描述实验或随机过程的特定结果的值。在我们的掷骰子例子中，我们可以定义随机变量*Y*为掷一次骰子后正面朝上的面的点数，从而得到以下模型：
- en: '![Learning from data](img/00003.jpeg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![从数据中学习](img/00003.jpeg)'
- en: This model tells us that the probability of rolling a particular digit, say,
    3, is one in six. Notice that we are not making a definite prediction on the outcome
    of a particular roll of the die; instead, we are saying that each outcome is equally
    likely.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型告诉我们，掷出特定数字的概率，比如说3，是六分之一。请注意，我们并没有对掷骰子特定结果的预测做出明确判断；相反，我们是在说每个结果的可能性是相等的。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: Probability is a term that is commonly used in everyday speech, but at the same
    time sometimes results in confusion with regard to its actual interpretation.
    It turns out that there are a number of different ways of interpreting probability.
    Two commonly cited interpretations are **Frequentist probability** and **Bayesian
    probability**. Frequentist probability is associated with repeatable experiments,
    such as rolling a one-sided die. In this case, the probability of seeing the digit
    3, is just the relative proportion of the digit 3 coming up if this experiment
    were to be repeated an infinite number of times. Bayesian probability is associated
    with a subjective degree of belief or surprise at seeing a particular outcome
    and can, therefore, be used to give meaning to one-off events, such as the probability
    of a presidential candidate winning an election. In our die rolling experiment,
    we are as surprised to see the number 3 come up as with any other number. Note
    that in both cases, we are still talking about the same probability numerically
    (1/6); only the interpretation differs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 概率是一个在日常生活中经常使用的术语，但同时也可能对其实际解释产生混淆。实际上，存在多种不同的概率解释方式。两种常见的解释是**频率主义概率**和**贝叶斯概率**。频率主义概率与可重复的实验相关联，例如掷一个单面的骰子。在这种情况下，看到数字3的概率，就是如果这个实验无限次重复，数字3出现的相对比例。贝叶斯概率与看到特定结果的主观信念程度或惊讶程度相关，因此可以用来赋予一次性事件意义，例如总统候选人赢得选举的概率。在我们的掷骰子实验中，我们看到数字3出现的惊讶程度与其他任何数字一样。请注意，在这两种情况下，我们仍在谈论相同的概率数值（1/6）；只是解释不同。
- en: 'In the case of the die model, there aren''t any variables that we have to measure.
    In most cases, however, we''ll be looking at predictive models that involve a
    number of independent variables that are measured, and these will be used to predict
    a dependent variable. Predictive modeling draws on many diverse fields and as
    a result, depending on the particular literature you consult, you will often find
    different names for these. Let''s load a dataset into R before we expand on this
    point. R comes with a number of commonly cited datasets already loaded, and we''ll
    pick what is probably the most famous of all, the *iris dataset*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在骰子模型的例子中，我们没有需要测量的变量。然而，在大多数情况下，我们将研究涉及多个独立变量的预测模型，这些变量将被用来预测一个因变量。预测建模借鉴了许多不同的领域，因此，根据你参考的特定文献，你经常会发现这些名称的不同。在我们进一步探讨这一点之前，让我们将一个数据集加载到R中。R附带了一些常用的数据集已经加载，我们将选择其中最著名的，即*鸢尾花数据集*：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: To see what other datasets come bundled with R, we can use the `data()` command
    to obtain a list of datasets along with a short description of each. If we modify
    the data from a dataset, we can reload it by providing the name of the dataset
    in question as an input parameter to the `data()`command; for example, `data(iris)`
    reloads the iris dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看R附带的其他数据集，我们可以使用`data()`命令来获取数据集列表以及每个数据集的简要描述。如果我们修改了数据集中的数据，我们可以通过将数据集的名称作为输入参数提供给`data()`命令来重新加载它；例如，`data(iris)`重新加载了鸢尾花数据集。
- en: The `iris` dataset consists of measurements made on a total of 150 flower samples
    of three different species of iris. In the preceding code, we can see that there
    are four measurements made on each sample, namely the lengths and widths of the
    flower petals and sepals. The iris dataset is often used as a typical benchmark
    for different models that can predict the species of an iris flower sample, given
    the four previously mentioned measurements. Collectively, the sepal length, sepal
    width, petal length, and petal width are referred to as **features**, **attributes**,
    **predictors**, **dimensions**, or **independent variables** in literature. In
    this book, we prefer to use the word feature, but other terms are equally valid.
    Similarly, the species column in the data frame is what we are trying to predict
    with our model, and so it is referred to as the **dependent variable**, **output**,
    or **target**. Again, in this book, we will prefer one form for consistency, and
    will use output. Each row in the data frame corresponding to a single data point
    is referred to as an **observation**, though it typically involves observing the
    values of a number of features.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`iris`数据集包含了对150个不同品种的鸢尾花样本进行的测量。在前面的代码中，我们可以看到每个样本进行了四个测量，即花瓣和萼片的长度和宽度。鸢尾花数据集通常被用作预测鸢尾花样本物种的典型基准，前提是提供前面提到的四个测量值。在文献中，萼片长度、萼片宽度、花瓣长度和花瓣宽度统称为**特征**、**属性**、**预测变量**、**维度**或**自变量**。在这本书中，我们更喜欢使用“特征”这个词，但其他术语同样有效。同样，数据框中的物种列是我们试图用我们的模型预测的内容，因此它被称为**因变量**、**输出**或**目标**。再次强调，在这本书中，我们将为了保持一致性而偏好一种形式，并使用“输出”。数据框中对应单个数据点的每一行被称为**观测值**，尽管它通常涉及观察多个特征值。'
- en: As we will be using datasets, such as the iris data described earlier, to build
    our predictive models, it also helps to establish some symbol conventions. Here,
    the conventions are quite common in most of the literature. We'll use the capital
    letter, *Y*, to refer to the output variable, and subscripted capital letter,
    *X[i]*, to denote the *i^(th)* feature. For example, in our iris dataset, we have
    four features that we could refer to as *X[1]* through *X[4]*. We will use lower-case
    letters for individual observations, so that *x[1]* corresponds to the first observation.
    Note that *x[1]* itself is a vector of feature components, *x[ij]*, so that *x[12]*
    refers to the value of the second feature in the first observation. We'll try
    to use double suffixes sparingly and we won't use arrows or any other form of
    vector notation for simplicity. Most often, we will be discussing either observations
    or features and so the case of the variable will make it clear to the reader which
    of these two is being referenced.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用数据集，例如前面描述的鸢尾花数据，来构建我们的预测模型，因此建立一些符号约定也是有帮助的。在这里，这些约定在大多数文献中都很常见。我们将使用大写字母
    *Y* 来指代输出变量，并使用下标大写字母 *X[i]* 来表示第 *i* 个特征。例如，在我们的鸢尾花数据集中，我们有四个特征，我们可以将其称为 *X[1]*
    到 *X[4]*。我们将使用小写字母表示单个观测值，因此 *x[1]* 对应于第一个观测值。请注意，*x[1]* 本身是一个特征成分的向量，*x[ij]*，因此
    *x[12]* 指的是第一个观测值中第二个特征的价值。我们将尽量少地使用双下标，并且为了简单起见，我们不会使用箭头或其他形式的向量符号。通常，我们讨论的是观测值或特征，因此变量的大小写将使读者清楚我们正在引用的是这两个中的哪一个。
- en: 'When thinking about a predictive model using a dataset, we are generally making
    the assumption that for a model with *n* features, there is a true or ideal function,
    *f*, that maps the features to the output:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑使用数据集的预测模型时，我们通常假设对于具有 *n* 个特征的模型，存在一个真实或理想的函数 *f*，它将特征映射到输出：
- en: '![Learning from data](img/00004.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![从数据中学习](img/00004.jpeg)'
- en: 'We''ll refer to this function as our **target function**. In practice, as we
    train our model using the data available to us, we will produce our own function
    that we hope is a good estimate for the target function. We can represent this
    by using a caret on top of the symbol *f* to denote our predicted function, and
    also for the output, *Y*, since the output of our predicted function is the predicted
    output. Our predicted output will, unfortunately, not always agree with the actual
    output for all observations (in our data or in general):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个函数称为我们的**目标函数**。在实践中，当我们使用我们可用的数据训练模型时，我们将产生我们自己的函数，我们希望这个函数是对目标函数的良好估计。我们可以通过在符号
    *f* 上面放置一个撇号来表示我们的预测函数，以及输出 *Y*，因为预测函数的输出是预测输出。不幸的是，我们的预测输出并不总是与所有观测值（在我们的数据或一般情况中）的实际输出一致：
- en: '![Learning from data](img/00005.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![从数据中学习](img/00005.jpeg)'
- en: Given this, we can essentially summarize predictive modeling as a process that
    produces a function to predict a quantity, while minimizing the error it makes
    compared to the target function. A good question we can ask at this point is,
    Where does the error come from? Put differently, why are we generally not able
    to exactly reproduce the underlying target function by analyzing a dataset?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们可以将预测建模概括为一个产生函数以预测一个数量的过程，同时最小化它与目标函数之间的误差。此时我们可以提出的一个好问题是，误差从何而来？换句话说，为什么我们通常不能通过分析数据集来精确地再现底层的目标函数？
- en: The answer to this question is that in reality there are several potential sources
    of error that we must deal with. Remember that each observation in our dataset
    contains values for *n* features, and so we can think about our observations geometrically
    as points in an *n*-dimensional feature space. In this space, our underlying target
    function should pass through these points by the very definition of the target
    function. If we now think about this general problem of fitting a function to
    a finite set of points, we will quickly realize that there are actually infinite
    functions that could pass through the same set of points. The process of predictive
    modeling involves making a choice in the type of model that we will use for the
    data, thereby constraining the range of possible target functions to which we
    can fit our data. At the same time, the data's inherent randomness cannot be removed
    no matter what model we select. These ideas lead us to an important distinction
    in the types of error that we encounter during modeling, namely the **reducible
    error** and the **irreducible error**, respectively.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案在于现实中存在几个潜在的误差来源，我们必须处理。记住，我们数据集中的每一个观测值都包含 *n* 个特征值，因此我们可以将我们的观测值几何地视为
    *n* 维特征空间中的点。在这个空间中，我们的目标函数应该通过这些点，这正是目标函数的定义。如果我们现在考虑将函数拟合到有限点集的这个问题，我们会很快意识到实际上有无限多的函数可以穿过相同的点集。预测建模的过程涉及到在数据所使用的模型类型上做出选择，从而限制我们可以拟合数据的可能目标函数的范围。同时，无论我们选择什么模型，数据的固有随机性是无法消除的。这些想法使我们注意到在建模过程中遇到的误差类型的重要区别，即**可减少误差**和**不可减少误差**。
- en: The reducible error essentially refers to the error that we as predictive modelers
    can minimize by selecting a model structure that makes valid assumptions about
    the process being modeled and whose predicted function takes the same form as
    the underlying target function. For example, as we shall see in the next chapter,
    a linear model imposes a linear relationship between its features in order to
    compose the output.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可减少误差基本上是指我们作为预测模型师可以通过选择一个对建模过程做出有效假设且其预测函数与潜在目标函数具有相同形式的模型结构来最小化的误差。例如，正如我们将在下一章中看到的，线性模型通过在其特征之间施加线性关系来组成输出。
- en: This restrictive assumption means that, no matter what training method we use,
    how much data we have, and how much computational power we throw in, if the features
    aren't linearly related in the real world, then our model will necessarily produce
    an error for at least some possible observations. By contrast, an example of an
    irreducible error arises when trying to build a model with an insufficient feature
    set. This is typically the norm and not the exception. Often, discovering what
    features to use is one of the most time-consuming activities of building an accurate
    model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个限制性假设意味着，无论我们使用什么训练方法，我们有多少数据，以及我们投入多少计算能力，如果特征在现实世界中不是线性相关的，那么我们的模型必然会对至少一些可能的观测值产生误差。相比之下，一个不可减少误差的例子出现在尝试用不足的特征集构建模型时。这通常是常态而不是例外。通常，发现要使用哪些特征是构建准确模型中最耗时的一项活动。
- en: Sometimes, we may not be able to directly measure a feature that we know is
    important. At other times, collecting the data for too many features may simply
    be impractical or too costly. Furthermore, the solution to this problem is not
    simply a question of adding as many features as possible. Adding more features
    to a model makes it more complex and we run the risk of adding a feature that
    is unrelated to the output, thus introducing noise in our model. This also means
    that our model function will have more inputs and will, therefore, be a function
    in a higher dimensional space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们可能无法直接测量一个我们知道很重要的特征。在其他时候，收集太多特征的数据可能只是不切实际或过于昂贵。此外，解决这个问题的方法并不仅仅是添加尽可能多的特征。向模型添加更多特征会使模型更加复杂，我们冒着添加与输出无关的特征的风险，从而在我们的模型中引入噪声。这也意味着我们的模型函数将有更多的输入，因此将是一个更高维空间中的函数。
- en: Some of the potential practical consequences of adding more features to a model
    include increasing the time it will take to train the model, making convergence
    on a final solution harder, and actually reducing model accuracy under certain
    circumstances, such as with highly correlated features. Finally, another source
    of an irreducible error that we must live with is the error in measuring our features
    so that the data itself may be noisy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 向模型添加更多特征可能带来的潜在实际后果包括增加训练模型所需的时间，使最终解决方案的收敛更困难，以及在某些情况下（例如，与高度相关的特征一起）实际上降低模型精度。最后，我们必须接受的不可减少误差的另一个来源是我们测量特征时的误差，这样数据本身可能就是嘈杂的。
- en: Reducible errors can be minimized not only through selecting the right model,
    but also by ensuring that the model is trained correctly. Thus, reducible errors
    can also come from not finding the right specific function to use, given the model
    assumptions. For example, even when we have correctly chosen to train a linear
    model, there are infinitely many linear combinations of the features that we could
    use. Choosing the model parameters correctly, which in this case would be the
    coefficients of the linear model, is also an aspect of minimizing the reducible
    error. Of course, a large part of training a model correctly involves using a
    good optimization procedure to fit the model. In this book, we will at least give
    a high-level intuition of how each model that we study is trained. We generally
    avoid delving deeply into the mathematics of how optimization procedures work,
    but we do give pointers to the relevant literature for the interested reader to
    find out more.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 可减少误差不仅可以通过选择正确的模型来实现，还可以通过确保模型被正确训练来实现。因此，可减少误差也可能来自没有找到正确的特定函数来使用，考虑到模型假设。例如，即使我们正确选择了训练线性模型，我们仍然可以使用无限多的特征线性组合。正确选择模型参数（在这种情况下是线性模型的系数）也是最小化可减少误差的一个方面。当然，正确训练模型的大部分工作涉及使用良好的优化程序来拟合模型。在本书中，我们将至少给出我们研究的每个模型是如何训练的直观理解。我们通常避免深入探讨优化程序如何工作的数学，但我们确实为感兴趣的读者提供了相关文献的指针。
- en: The core components of a model
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型的核心理念
- en: 'So far we''ve established some central notions behind models and a common language
    to talk about data. In this section, we''ll look at what the core components of
    a statistical model are. The primary components are typically:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经建立了一些模型背后的核心概念和讨论数据的通用语言。在本节中，我们将探讨统计模型的核心理念是什么。主要组件通常是：
- en: A set of equations with parameters that need to be tuned
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组需要调整参数的方程
- en: Some data that is representative of a system or process that we are trying to
    model
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些代表我们试图建模的系统或过程的代表性数据
- en: A concept that describes the model's goodness of fit
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个描述模型拟合优度的概念
- en: A method to update the parameters to improve the model's goodness of fit
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种更新参数以改进模型拟合优度的方法
- en: 'As we''ll see in this book, most models, such as neural networks, linear regression,
    and support vector machines, have certain parameterized equations that describe
    them. Let''s look at a linear model attempting to predict the output, *Y*, from
    three input features, which we will call *X[1]*, *X[2]*, and *X[3]*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本书中看到的，大多数模型，如神经网络、线性回归和支持向量机，都有某些参数化方程来描述它们。让我们看看一个线性模型，它试图从三个输入特征（我们将它们称为
    *X[1]*、*X[2]* 和 *X[3]*）预测输出 *Y*：
- en: '![The core components of a model](img/00006.jpeg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![模型的核心理念](img/00006.jpeg)'
- en: This model has exactly one equation describing it and this equation provides
    the linear structure of the model. The equation is parameterized by four parameters,
    known as coefficients in this case, and they are the four *β* parameters. In the
    next chapter, we will see exactly what roles these play, but for this discussion,
    it is important to note that a linear model is an example of a parameterized model.
    The set of parameters is typically much smaller than the amount of data available.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型恰好有一个方程来描述它，这个方程提供了模型的线性结构。该方程由四个参数参数化，在这种情况下称为系数，它们是四个 *β* 参数。在下一章中，我们将看到这些参数的确切作用，但在此讨论中，重要的是要注意线性模型是参数化模型的一个例子。参数集通常比可用的数据量小得多。
- en: Given a set of equations and some data, we then talk about training the model.
    This involves assigning values to the model's parameters so that the model describes
    the data more accurately. We typically employ certain standard measures that describe
    a model's goodness of fit to the data, which is how well the model describes the
    training data. The training process is usually an iterative procedure that involves
    performing computations on the data so that new values for the parameters can
    be computed in order to increase the model's goodness of fit. For example, a model
    can have an objective or error function. By differentiating this and setting it
    to zero, we can find the combination of parameters that gives us the minimum error.
    Once we finish this process, we refer to the model as a trained model and say
    that the model has learned from the data. These terms are derived from the machine
    learning literature, although there is often a parallel made with statistics,
    a field that has its own nomenclature for this process. We will mostly use terms
    from machine learning in this book.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组方程和一些数据，我们接下来讨论训练模型。这涉及到为模型的参数赋值，以便模型能更准确地描述数据。我们通常采用某些标准度量来描述模型对数据的拟合优度，即模型描述训练数据的好坏。训练过程通常是一个迭代过程，涉及到对数据进行计算，以便可以计算参数的新值，从而提高模型的拟合优度。例如，模型可以有一个目标或误差函数。通过对该函数求导并令其等于零，我们可以找到一组参数组合，它给我们带来最小的误差。一旦我们完成这个过程，我们就称该模型为训练好的模型，并说该模型已经从数据中学习到了。这些术语来自机器学习文献，尽管通常与统计学领域（该领域有自己的术语）进行比较。在这本书中，我们将主要使用机器学习的术语。
- en: Our first model – k-nearest neighbors
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的第一个模型 - k最近邻
- en: 'In order to put some of the ideas in this chapter into perspective, we will
    present our first model for this book, **k-nearest neighbors**, which is commonly
    abbreviated as **kNN**. In a nutshell, this simple approach actually avoids building
    an explicit model to describe how the features in our data combine to produce
    a target function. Instead, it relies on the notion that, if we are trying to
    make a prediction on a data point that we have never seen before, we will look
    inside our original training data and find the *k* observations that are most
    similar to our new data point. We can then use some kind of averaging technique
    on the known value of the target function for these **k neighbors** to compute
    a prediction. Let''s use our iris dataset to understand this by way of an example.
    Suppose that we collect a new unidentified sample of an iris flower with the following
    measurements:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使本章的一些想法更清晰，我们将介绍本书的第一个模型，**k最近邻**，通常缩写为**kNN**。简而言之，这种方法实际上避免了构建一个显式的模型来描述我们数据中的特征是如何组合以产生目标函数的。相反，它依赖于这样的观点：如果我们试图对从未见过的数据点进行预测，我们将查看原始训练数据，并找到与我们的新数据点最相似的*k*个观测值。然后，我们可以使用某种平均技术对这些**k个邻居**的目标函数已知值进行计算，以得出预测。让我们通过一个例子来理解这一点。假设我们收集到一个新的未识别的鸢尾花样本，其测量值如下：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We would like to use the kNN algorithm in order to predict which species of
    flower we should use to identify our new sample. The first step in using the kNN
    algorithm is to determine the k-nearest neighbors of our new sample. In order
    to do this, we will have to give a more precise definition of what it means for
    two observations to be similar to each other. A common approach is to compute
    a numerical distance between two observations in the feature space. The intuition
    is that two observations that are similar will be close to each other in the feature
    space and therefore, the distance between them will be small. To compute the distance
    between two observations in the feature space, we often use the **Euclidean distance**,
    which is the length of a straight line between two points. The Euclidean distance
    between two observations, *x[1]* and *x[2]*, is computed as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用kNN算法来预测我们应该使用哪种花卉种类来识别我们的新样本。使用kNN算法的第一步是确定新样本的k个最近邻。为了做到这一点，我们不得不给出一个更精确的定义，即两个观测值彼此相似意味着什么。一种常见的方法是在特征空间中计算两个观测值之间的数值距离。直观上，相似的两个观测值在特征空间中会彼此靠近，因此它们之间的距离会很小。为了计算特征空间中两个观测值之间的距离，我们通常使用**欧几里得距离**，这是两点之间直线段的长度。两个观测值*x[1]*和*x[2]*之间的欧几里得距离计算如下：
- en: '![Our first model – k-nearest neighbors](img/00007.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![我们的第一个模型 - k最近邻](img/00007.jpeg)'
- en: Recall that the second suffix, *j*, in the preceding formula corresponds to
    the *j^(th)* feature. So, what this formula is essentially telling us is that
    for every feature, take the square of the difference in values of the two observations,
    sum up all these squared differences, and then take the square root of the result.
    There are many other possible definitions of distance, but this is one of the
    most frequently encountered in the kNN setting. We'll see more distance metrics
    in [Chapter 11](part0082_split_000.html#2E6E41-c6198d576bbb4f42b630392bd61137d7
    "Chapter 11. Topic Modeling"), *Recommendation Systems*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to find the nearest neighbors of our new sample iris flower, we''ll
    have to compute the distance to every point in the iris dataset and then sort
    the results. We''ll begin by subsetting the iris data frame to include only our
    features, thus excluding the species column, which is what we are trying to predict.
    We''ll then define our own function to compute the Euclidean distance. Next, we''ll
    use this to compute the distance to every iris observation in our data frame using
    the `apply()` function. Finally, we''ll use the `sort()` function of R with the
    `index.return` parameter set to `TRUE`, so that we also get back the indexes of
    the row numbers in our iris data frame corresponding to each distance computed:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `$x` attribute contains the actual values of the distances computed between
    our sample iris flower and the observations in the iris data frame. The `$ix`
    attribute contains the row numbers of the corresponding observations. If we want
    to find the five nearest neighbors, we can subset our original iris data frame
    using the first five entries from the `$ix` attribute as the row numbers:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see, four of the five nearest neighbors to our sample are the *versicolor*
    species, while the remaining one is the *virginica* species. For this type of
    problem where we are picking a class label, we can use a majority vote as our
    averaging technique to make our final prediction. Consequently, we would label
    our new sample as belonging to the versicolor species. Notice that setting the
    value of *k* to an odd number is a good idea, because it makes it less likely
    that we will have to contend with tie votes (and completely eliminates ties when
    the number of output labels is two).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a tie, the convention is usually to just resolve it by randomly
    picking among the tied labels. Notice that nowhere in this process have we made
    any attempt to describe how our four features are related to our output. As a
    result, we often refer to the kNN model as a **lazy learner** because essentially,
    all it has done is memorize the training data and use it directly during a prediction.
    We'll have more to say about our kNN model, but first we'll return to our general
    discussion on models and discuss different ways to classify them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Types of model
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With a broad idea of the basic components of a model, we are ready to explore
    some of the common distinctions that modelers use to categorize different models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在对模型的基本组成部分有一个广泛了解之后，我们准备探索一些模型师用来对不同的模型进行分类的常见区别。
- en: Supervised, unsupervised, semi-supervised, and reinforcement learning models
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习、无监督学习、半监督学习和强化学习模型
- en: We've already looked at the iris dataset, which consists of four features and
    one output variable, namely the species variable. Having the output variable available
    for all the observations in the training data is the defining characteristic of
    the **supervised learning** setting, which represents the most frequent scenario
    encountered. In a nutshell, the advantage of training a model under the supervised
    learning setting is that we have the correct answer that we should be predicting
    for the data points in our training data. As we saw in the previous section, kNN
    is a model that uses supervised learning, because the model makes its prediction
    for an input point by combining the values of the output variable for a small
    number of neighbors to that point. In this book, we will primarily focus on supervised
    learning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了鸢尾花数据集，它包含四个特征和一个输出变量，即物种变量。在训练数据中，所有观测值都可用输出变量是**监督学习**设置的标志性特征，这代表了最常遇到的场景。简而言之，在监督学习设置下训练模型的优点是我们有正确的答案，这是我们应为训练数据中的数据点预测的。正如我们在上一节中看到的，kNN
    是一个使用监督学习的模型，因为它通过结合该点附近少数邻居的输出变量值来对输入点进行预测。在这本书中，我们将主要关注监督学习。
- en: Using the availability of the value of the output variable as a way to discriminate
    between different models, we can also envisage a second scenario in which the
    output variable is not specified. This is known as the **unsupervised learning**
    setting. An unsupervised version of the iris dataset would consist of only the
    four features. If we don't have the species output variable available to us, then
    we clearly have no idea as to which species each observation refers to. Indeed,
    we won't know how many species of flower are represented in the dataset, or how
    many observations belong to each species. At first glance, it would seem that
    without this information, no useful predictive task could be carried out. In fact,
    what we can do is examine the data and create groups of observations based on
    how similar they are to each other, using the four features available to us. This
    process is known as **clustering**. One benefit of clustering is that we can discover
    natural groups of data points in our data; for example, we might be able to discover
    that the flower samples in an unsupervised version of our iris set form three
    distinct groups that correspond to three different species.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用输出变量值的可用性作为区分不同模型的方式，我们还可以设想第二种场景，其中输出变量未指定。这被称为**无监督学习**设置。无监督版本的鸢尾花数据集将只包含四个特征。如果我们没有可用的物种输出变量，那么我们显然不知道每个观测值指的是哪种物种。实际上，我们甚至不知道数据集中有多少种花卉物种，或者每种物种有多少观测值。乍一看，似乎没有这些信息，就无法执行任何有用的预测任务。事实上，我们可以检查数据，并基于我们可用的四个特征，根据观测值之间的相似性创建观测值组。这个过程被称为**聚类**。聚类的优点之一是我们可以在数据中发现自然的数据点组；例如，我们可能能够发现我们的鸢尾花集的无监督版本中的花样本形成了三个不同的组，分别对应三种不同的物种。
- en: Between unsupervised and supervised methods, which are two absolutes in terms
    of the availability of the output variable, reside the **semi-supervised** and
    **reinforcement learning** settings. Semi-supervised models are built using data
    for which a (typically quite small) fraction contains the values for the output
    variable, while the rest of the data is completely unlabeled. Many such models
    first use the labeled portion of the dataset in order to train the model coarsely,
    then incorporate the unlabeled data by projecting labels predicted by the model
    trained up this point.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督方法和监督方法之间，这两种方法在输出变量的可用性方面是两种绝对的区别，存在着**半监督学习**和**强化学习**设置。半监督模型是使用那些包含输出变量值的数据构建的，其中通常只有一小部分数据包含这些值，而其余的数据则完全未标记。许多这样的模型首先使用数据集的有标签部分来粗略地训练模型，然后通过将模型在此点之前预测的标签投影到未标记数据中，来包含这些未标记数据。
- en: In a reinforcement learning setting the output variable is not available, but
    other information that is directly linked with the output variable is provided.
    One example is predicting the next best move to win a chess game, based on data
    from complete chess games. Individual chess moves do not have output values in
    the training data, but for every game, the collective sequence of moves for each
    player resulted in either a win or a loss. Due to space constraints, semi-supervised
    and reinforcement settings aren't covered in this book.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习环境中，输出变量不可用，但提供了与输出变量直接相关的其他信息。一个例子是根据完整棋局的数据预测下一步的最佳走法。在训练数据中，单个棋步没有输出值，但对于每一场比赛，每个玩家的集体走法序列最终导致胜利或失败。由于篇幅限制，本书没有涵盖半监督和强化学习设置。
- en: Parametric and nonparametric models
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数和非参数模型
- en: 'In a previous section, we noted how most of the models we will encounter are
    **parametric models**, and we saw an example of a simple linear model. Parametric
    models have the characteristic that they tend to define a **functional form**.
    This means that they reduce the problem of selecting between all possible functions
    for the target function to a particular family of functions that form a parameter
    set. Selecting the specific function that will define the model essentially involves
    selecting precise values for the parameters. So, returning to our example of a
    three-feature linear model, we can see that we have the two following possible
    choices of parameters (the choices are infinite, of course; here we just demonstrate
    two specific ones):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个部分，我们提到了我们将遇到的大多数模型都是**参数模型**，并看到了一个简单线性模型的例子。参数模型的特点是它们倾向于定义**函数形式**。这意味着它们将选择目标函数所有可能函数的问题简化为特定函数族，该函数族形成一个参数集。选择将定义模型的特定函数本质上涉及选择参数的精确值。因此，回到我们三个特征线性模型的例子，我们可以看到我们有以下两种可能的参数选择（当然，选择是无限的；这里我们只演示两个具体的例子）：
- en: '![Parametric and nonparametric models](img/00008.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参数和非参数模型](img/00008.jpeg)'
- en: Here, we have used a subscript on the output *Y* variable to denote the two
    different possible models. Which of these might be a better choice? The answer
    is that it depends on the data. If we apply each of our models on the observations
    in our dataset, we will get the predicted output for every observation. With supervised
    learning, every observation in our training data is labeled with the correct value
    of the output variable. To assess our model's goodness of fit, we can define an
    error function that measures the degree to which our predicted outputs differ
    from the correct outputs. We then use this to pick between our two candidate models
    in this case, but more generally to iteratively improve a model by moving through
    a sequence of progressively better candidate models.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用输出变量 *Y* 的下标来表示两种不同的可能模型。哪一种可能更好？答案是这取决于数据。如果我们将我们的每个模型应用于数据集中的观测值，我们将为每个观测值得到预测输出。在监督学习中，我们训练数据集中的每个观测值都带有输出变量的正确值。为了评估我们模型拟合的好坏，我们可以定义一个误差函数，该函数衡量我们的预测输出与正确输出之间的差异程度。然后我们使用这个函数在这两种候选模型之间进行选择，但更普遍的是通过一系列逐渐更好的候选模型来迭代改进模型。
- en: Some parametric models are more flexible than linear models, meaning that they
    can be used to capture a greater variety of possible functions. Linear models,
    which require that the output be a linearly weighted combination of the input
    features, are considered strict. We can intuitively see that a more flexible model
    is more likely to allow us to approximate our input data with greater accuracy;
    however, when we look at overfitting, we'll see that this is not always a good
    thing. Models that are more flexible also tend to be more complex and, thus, training
    them often proves to be harder than training less flexible models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一些参数模型比线性模型更灵活，这意味着它们可以用来捕捉更多可能的函数。线性模型要求输出是输入特征的线性加权组合，被认为是严格的。我们可以直观地看到，更灵活的模型更有可能让我们以更高的精度近似输入数据；然而，当我们看到过拟合时，我们会看到这并不总是好事。更灵活的模型也往往更复杂，因此训练它们通常比训练不那么灵活的模型更困难。
- en: Models are not necessarily parameterized, in fact, the class of models that
    have no parameters is known (unsurprisingly) as **nonparametric models**. Nonparametric
    models generally make no assumptions on the particular form of the output function.
    There are different ways of constructing a target function without parameters.
    **Splines** are a common example of a nonparametric model. The key idea behind
    splines is that we envisage the output function, whose form is unknown to us,
    as being defined exactly at the points that correspond to all the observations
    in our training data. Between the points, the function is locally interpolated
    using smooth polynomial functions. Essentially, the output function is built in
    a piecewise manner in the space between the points in our training data. Unlike
    most scenarios, splines will guarantee 100% accuracy on the training data, whereas
    it is perfectly normal to have some errors in our training data. Another good
    example of a nonparametric model is the k-nearest neighbor algorithm that we've
    already seen.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型不一定需要参数化，实际上，没有参数的模型类别（不出所料）被称为**非参数模型**。非参数模型通常不对输出函数的特定形式做出假设。有不同方式构建没有参数的目标函数。**样条函数**是非参数模型的一个常见例子。样条函数背后的关键思想是我们设想输出函数，其形式对我们来说是未知的，它在对应于我们训练数据中所有观察点的点上被精确地定义。在点之间，函数通过使用平滑的多项式函数进行局部插值。本质上，输出函数是在我们的训练数据点之间的空间中以分段方式构建的。与大多数情况不同，样条函数将保证在训练数据上达到100%的准确率，而我们的训练数据中存在一些错误是完全正常的。另一个很好的非参数模型例子是我们已经看到的k-最近邻算法。
- en: Regression and classification models
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归和分类模型
- en: The distinction between **regression** and **classification** models has to
    do with the type of output we are trying to predict, and is generally relevant
    to supervised learning. Regression models try to predict a numerical or quantitative
    value, such as the stock market index, the amount of rainfall, or the cost of
    a project. Classification models try to predict a value from a finite (though
    still possibly large) set of classes or categories. Examples of this include predicting
    the topic of a website, the next word that will be typed by a user, a person's
    gender, or whether a patient has a particular disease given a series of symptoms.
    The majority of models that we will study in this book fall quite neatly into
    one of these two categories, although a few, such as neural networks, can be adapted
    to solve both types of problem. It is important to stress here that the distinction
    made is on the output only, and not on whether the feature values that are used
    to predict the output are quantitative or qualitative themselves. In general,
    features can be encoded in a way that allows both qualitative and quantitative
    features to be used in regression and classification models alike. Earlier, when
    we built a kNN model to predict the species of iris based on measurements of flower
    samples, we were solving a classification problem as our species output variable
    could take only one of three distinct labels.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**和**分类**模型之间的区别与我们要预测的输出类型有关，通常与监督学习相关。回归模型试图预测一个数值或定量值，例如股票市场指数、降雨量或项目的成本。分类模型试图从有限（尽管可能很大）的类别或类别集中预测一个值。这类例子包括预测网站的主题、用户接下来将要输入的下一个单词、一个人的性别，或者根据一系列症状预测患者是否患有特定疾病。在这本书中，我们将研究的多数模型都相当清晰地属于这两个类别之一，尽管一些模型，如神经网络，可以适应解决这两种类型的问题。在此强调，这里的区别仅在于输出，而不是用于预测输出的特征值本身是定量还是定性。一般来说，特征可以被编码成一种方式，使得定性和定量特征都可以在回归和分类模型中使用。早些时候，当我们构建一个kNN模型来根据花样本的测量值预测鸢尾花的物种时，我们解决的是一个分类问题，因为我们的物种输出变量只能取三个不同的标签之一。'
- en: The kNN approach can also be used in a regression setting; in this case, the
    model combines the numerical values of the output variable for the selected nearest
    neighbors by taking the mean or median in order to make its final prediction.
    Thus, kNN is also a model that can be used in both regression and classification
    settings.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: kNN方法也可以用于回归设置；在这种情况下，模型通过取平均值或中位数来结合所选最近邻的输出变量的数值，以便做出最终的预测。因此，kNN也是一个可以在回归和分类设置中使用的模型。
- en: Real-time and batch machine learning models
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时和批量机器学习模型
- en: Predictive models can use **real-time machine learning** or they can involve
    **batch learning**. The term real-time machine learning can refer to two different
    scenarios, although it certainly does not refer to the idea that real-time machine
    learning involves making a prediction in real time, that is, within a predefined
    time limit that is typically small. For example, once trained, a neural network
    model can produce its prediction of the output using only a few computations (depending
    on the number of inputs and network layers). This is not, however, what we mean
    when we talk about real-time machine learning.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 预测模型可以使用**实时机器学习**，也可以涉及**批量学习**。实时机器学习的术语可以指两种不同的场景，尽管它当然不是指实时机器学习涉及在实时内做出预测，即在通常较小的时间限制内做出预测。例如，一旦训练好，一个神经网络模型只需进行少量计算（取决于输入数量和网络层数）就能产生其输出预测。但这并不是我们谈论实时机器学习时所指的是什么。
- en: A good example of a model that uses real-time machine learning is a weather
    predictor that uses a stream of incoming readings from various meteorological
    instruments. Here, the real-time aspect of the model refers to the fact that we
    are taking only a recent window of readings in order to predict the weather. The
    further we go back in time, the less relevant the readings will be and we can,
    thus, choose to use only the latest information in order to make our prediction.
    Of course, models that are to be used in a real-time setting must also be able
    to compute their predictions quickly—it is not of much use if it takes hours for
    a system taking measurements in the morning to compute a prediction for the evening,
    as by the time the computation ends, the prediction won't be of much value.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用实时机器学习的良好例子是使用来自各种气象仪器的实时读数流进行天气预报的模型。在这里，模型的实时性指的是我们只取最近的一个读数窗口来预测天气。时间越往回推，读数的相关性就越低，因此我们可以选择只使用最新信息来做出预测。当然，用于实时环境的模型也必须能够快速计算其预测——如果早晨进行测量的系统需要数小时才能计算出晚上的预测，那么这几乎没有什么用处，因为当计算结束时，预测已经没有多少价值了。
- en: When talking about models that take into account information obtained over a
    recent time frame to make a prediction, we generally refer to models that have
    been trained on data that is assumed to be representative of all the data for
    which the model will be asked to make a prediction in the future. A second interpretation
    of real-time machine learning arises when we describe models that detect that
    the properties of the process being modeled have shifted in some way. We will
    focus on examples of the first kind in this book when we look at time series models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论考虑近期获得的信息来做出预测的模型时，我们通常指的是那些在假设代表未来模型将被要求做出预测的所有数据上训练过的模型。当我们描述检测到被建模的过程的性质以某种方式发生变化时，实时机器学习的第二种解释就出现了。当我们查看时间序列模型时，本书将重点关注第一种类型的例子。
- en: The process of predictive modeling
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测建模的过程
- en: By looking at some of the different characterizations of models, we've already
    hinted at various steps of the predictive modeling process. In this section, we
    will present these steps in a sequence and make sure we understand how each of
    these contributes to the overall success of the endeavor.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看一些不同的模型特征，我们已经暗示了预测建模过程中的各种步骤。在本节中，我们将按顺序介绍这些步骤，并确保我们理解每个步骤如何有助于整个工作的成功。
- en: Defining the model's objective
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义模型的客观目标
- en: In a nutshell, the first step of every project is to figure out precisely what
    the desired outcome is, as this will help steer us to make good decisions throughout
    the course of the project. In a predictive analytics project, this question involves
    drilling into the type of prediction that we want to make and understanding the
    task in detail. For example, suppose we are trying to build a model that predicts
    employee churn for a company. We first need to define this task precisely, while
    trying to avoid making the problem overly broad or overly specific. We could measure
    churn as the percentage of new full time hires that defect from the company within
    their first six months. Notice that once we properly define the problem, we have
    already made some progress in thinking about what data we will have to work with.
    For example, we won't have to collect data from part-time contractors or interns.
    This task also means that we should collect data from our own company only, but
    at the same time recognize that our model might not necessarily be applicable
    to make predictions for the workforce of a different company. If we are only interested
    in churn, it also means that we won't need to make predictions about employee
    performance or sick days (although it wouldn't hurt to ask the person for whom
    we are building the model, to avoid surprises in the future).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，每个项目的第一步是精确地确定期望的结果，因为这有助于我们在项目过程中做出良好的决策。在预测分析项目中，这个问题涉及到深入探讨我们想要进行的预测类型，并详细了解任务。例如，假设我们正在尝试构建一个预测公司员工流失率的模型。我们首先需要精确地定义这个任务，同时尽量避免使问题过于宽泛或过于具体。我们可以将流失率衡量为新全职员工在公司前六个月内离职的百分比。请注意，一旦我们正确地定义了问题，我们已经在思考我们将要使用的数据方面取得了一些进展。例如，我们不需要从兼职承包商或实习生那里收集数据。这项任务还意味着我们应该只从我们自己的公司收集数据，同时认识到我们的模型可能并不一定适用于为不同公司的员工群体做出预测。如果我们只对流失率感兴趣，这也意味着我们不需要预测员工的表现或病假（尽管避免未来的惊喜，询问我们为之人构建模型的人是有益的）。
- en: Once we have a precise enough idea of the model we want to build, the next logical
    question to ask is what sort of performance we are interested in achieving, and
    how we will measure this. That is to say, we need to define a performance metric
    for our model and then a minimum threshold of acceptable performance. We will
    go into substantial detail on how to assess the performance of models in this
    book. For now, we want to emphasize that, although it is not unusual to talk about
    assessing the performance of a model after we have trained it on some data, in
    practice it is important to remember that defining the expectations and performance
    target for our model is something that a predictive modeler should discuss with
    the stakeholders of a project at the very beginning. Models are never perfect
    and it is easy to spiral into a mode of forever trying to improve performance.
    Clear performance goals are not only useful in guiding us to decide which methods
    to use, but also in knowing when our model is good enough.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对想要构建的模型有了足够精确的想法，下一个合乎逻辑的问题是要问我们感兴趣实现什么样的性能，以及我们将如何衡量这一点。也就是说，我们需要为我们的模型定义一个性能指标，然后定义一个可接受的最低性能阈值。本书将详细讨论如何评估模型性能。现在，我们想要强调的是，尽管在用一些数据训练模型后讨论评估模型性能并不罕见，但在实践中，记住定义我们模型的期望和性能目标是预测模型师在项目初期就应该与项目利益相关者讨论的事情。模型永远不会完美，很容易陷入永远试图提高性能的模式。明确的目标性能不仅有助于我们决定使用哪些方法，而且有助于我们知道何时我们的模型已经足够好。
- en: Finally, we also need to think about the data that will be available to us when
    the time comes to collect it, and the context in which the model will be used.
    For example, suppose we know that our employee churn model will be used as one
    of the factors that determine whether a new applicant in our company will be hired.
    In this context, we should only collect data from our existing employees who were
    available before they were hired. We cannot use the result of their first performance
    review, as this data won't be available for a prospective applicant.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还需要考虑在收集数据时我们将能够获得的数据，以及模型将被使用的上下文。例如，假设我们知道我们的员工流失率模型将作为决定我们公司新申请人是否被录用的因素之一。在这种情况下，我们应该只收集在我们招聘之前就可供使用的现有员工的数据。我们不能使用他们的第一次绩效评估结果，因为这项数据不会对潜在申请人可用。
- en: Collecting the data
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集数据
- en: Training a model to make predictions is often a data-intensive venture, and
    if there is one thing that you can never have too much of in this business, it
    is data. Collecting the data can often be the most time- and resource-consuming
    part of the entire process, which is why it is so critical that the first step
    of defining the task and identifying the right data to be collected is done properly.
    When we learn about how a model such as logistic regression works we often do
    this by way of an example dataset and this is largely the approach we'll follow
    in this book. Unfortunately, we don't have a way to simulate the process of collecting
    the data, and it may seem that most of the effort is spent on training and refining
    a model. When learning about models using existing datasets, we should bear in
    mind that a lot of effort has usually gone into collecting, curating, and preprocessing
    the data. We will look at data preprocessing more closely in a subsequent section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个模型进行预测通常是一个数据密集型的项目，在这个行业中，如果你有什么东西永远都不嫌多，那就是数据。收集数据往往是整个过程中耗时和资源消耗最多的部分，这就是为什么确保定义任务和确定要收集的正确数据的第一步得到妥善处理是如此关键。当我们了解像逻辑回归这样的模型是如何工作时，我们通常是通过一个示例数据集来做到的，这也是本书我们将遵循的主要方法。不幸的是，我们没有一种方法来模拟收集数据的过程，可能会给人一种大部分努力都花在训练和改进模型上的印象。当我们使用现有数据集了解模型时，我们应该记住，通常已经投入了大量努力来收集、整理和预处理数据。我们将在下一节更详细地探讨数据预处理。
- en: While we are collecting data, we should always keep in mind whether we are collecting
    the right kind of data. Many of the sanity checks that we perform on data during
    preprocessing also apply during collection, in order for us to spot whether we
    have made a mistake early on in the process. For example, we should always check
    that we measure features correctly and in the right units. We should also make
    sure that we collect data from sources that are sufficiently recent, reliable,
    and relevant to the task at hand. In the employee churn model we described in
    the previous section, as we collect information about past employees we should
    ensure that we are consistent in measuring our features. For example, when measuring
    how many days a person has been working in our company, we should consistently
    use either calendar days or business days. We must also check that when collecting
    dates, such as when a person joined or left the company, we invariably either
    use the US format (month followed by day) or the European format (day followed
    by month) and do not mix the two, otherwise a date like 03/05/2014 will be ambiguous.
    We should also try to get information from as broad a sample as possible and not
    introduce a hidden bias in our data collection. For example, if we wanted a general
    model for employee churn, we would not want to collect data from only female employees
    or employees from a single department.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集数据的过程中，我们应始终牢记我们是否在收集正确类型的数据。我们在数据预处理期间对数据进行的大量合理性检查也适用于收集过程中，以便我们能够及早发现过程中是否犯了错误。例如，我们应该始终检查我们是否正确测量了特征以及是否使用了正确的单位。我们还应确保我们从足够新、可靠且与当前任务相关的来源收集数据。在我们之前章节中描述的员工流失模型中，当我们收集关于过去员工的信息时，我们应该确保我们在测量特征方面的一致性。例如，当我们测量一个人在我们公司工作了多少天时，我们应该始终一致地使用日历日或工作日。我们还必须检查在收集日期时，例如一个人加入或离开公司时，我们始终要么使用美国格式（月/日），要么使用欧洲格式（日/月），并且不要混合两种格式，否则像03/05/2014这样的日期将会模糊不清。我们还应尽可能从尽可能广泛的样本中获取信息，并在数据收集过程中避免引入隐藏的偏差。例如，如果我们想建立一个关于员工流失的通用模型，我们就不想只从女性员工或单一部门的员工那里收集数据。
- en: How do we know when we have collected enough data? Early on when we are collecting
    the data and have not built and tested any model, it is impossible to tell how
    much data we will eventually need, and there aren't any simple rules of thumb
    that we can follow. We can, however, anticipate that certain characteristics of
    our problem will require more data. For example, when building a classifier that
    will learn to predict from one of three classes, we may want to check whether
    we have enough observations representative of each class.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道我们已经收集了足够的数据？在早期收集数据时，如果我们还没有构建和测试任何模型，我们无法知道我们最终需要多少数据，也没有任何简单的经验法则可以遵循。然而，我们可以预测，我们问题的某些特征将需要更多的数据。例如，当我们构建一个将学会从三个类别之一进行预测的分类器时，我们可能想检查我们是否有了足够代表每个类别的观察结果。
- en: The greater the number of output classes we have, the more data we will need
    to collect. Similarly, for regression models, it is also useful to check that
    the range of the output variable in the training data corresponds to the range
    that we would like to predict. If we are building a regression model that covers
    a large output range, we will also need to collect more data compared to a regression
    model that covers a smaller output range under the same accuracy requirements.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们拥有的输出类别越多，就需要收集更多的数据。同样，对于回归模型，检查训练数据中输出变量的范围是否与我们想要预测的范围相符也是有用的。如果我们正在构建一个覆盖较大输出范围的回归模型，与在相同精度要求下覆盖较小输出范围的回归模型相比，我们也需要收集更多的数据。
- en: Another important factor to help us estimate how much data we will need, is
    the desired model performance. Intuitively, the higher the accuracy that we need
    for our model, the more data we should collect. We should also be aware that improving
    model performance is not a linear process. Getting from 90 to 95% accuracy can
    often require more effort and a lot more data, compared to making the leap from
    70 to 90%. Models that have fewer parameters or are simpler in their design, such
    as linear regression models, often tend to need less data than more complex models
    such as neural networks. Finally, the greater the number of features that we want
    to incorporate into our model, the greater the amount of data we should collect.
    In addition, we should be aware of the fact that this requirement for additional
    data is also not going to be linear. That is to say, building a model with twice
    the number of features often requires much more than twice the amount of original
    data. This should be readily apparent, if we think of the number of different
    combinations of inputs our model will be required to handle. Adding twice the
    number of dimensions results in far more than twice the number of possible input
    combinations. To understand this, suppose we have a model with three input features,
    each of which takes 10 possible values. We have *10³* *= 1000* possible input
    combinations. Adding a single extra feature that also takes 10 values raises this
    to 10,000 possible combinations, which is much more than twice the number of our
    initial input combinations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个帮助我们估计需要多少数据的重要因素是期望的模型性能。直观地讲，我们需要的模型精度越高，就应该收集更多的数据。我们还应该意识到，提高模型性能不是一个线性过程。从90%的准确率提升到95%，通常需要更多的努力和更多的数据，与从70%提升到90%相比，这种跨越需要更多的努力和数据。具有较少参数或设计更简单的模型，例如线性回归模型，通常比更复杂的模型，如神经网络，需要的数据更少。最后，我们想要将更多特征纳入模型，就应该收集更多的数据。此外，我们还应该意识到，这种对额外数据的需求也不是线性的。也就是说，构建具有两倍特征数量的模型，通常需要的原始数据量远不止两倍。如果我们考虑模型需要处理的不同输入组合的数量，这一点应该很容易理解。增加两倍的维度会导致可能的输入组合数量远超过两倍。为了理解这一点，假设我们有一个具有三个输入特征的模型，每个特征有10个可能的值。我们有10³=1000个可能的输入组合。增加一个额外的特征，该特征也有10个值，将组合数量提升到10,000，这比我们初始输入组合的数量多得多。
- en: There have been attempts to obtain a more quantifiable view of whether we have
    enough data for a particular dataset, but we will not have time to cover them
    in this book. A good place to start learning more about this area of predictive
    modeling is to study **learning curves**. In a nutshell, with this approach we
    build consecutive models on the same dataset by starting off with a small portion
    of the data and successively adding more. The idea is that if throughout this
    process the predictive accuracy on testing data always improves without tapering
    off, we probably could benefit from obtaining more data. As a final note for the
    data collection phase, even if we think we have enough data, we should always
    consider how much it would cost us (in terms of time and resources) in order to
    get more data, before making the choice to stop collecting and begin modeling.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 人们尝试过获取一个更量化的视角来判断我们是否为特定数据集收集了足够的数据，但在这本书中我们没有时间涵盖这些内容。学习更多关于预测建模这一领域的好方法是从研究**学习曲线**开始。简而言之，我们通过从数据的一小部分开始，并在数据集上连续构建模型，逐步添加更多数据来构建模型。其理念是，如果在整个过程中，测试数据的预测精度始终在提高而没有下降，那么我们可能从获取更多数据中受益。作为数据收集阶段的最后一点，即使我们认为我们已经有了足够的数据，在决定停止收集并开始建模之前，我们也应该考虑获取更多数据将花费我们多少（以时间和资源衡量）。
- en: Picking a model
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择模型
- en: Once we are clear on the prediction task, and we have the right kind of data,
    the next step is to pick our first model. To begin with, there is no best model
    overall, not even a best model using a few rules of thumb. In most cases, it makes
    sense to start off with a simple model, such as a Naïve Bayes model or a logistic
    regression in the case of a classification task, or a linear model in the case
    of regression. A simple model will give us a starting baseline performance, which
    we can then strive to improve. A simple model to start off with might also help
    in answering useful questions, such as how each feature contributes to the result,
    that is, how important is each feature and is the relationship with the output
    positively or negatively correlated. Sometimes, this kind of analysis itself warrants
    the production of a simple model first, followed by a more complex one, which
    will be used for the final prediction.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes a simple model might give us enough accuracy for the task at hand
    so that we won''t need to invest more effort in order to give us a little bit
    extra. On the other hand, a simple model will often end up being inadequate for
    the task, requiring us to pick something more complicated. Choosing a more complex
    model over a simpler one is not always a straightforward decision, even if we
    can see that the accuracy of the complex model will be much better. Certain constraints,
    such as the number of features we have or the availability of data, may prevent
    us from moving to a more complex model. Knowing how to choose a model involves
    understanding the various strengths and limitations of the models in our toolkit.
    For every model we encounter in this book, we will pay particular attention to
    learning these points. In a real-world project, to help guide our decision, we
    often go back to the task requirements and ask a few questions, such as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: What type of task do we have? Some models are only suited for particular tasks
    such as regression, classification, or clustering.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the model need to explain its predictions? Some models, such as decision
    trees, are better at giving insights that are easily interpretable to explain
    why they made a particular prediction.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we have any constraints on prediction time?
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to update the model frequently and is training time, therefore, important?
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the model work well if we have highly correlated features?
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the model scale well for the number of features and amount of data that
    we have available? If we have massive amounts of data, we may need a model whose
    training procedure can be parallelized to take advantage of parallel computer
    architectures, for example.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, even if our first analysis points towards a particular model, we
    will most likely want to try out a number of options before making our final decision.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing the data
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can use our data to train our model, we typically need to preprocess
    it. In this section, we will discuss a number of common preprocessing steps that
    we usually perform. Some of these are necessary in order to detect and resolve
    problems in our data, while others are useful in order to transform our data and
    make them applicable to the model we have chosen.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够使用数据来训练模型之前，我们通常需要对其进行预处理。在本节中，我们将讨论我们通常执行的一些常见预处理步骤。其中一些是为了检测和解决我们数据中的问题所必需的，而其他一些则是为了转换我们的数据，使它们适用于我们选择的模型。
- en: Exploratory data analysis
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Once we have some data and have decided to start working on a particular model,
    the very first thing we'll want to do is to look at the data itself. This is not
    necessarily a very structured part of the process; it mostly involves understanding
    what each feature measures and getting a sense of the data we have collected.
    It is really important to understand what each feature represents and the units
    in which it is measured. It is also a really good idea to check the consistent
    use of units. We sometimes call this investigative process of exploring and visualizing
    our data **exploratory data analysis**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有一些数据并决定开始工作于特定的模型，我们首先想要做的就是查看数据本身。这并不一定是一个结构化的过程部分；它主要涉及理解每个特征所测量的内容，以及对我们收集到的数据的感知。真正重要的是要理解每个特征代表什么以及它的测量单位。检查单位的一致使用也是一个非常好的主意。我们有时将探索和可视化我们数据的这一调查过程称为**探索性数据分析**。
- en: An excellent practice is to use the `summary()` function of R on our data frame
    to obtain some basic metrics for each feature, such as the mean and variance,
    as well as the largest and smallest values. Sometimes, it is easy to spot that
    a mistake has been made in data collection through inconsistencies in the data.
    For example, for a regression problem, multiple observations with identical feature
    values but wildly different outputs may (depending on the application) be a signal
    that there are erroneous measurements. Similarly, it is a good idea to know whether
    there are any features that have been measured in the presence of significant
    noise. This may sometimes lead to a different choice of model or it may mean that
    the feature should be ignored.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的实践是使用 R 的 `summary()` 函数对我们的数据框进行操作，以获取每个特征的某些基本指标，例如均值和方差，以及最大值和最小值。有时，通过数据中的不一致性，我们很容易发现数据收集过程中出现了错误。例如，对于一个回归问题，具有相同特征值但输出结果差异极大的多个观测值（根据应用情况）可能是一个信号，表明存在错误的测量。同样，了解是否存在任何在存在显著噪声的情况下测量的特征也是一个好主意。这有时可能导致模型选择的不同，或者意味着该特征应该被忽略。
- en: Tip
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Another useful function used to summarize features in a data frame is the `describe()`
    function in the `psych` package. This returns information about how skewed each
    feature is, as well as the usual measures of a location (such as the mean and
    median) and dispersion (such as the standard deviation).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用于总结数据框中特征的常用函数是 `psych` 包中的 `describe()` 函数。该函数返回有关每个特征偏斜程度的信息，以及位置（如均值和中位数）和分散度（如标准差）的常规度量。
- en: An essential part of exploratory data analysis is to use plots to visualize
    our data. There is a diverse array of plots that we can use depending on the context.
    For example, we might want to create box plots of our numerical features to visualize
    ranges and quartiles. Bar plots and mosaic plots are useful to visualize the proportions
    of our data under different combinations of values for categorical input features.
    We won't go into further detail on information visualization, as this is a field
    in its own right.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析的一个基本部分是使用图表来可视化我们的数据。根据上下文，我们可以使用各种图表。例如，我们可能想要创建数值特征的箱线图来可视化范围和四分位数。条形图和马赛克图有助于可视化不同组合的类别输入特征的数值比例。我们不会进一步详细介绍信息可视化，因为这是一个独立的领域。
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'R is an excellent platform to create visualizations. The `base` R package provides
    a number of different functions to plot data. Two excellent packages to create
    more advanced plots are `lattice` and `ggplot2`. Good references for these two,
    which also cover principles used to make effective visualizations, are *Lattice:
    Multivariate Data Visualization with R* and *ggplot2: Elegant Graphics for Data
    Analysis*, both of which are published by Springer under the Use R! series.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Feature transformations
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Often, we'll find that our numerical features are measured on scales that are
    completely different from each other. For example, we might measure a person's
    body temperature in degrees Celsius, so the numerical values will typically be
    in the range of 36-38\. At the same time, we might also measure a person's white
    blood cell count per microliter of blood. This feature generally takes values
    in the thousands. If we are to use these features as an input to an algorithm,
    such as kNN, we'd find that the large values of the white blood cell count feature
    dominate the Euclidean distance calculation. We could have several features in
    our input that are important and useful for classification, but if they were measured
    on scales that produce numerical values much smaller than one thousand, we'd essentially
    be picking our nearest neighbors mostly on the basis of a single feature, namely
    the white blood cell count. This problem comes up often and applies to many models,
    not just kNN. We handle this by transforming (also referred to as scaling) our
    input features before using them in our model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll discuss three popular options for feature scaling. When we know that
    our input features are close to being normally distributed, one possible transformation
    to use is **Z-score normalization**, which works by subtracting the mean and dividing
    it by the standard deviation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature transformations](img/00009.jpeg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: '*E(x)* is the expectation or mean of *x*, and the standard deviation is the
    square root of the variance of *x*, written as *Var(x)*. Notice that as a result
    of this transformation, the new feature will be centered on a mean of zero and
    will have unit variance. Another possible transformation, which is better when
    the input is uniformly distributed, is to scale all the features and outputs so
    that they lie within a single interval, typically the unit interval *[0,1]*:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature transformations](img/00010.jpeg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'A third option is known as the **Box-Cox transformation**. This is often applied
    when our input features are highly skewed (asymmetric) and our model requires
    the input features to be normally distributed or symmetrical at the very least:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature transformations](img/00011.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'As *λ* is in the denominator, it must take a value other than zero. The transformation
    is actually defined for a zero-valued *λ*: in this case, it is given by the natural
    logarithm of the input feature, *ln(x)*. Notice that this is a parameterized transform
    and so there is a need to specify a concrete value of *λ*. There are various ways
    to estimate an appropriate value for *λ* from the data itself. Indicatively, we''ll
    mention a technique to do this, known as cross-validation, which we will encounter
    later on in this book in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Support Vector Machines*.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original reference for the Box-Cox transformation is a paper published in
    1964 by the Journal of the Royal Statistical Society, titled *An analysis of Transformations*
    and authored by *G. E. P. Box* and *D. R. Cox*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: To get a feel for how these transformations work in practice, we'll try them
    out on the `Sepal.Length` feature from our iris dataset. Before we do this, however,
    we'll introduce the first R package that we will be working with, `caret`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The `caret` package is a very useful package that has a number of goals. It
    provides a number of helpful functions that are commonly used in the process of
    predictive modeling, from data preprocessing and visualization, to feature selection
    and resampling techniques. It also features a unified interface for many predictive
    modeling functions and provides functionalities for parallel processing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The definitive reference for predictive modeling using the `caret` package is
    a book called *Applied Predictive Modeling*, written by *Max Kuhn* and *Kjell
    Johnson* and published by *Springer*. *Max Kuhn* is the principal author of the
    `caret` package itself. The book also comes with a companion website at [http://appliedpredictivemodeling.com](http://appliedpredictivemodeling.com).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'When we transform our input features on the data we use to train our model,
    we must remember that we will need to apply the same transformation to the features
    of later inputs that we will use at prediction time. For this reason, transforming
    data using the `caret` package is done in two steps. In the first step, we use
    the `preProcess()` function that stores the parameters of the transformations
    to be applied to the data, and in the second step, we use the `predict()` function
    to actually compute the transformation. We tend to use the `preProcess()` function
    only once, and then the `predict()` function every time we need to apply the same
    transformation to some data. The `preProcess()` function takes a data frame with
    some numerical values as its first input, and we will also specify a vector containing
    the names of the transformations to be applied to the `method` parameter. The
    `predict()` function then takes the output of the previous function along with
    the data we want to transform, which in the case of the training data itself may
    well be the same data frame. Let''s see all this in action:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code:**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve created three new versions of the numerical features of the iris data,
    with the difference being that in each case we used a different transformation.
    We can visualize the effects of our transformations by plotting the density of
    the `Sepal.Length` feature for each scaled data frame using the `density()` function
    and plotting the results, as shown here:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature transformations](img/00012.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Notice that the Z-score and unit interval transformations preserve the overall
    shape of the density while shifting and scaling the values, whereas the Box-Cox
    transformation also changes the overall shape, resulting in a density that is
    less skewed than the original.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Encoding categorical features
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many models, from linear regression to neural networks, require all the inputs
    to be numerical, and so we often need a way to encode categorical fields on a
    numerical scale. For example, if we have a size feature that takes values in the
    set *{small, medium, large}*, we may want to represent this with the numerical
    values 1, 2, and 3, respectively. In the case of ordered categories, such as the
    size feature just described, this mapping probably makes sense.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'The number 3 is the largest on this scale and this corresponds to the *large*
    category, which is further away from the *small* category, represented by the
    number 1 than it is from the *medium* category, represented by the value 2\. Using
    this scale is only one possible mapping, and in particular, it forces the *medium*
    category to be equidistant from the *large* and *small* categories, which may
    or may not be appropriate based on our knowledge about the specific feature. In
    the case of unordered categories, such as brands or colors, we generally avoid
    mapping them onto a single numerical scale. For example, if we mapped the set
    *{blue, green, white, red, orange}* to the numbers one through five, respectively,
    then this scale is arbitrary and there is no reason why *red* is closer to *white*
    and far from *blue*. To overcome this, we create a series of indicator features,
    *I[i]*, which take the following form:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![Encoding categorical features](img/00013.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'We need as many indicator features as we have categories, so for our color
    example, we would create five indicator features. In this case, *I[1]*, might
    be:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Encoding categorical features](img/00014.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: 'In this way, our original color feature will be mapped to five indicator features
    and for every observation, only one of these indicator features takes the value
    1 and the rest will be 0 as each observation will involve one color value in our
    original feature. Indicator features are binary features as they only take on
    two values: 0 and 1.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We may often encounter an alternative approach that uses only *n-1* binary features
    to encode *n* levels of a factor. This is done by choosing one level to be the
    reference level and it is indicated where each one of the *n-1* binary features
    takes the value 0\. This can be more economical on the number of features and
    avoids introducing a linear dependence between them, but it violates the property
    that all features are equidistant from each other.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Missing data
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, data contains missing values, where, for certain observations, some
    features were unavailable or could not properly be measured. For example, suppose
    that in our iris dataset, we lost the measurement for a particular observation's
    petal length. We would then have a missing value for this flower sample in the
    `Petal.Length` feature. Most models do not have an innate ability to handle missing
    data. Typically, a missing value appears in our data as a blank entry or the symbol
    *NA*. We should check whether missing values are actually present in our data,
    but have been erroneously assigned a value, such as *0*, which is often a very
    legitimate feature value.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Before deciding how to handle missing data, especially when our approach will
    be to simply throw away observations with missing values, we should recognize
    that the particular values that are missing might follow a pattern. Concretely,
    we often distinguish between different so-called mechanisms for missing values.
    In the ideal **Missing Completely At Random** (**MCAR**) scenario, missing values
    occur independently from the true values of the features in which they occur,
    as well as from all other features. In this scenario, if we are missing a value
    for the length of a particular iris flower petal, then this occurs independently
    from how long the flower petal actually was and the value of any other feature,
    such as whether the observation was from the *versicolor* species or the *setosa*
    species. The **Missing At Random** (**MAR**) scenario is a less ideal situation.
    Here, a missing value is independent of the true value of the feature in question,
    but may be correlated with another feature. An example of this scenario is when
    missing petal length values mostly occur in the *setosa* samples in our iris dataset,
    as long as they still occur independently of the true petal length values.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In the **Missing Not At Random** (**MNAR**) scenario, which is the most problematic
    case, there is some sort of a pattern that explains when values might be missing
    based on the true values of the feature itself. For example, if we had difficulty
    in measuring very small petal lengths and ended up with missing values as a result,
    simply removing the incomplete samples would result in a sample of observations
    with above average petal lengths, and so our data would be biased.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of ways to handle missing values, but we will not delve deeply
    into this problem in this book. In the rare cases where we have missing values,
    we will exclude them from our datasets, but be aware that in a real project, we
    would investigate the source of the missing values in order to be sure that we
    can do this safely. Another approach is to attempt to guess or impute the missing
    values. The kNN algorithm itself is one way to do this by finding the nearest
    neighbors of a sample with a missing value in one feature. This is done by using
    a distance computation that excludes the dimension that contains the missing value.
    The missing value is then computed as the mean of the values of the nearest neighbors
    in this dimension.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The interested reader can find a detailed treatment of how to handle missing
    values in *Statistical Analysis with Missing Data*, *Second Edition*, by *Roderick
    J. A. Little* and *Donald B. Rubin*, published by Wiley.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Outliers
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Outliers** are also a problem that often needs to be addressed. An outlier
    is a particular observation that is very far from the rest of the data in one
    or more of its features. In some cases, this may represent an actual rare circumstance
    that is a legitimate behavior for the system we are trying to model. In other
    cases, it may be that there has been an error in measurement. For example, when
    reporting the ages of people, a value of 110 might be an outlier, which could
    happen because of a reporting error on an actual value of 11\. It could also be
    the result of a valid, albeit extremely rare measurement. Often, the domain of
    our problem will give us a good indication of whether outliers are likely to be
    measurement errors or not, and if so, as part of preprocessing the data, we will
    often want to exclude outliers from our data completely. In [Chapter 2](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 2. Tidying Data and Measuring Performance"), *Linear Regression*, we
    will look at outlier exclusion in more detail.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Removing problematic features
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Preprocessing a dataset can also involve the decision to drop some of the features
    if we know that they will cause us problems with our model. A common example is
    when two or more features are highly correlated with each other. In R, we can
    easily compute pairwise correlations on a data frame using the `cor()` function:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we can see that the `Petal.Length` feature is very highly correlated
    with the `Petal.Width` feature, with the correlation exceeding 0.96\. The `caret`
    package offers the `findCorrelation()` function, which takes a correlation matrix
    as an input, and the optional `cutoff` parameter, which specifies a threshold
    for the absolute value of a pairwise correlation. This then returns a (possibly
    zero length) vector that shows the columns to be removed from our data frame due
    to correlation. The default setting of `cutoff` is 0.9:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: An alternative approach to removing correlation is a complete transformation
    of the entire feature space as is done in many methods for dimensionality reduction,
    such as **Principal Component Analysis** (**PCA**) and **Singular Value Decomposition**
    (**SVD**). We'll see the former shortly, and the latter we'll visit in [Chapter
    11](part0082_split_000.html#2E6E41-c6198d576bbb4f42b630392bd61137d7 "Chapter 11. Topic
    Modeling"), *Recommendation Systems*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'In a similar vein, we might want to remove features that are **linear combinations**
    of each other. By linear combination of features, we mean a sum of features where
    each feature is multiplied by a scalar constant. To see how `caret` deals with
    these, we will create a new iris data frame with two additional columns, which
    we will call `Cmb` and `Cmb.N`, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we can see, `Cmb` is a perfect linear combination of the `Sepal.Length`
    and `Petal.Width` features. `Cmb.N` is a feature that is the same as `Cmb`, but
    with some added Gaussian noise with a mean of zero and a very small standard deviation
    (*0.1*), so that the values are very close to those of `Cmb`. The `caret` package
    can detect exact linear combinations of features, though not if the features are
    noisy, using the `findLinearCombos()` function:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see, the function only suggests that we should remove the fifth feature
    (`Cmb`) from our data frame, because it is an exact linear combination of the
    first and fourth features. Exact linear combinations are rare, but can sometimes
    arise when we have a very large number of features and redundancy occurs between
    them. Both correlated features as well as linear combinations are an issue with
    linear regression models, as we shall soon see in [Chapter 2](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 2. Tidying Data and Measuring Performance"), *Linear Regression*. In
    this chapter, we'll also see a method of detecting features that are very nearly
    linear combinations of each other.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'A final issue that we''ll look at for problematic features, is the issue of
    having features that do not vary at all in our dataset, or that have near zero
    variance. For some models, having these types of feature does not cause us problems.
    For others, it may create problems and we''ll demonstrate why this is the case.
    As in the previous example, we''ll create a new iris data frame, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `ZV` column has the constant number of `6.5` for all observations. The
    `Yellow` column is a fictional column that records whether an observation had
    some yellow color on the petal. All the observations, except the first, are made
    to have this feature set to `FALSE` and so this is a near zero variance column.
    The `caret` package uses a definition of near zero variance that checks whether
    the number of unique values that a feature takes as compared to the overall number
    of observations is very small, or whether the ratio of the most common value to
    the second most common value (referred to as the frequency ratio) is very high.
    The `nearZeroVar()` function applied to a data frame returns a vector containing
    the features that have zero or near zero variance. By setting the `saveMetrics`
    parameter to `TRUE`, we can see more information about the features in our data
    frame:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we can see that the `ZV` column has been identified as a zero variance
    column (which is also by definition a near zero variance column). The `Yellow`
    column does have a nonzero variance, but its high frequency ratio and low unique
    value percentage make it a near zero variance column. In practice, we tend to
    remove zero variance columns, as they don't have any information to give to our
    model. Removing near zero variance columns, however, is tricky and should be done
    with care. To understand this, consider the fact that a model for species prediction,
    using our newer iris dataset, might learn that if a sample has yellow in its petals,
    then regardless of all other predictors, we would predict the *setosa* species,
    as this is the species that corresponds to the only observation in our entire
    dataset that had the color yellow in its petals. This might indeed be true in
    reality, in which case, the yellow feature is informative and we should keep it.
    On the other hand, the presence of the color yellow on iris petals may be completely
    random and non-indicative of species, but also an extremely rare event. This would
    explain why only one observation in our dataset had the yellow color in its petals.
    In this case, keeping the feature is dangerous because of the aforementioned conclusion.
    Another potential problem with keeping this feature will become apparent when
    we look at splitting our data into training and test sets, as well as other cases
    of data splitting, such as cross-validation, described in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Support Vector Machines*. Here, the issue is that
    one split in our data may lead to unique values for a near zero variance column,
    for example, only `FALSE` values for our `Yellow` iris column.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and dimensionality reduction
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The number and type of features that we use with a model is one of the most
    important decisions that we will make in the predictive modeling process. Having
    the right features for a model will ensure that we have sufficient evidence on
    which to base a prediction. On the flip side, the number of features that we work
    with is precisely the number of dimensions that the model has. A large number
    of dimensions can be the source of several complications. High dimensional problems
    often suffer from **data sparsity**, which means that because of the number of
    dimensions available, the range of possible combinations of values across all
    the features grows so large that it is unlikely that we will ever collect enough
    data in order to have enough representative examples for training. In a similar
    vein, we often talk about the **curse of dimensionality**. This describes the
    fact that, because of the overwhelmingly large space of possible inputs, data
    points that we have collected are likely to be far away from each other in the
    feature space. As a result, local methods, such as k-nearest neighbors, that make
    predictions using observations in the training data that are close to the point
    for which we are trying to make a prediction, will not work as well in high dimensions.
    A large feature set is also problematic in that it may significantly increase
    the time needed to train (and predict, in some cases) our model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, there are two types of process that feature engineering involves.
    The first of these, which grows the feature space, is the design of new features
    based on features within our data. Sometimes, a new feature that is a product
    or ratio of two original features might work better. There are many ways to combine
    existing features into new ones, and often it is expert knowledge from the problem's
    particular application domain that might help guide us. In general though, this
    process takes experience and a lot of trial and error. Note that there is no guarantee
    that adding a new feature will not degrade performance. Sometimes, adding a feature
    that is very noisy or highly correlated with an existing feature may actually
    cause us to lose accuracy.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The second process in feature engineering is feature reduction or shrinkage,
    which reduces the size of the feature space. In the previous section on data preprocessing,
    we looked at how we can detect individual features that may be problematic for
    our model in some way. **Feature selection** refers to the process in which the
    subset of features that are the most informative for our target output is selected
    from the original pool of features. Some methods, such as tree-based models, have
    built-in feature selection, as we shall see in [Chapter 6](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 6. Support Vector Machines"), *Tree-based Methods*. In [Chapter 2](part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 2. Tidying Data and Measuring Performance"), *Linear Regression*, we'll
    also explore methods to perform feature selection for linear models.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Another way to reduce the overall number of features, a concept known as **dimensionality
    reduction**, is to transform the entire set of features into a completely new
    set of features that are fewer in number. A classic example of this is **Principal
    Component Analysis** (**PCA**).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, PCA creates a new set of input features, known as **principal
    components**, all of which are linear combinations of the original input features.
    For the first principal component, the linear combination weights are chosen in
    order to capture the maximum amount of variation in the data. If we could visualize
    the first principal component as a line in the original feature space, this would
    be the line in which the data varies the most. It also happens to be the line
    that is closest to all the data points in the original feature space. Every subsequent
    principal component attempts to capture a line of maximum variation, but in such
    a way that the new principal component is uncorrelated with the previous ones
    already computed. Thus, the second principal component selects the linear combination
    of original input features that have the highest degree of variation in the data,
    while being uncorrelated with the first principal component.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The principal components are ordered naturally in descending order according
    to the amount of variation that they capture. This allows us to perform dimensionality
    reduction in a simple manner by keeping the first *N* components, where we choose
    *N* so that the components chosen incorporate a minimum amount of the variance
    from the original dataset. We won't go into the details of the underlying linear
    algebra necessary to compute the principal components.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we''ll direct our attention to the fact that this process is sensitive
    to the variance and scale of the original features. For this reason, we often
    scale our features before carrying out this process. To visualize how useful PCA
    can be, we''ll once again turn to our faithful iris dataset. We can use the `caret`
    package to carry out PCA. To do this, we specify `pca` in the `method` parameter
    of the `preProcess()` function. We can also use the `thresh` parameter, which
    specifies the minimum variance we must retain. We''ll explicitly use the value
    `0.95` so that we retain 95% of the variance of the original data, but note that
    this is also the default value of this parameter:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As a result of this transformation, we are now left with only two features,
    so we can conclude that the first two principal components of the numerical iris
    features incorporate over 95% of the variation in the data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are interested in learning the weights that were used to compute the
    principal components, we can inspect the `rotation` attribute of the `pp_pca`
    object:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This means that the first principal component, `PC1`, was computed as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature engineering and dimensionality reduction](img/00015.jpeg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'Sometimes, instead of directly specifying a threshold for the total variance
    captured by the principal components, we might want to examine a plot of each
    principal component and its variance. This is known as a **scree plot**, and we
    can build this by first performing PCA and indicating that we want to keep all
    the components. To do this, instead of specifying a variance threshold, we set
    the `pcaComp` parameter, which is the number of principal components we want to
    keep. We will set this to `4`, which includes all of them, remembering that the
    total number of principal components is the same as the total number of original
    features or dimensions we started out with. We will then compute the variance
    and cumulative variance of these components and store it in a data frame. Finally,
    we will plot this in the figure that follows, noting that the numbers in brackets
    are cumulative percentages of variance captured:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Feature engineering and dimensionality reduction](img/00016.jpeg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: As we can see, the first principal component accounts for 73.45% of the total
    variance in the iris dataset, while together with the second component, the total
    variance captured is 96.27%. PCA is an unsupervised method for dimensionality
    reduction that does not make use of the output variable even when it is available.
    Instead, it looks at the data geometrically in the feature space. This means that
    we cannot ensure that PCA will give us a new feature space that will perform well
    in our prediction problem, beyond the computational advantages of having fewer
    features. These advantages might make PCA a viable choice even when there is reduction
    in model accuracy as long as this reduction is small and acceptable for the specific
    task. As a final note, we should point out that weights of the principal components,
    often referred to as **loadings,** are unique within a sign flip as long as they
    have been normalized. In cases where we have perfectly correlated features or
    perfect linear combinations we will obtain a few principal components that are
    exactly zero.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Training and assessing the model
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our earlier discussion of parametric models, we saw that they come with a
    procedure to train the model using a set of training data. Nonparametric models
    will typically either perform lazy learning, in which case there really isn't
    an actual training procedure at all beyond memorizing the training data, or, as
    in the case of splines, will perform local computations on the training data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Either way, if we are to assess the performance of our model, we need to split
    our data into a **training set** and a **test set**. The key idea is that we want
    to assess our model based on how we expect it to perform on unseen future data.
    We do this by using the test set, which is a portion (typically 15-30%) of the
    data we collected and set aside for this purpose and haven't used during training.
    For example, one possible divide is to have a training set with 80% of the observations
    in our original data, and a test set with the remaining 20%. The reason why we
    need a test set is that we cannot use the training set to fairly assess our model
    performance, since we fit our model to the training data and it does not represent
    data that we haven't seen before. From a prediction standpoint, if our goal was
    to maximize performance on our training data alone, then the best thing to do
    would be to simply memorize the input data along with the desired output values
    and our model would thus be a simple look-up table!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: A good question to ask would be. How do we decide how much data to use for training
    and testing? There is a trade-off that is involved here that makes the answer
    to this question nontrivial. On the one hand, we would like to use as much data
    as possible in our training set, so that the model has more examples from which
    to learn. On the other hand, we would like to have a large test set so that we
    can test our trained model using many examples in order to minimize the variance
    of our estimate of the model's predictive performance. If we only have a handful
    of observations in our test set, then we cannot really generalize about how our
    model performs on unseen data overall.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Another factor that comes into play is how much starting data we have collected.
    If we have very little data, we may have to use a larger amount in order to train
    our model, such as an 85-15 split. If we have enough data, then we might consider
    a 70-30 split so that we can get a more accurate prediction on our test set.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'To split a dataset using the `caret` package, we can use the `createDataPartition()`
    function to create a sampling vector containing the indices of the rows we will
    use in our training set. These are selected by randomly sampling the rows until
    a specified proportion of the rows have been sampled, using the `p` parameter:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tip
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is good practice, when reporting the results of a statistical analysis involving
    a random number generation, to apply the `set.seed()` function on a randomly chosen
    but fixed number. This function ensures that the random numbers that are generated
    from the next function call involving random number generation will be the same
    every time the code is run. This is done so that others who read the analysis
    are able to reproduce the results exactly. Note that if we have several functions
    in our code that perform random number generation, or the same function is called
    multiple times, we should ideally apply `set.seed()` before each one of them.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our sampling vector, which we created for the iris dataset, we can construct
    our training and test sets. We''ll do this for a few versions of the iris dataset
    that we built earlier when we experimented with different feature transformations:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We are now in a position to build and test three different models for the iris
    dataset. These are in turn, the unnormalized model, a model where the input features
    have been centered and scaled with a Z-score transformation, and the PCA model
    with two principal components. We could use our test set in order to measure the
    predictive performance of each of these models after we build them; however, this
    would mean that, in our final estimate of unseen accuracy, we will have used the
    test set in the model selection, thus producing a biased estimate. For this reason,
    we often maintain a separate split of the data, usually as large as the test set,
    known as the **validation set**. This is used to tune model parameters, such as
    *k* in kNN, and among different encodings and transformations of the input features
    before using the test set to predict unseen performance. In [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Support Vector Machines*, we'll discuss an alternative
    to this approach known as cross-validation.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Once we have split our data, trained our model by following the relevant training
    procedure that it requires, and tuned our model parameters, we then have to assess
    its performance on the test set. Typically, we won't find the same performance
    on our test set as on our training set. Sometimes, we may even find that the performance
    we see when we deploy our model does not match what we expected to see, based
    on the performance on our training or test sets. There are a number of possible
    reasons for this disparity in performance. The first of these is that the data
    we may have collected may either not be representative of the process that we
    are modeling, or that there are certain combinations of feature inputs that we
    simply did not encounter in our training data. This could produce results that
    are inconsistent with our expectations. This situation can happen both in the
    real world, but also with our test set if it contains outliers, for example. Another
    common situation is the problem of model **overfitting**.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is a problem in which some models, especially more flexible models,
    perform well on their training dataset, but perform significantly worse on an
    unseen test set. This occurs when a model matches the observations in the training
    data too closely and fails to generalize on unseen data. Put differently, the
    model is picking up on spurious details and variations in a training dataset,
    which are not representative of the underlying population as a whole. Overfitting
    is one of the key reasons why we do not choose our model based on its performance
    on the training data. Other sources of discrepancy between training and test data
    performance are model bias and variance. Together, these actually form a well-known
    trade-off in statistical modeling known as the **bias-variance tradeoff**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The variance of a statistical model refers to how much the model's predicted
    function would change, should a differently chosen training set (but generated
    from the exact same process or system that we are trying to predict as the original)
    be used to train the model. A low variance is desired because essentially, we
    don't want to predict a very different function with a different training set
    that is generated from the same process. Model bias refers to the errors inherently
    introduced in the predicted function, as a result of the limitation as to what
    functional forms the specific model can learn. For example, linear models introduce
    bias when trying to approximate nonlinear functions because they can only learn
    linear functions. The ideal scenario for a good predictive model is to have both
    a low variance and a low bias. It is important for a predictive modeler to be
    aware of the fact that there is a bias-variance trade-off that arises from the
    choice of models. Models that are typically more complex because of the fact that
    they make fewer assumptions on the target function are prone to less bias, but
    higher variance than simpler but more restrictive models, such as linear models.
    This is because more complex models are able to approximate the training data
    more closely due to their flexibility, but as a result, they are more sensitive
    to changes in training data. This, of course, is also related to the problem of
    overfitting that complex models often exhibit.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'We can actually see the effects of overfitting by first training some kNN models
    on our iris datasets. There are a number of packages that offer an implementation
    of the kNN algorithm, but we will use the `knn3()` function provided by the `caret`
    package with which we are familiar. To train a model using this function, all
    we have to do is provide it with a data frame that contains the numerical input
    features, a vector of output labels, and `k`, the number of nearest neighbors
    we want to use for the prediction:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To see the effect of different values of `k`, we will use the iris PCA model
    that is conveniently available in two dimensions for us to visualize and repeatedly
    train:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and assessing the model](img/00017.jpeg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: In the preceding plots, we have used different symbols to denote data points
    corresponding to different species. The lines shown in the plots correspond to
    the **decision boundaries** between the different species, which are the class
    labels of our output variable. Notice that using a low value of `k`, such as `1`,
    captures local variations in the data very closely and, as a result, the decision
    boundaries are very irregular. A higher value of `k` uses many neighbors to create
    a prediction, resulting in a smoothing effect and smoother decision boundaries.
    Tuning `k` in kNN is an example of tuning a model parameter to balance the effect
    of overfitting.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: We haven't mentioned any specific performance metrics in this section. There
    are different measures of model quality relevant to regression and classification,
    and we will address these after we wrap up our discussion on the predictive modeling
    process.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Repeating with different models and final model selection
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the first iteration of this process (and this is very much an iterative
    process!), we usually arrive at this stage, having trained and assessed a simple
    model. Simple models usually allow us to get to a quick and dirty solution with
    minimum effort, thus giving us an early sense of how far away we are from a model
    that will make predictions with reasonable accuracy. Simple models are also great
    at giving us a baseline level of performance against which we can benchmark the
    performance of future models. As modelers, we often acquire a preference toward
    one method over others, but it is important to remember that it is generally well
    worth the effort to try out different approaches to a problem and use the data
    to help us decide which one we should end up using.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Before picking the final model, it is worth considering whether it might be
    a good idea to use more than one model to solve our problem. In [Chapter 7](part0062_split_000.html#1R42S1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 7. Tree-Based Methods"), *Ensemble Methods*, we spend an entire chapter
    studying techniques that involve many models working together to boost the predictive
    accuracy of the overall system.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have chosen the final model to use, we want to finalize its implementation
    so that the end users can use it reliably. Programmers refer to this process as
    **deploying to production**. This is where sound software engineering principles
    become extremely important. The following guidelines offer some useful advice:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The model should be optimized to improve the speed at which it computes predictions.
    For example, this means ensuring that any features that are computed at runtime
    are done so efficiently.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model should be well documented. The final input features should be clearly
    defined, and the method and data used for training should be stored so that it
    can easily be retrained if changes need to be made. The original performance on
    the training and test set should also be stored as a reference for subsequent
    improvements.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model's performance should be monitored over time. This is important, not
    only as a means of verifying that the model works as intended, but also in order
    to catch any potential data shifts. If the process that is being modeled changes
    over time, it is likely that our model performance will degrade and this will
    signal the need for a new model to be trained.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The software used to implement the model should be properly tested using standard
    unit and integration tests. Often, we will use a lot of existing R packages whose
    functions have already undergone testing, but the final deployment of a model
    may require us to write some additional code ourselves, such as for feature computation.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deployed model should be able to handle errors in the input. For example,
    if some of the input features are missing, it should be made very clear to the
    user why the model is unable to make a prediction through appropriate error messages.
    Errors and warnings should also be logged, especially if the model is deployed
    for continuous predictions in real-time settings.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the fundamental ideas surrounding predictive models.
    We saw that there are many ways to categorize models, learning important distinctions
    in the process, such as supervised versus unsupervised learning and regression
    versus classification. Next, we outlined the steps involved in building a predictive
    model, starting from the process of data collection all the way to model evaluation
    and deployment. Critically, this process is an iterative one, and most often we
    arrive at our final model after having tried out and trained several different
    models.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: We also introduced our first model, the k-nearest neighbor model, which is useful
    in performing classification and regression alike. kNN is a very flexible model
    that doesn't make any explicit assumptions about the underlying data. Thus, it
    can fit a very complex decision boundary. It is a lazy learner, in that it doesn't
    construct a model to describe the relationship between the input features and
    the output variable. Thus, it doesn't require a long period of training. On the
    other hand, for data with many dimensions, it may take a long time to produce
    a prediction, and because the model needs to remember all the training data in
    order to find the nearest neighbors of a target point, it often also requires
    a lot of memory. kNN doesn't distinguish between the importance of different features,
    and the fact that it uses a distance metric in its prediction means that, on the
    one hand, it does not have any built-in way to handle missing data and, on the
    other, it often requires features to be transformed to similar scales. Finally,
    the model can be tuned by choosing an appropriate value of *k*, the number of
    nearest neighbors, to balance the degree of overfitting. With a firm grounding
    in the basics of the predictive modeling process, we will look at linear regression
    in the next chapter.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
