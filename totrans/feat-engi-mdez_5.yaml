- en: Feature Selection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: 'We''re halfway through our text and we have gotten our hands dirty with about
    a dozen datasets and have seen a great deal of feature selection methods that
    we, as data scientists and machine learning engineers, may utilize in our work
    and lives to ensure that we are getting the most out of our predictive modeling.
    So far, in dealing with data, we have worked with methods including:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了文本的一半，并且我们已经处理了大约一打数据集，看到了许多我们作为数据科学家和机器学习工程师在工作和生活中可能利用的特征选择方法，以确保我们能够从预测建模中获得最大收益。到目前为止，在处理数据时，我们已经使用了包括以下方法在内的方法：
- en: Feature understanding through the identification of levels of data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过识别数据级别来理解特征
- en: Feature improvements and imputing missing values
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征改进和缺失值填充
- en: Feature standardization and normalization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征标准化和归一化
- en: Each of the preceding methods has a place in our data pipeline and, more often
    than not, two or more methods are used in tandem with one another.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 上述每种方法都在我们的数据处理流程中占有一席之地，而且往往两种或更多方法会相互配合使用。
- en: The remainder of this text will focus on other methods of feature engineering
    that are, by nature, a bit more mathematical and complex than in the first half
    of this book. As the preceding workflow grows, we will do our best to spare the
    reader the inner workings of each and every statistical test we invoke and instead
    convey a broader picture of what the tests are trying to achieve. As authors and
    instructors, we are always open to your questions about any of the inner mechanisms
    of this work.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的剩余部分将专注于其他特征工程方法，这些方法在本质上比本书前半部分更为数学化和复杂。随着先前工作流程的增长，我们将尽力避免让读者了解我们调用的每一个统计测试的内部机制，而是传达一个更广泛的测试目标图景。作为作者和讲师，我们始终欢迎您就本工作的任何内部机制提出问题。
- en: We have come across one problem quite frequently in our discussion of features,
    and that problem is **noise**. Often, we are left working with features that may
    not be highly predictive of the response and, sometimes, can even hinder our models'
    performance in the prediction of the response. We used tools such as standardization
    and normalization to try to mitigate such damage, but at the end of the day, noise
    must be dealt with.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论特征的过程中，我们经常遇到一个问题，那就是**噪声**。我们常常不得不处理那些可能不是高度预测响应的特征，有时甚至可能阻碍我们的模型在预测响应方面的性能。我们使用标准化和归一化等工具来尝试减轻这种损害，但最终，噪声必须得到处理。
- en: 'In this chapter, we will address a subset of feature engineering called **feature
    selection**, which is the process of selecting which features, from the original
    batch of features, are the *best* when it comes to the model prediction pipeline.
    More formally, given *n* features, we search for a subset of *k*, where *k < n*
    features that improve our machine learning pipeline. This generally comes down
    to the statement:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一种称为**特征选择**的特征工程子集，这是从原始特征集中选择哪些特征在模型预测流程中是**最佳**的过程。更正式地说，给定 *n*
    个特征，我们寻找一个包含 *k* 个特征（其中 *k < n*）的子集，以改善我们的机器学习流程。这通常归结为以下陈述：
- en: '*Feature Selection attempts to weed out the noise in our data and remove it.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征选择旨在去除数据中的噪声并消除它*。'
- en: 'The definition of feature selection touches on two major points that must be
    addressed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择的定义涉及两个必须解决的问题：
- en: The methods in which we may find the subset of *k* features
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能找到的 *k* 个特征子集的方法
- en: The definition of *better* in the context of machine learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习背景下**更好**的定义
- en: 'The majority of this chapter is dedicated to the methods in which we may find
    such subsets of features and the basis on which such methods work. This chapter
    will break up the methods of feature selection into two broad subsections: **statistical-based**
    and **model-based** feature selection. This separation may not 100% capture the
    complexity of the science and art of feature selection, but will work to drive
    real and actionable results in our machine learning pipeline.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的大部分内容致力于探讨我们如何找到这样的特征子集以及这些方法运作的基础。本章将特征选择方法分为两大类：**基于统计**和**基于模型**的特征选择。这种划分可能无法完全捕捉特征选择这一科学和艺术领域的复杂性，但它有助于在我们的机器学习流程中产生真实且可操作的结果。
- en: Before we dive into the deep end of many of these methods, let's first discuss
    how we may better understand and define the idea of *better*, as it will frame
    the remainder of this chapter, as well as framing the remainder of this text.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨许多这些方法之前，让我们首先讨论如何更好地理解和定义**更好**的概念，因为它将界定本章的其余部分，以及界定本文本的其余部分。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中将涵盖以下主题：
- en: Achieving better performance in feature engineering
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特征工程中实现更好的性能
- en: Creating a baseline machine learning pipeline
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个基线机器学习管道
- en: The types of feature selection
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择类型
- en: Choosing the right feature selection method
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的特征选择方法
- en: Achieving better performance in feature engineering
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在特征工程中实现更好的性能
- en: 'Throughout this book, we have relied on a base definition of *better* when
    it came to the various feature engineering methods we put into place. Our implicit
    goal was to achieve better predictive performance measured purely on simple metrics
    such as accuracy for classification tasks and RMSE for regression tasks (mostly
    accuracy). There are other metrics we may measure and track to gauge predictive
    performance. For example, we will use the following metrics for classification:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们在实施各种特征工程方法时，都依赖于对**更好**的基本定义。我们的隐含目标是实现更好的预测性能，这种性能仅通过简单的指标来衡量，例如分类任务的准确率和回归任务的RMSE（主要是准确率）。我们还可以测量和跟踪其他指标来衡量预测性能。例如，我们将使用以下指标进行分类：
- en: True and false positive rate
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阳性率和假阳性率
- en: Sensitivity (AKA true positive rate) and specificity
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵敏度（也称为真阳性率）和特异性
- en: False negative and false positive rate
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阴性率和假阳性率
- en: 'and for regression, the metrics that will be applied are:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归，将应用以下指标：
- en: Mean absolute error
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差
- en: R²
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R²
- en: 'These lists go on, and while we will not be abandoning the idea of quantifying
    performance through metrics such as the ones precedingly listed, we may also measure
    other *meta metrics*, or metrics that do not directly correlate to the performance
    of the prediction of the model, rather, so-called **meta metrics** attempt to
    measure the performance *around* the prediction and include such ideas as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些列表将继续，虽然我们不会放弃通过如前所述的指标量化性能的想法，但我们也可以测量其他**元指标**，或者不直接与模型预测性能相关的指标，而是所谓的**元指标**试图衡量预测周围的性能，包括以下想法：
- en: Time in which the model needs to fit/train to the data
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型需要拟合/训练到数据的时间
- en: Time it takes for a fitted model to predict new instances of data
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型以预测新数据实例所需的时间
- en: The size of the data in case data must be persisted (stored for later)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据必须持久化（存储以供以后使用），则数据的大小
- en: 'These ideas will add to our definition of *better* machine learning as they
    help to encompass a much larger picture of our machine learning pipeline outside
    of model predictive performance. In order to help us track these metrics, let''s
    create a function that is generic enough to evaluate several models but specific
    enough to give us metrics for each one. We will call our function `get_best_model_and_accuracy` and it
    will do many jobs, such as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些想法将丰富我们对**更好**的机器学习的定义，因为它们有助于涵盖我们机器学习管道（除了模型预测性能之外）的更广阔的图景。为了帮助我们跟踪这些指标，让我们创建一个足够通用的函数来评估多个模型，但同时又足够具体，可以为我们每个模型提供指标。我们将我们的函数命名为`get_best_model_and_accuracy`，它将执行许多工作，例如：
- en: It will search across all given parameters in order to optimize the machine
    learning pipeline
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将搜索所有给定的参数以优化机器学习管道
- en: It will spit out some metrics that will help us assess the quality of the pipeline
    entered
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将输出一些指标，帮助我们评估输入管道的质量
- en: 'Let''s go ahead and define such a function with the help of the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义这样一个函数，以下代码将提供帮助：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The overall goal of this function is to act as a ground truth in that we will
    use it to evaluate every feature selection method in this chapter to give us a
    sense of standardization of evaluation. This is not really any different to what
    we have been doing already, but we are now formalizing our work as a function,
    and also using metrics other than accuracy to grade our feature selection modules
    and machine learning pipelines.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的整体目标是作为一个基准，我们将用它来评估本章中的每个特征选择方法，以给我们一个评估标准化的感觉。这实际上与我们之前所做的是一样的，但现在我们将我们的工作正式化为一个函数，并且还使用除了准确率之外的指标来评估我们的特征选择模块和机器学习管道。
- en: A case study – a credit card defaulting dataset
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个案例研究——信用卡违约数据集
- en: 'By intelligently extracting the most important signals from our data and ignoring
    noise, feature selection algorithms achieve two major outcomes:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved model performance**: By removing redundant data, we are less likely
    to make decisions based on noisy and irrelevant data, and it also allows our models
    to hone in on the important features, thereby improving model pipeline predictive
    performance'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced training and predicting time**: By fitting pipelines to less data,
    this generally results in improved model fitting and predicting times, making
    our pipelines faster overall'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to gain a realistic understanding of how and why noisy data gets in
    the way, let''s introduce our newest dataset, a credit card defaulting dataset.
    We will work with 23 features and one response variable. That response variable
    will be a Boolean, meaning it will either be True or False. The reason we are
    working with 23 features is that we want to see if we can find which of the 23
    features will help us in our machine learning pipelines and which ones will hurt
    us. We can import the datasets using the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To start, let''s bring in two common modules, `numpy` and `pandas`, and also
    set a random seed so that you and we will achieve the same results for consistency.
    Now, let''s bring in our latest dataset, using the following code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s go ahead and do some mandatory EDA. Let''s begin by checking how big
    a dataset we are working with, using the following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'So, we have `30,000 rows` (observations) and `24 columns` (1 response and 23
    features). We will not go in depth to describe the columns meanings at this time,
    but we do encourage the reader to check out the source of the data ([http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#](http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#)).
    For now, we will rely on good old-fashioned statistics to tell us more:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **count** | **mean** | **std** | **min** | **25%** | **50%** | **75%**
    | **max** |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| **LIMIT_BAL** | 30000.0 | 167484.322667 | 129747.661567 | 10000.0 | 50000.00
    | 140000.0 | 240000.00 | 1000000.0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| **SEX** | 30000.0 | 1.603733 | 0.489129 | 1.0 | 1.00 | 2.0 | 2.00 | 2.0 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| **EDUCATION** | 30000.0 | 1.853133 | 0.790349 | 0.0 | 1.00 | 2.0 | 2.00 |
    6.0 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| **MARRIAGE** | 30000.0 | 1.551867 | 0.521970 | 0.0 | 1.00 | 2.0 | 2.00 |
    3.0 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| **AGE** | 30000.0 | 35.485500 | 9.217904 | 21.0 | 28.00 | 34.0 | 41.00 |
    79.0 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| **PAY_0** | 30000.0 | -0.016700 | 1.123802 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| **PAY_2** | 30000.0 | -0.133767 | 1.197186 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: '| **PAY_3** | 30000.0 | -0.166200 | 1.196868 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
- en: '| **PAY_4** | 30000.0 | -0.220667 | 1.169139 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
- en: '| **PAY_5** | 30000.0 | -0.266200 | 1.133187 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
- en: '| **PAY_6** | 30000.0 | -0.291100 | 1.149988 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT1** | 30000.0 | 51223.330900 | 73635.860576 | -165580.0 | 3558.75
    | 22381.5 | 67091.00 | 964511.0 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT2** | 30000.0 | 49179.075167 | 71173.768783 | -69777.0 | 2984.75
    | 21200.0 | 64006.25 | 983931.0 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT3** | 30000.0 | 47013.154800 | 69349.387427 | -157264.0 | 2666.25
    | 20088.5 | 60164.75 | 1664089.0 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT4** | 30000.0 | 43262.948967 | 64332.856134 | -170000.0 | 2326.75
    | 19052.0 | 54506.00 | 891586.0 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT5** | 30000.0 | 40311.400967 | 60797.155770 | -81334.0 | 1763.00
    | 18104.5 | 50190.50 | 927171.0 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT6** | 30000.0 | 38871.760400 | 59554.107537 | -339603.0 | 1256.00
    | 17071.0 | 49198.25 | 961664.0 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT1** | 30000.0 | 5663.580500 | 16563.280354 | 0.0 | 1000.00 | 2100.0
    | 5006.00 | 873552.0 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT2** | 30000.0 | 5921.163500 | 23040.870402 | 0.0 | 833.00 | 2009.0
    | 5000.00 | 1684259.0 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT3** | 30000.0 | 5225.681500 | 17606.961470 | 0.0 | 390.00 | 1800.0
    | 4505.00 | 891586.0 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT4** | 30000.0 | 4826.076867 | 15666.159744 | 0.0 | 296.00 | 1500.0
    | 4013.25 | 621000.0 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT5** | 30000.0 | 4799.387633 | 15278.305679 | 0.0 | 252.50 | 1500.0
    | 4031.50 | 426529.0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT6** | 30000.0 | 5215.502567 | 17777.465775 | 0.0 | 117.75 | 1500.0
    | 4000.00 | 528666.0 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| **default payment next month** | 30000.0 | 0.221200 | 0.415062 | 0.0 | 0.00
    | 0.0 | 0.00 | 1.0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: The **default payment next month** is our response column and everything else
    is a feature/potential predictor of default. It is wildly clear that our features
    exist on wildly different scales, so that will be a factor in how we handle the
    data and which models we will pick. In previous chapters, we dealt heavily with
    data and features on different scales using solutions such as `StandardScalar`
    and normalization to alleviate some of these issues; however, in this chapter,
    we will largely choose to ignore such problems in order to focus on more relevant
    issues.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter of this book, we will focus on several case studies that
    will marry almost all of the techniques in this book on a longer-term analysis
    of a dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen in previous chapters, we know that null values are a big issue
    when dealing with machine learning, so let''s do a quick check to make sure that
    we don''t have any to deal with:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Phew! No missing values here. Again, we will deal with missing values again
    in future case studies, but for now, we have bigger fish to fry. Let''s go ahead
    and set up some variables for our machine learning pipelines, using the following
    code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As usual, we created our `X` and `y` variables. Our `X` matrix will have 30,000
    rows and 23 columns and our `y` is, as always, a 30,000 long pandas Series. Because
    we will be performing classification, we will, as usual, need to ascertain a null
    accuracy to ensure that our machine learning models are performing better than
    a baseline. We can get the null accuracy rate using the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So, the accuracy to beat, in this case, is **77.88%**, which is the percentage
    of people who did not default (0 meaning false to default).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个案例中需要超越的准确率是**77.88**%，这是没有违约（0表示没有违约）的人的百分比。
- en: Creating a baseline machine learning pipeline
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建基线机器学习流程
- en: 'In previous chapters, we offered to you, the reader, a single machine learning
    model to use throughout the chapter. In this chapter, we will do some work to
    find the best machine learning model for our needs and then work to enhance that
    model with feature selection. We will begin by importing four different machine
    learning models:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们向读者提供了一个单一的机器学习模型在整个章节中使用。在本章中，我们将做一些工作来找到最适合我们需求的机器学习模型，然后通过特征选择来增强该模型。我们将首先导入四个不同的机器学习模型：
- en: Logistic Regression
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: K-Nearest Neighbors
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-最近邻
- en: Decision Tree
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Random Forest
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'The code for importing the learning models is given as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 导入学习模型的代码如下所示：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once we are finished importing these modules, we will run them through our `get_best_model_`and`_accuracy`
    functions to get a baseline on how each one handles the raw data. We will have
    to first establish some variables to do so. We will use the following code to
    do this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成这些模块的导入，我们将通过我们的`get_best_model_`和`_accuracy`函数运行它们，以获得每个模块处理原始数据的基线。为此，我们首先需要建立一些变量。我们将使用以下代码来完成这项工作：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you feel uncomfortable with any of the models listed above, we recommend
    reading up on documentation, or referring to the Packt book, *The Principles of
    Data Science*, [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science),
    for a more detailed explanation of the algorithms.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以上列出的任何模型感到不舒服，我们建议阅读相关文档，或者参考Packt出版的《数据科学原理》一书，[https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)，以获得算法的更详细解释。
- en: 'Because we will be sending each model through our function, which invokes a
    grid search module, we need only create blank state models with no customized
    parameters set, as shown in the following code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将把每个模型通过我们的函数发送，该函数调用网格搜索模块，我们只需要创建没有设置自定义参数的空白状态模型，如下所示：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We are now going to run each of the four machine learning models through our
    evaluation function to see how well (or not) they do against our dataset. Recall
    that our number to beat at the moment is .7788, the baseline null accuracy. We
    will use the following code to run the models:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将运行每个四个机器学习模型通过我们的评估函数，看看它们在我们的数据集上的表现如何（或不好）。回想一下，我们目前要超越的数字是.7788，这是基线零准确率。我们将使用以下代码来运行这些模型：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see that the logistic regression has already beaten the null accuracy
    using the raw data and, on average, took 6/10 of a second to fit to a training
    set and only 20 milliseconds to score. This makes sense if we know that to fit,
    a logistic regression in **scikit-learn** must create a large matrix in memory,
    but to predict, it need only multiply and add scalars to one another.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，逻辑回归已经使用原始数据超越了零准确率，平均而言，只需要6/10秒来拟合训练集，并且只需要20毫秒来评分。如果我们知道在**scikit-learn**中，逻辑回归必须创建一个大的矩阵存储在内存中，但为了预测，它只需要将标量相乘和相加，这是有道理的。
- en: 'Now, let''s do the same with the KNN model, using the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下代码对KNN模型做同样的事情：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our KNN model, as expected, does much better on the fitting time. This is because,
    to fit to the data, the KNN only has to store the data in such a way that it is
    easily retrieved at prediction time, where it takes a hit on time. It's also worth
    mentioning the painfully obvious fact that the accuracy is not even better than
    the null accuracy! You might be wondering why, and if you're saying *hey wait
    a minute, doesn't KNN utilize the Euclidean Distance in order to make predictions,
    which can be thrown off by non-standardized data, a flaw that none of the other
    three machine learning models suffer?*, then you're 100% correct.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的KNN模型，正如预期的那样，在拟合时间上表现更好。这是因为，为了拟合数据，KNN只需要以某种方式存储数据，以便在预测时可以轻松检索，这会在时间上造成损失。还值得一提的是一个显而易见的事实，即准确率甚至没有超过零准确率！你可能想知道为什么，如果你说“嘿，等等，KNN不是利用欧几里得距离来做出预测吗，这可能会被非标准化数据所影响，而其他三个机器学习模型都没有这个问题”，那么你完全正确。
- en: 'KNN is a distance-based model, in that it uses a metric of closeness in space
    that assumes that all features are on the same scale, which we already know that
    our data is not on. So, for KNN, we will have to construct a more complicated
    pipeline to more accurately assess its baseline performance, using the following
    code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是一种基于距离的模型，它使用空间中相似度的度量，假设所有特征都在相同的尺度上，但我们已经知道我们的数据并不是这样的。因此，对于KNN，我们将不得不构建一个更复杂的管道来更准确地评估其基线性能，以下代码展示了如何实现：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The first thing to notice is that our modified code pipeline, which now includes
    a `StandardScalar` (which z-score normalizes our features) now beats the null
    accuracy at the very least, but also seriously hurts our predicting time, as we
    have added a step of preprocessing. So far, the logistic regression is in the
    lead with the best accuracy and the better overall timing of the pipeline. Let''s
    move on to our two tree-based models and start with the simpler of the two, the
    decision tree, with the help of the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，我们修改后的代码管道，现在包括了一个`StandardScalar`（它通过z分数标准化我们的特征），至少在null accuracy上有所提升，但同时也严重影响了我们的预测时间，因为我们增加了一个预处理步骤。到目前为止，逻辑回归在最佳准确率和更好的整体管道时间上处于领先地位。让我们继续前进，看看我们的两个基于树的模型，先从两个模型中较简单的一个开始，即决策树，以下代码将提供帮助：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Amazing! Already, we have a new lead in accuracy and, also, the decision tree
    is quick to both fit and predict. In fact, it beats logistic regression in its
    time to fit and beats the KNN in its time to predict. Let''s finish off our test
    by evaluating a random forest, using the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经在新准确率上取得了领先，而且决策树在拟合和预测方面都很快。事实上，它在拟合时间上击败了逻辑回归，在预测时间上击败了KNN。让我们通过以下代码评估随机森林来完成我们的测试：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Much better than either the Logistic Regression or the KNN, but not better
    than the decision tree. Let''s aggregate these results to see which model we should
    move forward with in optimizing using feature selection:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 比逻辑回归或KNN都要好，但不如决策树。让我们汇总这些结果，看看我们应该在优化时使用哪种模型：
- en: '| **Model Name** | **Accuracy (%)** | **Fit Time (s)** | **Predict Time (s)**
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| **模型名称** | **准确率 (%)** | **拟合时间 (s)** | **预测时间 (s)** |'
- en: '| Logistic Regression | .8096 | .602 | **.002** |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | .8096 | .602 | **.002** |'
- en: '| KNN (with scaling) | .8008 | **.035** | 6.72 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| KNN（缩放） | .8008 | **.035** | 6.72 |'
- en: '| Decision Tree | **.8203** | .158 | **.002** |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | **.8203** | .158 | **.002** |'
- en: '| Random Forest | .8196 | 1.107 | .044 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | .8196 | 1.107 | .044 |'
- en: 'The decision tree comes in first for accuracy and tied for first for predict
    time with logistic regression, while KNN with scaling takes the trophy for being
    the fastest to fit to our data. Overall, the decision tree appears to be the best
    model to move forward with, as it came in first for, arguably, our two most important
    metrics:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在准确率上排名第一，与逻辑回归在预测时间上并列第一，而经过缩放的KNN在拟合我们的数据方面速度最快。总的来说，决策树似乎是我们继续前进的最佳模型，因为它在我们的两个最重要的指标上排名第一：
- en: We definitely want the best accuracy to ensure that out of sample predictions
    are accurate
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们肯定希望获得最佳准确率，以确保样本外预测的准确性
- en: Having a prediction time is useful considering that the models are being utilized
    for real-time production usage
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到模型将被用于实时生产使用，预测时间是有用的
- en: The approach we are taking is one that selects a model before selecting any
    features. It is not required to work in this fashion, but we find that it generally
    saves the most time when working under pressure of time. For your purposes, we
    recommend that you experiment with many models concurrently and don't limit yourself
    to a single model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采取的方法是在选择任何特征之前选择一个模型。这不是必须的工作方式，但我们发现，在时间紧迫的情况下，这种方式通常可以节省最多的时间。就你的目的而言，我们建议你同时实验多种模型，不要将自己限制在单一模型上。
- en: 'Knowing that we will be using the decision tree for the remainder of this chapter,
    we know two more things:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 知道我们将使用决策树来完成本章的剩余部分，我们还知道两件事：
- en: The new baseline accuracy to beat is .8203, the accuracy the tree obtained when
    fitting to the entire dataset
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的基线准确率是.8203，这是树在拟合整个数据集时获得的准确率
- en: We no longer have to use our `StandardScaler`, as decision trees are unaffected
    by it when it comes to model performance
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不再需要使用`StandardScaler`，因为决策树在模型性能方面不受其影响
- en: The types of feature selection
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择类型
- en: 'Recall that our goal with feature selection is to improve our machine learning
    capabilities by increasing predictive power and reducing the time cost. To do
    this, we introduce two broad categories of feature selection: statistical-based
    and model-based. Statistical-based feature selection will rely heavily on statistical
    tests that are separate from our machine learning models in order to select features
    during the training phase of our pipeline. Model-based selection relies on a preprocessing
    step that involves training a secondary machine learning model and using that
    model''s predictive power to select features.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们进行特征选择的目标是通过提高预测能力和减少时间成本来提升我们的机器学习能力。为了实现这一点，我们引入了两种广泛的特征选择类别：基于统计和基于模型。基于统计的特征选择将严重依赖于与我们的机器学习模型分开的统计测试，以便在我们的管道训练阶段选择特征。基于模型的选择依赖于一个预处理步骤，该步骤涉及训练一个二级机器学习模型，并使用该模型的预测能力来选择特征。
- en: Both of these types of feature selection attempt to reduce the size of our data
    by subsetting from our original features only the best ones with the highest predictive
    power. We may intelligently choose which feature selection method might work best
    for us, but in reality, a very valid way of working in this domain is to work
    through examples of each method and measure the performance of the resulting pipeline.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型的特征选择都试图通过从原始特征中仅选择具有最高预测能力的最佳特征来减少我们的数据集大小。我们可能可以智能地选择哪种特征选择方法最适合我们，但现实中，在这个领域工作的一个非常有效的方法是逐一研究每种方法的示例，并衡量结果管道的性能。
- en: To begin, let's take a look at the subclass of feature selection modules that
    are reliant on statistical tests to select viable features from a dataset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看依赖于统计测试从数据集中选择可行特征的特性选择模块的子类。
- en: Statistical-based feature selection
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于统计的特征选择
- en: 'Statistics provides us with relatively quick and easy methods of interpreting
    both quantitative and qualitative data. We have used some statistical measures
    in previous chapters to obtain new knowledge and perspective around our data,
    specifically in that we recognized mean and standard deviation as metrics that
    enabled us to calculate z-scores and scale our data. In this chapter, we will
    rely on two new concepts to help us with our feature selection:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 统计为我们提供了相对快速和简单的方法来解释定量和定性数据。我们在前面的章节中使用了一些统计度量来获取关于我们数据的新知识和视角，特别是我们认识到均值和标准差作为度量，使我们能够计算z分数并缩放我们的数据。在本章中，我们将依靠两个新概念来帮助我们进行特征选择：
- en: Pearson correlations
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 皮尔逊相关性
- en: hypothesis testing
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设检验
- en: Both of these methods are known as **univariate** methods of feature selection,
    meaning that they are quick and handy when the problem is to select out *single *features
    at a time in order to create a better dataset for our machine learning pipeline.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都被称为特征选择的**单变量**方法，这意味着当问题是要一次选择一个**单个**特征以创建更好的机器学习管道数据集时，它们既快又方便。
- en: Using Pearson correlation to select features
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用皮尔逊相关性选择特征
- en: 'We have actually looked at correlations in this book already, but not in the
    context of feature selection. We already know that we can invoke a correlation
    calculation in pandas by calling the following method:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中已经讨论过相关性，但不是在特征选择的背景下。我们已经知道，我们可以通过调用以下方法在pandas中调用相关性计算：
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output of the preceding code produces is the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出结果是以下内容：
- en: '![](img/8f4e5d5e-793b-4831-b227-1119091011c5.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8f4e5d5e-793b-4831-b227-1119091011c5.png)'
- en: 'As a continuation of the preceding table we have:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 作为前一个表的延续，我们有：
- en: '![](img/6d3f21ce-2e87-4bb3-8121-2689f6fa4807.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6d3f21ce-2e87-4bb3-8121-2689f6fa4807.png)'
- en: The Pearson correlation coefficient (which is the default for pandas) measures
    the *linear* relationship between columns. The value of the coefficient varies
    between -1 and +1, where 0 implies no correlation between them. Correlations closer
    to -1 or +1 imply an extremely strong linear relationship.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数（这是pandas的默认值）衡量列之间的**线性**关系。系数的值在-1和+1之间变化，其中0表示它们之间没有相关性。接近-1或+1的相关性表示非常强的线性关系。
- en: It is worth noting that Pearson’s correlation generally requires that each column
    be normally distributed (which we are not assuming). We can also largely ignore
    this requirement because our dataset is large (over 500 is the threshold).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，皮尔逊的相关性通常要求每个列都是正态分布的（我们并没有假设这一点）。我们也可以在很大程度上忽略这一要求，因为我们的数据集很大（超过500是阈值）。
- en: 'The `pandas .corr()` method calculates a Pearson correlation coefficient for
    every column versus every other column. This 24 column by 24 row matrix is very
    unruly, and in the past, we used `heatmaps` to try and make the information more
    digestible:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `heatmap` generated will be as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/895d47ee-91af-4695-8494-a20af98bf76f.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: Note that the `heatmap` function automatically chose the most correlated features
    to show us. That being said, we are, for the moment, concerned with the features
    correlations to the response variable. We will assume that the more correlated
    a feature is to the response, the more useful it will be. Any feature that is
    not as strongly correlated will not be as useful to us.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Correlation coefficients are also used to determine feature interactions and
    redundancies. A key method of reducing overfitting in machine learning is spotting
    and removing these redundancies. We will be tackling this problem in our model-based
    selection methods.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s isolate the correlations between the features and the response variable,
    using the following code:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can ignore the final row, as is it is the response variable correlated perfectly
    to itself. We are looking for features that have correlation coefficient values
    close to -1 or +1\. These are the features that we might assume are going to be
    useful. Let's use pandas filtering to isolate features that have at least .2 correlation
    (positive or negative).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do this by first defining a pandas *mask*, which will act as our filter,
    using the following code:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Every `False` in the preceding pandas Series represents a feature that has
    a correlation value between -.2 and .2 inclusive, while `True` values correspond
    to features with preceding correlation values .2 or less than -0.2\. Let''s plug
    this mask into our pandas filtering, using the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The variable `highly_correlated_features` is supposed to hold the features
    of the dataframe that are highly correlated to the response; however, we do have
    to get rid of the name of the response column, as including that in our machine
    learning pipeline would be cheating:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So, now we have five features from our original dataset that are meant to be
    predictive of the response variable, so let''s try it out with the help of the
    following code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Our accuracy is definitely worse than the accuracy to beat, .8203, but also
    note that the fitting time saw about a 20-fold increase. Our model is able to
    learn almost as well as with the entire dataset with only five features. Moreover,
    it is able to learn as much in a much shorter timeframe.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Let's bring back our scikit-learn pipelines and include our correlation choosing
    methodology as a part of our preprocessing phase. To do this, we will have to
    create a custom transformer that invokes the logic we just went through, as a
    pipeline-ready class.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'We will call our class the `CustomCorrelationChooser` and it will have to implement
    both a fit and a transform logic, which are:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The fit logic will select columns from the features matrix that are higher than
    a specified threshold
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合逻辑将选择特征矩阵中高于指定阈值的列
- en: The transform logic will subset any future datasets to only include those columns
    that were deemed important
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换逻辑将子集任何未来的数据集，只包括被认为重要的列
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s take our new correlation feature selector for a spin, with the help
    of the following code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用以下代码来试用我们新的相关特征选择器：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Our class has selected the same five columns as we found earlier. Let''s test
    out the transform functionality by calling it on our `X` matrix, using the following
    code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的这个类别选择了我们之前找到的相同的五列。让我们通过在`X`矩阵上调用它来测试转换功能，以下代码如下：
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code produces the following table as the output:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生以下表格作为输出：
- en: '|  | **PAY_0** | **PAY_2** | **PAY_3** | **PAY_4** | **PAY_5** |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | **PAY_0** | **PAY_2** | **PAY_3** | **PAY_4** | **PAY_5** |'
- en: '| **0** | 2 | 2 | -1 | -1 | -2 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 2 | 2 | -1 | -1 | -2 |'
- en: '| **1** | -1 | 2 | 0 | 0 | 0 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **1** | -1 | 2 | 0 | 0 | 0 |'
- en: '| **2** | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0 | 0 | 0 | 0 | 0 |'
- en: '| **3** | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 0 | 0 | 0 | 0 | 0 |'
- en: '| **4** | -1 | 0 | -1 | 0 | 0 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| **4** | -1 | 0 | -1 | 0 | 0 |'
- en: 'We see that the `transform` method has eliminated the other columns and kept
    only the features that met our `.2` correlation threshold. Now, let''s put it
    all together in our pipeline, with the help of the following code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到`transform`方法已经消除了其他列，只保留了满足我们`.2`相关阈值的特征。现在，让我们在以下代码的帮助下将所有这些整合到我们的管道中：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Wow! Our first attempt at feature selection and we have already beaten our
    goal (albeit by a little bit). Our pipeline is showing us that if we threshold
    at `0.1`, we have eliminated noise enough to improve accuracy and also cut down
    on the fitting time (from .158 seconds without the selector). Let''s take a look
    at which columns our selector decided to keep:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我们在特征选择的第一尝试中已经超过了我们的目标（尽管只是略微）。我们的管道显示，如果我们以`0.1`为阈值，我们就已经消除了足够的噪声以改善准确性，并且还减少了拟合时间（从没有选择器的0.158秒）。让我们看看我们的选择器决定保留哪些列：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It appears that our selector has decided to keep the five columns that we found,
    as well as two more, the `LIMIT_BAL` and the `PAY_6` columns. Great! This is the
    beauty of automated pipeline gridsearching in scikit-learn. It allows our models
    to do what they do best and intuit things that we could not have on our own.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的选择器决定保留我们找到的五列，以及另外两列，即`LIMIT_BAL`和`PAY_6`列。太棒了！这是scikit-learn中自动化管道网格搜索的美丽之处。它允许我们的模型做它们最擅长的事情，并直觉到我们自己无法做到的事情。
- en: Feature selection using hypothesis testing
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用假设检验进行特征选择
- en: Hypothesis testing is a methodology in statistics that allows for a bit more
    complex statistical testing for individual features. Feature selection via hypothesis
    testing will attempt to select only the best features from a dataset, just as
    we were doing with our custom correlation chooser, but these tests rely more on
    formalized statistical methods and are interpreted through what are known as **p-values**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验是统计学中的一种方法，它允许对单个特征进行更复杂的统计检验。通过假设检验进行特征选择将尝试从数据集中选择最佳特征，正如我们在自定义相关选择器中所做的那样，但这些测试更多地依赖于形式化的统计方法，并通过所谓的**p值**进行解释。
- en: A hypothesis testis a statistical test that is used to figure out whether we
    can apply a certain condition for an entire population, given a data sample. The
    result of a hypothesis test tells us whether we should believe the hypothesis
    or reject it for an alternative one. Based on sample data from a population, a
    hypothesis test determines whether or not to reject the null hypothesis. We usually
    use a **p-value**(a non-negative decimal with an upper bound of 1, which is based
    on our significance level) to make this conclusion.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 假设检验是一种统计检验，用于确定在给定数据样本的情况下，我们是否可以应用对整个总体适用的某个条件。假设检验的结果告诉我们是否应该相信假设，或者拒绝它以选择另一个假设。基于来自总体的样本数据，假设检验确定是否拒绝零假设。我们通常使用**p值**（一个非负的小数，其上界为1，基于我们的显著性水平）来得出这个结论。
- en: 'In the case of feature selection, the hypothesis we wish to test is along the
    lines of: *True or False: This feature has no relevance to the response variable. *We
    want to test this hypothesis for every feature and decide whether the features
    hold some significance in the prediction of the response. In a way, this is how
    we dealt with the correlation logic. We basically said that, if a column''s correlation
    with the response is too weak, then we say that the hypothesis that the feature
    has no relevance is true. If the correlation coefficient was strong enough, then
    we can reject the hypothesis that the feature has no relevance in favor of an
    alternative hypothesis, that the feature does have some relevance.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin to use this for our data, we will have to bring in two new modules:
    `SelectKBest` and `f_classif`, using the following code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`SelectKBest` is basically just a wrapper that keeps a set amount of features
    that are the highest ranked according to some criterion. In this case, we will
    use the p-values of completed hypothesis testings as a ranking.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the p-value
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The p-values are a decimals between 0 and 1 that represent the probability that
    the data given to us occurred by chance under the hypothesis test. Simply put,
    the lower the p-value, the better the chance that we can reject the null hypothesis.
    For our purposes, the smaller the p-value, the better the chances that the feature
    has some relevance to our response variable and we should keep it.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: For a more in-depth handling of statistical testing, check out* Principles of
    Data Science*, [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science),
    by Packt Publishing.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The big take away from this is that the `f_classif` function will perform an
    ANOVA test (a type of hypothesis test) on each feature on its own (hence the name
    univariate testing) and assign that feature a p-value. The `SelectKBest` will
    rank the features by that p-value (the lower the better) and keep only the best
    k (a human input) features. Let's try this out in Python.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Ranking the p-value
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin by instantiating a `SelectKBest` module. We will manually enter
    a `k` value, `5`, meaning we wish to keep only the five best features according
    to the resulting p-values:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can then fit and transform our `X` matrix to select the features we want,
    as we did before with our custom selector:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If we want to inspect the `p-values` directly and see which columns were chosen,
    we can dive deeper into the select `k_best` variables:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code produces the following table as the output:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **column** | **p_value** |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| **5** | PAY_0 | 0.000000e+00 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| **6** | PAY_2 | 0.000000e+00 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| **7** | PAY_3 | 0.000000e+00 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| **8** | PAY_4 | 1.899297e-315 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| **9** | PAY_5 | 1.126608e-279 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: 'We can see that, once again, our selector is choosing the `PAY_X` columns as
    the most important. If we take a look at our `p-value` column, we will notice
    that our values are extremely small and close to zero. A common threshold for
    p-values is `0.05`, meaning that anything less than 0.05 may be considered significant,
    and these columns are extremely significant according to our tests. We can also
    directly see which columns meet a threshold of 0.05 using the pandas filtering
    methodology:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code produces the following table as the output:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **column** | **p_value** |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| **5** | PAY_0 | 0.000000e+00 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| **6** | PAY_2 | 0.000000e+00 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| **7** | PAY_3 | 0.000000e+00 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| **8** | PAY_4 | 1.899297e-315 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| **9** | PAY_5 | 1.126608e-279 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| **10** | PAY_6 | 7.296740e-234 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| **0** | LIMIT_BAL | 1.302244e-157 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| **17** | PAY_AMT1 | 1.146488e-36 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| **18** | PAY_AMT2 | 3.166657e-24 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| **20** | PAY_AMT4 | 6.830942e-23 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| **19** | PAY_AMT3 | 1.841770e-22 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| **21** | PAY_AMT5 | 1.241345e-21 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| **22** | PAY_AMT6 | 3.033589e-20 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| **1** | SEX | 4.395249e-12 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| **2** | EDUCATION | 1.225038e-06 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| **3** | MARRIAGE | 2.485364e-05 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| **11** | BILL_AMT1 | 6.673295e-04 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| **12** | BILL_AMT2 | 1.395736e-02 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| **13** | BILL_AMT3 | 1.476998e-02 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| **4** | AGE | 1.613685e-02 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: 'The majority of the columns have a low `p-value`, but not all. Let''s see the
    columns with a higher `p_value`, using the following code:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code produces the following table as the output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **column** | **p_value** |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| **14** | BILL_AMT4 | 0.078556 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| **15** | BILL_AMT5 | 0.241634 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| **16** | BILL_AMT6 | 0.352123 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: 'These three columns have quite a high `p-value`. Let''s use our `SelectKBest`
    in a pipeline to see if we can grid search our way into a better machine learning
    pipeline, using the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'It seems that our `SelectKBest` module is getting about the same accuracy as
    our custom transformer, but it''s getting there a bit quicker! Let''s see which
    columns our tests are selecting for us, with the help of the following code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding code produces the following table as the output:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **column** | **p_value** |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| **5** | PAY_0 | 0.000000e+00 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| **6** | PAY_0 | 0.000000e+00 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| **7** | PAY_0 | 0.000000e+00 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| **8** | PAY_0 | 1.899297e-315 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: '| **9** | PAY_0 | 1.126608e-279 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| **10** | PAY_0 | 7.296740e-234 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| **0** | LIMIT_BAL | 1.302244e-157 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: They appear to be the same columns that were chosen by our other statistical
    method. It's possible that our statistical method is limited to continually picking
    these seven columns for us.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other tests available besides ANOVA, such as Chi² and others, for
    regression tasks. They are all included in scikit-learn''s documentation. For
    more info on feature selection through univariate testing, check out the scikit-learn
    documentation here:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to model-based feature selection, it''s helpful to do a quick
    sanity check to ensure that we are on the right track. So far, we have seen two
    statistical methods for feature selection that gave us the same seven columns
    for optimal accuracy. But what if we were to take every column **except** those
    seven? We should expect a much lower accuracy and worse pipeline overall, right?
    Let''s make sure. The following code helps us to implement sanity checks:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: OK, so by selecting the columns except those seven, we see not only worse accuracy
    (almost as bad as the null accuracy), but also slower fitting times on average.
    With this, I believe we may move on to our next subset of feature selection techniques,
    the model-based methods.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Model-based feature selection
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our last section dealt with using statistical methods and testing in order to
    select features from the original dataset to improve our machine learning pipeline,
    both in predictive performance, as well as in time-complexity. In doing so, we
    were able to see first-hand the effects of using feature selection.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: A brief refresher on natural language processing
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If talking about feature selection has sounded familiar from the very beginning
    of this chapter, almost as if we were doing it even before we began with correlation
    coefficients and statistical testing, well, you aren't wrong. In [Chapter 4](430d621e-7ce6-48c0-9990-869e82a0d0c6.xhtml), *Feature Construction* when
    dealing with feature construction, we introduced the concept of the `CountVectorizer`,
    a module in scikit-learn designed to construct features from text columns and
    use them in machine learning pipelines.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CountVectorizer` had many parameters that we could alter in search of
    the best pipeline. Specifically, there were a few built-in feature selection parameters:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features`: This integer set a hard limit of the maximum number of features
    that the featurizer could remember. The features that were remembered were decided
    based on a ranking system where the rank of a token was the count of the token
    in the corpus.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_df`: This float limited the number of features by imposing a rule stating
    that a token may only appear in the dataset if it appeared in the corpus as a
    rate strictly greater than the value for `min_df`.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_df`: Similar to `min_df`, this float limits the number of features by
    only allowing tokens that appear in the corpus at a rate strictly lower than the
    value set for `max_df`.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop_words`: Limits the type of tokens allowed by matching them against a
    static list of tokens. If a token is found that exists in the `stop_words` set,
    that word, no matter if it occurs at the right amount to be allowed by `min_df`
    and `max_df`, is ignored.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the previous chapter, we briefly introduced a dataset aimed at predicting
    the sentiment of a tweet based purely on the words in that tweet. Let''s take
    some time to refresh our memories on how to use these parameters. Let''s begin
    by bringing in our `tweet` dataset, with the help of the following code:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To refresh our memory, let''s look at the first five `tweets`, using the following
    code:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code produces the following table as the output:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ItemID** | **Sentiment** | **SentimentText** |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 | is so sad for my APL frie... |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 0 | I missed the New Moon trail... |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3 | 1 | omg its already 7:30 :O |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| 3 | 4 | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| 4 | 5 | 0 | i think mi bf is cheating on me!!! ... |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: 'Let''s create a feature and a response variable. Recall that, because we are
    working with text, our feature variable will simply be the text column and not
    a two-dimensional matrix like it usually is:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s set up a pipeline and evaluate it using the same function that we''ve
    been using in this chapter, with the help of the following code:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: A decent score (recalling that the null accuracy was .564), but we were able
    to beat this in the last chapter by using a `FeatureUnion` module to combine features
    from `TfidfVectorizer` and `CountVectorizer`.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'To try out the techniques we''ve seen in this chapter, let''s go ahead and
    apply a `SelectKBest` in a pipeline with a `CountVectorizer`. Let''s see if we
    can rely not on the built-in `CountVectorizer` feature selection parameters, but
    instead, on statistical testing:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: It seems that `SelectKBest` didn't do as well for text tokens, and without `FeatureUnion`,
    we were unable to compete with the previous chapter's accuracy scores. Either
    way, for both pipelines, it is worth noting that the time it takes to both fit
    and predict are extremely poor. This is because statistical univariate methods
    are not optimal for a very large number of features, such as the features obtained
    from text vectorization.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Using machine learning to select features
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using `CountVectorizer` built-in feature selection tools is great when you are
    dealing with text; however, we are usually dealing with data already built into
    a row/column structure. We've seen the power of using purely statistical methodology
    for feature selection, and now let's see how we can invoke the awesome power of
    machine learning to, hopefully, do even more. The two main machine learning models
    that we will use in this section for the purposes of feature selection are tree-based
    models and linear models. They both have a notion of feature ranking that are
    useful when subsetting feature sets.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Before we go further, we believe it is worth mentioning again that these methods,
    while different in their methodology of selection, are attempting to find the
    optimal subset of features to improve our machine learning pipelines. The first
    method we will dive into will involve the internal importance metrics that algorithms
    such as decision trees and random forest models generate whilst fitting to training
    data.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based model feature selection metrics
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When fitting decision trees, the tree starts at the root node and greedily chooses
    the optimal split at every junction that optimizes a certain metric of **node
    purity**. By default, scikit-learn optimizes for the **gini** metric at every
    step. While each split is created, the model keeps track of how much each split
    helps the overall optimization goal. In doing so, tree-based models that choose
    splits based on such metrics have a notion of** feature importance**.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this further, let''s go ahead and fit a decision tree to our
    data and output the feature importance'' with the help of the following code:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Once our tree has fit to the data, we can call on the `feature_importances_
    attribute` to capture the importance of the feature, relative to the fitting of
    the tree:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The preceding code produces the following table as the output:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **feature** | **importance** |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| 5 | PAY_0 | 0.161829 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| 4 | AGE | 0.074121 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| 11 | BILL_AMT1 | 0.064363 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| 0 | LIMIT_BAL | 0.058788 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| 19 | PAY_AMT3 | 0.054911 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: What this table is telling us is that the most important feature while fitting
    was the column `PAY_0`, which matches up to what our statistical models were telling
    us earlier in this chapter. What is more notable are the second, third, and fifth
    most important features, as they didn't really show up before using our statistical
    tests. This is a good indicator that this method of feature selection might yield
    some new results for us.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that, earlier, we relied on a built in scikit-learn wrapper called SelectKBest
    to capture the top *k* features based on a ranking function like ANOVA p-values.
    We will introduce another similar style of wrapper called `SelectFromModel` which,
    like `SelectKBest`, will capture the top k most importance features. However,
    it will do so by listening to a machine learning model''s internal metric for
    feature importance rather than the p-values of a statistical test. We will use
    the following code to define the `SelectFromModel`:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The biggest difference in usage between `SelectFromModel` and `SelectKBest`
    is that `SelectFromModel` doesn''t take in an integer k, which represents the
    number of features to keep, but rather `SelectFromModel` uses a threshold for
    selection which acts as a hard minimum of importance to be selected. In this way,
    the model-based selectors of this chapter are able to move away from a human-inputted
    number of features to keep and instead rely on relative importance to include
    only as many features as the pipeline needs. Let''s instantiate our class as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s fit this `SelectFromModel` class to our data and invoke the transform
    method to watch our data get subsetted, with the help of the following code:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now that we know the basic mechanics of the module, let''s use it to select
    features for us in a pipeline. Recall that the accuracy to beat is .8206, which
    we got both from our correlation chooser and our ANOVA test (because they both
    returned the same features):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note first that, as part of the threshold parameter, we are able to include
    some reserved words rather than a float that represents the minimum importance
    to use. For example, the threshold of `mean` only selects features with an importance
    that is higher than average. Similarly, a value of median as a threshold only
    selects features that are more important than the median value. We may also include
    multiples to these reserved words so that `2.*mean` will only include features
    that are more important than twice the mean importance value.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a peak as to which features our decision tree-based selector is
    choosing for us. We can do this by invoking a method within `SelectFromModel`
    called `get_support()`. It will return an array of Booleans, one for each original
    feature column, and tell us which of the features it decided to keep, as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Wow! So the tree decided to keep all but two features, and still only did just
    as good as the tree did without selecting anything:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: For more information on decision trees and how they are fit using gini or entropy,
    look into the scikit-learn documentation or other texts that handle this topic
    in more depth.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: We could continue onward by trying several other tree-based models, such as
    RandomForest, ExtraTreesClassifier, and others, but perhaps we may be able to
    do better by utilizing a model other than a tree-based model.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Linear models and regularization
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `SelectFromModel` selector is able to handle any machine learning model
    that exposes a `feature_importances_` or**` coef_ attribute`** post-fitting. Tree-based
    models expose the former, while linear models expose the latter. After fitting,
    linear models such as Linear Regression, Logistic Regression, Support Vector Machines,
    and others all place coefficients in front of features that represent the slope
    of that feature/how much it affects the response when that feature is changed.
    `SelectFromModel` can equate this to a feature importance and choose features
    based on the coefficients given to features while fitting.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Before we can use these models, however, we must introduce a concept called
    **regularization**, which will help us select truly only the most important features.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to regularization
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In linear models, **regularization** is a method for imposing additional constraints
    to a learning model, where the goal is to prevent overfitting and improve the
    generalization of the data. This is done by adding extra terms to the *loss function*
    being optimized, meaning that, while fitting, regularized linear models may severely
    diminish, or even destroy features along the way. There are two widely used regularization
    methods, called L1 and L2 regularization. Both regularization techniques rely
    on the L-p Norm, which is defined for a vector as being:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68897f7a-1aea-478f-a42f-0f6a740ffa75.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
- en: '**L1 ****regularization**, also known as **lasso** regularization, uses the
    L1 Norm, which, using the above formula, reduces to the sum of the absolute values
    of the entries of a vector to limit the coefficients in such a way that they may
    disappear entirely and become 0\. If the coefficient of a feature drops to 0,
    then that feature will not have any say in the prediction of new data observations
    and definitely will not be chosen by a `SelectFromModel` selector.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 regularization**, also known as **ridge** regularization, imposes the
    L2 norm as a penalty (sum of the square of vector entries) so that coefficients
    cannot drop to 0, but they can become very, very tiny.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization also helps with multicollinearity, the problem of having multiple
    features in a dataset that are linearly related to one another. A Lasso Penalty
    (L1) will force coefficients of dependent features to 0, ensuring that they aren't
    chosen by the selector module, helping combat overfitting.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Linear model coefficients as another feature importance metric
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use L1 and L2 regularization to find optimal coefficients for our feature
    selection, just as we did with our tree-based models. Let''s use a logistic regression
    model as our model-based selector and gridsearch across both the L1 and L2 norm:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Finally! We got an accuracy better than our statistical testing selector. Let''s
    see which features our model-based selector decided to keep by invoking the `get_support()`
    method of `SelectFromModel` again:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Fascinating! Our logistic regression based selector kept most of the `PAY_X`
    columns but was also able to figure out that the sex, education, and marriage
    status of the person was going to play a hand in prediction. Let's continue our
    adventure by using one more model with our `SelectFromModel` selector module,
    a support vector machine classifier.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are unfamiliar with support vector machines, they are classification
    models that attempt to draw linear boundaries in space to separate binary labels.
    These linear boundaries are known as support vectors. For now, the most important
    difference between logistic regression and support vector classifiers are that
    SVCs are usually better equipped to optimize coefficients for maximizing accuracy
    for binary classification tasks, while logistic regression is better at modeling
    the probabilistic attributes of binary classification tasks. Let''s implement
    a Linear SVC model from scikit-learn as we did for decision trees and logistic
    regression and see how it fares, using the following code:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Great! The best accuracy we''ve gotten so far. We can see that the fitting
    time took a hit but if we are OK with that, couple the best accuracy so far with
    an outstandingly quick predicting time and we''ve got a great machine learning
    pipeline on our hands; one that leverages the power of regularization in the context
    of support vector classification to feed significant features into a decision
    tree classifier. Let''s see which features our selector chose to give us our best
    accuracy to date:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The only difference between these features and what our logistic regression
    got was the `PAY_4` column. But we can see that even removing a single column
    can affect our entire pipeline's performance.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right feature selection method
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, you may be feeling a bit overwhelmed with the information in
    this chapter. We have presented several ways of performing feature selection,
    some based on pure statistics and others based on the output of secondary machine
    learning models. It is natural to wonder how to decide which feature selection
    method is right for your data. In theory, if you are able to try multiple options,
    as we did in this chapter, that would be ideal, but we understand that it might
    not be feasible to do so. The following are some rules of thumbs that you can
    follow when you are trying to prioritize which feature selection module is more
    likely to offer greater results:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: If your features are mostly categorical, you should start by trying to implement
    a `SelectKBest` with a Chi² ranker or a tree-based model selector.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your features are largely quantitative (like ours were), using linear models
    as model-based selectors and relying on correlations tends to yield greater results,
    as was shown in this chapter.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are solving a binary classification problem, using a Support Vector Classification
    model along with a `SelectFromModel` selector will probably fit nicely, as the
    SVC tries to find coefficients to optimize for binary classification tasks.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A little bit of EDA can go a long way in manual feature selection. The importance
    of having domain knowledge in the domain from which the data originated cannot
    be understated.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That being said, these are meant only to be used as guidelines. As a data scientist, ultimately
    you decide which features you wish to keep to optimize the metric of your choosing.
    The methods that we provide in this text are here to help you in your discovery
    of the latent power of features hidden by noise and multicollinearity.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned a great deal about methodologies for selecting subsets
    of features in order to increase the performance of our machine learning pipelines
    in both a predictive capacity as well in-time-complexity.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that we chose had a relatively low number of features. If selecting,
    however, from a very large set of features (over a hundred), then the methods
    in this chapter will likely start to become entirely too cumbersome. We saw that
    in this chapter, when attempting to optimize a `CountVectorizer` pipeline, the
    time it would take to run a univariate test on every feature is not only astronomical;
    we would run a greater risk of experiencing multicollinearity in our features
    by sheer coincidence.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce purely mathematical transformations that
    we may apply to our data matrices in order to alleviate the trouble of working
    with vast quantities of features, or even a few highly uninterpretable features.
    We will begin to work with datasets that stray away from what we have seen before,
    such as image data, topic modeling data, and more.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
