- en: Feature Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''re halfway through our text and we have gotten our hands dirty with about
    a dozen datasets and have seen a great deal of feature selection methods that
    we, as data scientists and machine learning engineers, may utilize in our work
    and lives to ensure that we are getting the most out of our predictive modeling.
    So far, in dealing with data, we have worked with methods including:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature understanding through the identification of levels of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature improvements and imputing missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature standardization and normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the preceding methods has a place in our data pipeline and, more often
    than not, two or more methods are used in tandem with one another.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of this text will focus on other methods of feature engineering
    that are, by nature, a bit more mathematical and complex than in the first half
    of this book. As the preceding workflow grows, we will do our best to spare the
    reader the inner workings of each and every statistical test we invoke and instead
    convey a broader picture of what the tests are trying to achieve. As authors and
    instructors, we are always open to your questions about any of the inner mechanisms
    of this work.
  prefs: []
  type: TYPE_NORMAL
- en: We have come across one problem quite frequently in our discussion of features,
    and that problem is **noise**. Often, we are left working with features that may
    not be highly predictive of the response and, sometimes, can even hinder our models'
    performance in the prediction of the response. We used tools such as standardization
    and normalization to try to mitigate such damage, but at the end of the day, noise
    must be dealt with.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will address a subset of feature engineering called **feature
    selection**, which is the process of selecting which features, from the original
    batch of features, are the *best* when it comes to the model prediction pipeline.
    More formally, given *n* features, we search for a subset of *k*, where *k < n*
    features that improve our machine learning pipeline. This generally comes down
    to the statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feature Selection attempts to weed out the noise in our data and remove it.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of feature selection touches on two major points that must be
    addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: The methods in which we may find the subset of *k* features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The definition of *better* in the context of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The majority of this chapter is dedicated to the methods in which we may find
    such subsets of features and the basis on which such methods work. This chapter
    will break up the methods of feature selection into two broad subsections: **statistical-based**
    and **model-based** feature selection. This separation may not 100% capture the
    complexity of the science and art of feature selection, but will work to drive
    real and actionable results in our machine learning pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the deep end of many of these methods, let's first discuss
    how we may better understand and define the idea of *better*, as it will frame
    the remainder of this chapter, as well as framing the remainder of this text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Achieving better performance in feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a baseline machine learning pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The types of feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right feature selection method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving better performance in feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book, we have relied on a base definition of *better* when
    it came to the various feature engineering methods we put into place. Our implicit
    goal was to achieve better predictive performance measured purely on simple metrics
    such as accuracy for classification tasks and RMSE for regression tasks (mostly
    accuracy). There are other metrics we may measure and track to gauge predictive
    performance. For example, we will use the following metrics for classification:'
  prefs: []
  type: TYPE_NORMAL
- en: True and false positive rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitivity (AKA true positive rate) and specificity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False negative and false positive rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and for regression, the metrics that will be applied are:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R²
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These lists go on, and while we will not be abandoning the idea of quantifying
    performance through metrics such as the ones precedingly listed, we may also measure
    other *meta metrics*, or metrics that do not directly correlate to the performance
    of the prediction of the model, rather, so-called **meta metrics** attempt to
    measure the performance *around* the prediction and include such ideas as:'
  prefs: []
  type: TYPE_NORMAL
- en: Time in which the model needs to fit/train to the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time it takes for a fitted model to predict new instances of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the data in case data must be persisted (stored for later)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These ideas will add to our definition of *better* machine learning as they
    help to encompass a much larger picture of our machine learning pipeline outside
    of model predictive performance. In order to help us track these metrics, let''s
    create a function that is generic enough to evaluate several models but specific
    enough to give us metrics for each one. We will call our function `get_best_model_and_accuracy` and it
    will do many jobs, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: It will search across all given parameters in order to optimize the machine
    learning pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will spit out some metrics that will help us assess the quality of the pipeline
    entered
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s go ahead and define such a function with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The overall goal of this function is to act as a ground truth in that we will
    use it to evaluate every feature selection method in this chapter to give us a
    sense of standardization of evaluation. This is not really any different to what
    we have been doing already, but we are now formalizing our work as a function,
    and also using metrics other than accuracy to grade our feature selection modules
    and machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: A case study – a credit card defaulting dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By intelligently extracting the most important signals from our data and ignoring
    noise, feature selection algorithms achieve two major outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved model performance**: By removing redundant data, we are less likely
    to make decisions based on noisy and irrelevant data, and it also allows our models
    to hone in on the important features, thereby improving model pipeline predictive
    performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced training and predicting time**: By fitting pipelines to less data,
    this generally results in improved model fitting and predicting times, making
    our pipelines faster overall'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to gain a realistic understanding of how and why noisy data gets in
    the way, let''s introduce our newest dataset, a credit card defaulting dataset.
    We will work with 23 features and one response variable. That response variable
    will be a Boolean, meaning it will either be True or False. The reason we are
    working with 23 features is that we want to see if we can find which of the 23
    features will help us in our machine learning pipelines and which ones will hurt
    us. We can import the datasets using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To start, let''s bring in two common modules, `numpy` and `pandas`, and also
    set a random seed so that you and we will achieve the same results for consistency.
    Now, let''s bring in our latest dataset, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go ahead and do some mandatory EDA. Let''s begin by checking how big
    a dataset we are working with, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we have `30,000 rows` (observations) and `24 columns` (1 response and 23
    features). We will not go in depth to describe the columns meanings at this time,
    but we do encourage the reader to check out the source of the data ([http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#](http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#)).
    For now, we will rely on good old-fashioned statistics to tell us more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **count** | **mean** | **std** | **min** | **25%** | **50%** | **75%**
    | **max** |'
  prefs: []
  type: TYPE_TB
- en: '| **LIMIT_BAL** | 30000.0 | 167484.322667 | 129747.661567 | 10000.0 | 50000.00
    | 140000.0 | 240000.00 | 1000000.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **SEX** | 30000.0 | 1.603733 | 0.489129 | 1.0 | 1.00 | 2.0 | 2.00 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **EDUCATION** | 30000.0 | 1.853133 | 0.790349 | 0.0 | 1.00 | 2.0 | 2.00 |
    6.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **MARRIAGE** | 30000.0 | 1.551867 | 0.521970 | 0.0 | 1.00 | 2.0 | 2.00 |
    3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **AGE** | 30000.0 | 35.485500 | 9.217904 | 21.0 | 28.00 | 34.0 | 41.00 |
    79.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_0** | 30000.0 | -0.016700 | 1.123802 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_2** | 30000.0 | -0.133767 | 1.197186 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_3** | 30000.0 | -0.166200 | 1.196868 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_4** | 30000.0 | -0.220667 | 1.169139 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_5** | 30000.0 | -0.266200 | 1.133187 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_6** | 30000.0 | -0.291100 | 1.149988 | -2.0 | -1.00 | 0.0 | 0.00 |
    8.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT1** | 30000.0 | 51223.330900 | 73635.860576 | -165580.0 | 3558.75
    | 22381.5 | 67091.00 | 964511.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT2** | 30000.0 | 49179.075167 | 71173.768783 | -69777.0 | 2984.75
    | 21200.0 | 64006.25 | 983931.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT3** | 30000.0 | 47013.154800 | 69349.387427 | -157264.0 | 2666.25
    | 20088.5 | 60164.75 | 1664089.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT4** | 30000.0 | 43262.948967 | 64332.856134 | -170000.0 | 2326.75
    | 19052.0 | 54506.00 | 891586.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT5** | 30000.0 | 40311.400967 | 60797.155770 | -81334.0 | 1763.00
    | 18104.5 | 50190.50 | 927171.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **BILL_AMT6** | 30000.0 | 38871.760400 | 59554.107537 | -339603.0 | 1256.00
    | 17071.0 | 49198.25 | 961664.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT1** | 30000.0 | 5663.580500 | 16563.280354 | 0.0 | 1000.00 | 2100.0
    | 5006.00 | 873552.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT2** | 30000.0 | 5921.163500 | 23040.870402 | 0.0 | 833.00 | 2009.0
    | 5000.00 | 1684259.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT3** | 30000.0 | 5225.681500 | 17606.961470 | 0.0 | 390.00 | 1800.0
    | 4505.00 | 891586.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT4** | 30000.0 | 4826.076867 | 15666.159744 | 0.0 | 296.00 | 1500.0
    | 4013.25 | 621000.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT5** | 30000.0 | 4799.387633 | 15278.305679 | 0.0 | 252.50 | 1500.0
    | 4031.50 | 426529.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **PAY_AMT6** | 30000.0 | 5215.502567 | 17777.465775 | 0.0 | 117.75 | 1500.0
    | 4000.00 | 528666.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **default payment next month** | 30000.0 | 0.221200 | 0.415062 | 0.0 | 0.00
    | 0.0 | 0.00 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: The **default payment next month** is our response column and everything else
    is a feature/potential predictor of default. It is wildly clear that our features
    exist on wildly different scales, so that will be a factor in how we handle the
    data and which models we will pick. In previous chapters, we dealt heavily with
    data and features on different scales using solutions such as `StandardScalar`
    and normalization to alleviate some of these issues; however, in this chapter,
    we will largely choose to ignore such problems in order to focus on more relevant
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter of this book, we will focus on several case studies that
    will marry almost all of the techniques in this book on a longer-term analysis
    of a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen in previous chapters, we know that null values are a big issue
    when dealing with machine learning, so let''s do a quick check to make sure that
    we don''t have any to deal with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Phew! No missing values here. Again, we will deal with missing values again
    in future case studies, but for now, we have bigger fish to fry. Let''s go ahead
    and set up some variables for our machine learning pipelines, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, we created our `X` and `y` variables. Our `X` matrix will have 30,000
    rows and 23 columns and our `y` is, as always, a 30,000 long pandas Series. Because
    we will be performing classification, we will, as usual, need to ascertain a null
    accuracy to ensure that our machine learning models are performing better than
    a baseline. We can get the null accuracy rate using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: So, the accuracy to beat, in this case, is **77.88%**, which is the percentage
    of people who did not default (0 meaning false to default).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a baseline machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous chapters, we offered to you, the reader, a single machine learning
    model to use throughout the chapter. In this chapter, we will do some work to
    find the best machine learning model for our needs and then work to enhance that
    model with feature selection. We will begin by importing four different machine
    learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Nearest Neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for importing the learning models is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we are finished importing these modules, we will run them through our `get_best_model_`and`_accuracy`
    functions to get a baseline on how each one handles the raw data. We will have
    to first establish some variables to do so. We will use the following code to
    do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If you feel uncomfortable with any of the models listed above, we recommend
    reading up on documentation, or referring to the Packt book, *The Principles of
    Data Science*, [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science),
    for a more detailed explanation of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we will be sending each model through our function, which invokes a
    grid search module, we need only create blank state models with no customized
    parameters set, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now going to run each of the four machine learning models through our
    evaluation function to see how well (or not) they do against our dataset. Recall
    that our number to beat at the moment is .7788, the baseline null accuracy. We
    will use the following code to run the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the logistic regression has already beaten the null accuracy
    using the raw data and, on average, took 6/10 of a second to fit to a training
    set and only 20 milliseconds to score. This makes sense if we know that to fit,
    a logistic regression in **scikit-learn** must create a large matrix in memory,
    but to predict, it need only multiply and add scalars to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s do the same with the KNN model, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Our KNN model, as expected, does much better on the fitting time. This is because,
    to fit to the data, the KNN only has to store the data in such a way that it is
    easily retrieved at prediction time, where it takes a hit on time. It's also worth
    mentioning the painfully obvious fact that the accuracy is not even better than
    the null accuracy! You might be wondering why, and if you're saying *hey wait
    a minute, doesn't KNN utilize the Euclidean Distance in order to make predictions,
    which can be thrown off by non-standardized data, a flaw that none of the other
    three machine learning models suffer?*, then you're 100% correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'KNN is a distance-based model, in that it uses a metric of closeness in space
    that assumes that all features are on the same scale, which we already know that
    our data is not on. So, for KNN, we will have to construct a more complicated
    pipeline to more accurately assess its baseline performance, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to notice is that our modified code pipeline, which now includes
    a `StandardScalar` (which z-score normalizes our features) now beats the null
    accuracy at the very least, but also seriously hurts our predicting time, as we
    have added a step of preprocessing. So far, the logistic regression is in the
    lead with the best accuracy and the better overall timing of the pipeline. Let''s
    move on to our two tree-based models and start with the simpler of the two, the
    decision tree, with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Amazing! Already, we have a new lead in accuracy and, also, the decision tree
    is quick to both fit and predict. In fact, it beats logistic regression in its
    time to fit and beats the KNN in its time to predict. Let''s finish off our test
    by evaluating a random forest, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Much better than either the Logistic Regression or the KNN, but not better
    than the decision tree. Let''s aggregate these results to see which model we should
    move forward with in optimizing using feature selection:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model Name** | **Accuracy (%)** | **Fit Time (s)** | **Predict Time (s)**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic Regression | .8096 | .602 | **.002** |'
  prefs: []
  type: TYPE_TB
- en: '| KNN (with scaling) | .8008 | **.035** | 6.72 |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | **.8203** | .158 | **.002** |'
  prefs: []
  type: TYPE_TB
- en: '| Random Forest | .8196 | 1.107 | .044 |'
  prefs: []
  type: TYPE_TB
- en: 'The decision tree comes in first for accuracy and tied for first for predict
    time with logistic regression, while KNN with scaling takes the trophy for being
    the fastest to fit to our data. Overall, the decision tree appears to be the best
    model to move forward with, as it came in first for, arguably, our two most important
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: We definitely want the best accuracy to ensure that out of sample predictions
    are accurate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having a prediction time is useful considering that the models are being utilized
    for real-time production usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approach we are taking is one that selects a model before selecting any
    features. It is not required to work in this fashion, but we find that it generally
    saves the most time when working under pressure of time. For your purposes, we
    recommend that you experiment with many models concurrently and don't limit yourself
    to a single model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing that we will be using the decision tree for the remainder of this chapter,
    we know two more things:'
  prefs: []
  type: TYPE_NORMAL
- en: The new baseline accuracy to beat is .8203, the accuracy the tree obtained when
    fitting to the entire dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We no longer have to use our `StandardScaler`, as decision trees are unaffected
    by it when it comes to model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The types of feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that our goal with feature selection is to improve our machine learning
    capabilities by increasing predictive power and reducing the time cost. To do
    this, we introduce two broad categories of feature selection: statistical-based
    and model-based. Statistical-based feature selection will rely heavily on statistical
    tests that are separate from our machine learning models in order to select features
    during the training phase of our pipeline. Model-based selection relies on a preprocessing
    step that involves training a secondary machine learning model and using that
    model''s predictive power to select features.'
  prefs: []
  type: TYPE_NORMAL
- en: Both of these types of feature selection attempt to reduce the size of our data
    by subsetting from our original features only the best ones with the highest predictive
    power. We may intelligently choose which feature selection method might work best
    for us, but in reality, a very valid way of working in this domain is to work
    through examples of each method and measure the performance of the resulting pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, let's take a look at the subclass of feature selection modules that
    are reliant on statistical tests to select viable features from a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical-based feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Statistics provides us with relatively quick and easy methods of interpreting
    both quantitative and qualitative data. We have used some statistical measures
    in previous chapters to obtain new knowledge and perspective around our data,
    specifically in that we recognized mean and standard deviation as metrics that
    enabled us to calculate z-scores and scale our data. In this chapter, we will
    rely on two new concepts to help us with our feature selection:'
  prefs: []
  type: TYPE_NORMAL
- en: Pearson correlations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hypothesis testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these methods are known as **univariate** methods of feature selection,
    meaning that they are quick and handy when the problem is to select out *single *features
    at a time in order to create a better dataset for our machine learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Using Pearson correlation to select features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have actually looked at correlations in this book already, but not in the
    context of feature selection. We already know that we can invoke a correlation
    calculation in pandas by calling the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code produces is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f4e5d5e-793b-4831-b227-1119091011c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As a continuation of the preceding table we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d3f21ce-2e87-4bb3-8121-2689f6fa4807.png)'
  prefs: []
  type: TYPE_IMG
- en: The Pearson correlation coefficient (which is the default for pandas) measures
    the *linear* relationship between columns. The value of the coefficient varies
    between -1 and +1, where 0 implies no correlation between them. Correlations closer
    to -1 or +1 imply an extremely strong linear relationship.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that Pearson’s correlation generally requires that each column
    be normally distributed (which we are not assuming). We can also largely ignore
    this requirement because our dataset is large (over 500 is the threshold).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pandas .corr()` method calculates a Pearson correlation coefficient for
    every column versus every other column. This 24 column by 24 row matrix is very
    unruly, and in the past, we used `heatmaps` to try and make the information more
    digestible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `heatmap` generated will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/895d47ee-91af-4695-8494-a20af98bf76f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the `heatmap` function automatically chose the most correlated features
    to show us. That being said, we are, for the moment, concerned with the features
    correlations to the response variable. We will assume that the more correlated
    a feature is to the response, the more useful it will be. Any feature that is
    not as strongly correlated will not be as useful to us.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation coefficients are also used to determine feature interactions and
    redundancies. A key method of reducing overfitting in machine learning is spotting
    and removing these redundancies. We will be tackling this problem in our model-based
    selection methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s isolate the correlations between the features and the response variable,
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can ignore the final row, as is it is the response variable correlated perfectly
    to itself. We are looking for features that have correlation coefficient values
    close to -1 or +1\. These are the features that we might assume are going to be
    useful. Let's use pandas filtering to isolate features that have at least .2 correlation
    (positive or negative).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do this by first defining a pandas *mask*, which will act as our filter,
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Every `False` in the preceding pandas Series represents a feature that has
    a correlation value between -.2 and .2 inclusive, while `True` values correspond
    to features with preceding correlation values .2 or less than -0.2\. Let''s plug
    this mask into our pandas filtering, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The variable `highly_correlated_features` is supposed to hold the features
    of the dataframe that are highly correlated to the response; however, we do have
    to get rid of the name of the response column, as including that in our machine
    learning pipeline would be cheating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now we have five features from our original dataset that are meant to be
    predictive of the response variable, so let''s try it out with the help of the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Our accuracy is definitely worse than the accuracy to beat, .8203, but also
    note that the fitting time saw about a 20-fold increase. Our model is able to
    learn almost as well as with the entire dataset with only five features. Moreover,
    it is able to learn as much in a much shorter timeframe.
  prefs: []
  type: TYPE_NORMAL
- en: Let's bring back our scikit-learn pipelines and include our correlation choosing
    methodology as a part of our preprocessing phase. To do this, we will have to
    create a custom transformer that invokes the logic we just went through, as a
    pipeline-ready class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will call our class the `CustomCorrelationChooser` and it will have to implement
    both a fit and a transform logic, which are:'
  prefs: []
  type: TYPE_NORMAL
- en: The fit logic will select columns from the features matrix that are higher than
    a specified threshold
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transform logic will subset any future datasets to only include those columns
    that were deemed important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take our new correlation feature selector for a spin, with the help
    of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Our class has selected the same five columns as we found earlier. Let''s test
    out the transform functionality by calling it on our `X` matrix, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following table as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **PAY_0** | **PAY_2** | **PAY_3** | **PAY_4** | **PAY_5** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 2 | 2 | -1 | -1 | -2 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | -1 | 2 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | -1 | 0 | -1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: 'We see that the `transform` method has eliminated the other columns and kept
    only the features that met our `.2` correlation threshold. Now, let''s put it
    all together in our pipeline, with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Wow! Our first attempt at feature selection and we have already beaten our
    goal (albeit by a little bit). Our pipeline is showing us that if we threshold
    at `0.1`, we have eliminated noise enough to improve accuracy and also cut down
    on the fitting time (from .158 seconds without the selector). Let''s take a look
    at which columns our selector decided to keep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: It appears that our selector has decided to keep the five columns that we found,
    as well as two more, the `LIMIT_BAL` and the `PAY_6` columns. Great! This is the
    beauty of automated pipeline gridsearching in scikit-learn. It allows our models
    to do what they do best and intuit things that we could not have on our own.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection using hypothesis testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hypothesis testing is a methodology in statistics that allows for a bit more
    complex statistical testing for individual features. Feature selection via hypothesis
    testing will attempt to select only the best features from a dataset, just as
    we were doing with our custom correlation chooser, but these tests rely more on
    formalized statistical methods and are interpreted through what are known as **p-values**.
  prefs: []
  type: TYPE_NORMAL
- en: A hypothesis testis a statistical test that is used to figure out whether we
    can apply a certain condition for an entire population, given a data sample. The
    result of a hypothesis test tells us whether we should believe the hypothesis
    or reject it for an alternative one. Based on sample data from a population, a
    hypothesis test determines whether or not to reject the null hypothesis. We usually
    use a **p-value**(a non-negative decimal with an upper bound of 1, which is based
    on our significance level) to make this conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of feature selection, the hypothesis we wish to test is along the
    lines of: *True or False: This feature has no relevance to the response variable. *We
    want to test this hypothesis for every feature and decide whether the features
    hold some significance in the prediction of the response. In a way, this is how
    we dealt with the correlation logic. We basically said that, if a column''s correlation
    with the response is too weak, then we say that the hypothesis that the feature
    has no relevance is true. If the correlation coefficient was strong enough, then
    we can reject the hypothesis that the feature has no relevance in favor of an
    alternative hypothesis, that the feature does have some relevance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin to use this for our data, we will have to bring in two new modules:
    `SelectKBest` and `f_classif`, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`SelectKBest` is basically just a wrapper that keeps a set amount of features
    that are the highest ranked according to some criterion. In this case, we will
    use the p-values of completed hypothesis testings as a ranking.'
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the p-value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The p-values are a decimals between 0 and 1 that represent the probability that
    the data given to us occurred by chance under the hypothesis test. Simply put,
    the lower the p-value, the better the chance that we can reject the null hypothesis.
    For our purposes, the smaller the p-value, the better the chances that the feature
    has some relevance to our response variable and we should keep it.
  prefs: []
  type: TYPE_NORMAL
- en: For a more in-depth handling of statistical testing, check out* Principles of
    Data Science*, [https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science),
    by Packt Publishing.
  prefs: []
  type: TYPE_NORMAL
- en: The big take away from this is that the `f_classif` function will perform an
    ANOVA test (a type of hypothesis test) on each feature on its own (hence the name
    univariate testing) and assign that feature a p-value. The `SelectKBest` will
    rank the features by that p-value (the lower the better) and keep only the best
    k (a human input) features. Let's try this out in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking the p-value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin by instantiating a `SelectKBest` module. We will manually enter
    a `k` value, `5`, meaning we wish to keep only the five best features according
    to the resulting p-values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then fit and transform our `X` matrix to select the features we want,
    as we did before with our custom selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to inspect the `p-values` directly and see which columns were chosen,
    we can dive deeper into the select `k_best` variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following table as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **column** | **p_value** |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | PAY_0 | 0.000000e+00 |'
  prefs: []
  type: TYPE_TB
- en: '| **6** | PAY_2 | 0.000000e+00 |'
  prefs: []
  type: TYPE_TB
- en: '| **7** | PAY_3 | 0.000000e+00 |'
  prefs: []
  type: TYPE_TB
- en: '| **8** | PAY_4 | 1.899297e-315 |'
  prefs: []
  type: TYPE_TB
- en: '| **9** | PAY_5 | 1.126608e-279 |'
  prefs: []
  type: TYPE_TB
- en: 'We can see that, once again, our selector is choosing the `PAY_X` columns as
    the most important. If we take a look at our `p-value` column, we will notice
    that our values are extremely small and close to zero. A common threshold for
    p-values is `0.05`, meaning that anything less than 0.05 may be considered significant,
    and these columns are extremely significant according to our tests. We can also
    directly see which columns meet a threshold of 0.05 using the pandas filtering
    methodology:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following table as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **column** | **p_value** |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | PAY_0 | 0.000000e+00 |'
  prefs: []
  type: TYPE_TB
- en: '| **6** | PAY_2 | 0.000000e+00 |'
  prefs: []
  type: TYPE_TB
- en: '| **7** | PAY_3 | 0.000000e+00 |'
  prefs: []
  type: TYPE_TB
- en: '| **8** | PAY_4 | 1.899297e-315 |'
  prefs: []
  type: TYPE_TB
- en: '| **9** | PAY_5 | 1.126608e-279 |'
  prefs: []
  type: TYPE_TB
- en: '| **10** | PAY_6 | 7.296740e-234 |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | LIMIT_BAL | 1.302244e-157 |'
  prefs: []
  type: TYPE_TB
- en: '| **17** | PAY_AMT1 | 1.146488e-36 |'
  prefs: []
  type: TYPE_TB
- en: '| **18** | PAY_AMT2 | 3.166657e-24 |'
  prefs: []
  type: TYPE_TB
- en: '| **20** | PAY_AMT4 | 6.830942e-23 |'
  prefs: []
  type: TYPE_TB
- en: '| **19** | PAY_AMT3 | 1.841770e-22 |'
  prefs: []
  type: TYPE_TB
- en: '| **21** | PAY_AMT5 | 1.241345e-21 |'
  prefs: []
  type: TYPE_TB
- en: '| **22** | PAY_AMT6 | 3.033589e-20 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | SEX | 4.395249e-12 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | EDUCATION | 1.225038e-06 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | MARRIAGE | 2.485364e-05 |'
  prefs: []
  type: TYPE_TB
- en: '| **11** | BILL_AMT1 | 6.673295e-04 |'
  prefs: []
  type: TYPE_TB
- en: '| **12** | BILL_AMT2 | 1.395736e-02 |'
  prefs: []
  type: TYPE_TB
- en: '| **13** | BILL_AMT3 | 1.476998e-02 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | AGE | 1.613685e-02 |'
  prefs: []
  type: TYPE_TB
- en: 'The majority of the columns have a low `p-value`, but not all. Let''s see the
    columns with a higher `p_value`, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following table as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **column** | **p_value** |'
  prefs: []
  type: TYPE_TB
- en: '| **14** | BILL_AMT4 | 0.078556 |'
  prefs: []
  type: TYPE_TB
- en: '| **15** | BILL_AMT5 | 0.241634 |'
  prefs: []
  type: TYPE_TB
- en: '| **16** | BILL_AMT6 | 0.352123 |'
  prefs: []
  type: TYPE_TB
- en: 'These three columns have quite a high `p-value`. Let''s use our `SelectKBest`
    in a pipeline to see if we can grid search our way into a better machine learning
    pipeline, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that our `SelectKBest` module is getting about the same accuracy as
    our custom transformer, but it''s getting there a bit quicker! Let''s see which
    columns our tests are selecting for us, with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following table as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **column** | **p_value** |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | PAY_0 | 0.000000e+00 |'
  prefs: []
  type: TYPE_TB
- en: '| **6** | PAY_0 | 0.000000e+00 |'
  prefs: []
  type: TYPE_TB
- en: '| **7** | PAY_0 | 0.000000e+00 |'
  prefs: []
  type: TYPE_TB
- en: '| **8** | PAY_0 | 1.899297e-315 |'
  prefs: []
  type: TYPE_TB
- en: '| **9** | PAY_0 | 1.126608e-279 |'
  prefs: []
  type: TYPE_TB
- en: '| **10** | PAY_0 | 7.296740e-234 |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | LIMIT_BAL | 1.302244e-157 |'
  prefs: []
  type: TYPE_TB
- en: They appear to be the same columns that were chosen by our other statistical
    method. It's possible that our statistical method is limited to continually picking
    these seven columns for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other tests available besides ANOVA, such as Chi² and others, for
    regression tasks. They are all included in scikit-learn''s documentation. For
    more info on feature selection through univariate testing, check out the scikit-learn
    documentation here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to model-based feature selection, it''s helpful to do a quick
    sanity check to ensure that we are on the right track. So far, we have seen two
    statistical methods for feature selection that gave us the same seven columns
    for optimal accuracy. But what if we were to take every column **except** those
    seven? We should expect a much lower accuracy and worse pipeline overall, right?
    Let''s make sure. The following code helps us to implement sanity checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: OK, so by selecting the columns except those seven, we see not only worse accuracy
    (almost as bad as the null accuracy), but also slower fitting times on average.
    With this, I believe we may move on to our next subset of feature selection techniques,
    the model-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our last section dealt with using statistical methods and testing in order to
    select features from the original dataset to improve our machine learning pipeline,
    both in predictive performance, as well as in time-complexity. In doing so, we
    were able to see first-hand the effects of using feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: A brief refresher on natural language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If talking about feature selection has sounded familiar from the very beginning
    of this chapter, almost as if we were doing it even before we began with correlation
    coefficients and statistical testing, well, you aren't wrong. In [Chapter 4](430d621e-7ce6-48c0-9990-869e82a0d0c6.xhtml), *Feature Construction* when
    dealing with feature construction, we introduced the concept of the `CountVectorizer`,
    a module in scikit-learn designed to construct features from text columns and
    use them in machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `CountVectorizer` had many parameters that we could alter in search of
    the best pipeline. Specifically, there were a few built-in feature selection parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features`: This integer set a hard limit of the maximum number of features
    that the featurizer could remember. The features that were remembered were decided
    based on a ranking system where the rank of a token was the count of the token
    in the corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_df`: This float limited the number of features by imposing a rule stating
    that a token may only appear in the dataset if it appeared in the corpus as a
    rate strictly greater than the value for `min_df`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_df`: Similar to `min_df`, this float limits the number of features by
    only allowing tokens that appear in the corpus at a rate strictly lower than the
    value set for `max_df`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop_words`: Limits the type of tokens allowed by matching them against a
    static list of tokens. If a token is found that exists in the `stop_words` set,
    that word, no matter if it occurs at the right amount to be allowed by `min_df`
    and `max_df`, is ignored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the previous chapter, we briefly introduced a dataset aimed at predicting
    the sentiment of a tweet based purely on the words in that tweet. Let''s take
    some time to refresh our memories on how to use these parameters. Let''s begin
    by bringing in our `tweet` dataset, with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To refresh our memory, let''s look at the first five `tweets`, using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following table as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ItemID** | **Sentiment** | **SentimentText** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 | 0 | is so sad for my APL frie... |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | 0 | I missed the New Moon trail... |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3 | 1 | omg its already 7:30 :O |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 4 | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 5 | 0 | i think mi bf is cheating on me!!! ... |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s create a feature and a response variable. Recall that, because we are
    working with text, our feature variable will simply be the text column and not
    a two-dimensional matrix like it usually is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up a pipeline and evaluate it using the same function that we''ve
    been using in this chapter, with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: A decent score (recalling that the null accuracy was .564), but we were able
    to beat this in the last chapter by using a `FeatureUnion` module to combine features
    from `TfidfVectorizer` and `CountVectorizer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To try out the techniques we''ve seen in this chapter, let''s go ahead and
    apply a `SelectKBest` in a pipeline with a `CountVectorizer`. Let''s see if we
    can rely not on the built-in `CountVectorizer` feature selection parameters, but
    instead, on statistical testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: It seems that `SelectKBest` didn't do as well for text tokens, and without `FeatureUnion`,
    we were unable to compete with the previous chapter's accuracy scores. Either
    way, for both pipelines, it is worth noting that the time it takes to both fit
    and predict are extremely poor. This is because statistical univariate methods
    are not optimal for a very large number of features, such as the features obtained
    from text vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: Using machine learning to select features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using `CountVectorizer` built-in feature selection tools is great when you are
    dealing with text; however, we are usually dealing with data already built into
    a row/column structure. We've seen the power of using purely statistical methodology
    for feature selection, and now let's see how we can invoke the awesome power of
    machine learning to, hopefully, do even more. The two main machine learning models
    that we will use in this section for the purposes of feature selection are tree-based
    models and linear models. They both have a notion of feature ranking that are
    useful when subsetting feature sets.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go further, we believe it is worth mentioning again that these methods,
    while different in their methodology of selection, are attempting to find the
    optimal subset of features to improve our machine learning pipelines. The first
    method we will dive into will involve the internal importance metrics that algorithms
    such as decision trees and random forest models generate whilst fitting to training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based model feature selection metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When fitting decision trees, the tree starts at the root node and greedily chooses
    the optimal split at every junction that optimizes a certain metric of **node
    purity**. By default, scikit-learn optimizes for the **gini** metric at every
    step. While each split is created, the model keeps track of how much each split
    helps the overall optimization goal. In doing so, tree-based models that choose
    splits based on such metrics have a notion of** feature importance**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this further, let''s go ahead and fit a decision tree to our
    data and output the feature importance'' with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Once our tree has fit to the data, we can call on the `feature_importances_
    attribute` to capture the importance of the feature, relative to the fitting of
    the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following table as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **feature** | **importance** |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | PAY_0 | 0.161829 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | AGE | 0.074121 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | BILL_AMT1 | 0.064363 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | LIMIT_BAL | 0.058788 |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | PAY_AMT3 | 0.054911 |'
  prefs: []
  type: TYPE_TB
- en: What this table is telling us is that the most important feature while fitting
    was the column `PAY_0`, which matches up to what our statistical models were telling
    us earlier in this chapter. What is more notable are the second, third, and fifth
    most important features, as they didn't really show up before using our statistical
    tests. This is a good indicator that this method of feature selection might yield
    some new results for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that, earlier, we relied on a built in scikit-learn wrapper called SelectKBest
    to capture the top *k* features based on a ranking function like ANOVA p-values.
    We will introduce another similar style of wrapper called `SelectFromModel` which,
    like `SelectKBest`, will capture the top k most importance features. However,
    it will do so by listening to a machine learning model''s internal metric for
    feature importance rather than the p-values of a statistical test. We will use
    the following code to define the `SelectFromModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The biggest difference in usage between `SelectFromModel` and `SelectKBest`
    is that `SelectFromModel` doesn''t take in an integer k, which represents the
    number of features to keep, but rather `SelectFromModel` uses a threshold for
    selection which acts as a hard minimum of importance to be selected. In this way,
    the model-based selectors of this chapter are able to move away from a human-inputted
    number of features to keep and instead rely on relative importance to include
    only as many features as the pipeline needs. Let''s instantiate our class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s fit this `SelectFromModel` class to our data and invoke the transform
    method to watch our data get subsetted, with the help of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we know the basic mechanics of the module, let''s use it to select
    features for us in a pipeline. Recall that the accuracy to beat is .8206, which
    we got both from our correlation chooser and our ANOVA test (because they both
    returned the same features):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Note first that, as part of the threshold parameter, we are able to include
    some reserved words rather than a float that represents the minimum importance
    to use. For example, the threshold of `mean` only selects features with an importance
    that is higher than average. Similarly, a value of median as a threshold only
    selects features that are more important than the median value. We may also include
    multiples to these reserved words so that `2.*mean` will only include features
    that are more important than twice the mean importance value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a peak as to which features our decision tree-based selector is
    choosing for us. We can do this by invoking a method within `SelectFromModel`
    called `get_support()`. It will return an array of Booleans, one for each original
    feature column, and tell us which of the features it decided to keep, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Wow! So the tree decided to keep all but two features, and still only did just
    as good as the tree did without selecting anything:'
  prefs: []
  type: TYPE_NORMAL
- en: For more information on decision trees and how they are fit using gini or entropy,
    look into the scikit-learn documentation or other texts that handle this topic
    in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: We could continue onward by trying several other tree-based models, such as
    RandomForest, ExtraTreesClassifier, and others, but perhaps we may be able to
    do better by utilizing a model other than a tree-based model.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models and regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `SelectFromModel` selector is able to handle any machine learning model
    that exposes a `feature_importances_` or**` coef_ attribute`** post-fitting. Tree-based
    models expose the former, while linear models expose the latter. After fitting,
    linear models such as Linear Regression, Logistic Regression, Support Vector Machines,
    and others all place coefficients in front of features that represent the slope
    of that feature/how much it affects the response when that feature is changed.
    `SelectFromModel` can equate this to a feature importance and choose features
    based on the coefficients given to features while fitting.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can use these models, however, we must introduce a concept called
    **regularization**, which will help us select truly only the most important features.
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In linear models, **regularization** is a method for imposing additional constraints
    to a learning model, where the goal is to prevent overfitting and improve the
    generalization of the data. This is done by adding extra terms to the *loss function*
    being optimized, meaning that, while fitting, regularized linear models may severely
    diminish, or even destroy features along the way. There are two widely used regularization
    methods, called L1 and L2 regularization. Both regularization techniques rely
    on the L-p Norm, which is defined for a vector as being:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68897f7a-1aea-478f-a42f-0f6a740ffa75.png)'
  prefs: []
  type: TYPE_IMG
- en: '**L1 ****regularization**, also known as **lasso** regularization, uses the
    L1 Norm, which, using the above formula, reduces to the sum of the absolute values
    of the entries of a vector to limit the coefficients in such a way that they may
    disappear entirely and become 0\. If the coefficient of a feature drops to 0,
    then that feature will not have any say in the prediction of new data observations
    and definitely will not be chosen by a `SelectFromModel` selector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 regularization**, also known as **ridge** regularization, imposes the
    L2 norm as a penalty (sum of the square of vector entries) so that coefficients
    cannot drop to 0, but they can become very, very tiny.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization also helps with multicollinearity, the problem of having multiple
    features in a dataset that are linearly related to one another. A Lasso Penalty
    (L1) will force coefficients of dependent features to 0, ensuring that they aren't
    chosen by the selector module, helping combat overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Linear model coefficients as another feature importance metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use L1 and L2 regularization to find optimal coefficients for our feature
    selection, just as we did with our tree-based models. Let''s use a logistic regression
    model as our model-based selector and gridsearch across both the L1 and L2 norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally! We got an accuracy better than our statistical testing selector. Let''s
    see which features our model-based selector decided to keep by invoking the `get_support()`
    method of `SelectFromModel` again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Fascinating! Our logistic regression based selector kept most of the `PAY_X`
    columns but was also able to figure out that the sex, education, and marriage
    status of the person was going to play a hand in prediction. Let's continue our
    adventure by using one more model with our `SelectFromModel` selector module,
    a support vector machine classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are unfamiliar with support vector machines, they are classification
    models that attempt to draw linear boundaries in space to separate binary labels.
    These linear boundaries are known as support vectors. For now, the most important
    difference between logistic regression and support vector classifiers are that
    SVCs are usually better equipped to optimize coefficients for maximizing accuracy
    for binary classification tasks, while logistic regression is better at modeling
    the probabilistic attributes of binary classification tasks. Let''s implement
    a Linear SVC model from scikit-learn as we did for decision trees and logistic
    regression and see how it fares, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! The best accuracy we''ve gotten so far. We can see that the fitting
    time took a hit but if we are OK with that, couple the best accuracy so far with
    an outstandingly quick predicting time and we''ve got a great machine learning
    pipeline on our hands; one that leverages the power of regularization in the context
    of support vector classification to feed significant features into a decision
    tree classifier. Let''s see which features our selector chose to give us our best
    accuracy to date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The only difference between these features and what our logistic regression
    got was the `PAY_4` column. But we can see that even removing a single column
    can affect our entire pipeline's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right feature selection method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, you may be feeling a bit overwhelmed with the information in
    this chapter. We have presented several ways of performing feature selection,
    some based on pure statistics and others based on the output of secondary machine
    learning models. It is natural to wonder how to decide which feature selection
    method is right for your data. In theory, if you are able to try multiple options,
    as we did in this chapter, that would be ideal, but we understand that it might
    not be feasible to do so. The following are some rules of thumbs that you can
    follow when you are trying to prioritize which feature selection module is more
    likely to offer greater results:'
  prefs: []
  type: TYPE_NORMAL
- en: If your features are mostly categorical, you should start by trying to implement
    a `SelectKBest` with a Chi² ranker or a tree-based model selector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your features are largely quantitative (like ours were), using linear models
    as model-based selectors and relying on correlations tends to yield greater results,
    as was shown in this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are solving a binary classification problem, using a Support Vector Classification
    model along with a `SelectFromModel` selector will probably fit nicely, as the
    SVC tries to find coefficients to optimize for binary classification tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A little bit of EDA can go a long way in manual feature selection. The importance
    of having domain knowledge in the domain from which the data originated cannot
    be understated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That being said, these are meant only to be used as guidelines. As a data scientist, ultimately
    you decide which features you wish to keep to optimize the metric of your choosing.
    The methods that we provide in this text are here to help you in your discovery
    of the latent power of features hidden by noise and multicollinearity.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned a great deal about methodologies for selecting subsets
    of features in order to increase the performance of our machine learning pipelines
    in both a predictive capacity as well in-time-complexity.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that we chose had a relatively low number of features. If selecting,
    however, from a very large set of features (over a hundred), then the methods
    in this chapter will likely start to become entirely too cumbersome. We saw that
    in this chapter, when attempting to optimize a `CountVectorizer` pipeline, the
    time it would take to run a univariate test on every feature is not only astronomical;
    we would run a greater risk of experiencing multicollinearity in our features
    by sheer coincidence.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce purely mathematical transformations that
    we may apply to our data matrices in order to alleviate the trouble of working
    with vast quantities of features, or even a few highly uninterpretable features.
    We will begin to work with datasets that stray away from what we have seen before,
    such as image data, topic modeling data, and more.
  prefs: []
  type: TYPE_NORMAL
