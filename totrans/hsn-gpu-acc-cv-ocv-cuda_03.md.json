["```py\n#include \"stdio.h\"\n#include<iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n//Defining number of elements in array\n#define N 50000\n__global__ void gpuAdd(int *d_a, int *d_b, int *d_c)\n{\n    //Getting index of current kernel\n  int tid = threadIdx.x + blockIdx.x * blockDim.x; \n\n  while (tid < N)\n    {\n       d_c[tid] = d_a[tid] + d_b[tid];\n       tid += blockDim.x * gridDim.x;\n    }\n}\n```", "```py\ntid = threadIdx.x + blockIdx.x * blockDim.x; \n```", "```py\nint main(void) \n{\n    //Declare host and device arrays\n  int h_a[N], h_b[N], h_c[N];\n  int *d_a, *d_b, *d_c;\n\n    //Allocate Memory on Device\n  cudaMalloc((void**)&d_a, N * sizeof(int));\n  cudaMalloc((void**)&d_b, N * sizeof(int));\n  cudaMalloc((void**)&d_c, N * sizeof(int));\n    //Initialize host array\n  for (int i = 0; i < N; i++) \n  {\n    h_a[i] = 2 * i*i;\n    h_b[i] = i;\n  }\n\n  cudaMemcpy(d_a, h_a, N * sizeof(int), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, h_b, N * sizeof(int), cudaMemcpyHostToDevice);\n    //Kernel Call\n  gpuAdd << <512, 512 >> >(d_a, d_b, d_c);\n\n  cudaMemcpy(h_c, d_c, N * sizeof(int), cudaMemcpyDeviceToHost);\n    //This ensures that kernel execution is finishes before going forward\n  cudaDeviceSynchronize();\n  int Correct = 1;\n  printf(\"Vector addition on GPU \\n\");\n  for (int i = 0; i < N; i++) \n  {\n    if ((h_a[i] + h_b[i] != h_c[i]))\n      { Correct = 0; }\n  }\n  if (Correct == 1)\n  { \n    printf(\"GPU has computed Sum Correctly\\n\"); \n  }\n  else\n  { \n    printf(\"There is an Error in GPU Computation\\n\");\n  }\n    //Free up memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n   cudaFree(d_c);\n  return 0;\n}\n\n```", "```py\n#include <stdio.h>\n#define N 5\n\n__global__ void gpu_global_memory(int *d_a)\n{\n  d_a[threadIdx.x] = threadIdx.x;\n}\n\nint main(int argc, char **argv)\n{\n  int h_a[N]; \n  int *d_a; \n\n  cudaMalloc((void **)&d_a, sizeof(int) *N);\n  cudaMemcpy((void *)d_a, (void *)h_a, sizeof(int) *N, cudaMemcpyHostToDevice);\n\n  gpu_global_memory << <1, N >> >(d_a); \n  cudaMemcpy((void *)h_a, (void *)d_a, sizeof(int) *N, cudaMemcpyDeviceToHost);\n\n  printf(\"Array in Global Memory is: \\n\");\n  for (int i = 0; i < N; i++) \n  {\n    printf(\"At Index: %d --> %d \\n\", i, h_a[i]);\n  }\n  return 0;\n}\n```", "```py\n#include <stdio.h>\n#define N 5\n\n__global__ void gpu_local_memory(int d_in)\n{\n  int t_local; \n  t_local = d_in * threadIdx.x; \n  printf(\"Value of Local variable in current thread is: %d \\n\", t_local);\n}\nint main(int argc, char **argv)\n{\n  printf(\"Use of Local Memory on GPU:\\n\");\n  gpu_local_memory << <1, N >> >(5); \n  cudaDeviceSynchronize();\n  return 0;\n}\n```", "```py\n#include <stdio.h>\n__global__ void gpu_shared_memory(float *d_a)\n{\n  int i, index = threadIdx.x;\n  float average, sum = 0.0f;\n  //Defining shared memory\n  __shared__ float sh_arr[10];\n\n  sh_arr[index] = d_a[index];\n // This directive ensure all the writes to shared memory have completed\n\n  __syncthreads();  \n  for (i = 0; i<= index; i++) \n  { \n    sum += sh_arr[i]; \n  }\n  average = sum / (index + 1.0f);\n  d_a[index] = average;\n\n    //This statement is redundant and will have no effect on overall code execution  \n  sh_arr[index] = average;\n}\n```", "```py\nint main(int argc, char **argv)\n{\n   float h_a[10]; \n   float *d_a; \n\n      //Initialize host Array\n   for (int i = 0; i < 10; i++) \n   {\n     h_a[i] = i;\n   }\n\n    // allocate global memory on the device\n    cudaMalloc((void **)&d_a, sizeof(float) * 10);\n\n    // copy data from host memory  to device memory \n    cudaMemcpy((void *)d_a, (void *)h_a, sizeof(float) * 10,         cudaMemcpyHostToDevice);\n    gpu_shared_memory << <1, 10 >> >(d_a);\n\n    // copy the modified array back to the host\n    cudaMemcpy((void *)h_a, (void *)d_a, sizeof(float) * 10, cudaMemcpyDeviceToHost);\n    printf(\"Use of Shared Memory on GPU: \\n\");\n\n    for (int i = 0; i < 10; i++) \n    {\n      printf(\"The running average after %d element is %f \\n\", i, h_a[i]);\n    }\n    return 0;\n}\n```", "```py\ninclude <stdio.h>\n\n#define NUM_THREADS 10000\n#define SIZE 10\n\n#define BLOCK_WIDTH 100\n\n__global__ void gpu_increment_without_atomic(int *d_a)\n{\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Each thread increment elements which wraps at SIZE\n  tid = tid % SIZE;\n  d_a[tid] += 1;\n}\n\n```", "```py\nint main(int argc, char **argv)\n{\n  printf(\"%d total threads in %d blocks writing into %d array elements\\n\",\n  NUM_THREADS, NUM_THREADS / BLOCK_WIDTH, SIZE);\n\n  // declare and allocate host memory\n  int h_a[SIZE];\n  const int ARRAY_BYTES = SIZE * sizeof(int);\n  // declare and allocate GPU memory\n  int * d_a;\n  cudaMalloc((void **)&d_a, ARRAY_BYTES);\n\n  // Initialize GPU memory with zero value.\n  cudaMemset((void *)d_a, 0, ARRAY_BYTES);\n  gpu_increment_without_atomic << <NUM_THREADS / BLOCK_WIDTH, BLOCK_WIDTH >> >(d_a);\n\n  // copy back the array of sums from GPU and print\n  cudaMemcpy(h_a, d_a, ARRAY_BYTES, cudaMemcpyDeviceToHost);\n\n  printf(\"Number of times a particular Array index has been incremented without atomic add is: \\n\");\n  for (int i = 0; i < SIZE; i++)\n  {\n    printf(\"index: %d --> %d times\\n \", i, h_a[i]);\n  }\n  cudaFree(d_a);\n  return 0;\n}\n```", "```py\n#include <stdio.h>\n#define NUM_THREADS 10000\n#define SIZE 10\n#define BLOCK_WIDTH 100\n\n__global__ void gpu_increment_atomic(int *d_a)\n{\n  // Calculate thread index \n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Each thread increments elements which wraps at SIZE\n  tid = tid % SIZE;\n  atomicAdd(&d_a[tid], 1);\n}\n\n```", "```py\nint main(int argc, char **argv)\n{\n  printf(\"%d total threads in %d blocks writing into %d array elements\\n\",NUM_THREADS, NUM_THREADS / BLOCK_WIDTH, SIZE);\n\n  // declare and allocate host memory\n  int h_a[SIZE];\n  const int ARRAY_BYTES = SIZE * sizeof(int);\n\n  // declare and allocate GPU memory\n  int * d_a;\n  cudaMalloc((void **)&d_a, ARRAY_BYTES);\n\n   // Initialize GPU memory withzero value\n  cudaMemset((void *)d_a, 0, ARRAY_BYTES);\n\n  gpu_increment_atomic << <NUM_THREADS / BLOCK_WIDTH, BLOCK_WIDTH >> >(d_a);\n    // copy back the array from GPU and print\n  cudaMemcpy(h_a, d_a, ARRAY_BYTES, cudaMemcpyDeviceToHost);\n\n  printf(\"Number of times a particular Array index has been incremented is: \\n\");\n  for (int i = 0; i < SIZE; i++) \n  { \n     printf(\"index: %d --> %d times\\n \", i, h_a[i]); \n  }\n\n  cudaFree(d_a);\n  return 0;\n}\n```", "```py\n#include \"stdio.h\"\n#include<iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n//Defining two constants\n__constant__ int constant_f;\n__constant__ int constant_g;\n#define N 5\n\n//Kernel function for using constant memory \n__global__ void gpu_constant_memory(float *d_in, float *d_out) \n{\n  //Getting thread index for current kernel\n  int tid = threadIdx.x; \n  d_out[tid] = constant_f*d_in[tid] + constant_g;\n}\n\n```", "```py\nint main(void) \n{\n  //Defining Arrays for host\n  float h_in[N], h_out[N];\n  //Defining Pointers for device\n  float *d_in, *d_out;\n  int h_f = 2;\n  int h_g = 20;\n\n  // allocate the memory on the cpu\n  cudaMalloc((void**)&d_in, N * sizeof(float));\n  cudaMalloc((void**)&d_out, N * sizeof(float));\n\n  //Initializing Array\n  for (int i = 0; i < N; i++) \n  {\n    h_in[i] = i;\n  }\n\n  //Copy Array from host to device\n  cudaMemcpy(d_in, h_in, N * sizeof(float), cudaMemcpyHostToDevice);\n  //Copy constants to constant memory\n  cudaMemcpyToSymbol(constant_f, &h_f, sizeof(int),0,cudaMemcpyHostToDevice);\n  cudaMemcpyToSymbol(constant_g, &h_g, sizeof(int));\n\n  //Calling kernel with one block and N threads per block\n  gpu_constant_memory << <1, N >> >(d_in, d_out);\n\n  //Coping result back to host from device memory\n  cudaMemcpy(h_out, d_out, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n  //Printing result on console\n  printf(\"Use of Constant memory on GPU \\n\");\n  for (int i = 0; i < N; i++) \n  {\n    printf(\"The expression for index %f is %f\\n\", h_in[i], h_out[i]);\n  }\n\n  cudaFree(d_in);\n  cudaFree(d_out);\n  return 0;\n}\n```", "```py\n#include \"stdio.h\"\n#include<iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define NUM_THREADS 10\n#define N 10\n\n//Define texture reference for 1-d access\ntexture <float, 1, cudaReadModeElementType> textureRef;\n\n__global__ void gpu_texture_memory(int n, float *d_out)\n{\n    int idx = blockIdx.x*blockDim.x + threadIdx.x;\n    if (idx < n) {\n      float temp = tex1D(textureRef, float(idx));\n      d_out[idx] = temp;\n    }\n}\n```", "```py\nint main()\n{\n  //Calculate number of blocks to launch\n  int num_blocks = N / NUM_THREADS + ((N % NUM_THREADS) ? 1 : 0);\n  float *d_out;\n  // allocate space on the device for the results\n  cudaMalloc((void**)&d_out, sizeof(float) * N);\n  // allocate space on the host for the results\n  float *h_out = (float*)malloc(sizeof(float)*N);\n  float h_in[N];\n  for (int i = 0; i < N; i++) \n  {\n    h_in[i] = float(i);\n  }\n  //Define CUDA Array\n  cudaArray *cu_Array;\n  cudaMallocArray(&cu_Array, &textureRef.channelDesc, N, 1);\n\n  cudaMemcpyToArray(cu_Array, 0, 0, h_in, sizeof(float)*N, cudaMemcpyHostToDevice);\n\n  // bind a texture to the CUDA array\n  cudaBindTextureToArray(textureRef, cu_Array);\n\n  gpu_texture_memory << <num_blocks, NUM_THREADS >> >(N, d_out);\n\n  // copy result to host\n  cudaMemcpy(h_out, d_out, sizeof(float)*N, cudaMemcpyDeviceToHost);\n  printf(\"Use of Texture memory on GPU: \\n\");\n  // Print the result\n  for (int i = 0; i < N; i++) \n  {\n    printf(\"Average between two nearest element is : %f\\n\", h_out[i]);\n  }\n  free(h_out);\n  cudaFree(d_out);\n  cudaFreeArray(cu_Array);\n  cudaUnbindTexture(textureRef);\n}\n```", "```py\n(x1,x1,x3) . (y1,y2,y3) = x1y1 + x2y2 +x3y3\n```", "```py\n#include <stdio.h>\n#include<iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#define N 1024\n#define threadsPerBlock 512\n\n__global__ void gpu_dot(float *d_a, float *d_b, float *d_c) \n{\n  //Define Shared Memory\n  __shared__ float partial_sum[threadsPerBlock];\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int index = threadIdx.x;\n\n  float sum = 0;\n  while (tid < N) \n  {\n    sum += d_a[tid] * d_b[tid];\n    tid += blockDim.x * gridDim.x;\n  }\n\n  // set the partial sum in shared memory\n  partial_sum[index] = sum;\n\n  // synchronize threads in this block\n  __syncthreads();\n\n  //Calculate Patial sum for a current block using data in shared memory\n  int i = blockDim.x / 2;\n  while (i != 0) {\n    if (index < i)\n      {partial_sum[index] += partial_sum[index + i];}\n    __syncthreads();\n    i /= 2;\n  }\n  //Store result of partial sum for a block in global memory\n  if (index == 0)\n    d_c[blockIdx.x] = partial_sum[0];\n}\n\n```", "```py\nint main(void) \n{\n  float *h_a, *h_b, h_c, *partial_sum;\n  float *d_a, *d_b, *d_partial_sum;\n\n  //Calculate number of blocks and number of threads\n  int block_calc = (N + threadsPerBlock - 1) / threadsPerBlock;\n  int blocksPerGrid = (32 < block_calc ? 32 : block_calc);\n  // allocate memory on the cpu side\n  h_a = (float*)malloc(N * sizeof(float));\n  h_b = (float*)malloc(N * sizeof(float));\n  partial_sum = (float*)malloc(blocksPerGrid * sizeof(float));\n\n  // allocate the memory on the gpu\n  cudaMalloc((void**)&d_a, N * sizeof(float));\n  cudaMalloc((void**)&d_b, N * sizeof(float));\n  cudaMalloc((void**)&d_partial_sum, blocksPerGrid * sizeof(float));\n\n  // fill in the host mempory with data\n  for (int i = 0; i<N; i++) {\n    h_a[i] = i;\n    h_b[i] = 2;\n  }\n\n  // copy the arrays to the device\n  cudaMemcpy(d_a, h_a, N * sizeof(float), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, h_b, N * sizeof(float), cudaMemcpyHostToDevice);\n\n  gpu_dot << <blocksPerGrid, threadsPerBlock >> >(d_a, d_b, d_partial_sum);\n\n  // copy the array back to the host\n  cudaMemcpy(partial_sum, d_partial_sum, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost);\n\n  // Calculate final dot prodcut\n  h_c = 0;\n  for (int i = 0; i<blocksPerGrid; i++) \n {\n    h_c += partial_sum[i];\n  }\n\n}\n```", "```py\nprintf(\"The computed dot product is: %f\\n\", h_c);\n#define cpu_sum(x) (x*(x+1))\n  if (h_c == cpu_sum((float)(N - 1)))\n  {\n    printf(\"The dot product computed by GPU is correct\\n\");\n  }\n  else\n  {\n    printf(\"Error in dot product computation\");\n  }\n  // free memory on the gpu side\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_partial_sum);\n  // free memory on the cpu side\n  free(h_a);\n  free(h_b);\n  free(partial_sum);\n```", "```py\n\n#include <stdio.h>\n#include<iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <math.h>\n\n//This defines size of a small square box or thread dimensions in one block\n#define TILE_SIZE 2\n\n//Matrix multiplication using non shared kernel\n__global__ void gpu_Matrix_Mul_nonshared(float *d_a, float *d_b, float *d_c, const int size)\n{\n  int row, col;\n  col = TILE_SIZE * blockIdx.x + threadIdx.x;\n  row = TILE_SIZE * blockIdx.y + threadIdx.y;\n\n  for (int k = 0; k< size; k++)\n  {\n    d_c[row*size + col] += d_a[row * size + k] * d_b[k * size + col];\n  }\n}\n\n```", "```py\n// shared\n__global__ void gpu_Matrix_Mul_shared(float *d_a, float *d_b, float *d_c, const int size)\n{\n  int row, col;\n\n  __shared__ float shared_a[TILE_SIZE][TILE_SIZE];\n\n  __shared__ float shared_b[TILE_SIZE][TILE_SIZE];\n\n  // calculate thread id\n  col = TILE_SIZE * blockIdx.x + threadIdx.x;\n  row = TILE_SIZE * blockIdx.y + threadIdx.y;\n\n  for (int i = 0; i< size / TILE_SIZE; i++) \n  {\n    shared_a[threadIdx.y][threadIdx.x] = d_a[row* size + (i*TILE_SIZE + threadIdx.x)];\n    shared_b[threadIdx.y][threadIdx.x] = d_b[(i*TILE_SIZE + threadIdx.y) * size + col];\n    }\n    __syncthreads(); \n\n    for (int j = 0; j<TILE_SIZE; j++)\n      d_c[row*size + col] += shared_a[threadIdx.x][j] * shared_b[j][threadIdx.y];\n    __syncthreads(); // for synchronizing the threads\n\n  }\n}\n```", "```py\nint main()\n{\n   //Define size of the matrix\n  const int size = 4;\n   //Define host and device arrays\n  float h_a[size][size], h_b[size][size],h_result[size][size];\n  float *d_a, *d_b, *d_result; // device array\n  //input in host array\n  for (int i = 0; i<size; i++)\n  {\n    for (int j = 0; j<size; j++)\n    {\n      h_a[i][j] = i;\n      h_b[i][j] = j;\n    }\n  }\n\n  cudaMalloc((void **)&d_a, size*size*sizeof(int));\n  cudaMalloc((void **)&d_b, size*size * sizeof(int));\n  cudaMalloc((void **)&d_result, size*size* sizeof(int));\n  //copy host array to device array\n  cudaMemcpy(d_a, h_a, size*size* sizeof(int), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, h_b, size*size* sizeof(int), cudaMemcpyHostToDevice);\n  //calling kernel\n  dim3 dimGrid(size / TILE_SIZE, size / TILE_SIZE, 1);\n  dim3 dimBlock(TILE_SIZE, TILE_SIZE, 1);\n\n  gpu_Matrix_Mul_nonshared << <dimGrid, dimBlock >> > (d_a, d_b, d_result, size);\n  //gpu_Matrix_Mul_shared << <dimGrid, dimBlock >> > (d_a, d_b, d_result, size);\n\n  cudaMemcpy(h_result, d_result, size*size * sizeof(int), cudaMemcpyDeviceToHost);\n\n  return 0;\n}\n```", "```py\nprintf(\"The result of Matrix multiplication is: \\n\");\n\n  for (int i = 0; i< size; i++)\n  {\n    for (int j = 0; j < size; j++)\n    {\n      printf(\"%f \", h_result[i][j]);\n    }\n    printf(\"\\n\");\n  }\ncudaFree(d_a)\ncudaFree(d_b)\ncudaFree(d_result)\n```"]