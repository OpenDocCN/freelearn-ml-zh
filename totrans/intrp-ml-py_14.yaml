- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What’s Next for Machine Learning Interpretability?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the last thirteen chapters, we have explored the field of **Machine Learning**
    (**ML**) interpretability. As stated in the preface, it’s a broad area of research,
    most of which hasn’t even left the lab and become widely used yet, and this book
    has no intention of covering absolutely all of it. Instead, the objective is to
    present various interpretability tools in sufficient depth to be useful as a starting
    point for beginners and even complement the knowledge of more advanced readers.
    This chapter will summarize what we’ve learned in the context of the ecosystem
    of ML interpretability methods, and then speculate on what’s to come next!
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the main topics we are going to cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the current landscape of ML interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speculating on the future of ML interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the current landscape of ML interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will provide some context on how the book relates to the main goals
    of ML interpretability and how practitioners can start applying the methods to
    achieve those broad goals. Then, we’ll discuss the current areas of growth in
    research.
  prefs: []
  type: TYPE_NORMAL
- en: Tying everything together!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed in *Chapter 1*, *Interpretation, Interpretability, and Explainability;
    and Why Does It All Matter?*, there are three main themes when talking about ML
    interpretability: **Fairness, Accountability, and Transparency** (**FAT**), and
    each of these presents a series of concerns (see *Figure 14.1*). I think we can
    all agree these are all desirable properties for a model! Indeed, these concerns
    all present opportunities for the improvement of **Artificial Intelligence** (**AI**)
    systems. These improvements start by leveraging model interpretation methods to
    evaluate models, confirm or dispute assumptions, and find problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What your aim is will depend on what stage you are at in the ML workflow. If
    the model is already in production, the objective might be to evaluate it with
    a whole suite of metrics, but if the model is still in early development, the
    aim may be to find deeper problems that a metric won’t discover. Perhaps you are
    also just using black-box models for knowledge discovery as we did in *Chapter
    4* – in other words, leveraging the models to learn from the data with no plan
    to take it to production. If this is the case, you might confirm or dispute the
    assumptions you had about the data, and by extension, the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18406_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: ML interpretation methods'
  prefs: []
  type: TYPE_NORMAL
- en: In any case, none of these aims are mutually exclusive, and you should probably
    always be looking for problems and disputing assumptions, even when the model
    appears to be performing well!
  prefs: []
  type: TYPE_NORMAL
- en: And regardless of the aim and primary concern, it is recommended that you use
    many interpretation methods, not only because no technique is perfect but also
    because all problems and aims are interrelated. In other words, there’s no justice
    without consistency and no reliability without transparency.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, you can read *Figure 14.1* from bottom to top as if it were a pyramid,
    because transparency is foundational, followed by accountability in the second
    tier, and, ultimately, fairness is the cherry on top.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, even when the goal is to assess model fairness, the model should
    be stress-tested for robustness. Most relevant feature importances and interactions
    should be understood. Otherwise, it won’t matter if predictions aren’t robust
    and transparent.
  prefs: []
  type: TYPE_NORMAL
- en: There are many interpretation methods covered in *Figure 14.1*, and these are
    by no means every interpretation method available. They represent the most popular
    methods with well-maintained open-source libraries behind them. In this book,
    we have touched on most of them, albeit some of them only briefly. Those that
    weren’t discussed are in *italics* and those that were discussed have the relevant
    chapter numbers provided next to them. There’s been a focus on **model-agnostic**
    methods for **black-box supervised learning models**. Still, outside of this realm,
    there are also many other interpretation methods, such as those found in reinforcement
    learning, generative models, or the many statistical methods used strictly for
    linear regression. And even within the supervised learning black-box model realm,
    there are hundreds of application-specific model interpretation methods used for
    applications ranging from chemistry graph CNNs to customer churn classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, many of the methods discussed in this book can be tailored
    to a wide variety of applications. Integrated gradients can be used to interpret
    audio classifiers and weather forecasting models. Sensitivity analysis can be
    employed in financial modeling and infectious disease risk models. Causal inference
    methods can be leveraged to improve user experience and drug trials.
  prefs: []
  type: TYPE_NORMAL
- en: '*Improve* is the operative word here because interpretation methods have a
    flip side!'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, that flip side has been referred to as *tuning for interpretability*,
    which means creating solutions to problems with FAT. Those solutions can be appreciated
    in *Figure 14.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18406_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: Toolset to treat FAT issues'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have observed five approaches to interpretability solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mitigating Bias**: Any corrective measure that is taken to account for bias.
    Please note that this bias refers to the sampling, exclusion, prejudice, and measurement
    biases in the data, along with any other bias introduced into the ML workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Placing Guardrails**: Any solution that ensures that the model is constrained
    so that it doesn’t contradict the domain knowledge and predict without confidence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhancing Reliability**: Any fix that increases the confidence and consistency
    of predictions, excluding those that do so by reducing complexity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing Complexity**: Any means by which sparsity is introduced. As a side
    effect, this generally enhances reliability by generalizing better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensuring Privacy**: Any effort to secure private data and model architecture
    from third parties. We didn’t cover this approach in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also three areas in which these approaches can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data (“preprocessing”)**: By modifying the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model (“in-processing”)**: By modifying the model, its parameters, or training
    procedure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction (“postprocessing”)**: By intervening in the inference of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s a fourth area that can impact the other three – namely, data and algorithmic
    governance. This includes regulations and standards that dictate a certain methodology
    or framework. It’s a missing column because very few industries and jurisdictions
    have laws dictating what methods and approaches should be applied to comply with
    FAT. For instance, governance could impose a standard for explaining algorithmic
    decisions, data provenance, or a robustness certification threshold. We will discuss
    this further in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can tell in *Figure 14.2* that many of the methods repeat themselves for
    FAT. **Feature Selection and Engineering, Monotonic Constraints**, and **Regularization**
    benefit all three but are not always leveraged by the same approach. **Data Augmentation**
    also can enhance reliability for fairness and accountability. As with *Figure
    14.1*, the items in italics were not covered in the book, of which three topics
    stand out: **Uncertainty Estimation**, **Adversarial Robustness**, and **Privacy
    Preservation** are fascinating topics and deserve books of their own.'
  prefs: []
  type: TYPE_NORMAL
- en: Current trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most significant deterrents of AI adoption is a lack of interpretability,
    which is partially the reason why 50-90% of AI projects never take off (see the
    *Further reading* section for articles about this), and the other is the ethical
    transgressions that happen as a result of not complying with FAT. In this aspect,
    **Interpretable Machine Learning** (**iML**) has the power to lead ML as a whole
    because it can help with both goals with the corresponding methods in *Figure
    14.1* and *Figure 14.2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully, we are witnessing an increase in interest and production in iML,
    mostly under **Explainable Artificial Intelligence** (**XAI**) – see *Figure 14.3*.
    In the scientific community, iML is still the most popular term, but XAI dominates
    in public settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/18406_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: Publication and search trends for iML and XAI'
  prefs: []
  type: TYPE_NORMAL
- en: This means that just as ML is starting to get standardized, regulated, consolidated,
    and integrated into a whole host of other disciplines, interpretation will soon
    get a seat at the table.
  prefs: []
  type: TYPE_NORMAL
- en: ML is replacing software in all industries. And as more is getting automated,
    more models are deployed to the cloud, and it will get worse with the **Artificial
    Intelligence of Things** (**AIoT**). Deployment is not traditionally in the ML
    practitioner’s wheelhouse. That is why ML increasingly depends on **Machine Learning
    Operations** (**MLOps**). And the pace of automation means more tools are needed
    to build, test, deploy, and monitor these models. At the same time, there’s a
    need for the standardization of tools, methods, and metrics. Slowly but surely,
    this is happening. Since 2017, we have had the **Open Neural Network Exchange**
    (**ONNX**), an open standard for interoperability. And at the time of writing,
    the **International Organization for Standardization** (**ISO**) has over two
    dozen AI standards being written (and one published), several of which involve
    interpretability. Naturally, some things will get standardized because of common
    use, due to the consolidation of ML model classes, methods, libraries, service
    providers, and practices. Over time, one or a few in each area will emerge. Lastly,
    given ML’s outsized role in algorithmic decision-making, it’s only a matter of
    time before it is regulated. Only some financial markets regulate trading algorithms,
    such as the **Securities and Exchange Commission** (**SEC**) in the United States
    and the **Financial Conduct Authority** (**FCA**) in the UK. Besides that, only
    data privacy and provenance regulations are widely enforced, such as the HIPAA
    in the US and the LGPD in Brazil. The GDPR in the European Union takes this a
    bit further with the “right to an explanation” for algorithmic decisions but the
    intended scope and methodology are still unclear.
  prefs: []
  type: TYPE_NORMAL
- en: '**XAI versus IML – which one to use?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'My take: although they are understood as synonyms in industry and *iML* is
    regarded as more of an academic term, ML practitioners, even those in industry,
    should be wary about using the term *XAI*. Words can have outsized suggestive
    power. *Explainable* presumes full understanding, but *interpretable* leaves room
    for error, as there always should be when talking about models, and extraordinarily
    complex black-box ones at that. Furthermore, AI has captured the public imagination
    as a panacea or has been vilified as dangerous. Either way, along with the term
    *explainable*, it serves to make it even more filled with hubris for those who
    think it’s a panacea and perhaps calm some concerns for those who think it’s dangerous.
    XAI term might be serving a purpose as a marketing term. However, for those who
    build models, the suggestive power of the word *explainable* can make us overconfident
    in our interpretations. That being said, this is just an opinion.'
  prefs: []
  type: TYPE_NORMAL
- en: ML interpretability is growing quickly but is lagging behind ML. Some interpretation
    tools have been integrated into the cloud ecosystem, from SageMaker to DataRobot.
    They are yet to be fully automated, standardized, consolidated, and regulated,
    but there’s no doubt that this will happen.
  prefs: []
  type: TYPE_NORMAL
- en: Speculating on the future of ML interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’m used to hearing the metaphor of this period being the “Wild West of AI”,
    or worse, an “AI Gold Rush!” It conjures images of an unexplored and untamed territory
    being eagerly conquered, or worse, civilized. Yet, in the 19th century, the United
    States western areas were not too different from other regions on the planet and
    had already been inhabited by Native Americans for millennia, so the metaphor
    doesn’t quite work. Predicting with the accuracy and confidence that we can achieve
    with ML would spook our ancestors and is not a “natural” position for us humans.
    It’s more akin to flying than exploring unknown land.
  prefs: []
  type: TYPE_NORMAL
- en: The article *Toward the Jet Age of machine learning* (linked in the *Further
    reading* section at the end of this chapter) presents a much more fitting metaphor
    of AI being like the dawn of aviation. It’s new and exciting, and people still
    marvel at what we can do from down below (see *Figure 14.4*)!
  prefs: []
  type: TYPE_NORMAL
- en: 'However, aviation had yet to fulfill its potential. Decades after the barnstorming
    era, aviation matured into the safe, reliable, and efficient Jet Age of **commercial
    aviation**. In the case of aviation, the promise was that it could reliably take
    goods and people halfway around the world in less than a day. In AI’s case, the
    promise is that it can make fair, accountable, and transparent decisions – maybe
    not for any decision, but at least those it was designed to make unless it’s an
    example of **Artificial General Intelligence** (**AGI**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, outdoor, old  Description automatically generated](img/18406_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: Barnstorming during the 1920s (United States Library of Congress’s
    Prints and Photographs Division)'
  prefs: []
  type: TYPE_NORMAL
- en: So how do we get there? The following are a few ideas I anticipate will occur
    in the pursuit of reaching the Jet Age of ML.
  prefs: []
  type: TYPE_NORMAL
- en: A new vision for ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we intend to go farther with AI than we have ever gone before, the ML practitioners
    of tomorrow must be more aware of the dangers of the sky. And by the sky, I mean
    the new frontiers of predictive and prescriptive analytics. The risks are numerous
    and involve all kinds of biases and assumptions, problems with data both known
    and potential, and our models’ mathematical properties and limitations. It’s easy
    to be deceived by ML models thinking they are software. Still, in this analogy,
    the software is completely deterministic in nature – it’s solidly anchored to
    the ground, not hovering in the sky!
  prefs: []
  type: TYPE_NORMAL
- en: For civil aviation to become safe, it required a new mindset – a new culture.
    The fighter pilots of WWII, as capable as they were, had to be retrained to work
    in civil aviation. It’s not the same mission because when you know that you are
    carrying passengers on board and the stakes are high, everything changes.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical AI, and by extension, iML, ultimately require this awareness that models
    directly or indirectly carry passengers “on board,” and that models aren’t as
    robust as they seem. A robust model must be able to reliably withstand almost
    any condition over and over again in the same way the planes of today do. To that
    end, we need to be using more instruments, and those instruments come in the form
    of interpretation methods.
  prefs: []
  type: TYPE_NORMAL
- en: A multidisciplinary approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tighter integration with many disciplines is needed for models that comply with
    the principles of FAT. This means more significant involvement of AI ethicists,
    lawyers, sociologists, psychologists, human-centered designers, and countless
    other professions. Along with AI technologists and software engineers, they will
    help code best practices into standards and regulations.
  prefs: []
  type: TYPE_NORMAL
- en: Adequate standardization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: New standards will be needed not only for code, metrics, and methodologies but
    also for language. The language behind data has mostly been derived from statistics,
    math, computer science, and econometrics, which leads to a lot of confusion.
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing regulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It will likely be required that all production models fulfill the following
    specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: Are certifiably robust and fair
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are capable of explaining their reasoning behind one prediction with a TRACE
    command and, in some cases, are required to deliver the reasoning with the prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can abstain from a prediction they aren’t confident about
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yield confidence levels for all predictions (see the **conformal prediction**
    tutorial and book in the *Further reading* section)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have metadata with training data provenance (even if anonymized) and authorship
    and, when needed, regulatory compliance certificates and metadata tied to a public
    ledger – possibly a blockchain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have security certificates much like websites do to ensure a certain level of
    trust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expire, and stop working upon expiration, until they are retrained with new
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be taken offline automatically when they fail model diagnostics and only put
    online again when they pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have **Continuous Training/Continuous Integration** (**CT/CI**) pipelines that
    help retrain the model and perform the model diagnostics at regular intervals
    to avoid any model downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are diagnosed by a certified AI auditor when they fail catastrophically and
    cause public damage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New regulations will likely create new professions such as AI auditors and model
    diagnostics engineers. But they will also prop up MLOps engineers and ML automation
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: Seamless machine learning automation with built-in interpretation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the future, we won’t program an ML pipeline; it will mostly be a drag-and-drop
    capability with a dashboard offering all kinds of metrics. It will evolve to be
    mostly automated. Automation shouldn’t come as a surprise because some existing
    libraries perform automated feature-selection model training. Some interpretability-enhancing
    procedures may be done automatically, but most of them will require human discretion.
    However, interpretation ought to be injected throughout the process, much like
    planes that mostly fly themselves have instruments that alert pilots of issues;
    the value is in informing the ML practitioner of potential problems and improvements
    at every step. Did it find a feature to recommend for monotonic constraints? Did
    it find some imbalances that might need adjusting? Did it find anomalies in the
    data that might need some correction? Show the practitioner what needs to be seen
    to make an informed decision and let them make it.
  prefs: []
  type: TYPE_NORMAL
- en: Tighter integration with MLOps engineers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Certifiably robust models trained, validated, and deployed at a click of a button
    require more than just cloud infrastructure – they also need the orchestration
    of tools, configurations, and people trained in MLOps to monitor them and perform
    maintenance at regular intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Interpretable machine learning is an extensive topic, and this book has only
    covered some aspects of some of its most important areas on two levels: diagnosis
    and treatment. Practitioners can leverage the tools offered by the toolkit anywhere
    in the ML pipeline. However, it’s up to the practitioner to choose when and how
    to apply them.'
  prefs: []
  type: TYPE_NORMAL
- en: What matters most is to engage with the tools. Not using the interpretable machine
    learning toolkit is like flying a plane with very few instruments or none at all.
    Much like flying a plane operates under different weather conditions, machine
    learning models operate under different data conditions, and to be a skilled pilot
    or machine learning engineer, we can’t be overconfident and validate or rule out
    hypotheses with our instruments. And much like aviation took a few decades to
    become the safest mode of transportation, it will take AI a few decades to become
    the safest mode of decision-making. It will take a global village to get us there,
    but it will be an exciting journey! And remember, *the best way to predict the
    future is to create it*.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Joury, A., 2022, January 6, *Why 90% of machine learning models never hit the
    market*. TNW | Neural. Retrieved December 4, 2022: [https://thenextweb.com/news/why-most-machine-learning-models-never-hit-market-syndication](https://thenextweb.com/news/why-most-machine-learning-models-never-hit-market-syndication)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wiggers, K., 2019, July 8, *IDC: For 1 in 4 companies, half of all AI projects
    fail*. VentureBeat. Retrieved December 4, 2022: [https://venturebeat.com/ai/idc-for-1-in-4-companies-half-of-all-ai-projects-fail/](https://venturebeat.com/ai/idc-for-1-in-4-companies-half-of-all-ai-projects-fail/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: O’Neil, C., 2017, *Weapons of Math Destruction*. Penguin Books.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Talwalkar, A., 2018, April 25, *Toward the Jet Age of machine learning*. O’Reilly:
    [https://www.oreilly.com/content/toward-the-jet-age-of-machine-learning/](https://www.oreilly.com/content/toward-the-jet-age-of-machine-learning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajiv, S., 2022, September 22, *Getting predictions intervals with conformal
    inference*. Getting predictions intervals with conformal inference · Rajiv Shah’s
    Projects Blog. Retrieved December 4, 2022: [http://projects.rajivshah.com/blog/2022/09/24/conformal_predictions/](http://projects.rajivshah.com/blog/2022/09/24/conformal_predictions/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Angelopoulos, A.N., and Bates, S., 2021, *A Gentle Introduction to Conformal
    Prediction and Distribution-Free Uncertainty Quantification*: [https://arxiv.org/abs/2107.07511](https://arxiv.org/abs/2107.07511)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/inml](Chapter_14.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code107161072033138125.png)'
  prefs: []
  type: TYPE_IMG
