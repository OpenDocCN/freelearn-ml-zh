<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Recommendation Engines</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Building function compositions for data processing</li>
<li>Building machine learning pipelines</li>
<li>Finding the nearest neighbors</li>
<li>Constructing a k-nearest neighbors classifier</li>
<li>Constructing a k-nearest neighbors regressor</li>
<li>Computing the Euclidean distance score</li>
<li>Computing the Pearson correlation score</li>
<li>Finding similar users in the dataset</li>
<li>Generating movie recommendations</li>
<li>Implementing ranking algorithms</li>
<li>Building a filtering model using TensorFlow</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="mce-root">To address the recipes in this chapter, you will need the following files (which are available on GitHub):</p>
<ul>
<li><kbd>function_composition.py</kbd></li>
<li><kbd><span>pipeline.py</span></kbd></li>
</ul>
<ul>
<li><kbd><span>knn.py</span></kbd></li>
<li><kbd><span>nn_classification.py</span></kbd></li>
<li><kbd><span>nn_regression.py</span></kbd></li>
<li><kbd><span>euclidean_score.py</span></kbd></li>
<li><kbd><span>pearson_score.py</span></kbd></li>
<li><kbd><span>find_similar_users.py</span></kbd></li>
<li><kbd><span>movie_recommendations.py</span></kbd></li>
<li><kbd><span>LambdaMARTModel.py</span></kbd></li>
<li><kbd>train.txt</kbd></li>
<li><kbd>vali.txt</kbd></li>
<li><kbd>test.txt</kbd></li>
<li><kbd><span>TensorFilter.py</span></kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing the recommendation engine</h1>
                </header>
            
            <article>
                
<p>A recommendation engine is a model that can predict what a user may be interested in. When we apply this to the context of movies, for example, this becomes a movie recommendation engine. We filter items in our database by predicting how the current user might rate them. This helps us in connecting the user to the right content in our dataset. Why is this relevant? If you have a massive catalog, then the user may or may not find all the content that is relevant to them. By recommending the right content, you increase consumption. Companies such as Netflix heavily rely on recommendations to keep the user engaged.</p>
<p>Recommendation engines usually produce a set of recommendations using either collaborative filtering or content-based filtering. The difference between the two approaches is in the way that the recommendations are mined. Collaborative filtering builds a model from the past behavior of the current user, as well as ratings given by other users. We then use this model to predict what this user might be interested in. Content-based filtering, on the other hand, uses the characteristics of the item itself in order to recommend more items to the user. The similarity between items is the main driving force here. In this chapter, we will focus on collaborative filtering.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building function compositions for data processing</h1>
                </header>
            
            <article>
                
<p>One of the major parts of any machine learning system is the data processing pipeline. Before data is fed into the machine learning algorithm for training, we need to process it in different ways to make it suitable for that algorithm. Having a robust data processing pipeline goes a long way in building an accurate and scalable machine learning system. There are a lot of basic functionalities available, and data processing pipelines usually consist of a combination of these. Instead of calling these functions in a nested or loopy way, it's better to use the functional programming paradigm to build the combination. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>Let's take a look at how to combine these basic functions to form a reusable function composition. </span>In this recipe, we will create three basic functions and look at how to compose a pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to build function compositions for data processing:</p>
<ol>
<li>Create a new Python file and add the following line (the full code is in the <kbd>function_composition.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>from functools import reduce </pre>
<ol start="2">
<li>Let's define a function to add <kbd>3</kbd> to each element of the array:</li>
</ol>
<pre style="padding-left: 60px">def add3(input_array): 
    return map(lambda x: x+3, input_array) </pre>
<ol start="3">
<li>Now, let's define a second function to multiply <kbd>2</kbd> with each element of the array:</li>
</ol>
<pre style="padding-left: 60px">def mul2(input_array): return map(lambda x: x*2, input_array) </pre>
<ol start="4">
<li>Now, let's now define a third function to subtract <kbd>5</kbd> from each element of the array:</li>
</ol>
<pre style="padding-left: 60px">def sub5(input_array): 
    return map(lambda x: x-5, input_array)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Let's define a function composer that takes functions as input arguments and returns a composed function. This composed function is basically a function that applies all the input functions in a sequence:</li>
</ol>
<pre style="padding-left: 60px">def function_composer(*args): 
    return reduce(lambda f, g: lambda x: f(g(x)), args) </pre>
<p style="padding-left: 60px">We use the <kbd>reduce</kbd> function to combine all the input functions by successively applying the functions in a sequence.</p>
<ol start="6">
<li>We are now ready to play with this function composer. Let's define some data and a sequence of operations:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    arr = np.array([2,5,4,7]) 
 
    print("Operation: add3(mul2(sub5(arr)))") </pre>
<ol start="7">
<li>If we use the regular method, we apply this successively, as follows:</li>
</ol>
<pre>    arr1 = add3(arr) 
    arr2 = mul2(arr1) 
    arr3 = sub5(arr2) 
    print("Output using the lengthy way:", list(arr3)) </pre>
<ol start="8">
<li>Now, let's use the function composer to achieve the same thing in a single line:</li>
</ol>
<pre style="padding-left: 60px">    func_composed = function_composer(sub5, mul2, add3) 
    print("Output using function composition:", list(func_composed(arr)))  </pre>
<ol start="9">
<li>We can do the same thing in a single line with the previous method as well, but the notation becomes very nested and unreadable. Also, it is not reusable; you will have to write the whole thing again if you want to reuse this sequence of operations:</li>
</ol>
<pre style="padding-left: 60px">    print("Operation: sub5(add3(mul2(sub5(mul2(arr)))))\nOutput:", \<br/>            list(function_composer(mul2, sub5, mul2, add3, sub5)(arr)))</pre>
<ol start="10">
<li>If you run this code, you will get the following output on the Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Operation: add3(mul2(sub5(arr)))</strong><br/><strong>Output using the lengthy way: [5, 11, 9, 15]</strong><br/><strong>Output using function composition: [5, 11, 9, 15]</strong><br/><strong>Operation: sub5(add3(mul2(sub5(mul2(arr)))))</strong><br/><strong>Output: [-10, 2, -2, 10]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this recipe, we have created three basic functions and have learned how to compose a pipeline. To do this, the <kbd>reduce()</kbd> function was used. This function accepts a function and a sequence and returns a single value. </span></p>
<p class="mce-root"><span>The <kbd>reduce ()</kbd> function calculates the return value, as follows:</span></p>
<ul>
<li class="mce-root">To start, the function calculates the result by using the first two elements of the sequence.</li>
<li class="mce-root">Next, the function uses the result obtained in the previous step and the next value in the sequence.</li>
<li class="mce-root">This process is repeated until the end of the sequence.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The three basic functions used at the beginning of the recipe make use of the <kbd>map()</kbd> function. This function is used to apply a function on all the elements of a specific value. As a result, a map object is returned; this object is an iterator, so we can iterate over its elements. To print this object, we have converted the map object to sequence objects as a list. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to Python's official documentation of the <kbd>map()</kbd> function: <a href="https://docs.python.org/3/library/functions.html#map">https://docs.python.org/3/library/functions.html#map</a></span></li>
<li><span>Refer to Python's official documentation of the <kbd>reduce()</kbd> function: <a href="https://docs.python.org/3/library/functools.html?highlight=reduce#functools.reduce">https://docs.python.org/3/library/functools.html?highlight=reduce#functools.reduce</a></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building machine learning pipelines</h1>
                </header>
            
            <article>
                
<p>The <kbd>scikit-learn</kbd> library is used to build machine learning pipelines. When we define the functions, the library will build a composed object that makes the data go through the entire pipeline. This pipeline can include functions, such as preprocessing, feature selection, supervised learning, and unsupervised learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will be building a pipeline to take the input feature vector, select the top <em>k</em> features, and then classify them using a random forest classifier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to build machine learning pipelines:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>pipeline.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">from sklearn.datasets import samples_generator 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.feature_selection import SelectKBest, f_regression 
from sklearn.pipeline import Pipeline </pre>
<ol start="2">
<li>Let's generate some sample data to play with, as follows:</li>
</ol>
<pre style="padding-left: 60px"># generate sample data 
X, y = samples_generator.make_classification( 
        n_informative=4, n_features=20, n_redundant=0, random_state=5) </pre>
<p style="padding-left: 60px">This line generated <kbd>20</kbd> dimensional feature vectors because this is the default value. You can change it using the <kbd>n_features</kbd> parameter in the previous line.</p>
<ol start="3">
<li>Our first step of the pipeline is to select the <em>k</em> best features before the datapoint is used further. In this case, let's set <kbd>k</kbd> to <kbd>10</kbd>:</li>
</ol>
<pre style="padding-left: 60px"># Feature selector  
selector_k_best = SelectKBest(f_regression, k=10)</pre>
<ol start="4">
<li>The next step is to use a random forest classifier method to classify the data:</li>
</ol>
<pre style="padding-left: 60px"># Random forest classifier 
classifier = RandomForestClassifier(n_estimators=50, max_depth=4)</pre>
<ol start="5">
<li>We are now ready to build the pipeline. The <kbd>Pipeline()</kbd> method allows us to use predefined objects to build the pipeline:</li>
</ol>
<pre style="padding-left: 60px"># Build the machine learning pipeline 
pipeline_classifier = Pipeline([('selector', selector_k_best), ('rf', classifier)]) </pre>
<p style="padding-left: 60px">We can also assign names to the blocks in our pipeline. In the preceding line, we'll assign the <kbd>selector</kbd> name to our feature selector, and <kbd>rf</kbd> to our random forest classifier. You are free to use any other random names here!</p>
<ol start="6">
<li>We can also update these parameters as we go along. We can set the parameters using the names that we assigned in the previous step. For example, if we want to set <kbd>k</kbd> to <kbd>6</kbd> in the feature selector and set <kbd>n_estimators</kbd> to <kbd>25</kbd> in the random forest classifier, we can do so as demonstrated in the following code. Note that these are the variable names given in the previous step:</li>
</ol>
<pre style="padding-left: 60px">pipeline_classifier.set_params(selector__k=6,  
        rf__n_estimators=25) </pre>
<ol start="7">
<li>Let's go ahead and train the classifier:</li>
</ol>
<pre style="padding-left: 60px"># Training the classifier 
pipeline_classifier.fit(X, y) </pre>
<ol start="8">
<li>Let's now predict the output for the training data, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Predict the output prediction = pipeline_classifier.predict(X) print("Predictions:\n", prediction) </pre>
<ol start="9">
<li>Now, let's estimate the performance of this classifier, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Print score 
print("Score:", pipeline_classifier.score(X, y))                         </pre>
<ol start="10">
<li>We can also see which features will get selected, so let's go ahead and print them:</li>
</ol>
<pre style="padding-left: 60px"># Print the selected features chosen by the selector features_status = pipeline_classifier.named_steps['selector'].get_support() selected_features = [] for count, item in enumerate(features_status): if item: selected_features.append(count) print("Selected features (0-indexed):", ', '.join([str(x) for x in selected_features]))</pre>
<ol start="11">
<li>If you run this code, you will get the following output on your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Predictions:</strong><br/><strong> [1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 1</strong><br/><strong> 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1</strong><br/><strong> 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 0 1]</strong><br/><strong>Score: 0.95</strong><br/><strong>Selected features (0-indexed): 0, 5, 9, 10, 11, 15</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The advantage of selecting the <em>k</em> best features is that we will be able to work with low-dimensional data. This is helpful in reducing the computational complexity. The way in which we select the <em>k</em> best features is based on univariate feature selection. This performs univariate statistical tests and then extracts the top performing features from the feature vector. Univariate statistical tests refer to analysis techniques where a single variable is involved.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Once these tests are performed, each feature in the feature vector is assigned a score. Based on these scores, we select the top <em>k</em><em> </em>features. We do this as a preprocessing step in our classifier pipeline. Once we extract the top <em>k</em><em> </em><span>features, a k-dimensional feature vector is formed, and we use it as the input training data for the random forest classifier.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to</span><span> the o</span>fficial documentation of the <kbd>sklearn.ensemble.RandomForestClassifier()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a></li>
<li><span>Refer to the official documentation of the <kbd>sklearn.feature_selection.SelectKBest()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html</a></span></li>
</ul>
<p class="mce-root"/>
<ul>
<li><span>Refer to the official documentation of the <kbd>sklearn.pipeline.Pipeline()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html</a></span></li>
<li><span>Refer to the official documentation of the <kbd>sklearn.feature_selection.f_regression()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html</a></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding the nearest neighbors</h1>
                </header>
            
            <article>
                
<p>The nearest neighbors model refers to a general class of algorithms that aim to make a decision based on the number of nearest neighbors in the training dataset. The nearest neighbors method consists of finding a predefined number of training samples that are close to the distance from the new point and predicting the label. The number of samples can be user defined, consistent, or differ from each other – it depends on the local density of points. The distance can be calculated with any metric measure – the standard Euclidean distance is the most common choice. Neighbor-based methods simply remember all training data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will find the nearest neighbors using a series of points on a Cartesian plane.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to find the nearest neighbors, as follows:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>knn.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.neighbors import NearestNeighbors </pre>
<ol start="2">
<li>Let's create some sample two-dimensional data:</li>
</ol>
<pre style="padding-left: 60px"># Input data 
X = np.array([[1, 1], [1, 3], [2, 2], [2.5, 5], [3, 1],  
        [4, 2], [2, 3.5], [3, 3], [3.5, 4]])</pre>
<ol start="3">
<li>Our goal is to find the three closest neighbors to any given point, so let's define this parameter:</li>
</ol>
<pre style="padding-left: 60px"># Number of neighbors we want to find 
num_neighbors = 3 </pre>
<ol start="4">
<li>Let's define a random datapoint that's not present in the input data:</li>
</ol>
<pre style="padding-left: 60px"># Input point 
input_point = [2.6, 1.7] </pre>
<ol start="5">
<li>We need to see what this data looks like; let's plot it, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Plot datapoints 
plt.figure() 
plt.scatter(X[:,0], X[:,1], marker='o', s=25, color='k') </pre>
<ol start="6">
<li>In order to find the nearest neighbors, we need to define the <kbd>NearestNeighbors</kbd> object with the right parameters and train it on the input data:</li>
</ol>
<pre style="padding-left: 60px"># Build nearest neighbors model 
knn = NearestNeighbors(n_neighbors=num_neighbors, algorithm='ball_tree').fit(X) </pre>
<ol start="7">
<li>We can now find the <kbd>distances</kbd> parameter of the input point to all the points in the input data:</li>
</ol>
<pre style="padding-left: 60px">distances, indices = knn.kneighbors(input_point) </pre>
<ol start="8">
<li>We can print <kbd>k nearest neighbors</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Print the 'k' nearest neighbors 
print("k nearest neighbors")<br/>for rank, index in enumerate(indices[0][:num_neighbors]):<br/>    print(str(rank+1) + " --&gt;", X[index])</pre>
<p style="padding-left: 60px">The <kbd>indices</kbd> array is already sorted, so we just need to parse it and print the datapoints.</p>
<ol start="9">
<li>Now, let's plot the input datapoint and highlight the k-nearest neighbors:</li>
</ol>
<pre style="padding-left: 60px"># Plot the nearest neighbors  
plt.figure() 
plt.scatter(X[:,0], X[:,1], marker='o', s=25, color='k') 
plt.scatter(X[indices][0][:][:,0], X[indices][0][:][:,1],  
        marker='o', s=150, color='k', facecolors='none') 
plt.scatter(input_point[0], input_point[1], 
        marker='x', s=150, color='k', facecolors='none') 
 
plt.show() </pre>
<ol start="10">
<li>If you run this code, you will get the following output on your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>k nearest neighbors</strong><br/><strong>1 --&gt; [2. 2.]</strong><br/><strong>2 --&gt; [3. 1.]</strong><br/><strong>3 --&gt; [3. 3.]</strong></pre>
<p style="padding-left: 60px">Here is the plot of the input datapoints:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1018 image-border" src="assets/1f5c2f9d-5498-4c6d-8eb3-4e0d0c9f9346.png" style="width:35.75em;height:27.00em;"/></p>
<p style="padding-left: 60px">The second output diagram depicts the location of the test datapoint and the three nearest neighbors, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1019 image-border" src="assets/87cc760b-f611-4cb4-90c1-862b9e9f6b2d.png" style="width:34.00em;height:26.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this recipe, we looked for the nearest neighbors by using a series of points on a Cartesian plane. To do this, the space is partitioned into regions based on the positions and characteristics of the training objects. This can be considered as the training set for the algorithm even if it is not explicitly required by the initial conditions. To calculate the distance, the objects are represented through position vectors in a multidimensional space. Finally, a point is assigned to a class if it is the most frequent of the <em>k</em> examples closest to the object under examination. The proximity is measured by the distance between points. Neighbors are taken from a set of objects for which the correct classification is known.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To build the nearest neighbors model, the <kbd>BallTree</kbd> algorithm was used. <kbd>BallTree</kbd> is a data structure that organizes points in a multidimensional space. The algorithm gets its name because it partitions datapoints into a nested set of hyperspheres, known as <strong>balls</strong>. It's useful for a number of applications, most notably, the nearest neighbor search.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to</span><span> the o</span>fficial documentation of the <kbd>sklearn.neighbors.NearestNeighbors()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html</a></li>
<li><span>Refer to</span><span> the o</span>fficial documentation of the <kbd>sklearn.neighbors.BallTree()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree</a></li>
<li><span>Refer to</span><span> </span><em>Nearest Neighbors</em> (from Texas A&amp;M University College of Engineering): <a href="https://www.nada.kth.se/~stefanc/DATORSEENDE_AK/l8.pdf">https://www.nada.kth.se/~stefanc/DATORSEENDE_AK/l8.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing a k-nearest neighbors classifier</h1>
                </header>
            
            <article>
                
<p>The k-nearest neighbors algorithm is an algorithm that uses k-nearest neighbors in the training dataset to find the category of an unknown object. When we want to find the class that an unknown point belongs to, we find the k-nearest neighbors and take a majority vote.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will create a k-nearest neighbors classifier starting from the input data that contains a series of points arranged on a Cartesian plane that shows a grouping within three areas.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to build a k-nearest neighbors classifier:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>nn_classification.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
import matplotlib.cm as cm 
from sklearn import neighbors, datasets 
 
from utilities import load_data </pre>
<ol start="2">
<li>We will use the <kbd>data_nn_classifier.txt</kbd> file for input data. Let's load this input data:</li>
</ol>
<pre style="padding-left: 60px"># Load input data 
input_file = 'data_nn_classifier.txt' 
data = load_data(input_file) 
X, y = data[:,:-1], data[:,-1].astype(np.int) </pre>
<p style="padding-left: 60px">The first two columns contain input data, and the last column contains the labels. Hence, we separated them into <kbd>X</kbd> and <kbd>y</kbd>, as shown in the preceding code.</p>
<ol start="3">
<li>Now, let's visualize the input data, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Plot input data 
plt.figure() 
plt.title('Input datapoints') 
markers = '^sov&lt;&gt;hp' 
mapper = np.array([markers[i] for i in y]) 
for i in range(X.shape[0]): 
    plt.scatter(X[i, 0], X[i, 1], marker=mapper[i],  
            s=50, edgecolors='black', facecolors='none') </pre>
<p style="padding-left: 60px">We iterate through all the datapoints and use the appropriate markers to separate the classes.</p>
<ol start="4">
<li>In order to build the classifier, we need to specify the number of nearest neighbors that we want to consider. Let's define this parameter:</li>
</ol>
<pre style="padding-left: 60px"># Number of nearest neighbors to consider 
num_neighbors = 10</pre>
<p class="mce-root"/>
<ol start="5">
<li>In order to visualize the boundaries, we need to define a grid and evaluate the classifier on that grid. Let's define the step size:</li>
</ol>
<pre style="padding-left: 60px"># step size of the grid 
h = 0.01   </pre>
<ol start="6">
<li>We are now ready to build the k-nearest neighbors classifier. Let's define this and train it, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Create a K-Neighbours Classifier model and train it 
classifier = neighbors.KNeighborsClassifier(num_neighbors, weights='distance') 
classifier.fit(X, y)</pre>
<ol start="7">
<li>We need to create a mesh to plot the boundaries. Let's define this, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Create the mesh to plot the boundaries 
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 
x_grid, y_grid = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) </pre>
<ol start="8">
<li>Now, let's evaluate the <kbd>classifier</kbd> output for all the points:</li>
</ol>
<pre style="padding-left: 60px"># Compute the outputs for all the points on the mesh 
predicted_values = classifier.predict(np.c_[x_grid.ravel(), y_grid.ravel()]) </pre>
<ol start="9">
<li>Let's plot it, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Put the computed results on the map 
predicted_values = predicted_values.reshape(x_grid.shape) 
plt.figure() 
plt.pcolormesh(x_grid, y_grid, predicted_values, cmap=cm.Pastel1) </pre>
<ol start="10">
<li>Now that we have plotted the color mesh, let's overlay the training datapoints to see where they lie in relation to the boundaries:</li>
</ol>
<pre style="padding-left: 60px"># Overlay the training points on the map 
for i in range(X.shape[0]): 
    plt.scatter(X[i, 0], X[i, 1], marker=mapper[i],  
            s=50, edgecolors='black', facecolors='none') 
 
plt.xlim(x_grid.min(), x_grid.max()) 
plt.ylim(y_grid.min(), y_grid.max()) 
plt.title('k nearest neighbors classifier boundaries')</pre>
<p class="mce-root"/>
<ol start="11">
<li>Now, we can consider a test datapoint and see whether the classifier performs correctly. Let's define it and plot it, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Test input datapoint 
test_datapoint = [4.5, 3.6] 
plt.figure() 
plt.title('Test datapoint') 
for i in range(X.shape[0]): 
    plt.scatter(X[i, 0], X[i, 1], marker=mapper[i],  
            s=50, edgecolors='black', facecolors='none') 
 
plt.scatter(test_datapoint[0], test_datapoint[1], marker='x',  
        linewidth=3, s=200, facecolors='black') </pre>
<ol start="12">
<li>We need to extract the k-nearest neighbors classifier using the following model:</li>
</ol>
<pre style="padding-left: 60px"># Extract k nearest neighbors 
dist, indices = classifier.kneighbors(test_datapoint) </pre>
<ol start="13">
<li>Let's plot the k-nearest neighbors classifier and highlight it:</li>
</ol>
<pre style="padding-left: 60px"># Plot k nearest neighbors 
plt.figure() 
plt.title('k nearest neighbors') 
 
for i in indices: 
    plt.scatter(X[i, 0], X[i, 1], marker='o',  
            linewidth=3, s=100, facecolors='black') 
 
plt.scatter(test_datapoint[0], test_datapoint[1], marker='x',  
        linewidth=3, s=200, facecolors='black') 
 
for i in range(X.shape[0]): 
    plt.scatter(X[i, 0], X[i, 1], marker=mapper[i],  
            s=50, edgecolors='black', facecolors='none') 
 
plt.show() </pre>
<ol start="14">
<li>Now, let's print the <kbd>classifier</kbd> output on the Terminal:</li>
</ol>
<pre style="padding-left: 60px">print("Predicted output:", classifier.predict(test_datapoint)[0])</pre>
<p style="padding-left: 60px">The following result is printed: </p>
<pre style="padding-left: 60px"><strong>Predicted output: 2</strong></pre>
<p style="padding-left: 60px">Furthermore, a series of diagrams are shown. The first output diagram depicts the distribution of the input datapoints:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1020 image-border" src="assets/9f7bfe09-977b-4b0f-bf5e-5427fa2ef23a.png" style="width:29.33em;height:23.08em;"/></p>
<p style="padding-left: 60px">The second output diagram depicts the boundaries obtained using the <kbd>k-nearest neighbors</kbd> classifier:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1021 image-border" src="assets/e34cdba7-455f-4eeb-973d-81be00b44cbf.png" style="width:28.50em;height:22.75em;"/></p>
<p style="padding-left: 60px">The third output diagram depicts the location of the test datapoint:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1022 image-border" src="assets/1b7d05a3-6f5b-4fa9-8e28-4024c34dd5a7.png" style="width:29.67em;height:23.50em;"/></p>
<p style="padding-left: 60px">The fourth output diagram depicts the location of the 10 nearest neighbors:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1023 image-border" src="assets/169f394f-96e0-471d-a2c2-78c02098a65b.png" style="width:29.58em;height:23.67em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The k-nearest neighbors classifier stores all the available datapoints and classifies new datapoints based on a similarity metric. This similarity metric usually appears in the form of a distance function. This algorithm is a nonparametric technique, which means that it doesn't need to find out any underlying parameters before formulation. All we need to do is select a value of <kbd>k</kbd> that works for us.</p>
<p><span>Once we find out the k-nearest neighbors classifier, we take a majority vote. A new datapoint is classified by this majority vote of the k-nearest neighbors classifier. This datapoint is assigned to the class that is most common among its k-nearest neighbors. If we set the value of </span><kbd>k</kbd><span> to </span><kbd>1</kbd><span>, then this simply becomes a case of a nearest neighbor classifier where we just assign the datapoint to the class of its nearest neighbor in the training dataset. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>The k-nearest neighbor algorithm is based on the concept of classifying an unknown sample by considering the class of <em>k</em> samples closest to the training set. The new sample will be assigned to the class that most of the <em>k</em> nearest samples belong to. The choice of <em>k</em> is, therefore, very important for the sample to be assigned to the correct class. If k is too small, the classification may be sensitive to noise; if <em>k</em> is too large, the classification may be computationally expensive, and the neighborhood may include samples belonging to other classes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to</span><span> </span><em>kNN classifiers</em> (from the Faculty of Humanities, University of Amsterdam):<span> </span><a href="http://www.fon.hum.uva.nl/praat/manual/kNN_classifiers_1__What_is_a_kNN_classifier_.html">http://www.fon.hum.uva.nl/praat/manual/kNN_classifiers_1__What_is_a_kNN_classifier_.html</a></li>
<li class="mce-root"><span>Refer to</span><span> the o</span>fficial documentation of the <kbd>sklearn.neighbors()</kbd> module: <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors</a></li>
<li><span>Refer to</span><span> </span><em>Nearest neighbor methods</em> (from New York University): <a href="http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture11.pdf">http://people.csail.mit.edu/dsontag/courses/ml13/slides/lecture11.pdf</a></li>
<li><span>Refer to</span><span> the o</span>fficial documentation of the <kbd>sklearn.neighbors.KNeighborsClassifier()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing a k-nearest neighbors regressor</h1>
                </header>
            
            <article>
                
<p>We learned how to use the k-nearest neighbors algorithm to build a classifier. The good thing is that we can also use this algorithm as a regressor. The object's output is represented by its property value, which<span> is the average of the values of its</span> <span>k-nearest neighbors.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will see how to use the <span>k-nearest neighbors algorithm to build a </span>regressor.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to build a k-nearest neighbors regressor:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>nn_regression.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
from sklearn import neighbors </pre>
<ol start="2">
<li>Let's generate some sample Gaussian-distributed data:</li>
</ol>
<pre style="padding-left: 60px"># Generate sample data 
amplitude = 10 
num_points = 100 
X = amplitude * np.random.rand(num_points, 1) - 0.5 * amplitude </pre>
<ol start="3">
<li>We need to add some noise to the data to introduce some randomness into it. The goal of adding noise is to see whether our algorithm can get past it and still function in a robust way:</li>
</ol>
<pre style="padding-left: 60px"># Compute target and add noise 
y = np.sinc(X).ravel()  
y += 0.2 * (0.5 - np.random.rand(y.size))</pre>
<p class="mce-root"/>
<ol start="4">
<li>Now, let's visualize it, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Plot input data 
plt.figure() 
plt.scatter(X, y, s=40, c='k', facecolors='none') 
plt.title('Input data') </pre>
<ol start="5">
<li>We just generated some data and evaluated a continuous-valued function on all these points. Let's define a denser grid of points:</li>
</ol>
<pre style="padding-left: 60px"># Create the 1D grid with 10 times the density of the input data 
x_values = np.linspace(-0.5*amplitude, 0.5*amplitude, 10*num_points)[:, np.newaxis] </pre>
<p style="padding-left: 60px">We defined this denser grid because we want to evaluate our regressor on all of these points and look at how well it approximates our function.</p>
<ol start="6">
<li>Let's now define the number of nearest neighbors that we want to consider:</li>
</ol>
<pre style="padding-left: 60px"># Number of neighbors to consider  
n_neighbors = 8 </pre>
<ol start="7">
<li>Let's initialize and train the k-nearest neighbors regressor using the parameters that we defined earlier:</li>
</ol>
<pre style="padding-left: 60px"># Define and train the regressor 
knn_regressor = neighbors.KNeighborsRegressor(n_neighbors, weights='distance') 
y_values = knn_regressor.fit(X, y).predict(x_values) </pre>
<ol start="8">
<li>Let's see how the regressor performs by overlapping the input and output data on top of each other:</li>
</ol>
<pre style="padding-left: 60px">plt.figure() 
plt.scatter(X, y, s=40, c='k', facecolors='none', label='input data') 
plt.plot(x_values, y_values, c='k', linestyle='--', label='predicted values') 
plt.xlim(X.min() - 1, X.max() + 1) 
plt.ylim(y.min() - 0.2, y.max() + 0.2) 
plt.axis('tight') 
plt.legend() 
plt.title('K Nearest Neighbors Regressor') 
 
plt.show()</pre>
<ol start="9">
<li>If you run this code, the first diagram depicts the input datapoints:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1024 image-border" src="assets/b7a60e11-8e5a-4fb0-9f15-53ecb8bfe37b.png" style="width:31.92em;height:24.83em;"/></p>
<p style="padding-left: 60px">The second diagram depicts the values<span> predicted</span> by the regressor:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1025 image-border" src="assets/789d6996-9492-43c9-a5ac-a2ac90b7654c.png" style="width:32.08em;height:24.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The goal of a regressor is to predict continuous valued outputs. We don't have a fixed number of output categories in this case. We just have a set of real-valued output values, and we want our regressor to predict the output values for unknown datapoints. In this case, we used a <kbd>sinc</kbd> function to demonstrate the k-nearest neighbors regressor. This is also referred to as the <strong>cardinal sine function</strong>. A <kbd>sinc</kbd> function is defined by the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5aa5578d-f46b-4180-9d59-43cc7205db19.png" style="width:18.92em;height:3.08em;"/></p>
<p>When <kbd>x</kbd> is <kbd>0</kbd>, <em>sin(x)/x</em> takes the indeterminate form of <em>0/0</em>. Hence, we have to compute the limit of this function as <kbd>x</kbd> tends to be <kbd>0</kbd>. We used a set of values for training, and we defined a denser grid for testing. As you can see in the preceding diagram, the output curve is close to the training outputs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The main advantages of this method are that it does not require learning or the construction of a model; it can adapt its decision boundaries in an arbitrary way, producing a representation of the most flexible model; and it also guarantees the possibility of increasing the training set. However, this algorithm also has many drawbacks, including being susceptible to data noise, being sensitive to the presence of irrelevant features, and requiring a similarity measure to evaluate proximity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to</span><span> the o</span>fficial documentation of the <kbd>sklearn.neighbors.KNeighborsRegressor()</kbd> function: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html</a></li>
<li>Refer to <span><em>Regression Analysis with R</em>, Giuseppe Ciaburro,</span> Packt Publishing</li>
<li><span>Refer to </span><em>Comparison of Linear Regression with K-Nearest Neighbors</em> (from Duke University): <a href="http://www2.stat.duke.edu/~rcs46/lectures_2017/03-lr/03-knn.pdf">http://www2.stat.duke.edu/~rcs46/lectures_2017/03-lr/03-knn.pdf</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computing the Euclidean distance score</h1>
                </header>
            
            <article>
                
<p>Now that we have sufficient background in machine learning pipelines and the nearest neighbors classifier, let's start the discussion on recommendation engines. In order to build a recommendation engine, we need to define a similarity metric so that we can find users in the database who are similar to a given user. The Euclidean distance score is one such metric that we can use to compute the distance between datapoints. We will shift the discussion toward movie recommendation engines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will see how to compute the Euclidean score between two users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to compute the Euclidean distance score:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>euclidean_score.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import json 
import numpy as np </pre>
<ol start="2">
<li>We will now define a function to compute the Euclidean score between two users. The first step is to check whether the users are present in the database:</li>
</ol>
<pre style="padding-left: 60px"># Returns the Euclidean distance score between user1 and user2  
def euclidean_score(dataset, user1, user2): 
    if user1 not in dataset: 
        raise TypeError('User ' + user1 + ' not present in the dataset') 
 
    if user2 not in dataset: 
        raise TypeError('User ' + user2 + ' not present in the dataset') </pre>
<ol start="3">
<li>In order to compute the score, we need to extract the movies that both the users rated:</li>
</ol>
<pre>    # Movies rated by both user1 and user2 
    rated_by_both = {}  
 
    for item in dataset[user1]: 
        if item in dataset[user2]: 
            rated_by_both[item] = 1 </pre>
<ol start="4">
<li>If there are no common movies, then there is no similarity between the users (or at least, we cannot compute it given the ratings in the database):</li>
</ol>
<pre>    # If there are no common movies, the score is 0  
    if len(rated_by_both) == 0: 
        return 0 </pre>
<ol start="5">
<li>For each of the common ratings, we just compute the square root of the sum of squared differences and normalize it, so that the score is between 0 and 1:</li>
</ol>
<pre style="padding-left: 60px">    squared_differences = []  
 
    for item in dataset[user1]: 
        if item in dataset[user2]: 
            squared_differences.append(np.square(dataset[user1][item] - dataset[user2][item])) 
         
    return 1 / (1 + np.sqrt(np.sum(squared_differences)))  </pre>
<p style="padding-left: 60px">If the ratings are similar, then the sum of squared differences will be very low. Hence, the score will become high, which is what we want from this metric.</p>
<ol start="6">
<li>We will use the <kbd>movie_ratings.json</kbd> file as our data file. Let's load it, as follows:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    data_file = 'movie_ratings.json' 
 
    with open(data_file, 'r') as f: 
        data = json.loads(f.read()) </pre>
<ol start="7">
<li>Let's consider two random users and compute the Euclidean distance score:</li>
</ol>
<pre>    user1 = 'John Carson' 
    user2 = 'Michelle Peterson' 
 
    print("Euclidean score:")<br/>    print(euclidean_score(data, user1, user2)) </pre>
<ol start="8">
<li>When you run this code, you will see the following Euclidean distance score printed on the Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>0.29429805508554946</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In most cases, the distance used in the <span>nearest neighbors algorithm</span> is defined as the Euclidean distance between two<br/>
points, calculated according to the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/de18fb90-9e22-492e-ac02-1ce2a95fe7e2.png" style="width:15.17em;height:4.17em;"/></p>
<p>On a bidimensional plane, the Euclidean distance represents the minimum distance between two points, hence the straight line connecting two points. This distance is calculated as the square root of the sum of the squared difference between the elements of two vectors, as indicated in the previous formula.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">There are other types of metrics for calculating distances. All of these types generally try to avoid the square roots, since they are expensive in computational terms, and are the source of several errors. Metrics include <strong>Minkowski</strong>, <strong>Manhattan</strong>, and the <strong>cosine</strong> distance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to <span><em>MATLAB for Machine Learning</em></span>, Giuseppe Ciaburro, Packt Publishing</li>
<li><span>Refer to</span><span> </span><em>Similarities, Distances, and Manifold Learning</em> (from The University of York): <a href="http://simbad-fp7.eu/images/tutorial/02-ECCV2012Tutorial.pdf">http://simbad-fp7.eu/images/tutorial/02-ECCV2012Tutorial.pdf</a></li>
<li class="mce-root"><span>Refer to <em>The Euclidean distance</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Euclidean_distance">https://en.wikipedia.org/wiki/Euclidean_distance</a></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computing the Pearson correlation score</h1>
                </header>
            
            <article>
                
<p>The Euclidean distance score is a good metric, but it has some shortcomings. Hence, the Pearson correlation score is frequently used in recommendation engines. The Pearson correlation score between two statistical variables is an index that expresses a possible linear relation between them. It measures the tendency of two numerical variables to vary simultaneously.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will see how to compute the <span>Pearson correlation score</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to compute the Pearson correlation score:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>pearson_score.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import json 
import numpy as np </pre>
<ol start="2">
<li>We will define a function to compute the Pearson correlation score between two users in the database. Our first step is to confirm that these users exist in the database:</li>
</ol>
<pre style="padding-left: 60px"># Returns the Pearson correlation score between user1 and user2  
def pearson_score(dataset, user1, user2): 
    if user1 not in dataset: 
        raise TypeError('User ' + user1 + ' not present in the dataset') 
 
    if user2 not in dataset: 
        raise TypeError('User ' + user2 + ' not present in the dataset') </pre>
<ol start="3">
<li>The next step is to get the movies that both of these users rated:</li>
</ol>
<pre>    # Movies rated by both user1 and user2 
    rated_by_both = {} 
 
    for item in dataset[user1]: 
        if item in dataset[user2]: 
            rated_by_both[item] = 1 
 
    num_ratings = len(rated_by_both)  </pre>
<ol start="4">
<li>If there are no common movies, then there is no discernible similarity between these users; hence, we return <kbd>0</kbd>:</li>
</ol>
<pre>    # If there are no common movies, the score is 0  
    if num_ratings == 0: 
        return 0</pre>
<ol start="5">
<li>We need to compute the sum of squared values of common movie ratings:</li>
</ol>
<pre>    # Compute the sum of ratings of all the common preferences  
    user1_sum = np.sum([dataset[user1][item] for item in rated_by_both]) 
    user2_sum = np.sum([dataset[user2][item] for item in rated_by_both]) </pre>
<ol start="6">
<li>Now, let's compute the sum of squared ratings of all the common movie ratings:</li>
</ol>
<pre style="padding-left: 60px">    # Compute the sum of squared ratings of all the common preferences  
    user1_squared_sum = np.sum([np.square(dataset[user1][item]) for item in rated_by_both]) 
    user2_squared_sum = np.sum([np.square(dataset[user2][item]) for item in rated_by_both]) </pre>
<ol start="7">
<li>Let's now compute the sum of the products:</li>
</ol>
<pre style="padding-left: 60px">    # Compute the sum of products of the common ratings  
    product_sum = np.sum([dataset[user1][item] * dataset[user2][item] for item in rated_by_both]) </pre>
<ol start="8">
<li>We are now ready to compute the various elements that we require to calculate the Pearson correlation score:</li>
</ol>
<pre>    # Compute the Pearson correlation 
    Sxy = product_sum - (user1_sum * user2_sum / num_ratings) 
    Sxx = user1_squared_sum - np.square(user1_sum) / num_ratings 
    Syy = user2_squared_sum - np.square(user2_sum) / num_ratings </pre>
<ol start="9">
<li>We need to take care of the case where the denominator becomes <kbd>0</kbd>:</li>
</ol>
<pre>    if Sxx * Syy == 0: 
        return 0 </pre>
<ol start="10">
<li>If everything is good, we <kbd>return</kbd> the Pearson correlation score, as follows:</li>
</ol>
<pre>    return Sxy / np.sqrt(Sxx * Syy) </pre>
<ol start="11">
<li>Let's now define the <kbd>main</kbd> function and compute the Pearson correlation score between two users:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    data_file = 'movie_ratings.json' 
 
    with open(data_file, 'r') as f: 
        data = json.loads(f.read()) 
 
    user1 = 'John Carson' 
    user2 = 'Michelle Peterson' 
 
    print("Pearson score:")<br/>    print(pearson_score(data, user1, user2)) </pre>
<ol start="12">
<li>If you run this code, you will see the following Pearson correlation score printed on the Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Pearson score:</strong><br/><strong>0.39605901719066977</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The <em>r</em> correlation coefficient of Pearson measures the correlation between variables at intervals or equivalent ratios. It is given by the sum of the products of the standardized scores of the two variables (z<sub>x</sub> * z<sub>y</sub>) divided by the number of subjects (or observations), as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5ea3ed8d-7463-47fc-9550-ebfc7eaf7d12.png" style="width:7.75em;height:2.92em;"/></p>
<p>This coefficient can assume values ranging between -1.00 (between the two variables, there is a perfect negative correlation) and + 1.00 (between the two variables, there is a perfect positive correlation). A correlation of 0 indicates that there is no relationship between the two variables.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">It is necessary to remember that Pearson's formula is related to a linear relationship, and therefore, all the different forms of relationship can produce anomalous results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to <span><em>Regression Analysis with R</em></span>, Giuseppe Ciaburro, Packt Publishing</li>
<li class="mce-root"><span>Refer to</span><span> </span><em>The Pearson Correlation</em> (from Ken State University): <a href="https://libguides.library.kent.edu/spss/pearsoncorr">https://libguides.library.kent.edu/spss/pearsoncorr</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding similar users in the dataset</h1>
                </header>
            
            <article>
                
<p>One of the most important tasks in building a recommendation engine is finding users who are similar. This is useful in creating the recommendations that will be provided to these users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will see how to build a model to <span>find users who are similar.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to find similar users in the dataset:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>find_similar_users.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import json 
import numpy as np 
 
from pearson_score import pearson_score </pre>
<ol start="2">
<li>Let's define a function to find users who are similar to the input user. It takes three input arguments: the database, the input user, and the number of similar users that we are looking for. Our first step is to check whether the user is present in the database. If the user exists, we need to compute the Pearson correlation score between this user and all the other users in the database:</li>
</ol>
<pre style="padding-left: 60px"># Finds a specified number of users who are similar to the input user 
def find_similar_users(dataset, user, num_users): 
    if user not in dataset: 
        raise TypeError('User ' + user + ' not present in the dataset') 
 
    # Compute Pearson scores for all the users 
    scores = np.array([[x, pearson_score(dataset, user, x)] for x in dataset if user != x]) </pre>
<ol start="3">
<li>The next step is to sort these scores in descending order:</li>
</ol>
<pre>    # Sort the scores based on second column 
    scores_sorted = np.argsort(scores[:, 1]) 
 
    # Sort the scores in decreasing order (highest score first)  
    scored_sorted_dec = scores_sorted[::-1] </pre>
<ol start="4">
<li>Let's extract the <em>k</em> top scores and then return them:</li>
</ol>
<pre>    # Extract top 'k' indices 
    top_k = scored_sorted_dec[0:num_users]  
 
    return scores[top_k]  </pre>
<ol start="5">
<li>Let's now define the main function and load the input database:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    data_file = 'movie_ratings.json' 
 
    with open(data_file, 'r') as f: 
        data = json.loads(f.read()) </pre>
<ol start="6">
<li>We want to find three similar users to, <kbd>John Carson</kbd>, for example. We do this by using the following steps:</li>
</ol>
<pre>    user = 'John Carson'<br/>    print("Users similar to " + user + ":\n")<br/>    similar_users = find_similar_users(data, user, 3) <br/>    print("User\t\t\tSimilarity score\n")<br/>    for item in similar_users:<br/>        print(item[0], '\t\t', round(float(item[1]), 2))</pre>
<ol start="7">
<li>If you run this code, you will see the following printed on your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Users similar to John Carson:</strong><br/><br/><strong>User               Similarity score</strong><br/><strong>Michael Henry                  0.99</strong><br/><strong>Alex Roberts                   0.75</strong><br/><strong>Melissa Jones                  0.59</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we are looking for similar users to the input user. Given the database, the input user, and the number of similar users that we are looking for, we first check whether the user is present in the database. If the user exists, the Pearson correlation score between this user and all the other users in the database is computed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To calculate the <span>Pearson correlation score, the <kbd>pearson_score()</kbd> function was used. This function was defined in the previous <em>Computing the Pearson correlation score</em> recipe.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to</span><span> </span><em>Pearson's Correlation Coefficient</em> (from the University of the West of England): <a href="http://learntech.uwe.ac.uk/da/default.aspx?pageid=1442">http://learntech.uwe.ac.uk/da/default.aspx?pageid=1442</a></li>
<li class="mce-root"><span>Refer to</span><span> </span><em>Pearson correlation coefficient</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">https://en.wikipedia.org/wiki/Pearson_correlation_coefficient</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating movie recommendations</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will generate movie recommendations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will use all the functionality that we built in the previous recipes to build a movie recommendation engine. Let's take a look at how to build it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to generate movie recommendations:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>movie_recommendations.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import json 
import numpy as np 
 
from pearson_score import pearson_score </pre>
<ol start="2">
<li>We will define a function to generate movie recommendations for a given user. The first step is to check whether the user exists in the dataset:</li>
</ol>
<pre style="padding-left: 60px"># Generate recommendations for a given user 
def generate_recommendations(dataset, user): 
    if user not in dataset: 
        raise TypeError('User ' + user + ' not present in the dataset') </pre>
<ol start="3">
<li>Let's now compute the Pearson score of this user with all the other users in the dataset:</li>
</ol>
<pre>    total_scores = {} 
    similarity_sums = {} 
 
    for u in [x for x in dataset if x != user]: 
        similarity_score = pearson_score(dataset, user, u) 
 
        if similarity_score &lt;= 0: 
            continue </pre>
<ol start="4">
<li>We need to find the movies that haven't been rated by this user:</li>
</ol>
<pre style="padding-left: 60px">        for item in [x for x in dataset[u] if x not in dataset[user] or dataset[user][x] == 0]: 
            total_scores.update({item: dataset[u][item] * similarity_score}) 
            similarity_sums.update({item: similarity_score}) </pre>
<ol start="5">
<li>If the user has watched every single movie in the database, then we cannot recommend anything to this user. Let's take care of this condition:</li>
</ol>
<pre>    if len(total_scores) == 0: 
        return ['No recommendations possible'] </pre>
<ol start="6">
<li>We now have a list of these scores. Let's create a normalized list of movie ranks:</li>
</ol>
<pre>    # Create the normalized list 
    movie_ranks = np.array([[total/similarity_sums[item], item]  
            for item, total in total_scores.items()]) </pre>
<ol start="7">
<li>We need to sort the list in descending order based on the score:</li>
</ol>
<pre>    # Sort in decreasing order based on the first column 
    movie_ranks = movie_ranks[np.argsort(movie_ranks[:, 0])[::-1]] </pre>
<ol start="8">
<li>We are finally ready to extract the movie recommendations:</li>
</ol>
<pre>    # Extract the recommended movies 
    recommendations = [movie for _, movie in movie_ranks] 
 
    return recommendations</pre>
<p class="mce-root"/>
<ol start="9">
<li>Now, let's define the <kbd>main</kbd> function and load the dataset:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    data_file = 'movie_ratings.json' 
 
    with open(data_file, 'r') as f: 
        data = json.loads(f.read()) </pre>
<ol start="10">
<li>Let's now generate recommendations for <kbd>Michael Henry</kbd>, as follows:</li>
</ol>
<pre>    user = 'Michael Henry'<br/>    print("Recommendations for " + user + ":")<br/>    movies = generate_recommendations(data, user) <br/>    for i, movie in enumerate(movies):<br/>        print(str(i+1) + '. ' + movie)</pre>
<ol start="11">
<li>The <kbd>John Carson</kbd> user has watched all the movies. Therefore, if we try to generate recommendations for him, it should display 0 recommendations. Let's see whether this happens, as follows:</li>
</ol>
<pre>    user = 'John Carson' <br/>    print("Recommendations for " + user + ":")<br/>    movies = generate_recommendations(data, user) <br/>    for i, movie in enumerate(movies):<br/>        print(str(i+1) + '. ' + movie)</pre>
<ol start="12">
<li>If you run this code, you will see the following output on your Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong>Recommendations for Michael Henry:</strong><br/><strong>1. Jerry Maguire</strong><br/><strong>2. Inception</strong><br/><strong>3. Anger Management</strong><br/><strong>Recommendations for John Carson:</strong><br/><strong>1. No recommendations possible</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this recipe, we have built a movie recommendation engine. To generate recommendations for a given user, the following steps are performed:</span></p>
<ol>
<li class="mce-root"><span>First, we check whether the user is present in the database<br/></span></li>
<li class="mce-root"><span>Then, we calculate the Person correlation score<br/></span></li>
<li class="mce-root"><span>We then create the normalized list<br/></span></li>
</ol>
<ol start="4">
<li class="mce-root">Then, we sort this list in decreasing order based on the first column</li>
<li class="mce-root"><span>Finally, we extract the recommended movies</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>To build a movie recommendation engine,</span><span> the <kbd>pearson_score()</kbd> function was used. This function was defined in the previous <em>Computing the Pearson correlation score</em> recipe.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to</span><span> </span><em>Introduction to Correlation and Regression Analysis</em> (from Boston University School of Public Health): <a href="http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_multivariable/bs704_multivariable5.html">http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_multivariable/bs704_multivariable5.html</a></li>
<li class="mce-root"><span>Refer to</span><span> </span><em>Pearson Correlation Coefficient r</em> (From Penn State University): <a href="https://newonlinecourses.science.psu.edu/stat501/node/256/">https://newonlinecourses.science.psu.edu/stat501/node/256/</a></li>
<li class="mce-root"><span>Refer to</span><span> </span><em>Correlation and Causation</em> (from the Australian Bureau of Statistics): <a href="http://www.abs.gov.au/websitedbs/a3121120.nsf/home/statistical+language+-+correlation+and+causation">http://www.abs.gov.au/websitedbs/a3121120.nsf/home/statistical+language+-+correlation+and+causation</a></li>
</ul>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing ranking algorithms</h1>
                </header>
            
            <article>
                
<p><strong>Learning to rank</strong> (<strong>LTR</strong>) is a method that is used in the construction of classification models for information retrieval systems. The training data consists of lists of articles with an induced partial order that gives a numerical or ordinal score, or a binary judgment for each article. The purpose of the model is to order the elements into new lists according to the scores that take into account the judgments obtained from the articles.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use the <kbd>pyltr</kbd> package, which is a Python LTR toolkit with ranking models, evaluation metrics, and data-wrangling helpers. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to implement ranking algorithms:</p>
<ol>
<li>Create a new Python file and import the following package (the full code is in the <kbd><span>LambdaMARTModel.py</span></kbd><span> </span>file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import pyltr</pre>
<ol start="2">
<li>We will load the data contained in the Letor dataset that's <span>already provided for you (<kbd>train.txt</kbd>, <kbd>vali.txt</kbd>, and <kbd>test.txt</kbd>)</span>:</li>
</ol>
<pre style="padding-left: 60px">with open('train.txt') as trainfile, \<br/>        open('vali.txt') as valifile, \<br/>        open('test.txt') as testfile:<br/>    TrainX, Trainy, Trainqids, _ = pyltr.data.letor.read_dataset(trainfile)<br/>    ValX, Valy, Valqids, _ = pyltr.data.letor.read_dataset(valifile)<br/>    TestX, Testy, Testqids, _ = pyltr.data.letor.read_dataset(testfile)<br/>    metric = pyltr.metrics.NDCG(k=10)</pre>
<ol start="3">
<li>Let's now perform a validation of the data:</li>
</ol>
<pre>    monitor = pyltr.models.monitors.ValidationMonitor(<br/>                ValX, Valy, Valqids, metric=metric, stop_after=250)</pre>
<ol start="4">
<li>We will build the model, as follows:</li>
</ol>
<pre style="padding-left: 60px"> model = pyltr.models.LambdaMART(<br/>    metric=metric,<br/>    n_estimators=1000,<br/>    learning_rate=0.02,<br/>    max_features=0.5,<br/>    query_subsample=0.5,<br/>    max_leaf_nodes=10,<br/>    min_samples_leaf=64,<br/>    verbose=1,<br/>)</pre>
<ol start="5">
<li>Now, we can fit the model using the text data:</li>
</ol>
<pre style="padding-left: 60px">model.fit(TestX, Testy, Testqids, monitor=monitor)</pre>
<ol start="6">
<li>Next, we can predict the data, as follows:</li>
</ol>
<pre style="padding-left: 60px">Testpred = model.predict(TestX)</pre>
<ol start="7">
<li>Finally, we print the results, as follows:</li>
</ol>
<pre style="padding-left: 60px">print('Random ranking:', metric.calc_mean_random(Testqids, Testy))<br/>print('Our model:', metric.calc_mean(Testqids, Testy, Testpred))</pre>
<p style="padding-left: 60px">The following results are printed:</p>
<pre style="padding-left: 60px"><strong>Early termination at iteration 480</strong><br/><strong>Random ranking: 0.27258472902087394</strong><br/><strong>Our model: 0.5487673789992693</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">LambdaMART is the enhanced tree version of LambdaRank, which is, in turn, based on RankNet. RankNet, LambdaRank, and LambdaMART are algorithms that are used to solve classification problems in many contexts. RankNet, LambdaRank, and LambdaMART have been developed by Chris Burges and his group at Microsoft Research. RankNet was the first one to be developed, followed by LambdaRank, and then LambdaMART.</p>
<p>RankNet is based on the use of neural networks, but the underlying model is not limited to neural networks alone. The cost function for RankNet aims to minimize the number of reversals in the ranking. RankNet optimizes the cost function using the stochastic gradient descent.</p>
<p>The researchers found that during the RankNet training procedure, the costs are not required, only the gradients (λ) of the cost compared to the model score. You can think of these gradients as small arrows attached to each document in the classified list, indicating the direction in which we could move those documents. LambdaRank is based on this assumption.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Finally, LambdaMART combines the methods contained in LambdaRank and those present in <strong>multiple regression additive trees</strong> (<strong>MART</strong>). While MART uses decision trees with enhanced gradient for forecasting, LambdaMART uses enhanced gradient decision trees using a cost function derived from LambdaRank to solve a ranking task. LambdaMART proved to be more efficient than LambdaRank and the original RankNet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to</span><span> </span><em>Learning to Rank using Gradient Descent</em>: <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2005/08/icml_ranking.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2005/08/icml_ranking.pdf</a></li>
<li><span>Refer to</span><span> the </span>Python LTR toolkit: <a href="https://github.com/jma127/pyltr">https://github.com/jma127/pyltr</a></li>
<li><span>Refer to</span><span> </span><em>LETOR: Learning to Rank for Information Retrieval</em>: <a href="https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fbeijing%2Fprojects%2Fletor%2F">https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fbeijing%2Fprojects%2Fletor%2F</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a filtering model using TensorFlow</h1>
                </header>
            
            <article>
                
<p>Collaborative filtering refers to a class of tools and mechanisms that allow the retrieval of predictive information regarding the interests of a given set of users starting from a large and yet undifferentiated mass of knowledge. Collaborative filtering is widely used in the context of recommendation systems. A well-known category of collaborative algorithms is matrix factorization.</p>
<p class="mce-root">The fundamental assumption behind the concept of collaborative filtering is that every single user who has shown a certain set of preferences will continue to show them in the future. A popular example of collaborative filtering can be a system of suggested movies starting from a set of basic knowledge of the tastes and preferences of a given user. It should be noted that although this information is referring to a single user, they derive this from the knowledge that has been processed throughout the whole system of users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will see <span>how to build a collaborative filtering model for personalized recommendations using TensorFlow. We will use the MovieLens 1M dataset, which contains 1 million ratings from approximately 6,000 users for approximately 4,000 movies. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to build a filtering model using TensorFlow:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>TensorFilter.py</kbd><span> </span>file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import pandas as pd<br/>import tensorflow as tf</pre>
<ol start="2">
<li>We will load the data contained in the <span>MovieLens 1M dataset</span><span> that's </span><span>already provided for you (<kbd>ratings.csv</kbd>)</span>:</li>
</ol>
<pre style="padding-left: 60px">Data = pd.read_csv('ratings.csv', sep=';', names=['user', 'item', 'rating', 'timestamp'], header=None)<br/><br/>Data = Data.iloc[:,0:3]<br/><br/>NumItems = Data.item.nunique() <br/>NumUsers = Data.user.nunique()<br/><br/>print('Item: ', NumItems)<br/>print('Users: ', NumUsers)</pre>
<p style="padding-left: 60px">The following returns are returned:</p>
<pre style="padding-left: 60px"><strong>Item: 3706</strong><br/><strong>Users: 6040</strong></pre>
<ol start="3">
<li>Now, let's perform data scaling, as follows:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.preprocessing import MinMaxScaler<br/>scaler = MinMaxScaler()<br/>Data['rating'] = Data['rating'].values.astype(float)<br/>DataScaled = pd.DataFrame(scaler.fit_transform(Data['rating'].values.reshape(-1,1)))<br/>Data['rating'] = DataScaled</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>We will build the user item matrix, as follows:</li>
</ol>
<pre style="padding-left: 60px">UserItemMatrix = Data.pivot(index='user', columns='item', values='rating')<br/>UserItemMatrix.fillna(0, inplace=True)<br/><br/>Users = UserItemMatrix.index.tolist()<br/>Items = UserItemMatrix.columns.tolist()<br/><br/>UserItemMatrix = UserItemMatrix.as_matrix()</pre>
<ol start="5">
<li>Now, we can set some <span>network parameters, as follows</span>:</li>
</ol>
<pre style="padding-left: 60px">NumInput = NumItems<br/>NumHidden1 = 10<br/>NumHidden2 = 5</pre>
<ol start="6">
<li>Now, we will initialize the TensorFlow placeholder. Then, <kbd>weights</kbd> and <kbd>biases</kbd> are randomly initialized:</li>
</ol>
<pre style="padding-left: 60px">X = tf.placeholder(tf.float64, [None, NumInput])<br/><br/>weights = {<br/>    'EncoderH1': tf.Variable(tf.random_normal([NumInput, NumHidden1], dtype=tf.float64)),<br/>    'EncoderH2': tf.Variable(tf.random_normal([NumHidden1, NumHidden2], dtype=tf.float64)),<br/>    'DecoderH1': tf.Variable(tf.random_normal([NumHidden2, NumHidden1], dtype=tf.float64)),<br/>    'DecoderH2': tf.Variable(tf.random_normal([NumHidden1, NumInput], dtype=tf.float64)),<br/>}<br/><br/>biases = {<br/>    'EncoderB1': tf.Variable(tf.random_normal([NumHidden1], dtype=tf.float64)),<br/>    'EncoderB2': tf.Variable(tf.random_normal([NumHidden2], dtype=tf.float64)),<br/>    'DecoderB1': tf.Variable(tf.random_normal([NumHidden1], dtype=tf.float64)),<br/>    'DecoderB2': tf.Variable(tf.random_normal([NumInput], dtype=tf.float64)),<br/>}</pre>
<ol start="7">
<li>Now, we can build the encoder and decoder model, as follows:</li>
</ol>
<pre style="padding-left: 60px">def encoder(x):<br/>    Layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['EncoderH1']), biases['EncoderB1']))<br/>    Layer2 = tf.nn.sigmoid(tf.add(tf.matmul(Layer1, weights['EncoderH2']), biases['EncoderB2']))<br/>    return Layer2<br/><br/>def decoder(x):<br/>    Layer1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['DecoderH1']), biases['DecoderB1']))<br/>    Layer2 = tf.nn.sigmoid(tf.add(tf.matmul(Layer1, weights['DecoderH2']), biases['DecoderB2']))<br/>    return Layer2</pre>
<ol start="8">
<li>We will construct the model and predict the value, as follows:</li>
</ol>
<pre style="padding-left: 60px">EncoderOp = encoder(X)<br/>DecoderOp = decoder(EncoderOp)<br/><br/>YPred = DecoderOp<br/><br/>YTrue = X</pre>
<ol start="9">
<li>We will now define <kbd>loss</kbd> and <kbd>optimizer</kbd>, and minimize the squared error and the evaluation metrics:</li>
</ol>
<pre style="padding-left: 60px">loss = tf.losses.mean_squared_error(YTrue, YPred)<br/>Optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)<br/>EvalX = tf.placeholder(tf.int32, )<br/>EvalY = tf.placeholder(tf.int32, )<br/>Pre, PreOp = tf.metrics.precision(labels=EvalX, predictions=EvalY)</pre>
<ol start="10">
<li>Let's now initialize the variables, as follows:</li>
</ol>
<pre style="padding-left: 60px">Init = tf.global_variables_initializer()<br/>LocalInit = tf.local_variables_initializer()<br/>PredData = pd.DataFrame()</pre>
<ol start="11">
<li>Finally, <span>we can start to train our model</span>:</li>
</ol>
<pre style="padding-left: 60px">with tf.Session() as session:<br/>    Epochs = 120<br/>    BatchSize = 200<br/><br/>    session.run(Init)<br/>    session.run(LocalInit)<br/><br/>    NumBatches = int(UserItemMatrix.shape[0] / BatchSize)<br/>    UserItemMatrix = np.array_split(UserItemMatrix, NumBatches)<br/>    <br/>    for i in range(Epochs):<br/><br/>        AvgCost = 0<br/><br/>        for batch in UserItemMatrix:<br/>            _, l = session.run([Optimizer, loss], feed_dict={X: batch})<br/>            AvgCost += l<br/><br/>        AvgCost /= NumBatches<br/><br/>        print("Epoch: {} Loss: {}".format(i + 1, AvgCost))<br/><br/>    UserItemMatrix = np.concatenate(UserItemMatrix, axis=0)<br/><br/>    Preds = session.run(DecoderOp, feed_dict={X: UserItemMatrix})<br/><br/>    PredData = PredData.append(pd.DataFrame(Preds))<br/><br/>    PredData = PredData.stack().reset_index(name='rating')<br/>    PredData.columns = ['user', 'item', 'rating']<br/>    PredData['user'] = PredData['user'].map(lambda value: Users[value])<br/>    PredData['item'] = PredData['item'].map(lambda value: Items[value])<br/>    <br/>    keys = ['user', 'item']<br/>    Index1 = PredData.set_index(keys).index<br/>    Index2 = Data.set_index(keys).index<br/><br/>    TopTenRanked = PredData[~Index1.isin(Index2)]<br/>    TopTenRanked = TopTenRanked.sort_values(['user', 'rating'], ascending=[True, False])<br/>    TopTenRanked = TopTenRanked.groupby('user').head(10)<br/>    <br/>    print(TopTenRanked.head(n=10))</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><br/><strong>    user item rating</strong><br/><strong>2651 1 2858 0.295800</strong><br/><strong>1106 1 1196 0.278715</strong><br/><strong>1120 1 1210 0.251717</strong><br/><strong>2203 1 2396 0.227491</strong><br/><strong>1108 1 1198 0.213989</strong><br/><strong>579  1 593  0.201507</strong><br/><strong>802  1 858  0.196411</strong><br/><strong>2374 1 2571 0.195712</strong><br/><strong>309  1 318  0.191919</strong><br/><strong>2785 1 2997 0.188679</strong></pre>
<p style="padding-left: 30px">These are the top 10 results for the <kbd>1</kbd> user.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The collaborative filter approach focuses on finding users who have made similar judgments to the same objects, thus creating a link between users, to whom will be suggested objects that one of the two has reviewed in a positive way, or simply with which they have interacted. In this way, we look for associations between users, and no longer between objects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The user item matrix represents a user's preferences for an object, but, if read by columns, highlights who a certain movie was liked or disliked by. In this way, you can see how a similarity between two objects can also be expressed without the object matrix, simply by observing that the films that are liked by the same people are probably similar in some way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to</span><span> </span><em>Collaborative filtering</em> (from The University of Texas at Dallas): <a href="https://www.utdallas.edu/~nrr150130/cs6375/2015fa/lects/Lecture_23_CF.pdf">https://www.utdallas.edu/~nrr150130/cs6375/2015fa/lects/Lecture_23_CF.pdf</a></li>
<li class="mce-root"><span>Refer to</span><span> </span><em>Matrix Factorization and Collaborative Filtering</em> (from Carnegie Mellon University): <a href="https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture25-mf.pdf">https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture25-mf.pdf</a></li>
<li><span>Refer to the</span><span> </span><em>TensorFlow Tutorial</em> (from Stanford University): <a href="https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf">https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf</a></li>
</ul>


            </article>

            
        </section>
    </body></html>