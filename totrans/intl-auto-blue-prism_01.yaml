- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine Learning as a Service: the Digital Exchange and Web APIs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of how we interact with ML today is done through **web APIs**. Even when
    using **large language model** (**LLM**) chatbots through a web browser, web API
    calls are being made in the background to give us the reply. More often than not,
    your BP Processes will also use web APIs to obtain the results of an ML prediction
    that’s hosted online.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to explore the most popular ML web APIs for IA,
    how to find them on BP’s **Digital Exchange** (**DX**), how to connect them to
    BP, and how to build one yourself so that predictions can be made in your automation
    use cases. More specifically, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what the most common ML services are, some of their common use
    cases, and how to find them on the DX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going through two examples of using pre-built downloadable assets from the DX
    to make ML predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a BP **Web API Service** from scratch to connect to an ML service that
    is not currently available on the DX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By the end of the chapter, we’ll have covered examples of three of the most
    commonly used **machine learning as a service** (**MLaaS**) platforms: **Amazon
    Web Service** (**AWS**), **Azure**, and **Google Cloud Platform** (**GCP**). These
    examples also cover some of the most common IA use cases: extracting data from
    unstructured text, extracting data from forms, and extracting text from images.
    We will have also covered some of the key concepts that will inform our solution
    design in the future: **single** versus **batch** and **synchronous** versus **asynchronous**
    predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, ensure that the following are in place:'
  prefs: []
  type: TYPE_NORMAL
- en: A valid Blue Prism Portal ([https://portal.blueprism.com](https://portal.blueprism.com))
    account. This is required to download assets from the DX. An account can be created
    free of charge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An active account at AWS, Azure, and GCP. We will go over examples using each
    vendor in this chapter. All examples can be run within the free tiers offered
    by the services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the following file from GitHub at [https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/Ex_1_to_3.bprelease](https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/Ex_1_to_3.bprelease).
    Import the `.bprelease` file into BP. This contains sample Processes that will
    be used in our examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the DX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DX is BP’s marketplace, containing many BP-developed and community-submitted
    `.bprelease`, `.bpskill`, `.bpobject`, `.bpprocess`, and `.xml`. Most of the assets
    on the DX are free of charge to download and integrate; however, there are likely
    costs associated with using the ML API services themselves. Using pre-built assets
    from the DX is the *easiest and fastest way* to get ML into your automated business
    processes, provided the service fits your use case.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll see what popular ML services are available on the DX
    and what the potential use cases are. Real-life use case examples are provided
    whenever possible, drawn from my own research examining over 100 IA use cases
    and technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the DX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DX can be accessed at [https://digitalexchange.blueprism.com](https://digitalexchange.blueprism.com).
    You’ll need to log in using your BP Portal account credentials to download assets.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s much more available on the DX than just web APIs. Since we’re only
    interested in web APIs for this chapter, click on **More Filters** and filter
    based on **Connector** to narrow down the search results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Filtering for web APIs on the DX](img/B18416_01_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Filtering for web APIs on the DX
  prefs: []
  type: TYPE_NORMAL
- en: The **Search for Assets** search box can then be used to find specific ML services
    *if you already know the service name*. Later, I’ll provide a summary of all of
    the most popular search terms you can use to find ML assets on the DX based on
    their use case. But first, let’s discuss some fundamentals that are needed to
    use ML web APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning web API fundamentals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under traditional RPA, BP interacts with desktop applications or with websites
    through a web browser. Connecting to ML algorithms can’t normally be done through
    either of those methods. Instead, over 90% of commercially available algorithms,
    including those on AWS, Azure, GCP, OpenAI, and so on, are exposed as web APIs.
  prefs: []
  type: TYPE_NORMAL
- en: A standard web API call from a BP Digital Worker is shown in the following image.
    First, the Digital Worker makes an API request, which reaches the API endpoint
    through the Internet. The ML prediction is made and the endpoint returns an API
    response containing the prediction back to BP.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – The most common way to use ML with RPA: web API calls over the
    internet](img/B18416_01_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2 – The most common way to use ML with RPA: web API calls over the
    internet'
  prefs: []
  type: TYPE_NORMAL
- en: Many ML models are proprietary. Vendors want to protect their intellectual property
    by hosting models on their own servers, which have protected API endpoints. If
    your BP environment isn’t allowed to connect to the Internet, you’ll need to consult
    with your networking and security teams to see whether exemptions can be made.
  prefs: []
  type: TYPE_NORMAL
- en: If not, then using publicly hosted ML APIs won’t be possible. You’ll either
    have to develop and host your own models over the Intranet or make predictions
    directly on the Digital Workers themselves. Note that it’s the Digital Workers
    that make the ML requests. Some people mistakenly believe that the BP application
    server will make calls to the API endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Note the distinction between a *web API*, which is what we connect to get an
    ML prediction, and a *Web API Service*, which is a BP product feature. The web
    API services feature is used to define and set up connections to web APIs. We’ll
    create a Web API Service from scratch as the last example in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Official documentation on this product feature can be found here: [https://bpdocs.blueprism.com/bp-7-1/en-us/Web%20API/HTML/configure-api-definition.htm](https://bpdocs.blueprism.com/bp-7-1/en-us/Web%20API/HTML/configure-api-definition.htm)'
  prefs: []
  type: TYPE_NORMAL
- en: In the remaining parts of this section, we’ll look at what’s important to know
    about web APIs from an IA integration perspective. This includes **authentication**,
    **JSON**, **pricing**, **single** versus **batch** prediction, and **synchronous**
    versus **asynchronous** prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most web API services require you to sign up before they can be used. After
    signing up, you’ll be given one or more unique ID(s), similar to a username and
    password, to uniquely identify you. Depending on the specific service, these unique
    ID(s) might be enough to directly gain access and call your ML API to receive
    your prediction. This is shown in *Figure 1**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – When the unique IDs are valid as authentication credentials](img/B18416_01_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – When the unique IDs are valid as authentication credentials
  prefs: []
  type: TYPE_NORMAL
- en: An example of this is Azure. You can pass in the unique ID, called a **subscription
    key** in Azure terminology, to access your endpoint for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: However, most services will require you to use your ID(s) to request for a temporary
    access token before calling the ML API. First, you make a request to receive a
    temporary access token using your unique credentials. Then, pass in this temporary
    token along with the input data to your ML API request to get your predicted result.
    For example, AWS calls their unique IDs the **access key ID** and the **secret
    access key**. In GCP, they are called the **Client ID** and the **Client Secret**.
    Both AWS and GCP require passing in a temporary access token to the ML API call.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – A two-part API call: getting the access token and calling the
    ML API](img/B18416_01_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4 – A two-part API call: getting the access token and calling the
    ML API'
  prefs: []
  type: TYPE_NORMAL
- en: The specific way that this works differs from vendor to vendor. For instance,
    some will have you submit the unique IDs as query parameters, while others need
    you to submit them in the request headers. Some use a proprietary algorithm to
    generate your temporary access token (including AWS) while others use open standards
    such as OAuth2 (including GCP).
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to consult the vendor’s documentation to find out specific details.
    One of the main benefits of using a pre-built asset from the DX is that the authentication
    logic is usually handled for you! All you need to do is generate your unique credentials
    from the ML provider, and the temporary access token handling logic (if any) will
    be handled for you already.
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: JSON is one of the most popular data formats used to exchange data between web
    services. Almost all ML web APIs expect to receive JSON as input data and provide
    JSON as output data. When using a web API from the DX, almost all of the conversions
    between BP’s data types to JSON will be done for you.
  prefs: []
  type: TYPE_NORMAL
- en: There may be situations where you need to manually convert data from BP into
    JSON. This is most likely to happen for `Utility – JSON` VBO, which can be found
    in the `VBO` sub-folder where BP is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending files in JSON format requires you to first encode them, usually in
    Base64 format. There are three VBOs provided by BP on the DX that can help with
    this: `Utility – Encode Decode`, `Utility – File Manipulation`, and `Base64Encoder`.'
  prefs: []
  type: TYPE_NORMAL
- en: Pricing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MLaaS offerings typically have free usage up to a limit. After the free limit
    is used, you’ll start to pay per transaction. The limits are usually reset on
    a monthly basis. For example, let’s look at Azure’s Computer Vision API ([https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/)).
    At the time of writing this book, it offers 5,000 free transactions per month.
    For the next 1 million transactions, the cost is 10 cents per transaction. Between
    1 and 10 million transactions, the cost is 6.5 cents per transaction. As you make
    more transactions per month, your cost per transaction goes down.
  prefs: []
  type: TYPE_NORMAL
- en: The pricing varies based on many factors, such as what region you’re in, what
    specific service you’re using in their Computer Vision library, and whether you’re
    willing to pre-commit to using a minimum number of transactions.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that *one transaction is not the same as one API call*!
    For example, there may be a service that allows you to send in multiple documents
    at once. Each document might be counted as a separate transaction, despite being
    submitted in one API call. The devil is in the details, so you really need to
    look at the pricing pages carefully when evaluating how suitable an MLaaS offering
    is for your use case. This represents ongoing costs to the operation of the IA
    solution that must be weighed against the benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Single versus batch predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s key to understand the API by reading through its documentation before using
    it. One important thing to note when looking at the documentation is whether the
    API allows for only a single prediction per API request or whether it can accept
    multiple inputs in a batch for a single API call. There may be separate endpoints
    for one versus the other.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Batch prediction](img/B18416_01_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Batch prediction
  prefs: []
  type: TYPE_NORMAL
- en: For batch predictions, you’ll first send the input data for multiple predictions
    to the API endpoint in a single request (the first step in *Figure 1**.5*). The
    predictions will be made (second step) and the response will return all of the
    predicted results (third step). In the single prediction case, only one prediction
    can be made per API request call.
  prefs: []
  type: TYPE_NORMAL
- en: Note that *Single Request* and *Single Response* shown in the image are NOT
    characteristics of single versus batch; it is a question of synchronous versus
    asynchronous, which will be discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous vs. asynchronous predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Single versus batch refers to the number of predictions that can be requested
    for in a single API call. Synchronous versus asynchronous, on the other hand,
    refers to whether the prediction results are returned in the same API call as
    the request.
  prefs: []
  type: TYPE_NORMAL
- en: For a synchronous (also called **online**) call, the predicted result(s) is
    returned in the same call as the request. This is used when the prediction can
    be made relatively quickly, such as with short documents and single images. *Figure
    1**.5* also shows an online case where the response is returned in the same API
    call.
  prefs: []
  type: TYPE_NORMAL
- en: 'For asynchronous (also called **offline**) calls, you won’t receive the prediction
    in the same API call as the request. Instead, you’ll first receive a unique ID
    for your request that you can use to enquire about the status of your prediction
    job. Imagine if you need to process a large video as input. It may take minutes
    or even hours before the ML service can come back with its prediction. Let’s look
    at AWS’s Rekognition Video API as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – AWS’s Rekognition Video API: asynchronous calls](img/B18416_01_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6 – AWS’s Rekognition Video API: asynchronous calls'
  prefs: []
  type: TYPE_NORMAL
- en: First, you create an API request for a prediction by providing an S3 link to
    the video. In this first API call, you’ll receive a `JobId` back. For the second
    API call, you pass in the JobId to the `GetLabelDetection` endpoint, which returns
    the `JobStatus`. If the video is still being processed, you’ll receive a `JobStatus`
    of `IN_PROGRESS`.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the size of the video, you may need to wait a few seconds or even
    a few minutes before the prediction is finished. During this time, you can continue
    to call the `GetLabelDetection` API endpoint to check the status of your job.
    When it’s finished, the JobStatus will turn into `SUCCEEDED` and the predicted
    labels will be returned with the request.
  prefs: []
  type: TYPE_NORMAL
- en: While batch often implies asynchronous, this isn’t always the case! It’s possible
    for a single prediction to be asynchronous (such as in the AWS Rekognition Video
    example) and for batch prediction to be synchronous (as will be seen in the last
    example in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the basics of ML web APIs, let’s look at how we can use
    the DX to download pre-built assets and quickly make an ML prediction without
    having to develop an ML model ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of MLaaS on the DX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLaaS offerings, such as those from Azure, AWS, GCP, and IBM, allow you to quickly
    integrate pre-trained ML algorithms into BP. You’ll be able to use the ML parts
    of IA without the complexities of the ML lifecycle, such as training, choosing
    between algorithms, tuning, hosting, maintenance, etc. However, MLaaS is often
    less flexible, as the models are pre-trained and usually not customizable to your
    specific use case. Most of these services provide *generic* rather than *specific*
    ML capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: To save you time searching the DX, I’ve compiled and summarized a list of the
    most popular MLaaS offerings currently available, grouped by the type of input
    data you’re looking to provide into the algorithm. This will give you a sense
    of what IA cases you can quickly and simply implement. The types of input data
    are categorized as **Images**, **Video**, **Speech**, and **Text**.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal ML models that can accept different types of inputs are starting
    to appear. An example of this is GPT-4, which can accept both text and image as
    input. These models won’t be covered in this section, as they aren’t fully available
    during the writing of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Note that numerical data isn’t listed as an input. Numerical input is usually
    used for regression problems, and there aren’t many services that provide ML models
    that are generic enough to be useful for regression. If you’re trying to predict
    numerical data, you’ll probably need to build your own ML model.
  prefs: []
  type: TYPE_NORMAL
- en: From a development or proof-of-concept perspective, most ML vendors offer generous
    free usage tiers of their ML prediction services. This allows you to evaluate
    whether it makes sense to pursue a particular IA project in a short period of
    time.
  prefs: []
  type: TYPE_NORMAL
- en: While investigating 122 IA use cases for my thesis research, I found that 75%
    of them could potentially have been fulfilled by MLaaS, so investigating these
    services and the DX is well worth your time. Text-based services were by far the
    most common, representing over 66% of all IA use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Image services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Image services return insight into what’s found in images. Most of these services
    have specific requirements with regard to what images can be submitted to their
    service. This can include file size requirements, image formats (PNG and JPG are
    the most common), image resolution, minimum and maximum dimensions, and image
    orientation.
  prefs: []
  type: TYPE_NORMAL
- en: Some will require you to upload the files as part of the API request body content,
    while others will ask you to first upload the files somewhere and provide URLs
    to the images. Many of these APIs offer *both single and batch operations* and
    are typically *synchronous*, returning the detected labels in the same request
    as the initial response.
  prefs: []
  type: TYPE_NORMAL
- en: Image-based ML services only accounted for 10% of IA use cases that I found.
    An issue with these types of services is that the generic objects returned by
    the predictions aren’t specific enough to be useful. However, many of these services
    also allow you to customize the labels, meaning that you can further train an
    ML model, using the vendor’s model as a starting base, to match what you’re interested
    in finding. This is where many potential image-based use cases can be unlocked.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a prediction is made, you’ll get back the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of labels that are found in the image(s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X* and *Y* coordinates representing the boundaries of the discovered objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence scores showing the degree of certainty that the object is of a particular
    label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The services and potential use cases of image-based services include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Object detection**: This allows you to find labels in images drawn from generic
    categories, such as animals, people, cars, buildings, etc. This can be further
    customized, in many cases, to detect *specific objects*, such as through AWS’s
    **DetectCustomLabels** and Azure’s **Custom Vision**. Some real-life IA use cases
    include quality assurance in factories for counting and finding product defects
    and dispensing prescription medication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face and facial feature detection**: This predicts whether faces are present
    in an image and the gender, estimated age, and overall mood displayed by the face.
    This could be used for surveillance and the processing of applications that contain
    photographs. The only IA use case I found that implemented facial recognition
    was to take attendance and to quantify student engagement during online learning
    courses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image content moderation**: This allows us to determine whether an image
    contains adult or violent themes. This is most often used for user-uploaded content
    moderation on social media. I didn’t find real use cases for this in IA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text detection**: This returns where in the image text is found and what
    the text is. This is one of the most common image-based IA use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Form detection**: This tells us whether an image has extractable form fields
    or tables in it. This is generically useful for IA and could be used for processing
    invoices, receipts, and financial reporting data. A real-life example of this
    is extracting data from scans of hand-written bank account application forms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  | **Search term for** **DX skill** |'
  prefs: []
  type: TYPE_TB
- en: '| **Feature** | **AWS** | **Azure** | **Google Cloud** | **IBM** |'
  prefs: []
  type: TYPE_TB
- en: '| Object detection | Rekognition | Computer Vision | Cloud Vision API, Google
    Vision Skill | Visual Recognition Skill IBM Watson |'
  prefs: []
  type: TYPE_TB
- en: '| Face and facial feature detection | Rekognition | Computer Vision | Cloud
    Vision API, Google Vision Skill | Visual Recognition Skill IBM Watson |'
  prefs: []
  type: TYPE_TB
- en: '| Content moderation | Rekognition | Computer Vision | Cloud Vision API, Google
    Vision Skill | Visual Recognition Skill IBM Watson |'
  prefs: []
  type: TYPE_TB
- en: '| Text detection | Rekognition, Textract | Computer Vision | Cloud Vision API,
    Google Vision Skill | Visual Recognition Skill IBM Watson |'
  prefs: []
  type: TYPE_TB
- en: '| Form detection | Textract | Form Recognizer Client | Cloud Document AI |
    N/A* |'
  prefs: []
  type: TYPE_TB
- en: Table 1.1 – Search terms for image-based ML services on the DX
  prefs: []
  type: TYPE_NORMAL
- en: '***Available from the vendor but not on the DX, so you must build your own**
    **web API.**'
  prefs: []
  type: TYPE_NORMAL
- en: Video services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Services that take videos as input have a similar functionality to image services.
    They allow for the analysis of videos in *almost real time*. Depending on the
    service, you might be asked to provide your input as the URL of a stream, or to
    upload your video files to predesignated area.
  prefs: []
  type: TYPE_NORMAL
- en: Almost real time is a key point to keep in mind. Most ML services that process
    video are *asynchronous* and don’t immediately give you the predicted result.
    First, you make the call to the API to request a prediction. Then, you have to
    make a further call(s) to check whether the prediction is ready.
  prefs: []
  type: TYPE_NORMAL
- en: There are usually maximum limits set on the length of the videos (3–6 hours)
    and file sizes (10–50 GB) that can be processed. These limits differ from vendor
    to vendor. Please check their API documentation and follow their guidelines with
    regard to these limits, resolutions, camera angles, video bitrates, etc. In my
    research, I didn’t find any use cases that used video as the input source. Instead
    of video, it may make sense to take snapshots of the video at regular intervals
    (e.g., every 5 seconds) and to use image-based ML services instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a prediction is made, you’ll get back the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of labels that are found in the video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X* and *Y* coordinates representing the boundaries of the discovered objects'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start and end time ranges or timestamp for each label that is detected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence scores showing the degree of certainty that the object is of a particular
    label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The services and potential use cases of image-based services include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Object detection**: This can be used to detect generic labels, such as animals,
    people, cars, buildings, etc. Sample use cases include surveillance, drone footage
    analysis, and quality assurance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scene change detection**: This detects whether a video feed switches from
    one camera to another. This could be used for video cataloging, archiving, and
    trimming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content moderation**: This predicts whether a video contains adult or violent
    themes. It’s often used for user-uploaded content moderation on social media.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logo detection**: This detects logos in videos. It could be used to automatically
    flag or sensor brands and logos in video or to monitor user-submitted content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face detection**: This detects the presence of faces in video. It can be
    used to automate access control to facilities, to track movement, and for surveillance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text extraction**: This is used to extract text from video. Even if the text
    is partially obscured for portions of the video, these services can reconstruct
    the full text if the camera moves enough to cover the entire text during the course
    of the video. This could be used to extract presentation slide content from live
    events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  | **Search term for** **DX Skill** |'
  prefs: []
  type: TYPE_TB
- en: '| **Feature** | **AWS** | **Azure** | **Google Cloud** | **IBM** |'
  prefs: []
  type: TYPE_TB
- en: '| Object detection | N/A* | N/A* | Cloud Video Intelligence | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Scene changes | N/A* | N/A* | Cloud Video Intelligence | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Content moderation | N/A* | N/A* | Cloud Video Intelligence | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Logo detection | N/A | N/A | Cloud Video Intelligence | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Face detection | N/A* | N/A* | Cloud Video Intelligence | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Text extraction | N/A* | N/A* | Cloud Video Intelligence | N/A |'
  prefs: []
  type: TYPE_TB
- en: Table 1.2 - Search terms for video-based ML services on the DX
  prefs: []
  type: TYPE_NORMAL
- en: '***Available from the vendor but not on the DX, so you must build your own**
    **web API.**'
  prefs: []
  type: TYPE_NORMAL
- en: Speech services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These allow you to extract data from live-streamed audio and saved audio files.
    This can be done *synchronously* or *asynchronously*, depending on the length
    of the audio or the size of the file. Shorter files are typically processed synchronously.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the API, the vendor may require you to upload audio files directly
    into the ML endpoint or upload them to cloud storage. They will also have requirements
    on the length of the audio and the languages spoken. In my research, I did not
    find any implemented use cases of IA using speech as the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical outputs of a speech-based ML service are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The transcribed text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timestamps of intervals of the transcribed text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Common capabilities and use cases of speech ML are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speech transcription**: This converts spoken audio into text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text to speech**: This allows text to be read out loud as speech.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`purchase`, `modify`, or `cancel` `a ticket`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speaker recognition**: This helps you to identify different speakers. This
    works best when the number of speakers to predict is sent as an input to the algorithm.
    It also assigns each speaker with a unique identifier. You can use the unique
    identifier to map audio back to a human speaker manually if you already know the
    identities of the speakers. This could be used to automatically add subtitles
    that include speakers’ names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech translation**: This allows you to transcribe audio into text of a
    different language. This can also be achieved by chaining a speech transcription
    service to a text translation service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  | **Search term for** **DX Skill** |'
  prefs: []
  type: TYPE_TB
- en: '| **Feature** | **AWS** | **Azure** | **Google Cloud** | **IBM** |'
  prefs: []
  type: TYPE_TB
- en: '| Speech transcription | Transcribe | Speech to text | Cloud Speech-to-Text
    API | N/A* |'
  prefs: []
  type: TYPE_TB
- en: '| Text to speech | Polly | N/A* | Cloud Text-to-Speech | N/A* |'
  prefs: []
  type: TYPE_TB
- en: '| Intent recognition | N/A* | N/A* | N/A* | N/A* |'
  prefs: []
  type: TYPE_TB
- en: '| Speaker recognition | N/A* | Azure Speaker Recognition Beta | N/A* | N/A*
    |'
  prefs: []
  type: TYPE_TB
- en: '| Speech translation | N/A | N/A* | N/A | N/A |'
  prefs: []
  type: TYPE_TB
- en: Table 1.3 – Search terms for video-based ML services on the DX
  prefs: []
  type: TYPE_NORMAL
- en: '***Available from the vendor but not on the DX, so you must build your own**
    **web API.**'
  prefs: []
  type: TYPE_NORMAL
- en: Text services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text-based ML services are by far the most commonly used ML services in IA.
    Around two-thirds of the 122 IA use cases I found were text-based, 52% of the
    cases involved the use of translation, **natural language processing** (**NLP**),
    **named entity recognition** (**NER**), OCR, and document classification, and
    15% of the use cases used chatbots as an interface to trigger IA.
  prefs: []
  type: TYPE_NORMAL
- en: Text services are *usually* synchronous, and depending on the service, prediction
    requests can either be sent in one by one or in a batch. Text-based ML services
    take in the text to be analyzed and the language as input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical outputs received from text ML are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The predicted labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are common ways that text ML can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document classification**: This allows you to differentiate between different
    types of documents. This normally leads to further ML processing, such as entity
    recognition. An example of this is invoice processing. Document classification
    can be used to classify invoices by vendor. Another example is to separate documents
    into invoices and purchase orders because they often look similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity recognition**: This extracts text into predefined categories, such
    as names, dates, numerical figures, and events. This can be used for categorizing
    text, filtering CVs, and extracting data from unstructured reports. This is one
    of the most popular use cases, especially for processing invoices, claims, and
    other standardized forms. Many vendors allow you to customize the list of entities
    to match your specific use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`I waited for 45 minutes before it was my turn`, the key phrases could be “waited”
    and “45 minutes”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: This finds the high-level opinion or attitude expressed
    in a block of text. Some common labels returned include *positive*, *neutral*,
    *negative*, and *mixed*. Most models can’t distinguish more nuanced sentiments
    under the same label; for example, “excited” and “happy” would both be returned
    as positive despite being different in reality. Common ways this can be used is
    to triage emails or customer support tickets and to detect sentiment in social
    media and product reviews.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language detection**: This is typically used as a first step before sending
    the text to other text-based ML algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Translation**: This can translate text from one language to another in either
    real time or on demand. Over 50 languages are supported by all vendors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chatbots**: These services allow companies to create conversational interfaces
    for their applications. For example, one government agency used chatbots as an
    interface for people to request social security benefits, which further triggered
    IA to complete the processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question answering**: This is one of the main features of LLMs. As this technology
    is relatively new, I haven’t come across any use cases of this being used in IA
    just yet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text generation**: This is another relatively new use case enabled by LLMs.
    This would be useful as a way to create responses to user complaints or emails.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  | **Search term for** **DX Skill** |'
  prefs: []
  type: TYPE_TB
- en: '| **Feature** | **AWS** | **Azure** | **Google Cloud** | **IBM** |'
  prefs: []
  type: TYPE_TB
- en: '| Document classification | N/A* | Form Recognizer client | AutoML Natural
    Language, Cloud Natural Language API | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Entity recognition | Comprehend Capability AWS | Text analytics, OpenAI,
    ChatGPT | AutoML Natural Language, Cloud Natural Language API | Language understanding
    |'
  prefs: []
  type: TYPE_TB
- en: '| Key phrase extraction | Comprehend Capability AWS | Text analytics, OpenAI,
    ChatGPT | Cloud Natural Language API | Language understanding |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | Comprehend Capability AWS | Text analytics, OpenAI,
    ChatGPT | AutoML Natural Language, Cloud Natural Language API | Language understanding
    |'
  prefs: []
  type: TYPE_TB
- en: '| Language detection | Comprehend Capability AWS | Text analytics, OpenAI,
    ChatGPT | Cloud Natural Language API | Language understanding |'
  prefs: []
  type: TYPE_TB
- en: '| Translation | Translate capability AWS | Translate skill Azure Cloud, OpenAI,
    ChatGPT | Translate Skill Google Cloud | Language translation skill IBM |'
  prefs: []
  type: TYPE_TB
- en: '| Chatbots | N/A* | QnAMaker, OpenAI, ChatGPT | N/A* | N/A* |'
  prefs: []
  type: TYPE_TB
- en: '| Question answering | N/A | OpenAI, ChatGPT^ | N/A* | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Text generation | N/A | OpenAI, ChatGPT^ | N/A* | N/A |'
  prefs: []
  type: TYPE_TB
- en: Table 1.4 - Search terms for video-based ML services on the DX
  prefs: []
  type: TYPE_NORMAL
- en: '***Available from the vendor but not on the DX, so you must build your own**
    **web API.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**^OpenAI is not completely owned by Microsoft, although they have a** **strong
    partnership.**'
  prefs: []
  type: TYPE_NORMAL
- en: An interesting development has happened with the release of GPT-4, Bard, and
    other LLMs. These models have the innate ability to perform tasks such as text
    translation, entity extraction, language detection, sentiment analysis, etc.,
    despite not having been trained to do so explicitly. I expect that LLMs will become
    a viable alternative to many of the text-based and image-based ML APIs as they
    become more mature.
  prefs: []
  type: TYPE_NORMAL
- en: Vendor selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we looked at four major MLaaS vendors: AWS, Azure,
    GCP, and IBM. The decision to choose one over the other is complex and highly
    dependent on constraints that may already exist at your company. Selection criteria
    can include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Existing preferences or discounts at a particular vendor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing organizational or team knowledge in using one platform over another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The presence of must-have services that are offered by only one of the vendors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The availability of the service in a desirable geographic location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The costs of the service itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The accuracy of the service given your input data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the service supports your desired single versus batch or synchronous
    versus asynchronous processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data retention and security policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SLAs for uptime and API usage limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The availability of the service as a ready-to-use asset on the DX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before moving on to hands-on examples, let’s summarize what we’ve just learned.
    The DX is BP’s “app store,” where many connectors to ML services are available.
    Most of these connectors are web APIs that use JSON as a data exchange format.
    Some of the most challenging parts of using an ML web service (such as authentication)
    and specific API requirements are already included in the DX asset, allowing you
    to quickly integrate ML into your BP process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we looked at common use cases of ML from the main ML service vendors:
    AWS, Azure, GCP, and IBM. Each vendor has marketing names for its ML services.
    It’s not straightforward to know what each service does based on its name. I’ve
    broken down all of the services based on the type of input it expects to take
    in: image, video, speech, and text. I’ve also listed their marketing names as
    well as the search terms you can use to find them on the DX, if they exist. Finally,
    we looked at some factors that can influence our decision to choose one ML vendor
    over another.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll be going through three examples of using web APIs to
    make ML predictions. For the first two examples, we’ll be *downloading* and using
    two different assets from the DX. In the third example, we’ll be creating a Web
    API Service from scratch. One web API from each major vendor will be used: AWS,
    Azure, and GCP. These examples will require you to sign up for free accounts to
    use their services. If you don’t have accounts with each vendor, follow along
    with the ones you do have accounts with.'
  prefs: []
  type: TYPE_NORMAL
- en: From AWS, we’ll use Comprehend for text analysis of an email service request.
    With Azure, we’ll use Form Recognizer to extract data from an invoice. Finally,
    from GCP, we’ll use the Cloud Vision API to extract text from an image-based PDF.
    These three services were chosen because they’re the most popular ML service vendors,
    have different authentication methods, and cover the most common use cases encountered
    in IA, processing text data, forms, and images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most difficult parts of getting a web API working are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Authentication**: Part of this is done on the MLaaS vendor’s portal to generate
    the right keys. The other part is inputting those keys into the correct part of
    BP, usually a Credential. The DX asset’s documentation will guide you on how this
    should be done.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Formatting input data**: This normally requires reading the API’s documentation,
    but this is provided to you from the Web API Service or the Object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Formatting output data**: This normally requires reading the API’s documentation,
    but this is provided to you from the Web API Service or the Object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The three examples that we will go through and how they differ from one another
    are summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service** | **Implementation for API** **in BP** | **Authentication** |
    **Batch /** **single** | **Sync /** **async** | **Use case** |'
  prefs: []
  type: TYPE_TB
- en: '| AWS Comprehend | Object + web API | Temporary access token (object) + access
    key ID + secret access key | Single | Sync | Extract entities from support ticket
    text |'
  prefs: []
  type: TYPE_TB
- en: '| Azure Form Recognizer | Object + HTTP and JSON VBOs | Single subscription
    key | Single | Async | Extract data from digital invoices |'
  prefs: []
  type: TYPE_TB
- en: '| GCP Cloud Vision | Custom-built Web API Service | Temporary access token
    (OAuth2) + Client ID + Client Secret | Batch | Sync | Extract text from images
    |'
  prefs: []
  type: TYPE_TB
- en: Table 1.5 – A summary of the three examples we will go through
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – AWS Comprehend for text entity extraction, key phrase extraction,
    and sentiment analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS Comprehend is an ML service that extracts information and understands data
    in text. In this example, we’ll use Comprehend to triage cases for email support.
    Unlike submitting a support request through a web form or a chatbot where you
    can ask the user to categorize the issue for you, triaging issues sent directly
    through email has to be done either manually, through rules, or by ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s possible to build a custom model to directly classify text into your desired
    categories in Comprehend, but that’s outside the scope of this example. Instead,
    we’ll be using AWS’s pre-built models to extract the entities, key phrases, and
    sentiment of a support request. Even without a custom model, predictions from
    these pre-built models are still useful to the customer support agent. The support
    request used in this example is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: I am writing to request assistance with my iPhone 14, which has a cracked screen.
    My customer ID is abcd@email.com. I would greatly appreciate your help in resolving
    this issue. I understand that the device may be covered by a warranty or insurance
    plan, and I would like to explore all available options for repairing or replacing
    the phone. If possible, could you please provide me with information on the next
    steps I should take to initiate a repair or replacement request? Additionally,
    please let me know if there are any costs associated with this process.
  prefs: []
  type: TYPE_NORMAL
- en: Comprehend currently has a free usage tier of 5 million characters per API per
    month. The actions provided by the DX asset work in *single* and *synchronous*
    modes.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS web APIs use a proprietary algorithm to implement their refresh token authentication.
    Once you’ve imported the DX asset, you’ll notice that there’s both a Web API Service
    and an Object. The Object’s main purpose is to act as a wrapper around the web
    API that implements the custom authentication. In this example, we will be performing
    four high-level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the asset from the DX
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Importing the asset into BP
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuring a BP Credential
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing the DX API asset by making an ML prediction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Downloading from the DX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we’ll visit the DX ([https://digitalexchange.blueprism.com/dx/search](https://digitalexchange.blueprism.com/dx/search))
    to search for and download the AWS Comprehend asset:'
  prefs: []
  type: TYPE_NORMAL
- en: Visit the DX and type `comprehend capability aws` (capitalization doesn’t matter)
    into the search bar. Search for and click on the resulting **Comprehend Capability
    AWS** **Cloud** asset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the green download button on the right side of the screen and save
    the `.bprelease` asset to your computer. If you haven’t logged in already, the
    DX website will ask you to log in. This `.bprelease` contains one web API, one
    Object, and one Credential.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down further on the **Connector for Comprehend Capability AWS Cloud**
    page and download the **AWS Comprehend** **User Guide**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Downloading the asset and the AWS Comprehend User Guide](img/B18416_01_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Downloading the asset and the AWS Comprehend User Guide
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The AWS Comprehend User Guide provides links and details on how to set up your
    AWS account to allow for API calls from BP. Please follow the steps outlined in
    the user guide to properly set up your AWS account and authentication details.
    We won’t be going through the steps to set up your AWS account here.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the Comprehend asset into BP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s import the downloaded asset into BP and check what was imported:'
  prefs: []
  type: TYPE_NORMAL
- en: Open BP and log in. Click on **File** | **Import** | **Release / Skill**. Import
    the AWS Comprehend Release. Accept all of the defaults when importing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the `.bprelease` was successfully imported into BP. There should
    be one Object, one Web API Service, and one Credential. The Object can be found
    in the **Studio** section of BP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Finding the Object in the Studio section](img/B18416_01_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Finding the Object in the Studio section
  prefs: []
  type: TYPE_NORMAL
- en: Find the Comprehend web API under **System** | **Objects** | **Web** **API Services**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Finding AWS: Comprehend under System | Objects | Web API Services](img/B18416_01_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9 – Finding AWS: Comprehend under System | Objects | Web API Services'
  prefs: []
  type: TYPE_NORMAL
- en: Find the Credential under **System** | **Security** | **Credentials**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.10 – Finding the AWS Credential under System | Security | Credentials](img/B18416_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – Finding the AWS Credential under System | Security | Credentials
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the AWS credential
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we’ll import the `AWS` credentials (look at the downloaded user
    guide if you haven’t created these yet) into BP. Then, we’ll set the permissions
    on the Credential so that we can test it in a Process:'
  prefs: []
  type: TYPE_NORMAL
- en: Copy your AWS access key ID into the `AWS` credential. Copy your secret access
    key into the two **Password** fields.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Entering the AWS access key ID the secret access key](img/B18416_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – Entering the AWS access key ID the secret access key
  prefs: []
  type: TYPE_NORMAL
- en: On the **Access Rights** tab of the Credential, set **Security Roles** to **All
    Roles**, **Processes (legacy)** to **All Process** and **Resources (legacy)**
    to **All Resources**. While this is not best practice, we’re only doing this for
    testing purposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the AWS Comprehend object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, let’s test out the DX asset and make a prediction! Notice in the test
    Process that the Actions use the *Object* and not the Web API Service. Using the
    Web API Service directly wouldn’t work since it doesn’t contain the authentication
    parts. Take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open `Example 1 - Test AWS Comprehend` Process in the `Ch1` Group in the Process
    Studio. This Process also requires the `Utility – General` VBO to be imported.
    This can be found in the `VBO` sub-folder where BP is installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the Process in the Process Studio. This Process makes three API calls:
    1) detect entities, 2) find key phrases, and 3) detect sentiment. If successful,
    the three Collections on the `Main Page` in the **Predictions** Block are populated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.12 – The three predicted Collections from the three API calls](img/B18416_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – The three predicted Collections from the three API calls
  prefs: []
  type: TYPE_NORMAL
- en: 'Viewing the `Entities Response` Collection shows that AWS’s generic model can
    extract the email and phone model number:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – The phone model and customer ID are extracted as entities](img/B18416_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – The phone model and customer ID are extracted as entities
  prefs: []
  type: TYPE_NORMAL
- en: Even without a custom model to classify emails to the exact label, Comprehend’s
    pre-trained model is able to extract high-level information that can simplify
    the work of a customer support agent. We also see that the predictions come with
    a **Score**. This confidence score will be used as a design element for IA solutions
    in *Part 2* of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we used AWS Comprehend to extract data from unstructured text.
    Since AWS uses a custom authentication scheme, the DX asset comes packaged with
    both a Web API Service and an Object. Normally, dealing with authentication is
    complex, but the object handles everything for us. After importing the asset,
    we only need to set up a Credential to get things working. That’s how easy it
    is to connect BP with AWS’ Comprehend APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to our next example, which uses Azure.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – Azure Form Recognizer for invoice extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure’s Form Recognizer contains pre-built models that allow you to extract
    data from receipts, invoices, tax forms, business cards, and generic documents.
    Extracting data from invoices and receipts in usable formats *is a very common
    IA use case*. It’s so common that BP and most other RPA vendors have products
    specifically targeting invoice extraction. More information about BP’s document
    extraction product, Decipher, can be found in the last chapter of this book. A
    free pricing tier of Form Recognizer is available for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The invoice used in this example can be downloaded here for reference: [https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/ex2_invoice.pdf](https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/ex2_invoice.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This PDF is directly embedded into this example’s process as a binary Data Item,
    so downloading this is optional.
  prefs: []
  type: TYPE_NORMAL
- en: You can also build custom models inside of Form Recognizer to recognize your
    specific type of document, e.g., a bank or credit card statement. These custom
    models can unlock even more use cases for your business and be invoked by BP using
    the DX asset used in this example.
  prefs: []
  type: TYPE_NORMAL
- en: There are two major differences between this Azure API and AWS Comprehend beyond
    the APIs purpose. The first difference is the *authentication* scheme used. For
    Azure, we only need to pass in a subscription key together with every request.
    This is simpler than AWS, which requires a separate object to refresh the temporary
    access token. Unlike AWS Comprehend, the Form Recognizer calls are *asynchronous*,
    requiring you to repeatedly check whether the prediction is complete.
  prefs: []
  type: TYPE_NORMAL
- en: The DX asset for Form Recognizer is stored as a `.bprelease` file and contains
    one BP Object. This is different from most other web APIs available on the DX,
    which come packaged as a `.bpskill` file and appear in the **System** | **Objects**
    | **Web API Services** section of BP.
  prefs: []
  type: TYPE_NORMAL
- en: Form Recognizer was chosen as one of the examples because form processing is
    also a primary IA use case. The DX asset for Form Recognizer also shows a different
    way of implementing a web API connector that you can consider—implementing it
    directly as an object using the HTTP and JSON VBO instead of a Web API Service.
    Implementing as an Object allows companies running older versions of BP (< V6.4)
    to call web APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will be performing four high-level steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the asset from GitHub (or the DX)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Importing the asset into BP
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuring the Form Recognizer object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing the API call by making a prediction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Downloading from the DX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft has changed the URL endpoints since the Form Recognizer asset on the
    DX was released. As a result, the asset that’s on the DX won’t work without changing
    the URLs in it. An already modified release is provided at [https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/Ex_2_Azure_Form_Recognizer_Client_Service.bprelease](https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/Ex_2_Azure_Form_Recognizer_Client_Service.bprelease).
    I recommend downloading this GitHub version and not the version on the DX.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we won’t be downloading the asset from the DX, we still need to download
    the documentation so that the Azure API can be configured. Here, we will visit
    the DX ([https://digitalexchange.blueprism.com/dx/search](https://digitalexchange.blueprism.com/dx/search))
    to download the documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Type in `form recognizer client` (capitalization doesn’t matter) into the DX
    search bar and search. Click on the **Form Recognizer** **Client** asset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down further on the **Connector for Form Recognizer Client – 1.0.0**
    page and download the **User Guide**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Download the asset and the user guide](img/B18416_01_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – Download the asset and the user guide
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'The user guide provides details on how to set up your Azure account to allow
    for API calls from BP. Please follow the steps in the guide to properly set up
    Azure. The guide also lists two other VBOs that must be imported into BP: the
    `Utility - JSON` and `Utility – HTTP` VBOs. These are both included in the previous
    GitHub link. If you wish to download them separately, they are also available
    from the DX.'
  prefs: []
  type: TYPE_NORMAL
- en: Importing into BP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After downloading the asset, it must be imported into BP. This step is the
    same regardless of whether you’ve downloaded the asset from the DX or from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: Open BP and log in. Click **File** | **Import** | **Release / Skill** and import
    the asset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the `Form Recognizer Client Service` Object is present. Also verify
    that the correct versions of the HTTP and JSON VBOs as present, as required by
    the user guide.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.15 – Verifying that the three Objects are present](img/B18416_01_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – Verifying that the three Objects are present
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Form Recognizer Client Service object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The author of the DX asset has opted to store the authentication IDs as Data
    Items in the Object. This is *not a best practice*, as these should be stored
    as a Credential instead. But since this is how the Object was designed, let’s
    edit those Data Items to configure the authentication details needed to connect
    to the web API. If you don’t have a subscription key, please follow the asset
    documentation and create one.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The access token and base URL needed to configure the Object can be obtained
    from the **Form Recognizer** page on Azure’s website. The user guide shown in
    *Figure 1**.15* has more details on how to find this page.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `Form Recognizer Client Service` Object in the Object Studio. On the
    `Access Token` Collection and a `Base URL` Data Item that needs to be populated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.16 – Populating Access Token and Base URL on the Initialise Page](img/B18416_01_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – Populating Access Token and Base URL on the Initialise Page
  prefs: []
  type: TYPE_NORMAL
- en: Open the `Access Token` Collection and click on the **Initial Values** tab.
    Fill in the **Ocp-Apim-Subscription-Key** field with your subscription key and
    save.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.17 – Filling the subscription key into the access token](img/B18416_01_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 – Filling the subscription key into the access token
  prefs: []
  type: TYPE_NORMAL
- en: Open the `Base URL` Data Item and set the **Initial Value** to the endpoint
    URL from Azure’s website. Do not include a trailing slash at the end of the URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.18 – Filling in the Base URL without a trailing slash](img/B18416_01_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 – Filling in the Base URL without a trailing slash
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve configured the Azure Object, we can test it out.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Form Recognizer Client Service object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A test Process is included in the Release file imported in the *Technical requirements*
    section at the beginning of the chapter. This process requires `Utility – General`
    and `Utility – File Management` to be imported. Both can be found in the `VBO`
    sub-folder of the installation folder.
  prefs: []
  type: TYPE_NORMAL
- en: Open `Example 2 – Test Azure Form Recognizer Client Service` Process in the
    `Ch1` Group in the Process Studio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the Process in the Process Studio. This process makes at least two API calls.
    The first call sends the invoice file to request processing. This gives a **Result
    ID** in return. The second API call checks up to six times whether the *asynchronous*
    ML prediction is finished by referencing the **Result ID**. When the prediction
    is complete, the Collections and Data Item in the **Predictions** Block will become
    populated. You can verify that the extracted contents closely match the actual
    PDF document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.19 – Predictions from the Azure invoice Form Recognizer](img/B18416_01_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 – Predictions from the Azure invoice Form Recognizer
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ve seen two examples with two different types of web API authentication,
    as well as synchronous and asynchronous API calling cases. In the final example
    of this chapter, we will be creating a web API service from scratch for batch
    PDF processing in GCP.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3 – GCP Cloud Vision batch OCR processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cloud Vision from Google Cloud allows you to extract labels and text from images.
    In IA, this is most commonly used for the OCR of image-based text, such as photographs
    of documents and receipts. If documents are scanned, GCP recommends using its
    Document AI service instead. In this example, we’ll be extracting text from an
    image-based PDF with a small amount of handwriting on it. We’ll also be showing
    off a synchronous batch case where five PDF pages of an image-based document are
    processed at once and the predicted results for those five pages are received
    in the same request.
  prefs: []
  type: TYPE_NORMAL
- en: The open source PDF document used in this example can be found at [https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/ex3_pdf.pdf](https://github.com/PacktPublishing/Intelligent-Automation-with-Blue-Prism/blob/main/ch1/ex3_pdf.pdf).
    This PDF is already embedded into the test Process as a Binary Data Item, so downloading
    it is optional.
  prefs: []
  type: TYPE_NORMAL
- en: GCP uses OAuth2 for authentication, which is also a temporary access token method,
    similar to what AWS Comprehend does in the first example. The difference between
    OAuth2 and AWS’s methods is that AWS’ authentication is proprietary. OAuth2 is
    an open standard adopted by many vendors, and BP’s Web API Services feature can
    handle it directly. No Object wrapper will be needed, unlike in the AWS Comprehend
    case.
  prefs: []
  type: TYPE_NORMAL
- en: In this third example, we’ll be building a web API from scratch. There’s no
    DX asset for the API endpoint that we’ll be using. A key skill needed to build
    a web API from scratch is the ability to carefully read and understand the vendor’s
    API documentation. The relevant parts of GCP’s API documentation will be shown
    alongside the BP Web API Service configuration for reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example has very different steps compared to the two previous examples.
    At a high level, we will be performing four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Visiting the GCP website to set up the keys needed for API authentication
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Saving the API authentication keys into a Credential
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a Web API Service in BP
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing the web API by making a prediction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up a service account and key
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section contains steps that are performed on GCP’s website. The purpose
    is to generate authentication credentials that can be downloaded and used inside
    of a BP Web API Service. If these steps become outdated, please use the link in
    *step 1* as a reference instead:'
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://cloud.google.com/iam/docs/service-accounts-create#creating_a_service_account](https://cloud.google.com/iam/docs/service-accounts-create#creating_a_service_account)
    and follow the instructions to create your service account. Ensure that your service
    account has permission to access the *Vision API*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a key for your service account. Under the **Service Accounts** section
    of **IAM & Admin**, click on the **KEYS** tab and then **ADD KEY**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.20 – Creating a key for your service account](img/B18416_01_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.20 – Creating a key for your service account
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose **JSON** as the **Key type** and press **CREATE**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.21 – Creating a JSON key](img/B18416_01_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.21 – Creating a JSON key
  prefs: []
  type: TYPE_NORMAL
- en: Download the JSON file that appears after pressing **CREATE**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Saving the service account email and private key as a Credential
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ve created authentication credentials and downloaded them in JSON format.
    Now we need to save the relevant information into a BP credential:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the JSON file in any text editor. You’ll see a `private_key` row. Copy
    everything between the two double quotation marks into your clipboard. This is
    referred to as the **Client Secret** in GCP documentation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Visit **System** | **Security** | **Credentials** in BP. Click **New** to create
    a new Credential.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the `GCP Cloud Vision`. Set the **Type** to **OAuth 2.0 (JWT Bearer Token)**.
    Set **Issuer** to the service account email address, and for **Private Key**,
    paste what was copied in *step 1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.22 – Saving the credential with information from the GCP Portal](img/B18416_01_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.22 – Saving the credential with information from the GCP Portal
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **Access Rights** tab of the Credential. Set **Security Roles**
    to **All Roles**, **Processes (legacy)** to **All Process**, and **Resources (legacy)**
    to **All Resources**. While this isn’t best practice, we’re only testing and not
    deploying something to production. Save the credential.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a Web API Service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we will be creating a Web API Service from scratch. Google’s API documentation
    ([https://cloud.google.com/vision/docs/file-small-batch](https://cloud.google.com/vision/docs/file-small-batch))
    will be consulted frequently during this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: One key tip to consider while building a web API from scratch is to see whether
    the vendor has a *different* service already available on the DX. If so, download
    it, import it, and use it as a reference. Each vendor tries to keep its APIs internally
    consistent. Within a vendor, their APIs are likely using the same authentication
    methods and have similar JSON structures for their inputs and outputs. You’ll
    likely be able to copy parts of a working Web API Service into your own!
  prefs: []
  type: TYPE_NORMAL
- en: Visit **System** | **Objects** | **Web API Services**. Click on **Add Service**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set `Google Cloud Vision Batch Annotation Online`. This is an arbitrary name
    that will show up as a selectable option in an Action Stage. Set **Base URL**
    to [https://vision.googleapis.com/](https://vision.googleapis.com/), including
    the trailing slash. At the end of this step, your web API should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.23 – Setting the Name and Base URL](img/B18416_01_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.23 – Setting the Name and Base URL
  prefs: []
  type: TYPE_NORMAL
- en: 'The base URL is taken from GCP’s documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.24 – The base URL from GCP’s documentation](img/B18416_01_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.24 – The base URL from GCP’s documentation
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on `Content-Type` and `application/json; charset=utf-8`. The **Common
    Headers** section should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.25 – Setting the common headers](img/B18416_01_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.25 – Setting the common headers
  prefs: []
  type: TYPE_NORMAL
- en: 'These common headers are taken from the sample API request shown in GCP’s documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.26 – The headers from GCP’s documentation (the other headers can
    be ignored)](img/B18416_01_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.26 – The headers from GCP’s documentation (the other headers can be
    ignored)
  prefs: []
  type: TYPE_NORMAL
- en: Click on `GCP Cloud Vision`, which was created in *step 2* in the *Saving the
    service account email and private key as a credential* section. Untick the **Expose
    to** **Process** box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The token endpoint data is taken from [https://accounts.google.com/.well-known/openid-configuration](https://accounts.google.com/.well-known/openid-configuration)
    and the scope endpoint is taken from [https://developers.google.com/oauthplayground](https://developers.google.com/oauthplayground).
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Common Authentication** section should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.27 – Setting the common authentication](img/B18416_01_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.27 – Setting the common authentication
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on `Annotate Batch`. This is the name of the Action that will appear
    as selectable in BP. Each Action is equivalent to one web API endpoint. At the
    end of this step, your **Action** section should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.28 – Creating an action](img/B18416_01_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.28 – Creating an action
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on `mimeType`, `pages,` and `content`. Leave **Initial Value** blank
    and tick the **Expose** box for all three. At the end of this step, your **Parameters**
    section should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.29 – Setting the action parameters](img/B18416_01_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.29 – Setting the action parameters
  prefs: []
  type: TYPE_NORMAL
- en: These three parameters were taken from GCP’s examples of what they expect to
    receive in the request JSON body. We’re exposing these as the input parameters
    of our web API so that they can be passed in and changed according to the needs
    of the Process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.30 – The parameters from GCP’s documentation](img/B18416_01_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.30 – The parameters from GCP’s documentation
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on `/v1/files:annotate`, and **Body Content** to **Template**. Inside
    the **Template** text box, put in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we’ve inserted our parameters into the template using `[parameter
    name]`, similar to BP data items. Some areas of the `[[`) .This is how we can
    escape the square opening bracket character. We’ve also *hardcoded* the type of
    extraction to be `DOCUMENT_TEXT_DETECTION` and `LABEL_DETECTION`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'What this section does is set up the exact JSON structure that will be sent
    to the API endpoint. We want this structure to match up with *Figure 1**.30* exactly.
    At the end of this step, the **Request** section should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.31 – Set the request](img/B18416_01_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.31 – Set the request
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on `responses`, `$.responses`. This part defines what the web API sends
    back to the Action as Output Parameters. In our case, we’re just going to save
    the entire JSON response as a Collection. At the end of this step, your **Response**
    configuration should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.32 – Setting the response](img/B18416_01_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.32 – Setting the response
  prefs: []
  type: TYPE_NORMAL
- en: 'Our single output `responses` Collection is equivalent to the highlighted item
    in the sample response output of GCP’s documentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.33 – The response from GCP’s documentation](img/B18416_01_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.33 – The response from GCP’s documentation
  prefs: []
  type: TYPE_NORMAL
- en: Save the Web API Service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we’ve completed the setup of a new web API. Let’s test it out with a Process.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the batch image Web API Service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The sample Process should have been imported from the *Technical requirements*
    section. We also need to import an additional VBO to convert our PDF file into
    Base64 format, which is a requirement of the GCP Vision API:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and import the `Utility – File Manipulation` asset from the DX at [https://digitalexchange.blueprism.com/dx/entry/9648/solution/utility---file-manipulation](https://digitalexchange.blueprism.com/dx/entry/9648/solution/utility---file-manipulation).
    This is used to convert the PDF file into the Base64 format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open `Example 3 – Test GCP Batch PDF` Process in the `Ch1` Group in the Process
    Studio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the Process in the Process Studio. This Process makes one API call, which
    processes five PDF pages in a batch. The call is synchronous, as the predictions
    for the five pages are returned in the same call as the request. Once this is
    successful, the Collections and Data Item in the **Predictions** Block will become
    populated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.34 – Predictions from the newly created web API](img/B18416_01_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.34 – Predictions from the newly created web API
  prefs: []
  type: TYPE_NORMAL
- en: In this third example, we looked at a batch/synchronous processing case for
    extracting text from documents. We also built a Web API Service that uses OAuth2
    authentication from scratch. This was an exercise in inputting existing API information
    into the right sections of BP’s Web API Service. In the sample Process, we also
    used Base64 to encode the PDF file so that it could be sent as plain text in the
    API request body.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the DX and how web API connectors can be downloaded
    to greatly speed up our development time. When using a web API, the areas that
    people frequently get stuck on include authentication and correctly mapping the
    input and output JSON data according to the vendor’s API documentation.
  prefs: []
  type: TYPE_NORMAL
- en: ML web APIs also differ in two areas that affects IA solution design. These
    are single versus batch and synchronous versus asynchronous predictions. We’ll
    revisit these characteristics later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we looked at four major MLaaS vendors: AWS, Azure, GCP, and IBM. We explored
    what ML web APIs they have available on the DX. Each vendor uses marketing names
    for its services, obscuring what those services actually do. I’ve grouped and
    summarized what the services behind the marketing names actually do, provided
    search terms you can use to find them on the DX, and given you ideas on what use
    cases they can fulfill.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we went through three hands-on examples. Different vendors, API implementation
    methods, authentication methods, and use cases were selected to cover most of
    the scenarios you’ll encounter in real life. The vast majority of real-life IA
    examples I’ve found during my research have to do with text processing. Our examples
    showed three different IA text cases: extracting data from *unstructured text*,
    extracting data from *forms*, and extracting text from *images*.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move away from calling ML predictions from web
    APIs and look more at how BP can call models directly from the Digital Worker’s
    command line interface.
  prefs: []
  type: TYPE_NORMAL
