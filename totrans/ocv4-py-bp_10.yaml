- en: Learning to Detect and Track Objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you got your hands on deep convolutional neural networks
    and built deep classification and localization networks using transfer learning.
    You have started your deep learning journey and have familiarized yourself with
    a range of deep learning concepts. You now understand how deep models are trained
    and you are ready to learn about more advanced deep learning concepts.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will continue your deep learning journey, first using object
    detection models to detect multiple objects of different types in a video of a relevant
    scene such as a street view with cars and people. After that, you will learn how
    such models are built and trained.
  prefs: []
  type: TYPE_NORMAL
- en: In general, robust object detection models have a wide range of applications
    nowadays. Those areas include but are not limited to medicine, robotics, surveillance,
    and many others. Understanding how they work will allow you to use them for building
    your own real-life applications, as well as elaborating on new models on top of
    them.
  prefs: []
  type: TYPE_NORMAL
- en: After we cover object detection, we will implement the** Simple Online and Realtime
    Tracking** (**Sort**)algorithm, which is able to robustly track detected objects
    throughout frames. During the implementation of the Sort algorithm, you will also
    get acquainted with the **Kalman filter**, which in general is an important algorithm
    when working with time series.
  prefs: []
  type: TYPE_NORMAL
- en: A combination of a good detector and tracker finds multiple applications in
    industrial problems. In this chapter, we'll limit the applications by counting
    the total objects by their type as they appear throughout the video of the relevant
    scene. Once you understand how this specific task is achieved, you will probably have
    your own usage ideas that will end up in your own applications.
  prefs: []
  type: TYPE_NORMAL
- en: For example, having a good object tracker allows you to answer statistical questions
    such as which part of the scene appears more condensed? And, where do objects
    move more slowly or quickly during the observation time? In some scenarios, you
    might be interested in monitoring the trajectories of specific objects, estimating
    their speed or the time that they spend in different areas of the scene. Having
    a good tracker is the solution for all of these things.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the main script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting objects with SSD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding object detectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking detected objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a Sort tracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Kalman filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing the app in action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start the chapter by pointing out the technical requirements and planning
    the app.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in all of the chapters of the book, you need an appropriate installation
    of **OpenCV**, **SciPy**, and **NumPY**.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code that we present in this chapter at the GitHub repository
    at [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter10](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter10).
  prefs: []
  type: TYPE_NORMAL
- en: When running the app with Docker, the Docker container should have appropriate
    access to the **X11 server**. This app cannot run in **headless mode**. The best
    environment to run the app with Docker is a **Linux** desktop environment. On
    **macOS**, you can use **xQuartz **(refer, to [https://www.xquartz.org/](https://www.xquartz.org/))
    in order to create an accessible X11 server.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use one of the available Docker files in the repository in order
    to run the app.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously, the final app will be able to detect, track, and count
    objects in a scene. This will require the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`main.py`: This is the main script for detecting, tracking, and counting objects
    in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sort.py`: This is the module that implements the tracking algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will first prepare the main script. During the preparation, you will learn
    how to use detection networks, as well as how they work and how they are trained.
    In the same script, we will use the tracker to track and count objects.
  prefs: []
  type: TYPE_NORMAL
- en: After preparing the main script, we will prepare the tracking algorithm and
    will be able to run the app. Let's now start with the preparation of the main
    script.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the main script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main script will be responsible for the complete logic of the app. It will
    process a video stream and use an object-detection deep convolutional neural network
    combined with the tracking algorithm that we will prepare later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is used to track objects from frame to frame. It will also be
    responsible for illustrating results. The script will accept arguments and have
    some intrinsic constants, which are defined in the following initialization steps
    of the script:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with any other script, we start by importing all the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will use `argparse` as we want our script to accept arguments. We store the
    object classes in a separate file in order not to contaminate our script. Finally,
    we import our `Sort` tracker, which we will build later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create and parse arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Our first argument is the input, which can be a path to a video, the ID of a
    camera (`0` for the default camera), or a video stream **Universal Resource Identifier**
    (**URI**). For example, you will be able to connect the app to a remote IP camera
    using the **Real-time Transport Control Protocol** (**RTCP**).
  prefs: []
  type: TYPE_NORMAL
- en: The networks that we will use will predict the bounding boxes of objects. Each
    bounding box will have a score, which will specify how probable it is that the
    bounding box contains an object of a certain type.
  prefs: []
  type: TYPE_NORMAL
- en: The next parameter is `threshold`, which specifies the minimal value of the
    score. If the score is below `threshold`, then we will not consider the detection.
    The last parameter is `mode`, in which we want to run the script. If we run it
    in `detection` mode, the flow of the algorithm will stop after detecting objects and
    will not proceed further with tracking. The results of object detections will
    be illustrated in the frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV accepts the ID of a camera as an integer. If we specify the ID of a
    camera, the input argument will be a string instead of an integer. Hence, we need
    to convert it to an integer if required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the required constants:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this app, we will track cars and people. We will illustrate bounding boxes
    in a yellowish color and write text in white. We'll also define the standard input
    size of the **Single Shot Detector** (**SSD**) model that we are going to use
    for detection.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting objects with SSD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenCV has methods for importing models built with deep learning frameworks.
    We load the TensorFlow SSD model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter of the `readNetFromTensorflow` method accepts a path to
    a file that contains a TensorFlow model in binary **Protobuf** (**Protocol Buffers**)
    format. The second parameter is optional. It is a path to a text file that contains
    a graph definition of the model, again in Protobuf format.
  prefs: []
  type: TYPE_NORMAL
- en: Surely, the model file itself might contain the graph definition and OpenCV
    can read that definition from the model file. But, with many networks, it might
    be required to create a separate definition, as OpenCV cannot interpret all operations
    available in TensorFlow and those operations should be replaced with operations
    that OpenCV can interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now define functions that will be useful for illustrating detections.
    The first function is for illustrating a single bounding box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'From the previous code, the `illustrate_box` function accepts an image, a normalized
    bounding box as an array of four coordinates specifying two opposite corners of
    the box. It also accepts a caption for the box. Then, the following steps are
    covered in the function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It first extracts the size of the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It then extracts the two points, scales them by the size of the image, and
    converts them into integers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we draw the corresponding `rectangle` using the two points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we put the caption near the first point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The second function will illustrate all `detections`, given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code snippet, the second function accepts detections as a
    two-dimensional `numpy` array and a frame on which it illustrates the detections.
    Each detection consists of the class ID of the detected object, a score specifying
    the probability that the bounding box contains an object of the specified class,
    and the bounding box of the detection itself.
  prefs: []
  type: TYPE_NORMAL
- en: The function first extracts the previously stated values for all detections,
    then illustrates each bounding box of the detection using the `illustrate_box`
    methods. The class name and `score` are added as the caption for the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now connect to the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We pass the `input` argument to `VideoCapture`, which, as mentioned previously,
    can be a video file, stream, or camera ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have loaded the network, defined the required functions for illustration,
    and opened the video capture, we are ready to iterate over frames, detect objects,
    and illustrate the results. We use a `for` loop for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The body of the loop contains the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It sets the frame as the input of the `detector` network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`blobFromImage` creates a four-dimensional input for the network from the provided
    image. It also resizes the image to the input size and swaps the red and blue
    channels of the image as the network is trained on RGB images, whereas OpenCV
    reads frames in BGR.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then it makes a prediction with the network and gets the output in the desired
    format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: From the previous code, `forward` stands for forward propagation. The result
    is a two-dimensional `numpy` array. The first index of the array specifies the
    detection number, and the second index represents a specific detection, which
    is expressed by the object class, score, and four values specifying two corner
    coordinates of the bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, it extracts `scores` from `detections`, and filters out the ones
    that have a very low score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the cases when the script is running in `detection` mode, illustrate `detections`
    right away:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we have to set termination criteria:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have everything ready to run our script in detection mode. A sample
    result is shown in the image that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5b10d54-39ae-4182-bbc9-b48ba3457076.png)'
  prefs: []
  type: TYPE_IMG
- en: You can note in the frame from the preceding image that the SSD model has successfully
    detected all the cars and the single individual (person) visible in the scene.
    Let's now look at how we can use other object detectors.
  prefs: []
  type: TYPE_NORMAL
- en: Using other detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are using an object detector to get bounding boxes with
    their object types, which will be further processed by the Sort algorithm for
    tracking. In general, it does not matter by what exact means the boxes are obtained.
    In our case, we have used an SSD pre-trained model. Let's now understand how to
    replace it with a different model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first understand how we can use YOLO for this purpose. YOLO is also a
    single-stage detector and stands for **You Only Look Once** (**YOLO**). The original
    YOLO models are based on **Darknet**, which is another open-source neural network
    framework and is written in C++ and CUDA. OpenCV has the ability to load networks
    based on Darknet, similarly to how it loads TensorFlow models.
  prefs: []
  type: TYPE_NORMAL
- en: In order to load a YOLO model, you should first download the files containing
    the network configuration and the network weights.
  prefs: []
  type: TYPE_NORMAL
- en: The latter can be done by visiting [https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/).
    In our case, as an example, we will use **YOLOv3-tiny**, which is the most lightweight
    one at the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have downloaded the network configuration and weights, you can load
    them similarly to how you loaded the SSD model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The difference is that the `readNetFromDarknet` function is used instead of `readNetFromTensorflow`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use this detector instead of the SSD, we have several things to
    do:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to change the size of the input:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The network is originally trained in with the specified size. If you have a
    high-resolution input video stream and you want the network to detect small objects
    in the scene, you can set the input to a different size, which is a multiplier
    of 160, for example, size (640, 480). The larger the input size, the more small
    objects will be detected, but the network will make predictions slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to change class names:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Although the YOLO network is trained on the **COCO** dataset, the IDs of the
    objects are different. You can still run with the previous class names, but you
    will have the wrong names of the classes in that case.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the file from the darknet repository [https://github.com/pjreddie/darknet](https://github.com/pjreddie/darknet).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to slightly change the input:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In comparison with the input for SSD, we add `scalefactor`, which normalizes
    the input.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to successfully make predictions. Although, we are not completely
    ready to display the results with this detector. The problem is that the predictions
    of the YOLO model have a different format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each detection consists of the coordinates of the center of the bounding box:
    the width, the height of the bounding box, and a one-hot vector representing the
    probabilities of each type of object in the bounding box. In order to finalize
    the integration, we have to bring the detections in the format that we use in
    the app. The latter can be accomplished with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We extract the center coordinates of the bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We then also extract the width and height of the bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we extract `scores_one_hot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we find the `class_ids` of the maximum scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we extract the maximum scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we construct `detections` in the format consumed by the rest of the app
    using the results obtained in the previous steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now we can successfully run the app with the new detector. Depending on your
    needs, the available resources, and the required accuracy, you might want to use
    other detection models, such as other versions of SSD or **Mask-RCNN**, which
    is one of the most accurate object detection networks at the time of writing,
    although it is much slower than the SSD models.
  prefs: []
  type: TYPE_NORMAL
- en: You can try to load your model of choice with OpenCV, as we have done for both
    YOLO and SSD in this chapter. With this approach, you might encounter difficulties
    loading the model. For example, you might have to adapt the network configuration
    such that all the operations in the network can be processed by OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: The latter is particularly due to the fact that modern deep learning frameworks
    develop quite fast and OpenCV at least needs time to catch up to include all new
    operations. Another approach that you might prefer is to run a model using the
    original framework, similarly to what we did in [Chapter 9](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml),
    *Learning to Classify and Localize Objects*.
  prefs: []
  type: TYPE_NORMAL
- en: So now that we understand how to use detectors, let's look at how they work
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding object detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 9](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml),* Learning to Classify
    and Localize Objects*, we learned how to use the feature maps of a certain layer
    of a convolutional neural network to predict the bounding box of an object in
    the scene, which in our case was a head.
  prefs: []
  type: TYPE_NORMAL
- en: You might note that the difference between the localization network that we
    composed and the detection networks (that we used in this chapter) is that the
    detection networks predict multiple bounding boxes instead of a single one, as
    well as assigning a class to each of the bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now make a smooth transition between the two architectures so that you
    can understand how object detection networks like YOLO and SSD work.
  prefs: []
  type: TYPE_NORMAL
- en: The single-object detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First of all, let's look at how to predict the class in parallel with the box.
    In [Chapter 9](8baf5d4c-f1e9-4b76-b957-e19682cb9e68.xhtml),* Learning to Classify
    and Localize Objects*, you also learned how to make a classifier. Nothing limits
    us to combining classification with localization in a single network. That is
    done by connecting the classification and localization blocks to the same feature
    map of the base network and training it all together with a loss function, which
    is a sum of localization and classification losses. You can create and train such
    a network as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: The question remains, *what if there is no object in the scene?* To resolve
    this, we can simply add one more class that corresponds to the background and
    assign zero to the loss of the bounding box predictor when training. As a result,
    you will have a detector that detects multiple classes of objects but can only
    detect one object in the scene. Let's now look at how we can predict multiple
    boxes instead of one, and hence, arrive at a complete architecture of an object
    detector.
  prefs: []
  type: TYPE_NORMAL
- en: The sliding-window approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the earliest approaches to create an architecture that can detect multiple
    objects in the scene was the **sliding-window** approach. With this approach,
    you first build a classifier for objects of interest. Then, you pick a rectangle
    (a window) of a size that is several or many times smaller than the image where
    you want to detect an object. After that, you slide it across all possible locations
    in the image and classify whether there is an object of the chosen type in each
    position of the rectangle.
  prefs: []
  type: TYPE_NORMAL
- en: During sliding, a sliding size of between a fraction of the box size and the
    complete box size is used. The procedure is repeated with different sizes of the
    sliding window. Finally, you pick the window positions that have a class score
    above some threshold and you report that those window positions with their sizes
    are the bounding boxes of the chosen object classes.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this approach is, first of all, that a lot of classifications
    should be done on a single image, and hence the architecture of the detector will
    be quite heavy. Another problem is that the objects are localized only with the
    precision of the sliding size. Also, the sizes of the detection bounding boxes
    have to be equal to the sizes of the sliding windows. Surely, the detection could
    be improved if the slide size was reduced and the number of window sizes was increased,
    but this would result in an even greater computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: One of the ideas you already might have come up with is to combine the single-object
    detector with the sliding-window approach and take advantage of both.  For example,
    you could split the image into regions. For example, we could take a 5 x 5 grid
    and run the single-object detector in each cell of the grid.
  prefs: []
  type: TYPE_NORMAL
- en: You could go even further by creating more grids with a larger or smaller size,
    or by making the grid cells overlap. As a mini-project to get a deep understanding
    of the ideas covered, you might like to implement them and play with the results.
    Still, with these approaches, we make the architectures heavier, that is, once
    we enlarge the grid size or the number of grids in order to improve the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Single-pass detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previously stated ideas, we have used single-object classification or
    detection networks to achieve multiple-object detection. In all scenarios, for
    each predefined region, we feed the network with the complete image or part of
    it multiple times. In other words, we have multiple passes that result in heavy
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '*Wouldn''t it be nice to have a network that, once fed with an image, detects
    all the objects in the scene in a single pass?* An idea that you could try is
    to make more outputs for our single-object detector so that it predicts multiple
    boxes instead of one. This is a good idea, but there is a problem. Suppose we
    have multiple dogs in the scene that could appear in different locations and in
    different numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: '*How should we make an invariant correspondence between the dogs and the outputs?*
    If we make an attempt to train such a network by assigning boxes to the outputs,
    for example, from left to right, we will simply end up with predictions that are
    close to the average value of all positions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Networks such as SSD and YOLO tackle the issues and implement multiscale and
    multibox detection in a single pass. We can sum up their architecture with the
    following three components:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, they have a position-aware multibox detector connected to a feature
    map. We have discussed the training problem that arises when connecting several
    box predictors to the complete feature map. The problem with SSD and YOLO is solved
    by having a predictor that is connected to a small region of the feature map instead
    of a complete feature map.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It predicts boxes in the region of the image that corresponds to just that exact
    region of the feature map. Then, the same predictor predicts across all possible
    locations of the feature map. This operation is implemented using convolutional
    layers. There are convolutional kernels, with their activations, that slide across
    the feature map and have coordinates and classes as their output feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can obtain a similar operation if you go back to the code of
    the localization model and replace the last two layers, which flatten the output
    and create four fully connected neurons for predicting box coordinates with a
    convolutional layer with four kernels. Also, since the predictors act in a certain
    region and are aware only about that region, they predict coordinates that are
    relative to that region, instead of predicting coordinates that are relative to
    the complete image.
  prefs: []
  type: TYPE_NORMAL
- en: Both YOLO and SSD predict several boxes in each location instead of a single
    one. They predict offset coordinates from several **default boxes**, which are
    also called **anchor boxes**. These boxes are chosen sizes and shapes that are
    close to the objects in the dataset or the natural scene, so that relative coordinates
    have small values and even the default boxes match the object bounding boxes pretty
    well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, a car usually appears as a wide box and a person usually appears
    as a tall box. Multiple boxes allow you to achieve better accuracy as well as
    to have multiple predictions in the same area. For example, if a person is sitting
    on a bike somewhere in the image and we have a single box, then we would omit
    one of the objects. With multiple anchor boxes, the objects will correspond to
    different anchor boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Besides having multisize anchor boxes, they use several feature maps with different
    sizes to accomplish multiscale prediction. If the prediction module is connected
    to the top feature maps of the network with a small size, it is responsible for
    large objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is connected to one of the bottom feature maps, it is responsible for
    small objects. Once all the multibox predictions in the chosen feature maps are
    made, the results are translated to the absolute coordinates of the image and
    concatenated. As a result, we obtain the predictions in the form that we used
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in more implementation details, we advise you to read
    corresponding papers, as well as to analyze the corresponding implementation code.
  prefs: []
  type: TYPE_NORMAL
- en: So now that you understand how the detectors work, you are probably also interested
    in the principles of their training. However, before we understand those principles,
    let's understand the metric called **Intersection over Union**, which is heavily
    used when training and evaluating these networks as well as filtering their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: We will also implement a function to compute this metric, which we will use
    when building the Sort algorithm for tracking. Hence, you should note that understanding
    this metric is important not only for object detection but also for tracking.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about Intersection over Union
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Intersection over U****nion** (**IoU**), which is also called the **Jaccard
    index**, is defined as the size of the intersection divided by the size of the
    union and has the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e471002-bc57-445c-9aec-b4281d4824a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That formula is equivalent to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/889c18e0-6464-4bdf-9561-0af7113b054e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following diagram, we illustrate IoU for two boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b405602e-a935-4138-b019-46269ae54bc7.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous diagram, the union is the total area of the complete figure
    and the intersection is the part where the boxes overlap. The IoU can have a value
    in the range of (0,1) and reaches the maximal value only when the boxes match
    exactly. Once the boxes are separated, it becomes zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a function that accepts two bounding boxes and returns their
    `iou` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to calculate the `iou` value, the following steps are necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first extract the top-left and bottom-right coordinates of both bounding
    boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we get the element-wise `maximum` of the two top-left corners:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The two arrays are compared element-wise and the result will be a new array
    containing the larger values of the corresponding indexes in the array. In our
    case, maximum *x* and *y* coordinates are obtained and stored in `int_tl`. If
    the boxes intersect, this is the top-left corner of the intersection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we get the element-wise `minimum` of the bottom-right corners:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the previous case, this is the bottom-right corner of the intersection
    if the boxes intersect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we calculate areas of the bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The difference between the bottom-right and the top-left corner coordinates
    of a box is the width and height of the box, hence the product of the elements
    of the resulting array is the area of the bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we calculate the intersection area:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: If the boxes do not overlap, at least one element of the resulting array will
    be negative. Negative values are replaced with zeros. Hence, in such cases, the
    area is zero, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'And at last, we calculate IoU and `return` the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: So, now that you have understood what IoU is and have built a function to compute
    it, you are ready to learn how the detection networks used are trained.
  prefs: []
  type: TYPE_NORMAL
- en: Training SSD- and YOLO-like networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are already aware that networks such as YOLO and SSD predict objects with
    predefined anchor boxes. Out of all available boxes, only one box is chosen, which
    corresponds to the object. During prediction time, the box is assigned with the
    class of the object and the offsets are predicted.
  prefs: []
  type: TYPE_NORMAL
- en: '*So, the question is, how do we choose that single box?* You might already
    have guessed that IoU is used for that purpose. The correspondence between the
    ground truth boxes and anchor boxes can be made as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a matrix that contains all IoU values of all possible ground truth and
    anchor box pairs. Say, the row corresponds to the ground truth box and the column
    corresponds to anchor box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the maximal element in the matrix and assign the corresponding boxes to
    each other. Remove the row and column of the maximal element from the matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *step 2* until there are no ground truth boxes available, or in other
    words, until all the rows of the matrix are removed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the assignment is done, all that is left to do is to define a loss function
    for each box, sum the results as the total loss and train the network. The loss
    for the box offsets bounding boxes which contain objects can be simply defined
    as IoU—the greater the IoU, the closer the bounding box is to the ground truth,
    hence, it's negated value should be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: The anchor boxes that do not contain objects do not contribute to the loss.
    The loss of object classes is also straightforward—the anchor boxes that do not
    have assignments are trained with the background class and the ones that do have
    assignments are trained with their corresponding classes.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the considered networks has some modifications to the described loss
    so that it achieves better performance on the specific network. You can pick a
    network and define the loss described here on your own, which will be a good exercise
    for you. If you are building your own app and you need the corresponding trained
    network with relatively high accuracy in a limited amount of time, you might consider
    using the training methods that come with the code base of the corresponding network.
  prefs: []
  type: TYPE_NORMAL
- en: So, now that you have understood how to train these networks, let's continue
    the `main` script of the app and integrate it with the Sort tracker in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking detected objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we can successfully detect objects in each frame, we can track them by
    associating detections between frames. As mentioned previously, in this chapter,
    we are using the Sort algorithm for multiple-object tracking, which stands for
    **Simple Online and Realtime Tracking**.
  prefs: []
  type: TYPE_NORMAL
- en: Given sequences of multiple bounding boxes, this algorithm associates the boxes
    of sequence elements and fine-tunes the bounding box coordinates based on physical
    principles. One of the principles is that a physical object cannot rapidly change
    its speed or direction of movement. For example, under normal conditions, a moving
    car cannot reverse its movement direction between two consequent frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'We suppose that the detector annotates the objects correctly and we instantiate
    one **Multiple Object Trackers** (`mots`) for each class of objects that we want
    to track:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We store the instances in a  dictionary. The keys in the dictionary are set
    to the corresponding class IDs. We will track the detected objects using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The function accepts detections and an optional illustration frame. The main
    loop of the function iterates over the multi-object trackers that we have instantiated.
    Then, for each multi-object tracker, the following steps are covered:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first extract detections of the object type of the current multi-object
    tracker from all the passed detections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we update the tracker by passing the bounding boxes of the current object
    type to the `update` method of the tracker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `update` method returns the bounding box coordinates of the tracked objects
    associated with the IDs of the object.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the illustration frame is provided, illustrate the boxes in the frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: For each returned result, the corresponding bounding box will be drawn using
    our previously defined `illustrate_box` function. Each box will be annotated with
    the class name and the ID of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also want to define a function that will print general information about
    tracking on the frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: For each class of tracked objects, the function will write the total number
    of tracked objects and the number of currently tracked objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined the functions for tracking and illustration, we are
    ready to modify the main loop, which iterates over frames, so that we can run
    our app in tracking mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: From the previous snippet, if the app runs in tracking mode, the detected objects
    of the chosen classes will be tracked throughout frames using our `track` function
    and tracking information will be shown on the frame.
  prefs: []
  type: TYPE_NORMAL
- en: What's left to do is to elaborate on the tracking algorithm in order to finalize
    the complete app. We will do that in the next section with the help of the Sort
    tracker.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Sort tracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Sort algorithm is a simple yet robust real-time tracking algorithm for the
    multiple-object tracking of detected objects in video sequences. The algorithm
    has a mechanism to associate detections and trackers that results in a maximum
    of one detection box for each tracked object.
  prefs: []
  type: TYPE_NORMAL
- en: For each tracked object, the algorithm creates an instance of a single object-tracking
    class. Based on physical principles such as an object cannot rapidly change size
    or speed, the class instance can predict the feature location of the object and
    maintain tracking from frame to frame. The latter is achieved with the help of
    the **Kalman** filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the modules that we will use in the implementation of the algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As usual, the main dependencies are `numpy` and OpenCV. The unfamiliar `linear_sum_assignment` method
    will be used when associating detected objects with tracked ones.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now dive into the algorithm by first understanding what the Kalman Filter
    is, which is used in the implementation of a single box tracker in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Kalman filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kalman filter is a statistical model that has a wide range of applications
    in signal processing, control theory, and statistics. The Kalman filter is a complex
    model, but it could be thought of as an algorithm to **de-noise** the observations
    of an object that contain a lot of noise over time when we know the dynamics of
    the system with certain accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example, to illustrate how the Kalman filter works. Imagine
    we want to find the location of a train that moves on rails. The train will have
    a velocity, but unfortunately, the only measurements we have are from radar, which
    only shows the location of the train.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would like to accurately measure the location of the train. If we were to
    look at each radar measurement, we could learn the location of the train from
    it, but what if the radar is not very reliable and has high measurement noise.
    For example, the locations that radar reported are as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebcecf52-a143-4b3b-8b77-b4f520b2e9e7.png)'
  prefs: []
  type: TYPE_IMG
- en: '*What can we tell about the real location of the train at 3 p.m.?* Well, there
    is a possibility that the train was at position 5, but since we know that trains
    are heavy and change their speed very slowly, it would be very hard for the train
    to reverse its direction of travel twice in quick succession, to go to position
    5 and back. So, we can use some knowledge of how things work, and the previous
    observations, to make more reliable predictions about the location of the train.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we assumed that we could describe the train by its location
    and velocity, we would define the state to be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/750b462b-f85a-4033-950e-5dad31c8ac07.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* is the location of the train and *v* is the velocity of the train.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need a way to describe our model of the world, which is called the **state-transition
    model**—for a train, it is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ad60428-2633-4c3e-b254-98e617c14c06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We could write this in a matrix form using the state variable, *s*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3421676-135c-46cd-b1ad-eaa96ee347ba.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrix, *F*, is called the **state-transition matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: 'As such, we believe that the train doesn''t change its velocity and moves at
    a constant speed. This means that there should be a straight line on the graph
    of observations, but that''s too restrictive and we know that no real system behaves
    that way, so we allow for some noise being present in the system, that is, **process
    noise**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0482ee7-1982-4c30-95b1-56dc7dc3c2d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we make statistical assumptions about the nature of the process noise,
    this will become a statistical framework, which is usually what happens. But,
    this way, if we are uncertain about our state transition model, but certain about
    observations, surely the best solution would still be what the instruments reported.
    So, we need to tie our state to our observations. Notice that we are observing
    *x*, so the observation could be recovered by multiplying the state by a simple
    row matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e49b45c-88d5-43ae-b353-8ab4cb95ff8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But, as we said, we have to allow for the observations being imperfect (maybe
    our radar is very old, and sometimes has erroneous readings), that is, we need
    to allow for **observation noise**; thus, the final observation is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9a16bcf-3bfc-4d3e-b41c-3c5fee17584d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we can characterize process noise and observation noise, the Kalman
    filter will be able to give us good predictions for the locations of the train
    at each point, using only the observations *before* that time. The best way to
    parametrize noise is with a covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0db437b2-e554-45dd-897c-93bbbe583257.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Kalman filter has a recursive **state-transition model**, so we have to
    supply the initial value of the state. If we pick it to be `(0, 0)`, and if we
    assume that **process** **noise** and **measurement noise** are equally probable
    (this is a terrible assumption in real life), the Kalman filter will give us the
    following predictions for each point in time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48a67f29-b7f5-4978-9136-1d11bfd56edb.png)'
  prefs: []
  type: TYPE_IMG
- en: Since we believe our observations as much as our assumption that the velocity
    doesn't change, we got a smoothed curve (blue) that is not as extreme, but it
    is still not that convincing. So, we have to make sure that we encode our intuition
    in the variables that we pick.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we say that the **signal-to-noise ratio**, that is, the square root
    of the ratio of covariances, is 10, we will get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8fea792-6cd0-4bf0-b92b-c1036e39613e.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the velocity does indeed move very slowly, but we seem to have
    underestimated how far the train has gone. *Or have we?*
  prefs: []
  type: TYPE_NORMAL
- en: It's a really hard task to tune the Kalman filter, and there are many algorithms
    for doing that, but unfortunately, none are perfect. For this chapter, we will
    not cover those; we will try to pick parameters that make sense, and we will see
    that those parameters give decent results.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's revisit our single car tracking model, and see how we should model
    our system dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: Using a box tracker with the Kalman filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we have to figure out how to model each car's state. It might be better
    to start with the observation model; that is, *what can we measure about each
    car?*
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, the object detectors give us boxes, but the way they are presented is
    not the best physical interpretation; similar to the train example given previously,
    we want variables we can reason about and that are closer to the underlying dynamics
    of the traffic. So, we use the following observation model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79301632-1b42-4f7e-a72e-bad89cc97138.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *u* and *v* are the horizontal and vertical pixel locations of the center
    of the target, and *s* and *r* represent the scale (area) and the aspect ratio
    of the target’s bounding box respectively. Since our cars are moving around the
    screen and are moving further away or coming closer, both coordinates and the
    size of the bounding boxes will change over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming that nobody is driving like a lunatic, the velocities of the cars
    in the image should stay more or less constant; that''s why we can limit our model
    to the location and velocities of the objects. So, the state we will take is the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b31bf4a5-3f32-4b71-ba2c-ba7e570f2441.png)'
  prefs: []
  type: TYPE_IMG
- en: We have used a notation where the dot on top of a variable means the rate of
    change of that variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **state transition model** will be that the velocities and the aspect ratio
    stay constant over time (with some **process noise**). In the following screenshot,
    we have visualized all the boundary boxes, and their corresponding states (the
    location of the center and the velocity vector):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9b6ae4a-1f4e-4ab9-ac0f-0ed8e474079f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have set up the model so that what it observes is slightly
    different from what we receive from our tracker․ So, in the next section, we'll
    go over the transformation functions we need to go from a boundary box to and
    from the state space of the Kalman Filter.
  prefs: []
  type: TYPE_NORMAL
- en: Converting boundary boxes to observations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to pass the boundary boxes to the Kalman filter, we will have to define
    a transformation function from each boundary box to the observation model, and,
    in order to use the predicted boundary boxes for object tracking, we need to define
    a function from a state to a boundary box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a transformation function from a boundary box to an observation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we calculate the center coordinates of the boundary box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we calculate the width and height of the box, which we will use to calculate
    the size (that is, the area) and the scale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we calculate the size of `bbox`, that is, the area:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we calculate the aspect ratio, which is done just by dividing the
    width by the height:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then `return` the result as a 4 x 1 matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, since we know that we have to define the inverse transformation as well,
    let''s define `state_to_bbox`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It takes a 7 x 1 matrix as an argument and unpacks all the components that
    we need to construct a boundary box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it calculates the width and the height of the boundary box, from the
    aspect ratio and scale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, it calculates the coordinates of the center:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it calculates the half size of the box as a `numpy` tuple, and uses it
    to calculate the coordinates of the opposite corners of the box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we return the boundary box as a one-dimensional `numpy` array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Geared with the transformation functions, let's see how we can use OpenCV to
    build a Kalman filter.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Kalman filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, geared with our model, let's get our hands dirty and write a class that
    handles all this magic. We are going to write a custom class that will use `cv2.KalmanFilter`
    as a Kalman filter, but we will add some helper attributes to be able to keep
    track of each object.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s take a look at the initialization of the class, where we will
    set up our Kalman filter by passing the state model, transition matrix, and initial
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first start by initializing the class with the boundary box—`bbox`—and the
    label for the `label` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we set up some helper variables that will let us filter boxes as they
    appear and disappear in the tracker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we initialize `cv2.KalmanFilter` with the correct dimensionality and
    data type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the transition matrix and the corresponding process'' **noise covariance
    matrix**. The covariance matrix is a simple model that involves the movement of
    each object with the current constant velocity in the horizontal and vertical
    directions, and becomes bigger or smaller using a constant rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We also set how certain we are about the constant speed process. We choose
    a **diagonal covariance matrix**; that is, our state variable is not correlated,
    and we set the variance for location variables as `10`, and as 10,000 for velocity
    variables. We believe that location changes are more predictable than velocity
    changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we set the **Observation model** to be the following matrix, which implies
    that we are just measuring the first four variables in the state, that is, all
    the location variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have set the measurement of the noise covariance, we believe that
    the horizontal and vertical locations are greater than the aspect ratio and the
    zoom, so we give smaller values to those two measurement variances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set the initial position and the uncertainty associated with the
    Kalman filter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'After we are done setting up the Kalman filter, we need to be able to actually
    predict the new position of the object when it moves. We will do that by defining
    two more methods—`update` and `predict`. The `update` method will update the Kalman
    filter based on a new observation, and the `predict` method will predict a new
    position based on previous evidence. Now let''s take a look at the `update` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `update` method takes a boundary box of the new location, `bbox`,
    converts it to an observation, and calls the `correct` method on the OpenCV implementation.
    We have only added some variables to keep track of how long it has been since
    we have updated the object that we are tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s take a look at the `predict` function; its procedure is explained
    in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It first checks whether we have called `predict` twice in a row; if we have called
    it twice in a row, then it sets `self.hit_streak` to `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then it increments `self.time_since_update` by `1`, so we keep track of how
    long we have been tracking this object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we call the `predict` method of the OpenCV implementation and return a
    boundary box that corresponds with the prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: So, now that we have implemented a single-object tracker, the next step is to
    create a mechanism that can associate a detection box with a tracker, which we
    will do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Associating detections with trackers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the Sort algorithm, decisions about whether two bounding boxes should be
    considered to be of the same object are made based onIntersection over Union.
    Previously in this chapter, you learned about this metric and implemented a function
    to compute it. Here, we''ll define a function that will associate detection and
    tracking boxes based on their IoU value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The function accepts the bounding boxes of detections and the predicted boxes
    of trackers, as well as an IoU threshold. It returns matches as an array of pairs
    of corresponding indexes in the corresponding arrays, indexes of unmatched boxes
    of detections, and indexes of unmatched boxes of trackers. In order to achieve
    this, it takes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, it initializes a matrix in which the IoU values of each possible pair
    of boxes will be stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we iterate overdetection and tracker boxes, calculate IoU for each pair,
    and store the resulting values in the matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `iou_matrix`, we will find matching pairs such that the sum of the values
    of the IoUs of these pairs gets the maximal possible value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: For this purpose, we have used the **Hungarian algorithm**, which is implemented
    as the `linear_sum_assignment` function. It is a combinatorial optimization algorithm
    that solves the **assignment problem**.
  prefs: []
  type: TYPE_NORMAL
- en: In order to use this algorithm, we have passed the opposite values of `iou_matrix`.
    The algorithm associates indexes such that the total sum is minimal. Hence, we
    find the maximal value when we negate the matrix. The straightforward way to find
    these associations would be to iterate over all possible combinations and pick
    the one that has the maximal value.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with the latter approach is that the time complexity of it will
    be exponential and hence it will be too slow once we have multiple detections
    and trackers. Meanwhile, the Hungarian algorithm has a time complexity of ***O(n³)***.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we change the format of the result of the algorithm so that it appears
    as pairs of matched indexes in a `numpy` array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Then get the intersection over union values of the matches from `iou_matrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter out matches that have an IoU value that is too low:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, find the indexes of the detection boxes that were not matched:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, find the indexes of the tracker boxes that were not matched:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'At last, it returns the matches as well as the indexes of the unmatched detection
    and tracker boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: So, now that we have mechanisms to track a single object and to associate detections
    with single-object trackers, what's left to do is to create a class that will
    use these mechanisms to track multiple objects throughout frames. We will do this
    in the next section and then the algorithm will be complete.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the main class of the tracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The constructor of the class is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'It stores two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter is `max_age`, which specifies how many consecutive times
    a tracker of a certain object can remain without an associated box before we consider
    the object to have gone from the scene and delete the tracker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second parameter is `min_hits`, which specifies how many consecutive times
    a tracker should be associated with a box so that we consider it to be a certain
    object. It also creates properties for storing the trackers and counting the total
    number of trackers during the instance lifetime.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also define a method for creating an ID of a tracker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: The method increments the count of the trackers by one and returns the number
    as the ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to define the `update` method, which will do the heavy lifting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The `update` method accepts detection boxes and covers the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For all available `trackers`, it predicts their new locations and removes `trackers`
    with failed predictions right away:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get the predicted boxes of the `trackers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we associate the boxes predicted by the trackers with the detection boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We then update the matched `trackers` with the associated detections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'For all unmatched detections, we create new `trackers` that are initialized
    with the corresponding bounding box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We then compose the `return` value as an `array` of the tracker box and tracker
    ID concatenations of the relevant trackers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: In the previous codes snippet, we consider only those `trackers` that were updated
    with a detection box in the current frame and that have at least a `hit_streak` consecutive
    association with detection boxes. Depending on the particular application of the
    algorithm, you might want to change this behavior to make it a better fit for
    your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then clean up the `trackers` by removing the ones that have not been updated
    with a new bounding box for a while:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'At last, we `return` the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: So, now that we have completed the implementation of the algorithm, we have
    everything ready to run the app and see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: Seeing the app in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we run our app, it will use a passed video or another video stream, then
    process it and illustrate the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbd2cba6-34ff-4bb2-a4f1-0a88c752c84e.png)'
  prefs: []
  type: TYPE_IMG
- en: On each processed frame, it will display the object type, a bounding box, and
    the number of each tracked object. It will also display general information about
    tracking in the top-left corner of the frame. This general information consists
    of the total number of tracked video objects throughout for each type of tracked
    object, as well as the tracked objects currently available in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we have used an object detection network and combined
    it with a tracker to track and count objects over time. After reading through
    the chapter, you should now understand how detection networks work and understand
    their training mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: You have learned how you can import models built with other frameworks into
    OpenCV and bind them into an application that processes a video or uses other
    video streams such as your camera or a remote IP camera. You have implemented
    a simple, yet robust, algorithm for tracking, which, in combination with a robust
    detector network, allows the answering of multiple statistical questions related
    to video data.
  prefs: []
  type: TYPE_NORMAL
- en: You can now use and train object detection networks of your choice in order
    to create your own highly accurate applications that implement their functionality
    around object detection and tracking.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the course of the book, you have made yourself familiar with a background
    in one of the main branches of machine learning, called **computer vision**. You started
    by using simple approaches such as image filters and shape analysis techniques.
    Then, you proceeded with classical feature extraction approaches and built several
    practical apps based on these approaches. After that, you learned about the statistical
    properties of a natural scene and you were able to use these properties to track
    unknown objects.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you started to learn about, use, and train supervised models such as **Support
    Vector Machines** (**SVMs**) and **cascading classifiers**. Having all this theoretical
    and practical knowledge about classical computer vision approaches, you dived
    into deep learning models, which nowadays give state-of-the-art results for many
    machine learning problems, especially in the field of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: You now understand how **convolutional networks** work and how deep learning
    models are trained, and you have built and trained your own networks on top of
    other pre-trained models. Having all this knowledge and practice, you can analyze,
    understand, and apply other computer vision models as well as elaborating on new
    models once you come up with new ideas. You are ready to work on your own **c****omputer
    vision** projects, which might change the world!
  prefs: []
  type: TYPE_NORMAL
