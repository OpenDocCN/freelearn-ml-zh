<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Convolutional Neural Networks</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we discussed the&#160;importance and applications of features. We now understand that the better the features are, the more accurate the results are going to be. In recent periods, the features have become more precise and as such better accuracy has been achieved. This is due to a new kind of feature extractor called <strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>) and they have shown remarkable accuracy in complex tasks, such as object detection in challenging domains, and classifying images with high accuracy, and are now quite ubiquitous in applications ranging from smartphone photo enhancements to satellite image analysis.&#160;</p>
<p>In this chapter, we will begin with an introduction to neural nets and continue into an explanation of CNNs and how to implement them. After this chapter, you will be able to write your own CNN from scratch for applications like image classification. The chapter includes:</p>
<ul>
<li>Datasets and libraries used in the various sections of the chapter</li>
<li>Introduction to neural networks with an explanation on simple neural network</li>
<li>CNN explanation and various components involved in it</li>
<li>An example of creating CNN for image classification</li>
<li>Description of transfer learning and statistics on various deep learning models</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Datasets and libraries used</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be using Keras to write neural nets with TensorFlow as backend. A detailed installation procedure is explained in <a href="prac-cv_ch02.html" target="_blank">Chapter 2</a>, <em>Libraries, Development Platforms and Datasets</em>. To check if you have Keras installed, in shell run:</p>
<pre><strong>python -c "import keras;print(keras.__version__)"</strong></pre>
<p>This will print the Keras version as well as which backend you are using. If you have TensorFlow installed and Keras is using TensorFlow, it will print <kbd>using Tensorflow backend</kbd>. If you have an older version of Keras and TensorFlow, there might be some issues, so please install or upgrade to the latest versions. We will also be using other libraries like <kbd>NumPy</kbd> and <kbd>OpenCV</kbd>.&#160;</p>
<div class="packt_infobox">We will be using the <kbd>Fashion–MNIST</kbd> dataset by&#160;<span>Zalando SE</span> which is available at&#160;<a href="https://github.com/zalandoresearch/fashion-mnist" target="_blank">https://github.com/zalandoresearch/fashion-mnist</a>. This can be downloaded directly with Keras and there is no requirement for a separate download. <kbd>Fashion-MNIST</kbd> is&#160;<span>MIT License (MIT) Copyright © [2017] Zalando SE.</span></div>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to neural networks</h1>
                </header>
            
            <article>
                
<p><span>Neural networks have been here for quite some time, with initial papers arriving more than a few decades ago. The recent popularity is due to the availability&#160;of better software for algorithms and proper hardware to run them. Initially, neural networks were motivated by how humans&#160;</span>perceive the world and were modeled according to biological neuron functions. This was continuously modified over the course of time and has since been evolving to get a better performance.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">A simple neural network</h1>
                </header>
            
            <article>
                
<p>A simple neural net consists of a node which takes in an input or a list of inputs and performs a transformation. An example is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="216" width="422" src="images/e84b36da-bb46-4778-a759-b4597fc8ad3d.png"/></div>
<p><span>Mathematically, it takes the inputs <em>x</em> and applies a transformation <em>W</em> to get the output <em>y</em>:</span></p>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img height="27" width="86" class="alignnone size-full wp-image-466 image-border" src="images/1375032a-e775-4d91-8855-4957b474767a.png"/></div>
<p><span>The input <em>x</em> can be a vector or multi-dimensional array. Based on the transformation matrix <em>W</em> we get the output <em>y</em> to be a vector or multi-dimensional array. This structure is further modified by also including the non-linear transformation <em>F</em>:</span></p>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img height="25" width="105" class="alignnone size-full wp-image-467 image-border" src="images/fa279536-9ab4-46aa-89b1-db651e0d0e18.png"/></div>
<p><span>Now, output <em>y</em> is not linearly dependent on input <em>x</em>, and as a result, the change in <em>x</em> does not proportionally change <em>y</em>. More often, these non-linear transformations consist of clipping all negative values after applying the transformation matrix <em>W</em> to the inputs <em>x</em>. A neuron consists of this complete operation.&#160;</span></p>
<p>These networks are stacked in layered structures, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="733" width="1492" class="alignnone size-full wp-image-469 image-border" src="images/2f851f33-3a9c-4dfd-8e6a-6fde0f3f2d53.png"/></div>
<p>These networks are also termed <strong>feedforward networks</strong>, as there are no loops and input flows through the network in one direction, like in a <strong>Directed Acyclic Graph</strong> (<strong>DAG</strong>). In these networks, the parameters are termed as <strong>weights</strong>, which perform a transformation on the inputs.</p>
<p>Using machine learning approaches to learn these weights, we can get an optimal network that performs the desired operation with good accuracy. For this, the requirement is to have a dataset of labeled inputs; for example, for a given input <em>x</em> we already know the output value <em>y</em>. During the learning of the weights of neural networks, also termed as <strong>training</strong>, for this dataset the input is passed through the network layer by layer. At each layer, the input from the previous layer is transformed according to the layer's properties. The final output is our prediction for <em>y</em> and we can measure how far our prediction of <em>y</em> is from the actual value. Once we have this measure, termed as a <strong>loss</strong>, we can then use it to update the weights using a derivative-based approach called <strong>gradient descent</strong>. Each weight is updated according to the change in the loss with respect to weights.</p>
<p>We will describe a simple example of a neural network using <kbd>NumPy</kbd> library.&#160;<span>In this example, we consider the input</span> <kbd>x</kbd> <span>a vector of size <kbd>1000</kbd> and we want to compute an output of size <kbd>2</kbd></span>:</p>
<pre><strong>dim_x = 1000 # input dims</strong><br/><strong>dim_y = 2 # output dims</strong></pre>
<p><span>We create a neural network that takes in this input <em>x</em> and applies a non-linear transformation with the weight matrix <em>W</em>:</span></p>
<div style="padding-left: 150px" class="mce-root CDPAlignLeft CDPAlign"><img height="46" width="168" class="alignnone size-full wp-image-470 image-border" src="images/a1d7976b-6804-4da5-a6f0-6e94abf54729.png"/></div>
<p>An example is shown as follows:</p>
<pre><strong>def net(x, w):</strong><br/><strong>    """</strong><br/><strong>    A simple neural net that performs non-linear transformation</strong><br/><strong>    Function : 1 / (1 + e^(-w*x)) </strong><br/><strong>    x: inputs </strong><br/><strong>    w: weight matrix</strong><br/><strong>    Returns the function value</strong><br/><strong>    """</strong><br/><strong>    return 1/(1+np.exp(-x.dot(w)))</strong></pre>
<p>To learn these weights <kbd>w</kbd>, we will follow the gradient descent method. For each input we will compute the gradient of loss with respect to <kbd>w</kbd> and update the weights as follows:</p>
<pre><strong>    # feed forward pass</strong><br/><strong>    y_pred = net(x, w)</strong><br/><br/><strong>    # compute loss</strong><br/><strong>    loss = compute_loss(y, y_pred)</strong><br/><strong>    print("Loss:", loss, "at step:", i)</strong><br/><br/><strong>    # compute grads using backprop on given net</strong><br/><strong>    w_grad = backprop(y, y_pred, w, x) </strong><br/><br/><strong>    # update weights with some learning rate</strong><br/><strong>    w -= lr * w_grad</strong></pre>
<p>This step is iterated repeatedly over our labeled dataset until our loss does not change significantly or the loss values start following some repetition. The <kbd>loss</kbd> function is defined as:</p>
<pre><strong>def compute_loss(y, y_pred):</strong><br/><strong>    """</strong><br/><strong>    Loss function : sum(y_pred**2 - y**2)</strong><br/><strong>    y: ground truth targets </strong><br/><strong>    y_pred: predicted target values</strong><br/><strong>    """</strong><br/><strong>    return np.mean((y_pred-y)**2)</strong> </pre>
<p>The overall code is as follows:&#160;</p>
<pre><strong>import numpy as np </strong><br/><br/><strong>dim_x = 1000 # input dims</strong><br/><strong>dim_y = 2 # output dims</strong><br/><strong>batch = 10 # batch size for training </strong><br/><strong>lr = 1e-4 # learning rate for weight update</strong><br/><strong>steps = 5000 # steps for learning </strong><br/><br/><strong># create random input and targets</strong><br/><strong>x = np.random.randn(batch, dim_x)</strong><br/><strong>y = np.random.randn(batch, dim_y)</strong><br/><br/><strong># initialize weight matrix </strong><br/><strong>w = np.random.randn(dim_x, dim_y)</strong><br/><br/><strong>def net(x, w):</strong><br/><strong>    """</strong><br/><strong>    A simple neural net that performs non-linear transformation</strong><br/><strong>    Function : 1 / (1 + e^(-w*x)) </strong><br/><strong>    x: inputs </strong><br/><strong>    w: weight matrix</strong><br/><strong>    Returns the function value</strong><br/><strong>    """</strong><br/><strong>    return 1/(1+np.exp(-x.dot(w)))</strong><br/><br/><strong>def compute_loss(y, y_pred):</strong><br/><strong>    """</strong><br/><strong>    Loss function : sum(y_pred**2 - y**2)</strong><br/><strong>    y: ground truth targets </strong><br/><strong>    y_pred: predicted target values</strong><br/><strong>    """</strong><br/><strong>    return np.mean((y_pred-y)**2) </strong><br/><br/><strong>def backprop(y, y_pred, w, x):</strong><br/><strong>    """</strong><br/><strong>    Backpropagation to compute gradients of weights</strong><br/><strong>    y : ground truth targets </strong><br/><strong>    y_pred : predicted targets </strong><br/><strong>    w : weights for the network</strong><br/><strong>    x : inputs to the net </strong><br/><strong>    """</strong><br/><strong>    # start from outer most</strong><br/><strong>    y_grad = 2.0 * (y_pred - y)</strong><br/><br/><strong>    # inner layer grads </strong><br/><strong>    w_grad = x.T.dot(y_grad * y_pred * (1 - y_pred))</strong><br/><strong>    return w_grad</strong><br/><br/><strong>for i in range(steps):</strong><br/><br/><strong>    # feed forward pass</strong><br/><strong>    y_pred = net(x, w)</strong><br/><br/><strong>    # compute loss</strong><br/><strong>    loss = compute_loss(y, y_pred)</strong><br/><strong>    print("Loss:", loss, "at step:", i)</strong><br/><br/><strong>    # compute grads using backprop on given net</strong><br/><strong>    w_grad = backprop(y, y_pred, w, x) </strong><br/><br/><strong>    # update weights with some learning rate</strong><br/><strong>    w -= lr * w_grad</strong></pre>
<p>On running the previous code, we can see the values of <kbd>loss</kbd> decreasing and settling down. The parameters here are learning rate and initial <kbd>w</kbd> values. A good choice for these values may cause the loss to decrease faster and settle early; however, a bad choice will lead to no decrease in loss, or sometimes an increase in loss over several iterations.</p>
<p>In this section, we have seen how to build a simple neural network. You can use this code and modify or add complex structures to play with it. Before we go further, to the explanation for CNNs, in the next section we will briefly revisit the convolution operation, which was explained in the previous chapter.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Revisiting the convolution operation</h1>
                </header>
            
            <article>
                
<p>Extending our discussion on filters from <a href="prac-cv_ch03.html" target="_blank">Chapter 3</a>, <em>Image Filtering and Transformations in OpenCV</em>, the convolution operation is taking a dot product of a&#160;shifted kernel matrix with a given input image. This process is explained in the following figure:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="298" width="269" class="alignnone size-full wp-image-471 image-border" src="images/5464aeef-7bde-4eeb-ad80-1cbf646c553c.png"/></div>
<p class="mce-root">As shown in the previous figure, a kernel is a small two-dimensional array that computes dot product with the input image (on the left) to create a block of the output image (on the right).</p>
<p class="mce-root">In convolution, the output image is generated by taking a dot product between an <strong>Input</strong> image and a <strong>Kernel</strong> matrix. This is then shifted along the image and after each shift, corresponding values of the output are generated using a dot product:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="316" width="560" src="images/8a28c429-8d56-4c7d-9e9e-fa7282c399d8.png"/></div>
<p class="mce-root">As we saw in the previous chapter, we can perform a convolution operation using OpenCV as follows:&#160;</p>
<pre><strong>kernel = np.ones((5,5),np.float32)/25</strong><br/><strong>dst = cv2.filter2D(gray,-1,kernel)</strong></pre>
<p>Here, we assume a kernel with equal values whose sum is 1 and is used to perform convolution on a grayscale image. In <a href="prac-cv_ch03.html" target="_blank">Chapter 3</a>, <em>Image Filtering and Transformations in OpenCV</em>, this was termed as <strong>smoothing operation</strong> because if we have a noisy grayscale image, the output will look smoother.</p>
<p><span class=" author-d-iz88z86z86za0dz67zz78zz78zz74zz68zjz80zz71z9iz90za3mz82z45adyt4z89zz86zz79zblz67zdy11z83zxz88zcz79zfz84zz76z1z85z">In this case, to perform a smoothing operation we already know the kernel values. If we know the kernels for extracting more complex features, we can do better inference from images. However, manually setting the values is unfeasible when we have to perform tasks like image classification and object detection. In such cases, models such as CNNs extract good features and perform better than other previous methods. In the next section, we will define a structure that will learn these kernel matrix values and compute richer features for a wide variety of applications.</span></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p>Convolutional Neural Networks, also known as <strong>ConvNets</strong>, use this convolution property in a neural network to compute better features, which can then be used to classify images or detect objects. As shown in the previous section, convolution consists of kernels which compute an output by sliding and taking a dot product with the input image. In a simple neural network, the neurons of a layer are connected to all the neurons of the next layer, but CNNs consist of convolution layers which have the property of the receptive field. Only a small portion of a previous layer's neurons are connected to the neurons of the current layer. As a result, small region features are computed through every layer as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="279" width="304" src="images/e160bc89-12ef-48f4-9c38-cb960631651f.png"/>&#160;</div>
<p>As we have seen in a&#160;simple neural network, the neuron takes an input from one or more of previous neurons' output to perform a non-linear transformation. In CNNs, this is further combined with the convolution approach. We assume a set of kernels with varied values called <strong>weights</strong>. Each of these kernels is convolved with an input to create a response matrix. There is then a non-linear transformation called <strong>activation</strong> for each of the values in the convolved output. The output from each of the kernels after activation is stacked to create the output of our operation such that for <em>K</em> kernels the output is of <em>K x H<sub>o</sub> x W<sub>o</sub></em>&#160;size, with <em>H<sub>o</sub></em> and <em>W<sub>o</sub></em> as the height and width of our output. This makes one layer of CNN.</p>
<p>The output from the previous layer is then used again as the input for the next layer with another set of kernels&#160;<em>K<sub>2</sub></em>, and a new response is computed by first convolving each of the kernels and then taking the non-linear transformation of the response.&#160;</p>
<p>In general, CNNs are composed of the following types of layers:</p>
<ul>
<li>Convolutional layers</li>
<li>Fully connected layers</li>
<li>Activation layers</li>
<li>Pooling layers</li>
</ul>
<p>Each of these layers will be explained in the following sections. In recent developments, some more components have been added to CNNs, but the preceding components still remain important.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The convolution layer</h1>
                </header>
            
            <article>
                
<p><span>A key component of a CNN is the convolution layer, which performs a dot product of a kernel matrix with part of an image and generates an output. This is followed by shifting and repeating the same operation over a complete image and is termed&#160;<strong>convolution</strong>. The region of the input which is taken for the dot product is called the <strong>receptive field</strong> of the convolution layer. In each convolution layer, there is a set of kernels and they are collectively termed</span><strong>&#160;filters</strong>.<span>&#160;</span></p>
<p><span>The input for a convolution layer is an n-dimensional array, meaning the input is an image of the form <em>Width x Height x Depth</em>.&#160;</span><span>For example, if we have a grayscale image of the size 32 x 32, width and height, then the input is 32 x 32 x 1&#160;where depth is the number of color channels in this case, and is represented by the third dimension. Similarly, for a colored image of size 512, the input is 512 x 512 x 3. All the kernels in filters also have the same depth as the input.&#160;</span></p>
<p><span>The parameters for the layer are the number of filters, filter size, strides, and padding value. Of these,</span> <em>filter</em> <span>values are the only learnable&#160;parameters. <em>Strides</em> refer to the amount of shift in pixels for a kernel. With a stride of 1, the kernel is moved left by 1 pixel and the dot product is taken with the corresponding input region. With a stride of 2, the kernel is moved by 2 pixels and the same operation takes place. On each input, at the boundary, the kernel can only overlap a certain region inside the image. Boundaries are therefore padded with zeros for the kernel to capture the complete image region. The</span> padding value sets t<span>he way to pad the image boundary.</span></p>
<p>The size of the output depends on these parameter values. We can use Keras to write CNN and perform operations on images. An example of writing one convolution layer is:</p>
<pre><strong>y = Conv2D(filters=32, </strong><br/><strong>           kernel_size=(5,5), </strong><br/><strong>           strides=1, padding="same")(x)</strong></pre>
<p>Let's create an example model to see the properties of convolution layer:</p>
<pre><strong>from keras.layers import Conv2D, Input</strong><br/><strong>from keras.models import Model</strong><br/><br/><strong>def print_model():</strong><br/><strong>    """</strong><br/><strong>    Creates a sample model and prints output shape</strong><br/><strong>    Use this to analyse convolution parameters</strong><br/><strong>    """</strong><br/><strong>    # create input with given shape </strong><br/><strong>    x = Input(shape=(512,512,3))</strong><br/><br/><strong>    # create a convolution layer</strong><br/><strong>    y = Conv2D(filters=32, </strong><br/><strong>               kernel_size=(5,5), </strong><br/><strong>               strides=1, padding="same",</strong><br/><strong>               use_bias=False)(x)</strong><br/>    <br/><strong>    # create model </strong><br/><strong>    model = Model(inputs=x, outputs=y)</strong><br/><br/><strong>    # prints our model created</strong><br/><strong>    model.summary()</strong><br/><br/><strong>print_model()</strong></pre>
<div class="packt_infobox"><span><span>When you execute codes, please ignore the warnings shown, such as:</span></span> <span>Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA.</span></div>
<p>On executing this code, we can see our model and output after each layer as:</p>
<pre>_________________________________________________________________<br/><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 512, 512, 3) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_1 (Conv2D) (None, 512, 512, 32) 2400</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 2,400</strong><br/><strong>Trainable params: 2,400</strong><br/><strong>Non-trainable params: 0</strong><br/>_________________________________________________________________</pre>
<p>Here, we have set the input to be of the shape 512 x 512 x 3, and for convolution, we use 32 filters with the size 5 x 5. The stride values we have set to 1, and using the same padding for the edges, we make sure a kernel captures all of the images. We will not use bias for this example. The output after convolution is of the shape&#160;(None, 512, 512, 32) of the shape&#160;<span>(samples, width, height, filters)</span>. For the discussion, we will ignore the sample's value. The width and height of the output is 512 with the depth as 32. The number of filters we used to set the output's depth value. The total number of parameters for this layer is 5 x 5 x 3 x 32&#160;(kernel_size * number of filters) which is 2400.</p>
<p>Let's try another run. Now, we set strides to 2 in the previous code. On execution, we get the output as:</p>
<pre>_________________________________________________________________<br/><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 512, 512, 3) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_1 (Conv2D) (None, 256, 256, 32) 2400</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 2,400</strong><br/><strong>Trainable params: 2,400</strong><br/><strong>Non-trainable params: 0</strong><br/>_________________________________________________________________</pre>
<p>As we can see, the convolution output shape (width, height) is reduced to half of the input size. This is due to the stride option that we have chosen. Using strides 2, it will skip one pixel, making the output half of the input. Let's increase the stride to 4. The output will be:</p>
<pre>_________________________________________________________________<br/><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 512, 512, 3) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_1 (Conv2D) (None, 128, 128, 32) 2400</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 2,400</strong><br/><strong>Trainable params: 2,400</strong><br/><strong>Non-trainable params: 0</strong><br/>_________________________________________________________________ </pre>
<p>Here, we can see that the output shape (width and height) is reduced to one-fourth of the input.</p>
<p>If we set our strides to 1, and padding to valid, the output is:</p>
<pre>_________________________________________________________________<br/><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 512, 512, 3) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_1 (Conv2D) (None, 508, 508, 32) 2400</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 2,400</strong><br/><strong>Trainable params: 2,400</strong><br/><strong>Non-trainable params: 0</strong><br/>_________________________________________________________________</pre>
<p>Now, even if we set strides to 1, the output shape (width and height) is reduced to <kbd>508</kbd>. This is due to the lack of a padding set, and the kernel cannot be applied to the edges of the input.&#160;</p>
<p>We can compute the output shape as <img height="16" width="116" class="alignnone size-full wp-image-487 image-border" src="images/d47fc610-6adc-4264-b6b8-6b2e4c0dc9b0.png"/>, where <em>I</em> is input size, <em>K</em> is kernel size, <em>P</em> is padding used and <em>S</em> is stride value. If we use the same padding, the <em>P</em> value is<img height="20" width="64" class="alignnone size-full wp-image-488 image-border" src="images/535f27e6-8f4e-439e-ad0d-5b6a3548f75b.png"/>.&#160;Otherwise, if we use the valid padding then <em>P</em> value is zero.&#160;</p>
<p>Here, we have set another parameter <kbd>use_bias=False</kbd>. On setting this as true, it will add a constant value to each kernel and for a convolution layer, the bias parameter is the same as the number of filters used. So, if we set <kbd>use_bias to True</kbd>, with strides 1 and the same padding, we get:</p>
<pre>_________________________________________________________________<br/><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 512, 512, 3) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_1 (Conv2D) (None, 512, 512, 32) 2432</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 2,432</strong><br/><strong>Trainable params: 2,432</strong><br/><strong>Non-trainable params: 0</strong><br/>_________________________________________________________________</pre>
<p>The total number of parameters is increased by <kbd>32</kbd>, which is the number of filters used in this layer. We have seen how to design the convolution layer and what happens when we use different parameters for the convolution layer.</p>
<p>The crucial thing is, what are the kernel values in the filters that will give the desired output?&#160;<span>To have good performance, we would like to get the output to consist of high-quality features from the input. Setting values for the filters manually is not feasible as the number of filters grows quite large and the combinations of these values are practically infinite. We learn the filter values using optimization techniques which use a dataset of inputs and targets and tries to predict as close to the targets as possible. The optimization then updates the weights after each iteration.&#160;</span></p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The activation layer</h1>
                </header>
            
            <article>
                
<p>As we saw in the case of the simple neural network, the weighted output is passed through a non-linear transformation. This non-linear layer is often referred to as the activation layer. Some common types of activation are:</p>
<ul>
<li>Sigmoid: <img height="17" width="112" class="alignnone size-full wp-image-489 image-border" src="images/c032bfe3-45d3-42d2-9f86-a8c3dd69f4f4.png"/></li>
<li>ReLU: <img height="20" width="124" class="alignnone size-full wp-image-491 image-border" src="images/297ac62e-1aa0-4803-9caf-b31c9efd7c3e.png"/></li>
<li>Tanh: <img height="20" width="110" class="alignnone size-full wp-image-492 image-border" src="images/ba5882e8-f64b-4322-bf56-3a05f3bcb3e9.png"/></li>
<li>Leaky ReLU:<img height="20" width="136" class="alignnone size-full wp-image-493 image-border" src="images/ae3fbb33-c7d3-4f9e-9a27-32fccc3fcbfd.png"/>&#160;where <img height="19" width="20" class="alignnone size-full wp-image-495 image-border" src="images/f88cbe14-ce23-4efa-afe8-0f1a41ea67f2.png"/>is a small positive float</li>
<li>Softmax:&#160;<img height="32" width="117" class="alignnone size-full wp-image-494 image-border" src="images/182b2e91-8e68-4000-a462-795997e94cd1.png"/>&#160;<span>this is often used to represent probability for a class</span></li>
</ul>
<p>The most common choice of activation function is <strong>Rectified Linear Unit</strong><span>&#160;(</span><strong>ReLU</strong>) and this performs well in the majority of cases. In our previous code we can add an activation layer as follows:</p>
<pre><strong>from keras.layers import Conv2D, Input, Activation</strong><br/><strong>from keras.models import Model</strong><br/><br/><strong>def print_model():</strong><br/><strong>    """</strong><br/><strong>    Creates a sample model and prints output shape</strong><br/><strong>    Use this to analyse convolution parameters</strong><br/><strong>    """</strong><br/><strong>    # create input with given shape </strong><br/><strong>    x = Input(shape=(512,512,3))</strong><br/><br/><strong>    # create a convolution layer</strong><br/><strong>    conv = Conv2D(filters=32, </strong><br/><strong>               kernel_size=(5,5), </strong><br/><strong>               strides=1, padding="same",</strong><br/><strong>               use_bias=True)(x)</strong><br/>    <br/><strong>    # add activation layer</strong><br/><strong>    y = Activation('relu')(conv)</strong><br/>    <br/><strong>    # create model </strong><br/><strong>    model = Model(inputs=x, outputs=y)</strong><br/><br/><strong>    # prints our model created</strong><br/><strong>    model.summary()</strong><br/><br/><strong>print_model() </strong> </pre>
<p>The output of executing the code is as follows:</p>
<pre>_________________________________________________________________<br/><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 512, 512, 3) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_1 (Conv2D) (None, 512, 512, 32) 2432</strong><br/><strong>_________________________________________________________________</strong><br/><strong>activation_1 (Activation) (None, 512, 512, 32) 0</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 2,432</strong><br/><strong>Trainable params: 2,432</strong><br/><strong>Non-trainable params: 0</strong><br/>_________________________________________________________________</pre>
<p>As we can see from our formulation of activation functions, it does not contain any trainable parameters. In Keras, the activation layer can also be added to the convolution layer as follows:</p>
<pre><strong>conv = Conv2D(filters=32, </strong><br/><strong>               kernel_size=(5,5), activation="relu", </strong><br/><strong>               strides=1, padding="same",</strong><br/><strong>               use_bias=True)(x)</strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The pooling layer</h1>
                </header>
            
            <article>
                
<p>Pooling takes in a region of inputs and either a max or an average value of that region is produced as output. In effect, it reduces the size of an input by sampling in the local region. This layer is inserted between 2 to 3 convolution layers to reduce the resolution of the output, thereby reducing the requirement for parameters. A visual representation of the pooling operation is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="354" width="277" class="alignnone size-full wp-image-496 image-border" src="images/2ccdb84b-1c02-406e-b2f9-a1bf17bb7fe8.png"/></div>
<p>In the previous figure, the input is a two-dimensional array of 2 x 2 size and after pooling operation the output is of size 1 x 1. This can be generated by taking average of the values in the previous array or the maximum value.&#160;</p>
<p>To show how the output shape changes after this operation, we can use the following code:&#160;</p>
<pre><strong>from keras.layers import Conv2D, Input, MaxPooling2D</strong><br/><strong>from keras.models import Model</strong><br/><br/><strong>def print_model():</strong><br/><strong>    """</strong><br/><strong>    Creates a sample model and prints output shape</strong><br/><strong>    Use this to analyse Pooling parameters</strong><br/><strong>    """</strong><br/><strong>    # create input with given shape </strong><br/><strong>    x = Input(shape=(512,512,3))</strong><br/><br/><strong>    # create a convolution layer</strong><br/><strong>    conv = Conv2D(filters=32, </strong><br/><strong>               kernel_size=(5,5), activation="relu", </strong><br/><strong>               strides=1, padding="same",</strong><br/><strong>               use_bias=True)(x)</strong><br/><br/><strong>    pool = MaxPooling2D(pool_size=(2,2))(conv)</strong><br/>    <br/><strong>    # create model </strong><br/><strong>    model = Model(inputs=x, outputs=pool)</strong><br/><br/><strong>    # prints our model created</strong><br/><strong>    model.summary()</strong><br/><br/><strong>print_model()</strong></pre>
<p>In the previous code, we used a convolution layer and added a pooling operation to it. When we execute it, the expected output is:</p>
<pre>_________________________________________________________________<br/><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 512, 512, 3) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_1 (Conv2D) (None, 512, 512, 32) 2432</strong><br/><strong>_________________________________________________________________</strong><br/><strong>max_pooling2d_1 (MaxPooling2 (None, 256, 256, 32) 0</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 2,432</strong><br/><strong>Trainable params: 2,432</strong><br/><strong>Non-trainable params: 0</strong><br/>_________________________________________________________________</pre>
<p>Here, we set the pooling parameter to <kbd>(2,2)</kbd>, representing the width and height of the pooling operation. The depth for pooling will be set according to the depth of the input to the pooling layer. The resulting output is of half the shape in terms of width and height; however, there is no change in depth size.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The fully connected layer</h1>
                </header>
            
            <article>
                
<p>This is a simple neural network layer where each neuron in the current layer is connected to all the neurons in the previous layer. This is often referred to as <kbd>Dense</kbd> or <kbd>Linear</kbd> in various deep learning libraries. In Keras, this can be implemented as follows:</p>
<pre><strong>from keras.layers import Dense, Input</strong><br/><strong>from keras.models import Model</strong><br/><br/><strong>def print_model():</strong><br/><strong>    """</strong><br/><strong>    Creates a sample model and prints output shape</strong><br/><strong>    Use this to analyse dense/Fully Connected parameters</strong><br/><strong>    """</strong><br/><strong>    # create input with given shape </strong><br/><strong>    x = Input(shape=(512,))</strong><br/><br/><strong>    # create a fully connected layer layer</strong><br/><strong>    y = Dense(32)(x)</strong><br/>    <br/><strong>    # create model </strong><br/><strong>    model = Model(inputs=x, outputs=y)</strong><br/><br/><strong>    # prints our model created</strong><br/><strong>    model.summary()</strong><br/><br/><strong>print_model()</strong></pre>
<p>As we execute this code, we can see the output shapes, as well as the number of trainable parameters, as follows:&#160;</p>
<pre><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 512) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>dense_1 (Dense) (None, 32) 16416</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 16,416</strong><br/><strong>Trainable params: 16,416</strong><br/><strong>Non-trainable params: 0</strong><br/>_________________________________________________________________</pre>
<p>The total parameters for this layer are given by <img height="15" width="78" class="alignnone size-full wp-image-497 image-border" src="images/0835407b-ec13-4fac-97be-9671f468a0e5.png"/>&#160;where<em>&#160;I<sub>s</sub></em>&#160;is input shape and&#160;<em>O<sub>s</sub></em>&#160;is output shape. In our example, we used an input of shape <kbd>512</kbd> and an output of shape <kbd>32</kbd> and get a total of 16416 parameters with bias. This is quite large compared to a similar convolution layer block, therefore in recent models, there has been a trend towards using more convolution blocks rather than fully connected blocks. Nonetheless, this layer still plays a major role in designing simple convolution neural net blocks.&#160;</p>
<p>In this section, we saw what CNNs are and what their components are. However, we haven't seen a way to set parameter values. Additionally, we have not seen several other layer structures, such as Batch Normalization and Dropout. These other layers also play major roles in designing CNN models.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Batch Normalization</h1>
                </header>
            
            <article>
                
<p>This is applied to normalize the output from the input layer with mean 0 and variance 1 as:&#160;</p>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img height="79" width="158" class="alignnone size-full wp-image-498 image-border" src="images/ccb54ecc-c993-40e5-a529-c29c0e34177d.png"/></div>
<p>This layer also has learnable parameters (which are optional in most of the deep learning libraries) to squash output in a given range:</p>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img height="36" width="194" class="alignnone size-full wp-image-499 image-border" src="images/7b89d853-6cac-4370-b86a-83af9f4c1efc.png"/></div>
<p>Here&#160;γ and&#160;β are learnable parameters. Batch Normalization improves training by faster convergence as well as acting as a kind of regularization. However, since there are learnable parameters, the effect of normalization is different in training and testing. &#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dropout</h1>
                </header>
            
            <article>
                
<p>One of the important layers that helps prevent overfitting is Dropout. It randomly drops, with some probability, the neurons from the previous layer to be used as input to the next layer. This acts like we are training an ensemble of neural networks. &#160;</p>
<p>In the following section, we will see how to implement a model in Keras and perform the learning of parameters, which we skipped in this section. &#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CNN in practice&#160;</h1>
                </header>
            
            <article>
                
<p>We will now start with our implementation of a convolutional neural net in Keras. For our example case, we will train a network to classify <kbd>Fashion-MNIST</kbd>. This is a dataset of grayscale images of fashion products, of the size 28 x 28. The total number of images is 70,000, with 60,000 as training and 10,000 as a test. There are ten categories in this dataset, which are t-shirt, trousers, pullover, dress, coat, sandal, shirt, sneakers, bag, and ankle boots. Labels for each are marked with a category number from 0-9.&#160; &#160;</p>
<p>We can load this dataset as follows:</p>
<pre><strong>from keras.datasets import fashion_mnist</strong><br/><strong>(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</strong></pre>
<p>The previous code block doesn't output a visualization of the dataset, so following image is to show what dataset we will be using:</p>
<div class="CDPAlignCenter CDPAlign"><img height="597" width="597" src="images/fe0bbbb7-e19c-4bcf-a570-783f0b8959a8.png"/></div>
<p>It will split the data into the train and test sets with both inputs <kbd>x</kbd> as well as the label <kbd>y</kbd>.&#160;&#160;</p>
<p>The convolution layer is written as follows:</p>
<pre><strong>def conv3x3(input_x,nb_filters):</strong><br/><strong>    """</strong><br/><strong>    Wrapper around convolution layer</strong><br/><strong>    Inputs:</strong><br/><strong>        input_x: input layer / tensor</strong><br/><strong>        nb_filter: Number of filters for convolution</strong><br/><strong>    """</strong><br/><strong>    return Conv2D(nb_filters, kernel_size=(3,3), use_bias=False,</strong><br/><strong>               activation='relu', padding="same")(input_x)</strong></pre>
<p>The pooling layer is written as follows:&#160;</p>
<pre><strong>x = MaxPooling2D(pool_size=(2,2))(input)</strong></pre>
<p>The overall output layer is as follows:</p>
<pre><strong>preds = Dense(nb_class, activation='softmax')(x)</strong></pre>
<p>The complete model is as follows:</p>
<pre><strong>def create_model(img_h=28, img_w=28):</strong><br/><strong>    """</strong><br/><strong>    Creates a CNN model for training. </strong><br/><strong>    Inputs: </strong><br/><strong>        img_h: input image height</strong><br/><strong>        img_w: input image width</strong><br/><strong>    Returns:</strong><br/><strong>        Model structure </strong><br/><strong>    """</strong><br/>    <br/><strong>    inputs = Input(shape=(img_h, img_w, 1))</strong><br/><br/><strong>    x = conv3x3(inputs, 32)</strong><br/><strong>    x = conv3x3(x, 32)</strong><br/><strong>    x = MaxPooling2D(pool_size=(2,2))(x) </strong><br/><strong>    x = conv3x3(x, 64)</strong><br/><strong>    x = conv3x3(x, 64)</strong><br/><strong>    x = MaxPooling2D(pool_size=(2,2))(x) </strong><br/><strong>    x = conv3x3(x, 128)</strong><br/><strong>    x = MaxPooling2D(pool_size=(2,2))(x) </strong><br/><strong>    x = Flatten()(x)</strong><br/><strong>    x = Dense(128, activation="relu")(x)</strong><br/><strong>    preds = Dense(nb_class, activation='softmax')(x)</strong><br/>    <br/><strong>    model = Model(inputs=inputs, outputs=preds)</strong><br/><strong>    print(model.summary())</strong><br/><strong>    return model</strong></pre>
<p>On running the previous code block,&#160; created model can be seen as, where each row is a layer type arranged sequentially with input layer on the top:&#160;&#160;</p>
<pre><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 28, 28, 1) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_1 (Conv2D) (None, 28, 28, 32) 288</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_2 (Conv2D) (None, 28, 28, 32) 9216</strong><br/><strong>_________________________________________________________________</strong><br/><strong>max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_3 (Conv2D) (None, 14, 14, 64) 18432</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_4 (Conv2D) (None, 14, 14, 64) 36864</strong><br/><strong>_________________________________________________________________</strong><br/><strong>max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>conv2d_5 (Conv2D) (None, 7, 7, 128) 73728</strong><br/><strong>_________________________________________________________________</strong><br/><strong>max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>flatten_1 (Flatten) (None, 1152) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>dense_1 (Dense) (None, 128) 147584</strong><br/><strong>_________________________________________________________________</strong><br/><strong>dense_2 (Dense) (None, 10) 1290</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 287,402</strong><br/><strong>Trainable params: 287,402</strong><br/><strong>Non-trainable params: 0</strong><br/>_________________________________________________________________</pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fashion-MNIST classifier training code</h1>
                </header>
            
            <article>
                
<p>In the section, we will see a classifier model on <kbd>Fashion-MNIST</kbd> dataset. This will take in input grayscale image and outputs one of the pre-defined 10 classes. In the following steps, we will build the model:</p>
<ol>
<li>First, we import the relevant libraries and modules:</li>
</ol>
<pre style="padding-left: 60px"><strong>import keras </strong><br/><strong>import keras.backend as K</strong><br/><strong>from keras.layers import Dense, Conv2D, Input, MaxPooling2D, Flatten</strong><br/><strong>from keras.models import Model</strong><br/><strong>from keras.datasets import fashion_mnist</strong><br/><strong>from keras.callbacks import ModelCheckpoint</strong></pre>
<ol start="2">
<li>&#160;We define the input height and width parameters to be used throughout, as well as other parameters. Here, an epoch defines one iteration over all of the data. So, the number of <kbd>epochs</kbd> means the total number of iterations over all of the data:&#160;</li>
</ol>
<pre style="padding-left: 60px"><strong># setup parameters</strong><br/><strong>batch_sz = 128  # batch size </strong><br/><strong>nb_class = 10  # target number of classes</strong><br/><strong>nb_epochs = 10 # training epochs</strong><br/><strong>img_h, img_w = 28, 28  # input dimensions</strong></pre>
<ol start="3">
<li>Let's download and prepare the dataset for training and validation. There is already an inbuilt function to do this in Keras:</li>
</ol>
<pre style="padding-left: 60px"><strong>def get_dataset():</strong><br/><strong>    """</strong><br/><strong>    Return processed and reshaped dataset for training</strong><br/><strong>    In this cases Fashion-mnist dataset.</strong><br/><strong>    """</strong><br/><strong>    # load mnist dataset</strong><br/><strong>    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()</strong><br/>    <br/><strong>    # test and train datasets</strong><br/><strong>    print("Nb Train:", x_train.shape[0], "Nb test:",x_test.shape[0])</strong><br/><strong>    x_train = x_train.reshape(x_train.shape[0], img_h, img_w, 1)</strong><br/><strong>    x_test = x_test.reshape(x_test.shape[0], img_h, img_w, 1)</strong><br/><strong>    in_shape = (img_h, img_w, 1)</strong><br/><br/><strong>    # normalize inputs</strong><br/><strong>    x_train = x_train.astype('float32')</strong><br/><strong>    x_test = x_test.astype('float32')</strong><br/><strong>    x_train /= 255.0</strong><br/><strong>    x_test /= 255.0</strong><br/><br/><strong>    # convert to one hot vectors </strong><br/><strong>    y_train = keras.utils.to_categorical(y_train, nb_class)</strong><br/><strong>    y_test = keras.utils.to_categorical(y_test, nb_class)</strong><br/><strong>    return x_train, x_test, y_train, y_test</strong><br/><br/><strong>x_train, x_test, y_train, y_test = get_dataset()</strong></pre>
<ol start="4">
<li>We will build the model using the wrapper convolution function defined earlier:</li>
</ol>
<pre style="padding-left: 60px"><strong>def conv3x3(input_x,nb_filters):</strong><br/><strong>    """</strong><br/><strong>    Wrapper around convolution layer</strong><br/><strong>    Inputs:</strong><br/><strong>        input_x: input layer / tensor</strong><br/><strong>        nb_filter: Number of filters for convolution</strong><br/><strong>    """</strong><br/><strong>    return Conv2D(nb_filters, kernel_size=(3,3), use_bias=False,</strong><br/><strong>               activation='relu', padding="same")(input_x)</strong><br/><br/><strong>def create_model(img_h=28, img_w=28):</strong><br/><strong>    """</strong><br/><strong>    Creates a CNN model for training. </strong><br/><strong>    Inputs: </strong><br/><strong>        img_h: input image height</strong><br/><strong>        img_w: input image width</strong><br/><strong>    Returns:</strong><br/><strong>        Model structure </strong><br/><strong>    """</strong><br/>    <br/><strong>    inputs = Input(shape=(img_h, img_w, 1))</strong><br/><br/><strong>    x = conv3x3(inputs, 32)</strong><br/><strong>    x = conv3x3(x, 32)</strong><br/><strong>    x = MaxPooling2D(pool_size=(2,2))(x) </strong><br/><strong>    x = conv3x3(x, 64)</strong><br/><strong>    x = conv3x3(x, 64)</strong><br/><strong>    x = MaxPooling2D(pool_size=(2,2))(x) </strong><br/><strong>    x = conv3x3(x, 128)</strong><br/><strong>    x = MaxPooling2D(pool_size=(2,2))(x) </strong><br/><strong>    x = Flatten()(x)</strong><br/><strong>    x = Dense(128, activation="relu")(x)</strong><br/><strong>    preds = Dense(nb_class, activation='softmax')(x)</strong><br/>    <br/><strong>    model = Model(inputs=inputs, outputs=preds)</strong><br/><strong>    print(model.summary())</strong><br/><strong>    return model</strong><br/><br/><strong>model = create_model()</strong></pre>
<ol start="5">
<li>Let's set up the <kbd>optimizer</kbd>, <kbd>loss</kbd> function, and <kbd>metrics</kbd> to evaluate our predictions:&#160;</li>
</ol>
<pre style="padding-left: 60px"><strong># setup optimizer, loss function and metrics for model</strong><br/><strong>model.compile(loss=keras.losses.categorical_crossentropy,</strong><br/><strong>              optimizer=keras.optimizers.Adam(),</strong><br/><strong>              metrics=['accuracy'])</strong></pre>
<ol start="6">
<li>This is optional if we would like to save our model after every epoch:&#160;</li>
</ol>
<pre style="padding-left: 60px"><strong># To save model after each epoch of training</strong><br/><strong>callback = ModelCheckpoint('mnist_cnn.h5')</strong></pre>
<ol start="7">
<li>Let's begin training our model:&#160;</li>
</ol>
<pre style="padding-left: 60px"><strong># start training</strong><br/><strong>model.fit(x_train, y_train,</strong><br/><strong>          batch_size=batch_sz,</strong><br/><strong>          epochs=nb_epochs,</strong><br/><strong>          verbose=1,</strong><br/><strong>          validation_data=(x_test, y_test), </strong><br/><strong>          callbacks=[callback])</strong></pre>
<ol start="8">
<li>The previous piece of code will run for a while if you are using the only CPU. After 10 epochs, it will say <kbd>val_acc= 0.92</kbd> (approximately). This means our trained model can perform with about 92% accuracy on unseen <kbd>Fashion-MNIST</kbd> data.</li>
<li>Once all epoch training finishes, final evaluation is computed as:</li>
</ol>
<pre style="padding-left: 60px"><strong># Evaluate and print accuracy</strong><br/><strong>score = model.evaluate(x_test, y_test, verbose=0)</strong><br/><strong>print('Test loss:', score[0])</strong><br/><strong>print('Test accuracy:', score[1])</strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Analysis of CNNs&#160;</h1>
                </header>
            
            <article>
                
<p>Research on different kinds of CNNs is still ongoing, and year after year we see improvements in accuracy for models on complex datasets. These improvements are in terms of both model structure and how to train these models effectively.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Popular CNN architectures</h1>
                </header>
            
            <article>
                
<p>In the recent few years, the following have become popular in various practical applications. In this section, we will see some of the popular architectures and how to load them in Keras.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">VGGNet</h1>
                </header>
            
            <article>
                
<p>This was introduced in 2014 by&#160;<span>Karen Simonyan and&#160;</span><span>Andrew Zisserman, in the paper</span> <em>Very&#160;Deep&#160;Convolution&#160;Networks&#160;for&#160;Large-Scale&#160;Image&#160;Recognition,</em><span>&#160;<a href="https://arxiv.org/abs/1409.1556" target="_blank">https://arxiv.org/abs/1409.1556</a>.</span></p>
<p><span>This was one of the initial papers that improved the performance of object classification models and was one of the top performing models in the <strong>Imagenet Large Scale Visual Recognition Challenge</strong> (<strong>ILSVRC</strong>) 2014, the dataset for this was introduced in <a href="prac-cv_ch02.html" target="_blank">Chapter 2</a>, <em>Libraries, Development Platform, and Datasets</em>. The performance gain was about 4% from the previous best model, and as a result, it became quite popular. There were several versions of the model but the most popular are VGG16 and VGG19. We can see a pretrained VGG16 model in Keras:</span></p>
<pre><strong>from keras.applications.vgg16 import VGG16</strong><br/><br/><strong>def print_model():</strong><br/><strong>    """</strong><br/><strong>    Loads VGGNet and prints model structure</strong><br/><strong>    """</strong><br/>    <br/><strong>    # create model </strong><br/><strong>    model = VGG16(weights='imagenet')</strong><br/><br/><strong>    # prints our model created</strong><br/><strong>    model.summary()</strong><br/><br/><strong>print_model()</strong></pre>
<p>On execution, we can see the output as follows:</p>
<pre>_________________________________________________________________<br/><strong>Layer (type) Output Shape Param #</strong><br/><strong>=================================================================</strong><br/><strong>input_1 (InputLayer) (None, 224, 224, 3) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block1_conv1 (Conv2D) (None, 224, 224, 64) 1792</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block1_conv2 (Conv2D) (None, 224, 224, 64) 36928</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block1_pool (MaxPooling2D) (None, 112, 112, 64) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block2_conv1 (Conv2D) (None, 112, 112, 128) 73856</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block2_conv2 (Conv2D) (None, 112, 112, 128) 147584</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block2_pool (MaxPooling2D) (None, 56, 56, 128) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block3_conv1 (Conv2D) (None, 56, 56, 256) 295168</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block3_conv2 (Conv2D) (None, 56, 56, 256) 590080</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block3_conv3 (Conv2D) (None, 56, 56, 256) 590080</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block3_pool (MaxPooling2D) (None, 28, 28, 256) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block4_pool (MaxPooling2D) (None, 14, 14, 512) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808</strong><br/><strong>_________________________________________________________________</strong><br/><strong>block5_pool (MaxPooling2D) (None, 7, 7, 512) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>flatten (Flatten) (None, 25088) 0</strong><br/><strong>_________________________________________________________________</strong><br/><strong>fc1 (Dense) (None, 4096) 102764544</strong><br/><strong>_________________________________________________________________</strong><br/><strong>fc2 (Dense) (None, 4096) 16781312</strong><br/><strong>_________________________________________________________________</strong><br/><strong>predictions (Dense) (None, 1000) 4097000</strong><br/><strong>=================================================================</strong><br/><strong>Total params: 138,357,544</strong><br/><strong>Trainable params: 138,357,544</strong><br/><strong>Non-trainable params: 0</strong><br/><strong>_________________________________</strong>________________________________</pre>
<p>Since the total number of parameters is quite large, training such a model from scratch will also require a huge amount of data of the order of a few hundred thousand.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Inception models</h1>
                </header>
            
            <article>
                
<p>These were successful in using parallel structures in the convolution network, which further increased the performance of models in the same competition. It was proposed and refined by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna in the paper <em>Rethinking the Inception Architecture for Computer Vision</em>, <a href="https://arxiv.org/abs/1512.00567" target="_blank">https://arxiv.org/abs/1512.00567</a>. The model structure for inception-v3 is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="531" width="1600" class="alignnone size-full wp-image-500 image-border" src="images/aeee76d5-2e68-41dc-9df7-335e88166a31.png"/></div>
<p>We can use this model in Keras:</p>
<pre><strong>from keras.applications.inception_v3 import InceptionV3</strong><br/><br/><strong>def print_model():</strong><br/><strong>    """</strong><br/><strong>    Loads InceptionV3 model and prints model structure</strong><br/><strong>    """</strong><br/>    <br/><strong>    # create model </strong><br/><strong>    model = InceptionV3(weights='imagenet')</strong><br/><br/><strong>    # prints our model created</strong><br/><strong>    model.summary()</strong><br/><br/><strong>print_model()</strong></pre>
<p>On execution, it will print out the model structure.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">ResNet model</h1>
                </header>
            
            <article>
                
<p>Extending more on parallel structure, Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. introduced&#160;<em>Deep Residual Learning for Image Recognition</em>&#160;&#160;<a href="https://arxiv.org/abs/1512.03385" target="_blank">https://arxiv.org/abs/1512.03385</a>&#160;that uses skip connection. The basic block of ResNet is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="images/1e122e62-bd2e-4b66-be3f-1a11c5028ec9.png"/></div>
<p>These blocks are repeated and stacked over to create a large network with a depth of 18 for 18 blocks, 50 for 50 blocks, and so on. They have shown remarkable performance both in terms of accuracy and computation time. In the following code, we will see how to use this to predict the top-5 probable categories from an image.&#160;</p>
<p>The input to the model is the following image of a train locomotive engine, which is any normal smartphone photo. We want to see if the pretrained ResNet-50 model can do close to ground truth predictions:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="354" width="531" src="images/31524fda-12fd-45ee-a27a-0e18c8547a6d.png"/></div>
<p>Let's load the required imports as:</p>
<pre>from keras.applications.resnet50 import ResNet50<br/>import numpy as np <br/>import cv2 <br/>from keras.applications.resnet50 import preprocess_input, decode_predictions<br/>import time</pre>
<p><span>We now begin creating a model to detect object in previously shown figure:</span></p>
<ol>
<li>The first thing we do is setup loading the ResNet-50 pretrained model:</li>
</ol>
<pre style="padding-left: 60px"><strong>def get_model():</strong><br/><strong>    """</strong><br/><strong>    Loads Resnet and prints model structure</strong><br/><strong>    Returns resnet as model.</strong><br/><strong>    """</strong><br/>    <br/><strong>    # create model </strong><br/><strong>    model = ResNet50(weights='imagenet')</strong><br/><br/><strong>    # To print our model loaded</strong><br/><strong>    model.summary()</strong><br/><strong>    return model</strong></pre>
<ol start="2">
<li>We need to preprocess the image for a specific input type for ResNet. In this case, the input is the mean, normalized to the size (1, 224, 224, 3):</li>
</ol>
<pre style="padding-left: 60px"><strong>def preprocess_img(img):</strong><br/><strong>    # apply opencv preprocessing</strong><br/><strong>    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</strong><br/><strong>    img = cv2.resize(img, (224, 224)) </strong><br/><strong>    img = img[np.newaxis, :, :, :]</strong><br/><strong>    # convert to float type</strong><br/><strong>    img = np.asarray(img, dtype=np.float) </strong><br/>    <br/><strong>    # further use imagenet specific preprocessing</strong><br/><strong>    # this applies color channel specific mean normalization</strong><br/><strong>    x = preprocess_input(img)</strong><br/><strong>    print(x.shape)</strong><br/><strong>    return x</strong></pre>
<ol start="3">
<li>Let's go ahead and load the image and apply preprocessing:</li>
</ol>
<pre style="padding-left: 60px"><strong># read input image and preprocess</strong><br/><strong>img = cv2.imread('../figures/train1.png')</strong><br/><strong>input_x = preprocess_img(img)</strong></pre>
<ol start="4">
<li>We will now load the model and pass the processed input through the trained model. This also computes the runtime:&#160;</li>
</ol>
<pre style="padding-left: 60px"><strong># create model with pretrained weights</strong><br/><strong>resnet_model = get_model()</strong><br/><br/><strong># run predictions only , no training</strong><br/><strong>start = time.time()</strong><br/><strong>preds = resnet_model.predict(input_x)</strong><br/><strong>print(time.time() - start)</strong></pre>
<ol start="5">
<li>We have got predictions, but these are just probability values and not class names. We will now print class names corresponding to only the top-5 probable predictions:</li>
</ol>
<pre style="padding-left: 60px"><strong># decode prediction to index of classes, top 5 predictions</strong><br/><strong>print('Predicted:', decode_predictions(preds, top=5)[0])</strong> </pre>
<p>The output for this is as follows:</p>
<pre><strong>Predicted: [('n04310018', 'steam_locomotive', 0.89800948), ('n03895866', 'passenger_car', 0.066653267), ('n03599486', 'jinrikisha', 0.0083348891), ('n03417042', 'garbage_truck', 0.0052676937), ('n04266014', 'space_shuttle', 0.0040852665)]</strong></pre>
<p><kbd>n04310018</kbd>&#160;and&#160;<kbd>steam_locomotive</kbd> are the class index and names. The value denoted after that is the probability for prediction. So, the pretrained model is 89% probable that the input image is a steam locomotive. This is quite impressive since the input image is of a locomotive which is not in service and has probably never been seen by the model during training.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Transfer learning</h1>
                </header>
            
            <article>
                
<p>In the previous section we saw three different model types, but in deep learning models, we are not limited to these. Every year there are better performing model architectures being published. However, the performance of these models totally depends on training data and their performance is due to the millions of images they are trained on. Getting such large datasets and training them for task specific purposes is not cost effective, as well as being time consuming. Nonetheless, the models can be used in various domains by doing a special type of training called <strong>transfer learning</strong>.</p>
<p>In transfer learning, we fix a part of a model from the input to a given layer (also known as <strong>freezing a model</strong>), such that the pretrained weights will help in computing rich features from the image. The remaining part is trained on task specific datasets. As a result, the remaining part of the model learns better features even with small datasets. The choice of how much of a model to freeze depends on available datasets and repeated experimentation.</p>
<p>Further, we will show a comparison of previous models, to understand better which model to use. The first plot is the number of parameters in each of the models. As the newer models were released, they became more efficient in terms of number of parameters to train:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="547" width="884" src="images/2fe55971-191c-4f37-a853-645e24a8f535.png"/></div>
<p>Also, we show the comparison of accuracy for the ILSVRC challenge across different years. This shows that the model gets better with less parameters and better model structures:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="377" width="611" src="images/6786a26f-d30e-43ad-8906-7f530bb70d5f.png"/></div>
<p>In this section, we saw that even lacking of large dataset for a particular task, we can still achieve good performance by transferring the learning from model trained on other similar dataset. In most practical applications, we use models trained on ImageNet dataset, however choice of model is, decided by the user, based on criteria such as more accuracy model or faster model.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we had an introduction to CNNs and their basic components. We also saw how to train a model from scratch on an example dataset. Later, we learnt to use pretrained models to perform prediction and also transfer learning to re-utilize trained models for our tasks.</p>
<p>These trained models and CNNs are not only used for image classification but also on more complex tasks like object detection and segmentation, as we will see in upcoming chapters. &#160;&#160;</p>
<p class="mce-root"></p>


            </article>

            
        </section>
    </div>
</body>
</html>