<html><head></head><body>
		<div id="_idContainer135">
			<h1 id="_idParaDest-138" class="chapter-number"><a id="_idTextAnchor205"/>7</h1>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor206"/>Data-Level Deep Learning Methods</h1>
			<p>You learned about various sampling methods in the previous chapters. Collectively, we call these <a id="_idIndexMarker491"/>methods <em class="italic">data-level methods</em> in this book. These methods include random undersampling, random oversampling, NearMiss, and SMOTE. We also explored how these methods work with classical machine <span class="No-Break">learning algorithms.</span></p>
			<p>In this chapter, we’ll explore how to apply familiar sampling methods to deep learning models. Deep learning offers unique opportunities to enhance these methods further. We’ll delve into elegant techniques to combine deep learning with oversampling and undersampling. Additionally, we’ll learn how to implement various sampling methods with a basic neural network. We’ll also cover dynamic sampling, which involves adjusting the data sample across multiple training iterations, using varying balancing ratios for each iteration. Then, we will learn to use some data augmentation techniques for both images and text. We’ll end the chapter by highlighting key takeaways from a variety of other <span class="No-Break">data-level techniques.</span></p>
			<p>We will cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li><span class="No-Break">Preparing data</span></li>
				<li>Sampling techniques for deep <span class="No-Break">learning models</span></li>
				<li>Data-level techniques for <span class="No-Break">text classification</span></li>
				<li>A discussion of other data-level deep learning methods and their <span class="No-Break">key ideas</span></li>
			</ul>
			<p>It is not always straightforward to port methods to handle data imbalance, which worked well on classical ML models, into the deep learning world. The challenges and opportunities of deep learning models differ from the classical ML models primarily because of the difference in the type of data these models have to deal with. While classical ML models mostly deal with tabular and structured data, deep learning models typically deal with unstructured data, such as images, text, audio, and video, which is fundamentally different from <span class="No-Break">tabular data.</span></p>
			<p>We will discuss various techniques to deal with imbalance problems in computer vision. In the first part of the chapter, we will focus on various techniques, such as sampling and data augmentation, to handle class imbalance when training convolutional neural networks on image and <span class="No-Break">text data.</span></p>
			<p>In the latter half of the chapter, we will discuss common data-level techniques that can be applied to text problems. A lot of computer vision techniques can be successfully applied<a id="_idIndexMarker492"/> to NLP <span class="No-Break">problems, too.</span></p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor207"/>Technical requirements</h1>
			<p>Similar to prior chapters, we will continue to utilize common libraries such as <strong class="source-inline">torch</strong>, <strong class="source-inline">torchvision</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">scikit-learn</strong>. We will also use <strong class="source-inline">nlpaug</strong> for NLP-related functionalities. The code and notebooks for this chapter are available on GitHub at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07">https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07</a>. You can open the GitHub notebooks using Google Colab by clicking on the <strong class="bold">Open in Colab</strong> icon at the top of the chapter’s notebook, or by launching it from <a href="https://colab.research.google.com">https://colab.research.google.com</a>, using the GitHub URL of <span class="No-Break">the notebook.</span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor208"/>Preparing the data</h1>
			<p>In this chapter, we <a id="_idIndexMarker493"/>are going to use the classic MNIST dataset. This dataset contains 28-pixel x 28-pixel images of handwritten digits. The task for the model is to take an image as input and identify the digit in the image. We will use <strong class="source-inline">PyTorch</strong>, a popular deep-learning library, to demonstrate the algorithms. Let’s prepare the <span class="No-Break">data now.</span></p>
			<p>The first step in the process will be to import the libraries. We will need NumPy (as we deal with <strong class="source-inline">numpy</strong> arrays), <strong class="source-inline">torchvision</strong> (to load MNIST data), <strong class="source-inline">torch</strong>, <strong class="source-inline">random</strong>, and <span class="No-Break"><strong class="source-inline">copy</strong></span><span class="No-Break"> libraries.</span></p>
			<p>Next, we can download the MNIST data from <strong class="source-inline">torchvision.datasets</strong>. The <strong class="source-inline">torchvision</strong> library is a part of the <strong class="source-inline">PyTorch</strong> framework, which contains datasets, models, and common image transformers for computer vision tasks. The following code will download the MNIST dataset from <span class="No-Break">this library:</span></p>
			<pre class="source-code">
img_transform = torchvision.transforms.ToTensor()
trainset = torchvision.datasets.MNIST<a id="_idTextAnchor209"/>(\
    root='/tmp/mnist', train=True,\
    download=True, transform=img_transform)
testset = torchvision.datasets.MNIST(root='/tmp/mnist',\
    train=False, transform=img_transform)</pre>			<p>Once the data is downloaded from <strong class="source-inline">torchvision</strong>, we can load it into the <strong class="source-inline">Dataloader</strong> utility of <strong class="source-inline">PyTorch</strong>, which creates batches of data and provides us with a Python-style iterator over the batches. The following code does exactly that. Here, we are creating batches of size 64 <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">train_loader</strong></span><span class="No-Break">.</span></p>
			<pre class="source-code">
train_loader = torch.utils.data.DataLoader(trainset,\
    batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=testset,\
    batch_size=500)</pre>			<p>Since we are <a id="_idIndexMarker494"/>interested in imbalanced datasets, we will convert our MNIST dataset, which is a balanced dataset, into a long-tailed version of itself by deleting examples from various classes. We are omitting that implementation here to save space; you can refer to the chapter’s GitHub repository for details. We assume that you have the <strong class="source-inline">imbalanced_train_loader</strong> class created from the <span class="No-Break">imbalanced trainset.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> shows the distribution of samples in the imbalanced <span class="No-Break">MNIST dataset:</span></p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B17259_07_01.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – A bar chart of the counts of examples from each digit class</p>
			<p>Next, we will learn <a id="_idIndexMarker495"/>to create a training loop <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">PyTorch</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor210"/>Creating the training loop</h2>
			<p>Before creating<a id="_idIndexMarker496"/> the training loop, we should import the <strong class="source-inline">torch.nn</strong> and <strong class="source-inline">torch.optim</strong> packages. The <strong class="source-inline">torch.nn</strong> package provides all the building blocks to create a neural network graph, while the <strong class="source-inline">torch.optim</strong> package provides us with most of the common <span class="No-Break">optimization algorithms.</span></p>
			<pre class="source-code">
import torch.nn as nn
import torch.optim as optim</pre>			<p>Since we will need some hyperparameters, let’s <span class="No-Break">define them:</span></p>
			<pre class="source-code">
input_size = 28 * 28 # 784
num_classes = 10
num_epochs = 20
learning_rate = 0.01</pre>			<p>After setting up the hyperparameters, we can define the <strong class="source-inline">train</strong> function, which will take PyTorch’s data loader as input and return a model fitted to the data. To create a trained model, we will need a model, a loss criterion, and an optimizer. We will use a single-layer linear neural network as the model here. You can design your own neural network architecture based on your requirements. For the loss criterion, we will use <strong class="source-inline">CrossEntropyLoss</strong>, and we will<a id="_idIndexMarker497"/> use <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>) as <span class="No-Break">the optimizer.</span></p>
			<p>We will train the model for <strong class="source-inline">num_epochs</strong> epochs. We will discuss how the model is trained during a single epoch in the next paragraph. For now, we will abstract that part out in the <span class="No-Break"><strong class="source-inline">run_epoch</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
def train(trainloader):
    model = nn.Linear(input_size, num_classes)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), learning_rate)
    for epoch in range(num_epochs):
        run_epoch(trainloader, model, criterion, optimizer, \
            total_step, epoch)
    return model</pre>			<p>During every epoch, we <a id="_idIndexMarker498"/>will train our model over the whole training data once. As discussed earlier, dataloader divides the data into multiple batches. First, we will have to match the shape of the images in the batch with the input dimension of our model. We will take the current batch and do a forward pass over the model, calculating the predictions and loss over the predictions in one go. Then, we will backpropagate the loss to update the <span class="No-Break">model weights:</span></p>
			<pre class="source-code">
def run_epoch(
    trainloader, model, criterion, optimizer, total_step, epoch
):
    for i, (images, labels) in enumerate(trainloader):
        # Reshape images to (batch_size, input_size)
        images = images.reshape(-1, input_size)
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, torch.tensor(labels))
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</pre>			<p>To get a trained model, we can now send the data loader to the <span class="No-Break"><strong class="source-inline">train</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
model = train(imbalanced_train_l<a id="_idTextAnchor211"/>oader)</pre>			<p>For all image-related methods in this chapter, we’ll employ the model code detailed next. We will create a <strong class="source-inline">PyTorch</strong> neural network named <strong class="source-inline">Net</strong> that features two convolutional layers, a dropout mechanism, and a pair of fully connected layers. Through the <strong class="source-inline">forward</strong> function, the<a id="_idIndexMarker499"/> model seamlessly integrates these layers using <strong class="source-inline">ReLU</strong> activations and max-pooling, manipulating the input, <strong class="source-inline">x</strong>. The result is <strong class="source-inline">log_softmax</strong> of the <span class="No-Break">computed output:</span></p>
			<pre class="source-code">
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = torch.nn.Dropout2d()
        self.fc1 = torch.nn.Linear(320, 50)
        self.fc2 = torch.nn.Linear(50, 10)
    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(
            self.conv2_drop(self.conv2(x)),2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)</pre>			<p>Next, let’s break down some of <span class="No-Break">these terms:</span></p>
			<ul>
				<li><strong class="bold">Conv2d Layers (convolutional layers)</strong>: These are the primary layers you’d find in<a id="_idIndexMarker500"/> a <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>), which is especially popular for image data. The main function of these layers is to scan the input data (such as an image) with a filter to detect patterns (such as edges or textures). In our <strong class="source-inline">Net</strong> class, there are two such layers – <strong class="source-inline">conv1</strong> and <strong class="source-inline">conv2</strong>. The numbers <strong class="source-inline">(1, 10)</strong> and <strong class="source-inline">(10, 20)</strong> are simply the input and output channels. The term <strong class="source-inline">kernel_size=5</strong> means that a 5 x 5 grid (or filter) is used to scan <span class="No-Break">the input.</span></li>
				<li><strong class="bold">Dropout2d (Dropout for 2D Data)</strong>: Dropout is a technique to prevent overfitting (a scenario where our model performs exceptionally well on training data but poorly <a id="_idIndexMarker501"/>on unseen data). By “dropping out” or turning off certain neurons during training, the model becomes more robust. <strong class="source-inline">conv2_drop</strong> is a dropout layer of type <strong class="source-inline">Dropout2d</strong>, specifically designed for 2D data (such <span class="No-Break">as images).</span></li>
				<li><strong class="bold">Linear layers</strong>: These are fully connected layers where each neuron is connected to every neuron in the previous layer. Our class has two linear layers, <strong class="source-inline">fc1</strong> and <strong class="source-inline">fc2</strong>, which further process the patterns recognized by the convolutional layers to <span class="No-Break">make predictions.</span></li>
				<li><strong class="bold">Activation and </strong><span class="No-Break"><strong class="bold">pooling functions</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">F.relu</strong> is an activation function that introduces non-linearity to the model, enabling it to learn <span class="No-Break">complex patterns</span></li><li><strong class="source-inline">F.max_pool2d</strong> is a pooling function that reduces the spatial dimensions of the data while retaining <span class="No-Break">important features</span></li><li>Finally, <strong class="source-inline">F.log_softmax</strong> is an activation function commonly used for classification tasks to produce probabilities for <span class="No-Break">each class</span></li></ul></li>
			</ul>
			<p>In essence, the <strong class="source-inline">Net</strong> class defines a neural network that first detects patterns in data using convolutional layers, reduces overfitting using dropout, and then makes predictions using fully connected layers. The forward method is a sequence of operations that<a id="_idIndexMarker502"/> define how data flows through <span class="No-Break">this network.</span></p>
			<p>In the next section, we will learn how to use the <strong class="source-inline">train</strong> function with <span class="No-Break">oversampling methods.</span></p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor212"/>Sampling techniques for deep learning models</h1>
			<p>In this section, we’ll explore<a id="_idIndexMarker503"/> some sampling methods, such as random oversampling and weighted sampling, for deep learning models. We’ll then transition into data augmentation techniques, which bolster model robustness and mitigate dataset limitations. While large datasets are ideal for deep learning, real-world constraints often make them hard to obtain. We will also look at some advanced augmentations, such as CutMix and MixUp. We’ll start with standard methods before discussing these <span class="No-Break">advanced techniques.</span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor213"/>Random oversampling</h2>
			<p>Here, we will apply <a id="_idIndexMarker504"/>the plain<a id="_idIndexMarker505"/> old random oversampling we learned in <a href="B17259_02.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Oversampling Methods</em>, but using image data as input to a neural network. The basic idea is to duplicate samples from the minority classes randomly until we end up with an equal number of samples from each class. This technique often performs better than <span class="No-Break">no sampling.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Make sure to train the model for enough epochs so that it has fully been fitted to the data. Under-training will likely lead to suboptimal <span class="No-Break">model performance.</span></p>
			<p>Let’s spend some time working with the code. There are a few simple steps we need to follow. First, we need to convert data from the data loaders into tensors. Our <strong class="source-inline">RandomOverSampler</strong> API from <strong class="source-inline">imbalanced-learn</strong> doesn’t work directly with <span class="No-Break">data loaders:</span></p>
			<pre class="source-code">
X=torch.stack(tuple(imbalanced_train_loader.dataset.data))
y=torch.tensor(imbalanced_train_loader.dataset.targets)</pre>			<p>We also need to reshape the <strong class="source-inline">X</strong> tensor for <strong class="source-inline">RandomOverSampler</strong> to work with two-dimensional inputs, as each of our images is a 28 x <span class="No-Break">28 matrix:</span></p>
			<pre class="source-code">
reshaped_X = X.reshape(X.shape[0],-1)</pre>			<p>Now, we can import the <strong class="source-inline">RandomOverSampler</strong> class from the <strong class="source-inline">imbalanced-learn</strong> library, define an <strong class="source-inline">oversampler</strong> object, and resample our data <span class="No-Break">using it:</span></p>
			<pre class="source-code">
from imblearn.over_sampling import RandomOverSampler
oversampler = RandomOverSampler()
oversampled_X, oversampled_y = oversampler.fit_resample(reshaped_X, y)</pre>			<p>After resampling the data, we need to reshape it again back to the <span class="No-Break">original form:</span></p>
			<pre class="source-code">
oversampled_X = oversampled_X.reshape(-1,28,28)</pre>			<p>We can now create <a id="_idIndexMarker506"/>a new data<a id="_idIndexMarker507"/> loader using the <span class="No-Break">oversampled data:</span></p>
			<pre class="source-code">
balanced_train_dataset = copy.deepcopy(imbalanced_train_dataset)
balanced_train_dataset.targets = torch.from_numpy(oversampled_y)
balanced_train_dataset.data = torch.from_numpy(oversampled_X)
balanced_train_loader = torch.utils.data.DataLoader( \
    balanced_train_dataset, batch_size=100, shuffle=True)</pre>			<p>Finally, we can train our model using the new data loader. For this step, we can use the <strong class="source-inline">train</strong> function defined in the <span class="No-Break">previous section:</span></p>
			<pre class="source-code">
balanced_data_model = train(balanced_train_loader)</pre>			<p>That’s all we need to do to use the random oversampling technique with deep <span class="No-Break">learning models.</span></p>
			<p>A similar strategy can be used for <strong class="source-inline">RandomUnderSampling</strong> from the <span class="No-Break"><strong class="source-inline">imbalanced-learn</strong></span><span class="No-Break"> library.</span></p>
			<p><strong class="source-inline">PyTorch</strong> provides a <strong class="source-inline">WeightedRandomSampler</strong> API, which is similar to the <strong class="source-inline">sample_weight</strong> parameter from <strong class="source-inline">scikit-learn</strong> (found in many of the fit methods in <strong class="source-inline">scikit-learn</strong> estimators (such as <strong class="source-inline">RandomForestClassifier</strong> and <strong class="source-inline">LogisticRegression</strong>) and serves a similar purpose of assigning a weight to each sample of the training dataset. We had a detailed discussion of the differences between <strong class="source-inline">class_weight</strong> and <strong class="source-inline">sample_weight</strong> in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Cost-Sensitive Learning</em></span><span class="No-Break">.</span></p>
			<p>We can specify <strong class="source-inline">weights</strong> as a parameter to <strong class="source-inline">WeightedRamdomSampler</strong> so that it can automatically weigh the examples in the batch, according to the weight of each sample. The <strong class="source-inline">weights</strong> parameter <a id="_idIndexMarker508"/>values<a id="_idIndexMarker509"/> are typically the inverse of the frequency of various classes in <span class="No-Break">the dataset:</span></p>
			<pre class="source-code">
class_counts = pd.Series(\
    imbalanced_train_loader.dataset.targets.numpy()).value_counts()
class_weights = 1.0/class_counts</pre>			<p><strong class="source-inline">class_weights</strong> is more for the minority class labels than the majority class labels. Let’s compute the <span class="No-Break"><strong class="source-inline">weightedRamdomSampler</strong></span><span class="No-Break"> values:</span></p>
			<pre class="source-code">
weightedRandomSampler = \
    WeightedRandomSampler(weights=class_weights, \
    num_samples=len(imbalanced_train_dataset), \
    replacement=True)
weightedRandomSampler_dataloader = \
    torch.utils.data.DataLoader(imbalanced_train_dataset,\
    sampler=weightedRandomSample<a id="_idTextAnchor214"/>r, batch_size=64)</pre>			<p>In the next section, we <a id="_idIndexMarker510"/>will learn <a id="_idIndexMarker511"/>how to sample <span class="No-Break">data dynamically.</span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor215"/>Dynamic sampling</h2>
			<p>Dynamic <a id="_idIndexMarker512"/>sampling [1] is an advanced<a id="_idIndexMarker513"/> technique that can self-adjust the sampling rate as training progresses. It promises to adapt according to the problem’s complexity and class imbalance, with almost no hyperparameter tuning. It is just one more tool in your arsenal to try on your dataset, especially when you have imbalanced image data at hand, and see whether it gives a better performance than the other techniques we’ve discussed so far in <span class="No-Break">this chapter.</span></p>
			<p>The basic idea of dynamic sampling is to dynamically adjust the sampling rate for various classes, depending on whether they are doing well or worse in a particular training iteration when compared to the prior iteration. If a class is performing comparatively poorly, then the class is oversampled in the next iteration, and <span class="No-Break">vice versa.</span></p>
			<h3>The details of the algorithm</h3>
			<p>These<a id="_idIndexMarker514"/> are the core components of <span class="No-Break">dynamic sampling:</span></p>
			<ul>
				<li><strong class="bold">Real-time data augmentation</strong>: Apply<a id="_idIndexMarker515"/> various kinds of image transformations to the images of each training batch. These transformations can be rotation, flipping, adjusting brightness, translation, adjusting contrast/color, noise injection, and so on. As discussed earlier, this step helps to reduce model overfitting and <span class="No-Break">improves generalization.</span></li>
				<li><strong class="bold">Dynamic sampling method</strong>: In each iteration, a sample size (given by a certain formula) is chosen, and a model is trained with that sample size. The classes with lower F1 scores are sampled at a higher rate in the next iteration, forcing the model to focus more on previously misclassified examples. The number of images, <span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span>, for the next iteration is updated according to the <span class="No-Break">following formula:</span><p class="list-inset"><span class="_-----MathTools-_Math_Variable">UpdateSampleSize</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">N</span></p><p class="list-inset"><span class="No-Break">Here:</span></p><ul><li><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Variable">j</span> is the F1-score of class<span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span> in <span class="No-Break">iteration </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span></li><li><span class="_-----MathTools-_Math_Variable">N</span> = the average number of samples in <span class="No-Break">all classes</span></li></ul></li>
			</ul>
			<p>For a particular training epoch, let’s say we got the following F1 score for each of the three classes, <strong class="bold">A</strong>, <strong class="bold">B</strong>, and <strong class="bold">C</strong>, on our <span class="No-Break">validation dataset:</span></p>
			<table id="table001-6" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Class</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">F1 score</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>A</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.1</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>B</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.2</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>C</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.3</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 7.1 – Sample F1-scores of each class after some epoch</p>
			<p>Here is how we would<a id="_idIndexMarker516"/> compute the weight of each class for the next epoch <span class="No-Break">of training:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">Weight</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">class</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Base">______________________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">0.9</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.9</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">0.8</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">0.7</span><span class="_-----MathTools-_Math_Number"> </span></p>
			<p><span class="_-----MathTools-_Math_Space">                                                   </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0.375</span></span></p>
			<p><em class="italic">Weight (class B) = N* 0.8/2.4 = </em><span class="No-Break"><em class="italic">N*0.33</em></span></p>
			<p><em class="italic">Weight (class C) = N* 0.7/2.4 = </em><span class="No-Break"><em class="italic">N* 0.29</em></span></p>
			<p>This means that we will sample class A at a higher rate than class B and class C, which makes sense because the performance on class A was weaker than that on classes B <span class="No-Break">and C.</span></p>
			<p>A second model is trained through transfer learning, without sampling, to prevent the minority classes from overfitting. At inference time, the model output is a function of <span class="No-Break">both models.</span></p>
			<p>There are additional details<a id="_idIndexMarker517"/> about the <strong class="bold">DynamicSampling</strong> algorithm that we have omitted here due to space constraints. You can find the complete implementation code in the corresponding GitHub repository for <span class="No-Break">this chapter.</span></p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B17259_07_02.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – An overall model accuracy comparison of various sampling techniques</p>
			<p><em class="italic">Table 7.2</em> shows the<a id="_idIndexMarker518"/> per-class model accuracy using various sampling techniques, including the baseline, where no sampling <span class="No-Break">is done.</span></p>
			<table id="table002-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor216"/></strong><span class="No-Break"><strong class="bold">Class</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Baseline</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Weighted </strong><span class="No-Break"><strong class="bold">random sampler</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Dynamic </strong><span class="No-Break"><strong class="bold">sampler</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Random </strong><span class="No-Break"><strong class="bold">oversampling</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Random </strong><span class="No-Break"><strong class="bold">undersampling</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">99.9</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">99.0</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">92.4</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">99.1</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.2</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">99.7</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">99.2</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">96.8</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">99.2</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">90.7</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">98.5</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">98.3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">93.5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">98.5</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">70.8</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.4</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">96.8</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">98.3</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">74.4</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">98.3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">98.0</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">91.9</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">98.6</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">79.6</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">96.2</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">96.0</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">98.1</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">52.8</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>6</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">94.5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">98.7</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">77.6</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>7</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">89.7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">94.7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">96.5</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">94.1</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">81.1</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>8</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">63.3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">91.5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">96.9</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">93.0</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">60.2</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>9</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">50.7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">92.6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">97.7</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">91.8</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">56.5</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 7.2 – A per-class model accuracy comparison of various sampling techniques (the highest value for a class is in bold)</p>
			<p>Here are some <a id="_idIndexMarker519"/>insights from <span class="No-Break">the results:</span></p>
			<ul>
				<li>In terms of overall<a id="_idIndexMarker520"/> performance, <strong class="bold">Random OverSampling</strong> (<strong class="bold">ROS</strong>) performed the best, while <strong class="bold">Random Undersampling</strong> (<strong class="bold">RUS</strong>) did <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker521"/></span><span class="No-Break"> worst.</span></li>
				<li>Although ROS did the best, it can be computationally very expensive due to data cloning, making it less suitable for large datasets and <span class="No-Break">industrial settings.</span></li>
				<li>Dynamic sampling did a little worse than ROS; it did best on the minority classes 6–9 and would be our preferred choice here. However, due to its increased complexity, our second choice will be the weighted <span class="No-Break">random sampler.</span></li>
				<li>The baseline and weighted random sampler techniques are stable across classes; RUS is notably variable and performs poorly on <span class="No-Break">most classes.</span></li>
			</ul>
			<p>Similarly, we can apply SMOTE in the same way as <strong class="source-inline">RandomOverSampler</strong>. Please note that while SMOTE can be applied to images, its use of a linear subspace of the original data is <span class="No-Break">often limiting.</span></p>
			<p>This ends our discussion of the various sampling techniques. In the next section, we will focus on data <a id="_idIndexMarker522"/>augmentation techniques specifically designed for images to achieve more <span class="No-Break">effective oversampling.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor217"/>Data augmentation techniques for vision</h2>
			<p>Today, a variety of <a id="_idIndexMarker523"/>custom <a id="_idIndexMarker524"/>augmentation techniques are used for various kinds of data, such as images, audio, video, and even text data. In the vision realm, this includes techniques such as rotating, scaling, cropping, blurring, adding noise to an image, and a host of other techniques, including combining those techniques all at once in some appropriate sequence. Image data augmentation is not really a recent innovation. Some image augmentation techniques can also be found in the LeNet-5 model paper [2] from 1998, for example. Similarly, the AlexNet model [3] from 2012 uses random cropping, flipping, changing the color intensity of RGB channels, and so on to reduce errors during <span class="No-Break">model training.</span></p>
			<p>Let’s discuss why data augmentation can often <span class="No-Break">be helpful:</span></p>
			<ul>
				<li>In problems where we have limited data or imbalanced data, it may not always be possible to gather more data. This could be because either gathering more data is difficult in the first place (for example, waiting for more fraud to occur when dealing with credit card fraud, or gathering satellite images where we have to pay satellite operators, which can be quite expensive) or labeling the data is difficult or expensive (for example, to label medical image datasets, we need <span class="No-Break">domain experts).</span></li>
				<li>Data augmentation can help reduce overfitting and improve the overall performance of a model. One motivation for this practice is that attributes such as lighting, noise, color, scale, and focus in the training set may not align with those in the real-world images on which we run inference. Additionally, augmentation diversifies a dataset to help the model generalize better. For example, if the model is trained only on images of cats facing right, it may not perform well on images where the cat faces left. Therefore, it’s advisable to always apply valid transformations to augment image datasets, as most models gain performance with more <span class="No-Break">diverse data.</span></li>
			</ul>
			<p>Data augmentation is widely used for computer vision tasks such as object detection, classification, and segmentation. It can be very useful for NLP tasks as well. In the computer vision world, there are lots of open source libraries that help standardize the various image augmentation techniques, while the NLP tools for data augmentation space have yet <span class="No-Break">to mature.</span></p>
			<p>While there are several popular open source libraries for image augmentation, such as <strong class="source-inline">imgaug</strong>, Facebook’s <strong class="source-inline">AugLy</strong>, and <strong class="source-inline">Albumentations</strong>, we will use <strong class="source-inline">torchvision</strong> in this book. As a part of the PyTorch ecosystem, it offers seamless integration with PyTorch workflows, a range of common image transformations, as well as pre-trained models and datasets, making it a convenient and comprehensive choice for computer vision tasks. If you need more advanced augmentations, or if speed is a concern, <strong class="source-inline">Albumentations</strong> <a id="_idIndexMarker525"/>may be a <a id="_idIndexMarker526"/><span class="No-Break">better choice.</span></p>
			<p>We can use <strong class="source-inline">torchvision.transforms.Pad</strong> to add some padding to the <span class="No-Break">image boundaries:</span></p>
			<pre class="source-code">
padded_imgs = [torchvision.transforms.Pad(padding=90)(orig_img)]
plot(padded_imgs)</pre>			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B17259_07_03.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – The results of applying the Pad function to the image</p>
			<p>The <strong class="source-inline">torchvision.transforms.FiveCrop</strong> class transforms and crops the given image into four corners and the <span class="No-Break">central crop:</span></p>
			<pre class="source-code">
(top_left, top_right, bottom_left, bottom_right, center) =\
    torchvision.transforms.FiveCrop(size=(100,100))(orig_img)
plot([top_left, top_right, bottom_left, bottom_right, center])</pre>			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B17259_07_04.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – The results of applying the FiveCrop function on the image</p>
			<p><strong class="source-inline">torchvision.transforms.CenterCrop</strong> is a similar class to crop images from <span class="No-Break">the center.</span></p>
			<p>The <strong class="source-inline">torchvision.transforms.ColorJitter</strong> class changes the brightness, saturation, and other similar properties of <span class="No-Break">the image:</span></p>
			<pre class="source-code">
jitter = torchvision.transforms.ColorJitter(brightness=.7, hue=.5)
jitted_imgs = [jitter(orig_img) for _ in range(3)]
plot(jitted_imgs)</pre>			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B17259_07_05.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – The results of applying the ColorJitter function on the image</p>
			<p><strong class="source-inline">GaussianBlur</strong> can add <a id="_idIndexMarker527"/>some <a id="_idIndexMarker528"/>blurring to <span class="No-Break">the images:</span></p>
			<pre class="source-code">
gaussian_blurrer = \
    torchvision.transforms.GaussianBlur(kernel_size=(9,\
    11), sigma=(0.1, 5))
blurred_imgs = [gaussian_blurrer(orig_img) for _ in \
    range(4)]
plot(blurred_imgs)</pre>			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B17259_07_06.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – The results of applying the GaussianBlur function on the image</p>
			<p>The <strong class="source-inline">torchvision.transforms.RandomRotation</strong> class transform rotates an image at a <span class="No-Break">random angle:</span></p>
			<pre class="source-code">
rotater = torchvision.transforms.RandomRotation(degrees=(0, 50))
rotated_imgs = [rotater(orig_img) for _ in range(4)]
plot(rotated_imgs)</pre>			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B17259_07_07.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – The results of random rotation on the original image (leftmost)</p>
			<p>Consider exploring the<a id="_idIndexMarker529"/> other <a id="_idIndexMarker530"/>image transformation functionalities supported by the <strong class="source-inline">torchvision.transforms</strong> class that we didn’t <span class="No-Break">discuss here.</span></p>
			<p>Cutout masks out random square regions of input images during training. While it may seem like this technique removes unnecessary portions of the image, it’s important to note that the areas to be masked are typically selected at random. The primary aim is to force the neural network to generalize better by ensuring it does not overly rely on any specific set of pixels within a <span class="No-Break">given image.</span></p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B17259_07_08.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – The result of applying the cutout function on an image</p>
			<p class="callout-heading">🚀 Deep learning data-level techniques in production at Etsy/Booking/Wayfair</p>
			<p class="callout"><strong class="bold">🎯</strong><strong class="bold"> Problem </strong><span class="No-Break"><strong class="bold">being solved:</strong></span></p>
			<p class="callout">Etsy, <a href="http://Booking.com">Booking.com</a>, and Wayfair leveraged user behavior to enhance personalization. Etsy focused on item recommendations based on browsing history [4], <a href="http://Booking.com">Booking.com</a> tailored search results to boost bookings [5], and Wayfair optimized product image angles to improve CTRs [6]. All aimed to utilize data-driven strategies for better user experience <span class="No-Break">and performance.</span></p>
			<p class="callout"><strong class="bold">⚖️</strong><strong class="bold"> Data </strong><span class="No-Break"><strong class="bold">imbalance issue:</strong></span></p>
			<p class="callout">Etsy, <a href="http://Booking.com">Booking.com</a>, and Wayfair each grappled with data imbalance issues in their machine learning projects. Etsy faced a power law distribution in user sessions, where most users interacted with only a few listings within a one-hour window. <a href="http://Booking.com">Booking.com</a> dealt with imbalanced classes in hotel images, as photos of bedrooms and bathrooms vastly outnumbered those of other facilities such as saunas or table tennis. Wayfair encountered an imbalance in real-world images of furniture, with a majority of images showing the “front” angle, leading to poor performance for other angles. All three companies had to address these imbalances to improve the performance and fairness of <span class="No-Break">their models.</span></p>
			<p class="callout"><strong class="bold">🎨</strong><strong class="bold"> Data </strong><span class="No-Break"><strong class="bold">augmentation strategy:</strong></span></p>
			<p class="callout">Etsy, <a href="http://Booking.com">Booking.com</a>, and Wayfair each had unique data augmentation strategies to address their specific challenges. Etsy used image random rotation, translation, zoom, and color contrast transformation to augment their dataset. <a href="http://Booking.com">Booking.com</a> employed a variety of techniques, including mirroring, random cropping, affine transformation, aspect ratio distortion, color manipulation, and contrast enhancement. They increased their labeled data by 10 times through these methods, applying distortions on the fly during training. Wayfair took a different approach by creating synthetic data with 3D models, generating 100 views for each 3D model of chairs and sofas, thus providing granular angle information <span class="No-Break">for training.</span></p>
			<p>Next, let’s look at some of the more advanced techniques, such as CutMix, MixUp, and AugMix, which are <a id="_idIndexMarker531"/>types of <strong class="bold">Mixed Sample Data Augmentation</strong> (<span class="No-Break"><strong class="bold">MSDA</strong></span><span class="No-Break">) techniques.</span></p>
			<p>MSDA is a set of<a id="_idIndexMarker532"/> techniques <a id="_idIndexMarker533"/>that involve mixing data samples to produce an augmented dataset, used to train a model (<span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">).</span></p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B17259_07_09.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Common MSDA techniques</p>
			<h3>CutMix</h3>
			<p>CutMix [7] is an<a id="_idIndexMarker534"/> image data augmentation technique where patches are cut and <a id="_idIndexMarker535"/>pasted among training images. Specifically, a portion of an image is replaced by a portion of another image, as <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B17259_07_10.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – The result of applying the CutMix function to the images (image 1 and image 2)</p>
			<p>It’s designed to encourage a model to make more localized, fine-grained predictions, thus improving overall generalization. CutMix also enforces consistent predictions outside the mixed regions, further enhancing <span class="No-Break">model robustness.</span></p>
			<p><strong class="source-inline">torchvision</strong> also offers an in-built API for CutMix, <strong class="source-inline">torchvision.transforms.v2.CutMix</strong>, so we don’t have to implement it <span class="No-Break">from scratch.</span></p>
			<p>The full notebook with CutMix implementation from scratch can be found in the <span class="No-Break">GitHub repo.</span></p>
			<p>CutMix often shows<a id="_idIndexMarker536"/> improvements over traditional augmentation techniques on benchmark datasets, such as CIFAR-10, CIFAR-100, and <span class="No-Break">ImageNet [7].</span></p>
			<h3>MixUp</h3>
			<p>MixUp [8] creates<a id="_idIndexMarker537"/> virtual training examples<a id="_idIndexMarker538"/> by forming combinations of pairs of inputs and their <span class="No-Break">corresponding<a id="_idTextAnchor218"/> labels.</span></p>
			<p>If (<span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span>, <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span>) and (<span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span>, <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span>) is an arbitrary pair of images in dataset D, where <span class="_-----MathTools-_Math_Variable">x</span> is the image while <span class="_-----MathTools-_Math_Variable">y</span> is its label, a mixed sample <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">~</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable"> </span>, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">~</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span> can be generated using the <span class="No-Break">following equations:</span></p>
			<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">~</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span></p>
			<p><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">~</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span></p>
			<p>where <span class="_-----MathTools-_Math_Variable">λ</span> is the mixing factor sampled from the beta distribution. The Beta distribution is a flexible, continuous probability distribution defined on the interval [<span class="No-Break">0, 1].</span></p>
			<p>MixUp acts as a regularizer, preventing overfitting and enhancing the generalization capabilities of models. The following implementation shuffles the data and targets, and then combines them using a weighted average, based on a value sampled from the Beta distribution, creating mixed <a id="_idIndexMarker539"/>data and targets <span class="No-Break">for </span><span class="No-Break"><a id="_idIndexMarker540"/></span><span class="No-Break">augmentation:</span></p>
			<pre class="source-code">
def mixup(data, target, alpha):
    indices = torch.randperm(data.size(0))
    shuffled_data = data[indices]
    shuffled_target = target[indices]
    lamda = np.random.beta(alpha, alpha)
    data = lamda * data + (1 - lamda) * shuffled_data
    target = lamda * target + (1 - lamda) * shuffled_target
    return data, target</pre>			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B17259_07_11.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – The result of applying MixUp on the images (image 1 and image 2)</p>
			<p>On datasets such as CIFAR-100, MixUp has been found to provide significant gains in test accuracy compared to models trained without <span class="No-Break">MixUp [8].</span></p>
			<p>Similar to CutMix, <strong class="source-inline">torchvision</strong> provides a built-in API called <strong class="source-inline">torchvision.transforms.v2.MixUp</strong>, eliminating<a id="_idIndexMarker541"/> the need <a id="_idIndexMarker542"/>for <span class="No-Break">manual implementation.</span></p>
			<h3>AugMix</h3>
			<p>The augmentation<a id="_idIndexMarker543"/> techniques that we have studied <a id="_idIndexMarker544"/>so far have all been fixed augmentations, but deep learning models can memorize them [9] and their performance can plateau. This is where AugMix [10] can be helpful, as it produces a diverse set of augmented images by performing several random augmentations in <span class="No-Break">a sequence.</span></p>
			<p>AugMix improves model robustness and uncertainty without requiring any changes to the model architecture. The full AugMix algorithm also uses a special kind of loss function, but we will skip that <span class="No-Break">for simplicity.</span></p>
			<p>The following code presents a simplified version of AugMix’s <span class="No-Break">core logic:</span></p>
			<pre class="source-code">
from torchvision.transforms import transforms
def simple_augmix(image):
    # Our box of magic tricks
    magic_tricks = [
        transforms.RandomHorizontalFlip(),
        transforms.RandomAffine(degrees=30)
        # other transforms here
    ]
    # Pick a random number of tricks to use
    num_tricks = np.random.randint(0, len(magic_tricks) + 1)
    # Create a new picture by mixing transformed ones
    new_picture = torch.zeros_like(image)
    # Let's use 4 mixed images for our example
    for _ in range(4):
        transformed_picture = image.clone()
        for _ in range(num_tricks):
            trick = np.random.choice(magic_tricks)
            transformed_picture = trick(transformed_picture)
            # Add the transformed picture to our new picture
            new_picture += (1/4) * transformed_picture
    return new_picture</pre>			<p>At the end of the<a id="_idIndexMarker545"/> function, we combine images <a id="_idIndexMarker546"/>by using equal weight for each of the four transformed pictures. The actual AugMix implementation uses a Dirichlet distribution function to combine the images. A Dirichlet distribution is a generalization of the beta distribution that we saw in the <span class="No-Break">MixUp technique.</span></p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B17259_07_12.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – The result of applying the Augmix function to four different images</p>
			<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.12</em>, the top row shows the original image, while the bottom row shows the result of applying AugMix. Images 1 and 3 don’t seem to have changed, but images 2 and 4 have <span class="No-Break">noticeable changes.</span></p>
			<p>According to the AugMix paper [10], in experiments with ImageNet and CIFAR, AugMix achieved reduced test errors while providing improved robustness <span class="No-Break">against corruption.</span></p>
			<p>We don’t need <a id="_idIndexMarker547"/>to<a id="_idIndexMarker548"/> create AugMix from scratch, as <strong class="source-inline">torchvision</strong> provides a built-in API called <strong class="source-inline">torchvision.transforms.AugMix</strong> for <span class="No-Break">this purpose.</span></p>
			<h3>Remix</h3>
			<p>Standard data<a id="_idIndexMarker549"/> augmentation techniques such as <a id="_idIndexMarker550"/>MixUp and CutMix may not be sufficient to handle class imbalances, as they do not take the distribution of class labels into account. Remix [11] addresses the challenges of training deep learning models on <span class="No-Break">imbalanced datasets.</span></p>
			<p>MixUp and CutMix utilize the same mixing factor to combine samples in both the feature space and the label space. In the context of imbalanced data, the authors of the Remix paper [11] argued that this approach may not be optimal. Therefore, they proposed to separate the mixing factors, allowing for more flexibility in their application. By doing so, greater weight can be assigned to the minority class, enabling the creation of labels that are more favorable to the <span class="No-Break">underrepresented class.</span></p>
			<p>If (<span class="_-----MathTools-_Math_Variable">x</span><span class="subscript">i</span>, <span class="_-----MathTools-_Math_Variable">y</span><span class="subscript">i</span>; <span class="_-----MathTools-_Math_Variable">x</span><span class="subscript">i</span>, <span class="_-----MathTools-_Math_Variable">y</span><span class="subscript">j</span>) is an arbitrary pair of images in dataset D, a mixed sample <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">RM</span>, <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">RM</span> can be generated using the <span class="No-Break">following equations</span><span class="No-Break">:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">RM</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="subscript">x</span><span class="_-----MathTools-_Math_Variable">x</span><span class="subscript">i</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">λ</span></span><span class="No-Break"><span class="subscript">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="subscript">j</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">RM</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="subscript">y</span><span class="_-----MathTools-_Math_Variable">y</span><span class="subscript">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">λ</span></span><span class="No-Break"><span class="subscript">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="subscript">j</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable">λ</span><span class="subscript">x</span> and <span class="_-----MathTools-_Math_Variable">λ</span><span class="subscript">y</span> are the mixing factors sampled from the <span class="No-Break">beta distribution.</span></p>
			<p>Here is a <a id="_idIndexMarker551"/><span class="No-Break">simplified</span><span class="No-Break"><a id="_idIndexMarker552"/></span><span class="No-Break"> implementation:</span></p>
			<pre class="source-code">
def remix_data(inputs, labels, class_counts, alpha=1.0):
    lambda_x = np.random.beta(alpha, alpha) if alpha &gt; 0 else 1
    # Constants for controlling the remixing conditions.
    K = 3
    tau = 0.5
    # Shuffle the indices randomly.
    random_indices = torch.randperm(inputs.size()[0])
    # Determine lambda_y values based on class counts and lambda_x.
    lambda_y_values = []
    for i, j in enumerate(random_indices):
        class_count_ratio = (
            class_counts[labels[i]] / class_counts[labels[j]]
        )
        if class_count_ratio &gt;= K and lambda_x &lt; tau:
            lambda_y_values.append(0)
        else:
            lambda_y_values.append(lambda_x)
    lambda_y = torch.tensor(lambda_y_values)
    # Mix inputs, labels based on lambda_x, lambda_y, and shuffled indices.
    mixed_inputs = (
        lambda_x * inputs + (1 - lambda_x) * inputs[random_indices, :]
    )
    mixed_labels = (
        lambda_y * labels + (1 - lambda_y) * labels[random_indices]
    )
    return mixed_inputs, mixed_labels</pre>			<h3>Combining previous techniques</h3>
			<p>It’s possible to<a id="_idIndexMarker553"/> combine these methods to introduce even more diversity to the training data, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">CutMix and MixUp</strong>: These can be alternated or used in tandem, creating regions in images that are replaced with parts of other images while also blending <span class="No-Break">images pixel-wise</span></li>
				<li><strong class="bold">Sequential</strong>: You could sequentially apply these techniques (e.g., use MixUp first and then CutMix) to further diversify the <span class="No-Break">augmented dataset</span></li>
			</ul>
			<p>When combining these methods, it’s important to carefully manage the probabilities and strengths of each method, thus avoiding introducing too much noise or making the training data too divergent from the <span class="No-Break">original distribution.</span></p>
			<p>Also, while combining these methods might improve model robustness and generalization in certain scenarios, it can also make training more computationally intensive and complex. It’s essential to balance the benefits against the <span class="No-Break">potential trade-offs.</span></p>
			<p>Remember, always validate the effectiveness of combined augmentations on a validation set to ensure they are beneficial for the task <span class="No-Break">at hand.</span></p>
			<p>Let’s use a long-tailed version of a different dataset called Fashion-MNIST for the techniques we just discussed (<span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.13</em>). Fashion-MNIST is another MNIST variant, consisting of 60,000 training and 10,000 testing images of 10 different clothing items, such as shoes, shirts, and dresses, each represented in a grayscale image of <span class="No-Break">28x28 pixels.</span></p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B17259_07_13.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – Imbalanced FashionMNIST</p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.14</em> shows the<a id="_idIndexMarker554"/> overall model accuracy when trained using CutMix, MixUp, a combination of both, and Remix on the imbalanced <span class="No-Break">FashionMNIST dataset.</span></p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B17259_07_14.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – Overall model accuracy (FashionMNIST)</p>
			<p>The difference in the <a id="_idIndexMarker555"/>performance of these techniques is more apparent when looking at the class-wise accuracy numbers (<span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">).</span></p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B17259_07_15.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Class-wise model accuracy (the FashionMNIST dataset)</p>
			<p>Based on the given data, here <a id="_idIndexMarker556"/>are some insightful conclusions that can be drawn for the various techniques, especially in the context of <span class="No-Break">imbalanced data:</span></p>
			<ul>
				<li><strong class="bold">Overall performance</strong>: <strong class="bold">Cutmix</strong> and <strong class="bold">Remix</strong> generally offer the highest performance across most classes, followed closely by <strong class="bold">Mixup</strong> and <strong class="bold">Cutmix+Mixup</strong>. <strong class="bold">Baseline</strong> seems to be the least effective <span class="No-Break">in general.</span></li>
				<li><strong class="bold">Performance on minority classes</strong>: For the minority class labeled as “6,” all techniques show relatively low performance compared to other classes. However, <strong class="bold">Mixup</strong> and <strong class="bold">Cutmix+Mixup</strong> offer a slight improvement over <span class="No-Break">the baseline.</span></li>
				<li><strong class="bold">Consistency across classes</strong>: <strong class="bold">Cutmix</strong> and <strong class="bold">Mixup</strong> are more consistent across different classes, excluding class “6,” where they are only marginally better. <strong class="bold">Baseline</strong>, on the other hand, shows significant variability, performing extremely well on some classes (such as “0” and “5”) but poorly on others (such <span class="No-Break">as “6”).</span></li>
				<li><strong class="bold">Techniques suited for specific classes</strong>: <strong class="bold">Cutmix</strong> performs exceptionally well for the classes labeled “1” and “8,” where it outperforms all <span class="No-Break">other techniques.</span><p class="list-inset"><strong class="bold">Remix</strong> is particularly strong for the class labeled “2,” where it outshines all <span class="No-Break">other techniques.</span></p></li>
				<li><strong class="bold">Complexity versus benefit</strong>: <strong class="bold">Cutmix+Mixup</strong> does not offer a significant improvement over <strong class="bold">Cutmix</strong> or <strong class="bold">Mixup</strong> individually, raising questions about whether the additional computational complexity <span class="No-Break">is justified.</span></li>
				<li><strong class="bold">Generalizability</strong>: <strong class="bold">Cutmix</strong> and <strong class="bold">Mixup</strong> appear to be the most robust techniques, showing high performance across most classes. These techniques would likely perform well on unseen data and are potentially good choices for <span class="No-Break">imbalanced datasets.</span></li>
				<li><strong class="bold">Trade-offs</strong>: <strong class="bold">Cutmix</strong> offers high performance but may not be the best for minority classes. <strong class="bold">Mixup</strong>, although slightly less effective overall, offers more balanced performance across classes, including <span class="No-Break">minority ones.</span></li>
			</ul>
			<p>The following are<a id="_idIndexMarker557"/> some points to be careful about while performing these <span class="No-Break">image augmentations:</span></p>
			<ul>
				<li>We must ensure that data augmentations preserve the original labels. For instance, rotating digits such as 6 and 9 can be problematic in digit recognition tasks. Similarly, cropping an image could invalidate its label, such as removing a cat from a “cat” image. This is especially crucial in complex tasks, such as image segmentation in self-driving cars, where augmentations can alter output labels <span class="No-Break">or masks.</span></li>
				<li>While geometric and color transformations often increase memory usage and training time, they are not inherently problematic. Although color alterations can sometimes remove important details and affect label integrity, smart manipulation can also be beneficial. For instance, tweaking the color space to mimic different lighting or camera lenses can improve <span class="No-Break">model performance.</span></li>
			</ul>
			<p>To overcome some of the increased memory, time, and cost issues, as mentioned, a technique called <strong class="source-inline">AutoAugment</strong> is available in <strong class="source-inline">PyTorch</strong>, which can automatically search for the best <a id="_idIndexMarker558"/>augmentation policies on the dataset <span class="No-Break">being used.</span></p>
			<p class="callout-heading">🚀 Deep learning data-level techniques in production at Grab</p>
			<p class="callout"><strong class="bold">🎯</strong><strong class="bold"> Problem </strong><span class="No-Break"><strong class="bold">being solved:</strong></span></p>
			<p class="callout">Grab, a ride-hailing and food delivery company in South-East Asia, faced the primary challenge of anonymizing faces and license plates in images collected for their geotagged imagery platform, KartaView [12]. This was essential to ensure <span class="No-Break">user privacy.</span></p>
			<p class="callout"><strong class="bold">⚖️</strong><strong class="bold"> Data </strong><span class="No-Break"><strong class="bold">imbalance issue</strong></span><span class="No-Break">:</span></p>
			<p class="callout">The dataset used by Grab was imbalanced, particularly in terms of the object sizes. Larger regions of interest, such as close-ups of faces or license plates, were underrepresented. This skewed distribution led to poor model performance in detecting these <span class="No-Break">larger objects.</span></p>
			<p class="callout"><strong class="bold">🎨</strong><strong class="bold"> Data </strong><span class="No-Break"><strong class="bold">augmentation strategy:</strong></span></p>
			<p class="callout">Grab employed a multi-pronged data augmentation approach to address <span class="No-Break">the imbalance:</span></p>
			<p class="callout">•  <strong class="bold">Offline augmentation</strong>: One key method they used was the “image view splitting,” where each original image is divided into multiple “views” with predefined properties. This was crucial to accommodate different types of images such as perspective, wide field of view, and 360-degree equirectangular images. Each “view” was treated as a separate image with its tags, which helped the model generalize better. They also implemented oversampling for images with larger tags, addressing the imbalance in their dataset. This was vital for their anchor-based object detection model, as the imbalance was affecting the model’s performance in identifying <span class="No-Break">larger objects.</span></p>
			<p class="callout">•  <strong class="bold">Online augmentation</strong>: They used YOLOv4 for object detection, which allowed for a variety of online augmentations, such as saturation, exposure, hue, flip, <span class="No-Break">and mosaic.</span></p>
			<p>Modern techniques such as autoencoders and adversarial networks, specifically <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>), have recently gained traction in creating synthetic <a id="_idIndexMarker559"/>data to enhance image datasets. A GAN comprises two neural networks – the generator, which produces synthetic data, and the discriminator, which evaluates the authenticity of this data. Together, they work to create realistic and high-quality synthetic samples. GANs have also been applied to generate synthetic tabular data. For example, they’ve been used to create synthetic medical images that significantly improve diagnostic models. We’ll explore these cutting-edge techniques in greater detail toward <a id="_idIndexMarker560"/>the end of the chapter. In the next section, we will learn about applying data-level techniques to <span class="No-Break">NLP problems.</span></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor219"/>Data-level techniques for text classification</h1>
			<p>Data imbalance, wherein <a id="_idIndexMarker561"/>certain classes in a dataset are underrepresented, is not just an issue confined to image or structured data domains. In NLP, imbalanced datasets can lead to biased models that might perform well on the majority class but are likely to misclassify underrepresented ones. To address this challenge, numerous strategies have <span class="No-Break">been devised.</span></p>
			<p>In NLP, data augmentation can boost model performance, especially with limited training data. <em class="italic">Table 7.3</em> categorizes the various data augmentation techniques for <span class="No-Break">text data:</span></p>
			<table id="table003-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Level</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Method</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Example techniques</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="2">
							<p><span class="No-Break">Character level</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Noise</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Introducing randomness at the <span class="No-Break">character level</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Jumbling characters</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Rule-based</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Transformations based on <span class="No-Break">predefined rules</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Capitalization</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="4">
							<p><span class="No-Break">Word level</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Noise</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Random <span class="No-Break">word changes</span></p>
						</td>
						<td class="No-Table-Style">
							<p>“cat” <span class="No-Break">to “dog”</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Synonyms</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Replacing words with <span class="No-Break">their synonyms</span></p>
						</td>
						<td class="No-Table-Style">
							<p>“happy” <span class="No-Break">to “joyful”</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Embeddings</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Using word embeddings <span class="No-Break">for replacement</span></p>
						</td>
						<td class="No-Table-Style">
							<p>“king” <span class="No-Break">to “monarch”</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Language models</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Leveraging advanced language models for <span class="No-Break">word replacement</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">BERT</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="2">
							<p><span class="No-Break">Phrase level</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Structure</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Altering the structure <span class="No-Break">of phrases</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Changing <span class="No-Break">word order</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Interpolation</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Merging features of <span class="No-Break">two phrases</span></p>
						</td>
						<td class="No-Table-Style">
							<p>“The cat sat” + “The dog barked” = “The <span class="No-Break">cat barked”</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" rowspan="2">
							<p><span class="No-Break">Document level</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Translation</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Translating the document to another language <span class="No-Break">and back</span></p>
						</td>
						<td class="No-Table-Style">
							<p>English to French <span class="No-Break">to English</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Generative</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Using models to generate <span class="No-Break">new content</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">GPT-3</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 7.3 – The categorization of different data augmentation methods (adapted from [13])</p>
			<p>Data augmentation techniques for text can be categorized into character, word, phrase, and document levels. Techniques vary from jumbling characters to using models such as BERT and GPT-3. This taxonomy guides us through NLP data <span class="No-Break">augmentation methods.</span></p>
			<p><em class="italic">Table 7.3</em> shows various data augmentation methods used in NLP. We will break down the methods based on the level at which the data is manipulated – character, word, phrase, and document. Each level has its unique set of methods, such as introducing “noise” at the character level or leveraging “language models” at the word level. These methods are not just random transformations; they are often carefully designed to preserve the semantic meaning of the text while <span class="No-Break">introducing variability.</span></p>
			<p>What sets this categorization apart is its multilayered approach, which allows a more targeted application of data augmentation methods. For instance, if you’re dealing with short text snippets, methods at the character or word level may be more appropriate. On the other hand, if you’re working with longer documents or need to generate entirely new content, then methods at the document level, such as “generative” techniques, come <span class="No-Break">into play.</span></p>
			<p>In the subsequent sections, we will explore a text classification dataset that is imbalanced and illustrate various data augmentation techniques using it. These methodologies are designed to synthesize additional data, thereby enhancing a model’s ability to learn and generalize from the <span class="No-Break">imbalanced information.</span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor220"/>Dataset and baseline model</h2>
			<p>Let’s take the spam text message classification dataset available on Kaggle (<a href="https://www.kaggle.com/datasets/team-ai/spam-text-message-classification">https://www.kaggle.com/datasets/team-ai/spam-text-message-classification</a>). This dataset, primarily used to distinguish spam from legitimate messages, presents an imbalance with a majority of “ham” (legitimate) messages and a minority of “spam” messages. We are skipping the code here to save space. You can find the notebook in the GitHub repo with the <span class="No-Break">name </span><span class="No-Break"><strong class="source-inline">Data_level_techniques_NLP.ipynb</strong></span><span class="No-Break">.</span></p>
			<p>With a baseline model, we have the <span class="No-Break">following results:</span></p>
			<pre class="source-code">
               precision     recall     f1-score     support
ham               0.97        1.00        0.98        1216
spam              0.97        0.80        0.88         177
accuracy                                  0.97        1393
macro avg         0.97        0.90        0.93        1393
weighted avg      0.97        0.97        0.97        1393</pre>			<h3>Random oversampling</h3>
			<p>One basic technique to handle data imbalance is random oversampling, where instances of the minority class are replicated to balance out the class distribution. While this technique is easy to implement and often shows improved performance, it’s essential to be wary <span class="No-Break">of overfitting:</span></p>
			<pre class="source-code">
               precision     recall     f1-score     support
ham               0.99        0.99        0.99        1216
spam              0.93        0.91        0.92         177
accuracy                                  0.98        1393
macro avg         0.96        0.95        0.95        1393
weighted avg      0.98        0.98        0.98        1393</pre>			<p>Random oversampling shows a slight improvement in overall accuracy, rising from 0.97 to 0.98. The most notable gain is in the recall for the <strong class="source-inline">spam</strong> class, which increased from 0.80 to 0.91, indicating better identification of spam messages. However, the precision for <strong class="source-inline">spam</strong> dropped a bit from 0.97 <span class="No-Break">to 0.93.</span></p>
			<p>The macro average F1-score also improved from 0.93 to 0.95, suggesting that the model is now better at handling both classes (<strong class="source-inline">ham</strong> and <strong class="source-inline">spam</strong>) more equally. The weighted average metrics remain strong, reinforcing that the model’s overall performance has improved without sacrificing its ability to correctly classify the majority <span class="No-Break">class (</span><span class="No-Break"><strong class="source-inline">ham</strong></span><span class="No-Break">).</span></p>
			<p>Similarly, undersampling can be useful to reduce the size of the majority class, particularly by eliminating exact duplicate sentences. For example, you might not need 500 copies of “Thanks very much!” However, sentences with similar semantic meaning but different wording, such as “Thanks very much!” and “Thanks so much!”, should generally be retained. Exact duplicates can be identified, using methods such as string matching, while sentences with similar meanings can be detected using cosine similarity or the Jaccard similarity of <span class="No-Break">sentence embeddings.</span></p>
			<p class="callout-heading">🚀 Deep learning data-level techniques in production at Cloudflare</p>
			<p class="callout"><strong class="bold">🎯</strong><strong class="bold"> Problem </strong><span class="No-Break"><strong class="bold">being solved</strong></span><span class="No-Break">:</span></p>
			<p class="callout">Cloudflare [14] aimed to enhance its <strong class="bold">Web Application Firewall</strong> (<strong class="bold">WAF</strong>) to better identify malicious HTTP requests and protect against common attacks, such as SQL injection and <strong class="bold">cross-site </strong><span class="No-Break"><strong class="bold">scripting</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">XSS</strong></span><span class="No-Break">).</span></p>
			<p class="callout"><strong class="bold">⚖️</strong><strong class="bold"> Data </strong><span class="No-Break"><strong class="bold">imbalance issue</strong></span><span class="No-Break">:</span></p>
			<p class="callout">Creating a quality dataset to train the WAF model was difficult, due to strict privacy regulations and the absence of labeled data for malicious HTTP requests. The heterogeneity of samples also presented challenges as the requests came in various formats and encodings. There was a significant lack of samples for specific types of attacks, making the dataset imbalanced and leading to the risk of false positives <span class="No-Break">or negatives.</span></p>
			<p class="callout"><strong class="bold">🎨</strong><strong class="bold"> Data </strong><span class="No-Break"><strong class="bold">augmentation strategy</strong></span><span class="No-Break">:</span></p>
			<p class="callout">To tackle this, Cloudflare employed a combination of data augmentation and generation techniques. These included mutating benign content in various ways, generating pseudo-random noise samples, and using language models for synthetic data creation. The focus was on increasing the diversity of negative samples while maintaining the integrity of the content, thereby forcing the model to consider a broader spectrum of structural, semantic, and statistical properties for <span class="No-Break">better classification.</span></p>
			<p class="callout">🚀 <span class="No-Break"><strong class="bold">Model deployment</strong></span><span class="No-Break">:</span></p>
			<p class="callout">The model that they used significantly improved after employing these data augmentation techniques, with a remarkable F1 score of 0.99 after augmentation compared to 0.61 before. The model has been validated against Cloudflare’s signature-based WAF and was found to perform comparably, making <span class="No-Break">it production-ready.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor221"/>Document-level augmentation</h2>
			<p>In d<a id="_idTextAnchor222"/>ocument-level augmentation, entire documents are modified to create new examples, in order to preserve the broader semantic context or narrative flow of the document. One such technique is <span class="No-Break">back translation.</span></p>
			<h3>Back translation</h3>
			<p>Back translation involves translating a sentence to a different language and then reverting it back to the original (<span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.16</em>). This produces sentences that are syntactically different but semantically similar to the original text, providing a form <span class="No-Break">of augmentation.</span></p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B17259_07_16.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Demonstrating the back translation technique</p>
			<p>We generate the back-translated text and append it to the original dataset. Then, we use the full dataset to train the logistic regression model. Note that this can be a time-consuming process, since the translation model binaries are resource-intensive. It may also introduce errors, since some words may not be exactly translatable across languages. In the GitHub notebook, we used the <strong class="source-inline">BackTranslationAug</strong> API from the <strong class="source-inline">nlpaug</strong> <span class="No-Break">library [15].</span></p>
			<p>The following results show the classification metrics on the test set. The precision of the spam class shows improvement over the random oversampling technique, while recall is a <span class="No-Break">bit worse:</span></p>
			<pre class="source-code">
               precision     recall     f1-score     support
ham               0.98        1.00        0.99        1216
spam              0.96        0.86        0.91         177
accuracy                                  0.98        1393
macro avg         0.97        0.93        0.95        1393
weighted avg      0.98        0.98        0.98        1393</pre>			<p>Back translation maintains an overall accuracy of 0.98, similar to random oversampling. It slightly improves <strong class="source-inline">spam</strong> precision <a id="_idIndexMarker562"/>to 0.96 but lowers recall to 0.86. Both methods outperform the baseline, with back translation favoring precision over recall for the <span class="No-Break"><strong class="source-inline">spam</strong></span><span class="No-Break"> class.</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor223"/>Character and word-level augmentation</h2>
			<p>Let’s briefly go over<a id="_idIndexMarker563"/> a few<a id="_idIndexMarker564"/> character and word-level augmentation techniques that can be applied to <span class="No-Break">NLP problems.</span></p>
			<h3>Easy Data Augmentation techniques</h3>
			<p><strong class="bold">Easy Data Augmentation</strong> (<strong class="bold">EDA</strong>) is a suite of data<a id="_idIndexMarker565"/> augmentation techniques specific to text data. It includes simple operations such as synonym replacement, random insertion, random swap, and random deletion. These operations, being simple, ensure that the augmented data remains meaningful. The following table shows various metrics when using EDA on <span class="No-Break">the dataset:</span></p>
			<pre class="source-code">
               precision     recall      f1-score    support
ham               0.98        0.99        0.99        1216
spam              0.96        0.88        0.91         177
accuracy                                  0.98        1393
macro avg         0.97        0.93        0.95        1393
weighted avg      0.98        0.98        0.98        1393</pre>			<p>After applying EDA, the model retains an overall accuracy of 0.98, consistent with both random oversampling and back translation. The precision for <strong class="source-inline">spam</strong> is high at 0.96, similar to back translation, while the recall is slightly better at 0.88 compared to 0.86 with Back Translation. The macro and weighted averages remain robust at 0.95 and <span class="No-Break">0.98, respectively.</span></p>
			<p>EDA offers a balanced improvement in both precision and recall for the <strong class="source-inline">spam</strong> class, making it a strong <a id="_idIndexMarker566"/>contender among the data augmentation techniques <span class="No-Break">we’ve tried.</span></p>
			<table id="table004-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor224"/></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Precision</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Recall</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">F1-Score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Accuracy</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Baseline model</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">0.97</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.80</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.88</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.97</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Random oversampling</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.93</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">0.91</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">0.92</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">0.98</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Back translation</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.96</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.86</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.91</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">0.98</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">EDA</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.96</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.88</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.91</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">0.98</strong></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 7.4 – Comparing the results of the various NLP data-level techniques for the spam class (max per metric in bold)</p>
			<p>Overall, as we can see in <em class="italic">Table 7.4</em>, for our dataset, random oversampling excels in recall for <strong class="source-inline">spam</strong> but slightly lowers precision. Back translation boosts precision at a minor recall trade-off. EDA offers a balanced improvement in both. It’s important to note that these results are empirical and specific to the dataset used for this analysis. Data augmentation techniques can yield different outcomes, depending on the nature of the data, its distribution, and the problem being addressed. Therefore, while these techniques show promise in this context, their effectiveness may vary when applied to different datasets or <span class="No-Break">NLP tasks.</span></p>
			<p>We will not be covering phrase-level augmentation techniques in this book due to space constraints, but we<a id="_idIndexMarker567"/> recommend exploring them on <span class="No-Break">your own.</span></p>
			<p>Next, we will look at some miscellaneous data-level deep learning techniques at a <span class="No-Break">high level.</span></p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor225"/>Discussion of other data-level deep learning methods and their key ideas</h1>
			<p>In addition to the <a id="_idIndexMarker568"/>methods previously discussed, there is a rich array of other techniques specifically designed to address imbalanced data challenges. This section provides a high-level overview of these alternative approaches, each offering unique insights and potential advantages. While we will only touch upon their key ideas, we encourage you to delve deeper into the literature and explore them further if you find these <span class="No-Break">techniques intriguing.</span></p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor226"/>Two-phase learning</h2>
			<p>Two-phase<a id="_idIndexMarker569"/> learning [16][17] is a technique <a id="_idIndexMarker570"/>designed to enhance the performance of minority classes in multi-class classification problems, without compromising the performance of majority classes. The process involves two <span class="No-Break">training phases:</span></p>
			<ol>
				<li>In the first phase, a deep learning model is first trained on the dataset, which is balanced with respect to each class. Balancing can be done using sampling techniques such as random oversampling <span class="No-Break">or undersampling.</span></li>
				<li>In the second phase, we freeze all the layers except the last one, and then the model is fine-tuned using the <span class="No-Break">entire dataset.</span></li>
			</ol>
			<p>The first phase ensures that all layers are trained on a balanced dataset. The second phase calibrates the output probabilities by retraining the last layer with the entire dataset, reflecting the original imbalanced <span class="No-Break">class distribution.</span></p>
			<p>The order of the two phases can be reversed – that is, the first model is trained on the full imbalanced data and then <a id="_idIndexMarker571"/>fine-tuned<a id="_idIndexMarker572"/> on a balanced dataset in the second phase. This is <a id="_idIndexMarker573"/>called <strong class="bold">deferred sampling</strong>, since sampling is <span class="No-Break">done later.</span></p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor227"/>Expansive Over-Sampling</h2>
			<p>Introduced in a paper <a id="_idIndexMarker574"/>by Damien<a id="_idIndexMarker575"/> Dablain et al. [18], <strong class="bold">Expansive Over-Sampling</strong> (<strong class="bold">EOS</strong>) is another data augmentation technique used within a three-phase CNN training framework, designed for imbalanced data. It can be considered to incorporate both two-phase learning and data <span class="No-Break">augmentation techniques.</span></p>
			<p>EOS works by creating synthetic training instances as combinations between the minority class samples and their nearest “enemies” in the embedded space. The term “nearest enemies” refers to instances of other classes that are closest in the feature space to a given instance. By creating synthetic instances in this way, EOS aims to reduce the generalization gap, which is wider for <span class="No-Break">minority classes.</span></p>
			<p>The paper’s authors [18] claimed that this method improves accuracy and efficiency over common imbalanced learning techniques, requiring fewer parameters and less <span class="No-Break">training time.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor228"/>Using generative models for oversampling</h2>
			<p>Generative<a id="_idIndexMarker576"/> models, including <a id="_idIndexMarker577"/>GANs, <strong class="bold">Variational AutoEncoders</strong> (<strong class="bold">VAEs</strong>), diffusion<a id="_idIndexMarker578"/> models, and<a id="_idIndexMarker579"/> their derivatives, such as StyleGAN, StyleGAN2, and GPT-based models, have become prominent tools for producing data points that resemble <span class="No-Break">training data.</span></p>
			<p>VAEs, a specific type of generative model, consist of an encoder and decoder that work together to create new instances of data, such as realistic images, and can be used to balance imbalanced datasets. On the long-tailed version of MNIST, we got a decent performance improvement by using a VAE-augmented model when compared to the baseline model on the most imbalanced classes. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.17</em> shows the performance comparison after 50 epochs. You can find the notebook in the corresponding chapter of the <span class="No-Break">GitHub repo.</span></p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B17259_07_17.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – VAE-augmented model performance on the long-tailed MNIST dataset</p>
			<p>Diffusion models operate by progressively corrupting an image with noise and then reconstructing it, with applications in areas such as medical imaging. Examples include DALLE-2 and the open source stable <span class="No-Break">diffusion model.</span></p>
			<p>Recent studies [19] highlight the utility of synthetic data in enhancing zero-shot and few-shot image<a id="_idIndexMarker580"/> classification<a id="_idIndexMarker581"/> tasks. Specifically, text-to-image <a id="_idIndexMarker582"/>generation models, when used in conjunction with large-scale pre-trained models such as DALL-E and Stable Diffusion, significantly improve performance in scenarios where real-world data is sparse or unavailable. These generative models have gained prominence for their ability to create high-quality images based on natural language prompts, offering a potential solution for imbalanced datasets. For example, if there is a scarcity of images featuring a monkey seated in a car, these models can generate hundreds or even thousands of such images to augment training datasets. However, it’s worth noting that models trained solely on synthetic data may still underperform compared to those trained on <span class="No-Break">real data.</span></p>
			<p>These models often require significant computational resources, making them time-consuming and expensive to scale up, especially for vast datasets. Diffusion models, in particular, are computationally intensive, and potential overfitting can compromise model generalizability. Therefore, it is crucial to balance the benefits of data augmentation with the computational cost and potential challenges when employing these <a id="_idIndexMarker583"/>advanced<a id="_idIndexMarker584"/> <span class="No-Break">generative </span><span class="No-Break"><a id="_idIndexMarker585"/></span><span class="No-Break">models.</span></p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor229"/>DeepSMOTE</h2>
			<p>The <strong class="bold">Deep Synthetic Minority Oversampling</strong> (<strong class="bold">DeepSMOTE</strong>) technique [20] is essentially <a id="_idIndexMarker586"/>SMOTE adapted for deep learning<a id="_idIndexMarker587"/> models using an encoder-decoder architecture, with minor tweaks for image data. DeepSMOTE consists of three <span class="No-Break">major components:</span></p>
			<ul>
				<li><strong class="bold">An encoder/decoder framework to handle complex and high-dimensional data</strong>: An encoder/decoder<a id="_idIndexMarker588"/> framework is used to learn a compact feature representation of the image data. It is trained to reconstruct the original images from this compact form, ensuring that essential features <span class="No-Break">are captured.</span></li>
				<li><strong class="bold">SMOTE-based oversampling for generating synthetic instances</strong>: Once the feature representation is learned, SMOTE is applied in this feature space to generate synthetic instances of the minority class. This is particularly useful for image data where the raw data is high-dimensional and complex. SMOTE creates these synthetic instances by finding the <em class="italic">k</em>-nearest neighbors in the feature space and generating new instances that are interpolations between the instance under consideration and <span class="No-Break">its neighbors.</span></li>
				<li><strong class="bold">A dedicated loss function</strong>: DeepSMOTE introduces a specialized loss function that not only focuses on the reconstruction error (how well the decoder can reconstruct the original image from the encoded form) but also includes a penalty term, ensuring that the synthetic instances are useful for the <span class="No-Break">classification task.</span></li>
			</ul>
			<p>Unlike GAN-based oversampling, DeepSMOTE does not require a discriminator. It claims to generate high-quality, information-rich <a id="_idIndexMarker589"/>synthetic images that can be <span class="No-Break">visually inspected.</span></p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B17259_07_18.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – Demonstrating the DeepSMOTE technique (adapted from [20])</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor230"/>Neural style transfer</h2>
			<p>Neural style transfer is<a id="_idIndexMarker590"/> a technique in deep<a id="_idIndexMarker591"/> learning that artistically blends the content of one image with the style of another (<span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.19</em>). While its primary application is in art and image processing, the concept of generating synthetic data samples can be adapted to address data imbalance in machine learning. By drawing inspiration from style transfer, one could potentially generate synthetic samples for the minority class, blend features of different classes, or adapt domain-specific knowledge. However, care must be taken to ensure that synthetic data authentically represents real-world scenarios to avoid overfitting and poor generalization of <span class="No-Break">real data.</span></p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B17259_07_19.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – Demonstrating the neural style<a id="_idTextAnchor231"/> transfer technique</p>
			<p>We hope that this provides a thorough understanding of data-level deep learning methods to address <a id="_idIndexMarker592"/>imbalanced data, including<a id="_idIndexMarker593"/> oversampling, data augmentation, and various <span class="No-Break">other strategies.</span></p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor232"/>Summary</h1>
			<p>The transition of methods to handle data imbalance from classical machine learning models to deep learning models can pose unique challenges, primarily due to the distinct types of data that these models have to work with. Classical machine learning models typically deal with structured, tabular data, whereas deep learning models often grapple with unstructured data, such as images, text, audio, and video. This chapter explored how to adapt sampling techniques to work with deep learning models. To facilitate this, we used an imbalanced version of the MNIST dataset to train a model, which is then employed in conjunction with various <span class="No-Break">oversampling methods.</span></p>
			<p>Incorporating random oversampling with deep learning models involves duplicating samples from minority classes randomly, until each class has an equal number of samples. This is usually performed using APIs from libraries such as imbalanced-learn, Keras, TensorFlow, or PyTorch, which work together seamlessly for this purpose. Once data is oversampled, it can be sent for model training in PyTorch <span class="No-Break">or TensorFlow.</span></p>
			<p>The chapter also delved into different data augmentation techniques, which can be especially beneficial when dealing with limited or imbalanced data. Augmentation techniques include rotating, scaling, cropping, blurring, and adding noise, among other advanced techniques such as AugMix, CutMix, and MixUp. However, care must be taken to ensure these augmentations preserve the original labels and do not inadvertently alter vital information in the data. We discussed other methods, such as two-phase learning and dynamic sampling, as potential strategies to improve model performance on imbalanced data. We also learned about some data-level techniques applicable to text, such as back translation and EDA, while running them on a <span class="No-Break">spam/ham dataset.</span></p>
			<p>In the next chapter, we will look at some algorithm-based methods to deal with <span class="No-Break">imbalanced datasets.</span></p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor233"/>Questions</h1>
			<ol>
				<li>Apply Mixup interpolation to the Kaggle spam detection NLP dataset used in the chapter. See if Mixup helps to improve the model performance. You can refer to the paper <em class="italic">Augmenting Data with Mixup for Sentence Classification: An Empirical Study</em> by Guo et al. (<a href="https://arxiv.org/pdf/1905.08941.pdf">https://arxiv.org/pdf/1905.08941.pdf</a>) for <span class="No-Break">further reading.</span></li>
				<li>Refer to the FMix paper [21] and implement the FMix augmentation technique. Apply it to the Caltech101 dataset. See whether model performance improves by using FMix over the baseline <span class="No-Break">model performance.</span></li>
				<li>Apply the EOS technique described in the chapter to the CIFAR-10-LT (the long-tailed version of CIFAR-10) dataset, and see whether the model performance improves for the most <span class="No-Break">imbalanced classes.</span></li>
				<li>Apply the MDSA techniques we studied in this chapter to the CIFAR-10-LT dataset, and see whether the model performance improves for the most <span class="No-Break">imbalanced classes.</span></li>
			</ol>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor234"/>References</h1>
			<ol>
				<li value="1">Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S. Kaseb, Kent Gauen, Ryan Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, and Mei-Ling Shyu. 2018. <em class="italic">Dynamic Sampling in Convolutional Neural Networks for Imbalanced Data Classification</em>. In 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), pages 112–117, Miami, FL, <span class="No-Break">April. IEEE.</span></li>
				<li>LeNet-5 paper, <em class="italic">Gradient-based learning applied to document </em><span class="No-Break"><em class="italic">classification</em></span><span class="No-Break">: </span><a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf"><span class="No-Break">http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf</span></a><span class="No-Break">.</span></li>
				<li>AlexNet paper, <em class="italic">ImageNet Classification with Deep Convolutional Neural </em><span class="No-Break"><em class="italic">Networks</em></span><span class="No-Break">: </span><a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"><span class="No-Break">https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">Leveraging Real-Time User Actions to Personalize Etsy Ads</em> (<span class="No-Break">2023): </span><a href="https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads"><span class="No-Break">https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads</span></a><span class="No-Break">.</span></li>
				<li>Automated image tagging at <a href="http://Booking.com">Booking.com</a> (<span class="No-Break">2017): </span><a href="https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b"><span class="No-Break">https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">Shot Angle Prediction: Estimating Pose Angle with Deep Learning for Furniture Items Using Images Generated from 3D Models (</em><span class="No-Break"><em class="italic">2020)</em></span><span class="No-Break">: </span><a href="https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models"><span class="No-Break">https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models</span></a><span class="No-Break">.</span></li>
				<li>S. Yun, D. Han, S. Chun, S. J. Oh, Y. Yoo, and J. Choe, “<em class="italic">CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features</em>,” in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Korea (South): IEEE, Oct. 2019, pp. 6022–6031. <span class="No-Break">doi: </span><span class="No-Break">10.1109/ICCV.2019.00612</span><span class="No-Break">.</span></li>
				<li>H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “<em class="italic">mixup: Beyond Empirical Risk Minimization</em>.” arXiv, Apr. 27, 2018. Accessed: Feb. 11, 2023. [Online]. <span class="No-Break">Available: </span><a href="http://arxiv.org/abs/1710.09412"><span class="No-Break">http://arxiv.org/abs/1710.09412</span></a><span class="No-Break">.</span></li>
				<li>R. Geirhos, C. R. M. Temme, J. Rauber, H. H. Schütt, M. Bethge, and F. A. Wichmann, “<em class="italic">Generalisation in humans and deep </em><span class="No-Break"><em class="italic">neural networks</em></span><span class="No-Break">.”</span></li>
				<li>D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lakshminarayanan, “<em class="italic">AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty</em>.” arXiv, Feb. 17, 2020. Accessed: Aug. 01, 2023. [Online]. <span class="No-Break">Available: </span><a href="http://arxiv.org/abs/1912.02781"><span class="No-Break">http://arxiv.org/abs/1912.02781</span></a><span class="No-Break">.</span></li>
				<li>H.-P. Chou, S.-C. Chang, J.-Y. Pan, W. Wei, and D.-C. Juan, “<em class="italic">Remix: Rebalanced Mixup</em>.” arXiv, Nov. 19, 2020. Accessed: Aug. 15, 2023. [Online]. <span class="No-Break">Available: </span><a href="http://arxiv.org/abs/2007.03943"><span class="No-Break">http://arxiv.org/abs/2007.03943</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">Protecting Personal Data in Grab’s Imagery</em> (<span class="No-Break">2021): </span><a href="https://engineering.grab.com/protecting-personal-data-in-grabs-imagery"><span class="No-Break">https://engineering.grab.com/protecting-personal-data-in-grabs-imagery</span></a><span class="No-Break">.</span></li>
				<li>M. Bayer, M.-A. Kaufhold, and C. Reuter, “<em class="italic">A Survey on Data Augmentation for Text Classification</em>,” ACM Comput. Surv., vol. 55, no. 7, pp. 1–39, Jul. 2023, <span class="No-Break">doi: </span><span class="No-Break">10.1145/3544558</span><span class="No-Break">.</span></li>
				<li><em class="italic">Improving the accuracy of our machine learning WAF using data augmentation and sampling</em> (2022), Vikram <span class="No-Break">Grover: </span><a href="https://blog.cloudflare.com/data-generation-and-sampling-strategies/"><span class="No-Break">https://blog.cloudflare.com/data-generation-and-sampling-strategies/</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">Data augmentation for </em><span class="No-Break"><em class="italic">NLP</em></span><span class="No-Break">: </span><a href="https://github.com/makcedward/nlpaug"><span class="No-Break">https://github.com/makcedward/nlpaug</span></a><span class="No-Break">.</span></li>
				<li>B. Kang <em class="italic">et al.</em>, “<em class="italic">Decoupling Representation and Classifier for Long-Tailed Recognition</em>.” arXiv, Feb. 19, 2020. Accessed: Dec. 15, 2022. [Online]. <span class="No-Break">Available: </span><a href="http://arxiv.org/abs/1910.09217"><span class="No-Break">http://arxiv.org/abs/1910.09217</span></a><span class="No-Break">.</span></li>
				<li>K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “<em class="italic">Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss</em>”, [Online]. <span class="No-Break">Available: </span><a href="https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf"><span class="No-Break">https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf</span></a><span class="No-Break">.</span></li>
				<li>D. Dablain, C. Bellinger, B. Krawczyk, and N. Chawla, “<em class="italic">Efficient Augmentation for Imbalanced Deep Learning</em>.” arXiv, Oct. 17, 2022. Accessed: Jul. 23, 2023. [Online]. <span class="No-Break">Available: </span><a href="http://arxiv.org/abs/2207.06080"><span class="No-Break">http://arxiv.org/abs/2207.06080</span></a><span class="No-Break">.</span></li>
				<li>R. He <em class="italic">et al.</em>, “<em class="italic">Is synthetic data from generative models ready for image recognition?</em>” arXiv, Feb. 15, 2023. Accessed: Aug. 06, 2023. [Online]. <span class="No-Break">Available: </span><a href="http://arxiv.org/abs/2210.07574"><span class="No-Break">http://arxiv.org/abs/2210.07574</span></a><span class="No-Break">.</span></li>
				<li>D. Dablain, B. Krawczyk, and N. V. Chawla, “<em class="italic">DeepSMOTE: Fusing Deep Learning and SMOTE for Imbalanced Data</em>,” IEEE Transactions on Neural Networks and Learning Systems, pp. 1–15, 2022, <span class="No-Break">doi: </span><span class="No-Break">10.1109/TNNLS.2021.3136503</span><span class="No-Break">.</span></li>
				<li>E. Harris, A. Marcu, M. Painter, M. Niranjan, A. Prügel-Bennett, and J. Hare, “<em class="italic">FMix: Enhancing Mixed Sample Data Augmentation</em>.” arXiv, Feb. 28, 2021. Accessed: Aug. 08, 2023. [Online]. <span class="No-Break">Available: </span><a href="http://arxiv.org/abs/2002.12047"><span class="No-Break">http://arxiv.org/abs/2002.12047</span></a><span class="No-Break">.</span></li>
			</ol>
		</div>
	</body></html>