["```py\nmodel.add(tf.layers.conv2d({\n  inputShape: [28, 28, 1],\n  kernelSize: 5,\n  filters: 8,\n  strides: 1,\n  activation: 'relu',\n  kernelInitializer: 'varianceScaling'\n}));\nmodel.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));\nmodel.add(tf.layers.conv2d({\n  kernelSize: 5,\n  filters: 16,\n  strides: 1,\n  activation: 'relu',\n  kernelInitializer: 'varianceScaling'\n}));\nmodel.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));\nmodel.add(tf.layers.flatten());\nmodel.add(tf.layers.dense(\n    {units: 10, kernelInitializer: 'varianceScaling', activation: 'softmax'}));\n```", "```py\nmodel.add(tf.layers.dense(\n    {units: 100, kernelInitializer: 'varianceScaling', activation: 'sigmoid'}));\n```", "```py\nmodel.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));\nmodel.add(tf.layers.flatten());\nmodel.add(tf.layers.dense(\n {units: 100, kernelInitializer: 'varianceScaling', activation: 'sigmoid'}));\nmodel.add(tf.layers.dense(\n    {units: 10, kernelInitializer: 'varianceScaling', activation: 'softmax'}));\n\nconst LEARNING_RATE = 0.15;\n```", "```py\nconst TRAIN_BATCHES = 300;\n```", "```py\n...\nmodel.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));\nmodel.add(tf.layers.conv2d({\n  kernelSize: 5,\n  filters: 2,\n  strides: 1,\n  activation: 'relu',\n  kernelInitializer: 'varianceScaling'\n}));\nmodel.add(tf.layers.maxPooling2d({poolSize: [2, 2], strides: [2, 2]}));\n...\n```", "```py\nfunction createAndCompileModel(\n    layers, hiddenSize, rnnType, digits, vocabularySize) {\n    const maxLen = digits + 1 + digits;\n\n    const model = tf.sequential();\n    model.add(tf.layers.simpleRNN({\n        units: hiddenSize,\n        recurrentInitializer: 'glorotNormal',\n        inputShape: [maxLen, vocabularySize]\n    }));\n    model.add(tf.layers.repeatVector({n: digits + 1}));\n    model.add(tf.layers.simpleRNN({\n        units: hiddenSize,\n        recurrentInitializer: 'glorotNormal',\n        returnSequences: true\n    }));\n    model.add(tf.layers.timeDistributed(\n        {layer: tf.layers.dense({units: vocabularySize})}));\n    model.add(tf.layers.activation({activation: 'softmax'}));\n    model.compile({\n        loss: 'categoricalCrossentropy',\n        optimizer: 'adam',\n        metrics: ['accuracy']\n    });\n    return model;\n}\n```"]