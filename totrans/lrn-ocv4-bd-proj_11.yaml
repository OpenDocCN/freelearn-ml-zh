- en: Text Recognition with Tesseract
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](31f3c15b-57fb-42c6-b67b-5552dfdfa3ac.xhtml), *Developing Segmentation
    Algorithms for Text Recognition*, we covered the very basic OCR processing functions.
    Although they are quite useful for scanned or photographed documents, they are
    almost useless when dealing with text that casually appears in a picture.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll explore the OpenCV 4.0 text module, which deals specifically
    with scene text detection. Using this API, it is possible to detect the text that
    appears in a webcam video, or to analyze photographed images (like the ones in
    Street View or taken by a surveillance camera) to extract text information in
    real time. This allows for a wide range of applications to be created, from accessibility,
    to marketing, and even robotics fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand what scene text recognition is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how the text API works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the OpenCV 4.0 text API to detect text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the detected text into an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the text API and Tesseract integration to identify letters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires familiarity with the basic C++ programming language.
    All of the code used in this chapter can be downloaded from the following GitHub
    link: [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_11](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_11).
    The code can be executed on any operating system, though it is only tested on
    Ubuntu.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2Slht5A](http://bit.ly/2Slht5A)'
  prefs: []
  type: TYPE_NORMAL
- en: How the text API works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The text API implements the algorithm that was proposed by *Lukás Neumann* and
    *Jiri Matas* in the article *Real*-*Time Scene Text Localization and Recognition*
    during the **computer vision and pattern recognition** (**CVPR**) conference in
    2012\. This algorithm represented a significant increase in scene text detection,
    performing state-of-the art detection both in the CVPR database, as well as in
    the Google Street View database. Before using the API, let's take a look at how
    this algorithm works under to hood, and how it addresses the scene text detection
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**Remember**: The OpenCV 4.0 text API does not come with the standard OpenCV
    modules. It''s an additional module that''s present in the OpenCV `contrib` package.
    If you installed OpenCV using the Windows Installer, you should take a look back
    at [Chapter 1](96b225d4-84bc-4d49-b8b3-079b15f05cf0.xhtml), *Getting Started with
    OpenCV;* this will guide you on how to install these modules.'
  prefs: []
  type: TYPE_NORMAL
- en: The scene detection problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Detecting text that randomly appears in a scene is a problem that''s harder
    than it looks. There are several new variables that you need to take into account
    when you''re comparing to identified scanned text, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tridimensionality**: The text may be in any scale, orientation, or perspective.
    Also, the text may be partially occluded or interrupted. There are literally thousands
    of possible regions where it may appear in the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: Text can be in several different fonts and colors. The font may
    have outline borders. The background can be dark, light, or a complex image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Illumination and shadows**: The sunlight''s position and apparent color changes
    over time. Different weather conditions like fog or rain can generate noise. Illumination
    may be a problem even in closed spaces, since light reflects over colored objects
    and hits the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blurring**: Text may appear in a region that''s not prioritized by lens auto-focus.
    Blurring is also common in moving cameras, in perspective text, or in the presence
    of fog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following picture, taken from Google Street View, illustrates these problems.
    Note how several of these situations occur simultaneously in just a single image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5692ac93-9c3f-4ac5-a8bb-c8510e741663.png)'
  prefs: []
  type: TYPE_IMG
- en: Performing text detection to deal with such situations may prove computationally
    expensive, since there are **2*^n*** subsets of pixels, ***n*** being the number
    of pixels in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reduce complexity, two strategies are commonly applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use a sliding window to search just a subset of image rectangles**: This
    strategy just reduces the number of subsets to a smaller amount. The amount of
    regions varies according to the complexity of text being considered. Algorithms
    that deal just with text rotation may use small values, compared to the ones that
    also deal with rotation, skewing, perspective, and so on. The advantage of this
    approach is its simplicity, but they are usually limited to a narrow range of
    fonts and often to a lexicon of specific words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use of connected component analysis**: This approach assumes that pixels
    can be grouped into regions, where pixels have similar properties. These regions
    are supposed to have higher chances to be identified as characters. The advantage
    of this approach is that it does not depend on several text properties (orientation,
    scale, fonts, and so on), and they also provide a segmentation region that can
    be used to crop text to the OCR. This was the approach that we used in [Chapter
    10](31f3c15b-57fb-42c6-b67b-5552dfdfa3ac.xhtml), *Developing Segmentation Algorithms
    for Text Recognition*. Lighting could also affect the result, for example, if
    a shadow is cast over the letters, creating two distinct regions. However, since
    scene detection is commonly used in moving vehicles (for example, drones or cars)
    and with videos, the text will end up being detected eventually, since these lighting
    conditions will differ from frame to frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV 4.0 algorithm uses the second strategy by performing connected component
    analysis and searching for extremal regions.
  prefs: []
  type: TYPE_NORMAL
- en: Extremal regions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Extremal regions are connected areas that are characterized by almost uniform
    intensity, which is surrounded by a contrasted background. The stability of a
    region can be measured by calculating how resistant to thresholding variance the
    region is. This variance can be measured with a simple algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the threshold, generating an image, *A*. Detect its connected pixels regions
    (extremal regions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the threshold by a delta amount, generating an image, *B*. Detect its
    connected pixels regions (extremal regions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare image *B* with *A*. If a region in image A is similar to the same region
    in image *B*, add it to the same branch in the tree. The criteria of similarity
    may vary from implementation to implementation, but it's usually related to the
    image area or general shape. If a region in image *A* appears to be split in image
    *B*, create two new branches in the tree for the new regions and associate it
    with the previous branch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *A* = *B* and go back to step 2, until the maximum threshold is applied.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will assemble a tree of regions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9442351-9576-48c7-b6c7-46bf4361eb11.png)'
  prefs: []
  type: TYPE_IMG
- en: The resistance to variance is determined by counting how many nodes are in the
    same level. By analyzing this tree, it's also possible to determine the **maximally
    stable extremal regions** (**MSER**s), that is, the regions where the area remains
    stable in a wide variety of thresholds. In the previous diagram, it is clear that
    these areas would contain the letters ***O***, ***N***, and ***Y***. The main
    disadvantage of maximally extremal regions is that they are weak in the presence
    of blur. OpenCV provides a MSER feature detector in the **feature2d** module.
    Extremal regions are interesting because they are strongly invariant to illumination,
    scale, and orientation. They are good candidates for text as well, since they
    are also invariant, with regards to the type of font used, even when the font
    is styled. Each region can also be analyzed to determine its boundary ellipsis,
    and can have properties like affine transformation and area numerically determined.
    Finally, it's worth mentioning that this entire process is fast, which makes it
    a very good candidate for real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: Extremal region filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although MSERs are a common approach to define which extremal regions are worth
    working with, the *Neumann* and *Matas* algorithm uses a different approach, by
    submitting all extremal regions to a sequential classifier that''s been trained
    for character detection. This classifier works in two different stages:'
  prefs: []
  type: TYPE_NORMAL
- en: The first stage incrementally computes descriptors (bounding box, perimeter,
    area, and Euler number) for each region. These descriptors are submitted to a
    classifier that estimates how probable the region is to be a character in the
    alphabet. Then, only the regions of high probability are selected for stage 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this stage, the features of the whole area ratio, convex hull ratio, and
    the number of outer boundary inflexion points are calculated. This provides more
    detailed information that allows the classifier to discard non-text characters,
    but they are also much slower to calculate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under OpenCV, this process is implemented in a class called `ERFilter`. It
    is also possible to use different image single channel projections, such as *R*,
    *G*, *B*, Luminance, or gray scale conversion to increase the character recognition
    rates. Finally, all of the characters must be grouped into text blocks (such as
    words or paragraphs). OpenCV 3.0 provides two algorithms for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prune exhaustive search**: Also proposed by *Mattas* in 2011, this algorithm
    does not need any previous training or classification, but is limited to horizontally
    aligned text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical method for oriented text**: This deals with text in any orientation,
    but needs a trained classifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that since these operations require classifiers, it is also necessary
    to provide a trained set as input. OpenCV 4.0 provides some of these trained sets
    in the following sample package: [https://github.com/opencv/opencv_contrib/tree/master/modules/text/samples](https://github.com/opencv/opencv_contrib/tree/master/modules/text/samples).'
  prefs: []
  type: TYPE_NORMAL
- en: This also means that this algorithm is sensitive to the fonts used in classifier
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'A demonstration of this algorithm can be seen in the following video, which
    is provided by Neumann himself: [https://www.youtube.com/watch?v=ejd5gGea2Fo&feature=youtu.be](https://www.youtube.com/watch?v=ejd5gGea2Fo&feature=youtu.be).
    Once the text is segmented, it just needs to be sent to an OCR like Tesseract,
    similarly to what we did in [Chapter 10](31f3c15b-57fb-42c6-b67b-5552dfdfa3ac.xhtml),
    *Developing Segmentation Algorithms for Text Recognition*. The only difference
    is that now we will use OpenCV text module classes to interface with Tesseract,
    since they provide a way to encapsulate the specific OCR engine we are using.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the text API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enough theory. It's time to see how the text module works in practice. Let's
    study how we can use it to perform text detection, extraction, and identification.
  prefs: []
  type: TYPE_NORMAL
- en: Text detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start by creating a simple program so that we can perform text segmentation
    using **ERFilters**. In this program, we will use the trained classifiers from
    text API samples. You may download this from the OpenCV repository, but they are
    also available in this book's companion code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we start by including all of the necessary `libs` and `usings`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall from the *Extremal region filtering* section that the `ERFilter` works
    separately in each image channel. Therefore, we must provide a way to separate
    each desired channel in a different single channel, `cv::Mat`. This is done by
    the `separateChannels` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we verify whether the image is already a single channel image (grayscale
    image). If that''s the case, we just add this image – it does not need to be processed.
    Otherwise, we check if it''s an **RGB** image. For colored images, we call the
    `computeNMChannels` function to split the image into several channels. This function
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`src`: The source input array. It must be a colored image of type 8UC3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`channels`: A vector of `Mats` that will be filled with the resulting channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode`: Defines which channels will be computed. Two possible values can be
    used:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERFILTER_NM_RGBLGrad`: Indicates whether the algorithm will use RGB color,
    lightness, and gradient magnitude as channels (default)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERFILTER_NM_IHSGrad`: Indicates whether the image will be split by its intensity,
    hue, saturation, and gradient magnitude'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We also append the negative of all color components in the vector. Since the
    image will have three distinct channels (*R*, *G*, and *B*), this is usually enough.
    It's also possible to add the non-flipped channels, just like we did with the
    de-grayscaled image, but we'll end up with six channels, and this could be computer-intensive.
    Of course, you're free to test with your images if this leads to a better result.
    Finally, if another kind of image is provided, the function will terminate the
    program with an error message.
  prefs: []
  type: TYPE_NORMAL
- en: Negatives are appended, so the algorithms will cover both bright text in a dark
    background and dark text in a bright background. There is no sense in adding a
    negative for the gradient magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s proceed to the main method. We''ll use this program to segment the `easel.png`
    image, which is provided with the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c102affb-0070-4cab-9e5d-b8a0527490dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This picture was taken by a mobile phone camera while I was walking on the
    street. Let''s code this so that you may also use a different image easily by
    providing its name in the first program argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll convert the image to grayscale and separate its channels by calling
    the `separateChannels` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to work with all of the channels in a colored image, just replace
    the two first lines of this code extract to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to analyze six channels (RGB and inverted) instead of two (gray
    and inverted). Actually, the processing times will increase much more than the
    improvements that we can get. With the channels in hand, we need to create `ERFilters`
    for both stages of the algorithm. Luckily, the OpenCV text contribution module
    provides functions for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For the first stage, we call the `loadClassifierNM1` function to load a previously
    trained classification model. The .xml containing the training data is its only
    argument. Then, we call `createERFilterNM1` to create an instance of the `ERFilter`
    class that will perform the classification. The function has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters for this function are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cb`: The classification model. This is the same model we loaded with the `loadCassifierNM1`
    function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thresholdDelta`: The amount to be summed to the threshold in each algorithm
    iteration. The default value is `1`, but we''ll use `15` in our example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minArea`: The minimum area of the **extremal region** (**ER**), where text
    may be found. This is measured by the percentage of the image''s size. ERs with
    areas smaller than this are immediately discarded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxArea`: The maximum area of the ER where text may be found. This is also
    measured by the percentage of the image''s size. ERs with areas greater than this
    are immediately discarded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minProbability`: The minimum probability that a region must have to be a character
    in order to remain for the next stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nonMaxSupression`: This is used to indicate if non-maximum suppression will
    be done in each branch probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minProbabilityDiff`: The minimum probability difference between the minimum
    and maximum extreme region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The process for the second stage is similar. We call `loadClassifierNM2` to
    load the classifier model for the second stage and `createERFilterNM2` to create
    the second stage classifier. This function only takes the input parameters of
    the loaded classification model and a minimum probability that a region must achieve
    to be considered as a character. So, let''s call these algorithms in each channel
    to identify all possible text regions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we used the `run` function of the `ERFilter` class. This
    function takes two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The input channel**: This includes the image to be processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The regions**: In the first stage algorithm, this argument will be filled
    with the detected regions. In the second stage (performed by `filter2`), this
    argument must contain the regions selected in stage 1\. These will be processed
    and filtered by stage 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we release both filters, since they will not be needed in the program
    anymore. The final segmentation step is grouping all ERRegions into possible words
    and defining their bounding boxes. This is done by calling the `erGrouping` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This function has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the meaning of each parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`img`: Input image, also called the original image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regions`: Vector of single channel images where regions were extracted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`groups`: An output vector of indexes of grouped regions. Each group region
    contains all extremal regions of a single word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`groupRects`: A list of rectangles with the detected text regions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`method`: This is the method of grouping. It can be any of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERGROUPING_ORIENTATION_HORIZ`: The default value. This only generates groups
    with horizontally oriented text by doing an exhaustive search, as proposed originally
    by *Neumann* and *Matas*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERGROUPING_ORIENTATION_ANY`: This generates groups with text in any orientation,
    using single linkage clustering and classifiers. If you use this method, the filename
    of the classifier model must be provided in the next parameter.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Filename`: The name of the classifier model. This is only needed if `ERGROUPING_ORIENTATION_ANY`
    is selected.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minProbability`: The minimum detected probability of accepting a group. This
    is also only needed if `ERGROUPING_ORIENTATION_ANY` is selected.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code also provides a call to the second method, but it''s commented out.
    You may switch between the two to test this out. Just comment the previous call
    and uncomment this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For this call, we also used the default trained classifier that''s provided
    in the text module sample package. Finally, we draw the region boxes and show
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This program outputs the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a069ed42-9847-4e8d-a6e9-f662b1f80337.png)'
  prefs: []
  type: TYPE_IMG
- en: You may check the entire source code in the `detection.cpp` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'While most OpenCV text module functions are written to support both grayscale
    and colored images as its input parameter, at the time of writing this book, there
    were bugs preventing us from using grayscale images in functions such as `erGrouping`.
    For more information, take a look at the following GitHub link: [https://github.com/Itseez/opencv_contrib/issues/309](https://github.com/Itseez/opencv_contrib/issues/309).
    [](https://github.com/Itseez/opencv_contrib/issues/309) Always remember that the
    OpenCV contrib modules package is not as stable as the default OpenCV packages.'
  prefs: []
  type: TYPE_NORMAL
- en: Text extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have detected the regions, we must crop the text before submitting
    it to the OCR. We could simply use a function like `getRectSubpix` or `Mat::copy`,
    using each region rectangle as a **region of interest** (**ROI**) but, since the
    letters are skewed, some undesired text may be cropped as well. For example, this
    is what one of the regions would look like if we just extract the ROI based on
    its given rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f227149d-3d30-4a84-9a38-4af84e3ead02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fortunately, `ERFilter` provides us with an object called `ERStat`, which contains
    pixels inside each extremal region. With these pixels, we could use OpenCV''s
    `floodFill` function to reconstruct each letter. This function is capable of painting
    similar colored pixels based on a seed point, just like the **bucket** tool of
    most drawing applications. This is what the function signature looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s understand these parameters and how they will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`image`: The input image. We''ll use the channel image where the extremal region
    was taken. This is where the function normally does the flood fill, unless `FLOODFILL_MASK_ONLY`
    is supplied. In this case, the image remains untouched and the drawing occurs
    in the mask. That''s exactly what we will do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask`: The mask must be an image with two rows and two columns greater than
    the input image. When the flood fill draws a pixel, it verifies if the corresponding
    pixel in the mask is zero. In that case, it will draw and mark this pixel as one
    (or another value that''s passed into the flags). If the pixel is not zero, the
    flood fill does not paint the pixel. In our case, we''ll provide a blank mask
    so that every letter will get painted in the mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seedPoint`: The starting point. It''s similar to the place you click when
    you want to use the **bucket** tool of a graphic application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`newVal`: The new value of the repainted pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loDiff` and `upDiff`: These parameters represent the lower and upper differences
    between the pixel being processed and its neighbors. The neighbor will be painted
    if it falls into this range. If the `FLOODFILL_FIXED_RANGE` flag is used, the
    difference between the seed point and the pixels being processed will be used
    instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rect`: This is an optional parameter that limits the region where the flood
    fill will be applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags`: This value is represented by a bit mask:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The least significant 8 bits of the flag contains a connectivity value. A value
    of `4` indicates that all four edge pixels will be used, and a value of `8` will
    indicate that the diagonal pixels must also be taken into account. We'll use `4`
    for this parameter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The next 8 to 16 bits contains a value from `1` to `255`, which is used to fill
    the mask. Since we want to fill the mask with white, we'll use `255 << 8` for
    this value.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two more bits that can be set by adding the `FLOODFILL_FIXED_RANGE`
    and `FLOODFILL_MASK_ONLY` flags, as we already described.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll create a function called `drawER`. This function will receive four parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: A vector with all of the processed channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ERStat` region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The group that must be drawn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The group rectangle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This function will return an image with the word represented by this group.
    Let''s start this function by creating the mask image and defining the flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll loop through each group. It''s necessary to find the region index
    and its status. There''s a chance of this extreme region being the root, which
    does not contain any points. In this case, we''ll just ignore it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can read the pixel coordinate from the `ERStat` object. It''s represented
    by the pixel number, counting from top to bottom, left to right. This linear index
    must be converted to a row (*y*) and column (*z*) notation, using a formula similar
    to the one we saw in [Chapter 2](37cf2702-b8c6-41ff-a935-fd4030f8ce64.xhtml),
    *An Introduction to the Basics of OpenCV*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can call the `floodFill` function. The `ERStat` object gives us the
    value to use in the `loDiff` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After we do this for all of the regions in the group, we''ll end with an image
    that''s a little bigger than the original one, with a black background and the
    word in white letters. Now, let''s crop just the area of the letters. Since the
    region rectangle was given, we start by defining it as our region of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll find all non-zero pixels. This is the value we''ll use in the
    `minAreaRect` function to get the rotated rectangle around the letters. Finally,
    we will borrow the previous chapter''s `deskewAndCrop` function to crop and rotate
    the image for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result of the process for the easel image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae3ff958-8639-4052-aa97-c6474f119b77.png)'
  prefs: []
  type: TYPE_IMG
- en: Text recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](31f3c15b-57fb-42c6-b67b-5552dfdfa3ac.xhtml), *Developing Segmentation
    Algorithms for Text Recognition,* we used the Tesseract API directly to recognize
    the text regions. This time, we'll use OpenCV classes to accomplish the same goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'In OpenCV, all OCR-specific classes derive from the **BaseOCR** virtual class.
    This class provides a common interface for the OCR execution method itself. Specific
    implementations must inherit from this class. By default, the text module provides
    three different implementations: **OCRTesseract**, **OCRHMMDecoder**, and **OCRBeamSearchDecoder**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This hierarchy is depicted in the following class diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/463bba5c-a8d2-4b38-b981-7c4d7e23b74f.png)'
  prefs: []
  type: TYPE_IMG
- en: With this approach, we can separate the part of the code where the OCR mechanism
    is created from the execution itself. This makes it easier to change the OCR implementation
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s start by creating a method that decides which implementation we''ll
    use based on a string. We currently support just Tesseract, but you may take a
    look in this chapter''s code, where a demonstration with **HMMDecoder** is also
    provided. Also, we are accepting the OCR engine name in a string parameter, but
    we could improve our application''s flexibility by reading it from an external
    JSON or XML configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As you may have noticed, the function returns `Ptr<BaseOCR>`. Now, take a look
    at the highlighted code. It calls the `create` method to initialize a Tesseract
    OCR instance. Let''s take a look at its official signature, since it allows several
    specific parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s dissect each of these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`datapath`: This is the path to the root directory''s `tessdata` files. The
    path must end with a backslash `/` character. The `tessdata` directory contains
    the language files you installed. Passing `nullptr` to this parameter will make
    Tesseract search in its installation directory, which is the location where this
    folder is normally present. It''s common to change this value to `args[0]` when
    deploying an application and include the `tessdata` folder in your application
    path.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`language`: This is a three letter word with the language code (for example,
    eng for English, por for Portuguese, or hin for Hindi). Tesseract supports the
    loading of multiple language codes by using the `+` sign. Therefore, passing `eng+por`
    will load both English and Portuguese languages. Of course, you can only use languages
    that you have previously installed, otherwise the loading will fail. A language
    `config` file may specify that two or more languages must be loaded together.
    To prevent that, you may use a tilde `~`. For example, you can use `hin+~eng`
    to guarantee that English is not loaded with Hindi, even if it is configured to
    do so.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`whitelist`: This is the character that''s set to be considered for recognition.
    In the case that `nullptr` is passed, the characters will be `0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oem`: These are the OCR algorithms that will be used. It can have one of the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OEM_TESSERACT_ONLY`: Uses just Tesseract. It''s the fastest method, but it
    also has less precision.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OEM_CUBE_ONLY`: Uses Cube engine. It''s slower, but more precise. This will
    only work if your language was trained to support this engine mode. To check if
    that''s the case, look for `.cube` files for your language in the `tessdata` folder.
    The support for English language is guaranteed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OEM_TESSERACT_CUBE_COMBINED`: Combines both Tesseract and Cube to achieve
    the best possible OCR classification. This engine has the best accuracy and the
    slowest execution time.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OEM_DEFAULT`: Infers the strategy based in the language config file, command-line
    config file or, in the absence of both, use `OEM_TESSERACT_ONLY`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`psmode`: This is the segmentation mode. It can be any of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_OSD_ONLY:` Using this mode, Tesseract will just run its preprocessing
    algorithms to detect orientation and script detection.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_AUTO_OSD`: This tells Tesseract to do automatic page segmentation with
    orientation and script detection.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_AUTO_ONLY`: Does page segmentation, but avoids doing orientation, script
    detection, or OCR. This is the default value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_AUTO`: Does page segmentation and OCR, but avoids doing orientation or
    script detection.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_COLUMN`: Assumes that the text of variable sizes is displayed in
    a single column.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_BLOCK_VERT_TEXT`: Treats the image as a single uniform block of
    vertically aligned text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_BLOCK`: Assumes a single block of text. This is the default configuration.
    We will use this flag since our preprocessing phase guarantees this condition.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_LINE`: Indicates that the image contains only one line of text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_WORD`: Indicates that the image contains just one word.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_WORD_CIRCLE`: Indicates that the image is a just one word disposed
    in a circle.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_CHAR`: Indicates that the image contains a single character.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the last two parameters, it''s recommended that you use the `#include`
    Tesseract directory to use the constant names instead of directly inserting their
    values. The last step is to add text detection in our main function. To do this,
    just add the following code to the end of the main method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we started by calling our `initOCR` method to create a Tesseract
    instance. Note that the remaining code will not change if we chose a different
    OCR engine, since the run method signature is guaranteed by the `BaseOCR` class.
    Next, we iterate over each detected `ERFilter` group. Since each group represents
    a different word, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Call the previously created `drawER` function to create an image with the word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a text string called `word`, and call the `run` function to recognize
    the word image. The recognized word will be stored in the string.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the text string in the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s take a look at the `run` method signature. This method is defined in
    the `BaseOCR` class, and will be equal for all specific OCR implementations –
    even the ones that might be implemented in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, this is a pure virtual function that must be implemented by each
    specific class (such as the `OCRTesseract` class we just used):'
  prefs: []
  type: TYPE_NORMAL
- en: '`image`: The input image. It must be a RGB or a grayscale image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_rects`: We can provide a vector to be filled with the bounding box
    of each component (words or text lines) that''s detected by the OCR engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_texts`: If given, this vector will be filled with the text strings
    of each component detected by the OCR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_confidences`: If given, the vector will be filled with floats, with
    the confidence values of each component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_level`: Defines what a component is. It may have the values `OCR_LEVEL_WORD`
    (by default), or `OCR_LEVEL_TEXT_LINE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If necessary, you may prefer changing the component level to a word or line
    in the `run()` method instead of doing the same thing in the `psmode` parameter
    of the `create()` function. This is preferable since the `run` method will be
    supported by any OCR engine that decides to implement the `BaseOCR` class. Always
    remember that the `create()` method is where vendor-specific configurations are
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the program''s final output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2899645e-f4e4-416c-95f9-eca150d92aa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite a minor confusion with the `&` symbol, every word was perfectly recognized.
    You may check the entire source code in the `ocr.cpp` file, in this chapter's
    code file.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw that scene text recognition is a far more difficult
    OCR situation than working with scanned texts. We studied how the text module
    addresses this problem with extremal region identification using the *Newmann*
    and *Matas* algorithm. We also saw how to use this API with the `floodFill` function
    to extract the text in to an image and submit it to Tesseract OCR. Finally, we
    studied how the OpenCV text module integrates with Tesseract and other OCR engines,
    and how can we use its classes to identify what's written in an image.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will be introduced to deep learning in OpenCV. You
    will learn about object detection and classification by using the **you only look
    once** (**YOLO**) algorithm.
  prefs: []
  type: TYPE_NORMAL
