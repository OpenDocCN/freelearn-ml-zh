<html><head></head><body>
  <div id="_idContainer364" class="Basic-Text-Frame">
    <h1 class="chapterNumber">13</h1>
    <h1 id="_idParaDest-378" class="chapterTitle">Adversarial Robustness</h1>
    <p class="normal">Machine learning interpretation has many concerns, ranging from knowledge discovery to high-stakes ones with tangible ethical implications, like the fairness issues examined in the last two chapters. In this chapter, we will direct our attention to concerns involving reliability, safety, and security.</p>
    <p class="normal">As we realized using the <strong class="keyWord">contrastive explanation method</strong> in <em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Convolutional Neural Networks</em>, we can easily trick an image classifier into making embarrassingly false predictions. This ability can have serious ramifications. For instance, a perpetrator can place a black sticker on a yield sign, and while most drivers would still recognize this as a yield sign, a self-driving car may no longer recognize it and, as a result, crash. A bank robber could wear a cooling suit designed to trick the bank vault’s thermal imaging system, and while any human would notice it, the imaging system would fail to do so.</p>
    <p class="normal">The risk is not limited to sophisticated image classifiers. Other models can be tricked! The <strong class="keyWord">counterfactual examples</strong> produced in <em class="chapterRef">Chapter 6</em><em class="italic">, Anchors and Counterfactual Explanations,</em> are like adversarial examples except with the goal of deception. An attacker could leverage any misclassification example, straddling the decision boundary adversarially. For instance, a spammer could realize that adjusting some email attributes increases the likelihood of circumventing spam filters.</p>
    <p class="normal">Complex models are more vulnerable to adversarial attacks. So why would we trust them?! We can certainly make them more foolproof, and that’s what adversarial robustness entails. An adversary can purposely thwart a model in many ways, but we will focus on evasion attacks and briefly explain other forms of attacks. Then we will explain two defense methods: spatial smoothing preprocessing and adversarial training. Lastly, we will demonstrate one robustness evaluation method.</p>
    <p class="normal">These are the main topics we will cover:</p>
    <ul>
      <li class="bulletList">Learning about evasion attacks</li>
      <li class="bulletList">Defending against targeted attacks with preprocessing</li>
      <li class="bulletList">Shielding against any evasion attack through adversarial training of a robust classifier</li>
    </ul>
    <h1 id="_idParaDest-379" class="heading-1">Technical requirements</h1>
    <p class="normal">This chapter’s example uses the <code class="inlineCode">mldatasets</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">tensorflow</code>, <code class="inlineCode">keras</code>, <code class="inlineCode">adversarial-robustness-toolbox</code>, <code class="inlineCode">matplotlib</code>, and <code class="inlineCode">seaborn</code> libraries. Instructions on how to install all of these libraries are in the <em class="italic">Preface</em>. </p>
    <div class="note">
      <p class="normal">The code for this chapter is located here: <a href="https://packt.link/1MNrL"><span class="url">https://packt.link/1MNrL</span></a></p>
    </div>
    <h1 id="_idParaDest-380" class="heading-1">The mission</h1>
    <p class="normal">The privately contracted security services industry market worldwide is valued at over USD 250 billion and is growing at around 5% annually. However, it faces many challenges, such as shortages of adequately trained guards and specialized security experts in many jurisdictions, and a whole host of unexpected security threats. These threats include widespread coordinated cybersecurity attacks, massive riots, social upheaval, and, last but not least, health risks brought on by pandemics. Indeed, 2020 tested the industry with a wave of ransomware, misinformation attacks, protests, and COVID-19 to boot.</p>
    <p class="normal">In the wake of this, one of the largest hospital networks in the United States asked their contracted security company to monitor the correct use of masks by both visitors and personnel throughout the hospitals. The security company has struggled with this request because it diverts security personnel from tackling other threats, such as intruders, combative patients, and belligerent visitors. It has video surveillance in every hallway, operating room, waiting room, and hospital entrance. It’s impossible to have eyes on every camera feed every time, so the security company thought they could assist guards with deep learning models.</p>
    <p class="normal">These models already alert unusual activities, such as running in the hallways and brandishing weapons anywhere on the premises. They have proposed to the hospital network that they would like to add a new model that detects masks’ correct usage. Before COVID-19, there were policies in place for mandatory mask usage in certain areas of each hospital, and during COVID-19, it was required everywhere. The hospital administrators would like to turn on and off this monitoring feature, depending on pandemic risk levels moving forward. They realize that personnel get fatigued and forget to put masks back on, or they partially slip off at times. Many visitors are also hostile toward using masks and may wear one when entering the hospital but take it off when no guard is around. This isn’t always intentional, so they wouldn’t want to dispatch guards on every alert, unlike other threats. Instead, they’d rather use awareness and a little bit of shame to modify behavior and only intervene with repeat offenders:</p>
    <figure class="mediaobject"><img src="../Images/B18406_13_01.png" alt="A yellow sign on a pole  Description automatically generated with low confidence"/></figure>
    <p class="packt_figref">Figure 13.1: Radar speed signs like this one help curb speeding</p>
    <p class="normal">Awareness is a very effective method with radar speed signs (see <em class="italic">Figure 13.1</em>), which make roads safer by only making drivers aware that they are driving too fast. Likewise, having a screen at the end of heavily trafficked hallways showing snapshots of those who have recently either mistakenly or purposely not complied with mandatory mask usage potentially creates some embarrassment for offenders. The system will log repeat offenders so that security guards can look for them and either make them comply or ask them to vacate the premises.</p>
    <p class="normal">There’s some concern with visitors trying to trick the model into evading compliance, so the security company has hired you to ensure that the model is robust in the face of this kind of adversarial attack. Security officers have noticed some low-tech trickery before, such as people momentarily covering their faces with their hands or a part of their sweater when they realize cameras monitor them. Also, in one disturbing incident, a visitor dimmed the lights and sprayed some gel on a camera, and in another, an individual painted their mouth. However, there are concerns about higher-tech attacks, such as jamming the camera’s wireless signal or shining high-powered lasers directly into cameras. Devices that perform these attacks are increasingly easier to obtain and could impact other surveillance functions on a larger scale, like preventing theft. The security company hopes this robustness exercise can inform their efforts to improve every surveillance system and model.</p>
    <p class="normal">Eventually, the security company would like to produce its own dataset with face images from the hospitals they monitor. Meanwhile, synthetically masked faces from external sources are the best they can do to productionize a model in the short term. To this end, you have been provided a large dataset of synthetically correctly and incorrectly masked faces and their unmasked counterparts. The two datasets were combined into a single one, and the original dimensions of 1,024 × 1,024 were reduced to the thumbnail size of 124 × 124. Also, for efficiency’s sake, 21,000 images were sampled from roughly 210,000 in these datasets.</p>
    <h1 id="_idParaDest-381" class="heading-1">The approach</h1>
    <p class="normal">You’ve decided to take a four-fold approach:</p>
    <ul>
      <li class="bulletList">Exploring several possible evasion attacks to understand how vulnerable the model is to them and how credible they are as threats</li>
      <li class="bulletList">Using a preprocessing method to protect a model against these attacks</li>
      <li class="bulletList">Leveraging adversarial retraining to produce a robust classifier that is intrinsically less prone to many of these attacks</li>
      <li class="bulletList">Evaluating robustness with state-of-the-art methods to assure hospital administrators that the model is adversarially robust</li>
    </ul>
    <p class="normal">Let’s get started!</p>
    <h1 id="_idParaDest-382" class="heading-1">The preparations</h1>
    <p class="normal">You will<a id="_idIndexMarker1439"/> find the code for this example here: <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb</span></a> </p>
    <h2 id="_idParaDest-383" class="heading-2">Loading the libraries</h2>
    <p class="normal">To run<a id="_idIndexMarker1440"/> this example, you need to install the following libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mldatasets</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">numpy</code> and <code class="inlineCode">sklearn</code> (scikit-learn) to manipulate it</li>
      <li class="bulletList"><code class="inlineCode">tensorflow</code> to fit the models</li>
      <li class="bulletList"><code class="inlineCode">matplotlib</code> and <code class="inlineCode">seaborn</code> to visualize the interpretations</li>
    </ul>
    <p class="normal">You should load all of them first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> warnings
warnings.filterwarnings("ignore")
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> preprocessing
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras.utils <span class="hljs-keyword">import</span> get_file
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
<span class="hljs-keyword">from</span> art.estimators.classification <span class="hljs-keyword">import</span> KerasClassifier
<span class="hljs-keyword">from</span> art.attacks.evasion <span class="hljs-keyword">import</span> FastGradientMethod,\
                      ProjectedGradientDescent, BasicIterativeMethod
<span class="hljs-keyword">from</span> art.attacks.evasion <span class="hljs-keyword">import</span> CarliniLInfMethod
<span class="hljs-keyword">from</span> art.attacks.evasion <span class="hljs-keyword">import</span> AdversarialPatchNumpy
<span class="hljs-keyword">from</span> art.defences.preprocessor <span class="hljs-keyword">import</span> SpatialSmoothing
<span class="hljs-keyword">from</span> art.defences.trainer <span class="hljs-keyword">import</span> AdversarialTrainer
<span class="hljs-keyword">from</span> tqdm.notebook <span class="hljs-keyword">import</span> tqdm
</code></pre>
    <p class="normal">Let’s check that TensorFlow has loaded the right version with <code class="inlineCode">print(tf.__version__)</code>. The version should be 2.0 and above.</p>
    <p class="normal">We should also disable eager execution and verify that it worked with the following command: </p>
    <pre class="programlisting code"><code class="hljs-code">tf.compat.v1.disable_eager_execution()
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Eager execution enabled:'</span>, tf.executing_eagerly())
</code></pre>
    <p class="normal">The<a id="_idIndexMarker1441"/> output should say that it’s <code class="inlineCode">False</code>.</p>
    <div class="note">
      <p class="normal">In TensorFlow, turning eager execution mode on means that it doesn’t require a computational graph or a session. It’s the default for TensorFlow 2.x and later but not in prior versions, so you need to disable it to avoid incompatibilities with code that was optimized for prior versions of TensorFlow.</p>
    </div>
    <h2 id="_idParaDest-384" class="heading-2">Understanding and preparing the data</h2>
    <p class="normal">We will load the<a id="_idIndexMarker1442"/> data into four NumPy arrays, corresponding to the training/test datasets. While we are at it, we will divide <code class="inlineCode">X</code> face images by 255 because, that way, they will be of values between zero and one, which is better for deep learning models. We call this feature scaling. We will need to record the <code class="inlineCode">min_</code> and <code class="inlineCode">max_</code> for the training data because we will need these later:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train, X_test, y_train, y_test = mldatasets.load(
    <span class="hljs-string">"maskedface-net_thumbs_sampled"</span>, prepare=<span class="hljs-literal">True</span>
)
X_train, X_test = X_train / <span class="hljs-number">255.0</span>, X_test / <span class="hljs-number">255.0</span>
min_ = X_train.<span class="hljs-built_in">min</span>()
max_ = X_train.<span class="hljs-built_in">max</span>()
</code></pre>
    <p class="normal">It’s always important to verify our data when we load it to make sure it didn’t get corrupted:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">'X_train dim:\t%s'</span> % (X_train.<span class="code-highlight"><strong class="hljs-slc">shape</strong></span>,))
<span class="hljs-built_in">print</span>(<span class="hljs-string">'X_test dim:\t%s'</span> % (X_test.<span class="code-highlight"><strong class="hljs-slc">shape</strong></span>,))
<span class="hljs-built_in">print</span>(<span class="hljs-string">'y_train dim:\t%s'</span> % (y_train.<span class="code-highlight"><strong class="hljs-slc">shape</strong></span>,))
<span class="hljs-built_in">print</span>(<span class="hljs-string">'y_test dim:\t%s'</span> % (y_test.<span class="code-highlight"><strong class="hljs-slc">shape</strong></span>,))
<span class="hljs-built_in">print</span>(<span class="hljs-string">'X_train min:\t%s'</span> % (<span class="code-highlight"><strong class="hljs-slc">min_</strong></span>))
<span class="hljs-built_in">print</span>(<span class="hljs-string">'X_train max:\t%s'</span> % (<span class="code-highlight"><strong class="hljs-slc">max_</strong></span>))
<span class="hljs-built_in">print</span>(<span class="hljs-string">'y_train labels:\t%s'</span> % (np.<span class="code-highlight"><strong class="hljs-slc">unique</strong></span>(y_train)))
</code></pre>
    <p class="normal">The preceding snippet will produce the following output, which tells us that the images have dimensions of 128 × 128 pixels and three channels (color). There are 16,800 training images and 4,200 test images. The labels only have a 1 in the second value, which indicates <a id="_idIndexMarker1443"/>that they are not one-hot encoded. Indeed, by printing the unique values (<code class="inlineCode">np.unique(y_train)</code>), we can tell that labels are represented as text: <code class="inlineCode">Correct</code> for correctly masked, <code class="inlineCode">Incorrect</code> for incorrectly masked, and <code class="inlineCode">None</code> for no mask:</p>
    <pre class="programlisting con"><code class="hljs-con">X_train dim:    (16800, 128, 128, 3)
X_test dim: (4200, 128, 128, 3)
y_train dim:    (16800, 1)
y_test dim: (4200, 1)
X_train min:    0.0
X_train max:    1.0
y_train labels: ['Correct' 'Incorrect' 'None']
</code></pre>
    <p class="normal">Therefore, a preprocessing step we will need to perform is to <strong class="keyWord">One-Hot Encode</strong> (<strong class="keyWord">OHE</strong>) the <code class="inlineCode">y</code> labels because we will need the OHE form to evaluate the model’s predictive performance. Once we initialize the <code class="inlineCode">OneHotEncoder</code>, we will need to <code class="inlineCode">fit</code> it into the training data (<code class="inlineCode">y_train</code>). We can also extract the categories from the encoder into a list (<code class="inlineCode">labels_l</code>) to verify that it has all three:</p>
    <pre class="programlisting code"><code class="hljs-code">ohe = preprocessing.<span class="code-highlight"><strong class="hljs-slc">OneHotEncoder</strong></span>(sparse=<span class="hljs-literal">False</span>)
ohe.<span class="code-highlight"><strong class="hljs-slc">fit</strong></span>(y_train)
labels_l = ohe.<span class="code-highlight"><strong class="hljs-slc">categories_</strong></span>[<span class="hljs-number">0</span>].tolist()
<span class="hljs-built_in">print</span>(labels_l)
</code></pre>
    <p class="normal">For reproducibility’s sake, always initialize your random seeds like this:</p>
    <pre class="programlisting code"><code class="hljs-code">rand = <span class="hljs-number">9</span>
os.environ[<span class="hljs-string">'PYTHONHASHSEED'</span>] = <span class="hljs-built_in">str</span>(rand)
tf.random.set_seed(rand)
np.random.seed(rand)
</code></pre>
    <p class="normal">Making machine learning truly reproducible means also making it deterministic, which means that training with the same data will produce a model with the same parameters. Determinism is very difficult with deep learning and is often session-, platform-, and architecture-dependent. If you use an NVIDIA GPU, you can install a library called <code class="inlineCode">framework-reproducibility</code>.</p>
    <p class="normal">Many of the adversarial attack, defense, and evaluation methods we will study in this chapter are very resource-intensive, so if we used the entire test dataset with them, they could likely take many hours on a single method! For efficiency, it is strongly suggested to use samples of the test dataset. Therefore, we will create a medium 200-image sample (<code class="inlineCode">X_test_mdsample</code>, <code class="inlineCode">y_test_mdsample</code>) and a small 20-image sample (<code class="inlineCode">X_test_smsample</code>, <code class="inlineCode">y_test_smsample</code>) using <code class="inlineCode">np.random.choice</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">sampl_md_idxs = np.random.choice(X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-number">200</span>, replace=<span class="hljs-literal">False</span>)
X_test_mdsample = X_test[sampl_md_idxs]
y_test_mdsample = y_test[sampl_md_idxs]
sampl_sm_idxs = np.random.choice(X_test.shape[<span class="hljs-number">0</span>], <span class="hljs-number">20</span>, replace=<span class="hljs-literal">False</span>)
X_test_smsample = X_test[sampl_sm_idxs]
y_test_smsample = y_test[sampl_sm_idxs]
</code></pre>
    <p class="normal">We<a id="_idIndexMarker1444"/> have two sample sizes because some methods could take too long with a larger sample size. Now, let’s take a peek at what images are in our datasets. In the preceding code, we have taken a medium and small sample of our test dataset. We will place each image of our small sample in a 4 × 5 grid with the class label above it, with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.subplots(figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">12</span>))
<span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):
    plt.subplot(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, s+<span class="hljs-number">1</span>)
    plt.title(y_test_smsample[s][<span class="hljs-number">0</span>], fontsize=<span class="hljs-number">12</span>)
    plt.imshow(X_test_smsample[s], interpolation=<span class="hljs-string">'spline16'</span>)
    plt.axis(<span class="hljs-string">'off'</span>)
plt.show()
</code></pre>
    <p class="normal">The preceding code plots the grid of images in <em class="italic">Figure 13.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_13_02.png" alt="A collage of a person  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 13.2: A small test dataset sample of masked and unmasked faces</p>
    <p class="normal"><em class="italic">Figure 13.2</em> depicts<a id="_idIndexMarker1445"/> a variety of correctly and incorrectly masked and unmasked faces of all ages, genders, and ethnicities. Despite the variety, one thing to note about this dataset is that it only has light blue surgical masks represented, and images are mostly at a front-facing angle. Ideally, we would generate an even larger dataset with all colors and types of masks and augment it further with random rotations, shears, and brightness adjustments, either before or during training. These augmentations would make for a much more robust model. Nevertheless, we must differentiate between this general type of robustness and adversarial robustness. </p>
    <h2 id="_idParaDest-385" class="heading-2">Loading the CNN base model</h2>
    <p class="normal">You don’t<a id="_idIndexMarker1446"/> have to train the CNN base model, but the code to do so is provided nonetheless in the GitHub repository. The pretrained model has also been stored there. We can quickly load the model and output its summary like this:</p>
    <pre class="programlisting code"><code class="hljs-code">model_path = <span class="code-highlight"><strong class="hljs-slc">get_file</strong></span>(<span class="hljs-string">'CNN_Base_MaskedFace_Net.hdf5'</span>,\
    <span class="hljs-string">'https://github.com/PacktPublishing/Interpretable-Machine- </span>\
<span class="hljs-string">    Learning-with-Python/blob/master/models/ </span>\
<span class="hljs-string">    CNN_Base_MaskedFace_Net.hdf5?raw=true'</span>)
base_model = tf.keras.models.<span class="code-highlight"><strong class="hljs-slc">load_model</strong></span>(model_path)
base_model.<span class="code-highlight"><strong class="hljs-slc">summary</strong></span>()
</code></pre>
    <p class="normal">The preceding snippet outputs the following summary: </p>
    <pre class="programlisting con"><code class="hljs-con">Model: "CNN_Base_MaskedFaceNet_Model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 126, 126, 16)      448       _________________________________________________________________
maxpool2d_1 (MaxPooling2D)   (None, 63, 63, 16)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 61, 61, 32)        4640      
_________________________________________________________________
maxpool2d_2 (MaxPooling2D)   (None, 30, 30, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     
_________________________________________________________________
maxpool2d_3 (MaxPooling2D)   (None, 14, 14, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     
_________________________________________________________________
maxpool2d_4 (MaxPooling2D)   (None, 6, 6, 128)         0         
_________________________________________________________________
flatten_6 (Flatten)          (None, 4608)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 768)               3539712   
_________________________________________________________________
dropout_6 (Dropout)          (None, 768)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 2307      
=================================================================
Total params: 3,639,459
Trainable params: 3,639,459
Non-trainable params: 0
_________________________________________________________________
</code></pre>
    <p class="normal">The<a id="_idIndexMarker1447"/> summary has pretty much everything we need to know about the model. It has four convolutional layers (<code class="inlineCode">Conv2D</code>), each followed by a max pool layer (<code class="inlineCode">MaxPooling2D</code>). It then has a <code class="inlineCode">Flatten</code> layer and a fully connected layer (<code class="inlineCode">Dense</code>). Then, there’s more <code class="inlineCode">Dropout</code> before the second <code class="inlineCode">Dense</code> layer. Naturally, three neurons are in this final layer, corresponding to each class.</p>
    <h2 id="_idParaDest-386" class="heading-2">Assessing the CNN base classifier</h2>
    <p class="normal">We can<a id="_idIndexMarker1448"/> evaluate the model using the test dataset with the <code class="inlineCode">evaluate_multiclass_mdl</code> function. The arguments include the model (<code class="inlineCode">base_model</code>), our test data (<code class="inlineCode">X_test</code>), and the corresponding labels (<code class="inlineCode">y_test</code>), as well as the class names (<code class="inlineCode">labels_l</code>) and the encoder (<code class="inlineCode">ohe</code>). Lastly, we don’t need to plot the ROC curves since, given the high accuracy, they won’t be very informative (<code class="inlineCode">plot_roc=False</code>). This function returns the predicted labels and probabilities, which we can store as variables for later use:</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_pred, y_test_prob = mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_multiclass_mdl</strong></span>(
    base_model,
    X_test,
    y_test,
    labels_l,
    ohe,
    plot_conf_matrix=<span class="hljs-literal">True</span>,
    predopts={<span class="hljs-string">"verbose"</span>:<span class="hljs-number">1</span>}
)
</code></pre>
    <p class="normal">The preceding code generates <em class="italic">Figure 13.3</em>, with a confusion matrix and performance metrics for each class:</p>
    <figure class="mediaobject"><img src="../Images/B18406_13_03.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 13.3: The confusion matrix and predictive performance metrics for the base classifier, evaluated on the test dataset</p>
    <p class="normal">Even though the confusion matrix in <em class="italic">Figure 13.3</em> seems to suggest a perfect classification, pay<a id="_idIndexMarker1449"/> attention to the circled areas. We can tell the model had some issues with misclassifying incorrectly masked faces once we see the recall (99.5%) breakdown.</p>
    <p class="normal">Now, we can start attacking this model to assess how robust it actually is!</p>
    <h1 id="_idParaDest-387" class="heading-1">Learning about evasion attacks</h1>
    <p class="normal">There are six broad categories of adversarial attacks:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Evasion</strong>: designing an <a id="_idIndexMarker1450"/>input that can cause a <a id="_idIndexMarker1451"/>model to make an incorrect prediction, especially when it wouldn’t fool a human observer. It can either be targeted or untargeted, depending on the attacker’s intention to fool the model into misclassifying a specific class (targeted) or, rather, misclassifying any class (untargeted). The attack methods can be white-box if the attacker has full access to the model and its training dataset, or black-box with only inference access. Gray-box sits in the middle. Black-box is always model-agnostic, whereas white and gray-box methods might be.</li>
      <li class="bulletList"><strong class="keyWord">Poisoning</strong>: injecting<a id="_idIndexMarker1452"/> faulty training data or parameters<a id="_idIndexMarker1453"/> into a model can come in many forms, depending on the attacker’s capabilities and access. For instance, for systems with user-generated data, the attacker may be capable of adding faulty data or labels. If they have more access, perhaps they can modify large amounts of data. They can also adjust the learning algorithm, hyperparameters, or data augmentation schemes. Like evasion, poisoning can also be targeted and untargeted.</li>
      <li class="bulletList"><strong class="keyWord">Inference</strong>: extracting the training dataset through model inference. Inference attacks<a id="_idIndexMarker1454"/> also come in many forms <a id="_idIndexMarker1455"/>and can be used for espionage (privacy attacks) through membership inference, which confirms if one example (for instance, a specific person) was in the training dataset. Attribute inference ascertains if an example category (for instance, an ethnicity) was represented in the training data. Input inference (also known as model inversion) has attack methods to extract a training dataset from a model rather than guessing and confirming. These have broad privacy and regulatory implications, especially in medical and legal applications. </li>
      <li class="bulletList"><strong class="keyWord">Trojaning</strong>: this<a id="_idIndexMarker1456"/> implants malicious functionality<a id="_idIndexMarker1457"/> activated with a trigger during inference but requires retraining the model.</li>
      <li class="bulletList"><strong class="keyWord">Backdooring</strong>: similar to <a id="_idIndexMarker1458"/>trojans but a backdoor<a id="_idIndexMarker1459"/> remains, even when a model is retrained from scratch.</li>
      <li class="bulletList"><strong class="keyWord">Reprogramming</strong>: remote <a id="_idIndexMarker1460"/>sabotaging of a model during training by sneaking in examples that are specifically designed to produce specific outputs. For instance, if you provide enough examples labeled as tiger shark where four small black squares are always in the same place, the model will learn that that is a tiger shark, regardless of what it is, thus intentionally forcing the model to overfit.</li>
    </ul>
    <p class="normal">The first three are the most studied <a id="_idIndexMarker1461"/>forms of adversarial attacks. Attacks can be further subcategorized once we split them by stage and goal (see <em class="italic">Figure 13.4</em>). The stage is when the attack is perpetrated because it can impact the model training or its inference, and the goal is what the attacker hopes to gain from it. This chapter will only deal with evasion sabotage attacks because we expect hospital visitors, patients, and personnel to occasionally sabotage the production model:</p>
    <figure class="mediaobject"><img src="../Images/B18406_13_04.png" alt="" role="presentation"/></figure>
    <p class="packt_figref">Figure 13.4: Table of adversarial attack category methods by stage and goal</p>
    <p class="normal">Even though we use white-box methods to attack, defend, and evaluate a model’s robustness, we don’t expect attackers to have this level of access. We will only use white-box methods because we have full access to the model. In other circumstances, such as a bank surveillance system with a thermal imaging system and a corresponding model to detect perpetrators, we could expect professional attackers to use black-box methods to find vulnerabilities! So, as defenders of this system, we would be wise to try the very same attack methods.</p>
    <p class="normal">The library we will use for adversarial robustness<a id="_idIndexMarker1462"/> is called the <strong class="keyWord">Adversarial Robustness Toolbox</strong> (<strong class="keyWord">ART</strong>), and it’s supported by the <strong class="keyWord">LF AI &amp; Data Foundation</strong> – the <a id="_idIndexMarker1463"/>same folks that support other open-source projects such as AIX360 and AIF360, explored in <em class="chapterRef">Chapter 11</em><em class="italic">, Bias Mitigation and Causal Inference Methods</em>. ART requires that attacked models are abstracted in an estimator or classifier, even if it’s a black-box one. We will use <code class="inlineCode">KerasClassifier</code> for most of this chapter except for the last section, in which we will use <code class="inlineCode">TensorFlowV2Classifier</code>. Initializing an ART classifier is simple. You must specify the <code class="inlineCode">model</code>, and <a id="_idIndexMarker1464"/>sometimes there are other required attributes. For <code class="inlineCode">KerasClassifier</code>, all remaining attributes are optional, but it is recommended you use <code class="inlineCode">clip_values</code> to specify the range of the features. Many attacks are input permutations, so knowing what input values are allowed or feasible is essential:</p>
    <pre class="programlisting code"><code class="hljs-code">base_classifier = <span class="code-highlight"><strong class="hljs-slc">KerasClassifier</strong></span>(
    model=base_model, clip_values=(min_, max_)
)
y_test_mdsample_prob = np.<span class="code-highlight"><strong class="hljs-built_in-slc">max</strong></span>(
    y_test_prob[sampl_md_idxs], axis=<span class="hljs-number">1</span>
)
y_test_smsample_prob = np.<span class="code-highlight"><strong class="hljs-built_in-slc">max</strong></span>(
    y_test_prob[sampl_sm_idxs], axis=<span class="hljs-number">1</span>
)
</code></pre>
    <p class="normal">In the preceding code, we also prepare two arrays with probabilities for the predicted class of the medium and small samples. It is entirely optional, but these assist in placing the predicted probability next to the predicted label when plotting some examples.</p>
    <h2 id="_idParaDest-388" class="heading-2">Fast gradient sign method attack</h2>
    <p class="normal">One of the<a id="_idIndexMarker1465"/> most popular attack methods is<a id="_idIndexMarker1466"/> the <strong class="keyWord">Fast Gradient Sign Method</strong> (<strong class="keyWord">FSGM</strong> or <strong class="keyWord">FGM</strong>). As the name implies, it leverages a deep learning model’s gradient to find adversarial examples. It performs small perturbations on the pixels of the input image, either additions or subtractions. Which one to use depends on the gradient’s sign, which indicates what direction would increase or decrease the loss according to the pixel’s intensity.</p>
    <p class="normal">As with all ART attack methods, you first initialize it by providing the ART estimator or classifier. <code class="inlineCode">FastGradientMethod</code> also requires an attack step size <code class="inlineCode">eps</code>, which will condition the attack strength. Incidentally, <code class="inlineCode">eps</code> stands for epsilon (<img src="../Images/B18406_13_001.png" alt="" role="presentation"/>), which represents <a id="_idIndexMarker1467"/>error margins or infinitesimal approximation errors. A small step size will cause pixel intensity changes to be less visible, but it will also misclassify fewer examples. A larger step size will cause more examples to be misclassified with more visible changes:</p>
    <pre class="programlisting code"><code class="hljs-code">attack_fgsm = <span class="code-highlight"><strong class="hljs-slc">FastGradientMethod</strong></span>(base_classifier, eps=<span class="hljs-number">0.1</span>)
</code></pre>
    <p class="normal">After initializing, the next step is to <code class="inlineCode">generate</code> the adversarial examples. The only required attribute is original examples (<code class="inlineCode">X_test_mdsample</code>). Please note that FSGM can be targeted, so there’s an optional <code class="inlineCode">targeted</code> attribute in the initialization, but you would also need to provide corresponding labels in the generation. This attack is untargeted because the attacker’s intent is to sabotage the model:</p>
    <pre class="programlisting code"><code class="hljs-code">X_test_fgsm = attack_fgsm.<span class="code-highlight"><strong class="hljs-slc">generate</strong></span>(X_test_mdsample)
</code></pre>
    <p class="normal">Generating the adversarial examples with FSGM is quick, unlike other methods, hence the “Fast”!</p>
    <p class="normal">Now, we will do two things in one swoop. First, evaluate the adversarial examples (<code class="inlineCode">X_test_fgsm</code>) against our base classifier’s model (<code class="inlineCode">base_classifier.model</code>) with <code class="inlineCode">evaluate_multiclass_mdl</code>. Then we can employ <code class="inlineCode">compare_image_predictions</code> to plot a grid of images, contrasting the randomly selected adversarial examples (<code class="inlineCode">X_test_fgsm</code>) against the original ones (<code class="inlineCode">X_test_mdsample</code>) and their corresponding predicted labels (<code class="inlineCode">y_test_fgsm_pred</code>, <code class="inlineCode">y_test_mdsample</code>) and probabilities (<code class="inlineCode">y_test_fgsm_prob</code>, <code class="inlineCode">y_test_mdsample_prob</code>). We customize the titles and limit the grid to four examples (<code class="inlineCode">num_samples</code>). By default, <code class="inlineCode">compare_image_predictions</code> only compares misclassifications but an optional attribute, <code class="inlineCode">use_misclass</code>, can be set to <code class="inlineCode">False</code> to compare correct classifications:</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_fgsm_pred, y_test_fgsm_prob =\
    mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_multiclass_mdl</strong></span>(\
        base_classifier.model, X_test_fgsm, y_test_mdsample,\
        labels_l, ohe, plot_conf_matrix=<span class="hljs-literal">False</span>, plot_roc=<span class="hljs-literal">False</span>
    )
y_test_fgsm_prob = np.<span class="code-highlight"><strong class="hljs-built_in-slc">max</strong></span>(y_test_fgsm_prob, axis=<span class="hljs-number">1</span>)
mldatasets.<span class="code-highlight"><strong class="hljs-slc">compare_image_predictions</strong></span>(
    X_test_fgsm, X_test_mdsample, y_test_fgsm_pred,\
    y_test_mdsample.flatten(), y_test_fgsm_prob,\
    y_test_mdsample_prob, title_mod_prefix=<span class="hljs-string">"Attacked:"</span>,\
    title_difference_prefix=<span class="hljs-string">"FSGM Attack Average Perturbation:"</span>,\
    num_samples=<span class="hljs-number">4</span>
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1468"/>preceding code outputs a table first, which shows that the model has only 44% accuracy with FSGM-attacked examples! And even though it wasn’t a targeted attack, it was most effective toward correctly masked faces. So hypothetically, if perpetrators managed to cause this level of signal distortion or interference, they would severely undermine the security companies’ ability to monitor mask compliance.</p>
    <p class="normal">The code also outputs <em class="italic">Figure 13.5</em>, which shows some misclassifications caused by the FSGM attack. The attack pretty much evenly distributed noise throughout the images. It also shows that the image was only modified by a mean absolute error of 0.092, and since pixel values range between 0 and 1, this means 9.2%. If you were to calibrate attacks so that they are less detectable but still impactful, you must note that an <code class="inlineCode">eps</code> of 0.1 causes a 9.2% mean absolute perturbation, which reduces accuracy to 44%:</p>
    <figure class="mediaobject"><img src="../Images/B18406_13_05.png" alt="A collage of a person  Description automatically generated with low confidence"/></figure>
    <p class="packt_figref">Figure 13.5: Plot comparing FSGM-attacked versus the original images for the base classifier</p>
    <p class="normal">Speaking <a id="_idIndexMarker1469"/>of less detectable attacks, we will now learn about Carlini and Wagner attacks.</p>
    <h2 id="_idParaDest-389" class="heading-2">Carlini and Wagner infinity norm attack</h2>
    <p class="normal">In 2017, <strong class="keyWord">Carlini and Wagner</strong> (<strong class="keyWord">C&amp;W</strong>) employed<a id="_idIndexMarker1470"/> three <strong class="keyWord">norm-based distance metrics</strong>: <img src="../Images/B18406_13_002.png" alt="" role="presentation"/>, <img src="../Images/B18406_13_003.png" alt="" role="presentation"/>, and <img src="../Images/B18406_13_004.png" alt="" role="presentation"/>, measuring <a id="_idIndexMarker1471"/>the difference between the original and adversarial example. In<a id="_idIndexMarker1472"/> other papers, these metrics had already been discussed, including the FSGM one. The innovation introduced by C&amp;W was how these metrics were leveraged, using a gradient descent-based optimization algorithm designed to approximate a loss function minima. Specifically, to avoid getting stuck at a local minimum, they use multiple starting points in the gradient descent. And so that the process “yields a valid image,” it evaluates three methods to box-constrain the optimization problem. In this case, we want to find an adversarial example where the distances between that example and the original image are minimal, while also remaining realistic.</p>
    <p class="normal">All three C&amp;W attacks (<img src="../Images/B18406_13_002.png" alt="" role="presentation"/>, <img src="../Images/B18406_13_003.png" alt="" role="presentation"/>, and <img src="../Images/B18406_13_004.png" alt="" role="presentation"/>) use the Adam optimizer to quickly converge. Their<a id="_idIndexMarker1473"/> main difference is the distance metric, of which <img src="../Images/B18406_13_004.png" alt="" role="presentation"/> is arguably the best one. It’s defined as follows:</p>
    <p class="center"><img src="../Images/B18406_13_009.png" alt="" role="presentation"/></p>
    <p class="normal">And <a id="_idIndexMarker1474"/>because it’s the maximum distance to any coordinate, you make sure that the adversarial example is not just “on average” minimally different but also not too different anywhere in the feature space. That’s what would make an attack less detectable!</p>
    <p class="normal">Initializing C&amp;W infinity norm attacks and generating adversarial examples with them is similar to FSGM. To initialize <code class="inlineCode">CarliniLInfMethod</code>, we define optionally a <code class="inlineCode">batch_size</code> (the default is <code class="inlineCode">128</code>). Then, to <code class="inlineCode">generate</code> an untargeted adversarial attack, the same applies as with FSGM. Only <code class="inlineCode">X</code> is needed when untargeted, and <code class="inlineCode">y</code> when targeted:</p>
    <pre class="programlisting code"><code class="hljs-code">attack_cw = <span class="code-highlight"><strong class="hljs-slc">CarliniLInfMethod</strong></span>(
    base_classifier, batch_size=<span class="hljs-number">40</span>
)
X_test_cw = attack_cw.<span class="code-highlight"><strong class="hljs-slc">generate</strong></span>(X_test_mdsample)
</code></pre>
    <p class="normal">We will now evaluate the C&amp;W adversarial examples (<code class="inlineCode">X_test_cw</code>) just as we did with FSGM. It’s exactly the same code, only with <code class="inlineCode">fsgm</code> replaced with <code class="inlineCode">cw</code> and different titles in <code class="inlineCode">compare_image_predictions</code>. Just as with FSGM, the following code will yield a classification report and grid of images (<em class="italic">Figure 13.6</em>):</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_cw_pred, y_test_cw_prob =\
    mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_multiclass_mdl</strong></span>(
        base_classifier.model, X_test_cw, y_test_mdsample, labels_l,\
        ohe, plot_conf_matrix=<span class="hljs-literal">False</span>, plot_roc=<span class="hljs-literal">False</span>
    )
y_test_cw_prob = np.<span class="code-highlight"><strong class="hljs-built_in-slc">max</strong></span>(y_test_cw_prob, axis=<span class="hljs-number">1</span>)
mldatasets.<span class="code-highlight"><strong class="hljs-slc">compare_image_predictions</strong></span>(
    X_test_cw,\
    X_test_mdsample, y_test_cw_pred,\
    y_test_mdsample.flatten(), y_test_cw_prob,\
    y_test_mdsample_prob, title_mod_prefix=<span class="hljs-string">"Attacked:"</span>,\
    title_difference_prefix=<span class="hljs-string">"C&amp;W Inf Attack Average Perturbation"</span>,\
    num_samples=<span class="hljs-number">4</span>
)
</code></pre>
    <p class="normal">As <a id="_idIndexMarker1475"/>outputted by the preceding code, the C&amp;W adversarial examples have a 92% accuracy with our base model. The <a id="_idIndexMarker1476"/>drop is sufficient to render the model useless for its intended purpose. If the attacker disturbed a camera’s signal just enough, they could achieve the same results. And, as you can tell by <em class="italic">Figure 13.6</em>, the perturbation of 0.3% is tiny compared to FSGM, but it was sufficient to misclassify 8%, including the four in the grid that seem apparent to the naked eye: </p>
    <figure class="mediaobject"><img src="../Images/B18406_13_06.png" alt="A collage of a child  Description automatically generated with low confidence"/></figure>
    <p class="packt_figref">Figure 13.6: Plot comparing C&amp;W infinity norm-attacked versus the original images for the base classifier</p>
    <p class="normal">Sometimes<a id="_idIndexMarker1477"/> it doesn’t matter if an <a id="_idIndexMarker1478"/>attack goes undetected or not. The point of it is to make a statement, and that’s what adversarial patches can do.</p>
    <h2 id="_idParaDest-390" class="heading-2">Targeted adversarial patch attack</h2>
    <p class="normal"><strong class="keyWord">Adversarial Patches</strong> (<strong class="keyWord">APs</strong>) are<a id="_idIndexMarker1479"/> a robust, universal, and targeted method. You generate a patch you<a id="_idIndexMarker1480"/> can either superimpose on<a id="_idIndexMarker1481"/> an image, or print and physically place in a scene to trick a classifier into ignoring everything else in the scene. It is designed to work under a wide variety of conditions and transformations. Unlike other adversarial example generation approaches, there’s no intention of camouflaging the attack because, essentially, you replace a detectable portion of the scene with the patch. The <a id="_idIndexMarker1482"/>method works by leveraging a variant of <strong class="keyWord">Expectation Over Transformation</strong> (<strong class="keyWord">EOT</strong>), which trains images over transformations of a given patch on different locations of an image. What it learns is the patch that fools the classifier the most, given the training examples.</p>
    <p class="normal">This <a id="_idIndexMarker1483"/>method requires more parameters and steps than FSGM and C&amp;W. For starters, we will use <code class="inlineCode">AdversarialPatchNumpy</code>, which is the variant that works with any neural network image or video classifier. There’s also one for TensorFlow v2, but our base classifier is a <code class="inlineCode">KerasClassifier</code>. The first argument is the classifier (<code class="inlineCode">base_classifier</code>), and the other ones we will define are optional but highly recommended. The scaling ranges <code class="inlineCode">scale_min</code> and <code class="inlineCode">scale_max</code> are particularly important because they define how big can patches<a id="_idIndexMarker1484"/> be in relation to the images – in this case, we want to test no smaller than 40% and no larger than 70%. Besides that, it makes sense to define a target class (<code class="inlineCode">target</code>). In this case, we want the patch to target the “Correct” class. For the <code class="inlineCode">learning_rate</code> and max iterations (<code class="inlineCode">max_iter</code>), we use the defaults but note that these can be tuned to improve patch adversarial effectiveness:</p>
    <pre class="programlisting code"><code class="hljs-code">attack_ap = <span class="code-highlight"><strong class="hljs-slc">AdversarialPatchNumpy</strong></span>(
    base_classifier, scale_min=<span class="hljs-number">0.4</span>, scale_max=<span class="hljs-number">0.7</span>,\
    learning_rate=<span class="hljs-number">5.</span>, max_iter=<span class="hljs-number">500</span>,\
    batch_size=<span class="hljs-number">40</span>, target=<span class="hljs-number">0</span>
)
</code></pre>
    <p class="normal">We don’t want the patch generation algorithm to waste time testing patches everywhere in images, so we can direct this effort by using a Boolean mask. This mask tells it where it can center the patch. To make the mask, we start by creating an array of zeros, 128 × 128. Then we place ones in the rectangular area between pixels 80–93 and 45–84, which loosely corresponds to cover the center of the mouth area in most of the images. Lastly, we expand the array’s dimensions so that it’s <code class="inlineCode">(1, W, H)</code> and convert it to a Boolean. Then we can proceed to <code class="inlineCode">generate</code> patches using the small-size test dataset samples and the mask:</p>
    <pre class="programlisting code"><code class="hljs-code">placement_mask = np.<span class="code-highlight"><strong class="hljs-slc">zeros</strong></span>((<span class="hljs-number">128</span>,<span class="hljs-number">128</span>))
placement_mask[<span class="code-highlight"><strong class="hljs-number-slc">80</strong><strong class="hljs-slc">:</strong><strong class="hljs-number-slc">93</strong><strong class="hljs-slc">,</strong><strong class="hljs-number-slc">45</strong><strong class="hljs-slc">:</strong><strong class="hljs-number-slc">83</strong></span>] = <span class="hljs-number">1</span>
placement_mask = np.<span class="code-highlight"><strong class="hljs-slc">expand_dims</strong></span>(placement_mask, axis=<span class="hljs-number">0</span>).astype(<span class="hljs-built_in">bool</span>)
patch, patch_mask = attack_ap.<span class="code-highlight"><strong class="hljs-slc">generate</strong></span>(
    x=X_test_smsample,
    y=ohe.transform(y_test_smsample),
    mask=placement_mask
)
</code></pre>
    <p class="normal">We can now plot the patch with the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.<span class="code-highlight"><strong class="hljs-slc">imshow</strong></span>(patch * patch_mask)
</code></pre>
    <p class="normal">The preceding code produced the image in <em class="italic">Figure 13.7</em>. As expected, it has plenty of the shades of blue found in masks. It also has bright red and yellow hues, mostly missing from training examples, which confuse the classifier:</p>
    <figure class="mediaobject"><img src="../Images/B18406_13_07.png" alt="Chart  Description automatically generated with low confidence"/></figure>
    <p class="packt_figref">Figure 13.7: AP generated image to misclassify as correctly masked</p>
    <p class="normal">Unlike <a id="_idIndexMarker1485"/>other methods, <code class="inlineCode">generate</code> didn’t produce <a id="_idIndexMarker1486"/>adversarial examples but a single patch, which is an image we can then place on top of images to create adversarial examples. This task is performed with <code class="inlineCode">apply_patch</code>, which takes the original examples <code class="inlineCode">X_test_smsample</code> and a scale; we will use 55%. It is also recommended to use a <code class="inlineCode">mask</code> that will make sure the patch is applied where it makes more sense – in this case, in the area around the mouth: </p>
    <pre class="programlisting code"><code class="hljs-code">X_test_ap = attack_ap.<span class="code-highlight"><strong class="hljs-slc">apply_patch</strong></span>(
    X_test_smsample,
    scale=<span class="hljs-number">0.55</span>,
    mask=placement_mask
)
</code></pre>
    <p class="normal">Now it’s time to evaluate our attack and examine some misclassifications. We will do exactly as before and reuse the code that produced <em class="italic">Figure 13.5</em> and <em class="italic">Figure 13.7</em>, except we replace the variables so that they have <code class="inlineCode">ap</code> and a corresponding title:</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_ap_pred, y_test_ap_prob =\
    mldatasets.evaluate_multiclass_mdl(
        base_classifier.model, X_test_ap, y_test_smsample,
        labels_l, ohe, plot_conf_matrix=<span class="hljs-literal">False</span>, plot_roc=<span class="hljs-literal">False</span>
    )
y_test_ap_prob = np.<span class="hljs-built_in">max</span>(y_test_ap_prob, axis=<span class="hljs-number">1</span>)
mldatasets.compare_image_predictions(
    X_test_ap, X_test_smsample, y_test_ap_pred,\
    y_test_smsample.flatten(), y_test_ap_prob,
    y_test_smsample_prob, title_mod_prefix=<span class="hljs-string">"Attacked:"</span>,\
    title_difference_prefix=<span class="hljs-string">"AP Attack Average Perturbation:"</span>, num_samples=<span class="hljs-number">4</span>
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1487"/>preceding code yields the accuracy result of our attack at 65%, which is quite good considering how few examples it was <a id="_idIndexMarker1488"/>trained on. AP needs more data than other methods. Targeted attacks, in general, need more examples to understand how to best target one class. The preceding code also produced the grid of images in <em class="italic">Figure 13.8</em>, which demonstrates how, hypothetically, if people walked around holding a cardboard patch in front of their face, they could easily fool the model:</p>
    <figure class="mediaobject"><img src="../Images/B18406_13_08.png" alt="A picture containing person, posing, different, same  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 13.8: Plot comparing AP-attacked versus the original images for base classifier</p>
    <p class="normal">So far, we have studied three attack methods but haven’t yet tackled how to defend against these attacks. We will explore a couple of solutions next.</p>
    <h1 id="_idParaDest-391" class="heading-1">Defending against targeted attacks with preprocessing</h1>
    <p class="normal">There are five broad categories of adversarial defenses:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Preprocessing</strong>: changing<a id="_idIndexMarker1489"/> the model’s inputs so that they are harder to attack.</li>
      <li class="bulletList"><strong class="keyWord">Training</strong>: training a<a id="_idIndexMarker1490"/> new robust model that is designed to overcome attacks.</li>
      <li class="bulletList"><strong class="keyWord">Detection</strong>: detecting<a id="_idIndexMarker1491"/> attacks. For instance, you can train a model to detect adversarial examples.</li>
      <li class="bulletList"><strong class="keyWord">Transformer</strong>: modifying <a id="_idIndexMarker1492"/>model architecture and training so that it’s more robust – this may include techniques such as distillation, input filters, neuron pruning, and unlearning.</li>
      <li class="bulletList"><strong class="keyWord">Postprocessing</strong>: changing<a id="_idIndexMarker1493"/> model outputs to overcome production inference or model extraction attacks.</li>
    </ul>
    <p class="normal">Only the <a id="_idIndexMarker1494"/>first four defenses work <a id="_idIndexMarker1495"/>with evasion attacks, and in this chapter, we will only cover the<a id="_idIndexMarker1496"/> first two: <strong class="keyWord">preprocessing</strong> and <strong class="keyWord">adversarial training</strong>. FGSM and C&amp;W can be defended easily with either of these, but an AP is tougher to defend<a id="_idIndexMarker1497"/> against, so it might require a <a id="_idIndexMarker1498"/>stronger <strong class="keyWord">detection</strong> or <strong class="keyWord">transformer</strong> method.</p>
    <p class="normal">Before we defend, we must create a targeted attack. We will employ <strong class="keyWord">Projected Gradient Descent</strong> (<strong class="keyWord">PGD</strong>), which<a id="_idIndexMarker1499"/> is a strong attack very similar in output to FSGM – that is, it produces noisy images. We won’t explain PGD in detail here but what is important to note is, like FSGM, it is regarded as a <strong class="keyWord">first-order adversary</strong> because<a id="_idIndexMarker1500"/> it leverages first-order information about a network (due to gradient descent). Also, experiments prove that robustness against PGD ensures robustness against any first-order adversary. Specifically, PGD is a strong attack, so it makes for conclusive benchmarks.</p>
    <p class="normal">To create a targeted attack against the correctly masked class, it’s best that we only select examples that aren’t correctly masked (<code class="inlineCode">x_test_notmasked</code>) and their corresponding labels (<code class="inlineCode">y_test_notmasked</code>) and predicted probabilities (<code class="inlineCode">y_test_notmasked_prob</code>). Then, we want to create an array with the class (<code class="inlineCode">Correct</code>) that we want to generate adversarial examples for (<code class="inlineCode">y_test_masked</code>):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="code-highlight"><strong class="hljs-slc">not_masked_idxs</strong></span> = np.<span class="code-highlight"><strong class="hljs-slc">where</strong></span>(y_test_smsample != <span class="hljs-string">'Correct'</span>)[<span class="hljs-number">0</span>]
X_test_notmasked = X_test_smsample[<span class="code-highlight"><strong class="hljs-slc">not_masked_idxs</strong></span>]
y_test_notmasked = y_test_smsample[<span class="code-highlight"><strong class="hljs-slc">not_masked_idxs</strong></span>]
y_test_notmasked_prob = y_test_smsample_prob[<span class="code-highlight"><strong class="hljs-slc">not_masked_idxs</strong></span>]
y_test_masked = np.array(
    [<span class="hljs-string">'Correct'</span>] * X_test_notmasked.shape[<span class="hljs-number">0</span>]
).reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">We<a id="_idIndexMarker1501"/> initialize <code class="inlineCode">ProjectedGradientDescent</code> as we did with FSGM, except we will set the maximum perturbation (<code class="inlineCode">eps</code>), attack step size (<code class="inlineCode">eps_step</code>), maximum iterations (<code class="inlineCode">max_iter</code>), and <code class="inlineCode">targeted=True</code>. Precisely because it is targeted, we will set both <code class="inlineCode">X</code> and <code class="inlineCode">y</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">attack_pgd = <span class="code-highlight"><strong class="hljs-slc">ProjectedGradientDescent</strong></span>(
    base_classifier, eps=<span class="hljs-number">0.3</span>, eps_step=<span class="hljs-number">0.01</span>,\
    max_iter=<span class="hljs-number">40</span>, targeted=<span class="hljs-literal">True</span>
)
X_test_pgd = attack_pgd.<span class="code-highlight"><strong class="hljs-slc">generate</strong></span>(
    X_test_notmasked, y=ohe.transform(y_test_masked)
)
</code></pre>
    <p class="normal">Now let’s evaluate the PGD attack as we did before, but this time, let’s plot the confusion matrix (<code class="inlineCode">plot_conf_matrix=True</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_pgd_pred, y_test_pgd_prob =\
    mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_multiclass_mdl</strong></span>(
        base_classifier.model, X_test_pgd, y_test_notmasked,\
        labels_l, ohe, plot_conf_matrix=<span class="hljs-literal">True</span>, plot_roc=<span class="hljs-literal">False</span>
    )
y_test_pgd_prob = np.<span class="code-highlight"><strong class="hljs-built_in-slc">max</strong></span>(y_test_pgd_prob, axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">The preceding snippet produces the confusion matrix in <em class="italic">Figure 13.9</em>. The PGD attack was so effective that it produced an accuracy of 0%, making all unmasked and incorrectly masked examples appear to be masked:</p>
    <figure class="mediaobject"> <img src="../Images/B18406_13_09.png" alt="Chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 13.9: Confusion matrix for PGD attacked examples evaluated against the base classifier </p>
    <p class="normal">Next, let’s <a id="_idIndexMarker1502"/>run <code class="inlineCode">compare_image_prediction</code> to see some random misclassifications:</p>
    <pre class="programlisting code"><code class="hljs-code">mldatasets.<span class="code-highlight"><strong class="hljs-slc">compare_image_predictions</strong></span>(
    X_test_pgd, X_test_notmasked, y_test_pgd_pred,\
    y_test_notmasked.flatten(), y_test_pgd_prob,\
    y_test_smsample_prob, title_mod_prefix=<span class="hljs-string">"</span><span class="hljs-string">Attacked:"</span>,\
    num_samples=<span class="hljs-number">4</span>, title_difference_prefix=<span class="hljs-string">"PGD Attack Average Perturbation:"</span>
)
</code></pre>
    <p class="normal">The preceding code plots the grid of images in <em class="italic">Figure 13.10</em>. The mean absolute perturbation is the highest we’ve seen so far at 14.7%, and all unmasked faces in the grid are classified as correctly masked:</p>
    <figure class="mediaobject"><img src="../Images/B18406_13_10.png" alt="A collage of a person  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 13.10: Plot comparing PGD-attacked versus original images for the base classifier</p>
    <p class="normal">The<a id="_idIndexMarker1503"/> accuracy cannot get worse, and the images are grainy beyond repair. So how can we combat noise? If you recall, we have dealt with this problem before. In <em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Convolutional Neural Networks</em>, <strong class="keyWord">SmoothGrad</strong> improved <a id="_idIndexMarker1504"/>saliency maps by averaging the gradients. It’s a different application but the same principle – just as with a human, a noisy saliency map is more challenging to interpret than a smooth one, and a grainy image is much more challenging for a model to interpret than a smooth one.</p>
    <p class="normal"><strong class="keyWord">Spatial smoothing</strong> is <a id="_idIndexMarker1505"/>just a fancy way of saying blur! However, what’s novel about it being introduced as an adversarial defense method is that the proposed implementation (<code class="inlineCode">SpatialSmoothing</code>) calls for using the median and not the mean in a sliding window. The <code class="inlineCode">window_size</code> is configurable, and it is recommended to adjust it where it is most useful as a defense. Once the defense has been initialized, you plug in the adversarial examples (<code class="inlineCode">X_test_pgd</code>). It will output spatially smoothed adversarial examples (<code class="inlineCode">X_test_pgd_ss</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">defence_ss = <span class="code-highlight"><strong class="hljs-slc">SpatialSmoothing</strong></span>(window_size=<span class="hljs-number">11</span>)
X_test_pgd_ss, _ = <span class="code-highlight"><strong class="hljs-slc">defence_ss</strong></span>(X_test_pgd)
</code></pre>
    <p class="normal">Now we <a id="_idIndexMarker1506"/>can take the blurred adversarial examples produced and evaluate them as we did before – first, with <code class="inlineCode">evaluate_multiclass_mdl</code> to get predicted labels (<code class="inlineCode">y_test_pgd_ss_pred</code>) and probabilities (<code class="inlineCode">y_test_pgd_ss_prob</code>) and the output of some predictive performance metrics. With <code class="inlineCode">compare_image_predictions</code> to plot a grid of images, let’s use <code class="inlineCode">use_misclass=False</code> to compare properly classified images – in other words, the adversarial examples that were defended successfully:</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_pgd_ss_pred, y_test_pgd_ss_prob =\
    mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_multiclass_mdl</strong></span>(
        base_classifier.model, X_test_pgd_ss,\
        y_test_notmasked, labels_l, ohe,\
        plot_conf_matrix=<span class="hljs-literal">False</span>, plot_roc=<span class="hljs-literal">False</span>
)
y_test_pgd_ss_prob = np.<span class="code-highlight"><strong class="hljs-built_in-slc">max</strong></span>(y_test_pgd_ss_prob, axis=<span class="hljs-number">1</span>)
mldatasets.<span class="code-highlight"><strong class="hljs-slc">compare_image_predictions</strong></span>(
    X_test_pgd_ss, X_test_notmasked, y_test_pgd_ss_pred,\
    y_test_notmasked.flatten(), y_test_pgd_ss_prob,\
    y_test_notmasked_prob, use_misclass=<span class="hljs-literal">False</span>,\
    title_mod_prefix=<span class="hljs-string">"Attacked+Defended:"</span>, num_samples=<span class="hljs-number">4</span>,\
    title_difference_prefix=<span class="hljs-string">"PGD Attack &amp; Defended Average:"</span>
)
</code></pre>
    <p class="normal">The preceding code yields an accuracy of 54%, which is much better than 0% before the spatial smoothing defense. It also produces <em class="italic">Figure 13.11</em>, which demonstrates how blur effectively thwarted the PGD attack. It even halved the mean absolute perturbation! </p>
    <figure class="mediaobject"><img src="../Images/B18406_13_11.png" alt="A collage of people  Description automatically generated with low confidence"/></figure>
    <p class="packt_figref">Figure 13.11: Plot comparing spatially smoothed PGD-attacked images versus the original images for the base classifier</p>
    <p class="normal">Next, we will <a id="_idIndexMarker1507"/>try another defense method in our toolbox: adversarial training!</p>
    <h1 id="_idParaDest-392" class="heading-1">Shielding against any evasion attack by adversarial training of a robust classifier</h1>
    <p class="normal">In <em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Convolutional Neural Networks</em>, we identified a garbage image classifier <a id="_idIndexMarker1508"/>that would likely perform poorly in the intended environment of a municipal recycling plant. The <a id="_idIndexMarker1509"/>abysmal performance on out-of-sample data was due to the classifier being trained on a large variety of publicly available images that don’t match the expected conditions, or the characteristics of materials that are processed by a recycling plant. The chapter’s conclusion called for training a network with images that represent their intended environment to make for a more robust model.</p>
    <p class="normal">For model robustness, training data variety is critical, but only if it represents the intended environment. In statistical terms, it’s a question of using samples for training that accurately depict the population so that a model learns to classify them correctly. For adversarial robustness, the same principles apply. If you augment data to include plausible examples of adversarial attacks, the model will learn to classify them. That’s what adversarial training is in a nutshell.</p>
    <p class="normal">Machine learning<a id="_idIndexMarker1510"/> researchers in the adversarial robustness field suggest this form of defense is very effective<a id="_idIndexMarker1511"/> against any kind of evasion attack, essentially shielding it. That being said, it’s not impervious. Its effectiveness is contingent on using the right kind of adversarial examples in training, the optimal hyperparameters, and so forth. There are some guidelines outlined by researchers, such as increasing the number of neurons in the hidden layers and using PGD or BIM to <a id="_idIndexMarker1512"/>produce adversarial examples for the training. <strong class="keyWord">BIM</strong> stands for <strong class="keyWord">Basic Iterative Method</strong>. It’s like FSGM but not fast because it iterates to approximate the best adversarial example within a <img src="../Images/B18406_13_010.png" alt="" role="presentation"/>-neighborhood for the original image. The <code class="inlineCode">eps</code> attribute bounds this neighborhood.</p>
    <p class="normal">Training a robust model can be very resource-intensive. It is not required because we can download one already trained for us, but it’s important to understand how to perform this with ART. We will explain these steps to give the option of completing the model training with ART. Otherwise, just skip the steps and download the trained model. The <code class="inlineCode">robust_model</code> is very much like the <code class="inlineCode">base_model</code> except we use equal-sized filters in the four convolutional (<code class="inlineCode">Conv2D</code>) layers. We do this to decrease complexity to counter the complexity we add by quadrupling the neurons in the first hidden (<code class="inlineCode">Dense</code>) layer, as suggested by the machine learning researchers:</p>
    <pre class="programlisting code"><code class="hljs-code">robust_model = tf.keras.models.<span class="code-highlight"><strong class="hljs-slc">Sequential</strong></span>([
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">InputLayer</strong></span>(input_shape=X_train.shape[<span class="hljs-number">1</span>:]),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">Conv2D</strong></span>(<span class="hljs-number">32</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">MaxPooling2D</strong></span>(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">Conv2D</strong></span>(<span class="hljs-number">32</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">MaxPooling2D</strong></span>(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">Conv2D</strong></span>(<span class="hljs-number">32</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">MaxPooling2D</strong></span>(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">Conv2D</strong></span>(<span class="hljs-number">32</span>, kernel_size=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">MaxPooling2D</strong></span>(pool_size=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">Flatten</strong></span>(),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">Dense</strong></span>(<span class="hljs-number">3072</span>, activation=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">Dropout</strong></span>(<span class="hljs-number">0.2</span>),
    tf.keras.layers.<span class="code-highlight"><strong class="hljs-slc">Dense</strong></span>(<span class="hljs-number">3</span>, activation=<span class="hljs-string">'softmax'</span>)
], name=<span class="hljs-string">'CNN_Robust_MaskedFaceNet_Model'</span>)
robust_model.<span class="code-highlight"><strong class="hljs-built_in-slc">compile</strong></span>(
    optimizer=tf.keras.optimizers.Adam(lr=<span class="hljs-number">0.001</span>),
    loss=<span class="hljs-string">'categorical_crossentropy'</span>,
    metrics=[<span class="hljs-string">'accuracy'</span>])
robust_model.<span class="code-highlight"><strong class="hljs-slc">summary</strong></span>()
</code></pre>
    <p class="normal">The <code class="inlineCode">summary()</code> in <a id="_idIndexMarker1513"/>the <a id="_idIndexMarker1514"/>preceding code outputs the following. You can see that trainable parameters total around 3.6 million – similar to the base model:</p>
    <pre class="programlisting con"><code class="hljs-con">Model: "CNN_Robust_MaskedFaceNet_Model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 126, 126, 32)      896       
_________________________________________________________________
maxpool2d_1 (MaxPooling2D)   (None, 63, 63, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 61, 61, 32)        9248      
_________________________________________________________________
maxpool2d_2 (MaxPooling2D)   (None, 30, 30, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 28, 28, 32)        9248      
_________________________________________________________________
maxpool2d_3 (MaxPooling2D)   (None, 14, 14, 32)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 12, 12, 32)        9248      
_________________________________________________________________
maxpool2d_4 (MaxPooling2D)   (None, 6, 6, 32)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1152)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 3072)              3542016   
_________________________________________________________________
dropout (Dropout)            (None, 3072)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 9219      
=================================================================
Total params: 3,579,875
Trainable params: 3,579,875
Non-trainable params: 0
_________________________________________________________________
</code></pre>
    <p class="normal">Next, we<a id="_idIndexMarker1515"/> can adversarially train the<a id="_idIndexMarker1516"/> model by first initializing a new <code class="inlineCode">KerasClassifier</code> with the <code class="inlineCode">robust_model</code>. Then, we initialize a <code class="inlineCode">BasicIterativeMethod</code> attack on this classifier. Lastly, we initialize <code class="inlineCode">AdversarialTrainer</code> with the <code class="inlineCode">robust_classifier</code> and the BIM attack and <code class="inlineCode">fit</code> it. Please note that we saved the BIM attack into a variable called <code class="inlineCode">attacks</code> because this could be a list of ART attacks instead of a single one. Also, note that <code class="inlineCode">AdversarialTrainer</code> has an attribute called <code class="inlineCode">ratio</code>. This attribute determines what percentage of the training examples are adversarial examples. This percentage dramatically impacts the effectiveness of adversarial attacks. If it’s too low, it might not perform well with adversarial examples, and if it’s too high, it might perform less effectively with non-adversarial examples. If we run the <code class="inlineCode">trainer</code>, it will likely take many hours to complete, so don’t get alarmed:</p>
    <pre class="programlisting code"><code class="hljs-code">robust_classifier = <span class="code-highlight"><strong class="hljs-slc">KerasClassifier</strong></span>(
    model=robust_model, clip_values=(min_, max_)
)
attacks = <span class="code-highlight"><strong class="hljs-slc">BasicIterativeMethod</strong></span>(
    robust_classifier, eps=<span class="hljs-number">0.3</span>, eps_step=<span class="hljs-number">0.01</span>, max_iter=<span class="hljs-number">20</span>
)
trainer = <span class="code-highlight"><strong class="hljs-slc">AdversarialTrainer</strong></span>(
    robust_classifier, attacks, ratio=<span class="hljs-number">0.5</span>
)
trainer.fit(
    X_train, ohe.transform(y_train), nb_epochs=<span class="hljs-number">30</span>, batch_size=<span class="hljs-number">128</span>
)
</code></pre>
    <p class="normal">If you didn’t train the <code class="inlineCode">robust_classifier</code>, you can download a pretrained <code class="inlineCode">robust_model</code> and initialize the <code class="inlineCode">robust_classifier</code> with it like this:</p>
    <pre class="programlisting code"><code class="hljs-code">model_path = <span class="code-highlight"><strong class="hljs-slc">get_file</strong></span>(
    <span class="hljs-string">'CNN_Robust_MaskedFace_Net.hdf5'</span>,
    <span class="hljs-string">'https://github.com/PacktPublishing/Interpretable-Machine- \</span>
<span class="hljs-string">    Learning-with-Python/blob/master/models/ \</span>
<span class="hljs-string">    CNN_Robust_MaskedFace_Net.hdf5?raw=true'</span>
)
robust_model = tf.keras.models.<span class="code-highlight"><strong class="hljs-slc">load_model</strong></span>(model_path)
robust_classifier = <span class="code-highlight"><strong class="hljs-slc">KerasClassifier</strong></span>(
    model=robust_model, clip_values=(min_, max_)
)
</code></pre>
    <p class="normal">Now, let’s evaluate the <code class="inlineCode">robust_classifier</code> against the original test dataset using <code class="inlineCode">evaluate_multiclass_mdl</code>. We set <code class="inlineCode">plot_conf_matrix=True</code> to see the confusion matrix:</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_robust_pred, y_test_robust_prob =\
mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_multiclass_mdl</strong></span>(
    robust_classifier.model, X_test, y_test, labels_l, ohe,\
    plot_conf_matrix=<span class="hljs-literal">True</span>, predopts={<span class="hljs-string">"verbose"</span>:<span class="hljs-number">1</span>}
)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1517"/>preceding <a id="_idIndexMarker1518"/>code outputs the confusion matrix and performance metrics in <em class="italic">Figure 13.12</em>. It’s 1.8% less accurate than the base classifier. Most of the misclassifications are with correctly masked faces getting classified as incorrectly masked. There’s certainly a trade-off when choosing a 50% adversarial example ratio, or perhaps we can tune the hyperparameters or the model architecture to improve this:</p>
    <figure class="mediaobject"><img src="../Images/Image15514.png" alt="Chart, treemap chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 13.12: Robust classifier confusion metrics and performance metrics</p>
    <p class="normal">Let’s see <a id="_idIndexMarker1519"/>how the robust <a id="_idIndexMarker1520"/>model fares against adversarial attacks. Let’s use <code class="inlineCode">FastGradientMethod</code> again, but this time, replace <code class="inlineCode">base_classifier</code> with <code class="inlineCode">robust_classifier</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">attack_fgsm_robust = <span class="code-highlight"><strong class="hljs-slc">FastGradientMethod</strong></span>(
    robust_classifier, eps=<span class="hljs-number">0.1</span>
)
X_test_fgsm_robust = attack_fgsm_robust.<span class="code-highlight"><strong class="hljs-slc">generate</strong></span>(X_test_mdsample)
</code></pre>
    <p class="normal">Next, we <a id="_idIndexMarker1521"/>can employ <code class="inlineCode">evaluate_multiclass_mdl</code> and <code class="inlineCode">compare_image_predictions</code> to measure and observe the effectiveness of our attack, but this time against the <code class="inlineCode">robust_classifier</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_fgsm_robust_pred, y_test_fgsm_robust_prob =\
    mldatasets.<span class="code-highlight"><strong class="hljs-slc">evaluate_multiclass_mdl</strong></span>(
        robust_classifier.model, X_test_fgsm_robust,\
        y_test_mdsample, labels_l, ohe,\
        plot_conf_matrix=<span class="hljs-literal">False</span>, plot_roc=<span class="hljs-literal">False</span>
    )
y_test_fgsm_robust_prob = np.<span class="code-highlight"><strong class="hljs-built_in-slc">max</strong></span>(
    y_test_fgsm_robust_prob, axis=<span class="hljs-number">1</span>
)
mldatasets.<span class="code-highlight"><strong class="hljs-slc">compare_image_predictions</strong></span>(
    X_test_fgsm_robust, X_test_mdsample,
    y_test_fgsm_robust_pred, num_samples=<span class="hljs-number">4</span>,\
    y_test_mdsample.flatten(), y_test_fgsm_robust_prob,\
    y_test_mdsample_prob, title_mod_prefix=<span class="hljs-string">"Attacked:"</span>,\
    title_difference_prefix=<span class="hljs-string">"FSGM Attack Average Perturbation:"</span>
)
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker1522"/>snippet outputs some performance metrics, which evidenced a 95.5% accuracy. If you compare how an equally strengthened FSGM attack fared against the <code class="inlineCode">base_classifier</code>, it yielded a 44% accuracy. That was quite an improvement! The preceding code also produces the image grid in <em class="italic">Figure 13.13</em>. You can tell how the FSGM attack against the robust model makes less grainy and more patchy images. On average, they are less perturbed than they were against the base model because so few of them were successful, but those that were significantly degraded. It appears as if the FSGM reduced their color depth from millions of possible colors (24+ bits) to 256 (8-bit) or 16 (4-bit) colors. Of course, an evasion attack can’t actually do that, but what happened was that the FSGM algorithm converged at the same shades of blue, brown, red, and orange that could fool the classifier! Other shades remain unaltered: </p>
    <figure class="mediaobject"><img src="../Images/B18406_13_13.png" alt="A collage of a baby  Description automatically generated with low confidence"/></figure>
    <p class="packt_figref">Figure 13.13: Plot comparing FSGM-attacked versus the original images for the robust classifier</p>
    <p class="normal">So far, we have <a id="_idIndexMarker1523"/>evaluated<a id="_idIndexMarker1524"/> the robustness of models but only against one attack strength, without factoring in possible defenses, thus evaluating its robustness. In the next section, we will study a method that does this.</p>
    <h1 id="_idParaDest-393" class="heading-1">Evaluating adversarial robustness</h1>
    <p class="normal">It’s necessary <a id="_idIndexMarker1525"/>to test your systems in any engineering endeavor to see how vulnerable they are to attacks or accidental failures. However, security is a domain where you must stress-test your solutions to ascertain what level of attacks are needed to make your system break down beyond an acceptable threshold. Furthermore, figuring out what level of defense is needed to curtail an attack is useful information too.</p>
    <h2 id="_idParaDest-394" class="heading-2">Comparing model robustness with attack strength</h2>
    <p class="normal">We <a id="_idIndexMarker1526"/>now have two classifiers we can compare against an equally strengthened attack, and we try different attack strengths to see how they fare across all of them. We will use FSGM because it’s fast, but you could use any method!</p>
    <p class="normal">The first attack strength we can assess is no attack strength. In other words, what is the classification accuracy against the test dataset with no attack? We already had stored the predicted labels for both the base (<code class="inlineCode">y_test_pred</code>) and robust (<code class="inlineCode">y_test_robust_pred</code>) models, so this is easy to obtain with scikit-learn’s <code class="inlineCode">accuracy_score</code> metric:</p>
    <pre class="programlisting code"><code class="hljs-code">accuracy_base_0 = metrics.accuracy_score(
    y_test, y_test_pred
)
accuracy_robust_0 = metrics.accuracy_score(
    y_test, y_test_robust_pred
)
</code></pre>
    <p class="normal">We can now iterate across a range of attack strengths (<code class="inlineCode">eps_range</code>) between 0.01 and 0.9. Using <code class="inlineCode">linspace</code>, we can generate 9 values between 0.01 and 0.09 and 9 values between 0.1 and 0.9, and <code class="inlineCode">concatenate</code> them into a single array. We will test attacks for these 18 <code class="inlineCode">eps</code> values by <code class="inlineCode">for</code>-looping through all of them, attacking each model, and retrieving the post-attack accuracies with <code class="inlineCode">evaluate</code>. The respective accuracies are appended to two lists (<code class="inlineCode">accuracy_base</code>, <code class="inlineCode">accuracy_robust</code>). And after the <code class="inlineCode">for</code> loop, we prepend 0 to the <code class="inlineCode">eps_range</code> to account for the accuracies prior to any attacks:</p>
    <pre class="programlisting code"><code class="hljs-code">eps_range = np.<span class="code-highlight"><strong class="hljs-slc">concatenate</strong></span>(
    (np.linspace(<span class="hljs-number">0.01</span>, <span class="hljs-number">0.09</span>, <span class="hljs-number">9</span>), np.linspace(<span class="hljs-number">0.1</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">9</span>)), axis=<span class="hljs-number">0</span>
).tolist()
accuracy_base = [accuracy_base_0]
accuracy_robust = [accuracy_robust_0]
<span class="hljs-keyword">for</span> <span class="code-highlight"><strong class="hljs-slc">eps</strong></span> <span class="hljs-keyword">in</span> tqdm(eps_range, desc=<span class="hljs-string">'EPS'</span>):
    attack_fgsm.set_params(**{<span class="hljs-string">'eps'</span>: <span class="code-highlight"><strong class="hljs-slc">eps</strong></span>})
    X_test_fgsm_base_i =attack_fgsm.<span class="code-highlight"><strong class="hljs-slc">generate</strong></span>(X_test_mdsample)
    _, accuracy_base_i =\
    base_classifier.model.<span class="code-highlight"><strong class="hljs-slc">evaluate</strong></span>(
        X_test_fgsm_base_i, ohe.transform(y_test_mdsample)
    )
    attack_fgsm_robust.set_params(**{<span class="hljs-string">'eps'</span>: <span class="code-highlight"><strong class="hljs-slc">eps</strong></span>})
    X_test_fgsm_robust_i=attack_fgsm_robust.<span class="code-highlight"><strong class="hljs-slc">generate</strong></span>(
        X_test_mdsample
    )
    _, accuracy_robust_i =\
        robust_classifier.model.<span class="code-highlight"><strong class="hljs-slc">evaluate</strong></span>(
            X_test_fgsm_robust_i, ohe.transform(y_test_mdsample)
            )
    accuracy_base.append(accuracy_base_i)
    accuracy_robust.append(accuracy_robust_i) 
eps_range = [<span class="hljs-number">0</span>] + eps_range 
</code></pre>
    <p class="normal">Now, we can <a id="_idIndexMarker1527"/>plot the accuracies for both classifiers across all attack strengths with the following code: </p>
    <pre class="programlisting code"><code class="hljs-code">fig, ax = plt.subplots(figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">7</span>))
ax.plot(
    np.array(eps_range), np.array(accuracy_base),\
    <span class="hljs-string">'b–'</span>, label=<span class="hljs-string">'Base classifier'</span>
)
ax.plot(
    np.array(eps_range), np.array(accuracy_robust),\
    <span class="hljs-string">'r–'</span>, label=<span class="hljs-string">'Robust classifier'</span>
)
legend = ax.legend(loc=<span class="hljs-string">'upper center'</span>)
plt.xlabel(<span class="hljs-string">'Attack strength (eps)'</span>)
plt.ylabel(<span class="hljs-string">'Accuracy'</span>)
</code></pre>
    <p class="normal">The preceding code generates <em class="italic">Figure 13.14</em>, which demonstrates that the robust model performs better between attack strengths of 0.02 and 0.3 but then does consistently about 10% worse:</p>
    <figure class="mediaobject"><img src="../Images/Image15613.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 13.14: Accuracy measured for the robust and base classifiers at different FSGM attack strengths</p>
    <p class="normal">One thing<a id="_idIndexMarker1528"/> that <em class="italic">Figure 13.14</em> fails to account for is defenses. For example, if hospital cameras were constantly jammed or tampered with, the security company would be remiss not to defend their models. The easiest way to do so for this kind of attack is with some sort of smoothing. </p>
    <p class="normal">Adversarial training also produces an empirically robust classifier that you cannot guarantee will work under certain pre-defined circumstances, which is why there’s a need for certifiable defenses. </p>
    <h1 id="_idParaDest-395" class="heading-1">Mission accomplished</h1>
    <p class="normal">The mission was to perform some adversarial robustness tests on their face mask model to determine if hospital visitors and staff can evade mandatory mask compliance. The base model performed very poorly on many evasion attacks, from the most aggressive to the most subtle.</p>
    <p class="normal">You also looked at possible defenses to these attacks, such as spatial smoothing and adversarial retraining. And then, you explored ways to evaluate the robustness of your proposed defenses. You can now provide an end-to-end framework to defend against this kind of attack. That being said, what you did was only a proof of concept.</p>
    <p class="normal">Now, you can propose training a certifiably robust model against the attacks the hospitals expect to encounter the most. But first, you need the ingredients for a generally robust model. To this end, you will need to take all 210,000 images in the original dataset, make many variations on mask colors and types with them, and augment them even further with reasonable brightness, shear, and rotation transformations. Lastly, the robust model needs to be trained with several kinds of attacks, including several kinds of APs. These are important because they mimic the most common compliance evasion behavior of concealing faces with body parts or clothing items.</p>
    <h1 id="_idParaDest-396" class="heading-1">Summary</h1>
    <p class="normal">After reading this chapter, you should understand how attacks can be perpetrated on machine learning models and evasion attacks in particular. You should know how to perform FSGM, BIM, PGD, C&amp;W, and AP attacks, as well as how to defend against them with spatial smoothing and adversarial training. Last but not least, you know how to evaluate adversarial robustness.</p>
    <p class="normal">The next chapter is the last one, and it outlines some ideas on what’s next for machine learning interpretation.</p>
    <h1 id="_idParaDest-397" class="heading-1">Dataset sources</h1>
    <ul>
      <li class="bulletList">Adnane Cabani, Karim Hammoudi, Halim Benhabiles, and Mahmoud Melkemi, 2020, <em class="italic">MaskedFace-Net - A dataset of correctly/incorrectly masked face images in the context of COVID-19</em>, Smart Health, ISSN 2352–6483, Elsevier: <a href="https://doi.org/10.1016/j.smhl.2020.100144"><span class="url">https://doi.org/10.1016/j.smhl.2020.100144</span></a> (Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation)</li>
      <li class="bulletList">Karras, T., Laine, S., and Aila, T., 2019, <em class="italic">A Style-Based Generator Architecture for Generative Adversarial Networks</em>. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 4396–4405: <a href="https://arxiv.org/abs/1812.04948"><span class="url">https://arxiv.org/abs/1812.04948</span></a> (Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation)</li>
    </ul>
    <h1 id="_idParaDest-398" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Polyakov, A., 2019, Aug 6, <em class="italic">How to attack Machine Learning (Evasion, Poisoning, Inference, Trojans, Backdoors)</em> [blog post]: <a href="https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c"><span class="url">https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c</span></a></li>
      <li class="bulletList">Carlini, N., &amp; Wagner, D., 2017, <em class="italic">Towards Evaluating the Robustness of Neural Networks</em>. 2017 IEEE Symposium on Security and Privacy (SP), 39–57: <a href="https://arxiv.org/abs/1608.04644"><span class="url">https://arxiv.org/abs/1608.04644</span></a></li>
      <li class="bulletList">Brown, T., Mané, D., Roy, A., Abadi, M., and Gilmer, J., 2017, <em class="italic">Adversarial Patch</em>. ArXiv: <a href="https://arxiv.org/abs/1712.09665"><span class="url">https://arxiv.org/abs/1712.09665</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_13.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>