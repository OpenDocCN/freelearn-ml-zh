- en: Using Computational Photography with OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 0The goal of this chapter is to build on what we have covered in the previous
    chapters about photography and image processing and investigate some algorithms
    that OpenCV gives you access in a lot more detail. We'll focus on working with
    digital photography and building tools that will allow you to harness the power
    of OpenCV, and even think about using it as your go-to tool for editing your photos.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the 8-bit problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **gamma correction**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding **high-dynamic-range imaging** (**HDRI**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding panorama stitching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving panorama stitching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning the basics of digital photography and the concepts of high dynamic
    imaging will not only allow you to understand computational photography better,
    but it will make you a better photographer. Since we will explore these topics
    in detail, you will also understand how much work it takes to write a new algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Through this chapter, you will learn how to work with unprocessed (RAW) images
    directly from digital cameras, how to use OpenCV's computational photography tools,
    and how to use low-level OpenCV APIs to build a panorama stitching algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We have quite a few topics to cover, so let's roll up our sleeves and get started.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code that we present in this chapter at our GitHub repository
    at [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter5](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter5).
  prefs: []
  type: TYPE_NORMAL
- en: We will also use the `rawpy` and `exifread` Python packages for reading RAW
    images and reading image metadata. For the full list of requirements, you can
    refer to the `requirements.txt` file in the book's Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have multiple concepts to familiarize ourselves with. With a view to building
    your toolbox for image processing, we are going to develop the algorithms that
    we are going to familiarize ourselves with into Python scripts that use OpenCV
    to accomplish real-life problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use OpenCV to implement the following scripts so that you will be able
    to use them whenever you need to do photo processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gamma_correct.py`: This is a script that applies gamma correction to the input
    image and shows the resulting image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hdr.py`: This is a script that takes images as input and produces a **high
    dynamic range** (**HDR**) image as an output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`panorama.py`: This is a script that takes multiple images as input and produces
    a single stitched image that is larger than the individual images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll first start with a discussion of how digital photography works and the
    reason we can't take perfect pictures without needing to do post-processing. Let's
    start with the 8-bit problem for images.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the 8-bit problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typical **Joint Photographic Experts Group** (**JPEG**) images that we are used
    to seeing, work by encoding each pixel into 24 bits—one 8-bit number per **RGB**
    (**red**, **green**, **blue**) color component, which gives us an integer within
    the 0-255 range. This is just a number, 255, *but is it enough information or
    not?* To understand this, let's try to understand how these numbers are recorded
    and what these numbers mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most current digital cameras use a **Bayer filter, **or equivalent, that works
    using the same principles. A Bayer filter is an array of sensors of different
    colors placed on a grid similar to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/faac3543-7aab-4455-bcd0-a6cde3768ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image source—https://en.wikipedia.org/wiki/Bayer_filter#/media/File:Bayer_pattern_on_sensor.svg
    (CC SA 3.0)
  prefs: []
  type: TYPE_NORMAL
- en: In the previous diagram, each of these sensors measures the intensity of the
    light that gets into it, and a group of four sensors represents a single pixel.
    The data from these four sensors are combined to provide us with the three values
    for R, G, and B.
  prefs: []
  type: TYPE_NORMAL
- en: Different cameras might have a slightly different layout of red, green, and
    blue pixels, but at the end of the day, they are using small sensors that discretize
    the amount of radiation they get into a single value within the 0-255 range, where
    0 means no radiation at all and 255 means the brightest radiation that the sensor
    can record.
  prefs: []
  type: TYPE_NORMAL
- en: The range of brightness that is detectable is called the **dynamic range** or
    the **luminance range**. The ratio between the smallest amount of radiation that
    could be registered (that is, 1) and the highest (that is, 255) is called the **c****ontrast
    ratio.**
  prefs: []
  type: TYPE_NORMAL
- en: As we said, JPEG files have a contrast ratio of *255:1*. Most current LCD monitors
    have already surpassed that and have a contrast ratio of up to *1,000:1*. I bet
    you are waiting for your eye's ratio. I'm not sure about you, but most humans
    can see up to *15,000:1*.
  prefs: []
  type: TYPE_NORMAL
- en: So, we can see quite a lot more than even our best monitor can show, and a lot
    more than a simple JPEG file stores. Don't despair too much, because the latest
    digital cameras have been catching up and can now capture intensity ratios of
    up to *28,000:1* (the really expensive ones).
  prefs: []
  type: TYPE_NORMAL
- en: 'The small dynamic range is the reason that, when you are shooting a picture
    and you have the sun in the background, you either see the sun and the surroundings
    are all white without any detail, or everything in the foreground is extremely
    dark. Here is an example screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8e38611-5931-491a-9e4d-01998b5650b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image source—https://github.com/mamikonyana/winter-hills (CC SA 4.0)
  prefs: []
  type: TYPE_NORMAL
- en: So, the problem is that we either display things that are too bright, or we
    display things that are too dark. Before we move forward, let's take a look at
    how to read files that have more than 8 bits and import the data into OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about RAW images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since this chapter is about computational photography, some of you reading it
    are probably photography enthusiasts and love taking pictures using the RAW formats
    that your camera supports—be it **Nikon Electronic Format** (**NEF**) or **Canon
    Raw Version 2** (**CR2**).
  prefs: []
  type: TYPE_NORMAL
- en: Raw files usually capture a lot more information (usually more bits per pixel)
    than JPEG files, and if you are going to do a lot of post-processing, these files
    are a lot more convenient to work with, since they will produce higher-quality
    final images.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let''s take a look at how to open a CR2 file using Python and load it into
    OpenCV. For that, we will use a Python library called `rawpy`. For convenience,
    we will write a function called `load_image` that can handle both RAW images and
    regular JPEG files so we can abstract this part away and concentrate on more fun
    things in the rest of the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we take care of the imports (as promised, just one small extra library):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the function, adding an optional `bps` argument, which will let us
    control how much precision we want the images to have, that is, we want to check
    if we want the full 16 bits or are just 8 bits good enough:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, if the file has a `.CR2` extension, we open the file with `rawpy` and
    extract the image without trying to do any post-processing, since we want to do
    that with OpenCV:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As Canon (Canon Inc.—an optical products company) and OpenCV use a different
    ordering of colors, we switch from RGB to **BGR **(**blue**, **green**, and **red**),
    which is the default ordering in OpenCV and we `return` the resulting image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For anything that is not `.CR2`, we use OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now we know how to get all our images into OpenCV, it's time to get started
    with one of the brightest algorithms we have.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since my camera has a 14-bit dynamic range, we are going to use images captured
    with my camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Once we know how to load our pictures, let's try to see how we can best display
    them on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Using gamma correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Why is everybody still using JPEG files if they can only distinguish between
    255 different levels? **Does it mean it can only capture a dynamic range of 1:255?*
    It turns out there are clever tricks that people use.'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned before, the camera sensors capture values that are linear, that
    is, 4 means that it has 4 times more light than 1, and 80 has 8 times more light
    than 10\. But does the JPEG file format have to use a linear scale? It turns out
    that it doesn't. So, if we are willing to sacrifice the difference between two
    values, for example, 100 and 101, we can fit another value there.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this better, let''s look at the histogram of gray pixel values
    of a RAW image. Here is the code to generate that—just load the image, convert
    it to grayscale, and show the histogram using `pyplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result of the histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e368499-f598-40c1-bd9d-d1731a0930b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have two pictures: the left one is a *normal* picture, where you can see
    some clouds, but it''s almost impossible to see anything in the foreground, and
    the right one has tried to capture some detail in the trees, and because of that
    has burned all the clouds. *Is there a way to combine these?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take a closer look at the histograms, we see that the burned-out part
    is visible on the right-hand histogram because there are values that are 16,000
    that get encoded as 255, that is, white pixels. But on the left-hand picture,
    there are no white pixels. The way we encode 14-bit values into 8-bit values is
    very rudimentary: we just divide the values by *64 (=2⁶)*, so we lose the distinction
    between 2,500 and 2,501 and 2,502; instead, we only have 39 (out of 255) because
    the values in the 8-bit format have to be integers.'
  prefs: []
  type: TYPE_NORMAL
- en: This is where gamma corrections come in. Instead of simply showing the recorded
    value as the intensity, we are going to make some corrections, to make the image
    more visually appealing.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to use a non-linear function to try to emphasize the parts that
    we think are more important:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec71864e-36ff-4fea-8bcd-bf7fb0880fbb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s try to visualize this formula for two different values—**γ = 0.3** and **γ
    = 3**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8fd3ea0-b583-418a-91f7-f56ca2b0f7ce.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, small gammas put an emphasis on lower values; the pixel values
    from **0**-**50** are mapped to pixel values from **0**-**150** (more than half
    of the available values). The reverse is true of the higher gammas—the values
    from **200**-**250** are mapped to the values **100**-**250** (more than half
    of the available values). So, if you want to make your photo brighter, you should
    pick a gamma value of **γ < 1**, which is often called **gamma compression**.
    And if you want to make your photos dimmer to show more detail, you should pick
    a gamma value of **γ > 1**, which is called **gamma expansion.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using integers for *I*, we can start with a float number and get
    to O, then convert that number to an integer to lose even less of information.
    Let''s write some Python code to implement gamma correction:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s write a function to apply our formula. Because we are using 14-bit
    numbers, we will have to change it to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/beccc155-e722-44a5-a24e-b34c4b5fa6c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the relevant code will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have used the `@functools.lru_cache` decorator to make sure we don't
    compute anything twice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we just iterate over all the pixels and apply our transformation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s take a look at how to use this to show the new image alongside the
    regularly transformed 8-bit image. We will write a script for this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s configure a `parser` to load an image and allow setting the `gamma`
    value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the `gray` image as a `14bit` image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Use linear transformation to get output values as an integer in the range [`0`-`255`]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Use our `apply_gamma` function we wrote previously to get a gamma-corrected
    image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, plot both of the images together with their histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, `show` the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now plotted the histogram and will look at the magic that is elaborated
    in the following two images with their histograms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/180de854-3e0e-4164-b4b3-963cf8c863b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Look at the picture at the top right—you can see almost everything! And we are
    only getting started.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out gamma compensation works great on black and white images, but it
    can't do everything! It can either correct brightness and we lose most of the
    color information, or it can correct color information and we lose the brightness
    information. So, we have to find a new best friend—that is, HDRI.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding high-dynamic-range imaging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**High-dynamic-range** imaging (**HDR**) is a technique to produce images that
    have a greater dynamic range of luminosity (that is, contrast ratio) than could
    be displayed through the display medium, or captured with the camera using a single
    shot. There are two main ways to create such images—using special image sensors,
    such as an oversampled binary image sensor, or the way we will focus on here,
    by combining multiple **Standard Dynamic Range** (**SDR**) images to produce a
    combined HDR image.'
  prefs: []
  type: TYPE_NORMAL
- en: HDR imaging works with images that use more than 8 bits per channel (usually
    32-bit float values), allowing a much wider dynamic range. As we know, the *dynamic
    range* of a scene is the contrast ratio between its brightest and darkest parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a closer look at what the luminance values are of certain things
    that we can see. The following diagram shows values that we can easily see, from
    the dark sky (around *10^(-4) cd/m²*) to the sun during sunset (*10⁵ cd/m²*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2981dc9-a73e-499b-bab2-4393ddf84700.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see more than these values. Because some people can adjust their eyes
    to even darker places, we can definitely see the sun when it's not on the horizon
    but is higher up in the sky, probably up to *10⁸ cd/m²*, but this range is already
    quite a big range, so let's stick to it for now. For comparison, a usual 8-bit
    image has a contrast ratio of *256:1*, the human eye can see at one time around
    million to 1, and the 14-bit RAW format shows *2^(14):1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Display media also have limitations; for example, a typical IPS monitor has
    a contrast ratio of around *1,000:1*, and a VA monitor could have a contrast ratio
    of up to *6,000:1*. So, let''s place these values on this spectrum and see how
    they compare:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42cafd44-1d8a-4f16-a98f-737a8c9a4532.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, this doesn't look like we can see much, which is true since it takes time
    for us to adjust to different lighting conditions. The same is true about a camera.
    But in just one glance, our naked eye can see quite a lot more than even the best
    camera can. *So how can we remedy this?*
  prefs: []
  type: TYPE_NORMAL
- en: 'As we said, the trick is to take multiple pictures in quick succession, which
    most cameras allow with ease. If we were to take pictures in quick succession
    that complement each other, we could cover quite a big part of the spectrum with
    just five JPEGs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ddd82b2-79ca-437c-afe4-f86aa18ba231.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This seems a little too easy, but remember, taking five pictures is quite easy.
    But, we are talking about one picture that has all the dynamic range, not five
    separate pictures. There are two big problems with HDR images:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How can we combine multiple images into a single image?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How can we display an image that has a higher dynamic range than our display
    media?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, even before we can combine those images, let's take a closer look at
    how can we vary the exposure of the camera, that is, its sensitivity to light.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring ways to vary exposure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed earlier in this chapter, modern **Digital Single Lens Reflector**
    cameras (**DSLR**s), and other digital cameras as well, have a fixed sensor grid
    (usually placed as a Bayer filter), which just measures the light intensity of
    the camera.
  prefs: []
  type: TYPE_NORMAL
- en: I bet you have seen the same camera used to capture beautiful night pictures
    where the water looks like a silky cloud and stills that sports photographers
    have taken of a player at full stretch. *So how can they use the same camera for
    such different settings and get results that we see on the screen?*
  prefs: []
  type: TYPE_NORMAL
- en: When measuring the exposure, it's really hard to measure the luminance that
    is being captured. It's a lot easier to measure relative speed instead of measuring
    luminance in the power of 10, which could be quite difficult to adjust. We measure
    the speed in the power of 2; we call that a **stop**.
  prefs: []
  type: TYPE_NORMAL
- en: The trick is that, even though the camera is restricted, it has to be able to
    capture a limited luminance range per picture. The range itself could be moved
    along the luminance spectrum. To overcome this, let's study the shutter speed,
    aperture, and ISO speed parameters of the camera.
  prefs: []
  type: TYPE_NORMAL
- en: Shutter speed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shutter speed is not really the speed of the shutter, but it's the length of
    time for which a camera's shutter is open when taking a photograph. Thus, it's
    the amount of time for which the digital sensor inside the camera is exposed to
    light for collecting information. It's the most intuitive control out of all the
    camera controls because we can feel it happening.
  prefs: []
  type: TYPE_NORMAL
- en: Shutter speeds are usually measured in fractions of a second. For example, *1/60*
    is the fastest speed for which, if we shake the camera while clicking photos while
    it is held in our hands, it doesn't introduce a blur in the photograph. So if
    you are going to use your own pictures, make sure to not do this, or get yourself
    a tripod.
  prefs: []
  type: TYPE_NORMAL
- en: Aperture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Aperture is the diameter of the hole in the optical lens through which the
    light passes into the camera. The following picture shows examples of the opening
    set to different aperture values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22395445-cfda-4e3e-b0d4-1392f3995960.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Image source—https://en.wikipedia.org/wiki/Aperture#/media/File:Lenses_with_different_apertures.jpg
    (CC SA 4.0)
  prefs: []
  type: TYPE_NORMAL
- en: 'Aperture is usually measured using an *f-number*. The f-number is the ratio
    of the system''s focal length to the diameter of the opening (the entrance pupil).
    We won''t concern ourselves with the focal length of the lens; the only thing
    we need to know is that only zoom lenses have variable focal lengths, thus if
    we don''t change the magnification on the lens, the focal length will stay the
    same. So we can measure the **area** of the entrance pupil by squaring the inverse
    of the **f-number**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ab9769f-3113-47dc-a35b-521a9a2552d2.png)'
  prefs: []
  type: TYPE_IMG
- en: And, we know that the bigger the area, the more light we will get in our pictures.
    Thus, if we were to increase the f-number, that would correspond to a decrease
    in the size of the entrance pupil and our pictures would become darker, enabling
    us to take pictures during the afternoon.
  prefs: []
  type: TYPE_NORMAL
- en: ISO speed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ISO speed is the sensitivity of the sensors used in cameras. It is measured
    using numbers that map the sensitivity of the digital sensor to the chemical films
    that were used when computers were not around yet.
  prefs: []
  type: TYPE_NORMAL
- en: ISO speed is measured in two numbers; for example, *100/21°*, where the first
    number is the speed on the arithmetic scale and the second number is the number
    on the logarithmic scale. Since these numbers have a one-to-one mapping, usually
    the second one is omitted, and we simply write *ISO 100*. ISO 100 is two times
    less sensitive to light than ISO 200 and it is said that the difference is **1
    stop**.
  prefs: []
  type: TYPE_NORMAL
- en: It is easier to talk in powers of 2 rather than powers of 10, so photographers
    came up with the notion of **stops**. One stop is two times different, 2 stops
    are 4 times different, and so on. Thus, *n* stops are *2^n* times different. This
    analogy has become so widespread that people have started using fractional and
    real numbers for stops.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to control the exposure, let's try to look at the
    algorithms that can combine multiple pictures with different exposures into a
    single image.
  prefs: []
  type: TYPE_NORMAL
- en: Generating HDR images using multiple exposure images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, once we know how it's possible to get more pictures, we can take multiple
    photos that have very little or no overlapping dynamic range. Let's have a look
    at the most popular algorithm for HDR, first published by Paul E Debevec and Jitendra
    Malik in 2008.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that if you want to have good results, you have to have pictures
    that are overlapping, to make sure you have good accuracy and since there is noise
    in the photos. It''s usually common to have 1, 2, or at most 3 stops difference
    from picture to picture. If we were to shoot five 8-bit photos with a difference
    of 3 stops, we would cover the human eye''s one million to one sensitivity ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3359ab4c-0eaa-4fb3-b51c-c6985079210e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let's take a closer look at how the Debevec HDR algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s assume that the recorded values the camera sees are some function
    of the scene''s irradiance. We talked about this being linear before, but nothing
    is truly linear in real life. Let the recorded value matrix be ***Z*** and the
    irradiance matrix be **X**; we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cde2add2-a9a6-4caf-b1c1-634856164ae5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have also used **Δt** as the measure of exposure time, and the function
    ***f*** is called the **response function** of our camera. Also, we assume that
    if we double the exposure and half the irradiance, we will have the same output
    and vice versa. This should be true across all images, and the value of ***E*** should
    not change from picture to picture; only the recorded values of ***Z*** and the
    exposure time **Δt** can change. If we apply the **inverse response function**(** *f^(-1)***)
    and take the logarithm of both sides, then we get that for all of the pictures(***i***)
    that we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dde52fc9-57b0-487f-8e40-6e06e708d448.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the trick is to come up with an algorithm that can calculate the ***f^(-1)***,
    and that's what Debevec et al. have done.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, our pixel values are not going to follow this rule exactly, and we
    will have to fit an approximate solution, but, let's take a more detailed look
    at what these values are.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move forward, let's take a look at how we can recover **Δt[i]** values
    from the picture files in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting exposure strength from images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assuming the **principle of reciprocity** for all the camera parameters that
    we discussed previously, let''s try to come up with a function—`exposure_strength`—that
    returns a time equivalent to the exposure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s set a reference for ISO speed and f-stop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s use the `exifread` Python package, which makes it easy to read
    the metadata associated with the images. Most modern cameras record the metadata
    in this standard format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s extract the `f_stop` value and see how much bigger the entrance
    pupil area to the reference was:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s see how much more sensitive the ISO setting was:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s combine all the values with the shutter speed and return `exposure_time`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example of the values of the photographs that I am using for this
    demo, taken from the **Frozen River** photo collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Photograph** | **Aperture** | **ISO Speed** | **Shutter Speed** |'
  prefs: []
  type: TYPE_TB
- en: '| AM5D5669.CR2 | 6 3/8 | 100 | 1/60 |'
  prefs: []
  type: TYPE_TB
- en: '| AM5D5670.CR2 | 6 3/8 | 100 | 1/250 |'
  prefs: []
  type: TYPE_TB
- en: '| AM5D5671.CR2 | 6 3/8 | 100 | 1/160 |'
  prefs: []
  type: TYPE_TB
- en: '| AM5D5672.CR2 | 6 3/8 | 100 | 1/100 |'
  prefs: []
  type: TYPE_TB
- en: '| AM5D5673.CR2 | 6 3/8 | 100 | 1/40 |'
  prefs: []
  type: TYPE_TB
- en: '| AM5D5674.CR2 | 6 3/8 | 160 | 1/40 |'
  prefs: []
  type: TYPE_TB
- en: '| AM5D5676.CR2 | 6 3/8 | 250 | 1/40 |'
  prefs: []
  type: TYPE_TB
- en: 'This is the output of the time estimates for these pictures using the `exposure_strength`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now, once we have the exposure times, let's see how this can be used to get
    the camera response function.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the camera response function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s plot ![](img/cbafee7e-4b86-48bc-9cab-fce681e46113.png) on the *y* axis,
    and **Z[i]** on the *x* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/acd17978-8a98-4618-ad89-de957b72155a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What we are trying to do is to find an ***f^(-1)*** and, more importantly,
    the ![](img/d04631b2-e5aa-4874-b7b3-06192cc9d677.png) of all the pictures, such
    that when we add **log(E)** to the log exposure, we will have all the pixels on
    the same function. You can see the results of the Debevec algorithm in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/594d6332-a67a-4fc5-b877-afd67b331ccb.png)'
  prefs: []
  type: TYPE_IMG
- en: The Debevec algorithm estimates both the ***f^(-1)***, which passes approximately
    through all the pixels, and the ![](img/d04631b2-e5aa-4874-b7b3-06192cc9d677.png).
    The ***E*** matrix is the resulting HDR image matrix that we recover.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's take a look at how to implement this using OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Writing an HDR script using OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step of the script is going to be setting up the script arguments
    using Python''s built-in `argparse` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have set up two mutually exclusive arguments—`--image-dir`,
    a directory that contains the images, and `--images`, a list of images that we
    are going to use. And we make sure that we populate `args.images` with the list
    of all the images, so the rest of the script shouldn't worry about which of the
    options the user has chosen.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have all the command-line arguments, the rest of the procedure is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read all `images` into the memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the metadata and estimate exposure times using `exposure_strength`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculate the **camera response function**—`crf_debevec`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the camera response function to calculate the HDR image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the HDR image is of type `float32` and not `uint8`, as it contains
    the full dynamic range of all exposure images.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have the HDR image and we've come to the next important part. Let's see
    how we can show the HDR image using our 8-bit image representation.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying HDR images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Displaying HDR images is tricky. As we said, HDR has more values than the camera,
    so we need to figure out a way to display that. Luckily, OpenCV is here to help
    us again, and, as you've probably guessed by now, we can use gamma correction
    to map all the different values we have into a smaller spectrum of values in the
    range `0` to `255`. This process is called **Tone Mapping**.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV has a method for it that takes `gamma` as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have to `clip` all the values to become integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we can show our resulting HDR image using `pyplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following gorgeous image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bedf9ea-7f0e-4b41-a0cc-81602427037d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's see how can we extend the camera's field of view—potentially to 360
    degrees!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding panorama stitching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another very interesting topic in computational photography is **panorama stitching**.
    I'm sure most of you have a panorama function on your phone. This section will
    focus on the ideas behind panorama stitching and, instead of just calling a single
    function, we will go through all the steps involved in creating a panorama from
    a bunch of separate photos.
  prefs: []
  type: TYPE_NORMAL
- en: Writing script arguments and filtering images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We want to write a script that will take a list of images and will produce
    a single panorama picture. So, let''s set up the `ArgumentParser` for our script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Here, we created an instance of `ArgumentParser` and added arguments to pass
    either an image directory of a list of images. Then, we make sure we get all the
    images from the image directory if it is passed, instead of passing a list of
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, as you can imagine, the next step is to use a feature extractor and see
    what the common features that images share are. This is very much like the previous
    two chapters, that is, [Chapter 3](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml), *Finding
    Objects via Feature Matching and Perspective Transforms,* and [Chapter 4](efb28928-4399-4e3d-9ca3-6e773aaaa699.xhtml),
    *3D Scene Reconstruction Using Structure from Motion*. We will also write a function
    to filter those images that have common features, so the script is even more versatile.
    Let''s go through the function step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `SURF` feature extractor and compute all of the features of all
    images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `matcher` class that matches an image to its closest neighbors that
    share the most features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Filter the images and make sure that we have at least two images that share
    features so we can proceed with the algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `matcher` again to check whether we have removed any of the images
    and `return` the variables we will need in the future:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: After we have filtered the images and have all the features, we move on to the
    next step, which is setting up a blank canvas for the panorama stitching.
  prefs: []
  type: TYPE_NORMAL
- en: Figuring out relative positions and the final picture size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have separated all the connected pictures and know all the features,
    it's time to figure out how big the merged panorama is going to be and create
    the blank canvas to start adding pictures to it. First, we need to find the parameters
    of the pictures.
  prefs: []
  type: TYPE_NORMAL
- en: Finding camera parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to be able to merge images, we need to compute homography matrices
    of all the images and then use those to adjust the images so they can be merged
    together. We will write a function to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to create the `HomographyBasedEstimator()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the `estimator`, for extracting all the camera parameters, we
    use the matched `features` from different images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We make sure the `R` matrices have the correct type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we `return` all the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: It is possible to make these parameters better using a refiner, for example,
    `cv2.detail_BundleAdjusterRay`, but we'll keep things simple for now.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the canvas for the panorama
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it''s time to create the canvas. For that, we create a `warper` object
    based on our desired rotation schema. For simplicity, let''s assume a planar model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we `enumerate` over all the connected images and get all the regions
    of interest in each of the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we estimate the final `canvas_size` based on all regions of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's see how to use the canvas size to blend all the images together.
  prefs: []
  type: TYPE_NORMAL
- en: Blending the images together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we create a `MultiBandBlender` object, which will help us merge images
    together. Instead of just picking values from one or the other image, `blender`
    will do interpolation between the available values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for each of the connected images, we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We `warp` the image and get the `corner` locations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, calculate the `mask` of the image on the canvas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, convert the values into `np.int16` and `feed` it into `blender`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we use the `blend` function on `blender`, to get the final `result`,
    and save it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also scale the image down to 600 pixels wide and display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'When we use the images from [https://github.com/mamikonyana/yosemite-panorama](https://github.com/mamikonyana/yosemite-panorama), we
    have this wonderful panorama picture in the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7a027a5-0cc3-4c62-a350-720fe117d104.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that it's not perfect and the white balance requires correcting
    from picture to picture, but this is a great start. In the next section, we will
    work on refining the stitching output.
  prefs: []
  type: TYPE_NORMAL
- en: Improving panorama stitching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can either play with the script that we already have and add or remove certain
    features (for example, you can add a white balance compensator, to make sure you
    have a smoother transition from one picture to another), or you can tweak other
    parameters to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'But know this—when you need a quick panorama, OpenCV also has a handy `Stitcher`
    class that does most of what we have discussed already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet is probably a lot faster than uploading your photos to a panorama
    service to get a good picture—so enjoy creating panoramas!
  prefs: []
  type: TYPE_NORMAL
- en: Don't forget to add some code to crop the panorama so it doesn't have black
    pixels!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to take simple images that we can take from
    our cameras with limited abilities—either with limited dynamic range or limited
    field of view and use OpenCV to merge multiple images into a single one that is better
    than the original one.
  prefs: []
  type: TYPE_NORMAL
- en: We left you with three scripts that you could build upon. Most importantly,
    there are still a lot of features missing from `panorama.py`, and there are a
    lot of other HDR techniques. Best of all, it's possible to do HDR and panorama
    stitching at the same time. *Wouldn't it be splendid to just look around from
    the mountain top at sunset? Imagine that!*
  prefs: []
  type: TYPE_NORMAL
- en: This was the last chapter about camera photography. The rest of this book will
    focus on video monitoring and applying machine learning techniques to image processing
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on tracking visually salient and moving objects
    in a scene. This will give you an understanding of how to deal with non-static
    scenes. We will also explore how we can make an algorithm focus on what's important
    in a scene quickly, which is a technique known to speed up object detection, object
    recognition, object tracking, and content-aware image editing.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a lot of other topics to explore in computational photography:'
  prefs: []
  type: TYPE_NORMAL
- en: It's especially worth taking a look at the **Exposure Fusion** technique developed
    by Tom Mertens, et al. The *Exposure fusion* article by Tom Mertens, Jan Kautz,
    and Frank Van Reeth, in Computer Graphics and Applications, 2007, Pacific Graphics
    2007, proceedings at 15th Pacific Conference on, pages 382–390, IEEE, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Recovering High Dynamic Range Radiance Maps from Photographs* article by
    Paul E Debevec and Jitendra Malik, in ACM SIGGRAPH 2008 classes, 2008, page 31,
    ACM, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Frozen River** photo collection can be found at [https://github.com/mamikonyana/frozen-river](https://github.com/mamikonyana/frozen-river) and
    is verified with a CC-BY-SA-4.0 license.
  prefs: []
  type: TYPE_NORMAL
