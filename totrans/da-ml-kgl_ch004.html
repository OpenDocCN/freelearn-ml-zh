<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>Data Analysis and Machine Learning with Kaggletrue</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>

</head>
<body>

<section id="working-and-learning-with-kaggle-notebooks" class="level1 pkt" data-number="4">
<h1 data-number="4">Working and Learning with Kaggle Notebooks</h1>
<p>Kaggle notebooks—<em>which until recently were called kernels, so please excuse me if I occasionally use those terms interchangeably</em>—are Jupyter notebooks in the browser that can run free of charge. This means you can execute your experiments from any device with an internet connection, although something bigger than a mobile phone is probably a good idea. The technical specification of the environment (as of the time of this writing) is given below:</p>
<figure>
<img src="../media/file14.png" />
</figure>
<p>Without further ado, let us jump into it. The first thing we do is figure out how to set up a notebook.</p>
<section id="setting-up-a-kernel" class="level2" data-number="4.1">
<h2 data-number="4.1">Setting up a kernel</h2>
<p>There are two primary methods of creating a notebook: from the front page or from a dataset level.</p>
<p>To proceed with the first method, go to the <strong>Code</strong> section of the menu on the left-hand side of the landing page at <a href="https://www.kaggle.com/">https://www.kaggle.com/</a> and press the <strong>New Notebook</strong> button. This is a preferred method if you are planning an experiment involving uploading your own dataset:</p>
<figure>
<img src="../media/file15.png" />
</figure>
<p>Alternatively, you can go to the page of the dataset you are interested in and click the <strong>New Notebook</strong> button there:</p>
<figure>
<img src="../media/file16.png" />
</figure>
<p>Whichever method you chose, after clicking <strong>New Notebook</strong> you will be taken to your notebook page:</p>
<figure>
<img src="../media/file17.png" />
</figure>
<p>By default, a new notebook is initialized language set to <strong>Python</strong>—if you want to use R instead, click on the <strong>Language</strong> dropdown on the right-hand side and your notebook will switch to <strong>R</strong>:</p>
<figure>
<img src="../media/file18.png" />
</figure>
<p>An important aspect of using notebooks: you can always take an existing one (created by somebody) and clone it to modify and adjust to your needs. This can be achieved by pressing the <strong>Copy and Edit</strong> button on the kernel page, although in Kaggle parlance, the process is referred to as <strong>forking</strong>:</p>
<figure>
<img src="../media/file19.png" />
</figure>
<p>A note on etiquette: if you have participated in a Kaggle competition, you probably noticed that the leaderboard is flooded by forks of forks of well scoring notebooks. Nothing wrong with building on somebody else’s work—but if you do, remember to upvote the original author.</p>
<p>A notebook you create is private (i.e. only visible to you) by default. If you want to make it available to others, you can select</p>
</section>
<section id="upgrade-to-gcp" class="level2" data-number="4.2">
<h2 data-number="4.2">Upgrade to GCP</h2>
<p>Sometimes the resources provided freely by Kaggle are not sufficient for the task you need, and you need to move to a beefier machine. You can setup the whole environment yourself—or you can stay within the framework of notebooks but swap the underlying machine. This is there Google Cloud AI Notebooks come in.</p>
<p>In order to migrate your notebook to the GCP environment, go to the sideline menu on the left-hand side and click on <strong>Upgrade to Google Cloud AI Notebooks</strong>:</p>
<figure>
<img src="../media/file20.png" />
</figure>
<p>You will be greeted by the prompt:</p>
<figure>
<img src="../media/file21.png" />
</figure>
<p>After that, you will be redirected to the <strong>Google Cloud Platform</strong> console, where you need to configure your billing options—unlike Kaggle, GCP is not free. If it is your first time, you will need to complete a tutorial guiding you through the necessary steps:</p>
<figure>
<img src="../media/file22.png" />
</figure>
</section>
<section id="one-step-beyond" class="level2" data-number="4.3">
<h2 data-number="4.3">One step beyond</h2>
<p>As mentioned earlier in this chapter, Kaggle notebooks are a fantastic tool for education and participating in competitions—but they also serve another extremely useful purpose, namely a component of a portfolio you can use to demonstrate your data science skills.</p>
<p>There are many potential criteria to consider when building your data science portfolio (branding, audience reach, enabling a pitch to your potential employer etc.) but none of them matter if nobody can find them. Because Kaggle is part of Google, the notebooks are indexed by the most popular search engine in the world—so if someone is looking for a topic related to your code, it will show up in their search results.</p>
<p>Below I show a “personal” example: a few years ago, I wrote a notebook for a competition—the problem I wanted to tackle was adversarial validation (for those unfamiliar with the topic: a fairly easy way to see if your training and test sets have a similar distribution is to build a binary classifier trained to tell them apart). When writing this chapter, I tried to give it a try and lo and behold, it shows up high in the search results (notice the fact that I did not mention Kaggle or any personal details like name in my query):</p>
<figure>
<img src="../media/file23.png" />
</figure>
<p>Moving on to other benefits of using notebooks to demonstrate your skillset: just like competitions, datasets and discussions, notebooks can be awarded votes/medals and thus position you in the progression system and ranking. You can stay away from the competitions track and become an expert/master/grandmaster purely by focusing on high quality code the community appreciates. The most up-to-date version of the progression requirements can be found at <a href="https://www.kaggle.com/progression">https://www.kaggle.com/progression</a>, below we give a snapshot relevant to notebooks:</p>
<figure>
<img src="../media/file24.png" />
</figure>
<p>Your Kaggle profile comes with followers/following option and gives you a possibility to link other professional networks like LinkedIn or GitHub, so you can leverage the connection gained inside the community:</p>
<figure>
<img src="../media/file25.png" />
</figure>
<p>In this day and age, it is easy to be skeptical about claims of “community building”—but in the case of Kaggle, it happens to actually be true. Their brand recognition in the data science universe is second to none, both among practitioners and among recruiters who actually do their homework. In practice, this means that a (decent enough) Kaggle profile can get you through the door already—which, as we all know, is frequently the hardest step.</p>
</section>
<section id="kaggle-courses" class="level2" data-number="4.4">
<h2 data-number="4.4">Kaggle courses</h2>
<p>A great many things about Kaggle are about acquiring knowledge be it the things you learn in a competition, datasets you manage to find in the ever-growing repository or demonstration of a hitherto unknown model class, there is always something new to find out. The newest addition to that collection are the courses gathered under the <em>Kaggle Learn</em> label: <a href="https://www.kaggle.com/learn">https://www.kaggle.com/learn</a>. Those are micro-courses marketed by Kaggle as “the single fastest way to gain the skills you’ll need to do independent data science projects”, the core unifying theme being a crash course introduction across a variety of topics. Each course is divided into small chapters, followed by coding practice questions.</p>
<p>Below, we provide a brief summary of their content:</p>
<ul>
<li><strong>Python</strong>: <a href="https://www.kaggle.com/learn/python">https://www.kaggle.com/learn/python</a> You will learn the basics of functions, Boolean variables, loops, lists and dictionaries.</li>
<li><strong>Intro to ML / Intermediate ML</strong>: <a href="https://www.kaggle.com/learn/intro-to-machine-learning">https://www.kaggle.com/learn/intro-to-machine-learning</a> Those two courses are best viewed as a two-parter: the first one introduces different classes of models used in machine learning, followed by discussion of topics common to different models like under/overfitting or model validation. The second one goes deeper into feature engineering, dealing with missing values and handling categorical variables.</li>
<li><strong>Pandas</strong>: <a href="https://www.kaggle.com/learn/pandas">https://www.kaggle.com/learn/pandas</a>: this course provides a crash introduction to one of the most fundamental tools used in modern data science. You first learn how to create / read / write data, moving on to data cleaning (indexing, selecting, combining, grouping etc).</li>
<li><strong>Data visualization</strong>: <a href="https://www.kaggle.com/learn/data-visualization">https://www.kaggle.com/learn/data-visualization</a> Everybody knows a picture can be worth a thousand words – if you want to learn how to create such images summarizing your data science results, this course is for you. You will walk away know how to handle everything from line charts to heatmaps and scatterplots.</li>
<li><strong>Feature engineering</strong>: <a href="https://www.kaggle.com/learn/feature-engineering">https://www.kaggle.com/learn/feature-engineering</a> This short course demonstrates the basic ideas around encoding categorical data, general feature generation and selection.</li>
<li><strong>Data cleaning</strong>: <a href="https://www.kaggle.com/learn/data-cleaning">https://www.kaggle.com/learn/data-cleaning</a> Another short course, which helps address one of the most glaring omissions in the academic curriculum: making students realize how messy real-life data is.</li>
<li><strong>Intro to SQL / Advanced SQL</strong> <a href="https://www.kaggle.com/learn/intro-to-sql">https://www.kaggle.com/learn/intro-to-sql</a> In this tandem of courses, you will learn to extract data using SQL. Starting with basic SELECT variations, you will go through GROUP BY, HAVING, all the way to JOINs/UNIONs and explore analytic functions and nested data.</li>
<li><strong>Geospatial Analysis:</strong> <a href="https://www.kaggle.com/learn/geospatial-analysis">https://www.kaggle.com/learn/geospatial-analysis</a> <strong></strong> This course will teach you to create your first map using GeoPandas and introduce ways to create interactive and choropleth maps. Basics of proximity analysis are introduced as well.</li>
<li><strong>Intro to Deep Learning</strong> <a href="https://www.kaggle.com/learn/intro-to-deep-learning">https://www.kaggle.com/learn/intro-to-deep-learning</a> This course offers a crash introduction into what is arguably the most important methodology in modern deep learning. Using structured data, you will familiarize yourself with fundamental concepts like gradient descent, batch normalization and apply this knowledge to vintage problem of binary classification.</li>
<li><strong>Computer Vision</strong> <a href="https://www.kaggle.com/learn/computer-vision">https://www.kaggle.com/learn/computer-vision /</a><strong>NLP</strong> <a href="https://www.kaggle.com/learn/natural-language-processing">https://www.kaggle.com/learn/natural-language-processing</a> are two crash courses introducing the most important domains where deep learning has been successfully applied, producing impressive state-of-the-art results. Crucial topics of transfer learning and data augmentation are introduced, giving you the tools to hit the ground running.</li>
<li><strong>Game AI</strong> <a href="https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning">https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning</a> This course is a great wrap-up of the tech-focused part of the curriculum introduced by Kaggle in the learning modules. You will write a game-playing agent, tinker with its performance and use the minimax algorithm.</li>
<li><strong>Machine Learning Explainability</strong> <a href="https://www.kaggle.com/learn/machine-learning-explainability">https://www.kaggle.com/learn/machine-learning-explainability</a> <strong></strong> Building models is fun, but in the real world not everybody is a data scientist, so you might find yourself in a position when you need to explain what you have done to others. This is where the mini course on model explainability comes in: you will learn to assess how relevant your features are with three different methods: permutation importance, SHAP and partial dependence plots.</li>
<li><strong>AI Ethics</strong> <a href="https://www.kaggle.com/learn/intro-to-ai-ethics">https://www.kaggle.com/learn/intro-to-ai-ethics</a> This last course is a very interesting addition to the proposition: it discusses the practical tools to guide the moral design of AI systems. You will learn how to identify the bias in AI models, examine the concept of AI fairness and find out how to increase transparency by communicating ML models information.</li>
</ul>
<p>Apart from the original content created by Kaggle, there are multiple other learning opportunities available on the platform using kernels. A prominent example worth mentioning is the extremely popular fast.ai course: <a href="https://www.kaggle.com/general/63077">https://www.kaggle.com/general/63077</a></p>
<p>In this chapter, we have discussed Kaggle kernels: a multi-purpose, open coding environment, which can be used for education, experimentation as well promoting your data science project portfolio.</p>
</section>
</section>
</body>
</html>
