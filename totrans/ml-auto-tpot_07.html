<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer111">
			<h1 id="_idParaDest-64"><em class="italic"><a id="_idTextAnchor065"/>Chapter 5</em>: Parallel Training with TPOT and Dask</h1>
			<p>In this chapter, you'll dive into a bit of a more advanced topic; that is, automated machine learning. You'll learn how to handle machine learning tasks in a parallel manner by distributing the work on a Dask cluster. This chapter will be more theoretical than the previous two, but you will still learn many useful things.</p>
			<p>We'll cover essential topics and ideas behind parallelism in Python, and you'll learn how to achieve parallelism in a couple of different ways. Then, we'll dive deep into the Dask library, explore its basic functionality, and see how you can tie it with TPOT.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li>Introduction to parallelism in Python</li>
				<li>Introduction to the Dask library</li>
				<li>Training machine learning models with TPOT and Dask</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor066"/>Technical requirements</h1>
			<p>No prior exposure to Dask or even parallel programming is required for you to read and understand this chapter. Previous experience is helpful, as fitting this big of a concept into a few pages is close to impossible. You should still be able to follow and fully understand everything written here as all of the concepts will be explained.</p>
			<p>You can download the source code and dataset for this chapter here: <a href="https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter05">https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter05</a>.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor067"/>Introduction to parallelism in Python</h1>
			<p>Executing tasks sequentially (where the second one starts after the first one finishes) is required in <a id="_idIndexMarker290"/>some situations. For example, maybe the input of the <a id="_idIndexMarker291"/>second function relies on the output of the first one. If that's the case, these two functions (processes) can't be executed at the same time. </p>
			<p>But more often than not, that's not the case. Just imagine your program is connecting to three different API endpoints before the dashboard is displayed. The first API returns the current weather conditions, the second one returns the stock prices, and the last one returns today's exchange rates. There's no point in making the API calls one after the other. They don't rely on each other, so running them sequentially would be a huge waste of time.</p>
			<p>Not only that, but it would also be a waste of CPU cores. Most modern PCs have at least four CPU cores. If you're running things sequentially, you're only using a single core. Why not use all of them if you can?</p>
			<p>One of the ways to achieve parallelism in Python is with multiprocessing. It is a process-based parallelism technique. As you would imagine, Python has a <strong class="source-inline">multiprocessing</strong> library built into it, and this section will teach you how to use it. With Python 3.2 and beyond, this library stopped being the recommended way of implementing multiprocessing in your apps. There's a new kid on the block, and its name is <strong class="source-inline">concurrent.futures</strong>. It's yet another built-in library you'll learn how to use in this section.</p>
			<p>The simplest way to explain and understand multiprocessing is with Python's built-in <strong class="source-inline">time</strong> library. You can use it to track time differences and to pause program execution intentionally, among other things. This is just what we need because we can put in many print statements with some time gaps between them, and then see how the program acts when it's run sequentially and how it acts when it's run in parallel.</p>
			<p>You will learn how multiprocessing works in Python through a couple of hands-on examples. </p>
			<p>For starters, please take a look at the following code snippet. In it, the <strong class="source-inline">sleep_func()</strong> function has been declared. Its task is to print a message, pause the program executing for 1 second, and then to print another message as the function completes. We can monitor the time this function takes to run for an arbitrary number of times (let's say five) and then print out the execution time duration. The snippet is as follows:</p>
			<p class="source-code">import time </p>
			<p class="source-code">def sleep_func():</p>
			<p class="source-code">    print('Sleeping for a 1 second')</p>
			<p class="source-code">    time.sleep(1)</p>
			<p class="source-code">    print('Done.')</p>
			<p class="source-code">if __name__ == '__main__':</p>
			<p class="source-code">    time_start = time.time()</p>
			<p class="source-code">    # Run the function 5 times</p>
			<p class="source-code">    sleep_func()</p>
			<p class="source-code">    sleep_func()</p>
			<p class="source-code">    sleep_func()</p>
			<p class="source-code">    sleep_func()</p>
			<p class="source-code">    sleep_func()</p>
			<p class="source-code">    time_stop = time.time()</p>
			<p class="source-code">    print(f'Took {round(time_stop - time_start, 2)} seconds to execute!')</p>
			<p>The <a id="_idIndexMarker292"/>corresponding <a id="_idIndexMarker293"/>output is shown here:</p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Took 5.02 seconds to execute!</strong></p>
			<p>So, what happened here? Nothing unexpected, to say the least. The <strong class="source-inline">sleep_func()</strong> function <a id="_idIndexMarker294"/>executed sequentially five times. The execution time is approximately 5 seconds. You <a id="_idIndexMarker295"/>could also simplify the preceding snippet in the following manner:</p>
			<p class="source-code">import time </p>
			<p class="source-code">def sleep_func():</p>
			<p class="source-code">    print('Sleeping for a 1 second')</p>
			<p class="source-code">    time.sleep(1)</p>
			<p class="source-code">    print('Done.')</p>
			<p class="source-code">if __name__ == '__main__':</p>
			<p class="source-code">    time_start = time.time()</p>
			<p class="source-code">    # Run the function 5 times in loop</p>
			<p class="source-code">    for _ in range(5):</p>
			<p class="source-code">        sleep_func()</p>
			<p class="source-code">    time_stop = time.time()</p>
			<p class="source-code">    print(f'Took {round(time_stop - time_start, 2)} seconds to execute!')</p>
			<p>The <a id="_idIndexMarker296"/>result is <a id="_idIndexMarker297"/>identical, as you would expect:</p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Took 5.01 seconds to execute!</strong></p>
			<p>Is there a problem with this approach? Well, yes. We're wasting both time and CPU cores. The functions aren't dependent in any way, so why don't we run them in parallel? As we mentioned previously, there are two ways of doing this. Let's examine the older way first, through the <strong class="source-inline">multiprocessing</strong> library.</p>
			<p>It's a bit of a lengthy approach because it requires declaring a process, starting it, and joining it. It's not so tedious if you have only a few, but what if there are tens of processes in your program? It can become tedious fast. </p>
			<p>The following code snippet demonstrates how to run the <strong class="source-inline">sleep_func()</strong> function three times in parallel:</p>
			<p class="source-code">import time </p>
			<p class="source-code">from multiprocessing import Process</p>
			<p class="source-code">def sleep_func():</p>
			<p class="source-code">    print('Sleeping for a 1 second')</p>
			<p class="source-code">    time.sleep(1)</p>
			<p class="source-code">    print('Done.')</p>
			<p class="source-code">if __name__ == '__main__':</p>
			<p class="source-code">    time_start = time.time()</p>
			<p class="source-code">    process_1 = Process(target=sleep_func)</p>
			<p class="source-code">    process_2 = Process(target=sleep_func)</p>
			<p class="source-code">    process_3 = Process(target=sleep_func)</p>
			<p class="source-code">    process_1.start()</p>
			<p class="source-code">    process_2.start()</p>
			<p class="source-code">    process_3.start()</p>
			<p class="source-code">    process_1.join()</p>
			<p class="source-code">    process_2.join()</p>
			<p class="source-code">    process_3.join()</p>
			<p class="source-code">    time_stop = time.time()</p>
			<p class="source-code">    print(f'Took {round(time_stop - time_start, 2)} seconds to  execute!')</p>
			<p>The <a id="_idIndexMarker298"/>output <a id="_idIndexMarker299"/>is shown here:</p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Took 1.07 seconds to execute!</strong></p>
			<p>As you can see, each of the three processes was launched independently and in parallel, so they all managed to finish in a single second.</p>
			<p>Both <strong class="source-inline">Process()</strong> and <strong class="source-inline">start()</strong> are self-explanatory, but what is the <strong class="source-inline">join()</strong> function doing? Simply put, it tells Python to wait until the process is complete. If you call <strong class="source-inline">join()</strong> on all of the processes, the last two code lines won't execute until all of the processes are finished. For fun, try to remove the <strong class="source-inline">join()</strong> calls; you'll immediately get the gist.</p>
			<p>You now have <a id="_idIndexMarker300"/>a basic intuition behind multiprocessing, but the story <a id="_idIndexMarker301"/>doesn't end here. Python 3.2 introduced a new, improved way of executing tasks in parallel. The <strong class="source-inline">concurrent.futures</strong> library is the best one available as of yet, and you'll learn how to use it next.</p>
			<p>With it, you don't have to manage processes manually. Every executed function will return something, which is <strong class="source-inline">None</strong> in the case of our <strong class="source-inline">sleep_func()</strong> function. You can change it by returning the last statement instead of printing it. Furthermore, this new approach uses <strong class="source-inline">ProcessPoolExecutor()</strong> to run. You don't need to know anything about it; just remember that it is used to execute multiple processes at the same time. Codewise, simply put everything you want to run in parallel inside. This approach unlocks two new functions:</p>
			<ul>
				<li><strong class="source-inline">submit()</strong>: Used to <a id="_idIndexMarker302"/>run the function in parallel. The returned results will be appended to a list so that we can print them (or do anything else) with the next function.</li>
				<li><strong class="source-inline">result()</strong>: Used to <a id="_idIndexMarker303"/>obtain the returned value from the function. We'll simply print the result, but you're free to do anything else.</li>
			</ul>
			<p>To recap, we'll append the results to a list, and then print them out as the functions finish executing. The <a id="_idIndexMarker304"/>following snippet shows you how to implement <a id="_idIndexMarker305"/>multiprocessing with the most recent Python approach:</p>
			<p class="source-code">import time </p>
			<p class="source-code">import concurrent.futures</p>
			<p class="source-code">def sleep_func():</p>
			<p class="source-code">    print('Sleeping for a 1 second')</p>
			<p class="source-code">    time.sleep(1)</p>
			<p class="source-code">    return 'Done.'</p>
			<p class="source-code">if __name__ == '__main__':</p>
			<p class="source-code">    time_start = time.time()</p>
			<p class="source-code">    with concurrent.futures.ProcessPoolExecutor() as ppe:</p>
			<p class="source-code">        out = []</p>
			<p class="source-code">        for _ in range(5):</p>
			<p class="source-code">            out.append(ppe.submit(sleep_func))</p>
			<p class="source-code">        for curr in concurrent.futures.as_completed(out):</p>
			<p class="source-code">            print(curr.result())</p>
			<p class="source-code">    time_stop = time.time()</p>
			<p class="source-code">    print(f'Took {round(time_stop - time_start, 2)} seconds to execute!')</p>
			<p>The results are shown here:</p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for a 1 second</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Done.</strong></p>
			<p class="source-code"><strong class="bold">Took 1.17 seconds to execute!</strong></p>
			<p>As you can see, the program behaves similarly to what we had previously, with a few added benefits – you don't have to manage processes on your own, and the syntax is much cleaner.</p>
			<p>The one issue <a id="_idIndexMarker306"/>we have so far is the lack of function parameters. Currently, we're just calling a function that doesn't accept any parameters. That won't be <a id="_idIndexMarker307"/>the case most of the time, so it's important to learn how to handle function parameters as early as possible.</p>
			<p>We'll introduce a single parameter to our <strong class="source-inline">sleep_func()</strong> function that allows us to specify how long the execution will be paused. The print statements inside the function are updated accordingly. The sleep times are defined within the <strong class="source-inline">sleep_seconds</strong> list, and the value is passed to <strong class="source-inline">append()</strong> at each iteration as a second parameter. </p>
			<p>The entire snippet is shown here:</p>
			<p class="source-code">import time </p>
			<p class="source-code">import concurrent.futures</p>
			<p class="source-code">def sleep_func(how_long: int):</p>
			<p class="source-code">    print(f'Sleeping for a {how_long} seconds')</p>
			<p class="source-code">    time.sleep(how_long)</p>
			<p class="source-code">    return f'Finished sleeping for {how_long} seconds.'</p>
			<p class="source-code">if __name__ == '__main__':</p>
			<p class="source-code">    time_start = time.time()</p>
			<p class="source-code">    sleep_seconds = [1, 2, 3, 1, 2, 3]</p>
			<p class="source-code">    with concurrent.futures.ProcessPoolExecutor() as ppe:</p>
			<p class="source-code">        out = []</p>
			<p class="source-code">        for sleep_second in sleep_seconds:</p>
			<p class="source-code">            out.append(ppe.submit(sleep_func, sleep_second))</p>
			<p class="source-code">        for curr in concurrent.futures.as_completed(out):</p>
			<p class="source-code">            print(curr.result())</p>
			<p class="source-code">    time_stop = time.time()</p>
			<p class="source-code">    print(f'Took {round(time_stop - time_start, 2)} seconds to execute!')</p>
			<p>The <a id="_idIndexMarker308"/>results <a id="_idIndexMarker309"/>are shown here:</p>
			<p class="source-code"><strong class="bold">Sleeping for 1 seconds</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for 2 seconds</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for 3 seconds</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for 1 seconds</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for 2 seconds</strong></p>
			<p class="source-code"><strong class="bold">Sleeping for 3 seconds</strong></p>
			<p class="source-code"><strong class="bold">Finished sleeping for 1 seconds.</strong></p>
			<p class="source-code"><strong class="bold">Finished sleeping for 1 seconds.</strong></p>
			<p class="source-code"><strong class="bold">Finished sleeping for 2 seconds.</strong></p>
			<p class="source-code"><strong class="bold">Finished sleeping for 2 seconds.</strong></p>
			<p class="source-code"><strong class="bold">Finished sleeping for 3 seconds.</strong></p>
			<p class="source-code"><strong class="bold">Finished sleeping for 3 seconds.</strong></p>
			<p class="source-code"><strong class="bold">Took 3.24 seconds to execute!</strong></p>
			<p>That's how you can handle function parameters in parallel processing. Keep in mind that the executing <a id="_idIndexMarker310"/>time won't be exactly the same on every machine, as the runtime duration will depend on your hardware. As a general rule, you should definitely <a id="_idIndexMarker311"/>see a speed improvement compared to a non-parallelized version of the script. You now know the basics of parallel processing. In the next section, you'll learn where Python's Dask library comes into the picture, and in the section afterward, you'll combine parallel programming, Dask, and TPOT in order to build machine learning models.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor068"/>Introduction to the Dask library</h1>
			<p>You can think of Dask as one of the most revolutionary Python libraries for data processing at scale. If you are a regular pandas and NumPy user, you'll love Dask. The library allows <a id="_idIndexMarker312"/>you to work with data NumPy and pandas doesn't allow because they don't fit into the RAM.</p>
			<p>Dask supports both NumPy array and pandas DataFrame data structures, so you'll quickly get up to speed with it. It can run either on your computer or a cluster, making it that much easier to scale. You only need to write the code once and then choose the environment that you'll run it in. It's that simple.</p>
			<p>One other thing to note is that Dask allows you to run code in parallel with minimal changes. As you saw earlier, processing things in parallel means the execution time decreases, which is generally the behavior we want. Later, you'll learn how parallelism in Dask works with <strong class="source-inline">dask.delayed</strong>.</p>
			<p>To get started, you'll have to install the library. Make sure the correct environment is activated. Then, execute the following from the Terminal:</p>
			<p class="source-code"><strong class="bold">pipenv install "dask[complete]"</strong></p>
			<p>There are other installation options. For example, you could install only the arrays or DataFrames module, but it's a good idea to install everything from the start. Don't forget to put quotes around the library name, as not doing so will result in an error.</p>
			<p>If you've installed everything, you'll have access to three Dask collections – arrays, DataFrames, and bags. All of these can store datasets that are larger than your RAM size, and they can all partition data between RAM and a hard drive. </p>
			<p>Let's start with <a id="_idIndexMarker313"/>Dask arrays and compare them with a NumPy alternative. You can create <a id="_idIndexMarker314"/> a NumPy array of ones with 1,000x1,000x1,000 dimensions by executing the following code cell in a Notebook environment. The <strong class="source-inline">%%time</strong> magic command is used to measure the time needed for the cell to finish with the execution:</p>
			<p class="source-code">%%time</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">np_ones = np.ones((1000, 1000, 1000))</p>
			<p>Constructing larger arrays than this one results in a memory error on my machine, but this will do just fine for the comparisons. The corresponding output is shown here:</p>
			<p class="source-code"><strong class="bold">CPU times: user 1.86 s, sys: 2.21 s, total: 4.07 s</strong></p>
			<p class="source-code"><strong class="bold">Wall time: 4.35 s</strong></p>
			<p>As you can see, it took 4.35 seconds to create this array. Now, let's do the same with Dask:</p>
			<p class="source-code">%%time</p>
			<p class="source-code">import dask.array as da</p>
			<p class="source-code">da_ones = da.ones((1000, 1000, 1000))</p>
			<p>As you can see, the only change is in the library import name. The executing time results will probably come as a surprise if this is your first encounter with the Dask library. They are shown here:</p>
			<p class="source-code"><strong class="bold">CPU times: user 677 µs, sys: 12 µs, total: 689 µs</strong></p>
			<p class="source-code"><strong class="bold">Wall time: 696 µs </strong></p>
			<p>Yes, you are <a id="_idIndexMarker315"/>reading this right. Dask took 696 microseconds to create an array of identical dimensions, which is 6,250 times faster. Sure, you shouldn't expect this drastic reduction in execution time in the real world, but the differences should still be quite significant.</p>
			<p>Next, let's take <a id="_idIndexMarker316"/>a look at Dask DataFrames. The syntax should, once again, feel very similar, so it shouldn't take you much time to learn the library. To fully demonstrate Dask's capabilities, we'll create some large datasets that won't be able to fit in the memory of a single laptop. To be more precise, we'll create 10 CSV files that are time series-based, each presenting data for a single year aggregated by seconds and measured through five different features. That's a lot, and it will definitely take some time to create, but you should end up with 10 datasets where each is around 1 GB in size. If you have a laptop with 8 GB of RAM like me, there's no way you could fit it in memory.</p>
			<p>The following code snippet creates these datasets:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">from datetime import datetime</p>
			<p class="source-code">for year in np.arange(2010, 2020):</p>
			<p class="source-code">    dates = pd.date_range(</p>
			<p class="source-code">        start=datetime(year=year, month=1, day=1),</p>
			<p class="source-code">        end=datetime(year=year, month=12, day=31),</p>
			<p class="source-code">        freq='S'</p>
			<p class="source-code">    )</p>
			<p class="source-code">    df = pd.DataFrame()</p>
			<p class="source-code">    df['Date'] = dates</p>
			<p class="source-code">    for i in range(5):</p>
			<p class="source-code">        df[f'X{i}'] = np.random.randint(low=0, high=100, size=len(df))</p>
			<p class="source-code">        </p>
			<p class="source-code">    df.to_csv(f'data/{year}.csv', index=False)</p>
			<p class="source-code">!ls data/</p>
			<p>Just make sure to have this <strong class="source-inline">/data</strong> folder where your Notebook is and you'll be good to go. Also, make sure <a id="_idIndexMarker317"/>you have 10 GB of disk space if you're following along. The last line, <strong class="source-inline">!ls data/</strong>, lists all the files located in the <strong class="source-inline">data</strong> folder. Here's what you should see:</p>
			<p class="source-code"><strong class="bold">2010.csv 2012.csv 2014.csv 2016.csv 2018.csv</strong></p>
			<p class="source-code"><strong class="bold">2011.csv 2013.csv 2015.csv 2017.csv 2019.csv</strong></p>
			<p>Now, let's take a look at how much time it takes pandas to read in a single CSV file and perform a simple aggregation. To be more precise, the dataset is grouped by month and the sum is extracted. The following code snippet demonstrates how to do this:</p>
			<p class="source-code">%%time</p>
			<p class="source-code">df = pd.read_csv('data/2010.csv', parse_dates=['Date'])</p>
			<p class="source-code">avg = df.groupby(by=df['Date'].dt.month).sum()</p>
			<p>The results are shown here:</p>
			<p class="source-code"><strong class="bold">CPU times: user 26.5 s, sys: 9.7 s, total: 36.2 s</strong></p>
			<p class="source-code"><strong class="bold">Wall time: 42 s</strong></p>
			<p>As you can see, it took pandas 42 seconds to perform this computation. Not too shabby, but what if you absolutely need to load in all of the datasets and perform computations? Let's explore that next.</p>
			<p>You can use the <strong class="source-inline">glob</strong> library to get paths to desired files in a specified folder. You can then read all of them individually, and use the <strong class="source-inline">concat()</strong> function from pandas to stack them together. The aggregation is performed in the same way:</p>
			<p class="source-code">%%time</p>
			<p class="source-code">import glob</p>
			<p class="source-code">all_files = glob.glob('data/*.csv')</p>
			<p class="source-code">dfs = []</p>
			<p class="source-code">for fname in all_files:</p>
			<p class="source-code">    dfs.append(pd.read_csv(fname, parse_dates=['Date']))</p>
			<p class="source-code">    </p>
			<p class="source-code">df = pd.concat(dfs, axis=0)</p>
			<p class="source-code">agg = df.groupby(by=df['Date'].dt.year).sum()</p>
			<p>There isn't much to say here – the Notebook simply breaks. Storing 10 GB+ of data into RAM isn't feasible for an 8 GB RAM machine. One way you could get around this would be to load data in chunks, but that's a headache of its own.</p>
			<p>What can <a id="_idIndexMarker318"/>Dask do to help? Let's learn how to load in these CSVs with Dask and perform the same aggregation. You can use the following snippet to do so:</p>
			<p class="source-code">%%time</p>
			<p class="source-code">import dask.dataframe as dd</p>
			<p class="source-code">df = dd.read_csv('data/*.csv', parse_dates=['Date'])</p>
			<p class="source-code">agg = df.groupby(by=df['Date'].dt.year).sum().compute()</p>
			<p>The results will once again surprise you:</p>
			<p class="source-code"><strong class="bold">CPU times: user 5min 3s, sys: 1min 11s, total: 6min 15s</strong></p>
			<p class="source-code"><strong class="bold">Wall time: 3min 41s</strong></p>
			<p>That's correct – in less than 4 minutes, Dask managed to read over 10 GB of data to an 8 GB RAM machine. That alone should make you reconsider NumPy and pandas, especially if you're dealing with large amounts of data or you expect to deal with it in the near future. </p>
			<p>Finally, there are Dask bags. They are <a id="_idIndexMarker319"/>used for storing and processing general Python data types that can't fit into memory – for example, log data. We won't explore this data structure, but it's nice to know it exists.</p>
			<p>On the other hand, we will explore the concept of parallel processing with Dask. You learned in the <a id="_idIndexMarker320"/>previous section that there are no valid reasons to process data or perform any other operation sequentially, as the input of one doesn't rely on the output of another.</p>
			<p><em class="italic">Dask delayed</em> allows for parallel execution. Sure, you can still rely only on the multiprocessing concepts we learned earlier, but why? It can be a tedious approach, and Dask has something better to offer. With Dask, there's no need to change the programming syntax, as was the case with pure Python. You just need to annotate a function you want to be parallelized with the <strong class="source-inline">@dask.delayed</strong> decorator and you're good to go!</p>
			<p>You can parallelize multiple functions and then place them inside a computational graph. That's what we'll do next.</p>
			<p>The following code snippet declares two functions:</p>
			<ul>
				<li><strong class="source-inline">cube()</strong>: Returns a cube of a number</li>
				<li><strong class="source-inline">multiply()</strong>: Multiplies all numbers in a list and returns the product</li>
			</ul>
			<p>Here are the library imports you'll need:</p>
			<p class="source-code">import time</p>
			<p class="source-code">import dask</p>
			<p class="source-code">import math</p>
			<p class="source-code">from dask import delayed, compute</p>
			<p>Let's run the first function on five numbers and call the second function on the results to see what happens. Note the call to <strong class="source-inline">time.sleep()</strong> inside the <strong class="source-inline">cube()</strong> function. This will make spotting differences between parallelized and non-parallelized functions that much easier:</p>
			<p class="source-code">%%time</p>
			<p class="source-code">def cube(number: int) -&gt; int:</p>
			<p class="source-code">    print(f'cube({number}) called!')</p>
			<p class="source-code">    time.sleep(1)</p>
			<p class="source-code">    return number ** 3</p>
			<p class="source-code">def multiply(items: list) -&gt; int:</p>
			<p class="source-code">    print(f'multiply([{items}]) called!')</p>
			<p class="source-code">    return math.prod(items)</p>
			<p class="source-code">numbers = [1, 2, 3, 4, 5]</p>
			<p class="source-code">graph = multiply([cube(num) for num in numbers])</p>
			<p class="source-code">print(f'Total = {graph}')</p>
			<p>This is your regular (sequential) data processing. There's nothing wrong with it, especially when <a id="_idIndexMarker321"/>there are so few and simple operations. The corresponding output is shown here:</p>
			<p class="source-code"><strong class="bold">cube(1) called!</strong></p>
			<p class="source-code"><strong class="bold">cube(2) called!</strong></p>
			<p class="source-code"><strong class="bold">cube(3) called!</strong></p>
			<p class="source-code"><strong class="bold">cube(4) called!</strong></p>
			<p class="source-code"><strong class="bold">cube(5) called!</strong></p>
			<p class="source-code"><strong class="bold">multiply([[1, 8, 27, 64, 125]]) called!</strong></p>
			<p class="source-code"><strong class="bold">Total = 1728000</strong></p>
			<p class="source-code"><strong class="bold">CPU times: user 8.04 ms, sys: 4 ms, total: 12 ms</strong></p>
			<p class="source-code"><strong class="bold">Wall time: 5.02 s</strong></p>
			<p>As expected, the code cell took around 5 seconds to run because of sequential execution. Now, let's see the modifications you have to make to parallelize these functions:</p>
			<p class="source-code">%%time</p>
			<p class="source-code">@delayed</p>
			<p class="source-code">def cube(number: int) -&gt; int:</p>
			<p class="source-code">    print(f'cube({number}) called!')</p>
			<p class="source-code">    time.sleep(1)</p>
			<p class="source-code">    return number ** 3</p>
			<p class="source-code">@delayed</p>
			<p class="source-code">def multiply(items: list) -&gt; int:</p>
			<p class="source-code">    print(f'multiply([{items}]) called!')</p>
			<p class="source-code">    return math.prod(items)</p>
			<p class="source-code">numbers = [1, 2, 3, 4, 5]</p>
			<p class="source-code">graph = multiply([cube(num) for num in numbers])</p>
			<p class="source-code">print(f'Total = {graph.compute()}')</p>
			<p>So, there's <a id="_idIndexMarker322"/>only the <strong class="source-inline">@delayed</strong> decorator and a call to <strong class="source-inline">compute()</strong> on the graph. The results are displayed here:</p>
			<p class="source-code"><strong class="bold">cube(3) called!cube(2) called!cube(4) called!</strong></p>
			<p class="source-code"><strong class="bold">cube(1) called!</strong></p>
			<p class="source-code"><strong class="bold">cube(5) called!</strong></p>
			<p class="source-code"><strong class="bold">multiply([[1, 8, 27, 64, 125]]) called!</strong></p>
			<p class="source-code"><strong class="bold">Total = 1728000</strong></p>
			<p class="source-code"><strong class="bold">CPU times: user 6.37 ms, sys: 5.4 ms, total: 11.8 ms</strong></p>
			<p class="source-code"><strong class="bold">Wall time: 1.01 s</strong></p>
			<p>As expected, the whole thing took just over a second to run because of the parallel execution. The previously declared computational graph comes with one more handy feature – it's easy to visualize. You'll need to have <em class="italic">GraphViz</em> installed on your machine and as a Python library. The procedure is different for every OS, so we won't go through it here. A quick Google <a id="_idIndexMarker323"/>search will tell you how to install it. Once you're done, you can execute the following line of code:</p>
			<p class="source-code">graph.visualize()</p>
			<p>The corresponding visualization is shown here:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="Images/B16954_05_001.jpg" alt="Figure 5.1 – Visualization of a Dask computational graph&#13;&#10;" width="473" height="433"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Visualization of a Dask computational graph</p>
			<p>As you can see from the graph, the <strong class="source-inline">cube()</strong> function is called five times in parallel, and its results are <a id="_idIndexMarker324"/>stored in the buckets above it. Then, the <strong class="source-inline">multiply()</strong> function is called <a id="_idIndexMarker325"/>with these values and stores the product in the top bucket.</p>
			<p>That's all you need <a id="_idIndexMarker326"/>to know about the basics of Dask. You've learned how to work with Dask arrays and DataFrames, and also how to use Dask to process operations in parallel. Not only that, but you've also learned the crucial role Dask plays in modern-day data science and machine learning. Dataset sizes often exceed available memory, so modern solutions are required.</p>
			<p>In the following section, you'll learn how to train TPOT automated machine learning models with Dask.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor069"/>Training machine learning models with TPOT and Dask</h1>
			<p>Optimizing machine learning pipelines is, before everything, a time-consuming process. We can <a id="_idIndexMarker327"/>shorten it potentially <a id="_idIndexMarker328"/>significantly by running things in parallel. Dask and TPOT work great when combined, and this section will teach you how to train TPOT models on a Dask cluster. Don't let the word "cluster" scare you, as your laptop or PC will be enough.</p>
			<p>You'll have to <a id="_idIndexMarker329"/>install one more library to continue, and it is called <strong class="source-inline">dask-ml</strong>. As its name suggests, it's used to perform machine learning with Dask. Execute the following from the Terminal to install it:</p>
			<p class="source-code"><strong class="bold">pipenv install dask-ml</strong></p>
			<p>Once that's done, you can open up Jupyter Lab or your favorite Python code editor and start coding. Let's get started:</p>
			<ol>
				<li>Let's start with library imports. We'll also make a dataset decision here. This time, we won't spend any time on data cleaning, preparation, or examination. The goal is to have a dataset ready as soon as possible. The <strong class="source-inline">load_digits()</strong> function from scikit-learn comes in handy because it is designed to fetch many 8x8 pixel digit images for classification.<p>As some of the libraries often fill up your screen with unnecessary warnings, we'll use the <strong class="source-inline">warnings</strong> library to ignore them. Refer to the following snippet for all the library imports:</p><p class="source-code">import tpot</p><p class="source-code">from tpot import TPOTClassifier</p><p class="source-code">from sklearn.datasets import load_digits</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from dask.distributed import Client</p><p class="source-code">import warnings</p><p class="source-code">warnings.filterwarnings('ignore')</p><p>The only new thing here is the <strong class="source-inline">Client</strong> class from <strong class="source-inline">dask.distributed</strong>. It is used to establish a connection with the Dask cluster (your computer, in this case). </p></li>
				<li>You'll <a id="_idIndexMarker330"/>now make <a id="_idIndexMarker331"/>an instance of the client. This will immediately start the Dask cluster and use all the CPU cores you have available. Here's the code for instance creation and checking where the cluster runs:<p class="source-code">client = Client()</p><p class="source-code">client</p><p>Once executed, you should see the following output:</p><div id="_idContainer106" class="IMG---Figure"><img src="Images/B16954_05_002.jpg" alt="Figure 5.2 – Information on the Dask cluster&#13;&#10;" width="712" height="166"/></div><p class="figure-caption">Figure 5.2 – Information on the Dask cluster</p><p>You can <a id="_idIndexMarker332"/>click on the dashboard link, and it will take you to http://127.0.0.1:8787/status. The following screenshot shows what the dashboard <a id="_idIndexMarker333"/>should look like when it's opened for the first time (no tasks running):</p><div id="_idContainer107" class="IMG---Figure"><img src="Images/B16954_05_003.jpg" alt="Figure 5.3 – Dask cluster dashboard (no running tasks)&#13;&#10;" width="1379" height="669"/></div><p class="figure-caption">Figure 5.3 – Dask cluster dashboard (no running tasks)</p><p>The dashboard will become much more colorful once you start training the models. We'll do the necessary preparation next.</p></li>
				<li>You can call the <strong class="source-inline">load_digits()</strong> function to get the image data and then use the <strong class="source-inline">train_test_split()</strong> function to split the images into subsets for training and testing. The train/test ratio is 50:50 for this example, as we don't want to spend too much time on the training. The ratio should be higher for the training set in almost any scenario, so make sure to remember that.<p>Once the split is done, you can call <strong class="source-inline">.shape</strong> on the subsets to check their dimensionality. Here's the entire code snippet:</p><p class="source-code">digits = load_digits()</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(</p><p class="source-code">    digits.data,</p><p class="source-code">    digits.target,</p><p class="source-code">    test_size=0.5,</p><p class="source-code">)</p><p class="source-code">X_train.shape, X_test.shape</p><p>The <a id="_idIndexMarker334"/>corresponding output is shown in the following figure:</p><div id="_idContainer108" class="IMG---Figure"><img src="Images/B16954_05_004.jpg" alt="Figure 5.4 – Dimensionality of training and testing subsets&#13;&#10;" width="358" height="44"/></div><p class="figure-caption">Figure 5.4 – Dimensionality of training and testing subsets</p><p>Next stop – model training.</p></li>
				<li>You <a id="_idIndexMarker335"/>now have everything needed to train models with TPOT and Dask. You can do so in a very similar fashion to what you did previously. The key parameter here is <strong class="source-inline">use_dask</strong>. You should set it to <strong class="source-inline">True</strong> if you want to use Dask for training. The other parameters are well known:<p class="source-code">estimator = TPOTClassifier(</p><p class="source-code">    n_jobs=-1,</p><p class="source-code">    random_state=42,</p><p class="source-code">    use_dask=True,</p><p class="source-code">    verbosity=2,</p><p class="source-code">    max_time_mins=10</p><p class="source-code">)</p><p>Now, you're ready to call the <strong class="source-inline">fit()</strong> function and train the model on the training subset. Here's a line of code for doing so:</p><p class="source-code">estimator.fit(X_train, y_train)</p><p>The <a id="_idIndexMarker336"/>appearance of the Dask dashboard will change immediately after you start training the <a id="_idIndexMarker337"/>model. Here's what it will look like a couple of minutes into the process:</p></li>
			</ol>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="Images/B16954_05_005.jpg" alt="Figure 5.5 – Dask dashboard during training&#13;&#10;" width="1380" height="701"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Dask dashboard during training</p>
			<p>After 10 minutes, TPOT will finish optimizing the pipeline, and you'll see the following output in your Notebook:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="Images/B16954_05_006.jpg" alt="Figure 5.6 – TPOT optimization outputs&#13;&#10;" width="1117" height="290"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – TPOT optimization outputs</p>
			<p>And that's all you need to do to combine TPOT and Dask. </p>
			<p>You now <a id="_idIndexMarker338"/>know how to <a id="_idIndexMarker339"/>train models on a Dask cluster, which is the recommended way of doing things for larger datasets and more challenging problems.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor070"/>Summary</h1>
			<p>This chapter was packed with information not only about TPOT and training models in a parallel manner, but also about parallelism in general. You've learned a lot – from how to parallelize basic functions that do nothing but sleep for a while, to parallelizing function with parameters and Dask fundamentals, to training machine learning models with TPOT and Dask on a Dask cluster.</p>
			<p>By now, you know how to solve regression and classification tasks in an automated manner, and how to parallelize the training process. The following chapter, <a href="B16954_06_Final_SK_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 6</em></a><em class="italic">, Getting Started with Deep Learning – Crash Course in Neural Networks</em>, will provide you with the required knowledge on neural networks. It will form a basis for <a href="B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086"><em class="italic">Chapter 7</em></a><em class="italic">, Neural Network Classifier with TPOT</em>, where we'll dive deep into training automated machine learning models with state-of-the-art neural network algorithms.</p>
			<p>As always, please feel free to practice solving both regression and classification tasks with TPOT, but this time, try to parallelize the process with Dask.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor071"/>Q&amp;A</h1>
			<ol>
				<li value="1">Define the term "parallelism."</li>
				<li>Explain which types of tasks can and can't be parallelized.</li>
				<li>List and explain three options for implementing parallelism in your applications (all are listed in this chapter).</li>
				<li>What is Dask and what makes it superior to NumPy and pandas for larger datasets?</li>
				<li>Name and explain three basic data structures that are implemented in Dask.</li>
				<li>What is a Dask cluster?</li>
				<li>What do you have to do to tell TPOT it should use Dask for training?</li>
			</ol>
		</div>
	</div></body></html>