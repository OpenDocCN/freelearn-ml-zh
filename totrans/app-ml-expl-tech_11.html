<html><head></head><body>
		<div id="_idContainer173">
			<h1 id="_idParaDest-166"><em class="italic"><a id="_idTextAnchor172"/>Chapter 9</em>: Other Popular XAI Frameworks</h1>
			<p>In the previous chapter, we covered the <strong class="bold">TCAV framework</strong> from <strong class="bold">Google AI</strong>, which is used for producing <em class="italic">human-friendly concept-based explanations</em>. We also discussed the other widely used explanation frameworks: <strong class="bold">LIME</strong> and <strong class="bold">SHAP</strong>. However, LIME, SHAP, and even TCAV have certain limitations, which we discussed in earlier chapters. None of these frameworks covers all the four dimensions of explainability for non-technical end-users. Due to these known drawbacks, the search for a robust <strong class="bold">Explainable AI</strong> (<strong class="bold">XAI</strong>) framework is still on.</p>
			<p>The journey toward finding a robust XAI framework and addressing the known limitations of the popular XAI modules has led to the discovery and development of many other robust frameworks trying to address different aspects of ML model explainability. In this chapter, we will cover these other popular XAI frameworks apart from LIME, SHAP and TCAV.</p>
			<p>More specifically, we will discuss about the important features, and key advantages of each of these frameworks. We will also explore how you can apply each framework in practice. Covering everything about each framework is beyond the scope of this chapter. But in this chapter, you will learn the most important features and practical application of these frameworks. In this chapter, we will cover the following list of widely used XAI frameworks:</p>
			<ul>
				<li>DALEX</li>
				<li>Explainerdashboard</li>
				<li>InterpretML</li>
				<li>ALIBI</li>
				<li>DiCE</li>
				<li>ELI5</li>
				<li>H2O AutoML explainer</li>
			</ul>
			<p>At the end of the chapter, I will also share a quick comparison guide comparing all these XAI frameworks to help you to decide on the framework depending on your problem. Now, let's begin!</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor173"/>Technical requirements </h1>
			<p>This code tutorial with necessary resources can be downloaded or cloned from the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09</a>. Like the other chapters, the Python and Jupyter notebooks are used to implement the practical application of the theoretical concepts covered in this chapter. But I will recommend you run the notebooks only after you go through this chapter for a better understanding. Most of the datasets used in the tutorials are also provided in the code repository: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets</a>.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor174"/>DALEX</h1>
			<p>In the <em class="italic">Dimensions of explainability</em> section of <a href="B18216_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Foundational Concepts of Explainability Techniques</em>, we discussed the four different dimensions of explainability – <em class="italic">data</em>, <em class="italic">model</em>, <em class="italic">outcome</em>, and <em class="italic">end user</em>. Most explainability frameworks such as LIME, SHAP, and TCAV provide model-centric explainability. </p>
			<p><strong class="bold">DALEX</strong> (<strong class="bold">moDel Agnostic Language for Exploration and eXplanation</strong>) is one of the very few widely used XAI frameworks that tries to address most of the dimensions of explainability. DALEX is <a id="_idIndexMarker602"/>model-agnostic and can provide some metadata about the underlying dataset to give some context to the explanation. This framework gives you insights into the model performance and model fairness, and it also provides global and local model explainability. </p>
			<p>The developers of the DALEX framework wanted to comply with the following list of requirements, which they have defined in order to explain complex black-box algorithms:</p>
			<ul>
				<li><strong class="bold">Prediction's justifications</strong>: According to the developers of DALEX, ML model users should be able to understand the variable or feature attributions of the final prediction.</li>
				<li><strong class="bold">Prediction's speculations</strong>: Hypothesizing the what-if scenarios or understanding the sensitivity of particular features of a dataset to the model outcome are other factors considered by the developers of DALEX.</li>
				<li><strong class="bold">Prediction's validations</strong>: For each predicted outcome of a model, the users should be able to verify the strength of the evidence that confirms a particular prediction of the model.</li>
			</ul>
			<p>DALEX is <a id="_idIndexMarker603"/>designed to comply with the preceding requirements using the various explanation methods provided by the framework. <em class="italic">Figure 9.1</em> illustrates the model exploration stack of DALEX:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B18216_09_001.jpg" alt="Figure 9.1 – The model exploration stack of DALEX&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – The model exploration stack of DALEX</p>
			<p>Next, I will <a id="_idIndexMarker604"/>walk you through an example of how to explore DALEX for explaining a black-box model in practice.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor175"/>Setting up DALEX for model explainability</h2>
			<p>In this section, you will learn to setup DALEX in Python. Before <a id="_idIndexMarker605"/>starting the code walk-through, I would ask you to check the notebook at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb</a>. It contains the steps needed to understand the concept that we are going to now discuss in depth. I also recommend that you take a look at the GitHub project repository of DALEX at <a href="https://github.com/ModelOriented/DALEX/tree/master/python/dalex">https://github.com/ModelOriented/DALEX/tree/master/python/dalex</a> in case you need additional details while executing the notebook. </p>
			<p>The DALEX Python framework can be installed using the <strong class="source-inline">pip</strong> installer: </p>
			<p class="source-code">pip install dalex -U</p>
			<p>If you want to use any additional features of DALEX that require an optional dependency, you can try the following command:</p>
			<p class="source-code">pip install dalex[full]</p>
			<p>You can validate the successful installation of the package by importing it into the Jupyter notebooks using the following command:</p>
			<p class="source-code">import dalex as dx</p>
			<p>Hopefully, your import should be successful; otherwise, if you get any errors, you will need to reinstall the framework or separately install its dependencies.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor176"/>Discussions about the dataset</h2>
			<p>Next, let's briefly about the dataset that is being used for this example. For this example, I have <a id="_idIndexMarker606"/>used the <em class="italic">FIFA Club Position Prediction dataset</em> (<a href="https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset">https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset</a>) to predict the valuation of a football player in Euros, based on their skill and abilities. So, this is a regression problem that can be solved by regression ML models.</p>
			<p class="callout-heading">FIFA Club Position dataset citation</p>
			<p class="callout">Bhattacharya A. (2022). Kaggle - FIFA Club Position Prediction dataset: <a href="https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset">https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset</a></p>
			<p>Similar to all other standard ML solution flows, we start with the data inspection process. The dataset can be loaded as a pandas DataFrame, and we can inspect the dimension of the dataset, the features that are present, and the data type of each feature. Additionally, we can perform any necessary data transformation steps such as dropping <a id="_idIndexMarker607"/>irrelevant features, checking for missing values, and data imputation to fill in missing values for relevant features. I recommend that you follow the necessary steps provided in the notebook, but feel free to include other additional steps and explore the dataset in more depth.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor177"/>Training the model</h2>
			<p>For this example, I have used a random forest regressor algorithm to fit a model after dividing <a id="_idIndexMarker608"/>the data into the training set and the validation set. This can be done using the following lines of code:</p>
			<pre class="source-code">x_train,x_valid,y_train,y_valid = train_test_split(</pre>
			<pre class="source-code">  df_train,labels,test_size=0.2,random_state=123)</pre>
			<pre class="source-code">model = RandomForestRegressor(</pre>
			<pre class="source-code">  n_estimators=790, min_samples_split = 3, </pre>
			<pre class="source-code">  random_state=123).fit(x_train, y_train)</pre>
			<p>We do minimum hyperparameter tuning to train the model as our objective is not to build a highly efficient model. Instead, our goal is to use this model as a black-box model and use DALEX to explain the model. So, let's proceed to the model explainability part using DALEX.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor178"/>Model explainability using DALEX</h2>
			<p>DALEX is <a id="_idIndexMarker609"/>model-agnostic as it does not assume anything about the model and can work with any algorithm. So, it considers the model as a black box. Before exploring how to use DALEX in Python, let's discuss the following key advantages of this framework:</p>
			<ul>
				<li><strong class="bold">DALEX provides a uniform abstraction over different prediction models</strong>: As an explainer, DALEX is quite robust and works well with different types of model frameworks such as scikit-learn, H2O, TensorFlow, and more. It can <a id="_idIndexMarker610"/>work with data provided in different formats such as a NumPy array or pandas DataFrame. It provides additional metadata about the data or the model, which makes it easier to develop an end-to-end model explainability pipeline in production.</li>
				<li><strong class="bold">DALEX has a robust API structure</strong>: The concise API structure of DALEX ensures that a consistent grammar and coding structure is used for model analysis. Using just a few lines of code, we can apply the various explainability methods and explain any black-box model.</li>
				<li><strong class="bold">It can provide local explanations for an inference data instance</strong>: Prediction-level explainability for a single inference data instance can be easily obtained during DALEX. There are different methods available in DALEX such as interactive breakdown plots, SHAP feature importance plots, and what-if analysis plots, which can be used for local explanations. We will cover these methods, in more detail, in the next section.</li>
				<li><strong class="bold">It can also provide global explanations while considering the entire dataset and the model</strong>: Model-level global explanations can also be provided using DALEX partial dependence plots, accumulated dependence plots, global variable importance plots, and more. </li>
				<li><strong class="bold">Bias and fairness checks can be easily done using DALEX</strong>: DALEX provides quantitative ways in which to measure model fairness and bias. Unlike DALEX, most of the XAI frameworks do not provide explicit methods to evaluate model fairness.</li>
				<li><strong class="bold">The DALEX ARENA platform can be used to build an interactive dashboard for better user engagement</strong>: DALEX can be used to create an interactive web app platform that can be used to design a custom dashboard to show interactive visualizations for the different model explainability methods that are available in DALEX. I think this unique feature of DALEX gives you the opportunity to create better user engagement by providing a tailor-made dashboard to meet the specific end user's needs. </li>
			</ul>
			<p>Considering all of these key benefits, let's now proceed with learning how to apply these features available in DALEX.</p>
			<p>First, we <a id="_idIndexMarker611"/>need to create a DALEX model explainer object, which takes the trained model, data, and model type as input. This can be done using the following lines of code:</p>
			<pre class="source-code"># Create DALEX Explainer object </pre>
			<pre class="source-code">explainer = dx.Explainer(model, </pre>
			<pre class="source-code">                         x_valid, y_valid, </pre>
			<pre class="source-code">                         model_type = 'regression',</pre>
			<pre class="source-code">                         label='Random Forest')</pre>
			<p>Once the explainer object has been created, it also provides additional metadata about the model, which is shown as follows.</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B18216_09_002.jpg" alt="Figure 9.2 – The DALEX explainer metadata&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – The DALEX explainer metadata</p>
			<p>This initial metadata is very useful for building automated pipelines for certain production-level <a id="_idIndexMarker612"/>systems. Next, let's explore some model-level explanations provided by DALEX.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor179"/>Model-level explanations</h2>
			<p>Model-level <a id="_idIndexMarker613"/>explanations are <a id="_idIndexMarker614"/>global explanations produced by DALEX. The consider model performance and the overall impact of all the features considered during prediction. The performance of a model can be checked using a single line of code:</p>
			<pre class="source-code">model_performance = explainer.model_performance("regression")</pre>
			<p>Depending upon the type of ML model, different model evaluation metrics can be applied. In this example, we are dealing with a regression problem, and hence, DALEX uses the metrics MSE, RMSE, R<span class="superscript">2</span>, MAE, and so on. For a classification problem, metrics such as accuracy, precision, recall, and more will be used. As covered in <a href="B18216_03_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, <em class="italic">Data-Centric Approaches</em>, by evaluating the model performance, we get to estimate the <em class="italic">data forecastability</em> of the model, which gives us an indication of the degree of correctness of the predicted outcome. </p>
			<p>DALEX provides methods such as global feature importance, <strong class="bold">partial dependence plots</strong> (<strong class="bold">PDPs</strong>), and accumulated dependency plots to analyze the feature-based explanations <a id="_idIndexMarker615"/>for model-level predictions. First, let's try out the variable of feature importance plots:</p>
			<pre class="source-code">Var_Importance = explainer.model_parts(</pre>
			<pre class="source-code">  variable_groups=variable_groups, B=15, random_state=123)</pre>
			<pre class="source-code">Var_Importance.plot(max_vars=10, </pre>
			<pre class="source-code">                    rounding_function=np.rint, </pre>
			<pre class="source-code">                    digits=None, </pre>
			<pre class="source-code">                    vertical_spacing=0.15,</pre>
			<pre class="source-code">                    title = 'Feature Importance')</pre>
			<p>This will produce the following plot:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B18216_09_003.jpg" alt="Figure 9.3 – A feature importance plot from DALEX for global feature-based explanations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – A feature importance plot from DALEX for global feature-based explanations</p>
			<p>In <em class="italic">Figure 9.3</em>, we can see that the trained model considers the abilities of the players, which comprise the overall rating of the player, the potential rating of the player, and other <a id="_idIndexMarker616"/>abilities such as pace, dribbling skill, strength, and stamina to be the most important factors for deciding the player's valuation. </p>
			<p>Similar to feature importance, we can generate PDPs. Accumulated dependency plots can also be generated using the following few lines of code:</p>
			<pre class="source-code">pdp = explainer.model_profile(type = 'partial', N=800)</pre>
			<pre class="source-code">pdp.plot(variables = ['age', 'potential'])</pre>
			<pre class="source-code">ald = explainer.model_profile(type = 'accumulated', N=800)</pre>
			<pre class="source-code">ald.plot(variables = ['age', 'movement_reactions'])</pre>
			<p>This will create the following plots for the aggregated profiles of the players:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B18216_09_004.jpg" alt="Figure 9.4 – A PDP aggregate profile plot for the age and potential features with &#13;&#10;predictions for model-level explanations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – A PDP aggregate profile plot for the age and potential features with predictions for model-level explanations</p>
			<p><em class="italic">Figure 9.4</em> shows how the overall features of <strong class="source-inline">age</strong> and <strong class="source-inline">potential</strong> vary with the predicted valuation of football players. From the plot, we can understand that with an increase <a id="_idIndexMarker617"/>in the player's age, the predicted valuation decreases. Similarly, with an increase in a player's potential rating, the player's valuation increases. All of these observations are also quite consistent with the real-world observation for deciding a player's valuation. Next, let's see how to obtain a prediction-level explanation using DALEX.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor180"/>Prediction-level explanations</h2>
			<p>DALEX can provide model-agnostic local or prediction-level explanations along with a global <a id="_idIndexMarker618"/>explanation. It uses techniques such as interactive breakdown profiles, SHAP feature importance values, and Ceteris Paribus profiles (what-if profiles) to explain model predictions at the individual data instance level. To <a id="_idIndexMarker619"/>understand the practical importance of these techniques, let's use these techniques for our use case to explain an ML model trained to predict the overall valuation of a football player. For our example, we will compare the prediction-level explanations of three players – Cristiano Ronaldo, Lionel Messi, and Jadon Sancho. </p>
			<p>First, let's try out interactive breakdown plots. This can be done using the following lines of code:</p>
			<pre class="source-code">prediction_level = {'interactive_breakdown':[], 'shap':[]}</pre>
			<pre class="source-code">ibd = explainer.predict_parts(</pre>
			<pre class="source-code">  player, type='break_down_interactions', label=name)</pre>
			<pre class="source-code">prediction_level['interactive_breakdown'].append(ibd)</pre>
			<pre class="source-code">prediction_level['interactive_breakdown'][0].plot(</pre>
			<pre class="source-code">  prediction_level['interactive_breakdown'][1:3],</pre>
			<pre class="source-code">  rounding_function=lambda x, </pre>
			<pre class="source-code">  digits: np.rint(x, digits).astype(np.int),</pre>
			<pre class="source-code">  digits=None, </pre>
			<pre class="source-code">  max_vars=15)</pre>
			<p>This will <a id="_idIndexMarker620"/>generate the following interactive breakdown profile plot for each player:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B18216_09_005.jpg" alt="Figure 9.5 – An interactive breakdown plot from DALEX&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – An interactive breakdown plot from DALEX</p>
			<p><em class="italic">Figure 9.5</em> shows the interactive breakdown plot comparing the model predictions for the three selected players. This plot illustrates the contribution of each feature to the final predicted value. The feature values that increase the model prediction value are shown in a different color than the features that decrease the prediction value. This plot shows the breakdown of the total predicted value with respect to each feature value of the data instance.</p>
			<p>Now, all three players are world-class professional football players; however, Ronaldo and Messi are veteran players and living legends of the game, as compared to Sancho, who is a young <a id="_idIndexMarker621"/>talent. So, if you observe the plot, it shows that for Ronaldo and Messi, the <strong class="source-inline">age</strong> feature reduces the predicted value, while for Sancho, it slightly increases. It is quite interesting to observe that the model has been able to learn how increasing the age of football players can reduce their valuation. This observation is also consistent with the observation of domain experts who value younger players with higher potential to have a higher market value. Similar to breakdown plots, DALEX also provides SHAP feature importance plots to analyze the contribution of the features. This method gives similar information such as breakdown plots, but the feature importance is calculated based on SHAP values. It can be obtained using the following lines of code:</p>
			<pre class="source-code">sh = explainer.predict_parts(player, type='shap', B=10, </pre>
			<pre class="source-code">                             label=name)</pre>
			<pre class="source-code">prediction_level['shap'].append(sh)</pre>
			<pre class="source-code">prediction_level['shap'][0].plot(</pre>
			<pre class="source-code">  prediction_level['shap'][1:3],</pre>
			<pre class="source-code">  rounding_function=lambda x, </pre>
			<pre class="source-code">  digits: np.rint(x, digits).astype(np.int),</pre>
			<pre class="source-code">  digits=None, </pre>
			<pre class="source-code">  max_vars=15)</pre>
			<p>Next, we will use <em class="italic">What-If</em> plots based on the <strong class="bold">Ceteris Paribus profile</strong> in DALEX. The Ceteris Paribus profile is similar to <strong class="bold">Sensitivity Analysis</strong>, which was covered in <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>. It is based on the <em class="italic">Ceteris Paribus principle</em>, which means that when everything else remains unchanged, we can determine how a change in a particular feature will affect the model prediction. This process is <a id="_idIndexMarker622"/>often referred to as <strong class="bold">What-If model analysis</strong> or <strong class="bold">Individual Conditional Expectations</strong>. In terms of application, in our example, we can use this <a id="_idIndexMarker623"/>method to find out how the predicted valuation of Jadon Sancho <a id="_idIndexMarker624"/>might vary as he grows older or if his overall potential increases. We can find this out by using the following lines of code:</p>
			<pre class="source-code">ceteris_paribus_profile = explainer.predict_profile(</pre>
			<pre class="source-code">    player, </pre>
			<pre class="source-code">    variables=['age', 'potential'],</pre>
			<pre class="source-code">    label=name) # variables to calculate </pre>
			<pre class="source-code">ceteris_paribus_profile.plot(size=3, </pre>
			<pre class="source-code">                             title= f"What If? {name}")</pre>
			<p>This will produce the following interactive what-if plot in DALEX:</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B18216_09_006.jpg" alt="Figure 9.6 – An interactive what-if plot in DALEX&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – An interactive what-if plot in DALEX</p>
			<p><em class="italic">Figure 9.6</em> shows that for Jadon Sancho, the market valuation will start decreasing as he grows older; however, it can also increase with an increase in overall potential ratings. I would strongly recommend that you explore all of these prediction-level explanation <a id="_idIndexMarker625"/>options for the features and from the tutorial notebook provided in the project repository: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DALEX_example.ipynb</a>. Next, we will use DALEX to evaluate model fairness.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor181"/>Evaluating model fairness</h2>
			<p>A <strong class="bold">Model Fairness</strong> check is another <a id="_idIndexMarker626"/>important feature of DALEX. Although, model fairness <a id="_idIndexMarker627"/>and bias detection are more important to consider for classification problems relying on features related to gender, race, ethnicity, nationality, and other similar demographic features. However, we will apply this to regression models, too. For more details regarding model fairness checks using DALEX, please <a id="_idIndexMarker628"/>refer to <a href="https://dalex.drwhy.ai/python-dalex-fairness.html">https://dalex.drwhy.ai/python-dalex-fairness.html</a>. Now, let's see whether our model is free from any bias and is fair! </p>
			<p>We will create a <strong class="bold">protected variable</strong> and <strong class="bold">privileged variable</strong> for the fairness check. In fairness checks in ML, we try to ensure that a protected variable is free from any bias. If we <a id="_idIndexMarker629"/>anticipate any feature value or group to have any bias due to factors such as an imbalanced dataset, we can declare them as privileged variables. For our use case, we will perform a fairness check for three different sets of players based on their age. </p>
			<p>All players less than 20 years are considered to be <em class="italic">youth</em> players, players between the ages of 20 and 30 are considered to be <em class="italic">developing</em> players, and players above 30 years are considered to be <em class="italic">developed</em> players. Now, let's do our fairness check using DALEX:</p>
			<pre class="source-code">protected = np.where(x_valid.age &lt; 30, np.where(x_valid.age &lt; 20, 'youth', 'developing'), 'developed')</pre>
			<pre class="source-code">privileged = 'youth'</pre>
			<pre class="source-code">fairness = explainer.model_fairness(protected=protected,</pre>
			<pre class="source-code">                                    privileged=privileged)</pre>
			<pre class="source-code">fairness.fairness_check(epsilon = 0.7)</pre>
			<p>This is the outcome of the fairness checks:</p>
			<p class="source-code">No bias was detected! Conclusion: your model is fair in terms of checked fairness criteria.</p>
			<p>We can also check the quantitative evidence of the fairness checks and plot them to analyze further:</p>
			<pre class="source-code">fairness.result</pre>
			<pre class="source-code">fairness.plot()</pre>
			<p>This will generate the following plot for analyzing model fairness checks using DALEX:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B18216_09_007.jpg" alt="Figure 9.7 – A model fairness plot using DALEX&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – A model fairness plot using DALEX</p>
			<p>As shown in <em class="italic">Figure 9.7</em>, the model fairness using DALEX for regression models is done with respect to the metrics of independence, separation, and sufficiency. For classification <a id="_idIndexMarker630"/>models, these metrics could vary, but the API function usage is the same. Next, we will discuss the ARENA web-based tool for building interactive dashboards using DALEX.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor182"/>Interactive dashboards using ARENA</h2>
			<p>Another interesting feature of DALEX is the ARENA dashboard platform to create an interactive <a id="_idIndexMarker631"/>web app that can be used <a id="_idIndexMarker632"/>to design a custom dashboard for keeping all the DALEX interactive visualizations that were obtained using different model explainability methods. This particular feature of DALEX gives us an opportunity to create better user engagement by creating a custom dashboard for a specific problem.</p>
			<p>Before starting, we need to create a DALEX Arena dataset:</p>
			<pre class="source-code">arena_dataset = df_test[:400].set_index('short_name')</pre>
			<p>Next, we need to create an <strong class="source-inline">Arena</strong> object and push the DALEX explainer object created from the black-box model that is being explained:</p>
			<pre class="source-code">arena = dx.Arena()</pre>
			<pre class="source-code"># push DALEX explainer object</pre>
			<pre class="source-code">arena.push_model(explainer)</pre>
			<p>Following <a id="_idIndexMarker633"/>this, we just need to push the Arena dataset <a id="_idIndexMarker634"/>and start the server to make our Arena platform live:</p>
			<pre class="source-code"># push whole test dataset (including target column)</pre>
			<pre class="source-code">arena.push_observations(arena_dataset)</pre>
			<pre class="source-code"># run server on port 9294</pre>
			<pre class="source-code">arena.run_server(port=9294)</pre>
			<p>Based on the port provided, the DALEX server will be running on <a href="https://arena.drwhy.ai/?data=http://127.0.0.1:9294/">https://arena.drwhy.ai/?data=http://127.0.0.1:9294/</a>. Initially, you will get a blank dashboard, but you can easily drag and drop the visuals from the right-hand side panel to make your own custom dashboard, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B18216_09_008.jpg" alt="Figure 9.8 – An interactive Arena dashboard created using DALEX&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – An interactive Arena dashboard created using DALEX</p>
			<p>Also, you can load an existing dashboard from a configuration JSON or export a build dashboard <a id="_idIndexMarker635"/>as a configuration JSON file. Try <a id="_idIndexMarker636"/>recreating the dashboard, as shown in <em class="italic">Figure 9.8</em>, using the configuration JSON file provided in the code repository at <a href="https://raw.githubusercontent.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/main/Chapter09/dalex_sessions/session-1647894542387.json">https://raw.githubusercontent.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/main/Chapter09/dalex_sessions/session-1647894542387.json</a>. </p>
			<p>Overall, I have found DALEX to be a very interesting and powerful XAI framework. There are many more examples available at <a href="https://github.com/ModelOriented/DALEX">https://github.com/ModelOriented/DALEX</a> and <a href="https://github.com/ModelOriented/DrWhy/blob/master/README.md">https://github.com/ModelOriented/DrWhy/blob/master/README.md</a>. Please do explore all of them. However, DALEX seems to be restricted to structured data. I think as a future scope, making DALEX easily applicable with image and text data would increase its adoption across the AI research community. In the next section, we will explore Explainerdashboard, which is another interesting XAI framework.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor183"/>Explainerdashboard</h1>
			<p>The AI research community has always considered interactive visualization to be an important approach for interpreting ML model predictions. In this section, we will cover <strong class="bold">Explainerdashboard</strong>, which is an interesting Python framework that can spin up a comprehensive interactive <a id="_idIndexMarker637"/>dashboard covering various aspects of model explainability with just minimal lines of code. Although this framework supports only scikit-learn-compatible models (including XGBoost, CatBoost, and LightGBM), it can provide model-agnostic global and local explainability. Currently, it supports SHAP-based feature importance and interactions, PDPs, model performance analysis, what-if model analysis, and even decision-tree-based breakdown analysis plots. </p>
			<p>The framework allows customization of the dashboard, but I think the default version includes all supported aspects of model explainability. The generated web-app-based dashboards <a id="_idIndexMarker638"/>can be exported as static web pages directly from a live dashboard. Otherwise, the dashboards can be programmatically deployed as a web app through an automated <strong class="bold">Continuous Integration</strong> (<strong class="bold">CI</strong>)/<strong class="bold">Continuous Deployment</strong> (<strong class="bold">CD</strong>) deployment process. I recommend that you <a id="_idIndexMarker639"/>go through the official documentation of the framework (<a href="https://explainerdashboard.readthedocs.io/en/latest/">https://explainerdashboard.readthedocs.io/en/latest/</a>) and the GitHub project repository (<a href="https://github.com/oegedijk/explainerdashboard">https://github.com/oegedijk/explainerdashboard</a>) before we get started with the walk-through tutorial example next.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor184"/>Setting up Explainerdashboard</h2>
			<p>The complete tutorial notebook is provided in the code repository for this chapter at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb</a>. However, in this section, I will provide a complete walk-through of the tutorial. The same <em class="italic">FIFA Club Position Prediction dataset</em> (<a href="https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset">https://www.kaggle.com/datasets/adityabhattacharya/fifa-club-position-prediction-dataset</a>) will be used for this tutorial, too. But instead of using the dataset to predict the valuation of football players, here, I will use this dataset to predict the league position of the football club for the next season based on the skills and ability of the football players playing for the club. </p>
			<p>The real-world task of predicting a club league position for a future season is more complex, and there are several other variables that need to be included to get an accurate prediction. However, this prediction problem is solely based on the quality of the players playing for the club. </p>
			<p>To get started with the tutorial, you will need to install all of the required dependencies to run the notebook. If you have executed all the previous tutorial examples, then most of the Python modules <a id="_idIndexMarker640"/>should be installed, except for <strong class="bold">Explainerdashboard</strong>. You can install <strong class="source-inline">explainerdashboard</strong> using the <strong class="source-inline">pip</strong> installer:</p>
			<pre class="source-code">!pip install explainerdashboard</pre>
			<p>The Explainerdashboard framework does have a dependency on the <strong class="source-inline">graphviz</strong> module, which makes it slightly tedious to install depending on your system. At the time of writing, I have discovered that version 0.18 works best with Explainerdashboard. This can be installed using the pip installer:</p>
			<pre class="source-code">!pip install graphviz==0.18</pre>
			<p><strong class="bold">Graphviz</strong> is an open source graph visualization software that is needed for the decision tree breakdown plot used in Explainerdashboard. In spite of the pip installer, you might also need to install the <strong class="source-inline">graphviz</strong> binaries depending on the operating system that you are using. Please visit <a href="https://graphviz.org/">https://graphviz.org/</a> to find out more. Additionally, if you are facing any friction <a id="_idIndexMarker641"/>during the setup of this module, take a look at the installation instructions provided at <a href="https://pypi.org/project/graphviz/">https://pypi.org/project/graphviz/</a>.</p>
			<p>We will <a id="_idIndexMarker642"/>consider this ML problem to be a regression problem. Therefore, similar to the DALEX example, we will need to perform the same data preprocessing, feature engineering, model training, and evaluation steps. I recommend that you follow the steps provided in the notebook at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/Explainer_dashboard_example.ipynb</a>. This contains the necessary details to get the trained model. We will use this trained model as a black box and use Explainerdashboard to explain it in the next section.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor185"/>Model explainability with Explainerdashboard</h2>
			<p>After the <a id="_idIndexMarker643"/>installation of the Explainerdashboard Python module is successful, you can import it to verify the installation:</p>
			<pre class="source-code">import explainerdashboard</pre>
			<p>For this example, we will use the <strong class="source-inline">RegressionExplainer</strong> and <strong class="source-inline">ExplainerDashboard</strong> submodules. So, we will load the specific submodules:</p>
			<pre class="source-code">from explainerdashboard import RegressionExplainer, ExplainerDashboard</pre>
			<p>Next, using just two lines of code, we can spin up the <strong class="source-inline">ExplainerDashboard</strong> submodule for this example:</p>
			<pre class="source-code">explainer = RegressionExplainer(model_skl, x_valid, y_valid)</pre>
			<pre class="source-code">ExplainerDashboard(explainer).run()</pre>
			<p>Once this step is running successfully, the dashboard should be running in <strong class="source-inline">localhost</strong> with port <strong class="source-inline">8050</strong> as the default port. So, you can visit <strong class="source-inline">http://localhost:8050/</strong> in the browser to view your explainer dashboard. </p>
			<p>The following <a id="_idIndexMarker644"/>lists the different explainability methods provided by Explainerdashboards:</p>
			<ul>
				<li><strong class="bold">Feature importance</strong>: Similar to other XAI frameworks, feature importance is an important method for gaining an understanding of the overall contribution of each attribute used for prediction. This framework uses SHAP values, permutation importance, and PDPs to analyze the contribution of each feature for the model prediction:</li>
			</ul>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B18216_09_009.jpg" alt="Figure 9.9 – Contribution plots and PDPs from Explainerdashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – Contribution plots and PDPs from Explainerdashboard</p>
			<ul>
				<li><strong class="bold">Model performance</strong>: Similar to DALEX, Explainerdashboard also allows you to analyze the model performance. For classification models, it uses metrics such as precision <a id="_idIndexMarker645"/>plots, confusion matrices, ROC-AUC plots, PR AUC plots, and more. For regression models, we will see plots such as residual plots, goodness-of-fit plots, and more:</li>
			</ul>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B18216_09_010.jpg" alt="Figure 9.10 – Model performance analysis plots for regression models in Explainerdashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10 – Model performance analysis plots for regression models in Explainerdashboard</p>
			<ul>
				<li><strong class="bold">Prediction-level analysis</strong>: Explainerdashboard provides interesting and interactive plots for getting local explanations. This is quite similar to other Python frameworks. It is very important to have for analyzing prediction-level outcomes. </li>
				<li><strong class="bold">What-if analysis</strong>: Another interesting option that Explainerdashboard provides is the what-if analysis feature. We can use this feature to vary the feature values <a id="_idIndexMarker646"/>and observe how the overall prediction gets changed. I find what-if analysis to be very useful for providing prescriptive insights:</li>
			</ul>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B18216_09_011.jpg" alt="Figure 9.11 – What-if model analysis using Explainerdashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11 – What-if model analysis using Explainerdashboard</p>
			<ul>
				<li><strong class="bold">Feature dependence and interactions</strong>: Analyzing the dependency and interactions between different features is another interesting explainability method provided in Explainerdashboard. Mostly, it uses SHAP methods for analyzing feature dependence and interactions.</li>
				<li><strong class="bold">Decision tree surrogate explainers</strong>: Explainerdashboard uses decision trees as <a id="_idIndexMarker647"/>surrogate explainers. Additionally, it uses the decision tree breakdown plot for model explainability:</li>
			</ul>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B18216_09_012.jpg" alt="Figure 9.12 – Decision tree surrogate explainers in Explainerdashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.12 – Decision tree surrogate explainers in Explainerdashboard</p>
			<p>To stop running the dashboards on your local system, you can simply interrupt the notebook cell. </p>
			<p>Explainerdashboard offers you many customization options as well. To customize your own dashboard from the given template, it is recommended that you refer to <a href="https://github.com/oegedijk/explainerdashboard#customizing-your-dashboard">https://github.com/oegedijk/explainerdashboard#customizing-your-dashboard</a>. You can also build multiple dashboards and compile all the dashboards as an explainer hub: <a href="https://github.com/oegedijk/explainerdashboard#explainerhub">https://github.com/oegedijk/explainerdashboard#explainerhub</a>. To deploy dashboards into a live web app that is accessible from anywhere, I would recommend you look at <a href="https://github.com/oegedijk/explainerdashboard#deployment">https://github.com/oegedijk/explainerdashboard#deployment</a>. </p>
			<p>In comparison to DALEX, I would say that Explainerdashboard is slightly behind as it is only restricted to scikit-learn-compatible models. This means that with complex deep learning models <a id="_idIndexMarker648"/>built on unstructured data such as images and text, you can't use this framework. However, I found it easy to use and very useful for ML models built on tabular datasets. In the next section, we will cover the InterpretML XAI framework from Microsoft.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor186"/>InterpretML</h1>
			<p>InterpretML (https://interpret.ml/) is an XAI toolkit <a id="_idIndexMarker649"/>from Microsoft. It <a id="_idIndexMarker650"/>aims to provide a comprehensive understanding of ML models for the purpose of model debugging, outcome explainability, and regulatory audits of ML models. With this Python module, we can either train <em class="italic">interpretable glassbox models</em> or <em class="italic">explain black-box models</em>. </p>
			<p>In <a href="B18216_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Foundational Concepts of Explainability Techniques</em>, we discovered that some models such as decision trees, linear models, or rule-fit algorithms are inherently explainable. However, these models are not efficient for complex datasets. Usually, these models are termed glass-box models as opposed to black-box models, as they are extremely transparent. </p>
			<p>Microsoft Research developed another algorithm called <strong class="bold">Explainable Boosting Machine</strong> (<strong class="bold">EBM</strong>), which introduces <a id="_idIndexMarker651"/>modern ML techniques such as boosting, bagging, and automatic interaction detection into classical algorithms such as <strong class="bold">Generalized Additive Models</strong> (<strong class="bold">GAMs</strong>). Researchers have also <a id="_idIndexMarker652"/>found that EBMs are accurate as random forests and gradient-boosted trees, but unlike such black-box models, EBMs are explainable and transparent. Therefore, EBMs are glass-box models that are built into the InterpretML framework.</p>
			<p>In comparison to DALEX and Explainerdashboard, InterpretML is slightly behind in terms of both usability and adoption. However, since this framework as a great potential to evolve further, it is important to discuss this framework. Before discussing the code tutorial, let us discuss about the explainability techniques that are supported by this framework.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor187"/>Supported explanation methods</h2>
			<p>At the time <a id="_idIndexMarker653"/>of writing, the following table illustrates the supported explanation methods in InterpretML, as mentioned in the GitHub project source at <a href="https://github.com/interpretml/interpret#supported-techniques">https://github.com/interpretml/interpret#supported-techniques</a>:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B18216_09_013.jpg" alt="Figure 9.13 – Explanation methods supported in InterpretML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.13 – Explanation methods supported in InterpretML</p>
			<p>I recommend that <a id="_idIndexMarker654"/>you keep an eye on the project documentation, as I am quite certain the supported explanation methods for this framework will increase for InterpretML. Next, let's explore how to use this framework in practice.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor188"/>Setting up InterpretML</h2>
			<p>In this section, I will walk you through the tutorial example of InterpretML that is provided <a id="_idIndexMarker655"/>in the code repository at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/InterpretML_example.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/InterpretML_example.ipynb</a>. In the tutorial, we used InterpretML to explain an ML model trained for hepatitis detection, which is a classification problem. </p>
			<p>To begin the problem, you need to have the InterpretML Python module installed. You can use the pip installer for this:</p>
			<pre class="source-code">pip install interpret</pre>
			<p>Although the framework is supported by Windows, Mac, and Linux, it does require you to have a Python version that is higher than 3.6. You can validate whether the installation is successful by importing the module:</p>
			<pre class="source-code">import interpret as iml</pre>
			<p>Next, let's discuss the dataset that is used in this tutorial.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor189"/>Discussions about the dataset</h2>
			<p>The hepatitis detection <a id="_idIndexMarker656"/>dataset is taken from the UCI Machine Learning repository at https://archive.ics.uci.edu/ml/datasets/hepatitis. It has 155 records and 20 features of different types <a id="_idIndexMarker657"/>for the detection of the hepatitis disease. Therefore, this dataset is used for solving binary classification problems. For your convenience, I have added this dataset to the code repository at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/Hepatitis_Data">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/Hepatitis_Data</a>. </p>
			<p class="callout-heading">Hepatitis Dataset Citation</p>
			<p class="callout"><em class="italic">G.Gong (Carnegie-Mellon University) via Bojan Cestnik, Jozef Stefan Institute (</em><a href="https://archive.ics.uci.edu/ml/datasets/hepatitis">https://archive.ics.uci.edu/ml/datasets/hepatitis</a><em class="italic">)</em></p>
			<p>More details about the dataset and initial exploration results are included in the tutorial notebook. However, on a very high level, <em class="italic">the dataset is imbalanced</em>, it has <em class="italic">missing values</em>, and it has both <em class="italic">categorical</em> and <em class="italic">continuous</em> variables. Therefore, it needs necessary transformation before the model can be built. All these necessary steps are included in the tutorial notebook, but please feel free to explore additional methods for building a better ML model.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor190"/>Training the model</h2>
			<p>For this <a id="_idIndexMarker658"/>example, after dividing the entire data into a training set and a test set, I have trained a random forest classifier with minimum hyperparameter tuning:</p>
			<pre class="source-code">x_train, x_test, y_train, y_test = train_test_split(</pre>
			<pre class="source-code">  encoded, label, test_size=0.3, random_state=123)</pre>
			<pre class="source-code">model = RandomForestClassifier(</pre>
			<pre class="source-code">  n_estimators=500, min_samples_split = 3,</pre>
			<pre class="source-code">  random_state=123).fit(x_train, y_train)</pre>
			<p>Note that sufficient hyperparameter tuning is not done for this model, as we are more interested in the model explainability part with InterpretML rather than learning how to build an efficient ML model. However, I encourage you to explore hyperparameters tuning further to get a better model.</p>
			<p>On evaluating <a id="_idIndexMarker659"/>the model on the test data, we have received an accuracy of 85% and an <strong class="bold">Area Under the ROC Curve</strong> (<strong class="bold">AUC</strong>) score of 70%. The AUC score is much lower than the accuracy as the dataset used is imbalanced. This indicates that a metric such as accuracy can be <a id="_idIndexMarker660"/>misleading. Therefore, it is better to consider metrics such as the AUC score, F1 score, and confusion matrix instead of accuracy for model evaluation.</p>
			<p>Next, we will use InterpretML for model explainability.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor191"/>Explainability with InterpretML</h2>
			<p>As mentioned earlier, with InterpretML. you can either use interpretable glass-box models as <a id="_idIndexMarker661"/>surrogate explainers or explore certain <a id="_idIndexMarker662"/>model-agnostic methods to explain black-box models. With both approaches, you can get an interactive dashboard for analyzing the various aspects of explainability. First, I will cover the model explainability using glass-box models in InterpretML.</p>
			<h3>Explaining with glass-box models using InterpretML</h3>
			<p>InterpretML supports <a id="_idIndexMarker663"/>interpretable glass-box models <a id="_idIndexMarker664"/>such as the <strong class="bold">Explainable Boosting Machine</strong> (<strong class="bold">EBM</strong>), <strong class="bold">Decision Tree</strong>, and <strong class="bold">Rule-Fit</strong> algorithms. These algorithms are applied as surrogate explainers for providing post hoc model explainability. First, let's try out the EBM algorithm.</p>
			<h4>EBM</h4>
			<p>To explain a model with EBM, we need to load the required submodule in Python:</p>
			<pre class="source-code">from interpret.glassbox import ExplainableBoostingClassifier</pre>
			<p>Once the EBM submodule has been successfully imported, we just need to create a trained surrogate explainer object:</p>
			<pre class="source-code">ebm = ExplainableBoostingClassifier(feature_types=feature_types)</pre>
			<pre class="source-code">ebm.fit(x_train, y_train)</pre>
			<p>The <strong class="source-inline">ebm</strong> variable is the EBM explainer object. We can use this variable to get global or local explanations. The framework <a id="_idIndexMarker665"/>only supports feature importance-based global and local explainability but creates an interactive plot for further analysis:</p>
			<pre class="source-code"># Showing Global Explanations</pre>
			<pre class="source-code">ebm_global = ebm.explain_global()</pre>
			<pre class="source-code">iml.show(ebm_global)</pre>
			<pre class="source-code"># Local explanation using EBM</pre>
			<pre class="source-code">ebm_local = ebm.explain_local(x_test[5:6], y_test[5:6], </pre>
			<pre class="source-code">                              name = 'Local Explanation')</pre>
			<pre class="source-code">iml.show(ebm_local)</pre>
			<p><em class="italic">Figure 9.14</em> illustrates the global feature importance plot and variation of the <em class="italic">Age</em> feature with the overall data distribution obtained using InterpretML:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B18216_09_014.jpg" alt="Figure 9.14 – Global explanation plots using InterpretML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.14 – Global explanation plots using InterpretML</p>
			<p>Feature importance for the local explanation, which has been done at the prediction level of the individual data instance, is shown in <em class="italic">Figure 9.15</em>: </p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B18216_09_015.jpg" alt="Figure 9.15 – Local explanation using InterpretML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.15 – Local explanation using InterpretML</p>
			<p>Next, we will explore rule-based <a id="_idIndexMarker666"/>algorithms in InterpretML as <em class="italic">surrogate explainers</em>, as discussed in <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>.</p>
			<h4>Decision rule list</h4>
			<p>Similar to EBM, another popular glass-box surrogate explainer that is available in InterpretML <a id="_idIndexMarker667"/>is the decision <a id="_idIndexMarker668"/>rule list. This is similar to the rule-fit algorithm, which can learn specific rules from the dataset to explain the logical working of the model. We can apply this method using InterpretML in the following way:</p>
			<pre class="source-code">from interpret.glassbox import DecisionListClassifier</pre>
			<pre class="source-code">dlc = DecisionListClassifier(feature_types=feature_types)</pre>
			<pre class="source-code">dlc.fit(x_train, y_train)</pre>
			<pre class="source-code"># Showing Global Explanations</pre>
			<pre class="source-code">dlc_global = dlc.explain_global()</pre>
			<pre class="source-code">iml.show(dlc_global)</pre>
			<p>With this <a id="_idIndexMarker669"/>method, the framework <a id="_idIndexMarker670"/>displays the learned rules, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/B18216_09_016.jpg" alt="Figure 9.16 – Decision rule list using InterpretML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.16 – Decision rule list using InterpretML</p>
			<p>As we can see in <em class="italic">Figure 9.16</em>, it generates a list of learned rules. Next, we will explore the decision tree-based surrogate explainer in InterpretML.</p>
			<h4>Decision tree </h4>
			<p>Similar to <a id="_idIndexMarker671"/>a decision rule list, we can <a id="_idIndexMarker672"/>also fit the decision tree algorithm as a surrogate explainer using InterpretML for model explainability. The API syntax is also quite similar for applying a decision tree classifier:</p>
			<pre class="source-code">from interpret.glassbox import ClassificationTree</pre>
			<pre class="source-code">dtc = ClassificationTree(feature_types=feature_types)</pre>
			<pre class="source-code">dtc.fit(x_train, y_train)</pre>
			<pre class="source-code"># Showing Global Explanations</pre>
			<pre class="source-code">dtc_global = dtc.explain_global()</pre>
			<pre class="source-code">iml.show(dtc_global)</pre>
			<p>This produces a decision tree breakdown plot, as shown in the following screenshot, for the model explanation:</p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/B18216_09_017.jpg" alt="Figure 9.17 – A decision tree-based surrogate explainer in InterpretML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.17 – A decision tree-based surrogate explainer in InterpretML</p>
			<p>Now all <a id="_idIndexMarker673"/>of these individual <a id="_idIndexMarker674"/>components can also be clubbed together into one single dashboard using the following line of code:</p>
			<pre class="source-code">iml.show([ebm_global, ebm_local, dlc_global, dtc_global])</pre>
			<p><em class="italic">Figure 9.18</em> illustrates the consolidated interactive dashboard using InterpretML:</p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/B18216_09_018.jpg" alt="Figure 9.18 – The InterpretML dashboard consolidating all individual plots&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.18 – The InterpretML dashboard consolidating all individual plots</p>
			<p>In the next <a id="_idIndexMarker675"/>section, we will cover the <a id="_idIndexMarker676"/>various methods that are available in InterpretML for providing a model-agnostic explanation of the black-box model.</p>
			<h3>Explaining black-box models using InterpretML</h3>
			<p>In this section, we will cover the four different methods supported in InterpretML for explaining <a id="_idIndexMarker677"/>black-box models. We will only cover the code part as the visualizations of feature importance and feature variation are very similar to the glass-box models. I do recommend looking at the tutorial notebook for interacting with the generated plots to gain more insights. The methods supported are LIME, Kernel SHAP, Morris Sensitivity, and Partial Dependence:</p>
			<pre class="source-code">from interpret.blackbox import LimeTabular, ShapKernel, MorrisSensitivity, PartialDependence</pre>
			<p>First, we will explore the LIME tabular method:</p>
			<pre class="source-code">#The InterpretML Blackbox explainers need a predict function, and optionally a dataset</pre>
			<pre class="source-code">lime = LimeTabular(predict_fn=model.predict_proba, data=x_train.astype('float').values, random_state=123)</pre>
			<pre class="source-code">#Select the instances to explain, optionally pass in labels if you have them</pre>
			<pre class="source-code">lime_local = lime.explain_local(</pre>
			<pre class="source-code">  x_test[:5].astype('float').values, </pre>
			<pre class="source-code">  y_test[:5], name='LIME')</pre>
			<p>Next, InterpretML provides <a id="_idIndexMarker678"/>the SHAP Kernel method for a model-agnostic SHAP-based explanation:</p>
			<pre class="source-code"># SHAP explanation</pre>
			<pre class="source-code">background_val = np.median(</pre>
			<pre class="source-code">  x_train.astype('float').values, axis=0).reshape(1, -1)</pre>
			<pre class="source-code">shap = ShapKernel(predict_fn=model.predict_proba, </pre>
			<pre class="source-code">                  data=background_val, </pre>
			<pre class="source-code">                  feature_names=list(x_train.columns))</pre>
			<pre class="source-code">shap_local = shap.explain_local(</pre>
			<pre class="source-code">  x_test[:5].astype('float').values, </pre>
			<pre class="source-code">  y_test[:5], name='SHAP')</pre>
			<p>Another model-agnostic global explanation method that is supported is <strong class="bold">Morris Sensitivity</strong>, which is used to obtain the overall sensitivity of the features:</p>
			<pre class="source-code"># Morris Sensitivity</pre>
			<pre class="source-code">sensitivity = MorrisSensitivity(</pre>
			<pre class="source-code">  predict_fn=model.predict_proba, </pre>
			<pre class="source-code">  data=x_train.astype('float').values,                                </pre>
			<pre class="source-code">  feature_names=list(x_train.columns),                                </pre>
			<pre class="source-code">  feature_types=feature_types)</pre>
			<pre class="source-code">sensitivity_global = sensitivity.explain_global(name="Global Sensitivity")</pre>
			<p>InterpretML also supports PDPs for analyzing feature dependence:</p>
			<pre class="source-code"># Partial Dependence</pre>
			<pre class="source-code">pdp = PartialDependence(</pre>
			<pre class="source-code">  predict_fn=model.predict_proba, </pre>
			<pre class="source-code">  data=x_train.astype('float').values,                        </pre>
			<pre class="source-code">  feature_names=list(x_train.columns),</pre>
			<pre class="source-code">  feature_types=feature_types)</pre>
			<pre class="source-code">pdp_global = pdp.explain_global(name='Partial Dependence')</pre>
			<p>Finally, everything <a id="_idIndexMarker679"/>can be consolidated into a single dashboard using a single line of code:</p>
			<pre class="source-code">iml.show([lime_local, shap_local, sensitivity_global, </pre>
			<pre class="source-code">          pdp_global])</pre>
			<p>This will create a similar interactive dashboard, as shown in <em class="italic">Figure 9.18</em>.</p>
			<p>With the various surrogate explainers and interactive dashboards, this framework does have a lot of potential, even though there are quite a few limitations. It is restricted to tabular datasets, it is not compatible with model frameworks such as PyTorch, TensorFlow, and H20, and I think the model explanation methods are also limited. Improving these limitations can definitely increase the adoption of this framework. </p>
			<p>Next, we will cover another popular XAI framework – ALIBI for model explanation. </p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor192"/>ALIBI</h1>
			<p>ALIBI is another <a id="_idIndexMarker680"/>popular XAI framework that supports both local and global explanations for classification and regression models. In <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>, we did explore this framework for getting counterfactual examples, but ALIBI does include other model explainability methods too, which we will explore in this section. Primarily, ALIBI is popular for the following list of model explanation <a id="_idIndexMarker681"/>methods:</p>
			<ul>
				<li><strong class="bold">Anchor explanations</strong>: An anchor explanation is defined as a rule that sufficiently <a id="_idIndexMarker682"/>revolves or anchors around the local prediction. This means that if the anchor value is present in the data instance, the model prediction is almost always the same, irrespective of changes to other feature values.</li>
				<li><strong class="bold">Counterfactual Explanations</strong> (<strong class="bold">CFEs</strong>): We have seen counterfactuals in <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>. CFEs indicate which feature values should <a id="_idIndexMarker683"/>change, and by how much, to produce a different outcome.</li>
				<li><strong class="bold">Contrastive Explanation Methods</strong> (<strong class="bold">CEMs</strong>): CEMs are used with classification models <a id="_idIndexMarker684"/>for local explanations in terms of <strong class="bold">Pertinent Positives</strong> (<strong class="bold">PPs</strong>), meaning features that should be minimally and sufficiently <a id="_idIndexMarker685"/>present to justify a given classification, and <strong class="bold">Pertinent Negatives</strong> (<strong class="bold">PNs</strong>), meaning features that minimally and necessarily <a id="_idIndexMarker686"/>absent to justify the classification.</li>
				<li><strong class="bold">Accumulated Local Effects</strong> (<strong class="bold">ALE</strong>): ALE plots illustrate how attributes influence <a id="_idIndexMarker687"/> the overall prediction of an ML model. ALE plots are often considered to be unbiased and a faster alternative to PDPs, as covered in <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>.</li>
			</ul>
			<p>To get a detailed <a id="_idIndexMarker688"/>summary of the supported methods for model explanation, please take a look at <a href="https://github.com/SeldonIO/alibi#supported-methods">https://github.com/SeldonIO/alibi#supported-methods</a>. Please explore the official documentation of this framework to learn more about it: <a href="https://docs.seldon.io/projects/alibi/en/latest/examples/overview.html">https://docs.seldon.io/projects/alibi/en/latest/examples/overview.html</a>. </p>
			<p>Now, let me walk you through the code tutorial provided for ALIBI.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor193"/>Setting up ALIBI</h2>
			<p>The complete code tutorial is provided in the project repository for this chapter at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ALIBI_example.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ALIBI_example.ipynb</a>. If you have followed the tutorials for <em class="italic">Counterfactual explanations</em> from <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>, you should have ALIBI installed already. </p>
			<p>You can import the submodules that we are going to use for this example from ALIBI:</p>
			<pre class="source-code">import alibi</pre>
			<pre class="source-code">from alibi.explainers import AnchorTabular, CEM, CounterfactualProto, ale </pre>
			<p>Next, let's discuss the dataset for this tutorial.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor194"/>Discussion about the dataset</h2>
			<p>For this <a id="_idIndexMarker689"/>example, we will use the <em class="italic">Occupancy Detection</em> dataset from the <em class="italic">UCI Machine Learning</em> repository at <a href="https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#">https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#</a>. This dataset is used for detecting <a id="_idIndexMarker690"/>whether a room is occupied or not from the different sensor values that are provided. Hence, this is a classification problem that can be solved by fitting ML classifiers on the given dataset. The detailed data inspection, preprocessing, and transformation steps are included in the tutorial notebook. On a very high level, the dataset is slightly imbalanced and, mostly, contains numerical features with no missing values. </p>
			<p class="callout-heading">Occupancy Detection dataset citation</p>
			<p class="callout">L.M. Candanedo, V. Feldheim (2016) - Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. (<a href="https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#">https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+#</a>)</p>
			<p>In this tutorial, I have demonstrated how to use a pipeline approach with scikit-learn for training ML models. This is a very neat way of building ML models, and it is especially useful when working on industrial problems that need to be deployed to the production system. To learn <a id="_idIndexMarker691"/>more about this approach, take a look at the official scikit-learn pipeline documentation at <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html</a>. </p>
			<p>Next, let's discuss the model that will be used for extracting explanations.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor195"/>Training the model</h2>
			<p>For this example, I have used a random forest classifier to train a model with minimal hyperparameter <a id="_idIndexMarker692"/>tuning. You can explore other ML classifiers too, as the choice of the algorithm doesn't matter. Our goal is to explore ALIBI for model explainability, which I will cover in the next section.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor196"/>Model explainability with ALIBI</h2>
			<p>Now, let's use the various explanation methods discussed earlier for the trained model, which we can consider a black box.</p>
			<h3>Using anchor explanations</h3>
			<p>In order to get the <a id="_idIndexMarker693"/>anchor points, first, we need to create an anchor explanation object:</p>
			<pre class="source-code">explainer = AnchorTabular(</pre>
			<pre class="source-code">  predict_fn, </pre>
			<pre class="source-code">  feature_names=list(df_train.columns), </pre>
			<pre class="source-code">  seed=123)</pre>
			<p>Next, we need to fit the explainer object on the training data:</p>
			<pre class="source-code">explainer.fit(df_train.values, disc_perc=[25, 50, 75])</pre>
			<p>We need to learn an anchor value for both the occupied class and the unoccupied class. This process involves providing a data instance belonging to each of these classes as input for estimating the anchor points. This can be done by using the following lines of code:</p>
			<pre class="source-code">class_names = ['not_occupied', 'occupied']</pre>
			<pre class="source-code">print('Prediction: ', </pre>
			<pre class="source-code">      class_names[explainer.predictor(</pre>
			<pre class="source-code">        df_test.values[5].reshape(1, -1))[0]])</pre>
			<pre class="source-code">explanation = explainer.explain(df_test.values[5], </pre>
			<pre class="source-code">                                threshold=0.8)</pre>
			<pre class="source-code">print('Anchor: %s' % (' AND '.join(explanation.anchor)))</pre>
			<pre class="source-code">print('Prediction: ', </pre>
			<pre class="source-code">      class_names[explainer.predictor(</pre>
			<pre class="source-code">        df_test.values[100].reshape(1, -1))[0]])</pre>
			<pre class="source-code">explanation = explainer.explain(df_test.values[100], </pre>
			<pre class="source-code">                                threshold=0.8)</pre>
			<pre class="source-code">print('Anchor: %s' % (' AND '.join(explanation.anchor)))</pre>
			<p>In this example, the anchor point for the occupied class is obtained when the light intensity value is greater than <strong class="source-inline">256.7</strong> and the <strong class="source-inline">CO2</strong> value is greater than <strong class="source-inline">638.8</strong>. In comparison, for the unoccupied class, it is obtained when the <strong class="source-inline">CO2</strong> value is greater than <strong class="source-inline">439</strong>. Essentially, this is telling us that if the sensor values measuring light intensity are greater than <strong class="source-inline">256.7</strong> and the <strong class="source-inline">CO2</strong> levels are greater than <strong class="source-inline">638.8</strong>, the model predicts that the room is occupied. </p>
			<p>The pattern learned by the model is actually appropriate, as whenever a room is occupied, it is <a id="_idIndexMarker694"/>more likely that the lights are turned on, and with more occupants, CO2 levels are also likely to increase. The anchor points for the unoccupied class are not very appropriate, intuitive, or interpretable, but this indicates that, usually, CO2 levels are lower when the room is not occupied. Typically, we get to learn about some threshold values of certain impact features that the model relies on for predicting the outcome.</p>
			<h3>Using CEM</h3>
			<p>With CEM, the main idea is to learn PPs or conditions that should be present to justify the occurrence <a id="_idIndexMarker695"/>of one class and PNs, which should be absent to indicate the occurrence of one class. This is used for model-agnostic local explainability. You can find out more about this method from this research literature at <a href="https://arxiv.org/pdf/1802.07623.pdf">https://arxiv.org/pdf/1802.07623.pdf</a>. </p>
			<p>To apply this in Python, we need to create a CEM object with the required hyper-parameters and fit the train values to learn the PP and PN values:</p>
			<pre class="source-code">cem = CEM(predict_fn, mode, shape, kappa=kappa, </pre>
			<pre class="source-code">          beta=beta, feature_range=feature_range, </pre>
			<pre class="source-code">          max_iterations=max_iterations, c_init=c_init, </pre>
			<pre class="source-code">          c_steps=c_steps, </pre>
			<pre class="source-code">          learning_rate_init=lr_init, clip=clip)</pre>
			<pre class="source-code">cem.fit(df_train.values, no_info_type='median')</pre>
			<pre class="source-code">explanation = cem.explain(X, verbose=False)</pre>
			<p>In our example, the PP and PN values that have been learned show by how much the value should be increased or decreased to meet the minimum criteria for the correct outcome. Surprisingly, no PN value was obtained for our example. This indicates that all the features <a id="_idIndexMarker696"/>are important for the model. The absence of any feature or any particular value range does not help the model predict the outcome. Usually, for higher-dimensional data, the PN values would be important to analyze.</p>
			<h3>Using CFEs</h3>
			<p>In <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>, we looked at tutorial examples of how CFEs can be <a id="_idIndexMarker697"/>applied with ALIBI. We will follow a similar approach for this example, too. However, ALIBI does allow different algorithms <a id="_idIndexMarker698"/>to generate CFEs, which I highly recommend you explore: https://docs.seldon.io/projects/alibi/en/latest/methods/CF.html. In this chapter, we will stick to the prototype-based method covered in the CFE tutorial of <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>: </p>
			<pre class="source-code">cfe = CounterfactualProto(predict_fn,</pre>
			<pre class="source-code">                          shape,</pre>
			<pre class="source-code">                          use_kdtree=True, </pre>
			<pre class="source-code">                          theta=10., </pre>
			<pre class="source-code">                          max_iterations=1000,</pre>
			<pre class="source-code">                          c_init=1., </pre>
			<pre class="source-code">                          c_steps=10</pre>
			<pre class="source-code">                         )</pre>
			<pre class="source-code">cfe.fit(df_train.values, d_type='abdm', </pre>
			<pre class="source-code">        disc_perc=[25, 50, 75])</pre>
			<pre class="source-code">explanation = cfe.explain(X)</pre>
			<p>Once the explanation object is ready, we can actually compare the difference between CFEs and the original data instance to understand the change in the feature values required to flip the outcome. However, using this method to get the correct CFE can be slightly <a id="_idIndexMarker699"/>challenging as there are many hyperparameters that require the right tuning; therefore, the method can be challenging and tedious. Next, let's discuss how to use ALE plots for model explainability. </p>
			<h3>Using ALE plots</h3>
			<p>Similar to PDPs, as covered in <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>, ALE plots can be used to <a id="_idIndexMarker700"/>find the relationship of the individual features with respect to the target class. Let's see how to apply this in Python:</p>
			<pre class="source-code">proba_ale = ale.ALE(predict_fn, feature_names=numeric,</pre>
			<pre class="source-code">                    target_names=class_names)</pre>
			<pre class="source-code">proba_explain = proba_ale.explain(df_test.values)</pre>
			<pre class="source-code">ale.plot_ale(proba_explain, n_cols=3, </pre>
			<pre class="source-code">             fig_kw={'figwidth': 12, 'figheight': 8})</pre>
			<p>This will create the following ALE plots:</p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="image/B18216_09_019.jpg" alt="Figure 9.19 – ALE plots using ALIBI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.19 – ALE plots using ALIBI</p>
			<p>In <em class="italic">Figure 9.19</em>, we can see that the variance in feature values for the <strong class="source-inline">occupied</strong> and <strong class="source-inline">not occupied</strong> target classes is at the maximum level for the feature light, followed by CO2 and temperature, and at the lowest level for <strong class="source-inline">HumidityRatio</strong>. This gives us an indication of how the model prediction changes depending on the variation of the feature values.</p>
			<p>Overall, I feel that ALIBI is an interesting XAI framework that works with tabular and unstructured data such as text and images and does have a wide variety of techniques for <a id="_idIndexMarker701"/>the explainability of ML models. The only limitation I have found is that some of the methods are not very simplified, so they require a good amount of hyperparameter tuning to get reliable explanations. Please explore <a href="https://github.com/SeldonIO/alibi/tree/master/doc/source/examples">https://github.com/SeldonIO/alibi/tree/master/doc/source/examples</a> for other examples provided for ALIBI to get more practical expertise. In the next section, we will discuss DiCE as an XAI Python framework.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor197"/>DiCE</h1>
			<p><strong class="bold">Diverse Counterfactual Explanations</strong> (<strong class="bold">DiCE</strong>) is another popular XAI framework that we briefly covered in <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>, for the <em class="italic">CFE tutorial</em>. Interestingly, DiCE is also one of the key XAI frameworks from Microsoft Research, but <a id="_idIndexMarker702"/>it is yet to be integrated with the InterpretML module (I wonder why!). I find the entire idea of CFE to be very close to the ideal human-friendly explanation that gives actionable recommendations. This blog <a id="_idIndexMarker703"/>from Microsoft discusses the motivation and idea behind the DiCE framework: <a href="https://www.microsoft.com/en-us/research/blog/open-source-library-provides-explanation-for-machine-learning-through-diverse-counterfactuals/">https://www.microsoft.com/en-us/research/blog/open-source-library-provides-explanation-for-machine-learning-through-diverse-counterfactuals/</a>.</p>
			<p>In comparison to ALIBI CFE, I found DiCE to produce more appropriate CFEs with minimal hyperparameter tuning. That's why I feel it's important to mention DiCE, as it is primarily designed for example-based explanations. Next, let's discuss the CFE methods that are supported in DiCE.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor198"/>CFE methods supported in DiCE</h2>
			<p>DiCE can generate <a id="_idIndexMarker704"/>CFEs based on the following methods:</p>
			<ul>
				<li>Model-agnostic methods:<ul><li>KD-Tree</li><li>Genetic algorithm</li><li>Randomized sampling</li></ul></li>
				<li>Gradient-based methods (model-specific methods):<ul><li>Loss-based method for deep learning models</li><li><strong class="bold">Variational Auto-Encoder</strong> (<strong class="bold">VAE</strong>)-based methods</li></ul></li>
			</ul>
			<p>To learn more about all these methods, I request that you explore the official documentation of DiCE (https://github.com/interpretml/DiCE), which contains the necessary research literature for each method. Now, let's use DiCE for model explainability.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor199"/>Model explainability with DiCE</h2>
			<p>The complete tutorial example is provided at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DiCE_example.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/DiCE_example.ipynb</a>. For this example, I have used the same Occupancy Detection dataset that we used for the ALIBI tutorial. Since the same data <a id="_idIndexMarker705"/>preprocessing, transformation, model training, and evaluation steps have been used, we will directly proceed with the model explainability part with DALEX. The notebook tutorial contains all the necessary steps, so I recommend that you go through the notebook first. </p>
			<p>We will use the DiCE framework in the same way as we have done for the CFE tutorial from <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>. </p>
			<p>So, first, we need to define a DiCE data object:</p>
			<pre class="source-code">data_object = dice_ml.Data(</pre>
			<pre class="source-code">  dataframe = df_train[numeric + [target_variable]],</pre>
			<pre class="source-code">  continuous_features = numeric,</pre>
			<pre class="source-code">  outcome_name = target_variable</pre>
			<pre class="source-code">)</pre>
			<p>Next, we need to create a DiCE model object:</p>
			<pre class="source-code">model_object = dice_ml.Model(model=model,backend='sklearn')</pre>
			<p>Following this, we need to pass the data object and the model object for the DiCE explanation object:</p>
			<pre class="source-code">explainer = dice_ml.Dice(data_object, model_object, </pre>
			<pre class="source-code">                         method = 'random')</pre>
			<p>Next, we can take a query data instance and generate CFEs using the DiCE explainer object: </p>
			<pre class="source-code">test_query = df_test[400:401][numeric]</pre>
			<pre class="source-code">cfe = explainer.generate_counterfactuals(</pre>
			<pre class="source-code">    test_query, </pre>
			<pre class="source-code">    total_CFs=4, </pre>
			<pre class="source-code">    desired_range=None,</pre>
			<pre class="source-code">    desired_class="opposite",</pre>
			<pre class="source-code">    features_to_vary= numeric,</pre>
			<pre class="source-code">    permitted_range = { 'CO2' : [400, 1000]}, # Adding a constraint for CO2 feature</pre>
			<pre class="source-code">    random_seed = 123,</pre>
			<pre class="source-code">    verbose=True)</pre>
			<pre class="source-code">cfe.visualize_as_dataframe(show_only_changes=True)</pre>
			<p>This will produce a CFE DataFrame that shows the feature values that need to be changed to <a id="_idIndexMarker706"/>flip the model predicted outcome. The outcome of this approach is illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/B18216_09_020.jpg" alt="Figure 9.20 – CFE generated using the DiCE framework, which is displayed as a DataFrame&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.20 – CFE generated using the DiCE framework, which is displayed as a DataFrame</p>
			<p>Interestingly, CFEs don't only provide actionable insights from the data. However, they can also be used to generate local and global feature importance. The features that can be easily varied to alter the model prediction are considered to be more important by this approach of feature importance. Let's try applying the local feature importance using the DiCE method:</p>
			<pre class="source-code">local_importance = explainer.local_feature_importance(test_query)</pre>
			<pre class="source-code">print(local_importance.local_importance)</pre>
			<pre class="source-code">plt.figure(figsize=(10,5))</pre>
			<pre class="source-code">plt.bar(range(len(local_importance.local_importance[0])), </pre>
			<pre class="source-code">        list(local_importance.local_importance[0].values())/(np.sum(list(local_importance.local_importance[0].values()))), </pre>
			<pre class="source-code">        tick_label=list(local_importance.local_importance[0].keys()),</pre>
			<pre class="source-code">        color = list('byrgmc')</pre>
			<pre class="source-code">       )</pre>
			<pre class="source-code">plt.show()</pre>
			<p>This produces <a id="_idIndexMarker707"/>the following local feature importance plot for the test query selected:</p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/B18216_09_021.jpg" alt="Figure 9.21 – Local feature importance using DiCE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.21 – Local feature importance using DiCE</p>
			<p><em class="italic">Figure 9.21</em> shows that for the test query data, the humidity, light, and CO2 features are the most important for model prediction. This indicates that most CFEs would suggest changing the feature values of one of these features to alter the model prediction. </p>
			<p>Overall, DiCE is a very promising framework for robust CFEs. I recommend you explore the different algorithms to generate CFEs such as KD-Trees, random sampling, and genetic algorithms. DiCE examples can sometimes be very random. My recommendation is to always use a random seed to control the randomness, clearly define the <a id="_idIndexMarker708"/>actionable and non-actionable features, and set the boundary conditions of the actionable features to generate CFEs that are meaningful and practically feasible. Otherwise, the generated CFEs can be very random and practically not feasible and, therefore, less impactful to use. </p>
			<p>For other examples of the DiCE framework for multiclass classification or regression problems, please explore <a href="https://github.com/interpretml/DiCE/tree/master/docs/source/notebooks">https://github.com/interpretml/DiCE/tree/master/docs/source/notebooks</a>. Next, let's cover ELI5, which is one of the initial XAI frameworks that has been developed to produce simplistic explanations of ML models.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor200"/>ELI5</h1>
			<p><em class="italic">ELI5</em>, or <em class="italic">Explain Like I'm Five</em>, is a Python XAI library for debugging, inspecting, and explaining ML classifiers. It was one of the initial XAI frameworks developed to explain black-box <a id="_idIndexMarker709"/>models in the most simplified format. It supports a wide range of ML modeling frameworks such as scikit-learn compatible models, Keras, and more. It also has integrated LIME explainers and can work with tabular datasets along with unstructured data such as text and images. The library documentation is provided at <a href="https://eli5.readthedocs.io/en/latest/">https://eli5.readthedocs.io/en/latest/</a>, and the GitHub project is available at <a href="https://github.com/eli5-org/eli5">https://github.com/eli5-org/eli5</a>. </p>
			<p>In this section, we will cover the application part of ELI5 for a tabular dataset only, but please feel free to explore other examples that have been provided in the tutorial examples of ELI5 at <a href="https://eli5.readthedocs.io/en/latest/tutorials/index.html">https://eli5.readthedocs.io/en/latest/tutorials/index.html</a>. Next, let's get started with the walk-through of the code tutorial.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor201"/>Setting up ELI5</h2>
			<p>The complete tutorial of the ELI5 example is available in the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ELI5_example.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/ELI5_example.ipynb</a>. ELI5 can be installed in Python using the pip installer:</p>
			<pre class="source-code">pip install eli5</pre>
			<p>If the installation process is successful, you can verify it by importing the module in the Jupyter notebook:</p>
			<pre class="source-code">import eli5</pre>
			<p>For this example, we will use the same hepatitis detection dataset from the UCI Machine Learning repository (https://archive.ics.uci.edu/ml/datasets/hepatitis), which we used for the InterpretML example. Also, we have used a random forest classification model <a id="_idIndexMarker710"/>with minimum hyperparameter tuning that will be used as our black-box model. So, we will skip the discussion about the dataset and model part and proceed to the model explainability part using ELI5.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor202"/>Model explainability using ELI5</h2>
			<p>Applying ELI5 in Python <a id="_idIndexMarker711"/>is very easy and can be done with a few lines of code:</p>
			<pre class="source-code">eli5.show_weights(model, vec = DictVectorizer(), </pre>
			<pre class="source-code">                  feature_names = list(encoded.columns))</pre>
			<p>This will produce the following feature weight tabular visualization that can be used to analyze the global feature importance:</p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/B18216_09_022.jpg" alt="Figure 9.22 – Feature weights obtained using ELI5&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.22 – Feature weights obtained using ELI5</p>
			<p><em class="italic">Figure 9.22</em> indicates that the feature, <strong class="source-inline">BILIRUBIN</strong>, has the maximum weight and, hence, has the maximum contribution for influencing the model outcome. The +/- values shown beside the weight values can be considered to be confidence intervals. This method can be considered a very simple way to provide insights into the black-box model. ELI5 calculates the feature weights using tree models. Every node of the tree gives an output score that is used to estimate the total contribution of a feature. The total <a id="_idIndexMarker712"/>contribution on the decision path is how much the score changes from parent to child. The total weights of all the features sum up the total probability of the model for predicting a particular class. </p>
			<p>We can use this method for providing local explainability and for an inference data instance: </p>
			<pre class="source-code">no_missing = lambda feature_name, feature_value: not np.isnan(feature_value) # filter missing values</pre>
			<pre class="source-code">eli5.show_prediction(model, </pre>
			<pre class="source-code">                     x_test.iloc[1:2].astype('float'), </pre>
			<pre class="source-code">                     feature_names = list(encoded.columns), </pre>
			<pre class="source-code">                     show_feature_values=True, </pre>
			<pre class="source-code">                     feature_filter=no_missing,</pre>
			<pre class="source-code">                     target_names = {1:'Die', 2:'Live'},</pre>
			<pre class="source-code">                     top = 10,</pre>
			<pre class="source-code">                     show = ['feature_importances', </pre>
			<pre class="source-code">                             'targets', 'decision_tree', </pre>
			<pre class="source-code">                             'description'])</pre>
			<p>This will produce the following tabular visualization for analyzing the feature contributions of the inference data:</p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/B18216_09_023.jpg" alt="Figure 9.23 – Feature contributions using ELI5 for local explainability&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.23 – Feature contributions using ELI5 for local explainability</p>
			<p>In <em class="italic">Figure 9.23</em>, we can see the feature contributions using ELI5 for the local data used for prediction. There is a <strong class="source-inline">&lt;BIAS&gt;</strong> term that is added to the table. This is considered the expected <a id="_idIndexMarker713"/>average score output by the model, which depends on the distribution of the training data. To find out more, take <a id="_idIndexMarker714"/>a look at this Stack Overflow post: <a href="https://stackoverflow.com/questions/49402701/eli5-explaining-prediction-xgboost-model">https://stackoverflow.com/questions/49402701/eli5-explaining-prediction-xgboost-model</a>.</p>
			<p>Even though ELI5 is easy to use and probably the least complex of all the XAI frameworks covered so far, I would say that the framework is not comprehensive enough. Even the visualization provided to analyze the feature contributions appears to be very archaic and can be improved. Since ELI5 is one of the initial XAI frameworks that works with tabular data, images, and text data, it is important to know about it.</p>
			<p>In the next section, I will cover the model explainability of H2O AutoML models.</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor203"/>H2O AutoML explainers</h1>
			<p>Throughout this chapter, we have mostly used scikit-learn-based and TensorFlow-based models. However, when the idea of AutoML was first introduced, the H2O community was one of the <a id="_idIndexMarker715"/>earliest adopters of this concept and introduced the AutoML feature for the H2O ML framework: <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html</a>. </p>
			<p>Interestingly, H2O AutoML is very widely used in the industry, especially for high-volume datasets. Unfortunately, there are very few model explainability frameworks such as DALEX that are compatible with H2O models. H2O models have a good usage in both R and Python, and with the AutoML feature, this framework promises to spin up trained and tuned models to give the best performance in a very short time and with less effort. So, that's why I feel it is important to mention the H2O AutoML explainer in this chapter. This framework does have a built-in implementation of model explainability methods for explaining the predictions of an AutoML model (<a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html</a>). Next, let's dive deeper into H2O explainers.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor204"/>Explainability with H2O explainers</h2>
			<p>H2O explainers are only supported for H2O models. They can be used to provide both global and <a id="_idIndexMarker716"/>local explanations. The following list shows the supported methods to provide explanations in H2O:</p>
			<ul>
				<li>Model performance comparison (this is particularly useful for AutoML models that try different algorithms on the same dataset)</li>
				<li>Variable or feature importance (this is for both global and local explanations)</li>
				<li>Model correlation heatmaps</li>
				<li>TreeSHAP-based explanations (this is only for tree models)</li>
				<li>PDPs (this is for both global and local explanations)</li>
				<li>Individual conditional expectation plots, which are also referred to as what-if analysis plots (for both global and local explanations)</li>
			</ul>
			<p>You can find <a id="_idIndexMarker717"/>out more about H2O explainers at <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html</a>. The complete tutorial example is provided at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/H2o_AutoML_explain_example.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter09/H2o_AutoML_explain_example.ipynb</a>. In this example, I have demonstrated how to use H2O AutoML for predicting the league position of top <a id="_idIndexMarker718"/>football clubs based on the quality of their players using the FIFA Club Position Prediction dataset (<a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/FIFA_Club_Position">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter09/datasets/FIFA_Club_Position</a>). It is the same dataset that we used for the DALEX and Explainerdashboard tutorials.</p>
			<p>To install the H2O module, you can use the pip installer:</p>
			<pre class="source-code">pip install h2o</pre>
			<p>Since we have already covered the steps of data preparation and transformation in the previous tutorials, I will skip those steps here. But please do refer to the tutorial notebook for executing the end-to-end example.</p>
			<p>H2O models are not compatible with a pandas DataFrame. So, you will need to convert a pandas DataFrame into an H2O DataFrame. Let's see the lines of code for training the H2O AutoML module:</p>
			<pre class="source-code">import h2o</pre>
			<pre class="source-code">from h2o.automl import H2OAutoML</pre>
			<pre class="source-code"># Start the H2O cluster (locally) - Don't forget this step</pre>
			<pre class="source-code">h2o.init()</pre>
			<pre class="source-code">aml = H2OAutoML(max_models=20, seed=1)</pre>
			<pre class="source-code">train = x_train.copy()</pre>
			<pre class="source-code">valid = x_valid.copy()</pre>
			<pre class="source-code">train["position"] = y_train</pre>
			<pre class="source-code">valid["position"] = y_valid</pre>
			<pre class="source-code">x = list(train.columns)</pre>
			<pre class="source-code">y = "position"</pre>
			<pre class="source-code">training_frame = h2o.H2OFrame(train)</pre>
			<pre class="source-code">validation_frame=h2o.H2OFrame(valid) </pre>
			<pre class="source-code"># training the automl model</pre>
			<pre class="source-code">aml.train(x=x, y=y, training_frame=training_frame, </pre>
			<pre class="source-code">          validation_frame=validation_frame)</pre>
			<p>Once the AutoML training process is complete, we can get the best model and store it as a variable for future usage:</p>
			<pre class="source-code">model = aml.get_best_model()</pre>
			<p>For the model explainability part, we just need to use the <strong class="source-inline">explain</strong> method from an AutoML model object:</p>
			<pre class="source-code">aml.explain(validation_frame)</pre>
			<p>This automatically creates a wide range of supported XAI methods and generates visualizations to interpret the model. At the time of writing, the H2O explainability feature is newly <a id="_idIndexMarker719"/>released and is in the experimental phase. If you would like to give any feedback or find any bugs, please raise a ticket request on the H2O JIRA issue tracker (<a href="https://0xdata.atlassian.net/projects/PUBDEV">https://0xdata.atlassian.net/projects/PUBDEV</a>). </p>
			<p>With that, I have covered all the popular XAI frameworks apart from <em class="italic">LIME</em>, <em class="italic">SHAP</em>, and <em class="italic">TCAV</em> that are commonly used or have a high potential for explaining ML models. In the next section, I will give a quick comparison guide to compare all seven frameworks covered in this chapter.</p>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor205"/>Quick comparison guide</h1>
			<p>In this chapter, we discussed the different types of XAI frameworks available in Python. Of course, no one framework is absolutely perfect and can be used for all scenarios. Throughout the sections, I did mention the pros and cons of each framework, but I believe it will be really handy if you have a quick comparison guide to decide on your choice of XAI framework, considering your given problem. </p>
			<p>The following table illustrates a quick comparison guide for the seven XAI frameworks covered in this chapter. I have tried to compare these based on the different dimensions of explainability, their compatibility with various ML models, a qualitative assessment of human-friendly explanations, the robustness of the explanations produced, a qualitative assessment of scalability, and how fast the particular framework can be adopted in production-level systems:</p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/B18216_09_024.jpg" alt="Figure 9.24 – A quick comparison guide of the popular XAI frameworks covered in this chapter&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.24 – A quick comparison guide of the popular XAI frameworks covered in this chapter</p>
			<p>This brings us to the end of this chapter. Next, let me provide a summary of the main topics of discussion for this chapter.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor206"/>Summary</h1>
			<p>In this chapter, we covered the seven popular XAI frameworks that are available in Python: the <em class="italic">DALEX</em>, <em class="italic">Explainerdashboard</em>, <em class="italic">InterpretML</em>, <em class="italic">ALIBI</em>, <em class="italic">DiCE</em>, <em class="italic">ELI5</em>, and <em class="italic">H2O AutoML explainers</em>. We have discussed the supported explanation methods for each of the frameworks, the practical application of each, and the various pros and cons. So, we did cover a lot in this chapter! I also provided a quick comparison guide to help you decide which framework you should go for. This also brings us to the end of <em class="italic">Part 2</em> of this book, which gave you practical exposure to using XAI Python frameworks for problem-solving. </p>
			<p><em class="italic">Section 3</em> of this book is targeted mainly at the researchers and experts who share the same passion as I do: <em class="italic">bringing AI closer to end users</em>. So, in the next chapter, we will discuss the best practices of XAI that are recommended for designing human-friendly AI systems.</p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor207"/>References</h1>
			<p>For additional information, please refer to the following resources:</p>
			<ul>
				<li>The DALEX GitHub project: <a href="https://github.com/ModelOriented/DALEX">https://github.com/ModelOriented/DALEX</a></li>
				<li>The Explainerdashboard GitHub project: <a href="https://github.com/oegedijk/explainerdashboard">https://github.com/oegedijk/explainerdashboard</a></li>
				<li>The InterpretML GitHub project: <a href="https://github.com/interpretml/interpret">https://github.com/interpretml/interpret</a></li>
				<li>The ALIBI GitHub project: <a href="https://github.com/SeldonIO/alibi">https://github.com/SeldonIO/alibi</a></li>
				<li>The DiCE GitHub project: <a href="https://github.com/interpretml/DiCE">https://github.com/interpretml/DiCE</a></li>
				<li>The official ELI5 documentation: <a href="https://eli5.readthedocs.io/en/latest/overview.html">https://eli5.readthedocs.io/en/latest/overview.html</a></li>
				<li>Model Explainability using H2O: <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html#">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/explain.html#</a></li>
			</ul>
		</div>
	</body></html>