<html><head></head><body>
		<div id="_idContainer311">
			<h1 id="_idParaDest-112"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.1.1">Chapter 8: Understanding Deep Learning</span></h1>
			<p><span class="koboSpan" id="kobo.2.1">Throughout this book, we have examined the many tools and methods within the fields of supervised and unsupervised machine learning. </span><span class="koboSpan" id="kobo.2.2">Within the field of unsupervised learning, we explored </span><strong class="bold"><span class="koboSpan" id="kobo.3.1">clustering</span></strong><span class="koboSpan" id="kobo.4.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">dimensionality reduction</span></strong><span class="koboSpan" id="kobo.6.1">, while within the field of supervised learning, we explored </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">classification</span></strong><span class="koboSpan" id="kobo.8.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.9.1">regression</span></strong><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">Within all of these fields, we explored many of the most popular algorithms for developing powerful predictive models for our datasets. </span><span class="koboSpan" id="kobo.10.3">However, as we have seen with some of the data we have worked with, there are numerous limitations when it comes to these models' performance that cannot be overcome by additional tuning and hyperparameter optimization. </span><span class="koboSpan" id="kobo.10.4">In cases such as these, data scientists often turn to the field of </span><strong class="bold"><span class="koboSpan" id="kobo.11.1">deep learning</span></strong><span class="koboSpan" id="kobo.12.1">.</span></p>
			<p><span class="koboSpan" id="kobo.13.1">If you recall our overarching diagram of the artificial intelligence space that we saw in </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Chapter 5</span></em><span class="koboSpan" id="kobo.15.1">, </span><em class="italic"><span class="koboSpan" id="kobo.16.1">Introduction to Machine Learning</span></em><span class="koboSpan" id="kobo.17.1">, we noted that the overall space is known as </span><strong class="bold"><span class="koboSpan" id="kobo.18.1">Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.19.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.20.1">AI</span></strong><span class="koboSpan" id="kobo.21.1">). </span><span class="koboSpan" id="kobo.21.2">Within the AI space, we defined machine learning as the ability to develop models to learn or generalize from data and make predictions. </span><span class="koboSpan" id="kobo.21.3">We will now explore a subset of machine learning known as </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">deep learning</span></strong><span class="koboSpan" id="kobo.23.1">, which focuses on developing models and extracting patterns within data using deep neural networks.</span></p>
			<p><span class="koboSpan" id="kobo.24.1">Throughout this chapter, we will explore the ideas of neural networks and deep learning as they relate to the field of biotechnology. </span><span class="koboSpan" id="kobo.24.2">In particular, we will be covering the following topics:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.25.1">Understanding the field of deep learning</span></li>
				<li><span class="koboSpan" id="kobo.26.1">Exploring the types of deep learning models</span></li>
				<li><span class="koboSpan" id="kobo.27.1">Selecting an activation function</span></li>
				<li><span class="koboSpan" id="kobo.28.1">Measuring progress with loss</span></li>
				<li><span class="koboSpan" id="kobo.29.1">Developing models with the Keras library</span></li>
				<li><span class="koboSpan" id="kobo.30.1">Tutorial – protein sequence classification via LSTMs using Keras and MLflow</span></li>
				<li><span class="koboSpan" id="kobo.31.1">Tutorial – anomaly detection using AWS Lookout for Vision</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.32.1">With these sections in mind, let's get started!</span></p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.33.1">Understanding the field of deep learning</span></h1>
			<p><span class="koboSpan" id="kobo.34.1">As we mentioned in the introduction, deep learning is a subset or branch of the machine learning space that </span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.35.1">focuses on developing models using neural networks. </span><span class="koboSpan" id="kobo.35.2">The idea behind using neural networks for deep learning derives from neural networks found in the human brain. </span><span class="koboSpan" id="kobo.35.3">Let's learn more about this.</span></p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.36.1">Neural networks</span></h2>
			<p><span class="koboSpan" id="kobo.37.1">Similar to machine learning, the idea behind developing deep learning models is not to explicitly </span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.38.1">define the steps in which a decision or prediction is made. </span><span class="koboSpan" id="kobo.38.2">The main idea here is to generalize from the data. </span><span class="koboSpan" id="kobo.38.3">Deep learning makes this possible by drawing a parallel between the dendrites, cell body, and synapses of the human brain, which, within the context of deep learning, act as inputs, nodes, and outputs for a given model, as shown in the following diagram: </span></p>
			<div>
				<div id="_idContainer254" class="IMG---Figure">
					<span class="koboSpan" id="kobo.39.1"><img src="image/B17761_08_001.jpg" alt="Figure 8.1 – Comparison between the human brain and a neural network "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.40.1">Figure 8.1 – Comparison between the human brain and a neural network</span></p>
			<p><span class="koboSpan" id="kobo.41.1">Some of the biggest benefits behind such an implementation revolve around the idea of feature engineering. </span><span class="koboSpan" id="kobo.41.2">Earlier in this book, we saw how features can be created or summarized using various methods such as basic mathematical operations (x2) or through complex algorithms such as </span><strong class="bold"><span class="koboSpan" id="kobo.42.1">Principal Component Analysis</span></strong><span class="koboSpan" id="kobo.43.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.44.1">PCA</span></strong><span class="koboSpan" id="kobo.45.1">). </span><span class="koboSpan" id="kobo.45.2">Manually </span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.46.1">engineered features can be very time-consuming and not feasible in practice, which is where the field of deep learning can come in, with the ability to learn the many underlying features in a given dataset directly from the data. </span></p>
			<p><span class="koboSpan" id="kobo.47.1">Within the field of </span><strong class="bold"><span class="koboSpan" id="kobo.48.1">biotechnology</span></strong><span class="koboSpan" id="kobo.49.1">, most applications, ranging from the early stages of therapeutic </span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.50.1">discovery all the way downstream to manufacturing, are generally data-rich processes. </span><span class="koboSpan" id="kobo.50.2">However, much of the data that's been collected will have little to no use on its own, or perhaps the data that's been collected is for different batches of a particular molecule. </span><span class="koboSpan" id="kobo.50.3">Perhaps the data is extensive for some molecules and less extensive for others. </span><span class="koboSpan" id="kobo.50.4">In many </span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.51.1">of these cases, using deep learning </span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.52.1">models can come to your aid when it comes to features that are relative to the traditional machine learning models we have discussed so far.</span></p>
			<p><span class="koboSpan" id="kobo.53.1">We can think of features at three different levels: </span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.54.1">Low-level features</span></strong><span class="koboSpan" id="kobo.55.1">, such as </span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.56.1">individual amino acids, a protein, or the elements of a small molecule.</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.57.1">Mid-level features</span></strong><span class="koboSpan" id="kobo.58.1">, such as </span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.59.1">the amino acid sequences of a protein and the functional groups of a small molecule.</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.60.1">High-level features</span></strong><span class="koboSpan" id="kobo.61.1">, such as </span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.62.1">the overall structure or the classification of a protein or the geometric shape of a small molecule.</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.63.1">The following diagram shows a graphical representation of these features: </span></p>
			<div>
				<div id="_idContainer255" class="IMG---Figure">
					<span class="koboSpan" id="kobo.64.1"><img src="image/B17761_08_002.jpg" alt="Figure 8.2 – The three types of features and some associated examples "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.65.1">Figure 8.2 – The three types of features and some associated examples</span></p>
			<p><span class="koboSpan" id="kobo.66.1">In many instances, architecting a robust deep learning model can unlock a more powerful predictive model relative to its machine learning counterpart. </span><span class="koboSpan" id="kobo.66.2">In many of the machine learning models we have explored, we attempted to improve the model's performance not only by tuning and adjusting the hyperparameters but also by making a conscious decision to use datasets with a sufficient amount of data. </span><span class="koboSpan" id="kobo.66.3">Increasing the size of the datasets </span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.67.1">will likely not lead to any significant improvement in our machine learning models. </span><span class="koboSpan" id="kobo.67.2">However, this is not always the case with </span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.68.1">deep learning models, which tend to improve in performance when more data is made available. </span><span class="koboSpan" id="kobo.68.2">We can see a visual depiction of this in the following diagram:</span></p>
			<div>
				<div id="_idContainer256" class="IMG---Figure">
					<span class="koboSpan" id="kobo.69.1"><img src="image/B17761_08_003.jpg" alt="Figure 8.3 – A graphical representation of machine learning versus deep learning "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.70.1">Figure 8.3 – A graphical representation of machine learning versus deep learning</span></p>
			<p><span class="koboSpan" id="kobo.71.1">Using neural networks within the context of machine learning has seen a major surge in recent years, which can be attributed to the increased use of big data within most industries, the decreased expense of computational hardware such as CPUs and GPUs, and the growing community that supports much of the open source software and packages that are available today. </span><span class="koboSpan" id="kobo.71.2">Two of the most common packages out there for developing deep learning models are TensorFlow and Keras – we will explore these two later in this chapter. </span><span class="koboSpan" id="kobo.71.3">Before we do, let's go ahead and talk about the architecture behind a deep learning model.</span></p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.72.1">The perceptron</span></h2>
			<p><span class="koboSpan" id="kobo.73.1">One of the most important building blocks of any deep learning model is the perceptron. </span><span class="koboSpan" id="kobo.73.2">A perceptron is </span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.74.1">an algorithm that's used for developing </span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.75.1">supervised binary classifiers, first invented in 1958 by Frank Rosenblatt, who is sometimes called the father of deep learning. </span><span class="koboSpan" id="kobo.75.2">A perceptron generally consists of four major parts: </span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.76.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.77.1">input</span></strong><span class="koboSpan" id="kobo.78.1"> values, which </span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.79.1">are generally taken from a given dataset. </span></li>
				<li><span class="koboSpan" id="kobo.80.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.81.1">weights</span></strong><span class="koboSpan" id="kobo.82.1">, which </span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.83.1">are values by which the input values are multiplied.</span></li>
				<li><span class="koboSpan" id="kobo.84.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.85.1">net sum</span></strong><span class="koboSpan" id="kobo.86.1">, which is </span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.87.1">the sum of all the values from each of the inputs. </span></li>
				<li><span class="koboSpan" id="kobo.88.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">activation function</span></strong><span class="koboSpan" id="kobo.90.1">, which </span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.91.1">maps a resulting value to an output.</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.92.1">The following diagram shows a graphical representation of these four parts of a perceptron:</span></p>
			<div>
				<div id="_idContainer257" class="IMG---Figure">
					<span class="koboSpan" id="kobo.93.1"><img src="image/B17761_08_004.jpg" alt="Figure 8.4 – A graphical representation of a perceptron "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.94.1">Figure 8.4 – A graphical representation of a perceptron</span></p>
			<p><span class="koboSpan" id="kobo.95.1">There are three main steps that a perceptron takes to arrive at a predicted output from a given set of input values:</span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.96.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.97.1">input values</span></strong><span class="koboSpan" id="kobo.98.1"> (x</span><span class="subscript"><span class="koboSpan" id="kobo.99.1">1</span></span><span class="koboSpan" id="kobo.100.1">, x</span><span class="subscript"><span class="koboSpan" id="kobo.101.1">2</span></span><span class="koboSpan" id="kobo.102.1">, and so on) are multiplied by their respective weights (w</span><span class="subscript"><span class="koboSpan" id="kobo.103.1">1</span></span><span class="koboSpan" id="kobo.104.1">, w</span><span class="subscript"><span class="koboSpan" id="kobo.105.1">2</span></span><span class="koboSpan" id="kobo.106.1">, and so on). </span><span class="koboSpan" id="kobo.106.2">These weights are determined in the training process for this model so that a different weight is assigned to each of the input values.</span></li>
				<li><span class="koboSpan" id="kobo.107.1">All the values from each of the calculations are summed together in a value known as the </span><strong class="bold"><span class="koboSpan" id="kobo.108.1">weighted sum</span></strong><span class="koboSpan" id="kobo.109.1">.</span></li>
				<li><span class="koboSpan" id="kobo.110.1">The weighted sum is then applied to the </span><strong class="bold"><span class="koboSpan" id="kobo.111.1">activation function</span></strong><span class="koboSpan" id="kobo.112.1"> to map the value to a given output. </span><span class="koboSpan" id="kobo.112.2">The specific activation function that's used is dependent on the given situation. </span><span class="koboSpan" id="kobo.112.3">For example, within the context of a unit step activation function, values would either be mapped to 0 or 1.</span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.113.1">When viewed </span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.114.1">from a mathematical perspective, we can define the </span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.115.1">output value, </span><span class="koboSpan" id="kobo.116.1"><img src="image/Formula_B17761__08_016.png" alt=""/></span><span class="koboSpan" id="kobo.117.1">, as follows:</span></p>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<span class="koboSpan" id="kobo.118.1"><img src="image/Formula_B17761__08_001.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.119.1">In this equation, </span><em class="italic"><span class="koboSpan" id="kobo.120.1">g</span></em><span class="koboSpan" id="kobo.121.1"> is the activation function, </span><em class="italic"><span class="koboSpan" id="kobo.122.1">w</span></em><span class="subscript"><span class="koboSpan" id="kobo.123.1">o</span></span><span class="koboSpan" id="kobo.124.1"> is the bias, and the final components are the </span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.125.1">sum of the </span><strong class="bold"><span class="koboSpan" id="kobo.126.1">linear combination</span></strong><span class="koboSpan" id="kobo.127.1"> of input values:</span></p>
			<div>
				<div id="_idContainer260" class="IMG---Figure">
					<span class="koboSpan" id="kobo.128.1"><img src="image/Formula_B17761__08_002.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.129.1">So, in this equation, </span><span class="koboSpan" id="kobo.130.1"><img src="image/Formula_B17761__08_003.png" alt=""/></span><span class="koboSpan" id="kobo.131.1"> and </span><span class="koboSpan" id="kobo.132.1"><img src="image/Formula_B17761__08_004.png" alt=""/></span><span class="koboSpan" id="kobo.133.1"> account for the final output value.</span></p>
			<p><span class="koboSpan" id="kobo.134.1">A perceptron is one of the simplest deep learning building blocks out there and can be expanded quite drastically by increasing the number of </span><strong class="bold"><span class="koboSpan" id="kobo.135.1">hidden layers</span></strong><span class="koboSpan" id="kobo.136.1">. </span><span class="koboSpan" id="kobo.136.2">Hidden layers are the </span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.137.1">layers that lay in-between the input and output layers. </span><span class="koboSpan" id="kobo.137.2">Models with very few hidden </span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.138.1">layers are generally referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.139.1">neural networks</span></strong><span class="koboSpan" id="kobo.140.1"> or multilayer perceptrons, whereas models with many hidden layers are referred </span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.141.1">to as </span><strong class="bold"><span class="koboSpan" id="kobo.142.1">deep neural networks</span></strong><span class="koboSpan" id="kobo.143.1">. </span></p>
			<p><span class="koboSpan" id="kobo.144.1">Each of these layers </span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.145.1">consists of several </span><strong class="bold"><span class="koboSpan" id="kobo.146.1">nodes</span></strong><span class="koboSpan" id="kobo.147.1">, and the flow of data is similar to that of the perceptron we saw previously. </span><span class="koboSpan" id="kobo.147.2">The number of input nodes (</span><em class="italic"><span class="koboSpan" id="kobo.148.1">x</span></em><span class="subscript"><span class="koboSpan" id="kobo.149.1">1</span></span><em class="italic"><span class="koboSpan" id="kobo.150.1">, x</span></em><span class="subscript"><span class="koboSpan" id="kobo.151.1">2</span></span><em class="italic"><span class="koboSpan" id="kobo.152.1">, x</span></em><span class="subscript"><span class="koboSpan" id="kobo.153.1">3</span></span><span class="koboSpan" id="kobo.154.1">) generally </span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.155.1">corresponds to the number of </span><strong class="bold"><span class="koboSpan" id="kobo.156.1">features</span></strong><span class="koboSpan" id="kobo.157.1"> in a given dataset, whereas the number of output nodes generally corresponds to the number of outputs.</span></p>
			<p><span class="koboSpan" id="kobo.158.1">The following </span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.159.1">diagram is a graphical representation of the </span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.160.1">difference between neural networks and deep learning:</span></p>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<span class="koboSpan" id="kobo.161.1"><img src="image/B17761_08_005.jpg" alt="Figure 8.5 – Difference between neural networks and deep learning "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.162.1">Figure 8.5 – Difference between neural networks and deep learning</span></p>
			<p><span class="koboSpan" id="kobo.163.1">In the previous diagram, we can see the neural network or multilayer perceptron (on the left) consisting of an input layer, a single hidden layer with four nodes, and an output layer with four nodes. </span><span class="koboSpan" id="kobo.163.2">Similar to the single perceptron we saw earlier, the idea here is that each of the nodes within the hidden layer will intake the input nodes, multiplied by some value, and then pass them through an </span><strong class="bold"><span class="koboSpan" id="kobo.164.1">activation function</span></strong><span class="koboSpan" id="kobo.165.1"> to yield output. </span><span class="koboSpan" id="kobo.165.2">On the right, we can see a similar model, but the values are passed through several hidden layers before determining a final output value. </span></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor117"/><span class="koboSpan" id="kobo.166.1">Exploring the different types of deep learning models</span></h2>
			<p><span class="koboSpan" id="kobo.167.1">There are many different types of neural networks and deep learning architectures out there that </span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.168.1">differ in function, shape, data flow, and much more. </span><span class="koboSpan" id="kobo.168.2">There are three types of neural networks that have gained a great deal of popularity in recent years, given their promise and robustness with various types of data. </span><span class="koboSpan" id="kobo.168.3">First, we will explore the simplest of these architectures, known as a multilayer perceptron.</span></p>
			<h3><span class="koboSpan" id="kobo.169.1">Multilayer perceptron</span></h3>
			<p><span class="koboSpan" id="kobo.170.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.171.1">Multilayer Perceptron</span></strong><span class="koboSpan" id="kobo.172.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.173.1">MLP</span></strong><span class="koboSpan" id="kobo.174.1">) is one of the most basic types of </span><strong class="bold"><span class="koboSpan" id="kobo.175.1">Artificial Neural Networks</span></strong><span class="koboSpan" id="kobo.176.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.177.1">ANNs</span></strong><span class="koboSpan" id="kobo.178.1">). </span><span class="koboSpan" id="kobo.178.2">This </span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.179.1">type of network is simply </span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.180.1">composed of layers in which data flows in a forward manner, as shown in the previous diagram. </span><span class="koboSpan" id="kobo.180.2">Data flows from </span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.181.1">the input layer to one or more hidden layers, and then finally to an output layer in which a prediction is produced. </span><span class="koboSpan" id="kobo.181.2">In essence, each layer attempts to learn and calculate certain weights. </span><span class="koboSpan" id="kobo.181.3">ANNs and MLPs come in many different shapes and sizes: they can have a different number of nodes in each layer, a different number of inputs, or even a different number of outputs. </span><span class="koboSpan" id="kobo.181.4">We can see a visual depiction of this in the following diagram:</span></p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<span class="koboSpan" id="kobo.182.1"><img src="image/B17761_08_006.jpg" alt="Figure 8.6 – Two examples of MLPs "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.183.1">Figure 8.6 – Two examples of MLPs</span></p>
			<p><span class="koboSpan" id="kobo.184.1">MLP models are generally very versatile but are most commonly used for structured tabular </span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.185.1">data, such as the structured protein classification dataset we have been working with. </span><span class="koboSpan" id="kobo.185.2">In addition, they can </span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.186.1">be used for image data or even text data. </span><span class="koboSpan" id="kobo.186.2">MLPs, however, generally tend to suffer when it comes to sequential data such as protein sequences and time-series datasets.</span></p>
			<h3><span class="koboSpan" id="kobo.187.1">Convolutional neural networks</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.188.1">Convolutional Neural Networks</span></strong><span class="koboSpan" id="kobo.189.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.190.1">CNNs</span></strong><span class="koboSpan" id="kobo.191.1">) are deep learning algorithms that are commonly used for processing and analyzing image data. </span><span class="koboSpan" id="kobo.191.2">CNNs can take in images as input data </span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.192.1">and restructure them to determine the importance through weights and biases, allowing it to distinguish between </span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.193.1">the features of one image relative to another. </span><span class="koboSpan" id="kobo.193.2">Similar to our earlier discussion of how deep learning is similar to neurons in the brain, CNNs are also analogous to the connectivity of neurons in the human brain and the visual cortex when it comes to the sensitivity of regions, similar to the concept of receptive fields. </span><span class="koboSpan" id="kobo.193.3">One of the biggest areas of success for CNN models is their ability to capture spatial dependencies, as well as temporal dependencies, in images through the use of filters. </span><span class="koboSpan" id="kobo.193.4">We can see a visual representation of this in the following diagram:</span></p>
			<div>
				<div id="_idContainer265" class="IMG---Figure">
					<span class="koboSpan" id="kobo.194.1"><img src="image/B17761_08_007.jpg" alt="Figure 8.7 – A representation of a CNN "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.195.1">Figure 8.7 – A representation of a CNN</span></p>
			<p><span class="koboSpan" id="kobo.196.1">Let's take, for example, the idea of creating an image classification model. </span><span class="koboSpan" id="kobo.196.2">We could use an ANN and convert a 2D image of pixels by flattening it. </span><span class="koboSpan" id="kobo.196.3">An image with a 4x4 matrix of pixels would now become a 1x16 vector instead. </span><span class="koboSpan" id="kobo.196.4">This change would cause two main drawbacks: </span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.197.1">The spatial features of the image would be lost, and thereby reduce the robustness of any trained model.</span></li>
				<li><span class="koboSpan" id="kobo.198.1">The number of input features would increase quite drastically as the image size grows.</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.199.1">CNNs </span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.200.1">can overcome </span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.201.1">this by extracting high-level features from the images, allowing them to be quite effective with image-based datasets.</span></p>
			<h3><span class="koboSpan" id="kobo.202.1">Recurrent neural networks</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.203.1">Recurrent Neural Networks</span></strong><span class="koboSpan" id="kobo.204.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.205.1">RNNs</span></strong><span class="koboSpan" id="kobo.206.1">) are commonly used algorithms that are generally </span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.207.1">applied to sequence-based datasets. </span><span class="koboSpan" id="kobo.207.2">They are quite similar in architecture to the ANNs we discussed earlier, but RNNs </span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.208.1">can remember their input using internal memory, making them quite effective with sequential datasets in which previous data is of great importance.</span></p>
			<p><span class="koboSpan" id="kobo.209.1">Take, for example, a protein sequence consisting of various amino acids. </span><span class="koboSpan" id="kobo.209.2">To predict the class of the protein, or its general structure, the model would not only need to know which amino acids were used but the order in which they were used as well. </span><span class="koboSpan" id="kobo.209.3">RNNs and their many derivatives have been central to the many advances in deep learning within the field of biology and biotechnology. </span><span class="koboSpan" id="kobo.209.4">We can see a visual representation of this in the following diagram:</span></p>
			<div>
				<div id="_idContainer266" class="IMG---Figure">
					<span class="koboSpan" id="kobo.210.1"><img src="image/B17761_08_008.jpg" alt="Figure 8.8 – A representation of an ANN node versus an RNN node "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.211.1">Figure 8.8 – A representation of an ANN node versus an RNN node</span></p>
			<p><span class="koboSpan" id="kobo.212.1">There are </span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.213.1">several advantages when it comes to using RNNs as predictor models, with the main benefits being as follows:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.214.1">Their ability to capture the dependency between data points such as words in a sentence</span></li>
				<li><span class="koboSpan" id="kobo.215.1">Their ability to share parameters across time steps, thus decreasing the overall computational cost</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.216.1">Because </span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.217.1">of this, RNNs have become increasingly </span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.218.1">popular architectures for developing models that solve problems related to scientific sequence data such as proteins and DNA, as well as text and time-series data. </span></p>
			<h3><span class="koboSpan" id="kobo.219.1">Long short-term memory </span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.220.1">Long Short-Term Memory</span></strong><span class="koboSpan" id="kobo.221.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.222.1">LSTM</span></strong><span class="koboSpan" id="kobo.223.1">) models are a type of RNN designed with the capability </span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.224.1">of learning long-term dependencies when </span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.225.1">handling sequence-based problems. </span><span class="koboSpan" id="kobo.225.2">Commonly used with text-based data for classification, translation, and recognition, LSTMs have gained an unprecedented surge in popularity over the years. </span><span class="koboSpan" id="kobo.225.3">We can depict the structure of a standard RNN as we did previously, but structured slightly differently:</span></p>
			<div>
				<div id="_idContainer267" class="IMG---Figure">
					<span class="koboSpan" id="kobo.226.1"><img src="image/B17761_08_009.jpg" alt="Figure 8.9 – The inner workings of an RNN versus an LSTM "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.227.1">Figure 8.9 – The inner workings of an RNN versus an LSTM</span></p>
			<p><span class="koboSpan" id="kobo.228.1">In the preceding diagram, </span><em class="italic"><span class="koboSpan" id="kobo.229.1">X</span></em><span class="subscript"><span class="koboSpan" id="kobo.230.1">t</span></span><span class="koboSpan" id="kobo.231.1"> is an input vector, </span><em class="italic"><span class="koboSpan" id="kobo.232.1">h</span></em><span class="subscript"><span class="koboSpan" id="kobo.233.1">t</span></span><em class="italic"> </em><span class="koboSpan" id="kobo.234.1">is a hidden layer vector, and </span><em class="italic"><span class="koboSpan" id="kobo.235.1">o</span></em><span class="subscript"><span class="koboSpan" id="kobo.236.1">t</span></span><span class="koboSpan" id="kobo.237.1"> is an output vector. </span><span class="koboSpan" id="kobo.237.2">On the other hand, and using some of the same elements, an LSTM can be structured quite similarly. </span><span class="koboSpan" id="kobo.237.3">Without diving into too much detail, the core idea behind an LSTM is the cell state (the top horizontal line). </span><span class="koboSpan" id="kobo.237.4">This state operates similarly to a conveyor </span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.238.1">belt in which </span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.239.1">data flows linearly through it. </span><span class="koboSpan" id="kobo.239.2">Gates within the cell are methods that optionally allow information to be added to the state. </span><span class="koboSpan" id="kobo.239.3">An LSTM has three gates, all leading to the cell state.</span></p>
			<p><span class="koboSpan" id="kobo.240.1">Although LSTM models and their associated diagrams can be quite intimidating at first, they have proven their worth time and time again in various areas. </span><span class="koboSpan" id="kobo.240.2">Most recently, LSTM models have been used as generative models for antibody design, as well as classification models for protein sequence-structure classification. </span><span class="koboSpan" id="kobo.240.3">Now that we have explored several common deep learning architectures, let's go ahead and explore their main components: activation functions.</span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor118"/><span class="koboSpan" id="kobo.241.1">Selecting an activation function</span></h1>
			<p><span class="koboSpan" id="kobo.242.1">Recall that, in the previous section, we used an activation function to map a value to a particular output, depending on the value. </span><span class="koboSpan" id="kobo.242.2">We will define an activation function as a mathematical </span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.243.1">function that defines the output of an individual node using an input value. </span><span class="koboSpan" id="kobo.243.2">Using the analogy of the human brain, these functions simply act as gatekeepers, deciding what will be </span><em class="italic"><span class="koboSpan" id="kobo.244.1">fired off</span></em><span class="koboSpan" id="kobo.245.1"> to the next neuron. </span><span class="koboSpan" id="kobo.245.2">There are </span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.246.1">several features that an activation function should have to allow the model to learn most effectively from it: </span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.247.1">The avoidance of a vanishing gradient</span></li>
				<li><span class="koboSpan" id="kobo.248.1">A low computational expense</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.249.1">Artificial neural networks </span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.250.1">are trained using a process known as gradient descent. </span><span class="koboSpan" id="kobo.250.2">For this example, let's assume that there is a two-layer neural network:</span></p>
			<div>
				<div id="_idContainer268" class="IMG---Figure">
					<span class="koboSpan" id="kobo.251.1"><img src="image/Formula_B17761__08_005.jpg" alt=""/></span>
				</div>
			</div>
			<div>
				<div id="_idContainer269" class="IMG---Figure">
					<span class="koboSpan" id="kobo.252.1"><img src="image/Formula_B17761__08_006.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.253.1">The overall network can be represented as follows:</span></p>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<span class="koboSpan" id="kobo.254.1"><img src="image/Formula_B17761__08_007.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.255.1">When the weights </span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.256.1">are calculated in a step known as a backward pass, the </span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.257.1">result becomes as follows:</span></p>
			<div>
				<div id="_idContainer271" class="IMG---Figure">
					<span class="koboSpan" id="kobo.258.1"><img src="image/Formula_B17761__08_008.jpg" alt=""/></span>
				</div>
			</div>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<span class="koboSpan" id="kobo.259.1"><img src="image/Formula_B17761__08_009.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.260.1">Upon determining the derivative, the function becomes as follows:</span></p>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<span class="koboSpan" id="kobo.261.1"><img src="image/Formula_B17761__08_010.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.262.1">If this process were to continue through many layers during the backpropagation step, there would be a considerable reduction in the value of the gradient for the initial layers, thus </span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.263.1">halting the model's ability to learn. </span><span class="koboSpan" id="kobo.263.2">This is the concept of </span><strong class="bold"><span class="koboSpan" id="kobo.264.1">vanishing gradients</span></strong><span class="koboSpan" id="kobo.265.1">.</span></p>
			<p><span class="koboSpan" id="kobo.266.1">On the </span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.267.1">other hand, </span><strong class="bold"><span class="koboSpan" id="kobo.268.1">computational expense</span></strong><span class="koboSpan" id="kobo.269.1"> is also a feature that must be considered before designing and deploying any given model. </span><span class="koboSpan" id="kobo.269.2">The activation functions that are applied from one layer to another must be calculated many times, so the expense of the calculation should be kept to a minimum to avoid longer training periods. </span><span class="koboSpan" id="kobo.269.3">The flow </span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.270.1">of information from an input layer to an output layer is called </span><strong class="bold"><span class="koboSpan" id="kobo.271.1">forward propagation</span></strong><span class="koboSpan" id="kobo.272.1">. </span></p>
			<p><span class="koboSpan" id="kobo.273.1">There are many different types of activation functions out there that are commonly used for </span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.274.1">various purposes. </span><span class="koboSpan" id="kobo.274.2">Although this is not a hard rule, some activation functions are generally used </span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.275.1">with specific deep </span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.276.1">learning layers. </span><span class="koboSpan" id="kobo.276.2">For example, </span><strong class="bold"><span class="koboSpan" id="kobo.277.1">ReLU</span></strong><span class="koboSpan" id="kobo.278.1"> activation functions </span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.279.1">are commonly used with </span><strong class="bold"><span class="koboSpan" id="kobo.280.1">CNNs</span></strong><span class="koboSpan" id="kobo.281.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.282.1">MLPs</span></strong><span class="koboSpan" id="kobo.283.1">. </span><span class="koboSpan" id="kobo.283.2">On the </span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.284.1">other hand, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">sigmoid</span></strong><span class="koboSpan" id="kobo.286.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.287.1">Tanh</span></strong><span class="koboSpan" id="kobo.288.1"> activation functions are commonly used with </span><strong class="bold"><span class="koboSpan" id="kobo.289.1">RNNs</span></strong><span class="koboSpan" id="kobo.290.1">. </span><span class="koboSpan" id="kobo.290.2">Let's take a moment and look at the three most common activation functions you will likely encounter in your journey: </span></p>
			<div>
				<div id="_idContainer274" class="IMG---Figure">
					<span class="koboSpan" id="kobo.291.1"><img src="image/B17761_08_010.jpg" alt="Figure 8.10 – Various types of activation functions by model type "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.292.1">Figure 8.10 – Various types of activation functions by model type</span></p>
			<p><span class="koboSpan" id="kobo.293.1">With some of these types now in mind, let's go ahead and explore them in a little more detail. </span><strong class="source-inline"><span class="koboSpan" id="kobo.294.1">sigmoid</span></strong><span class="koboSpan" id="kobo.295.1"> functions are probably some of the most commonly used functions within the field of deep learning. </span><span class="koboSpan" id="kobo.295.2">It is a non-linear activation function that is also sometimes referred to as a logistic function (remember logistic regression? </span><span class="koboSpan" id="kobo.295.3">Hint hint). </span><strong class="source-inline"><span class="koboSpan" id="kobo.296.1">sigmoid</span></strong><span class="koboSpan" id="kobo.297.1"> functions are unique in the sense that they can map values to either a 0 or a 1. </span><span class="koboSpan" id="kobo.297.2">Using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.298.1">numpy</span></strong><span class="koboSpan" id="kobo.299.1"> library, we can easily put together a </span><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">Python</span></strong><span class="koboSpan" id="kobo.301.1"> function to calculate it:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.302.1">import numpy as np</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.303.1">def sigmoid_function(x):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.304.1">    return 1 / (1 + np.exp(-x))</span></p>
			<p><span class="koboSpan" id="kobo.305.1">Using this function alongside the </span><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">numpy</span></strong><span class="koboSpan" id="kobo.307.1"> library, we can generate some data and plot our </span><strong class="source-inline"><span class="koboSpan" id="kobo.308.1">sigmoid</span></strong><span class="koboSpan" id="kobo.309.1"> function accordingly:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.310.1">x1 = np.linspace(-10, 10, 100)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.311.1">y1 = [sigmoid_function(i) for i in x1]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.312.1">plt.plot(x1,y1)</span></p>
			<p><span class="koboSpan" id="kobo.313.1">In return, we yield </span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.314.1">the following diagram, showing the curved nature of a </span><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">sigmoid</span></strong><span class="koboSpan" id="kobo.316.1"> function. </span><span class="koboSpan" id="kobo.316.2">Notice how the upper and lower ranges are 1 and 0, respectively:</span></p>
			<div>
				<div id="_idContainer275" class="IMG---Figure">
					<span class="koboSpan" id="kobo.317.1"><img src="image/B17761_08_011.jpg" alt="Figure 8.11 – A simple sigmoid function "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.318.1">Figure 8.11 – A simple sigmoid function</span></p>
			<p><span class="koboSpan" id="kobo.319.1">One of the biggest issues with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.320.1">sigmoid</span></strong><span class="koboSpan" id="kobo.321.1"> activation function is that the outputs can </span><strong class="bold"><span class="koboSpan" id="kobo.322.1">saturate</span></strong><span class="koboSpan" id="kobo.323.1"> in the sense that values greater than 1.0 are mapped to one, and values that are smaller than 0 are mapped to 0. </span><span class="koboSpan" id="kobo.323.2">This can cause some models to fail to generalize or learn from the data and is related to the vanishing gradients issue we discussed earlier in this chapter.</span></p>
			<p><span class="koboSpan" id="kobo.324.1">On the other hand, another common activation function is </span><strong class="bold"><span class="koboSpan" id="kobo.325.1">Tanh</span></strong><span class="koboSpan" id="kobo.326.1">, which is very similar to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.327.1">sigmoid</span></strong><span class="koboSpan" id="kobo.328.1"> function. </span><span class="koboSpan" id="kobo.328.2">The Tanh function is symmetric in the sense that it passes through the point (0, 0) and it ranges to the values of 1 and -1, unlike its </span><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">sigmoid</span></strong><span class="koboSpan" id="kobo.330.1"> counterpart, making it a slightly better function. </span><span class="koboSpan" id="kobo.330.2">Instead of defining our functions in Python, as we did previously, we can take advantage of the optimized functions in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">numpy</span></strong><span class="koboSpan" id="kobo.332.1"> library:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.333.1">x2 = np.linspace(-5, 5, 100)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.334.1">y2 = np.tanh(x2)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.335.1">plt.plot(x2, y2)</span></p>
			<p><span class="koboSpan" id="kobo.336.1">Upon executing this code, we retrieve the following diagram. </span><span class="koboSpan" id="kobo.336.2">Notice how the center of the diagram is the point (0, 0), while the upper and lower values are 1.00 and -1.00, respectively:</span></p>
			<div>
				<div id="_idContainer276" class="IMG---Figure">
					<span class="koboSpan" id="kobo.337.1"><img src="image/B17761_08_012.jpg" alt="Figure 8.12 – A simple Tanh function "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.338.1">Figure 8.12 – A simple Tanh function</span></p>
			<p><span class="koboSpan" id="kobo.339.1">Similar to its </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">sigmoid</span></strong><span class="koboSpan" id="kobo.341.1"> counterpart, the </span><strong class="bold"><span class="koboSpan" id="kobo.342.1">Tanh</span></strong><span class="koboSpan" id="kobo.343.1"> activation function also suffers from the issue of </span><strong class="bold"><span class="koboSpan" id="kobo.344.1">vanishing gradients</span></strong><span class="koboSpan" id="kobo.345.1"> and saturation. </span><span class="koboSpan" id="kobo.345.2">However, given the fact that the range is -1 to 1, the gradient is somewhat stronger than </span><strong class="source-inline"><span class="koboSpan" id="kobo.346.1">sigmoid</span></strong><span class="koboSpan" id="kobo.347.1">, making it a slightly better function to use.</span></p>
			<p><span class="koboSpan" id="kobo.348.1">Finally, yet another </span><a id="_idIndexMarker822"/><span class="koboSpan" id="kobo.349.1">commonly used activation function is the </span><strong class="bold"><span class="koboSpan" id="kobo.350.1">Rectified Linear Unit</span></strong><span class="koboSpan" id="kobo.351.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.352.1">ReLU</span></strong><span class="koboSpan" id="kobo.353.1">). </span><strong class="bold"><span class="koboSpan" id="kobo.354.1">ReLU</span></strong><span class="koboSpan" id="kobo.355.1"> activation functions were specifically </span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.356.1">developed to avoid saturation when handling larger numbers. </span><span class="koboSpan" id="kobo.356.2">The non-linear nature of this function allows it to learn the patterns within the data, whereas the linear nature of the function allows it to be easily interpretable relative to the other functions we have seen so far. </span><span class="koboSpan" id="kobo.356.3">Let's go ahead and explore this in Python:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.357.1">def relu_function(x):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.358.1">    return np.array([0, x]).max()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.359.1">x3 = np.linspace(-5, 5, 100)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.360.1">y3 = [relu_function(i) for i in x3]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.361.1">plt.plot(x3, y3)</span></p>
			<p><span class="koboSpan" id="kobo.362.1">Executing this code yields the following diagram. </span><span class="koboSpan" id="kobo.362.2">Notice how the </span><strong class="bold"><span class="koboSpan" id="kobo.363.1">ReLU</span></strong><span class="koboSpan" id="kobo.364.1"> function takes advantage of both the linear and non-linear nature of activation functions, giving it the best of both worlds:</span></p>
			<div>
				<div id="_idContainer277" class="IMG---Figure">
					<span class="koboSpan" id="kobo.365.1"><img src="image/B17761_08_013.jpg" alt="Figure 8.13 – A simple ReLU function "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.366.1">Figure 8.13 – A simple ReLU function</span></p>
			<p><span class="koboSpan" id="kobo.367.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.368.1">ReLU</span></strong><span class="koboSpan" id="kobo.369.1"> activation function has become one of the most popular, if not </span><strong class="bold"><span class="koboSpan" id="kobo.370.1">the</span></strong><span class="koboSpan" id="kobo.371.1"> most popular, activation function among data scientists because of its ease of implementation, and its robust speed within </span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.372.1">the model development and training process. </span><strong class="bold"><span class="koboSpan" id="kobo.373.1">ReLU</span></strong><span class="koboSpan" id="kobo.374.1"> activation functions do, however, have their downsides. </span><span class="koboSpan" id="kobo.374.2">For example, the function cannot be differentiable when x = 0 (at point 0, 0), so </span><strong class="bold"><span class="koboSpan" id="kobo.375.1">gradient descent</span></strong><span class="koboSpan" id="kobo.376.1"> cannot be computed for that value. </span></p>
			<p><span class="koboSpan" id="kobo.377.1">Yet another activation function </span><a id="_idIndexMarker825"/><span class="koboSpan" id="kobo.378.1">worth mentioning is known as </span><strong class="bold"><span class="koboSpan" id="kobo.379.1">Softmax</span></strong><span class="koboSpan" id="kobo.380.1">. </span><strong class="bold"><span class="koboSpan" id="kobo.381.1">Softmax</span></strong><span class="koboSpan" id="kobo.382.1"> is very different from the other activation functions we have looked at so far because it computes a probability distribution for a list of values that are proportional to the relative scale of each of the values in the vector, the sum of which always equals 1. </span></p>
			<p><span class="koboSpan" id="kobo.383.1">Commonly </span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.384.1">used for </span><strong class="bold"><span class="koboSpan" id="kobo.385.1">multiclass</span></strong><span class="koboSpan" id="kobo.386.1"> classification, </span><strong class="bold"><span class="koboSpan" id="kobo.387.1">Softmax</span></strong><span class="koboSpan" id="kobo.388.1"> activation functions are used to normalize the outputs of a node, thus converting them from a weighted sum of values into </span><strong class="bold"><span class="koboSpan" id="kobo.389.1">probabilities</span></strong><span class="koboSpan" id="kobo.390.1">, with its probability being a member of a given class. </span><span class="koboSpan" id="kobo.390.2">We can demonstrate this in Python using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.391.1">numpy</span></strong><span class="koboSpan" id="kobo.392.1"> library:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.393.1">def softmax_function(x):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.394.1">    ex = np.exp(x - np.max(x))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.395.1">    return ex / ex.sum()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.396.1">x4 = [1, 2, 3, 4, 5]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.397.1">y4 = softmax_function(x4)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.398.1">print(y4)</span></p>
			<p><span class="koboSpan" id="kobo.399.1">Upon printing the values, we retrieve the following results:</span></p>
			<div>
				<div id="_idContainer278" class="IMG---Figure">
					<span class="koboSpan" id="kobo.400.1"><img src="image/B17761_08_014.jpg" alt="Figure 8.14 – Results of a Softmax function "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.401.1">Figure 8.14 – Results of a Softmax function</span></p>
			<p><span class="koboSpan" id="kobo.402.1">The two main advantages that come from using </span><strong class="bold"><span class="koboSpan" id="kobo.403.1">Softmax</span></strong><span class="koboSpan" id="kobo.404.1"> as an activation function are that the </span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.405.1">output values range between 0 and 1 and that they always sum to a value of 1.0. </span><span class="koboSpan" id="kobo.405.2">In return, this allows the function to be used to understand cross-entropy when it comes to the idea of divergence. </span><span class="koboSpan" id="kobo.405.3">We will visit this topic in more detail later in this chapter.</span></p>
			<p><span class="koboSpan" id="kobo.406.1">The various activation functions we have visited so far each have their pros and cons when it comes to using them in various applications. </span><span class="koboSpan" id="kobo.406.2">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.407.1">sigmoid</span></strong><span class="koboSpan" id="kobo.408.1"> functions are commonly used for binary and multilabel classification applications, whereas </span><strong class="bold"><span class="koboSpan" id="kobo.409.1">Softmax</span></strong><span class="koboSpan" id="kobo.410.1"> functions are generally used for multiclass classification. </span><span class="koboSpan" id="kobo.410.2">This is not a hard rule, but simply a guide to help you match a function with the highest chance of success with its respective application:</span></p>
			<div>
				<div id="_idContainer279" class="IMG---Figure">
					<span class="koboSpan" id="kobo.411.1"><img src="image/B17761_08_015.jpg" alt="Figure 8.15 – Activation functions by problem type "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.412.1">Figure 8.15 – Activation functions by problem type</span></p>
			<p><span class="koboSpan" id="kobo.413.1">Activation functions </span><a id="_idIndexMarker828"/><span class="koboSpan" id="kobo.414.1">are a vital part of any deep learning model and are often regarded as </span><em class="italic"><span class="koboSpan" id="kobo.415.1">game changers</span></em><span class="koboSpan" id="kobo.416.1"> since simply changing one function for another can boost the performance of a model quite drastically. </span><span class="koboSpan" id="kobo.416.2">We will take a closer look at how model performance can be quantified within the scope of deep learning in the following section.</span></p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor119"/><span class="koboSpan" id="kobo.417.1">Measuring progress with loss</span></h1>
			<p><span class="koboSpan" id="kobo.418.1">When we discussed the areas of classification and regression, we outlined a few measures </span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.419.1">to measure and quantify the performance of our models relative to one another. </span><span class="koboSpan" id="kobo.419.2">When it came to classification, we used </span><strong class="bold"><span class="koboSpan" id="kobo.420.1">precision</span></strong><span class="koboSpan" id="kobo.421.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.422.1">accuracy</span></strong><span class="koboSpan" id="kobo.423.1">, whereas, in regression, we used </span><strong class="bold"><span class="koboSpan" id="kobo.424.1">MAE</span></strong><span class="koboSpan" id="kobo.425.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.426.1">MSE</span></strong><span class="koboSpan" id="kobo.427.1">. </span><span class="koboSpan" id="kobo.427.2">Within the </span><a id="_idIndexMarker830"/><span class="koboSpan" id="kobo.428.1">confines of deep learning, we will use a metric known as </span><strong class="bold"><span class="koboSpan" id="kobo.429.1">loss</span></strong><span class="koboSpan" id="kobo.430.1">. </span><span class="koboSpan" id="kobo.430.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.431.1">loss</span></strong><span class="koboSpan" id="kobo.432.1"> of a neural network is simply a measure of the cost that's incurred from making an incorrect prediction. </span><span class="koboSpan" id="kobo.432.2">Take, for example, a simple neural network with three input values and a single output value:</span></p>
			<div>
				<div id="_idContainer280" class="IMG---Figure">
					<span class="koboSpan" id="kobo.433.1"><img src="image/B17761_08_016.jpg" alt="Figure 8.16 – A neural network showing input and output values "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.434.1">Figure 8.16 – A neural network showing input and output values</span></p>
			<p><span class="koboSpan" id="kobo.435.1">In this case, we have the values </span><em class="italic"><span class="koboSpan" id="kobo.436.1">[2.3, 3.3, 1.2]</span></em><span class="koboSpan" id="kobo.437.1"> being used as input values to the model, with a </span><a id="_idIndexMarker831"/><span class="koboSpan" id="kobo.438.1">predicted value of 0.2 relative to the actual value of 1.0. </span><span class="koboSpan" id="kobo.438.2">We can demonstrate the loss as follows:</span></p>
			<div>
				<div id="_idContainer281" class="IMG---Figure">
					<span class="koboSpan" id="kobo.439.1"><img src="image/Formula_B17761__08_011.jpg" alt=""/></span>
				</div>
			</div>
			<div>
				<div id="_idContainer282" class="IMG---Figure">
					<span class="koboSpan" id="kobo.440.1"><img src="image/Formula_B17761__08_012.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.441.1">In this function, </span><span class="koboSpan" id="kobo.442.1"><img src="image/Formula_B17761__08_013.png" alt=""/></span><span class="koboSpan" id="kobo.443.1">is the predicted value, while </span><span class="koboSpan" id="kobo.444.1"><img src="image/Formula_B17761__08_014.png" alt=""/></span><span class="koboSpan" id="kobo.445.1"> is the actual value.</span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.446.1">Empirical loss</span></strong><span class="koboSpan" id="kobo.447.1">, on the </span><a id="_idIndexMarker832"/><span class="koboSpan" id="kobo.448.1">other hand, is a measure of the total loss for the entirety of the dataset. </span><span class="koboSpan" id="kobo.448.2">We can represent empirical loss as follows:</span></p>
			<div>
				<div id="_idContainer285" class="IMG---Figure">
					<span class="koboSpan" id="kobo.449.1"><img src="image/Formula_B17761__08_015.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.450.1">In this function, we sum the total losses for all calculations.</span></p>
			<p><span class="koboSpan" id="kobo.451.1">Throughout the model training process, our main objective will be to minimize this loss in a process </span><a id="_idIndexMarker833"/><span class="koboSpan" id="kobo.452.1">known as </span><strong class="bold"><span class="koboSpan" id="kobo.453.1">loss optimization</span></strong><span class="koboSpan" id="kobo.454.1">. </span><span class="koboSpan" id="kobo.454.2">The main idea behind loss optimization is to identify a set of weights that help achieve the lowest loss possible. </span><span class="koboSpan" id="kobo.454.3">We </span><a id="_idIndexMarker834"/><span class="koboSpan" id="kobo.455.1">can visualize the idea of gradient descent as the process of moving from an initial starting value with a high loss, to a final value with a low loss. </span><span class="koboSpan" id="kobo.455.2">Our objective will be to ensure that converge in a global minimum rather than a local minimum, as depicted in the following diagram:</span></p>
			<div>
				<div id="_idContainer286" class="IMG---Figure">
					<span class="koboSpan" id="kobo.456.1"><img src="image/B17761_08_017.jpg" alt="Figure 8.17 – The process of loss optimization "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.457.1">Figure 8.17 – The process of loss optimization</span></p>
			<p><span class="koboSpan" id="kobo.458.1">Each step we take to get closer to the minimum value is known as a </span><strong class="bold"><span class="koboSpan" id="kobo.459.1">learning step</span></strong><span class="koboSpan" id="kobo.460.1">, the </span><strong class="bold"><span class="koboSpan" id="kobo.461.1">learning rate</span></strong><span class="koboSpan" id="kobo.462.1"> of which is generally determined by the user. </span><span class="koboSpan" id="kobo.462.2">This parameter is just one of many that we can specify using Keras, which we will learn about in the following section. </span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.463.1">Deep learning with Keras </span></h2>
			<p><span class="koboSpan" id="kobo.464.1">Within the realm of data science, the availability and use of various </span><strong class="bold"><span class="koboSpan" id="kobo.465.1">frameworks</span></strong><span class="koboSpan" id="kobo.466.1"> will always be </span><a id="_idIndexMarker835"/><span class="koboSpan" id="kobo.467.1">crucial to standardizing the methods we use to </span><a id="_idIndexMarker836"/><span class="koboSpan" id="kobo.468.1">develop and deploy models. </span><span class="koboSpan" id="kobo.468.2">So far, we have focused our </span><a id="_idIndexMarker837"/><span class="koboSpan" id="kobo.469.1">machine learning efforts on using the scikit-learn framework. </span><span class="koboSpan" id="kobo.469.2">Throughout this section, we will learn about three new frameworks specifically </span><a id="_idIndexMarker838"/><span class="koboSpan" id="kobo.470.1">focused on deep learning: </span><strong class="bold"><span class="koboSpan" id="kobo.471.1">Keras</span></strong><span class="koboSpan" id="kobo.472.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.473.1">TensorFlow</span></strong><span class="koboSpan" id="kobo.474.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.475.1">PyTorch</span></strong><span class="koboSpan" id="kobo.476.1">. </span><span class="koboSpan" id="kobo.476.2">These two </span><a id="_idIndexMarker839"/><span class="koboSpan" id="kobo.477.1">frameworks are the two most popular amongst data scientists </span><a id="_idIndexMarker840"/><span class="koboSpan" id="kobo.478.1">when it comes to developing various deep learning models as they offer a comprehensive list of APIs for numerous problems and use cases. </span></p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor121"/><span class="koboSpan" id="kobo.479.1">Understanding the differences between Keras and TensorFlow</span></h2>
			<p><span class="koboSpan" id="kobo.480.1">Although these two platforms allow users to develop deep learning models, there are a few differences </span><a id="_idIndexMarker841"/><span class="koboSpan" id="kobo.481.1">to know about. </span><span class="koboSpan" id="kobo.481.2">TensorFlow is known as an end-to-end </span><a id="_idIndexMarker842"/><span class="koboSpan" id="kobo.482.1">machine learning platform that offers a comprehensive list of libraries, tools, and numerous resources. </span><span class="koboSpan" id="kobo.482.2">Users can manage data, develop models, and </span><a id="_idIndexMarker843"/><span class="koboSpan" id="kobo.483.1">deploy solutions. </span><span class="koboSpan" id="kobo.483.2">Unlike most other libraries, </span><strong class="bold"><span class="koboSpan" id="kobo.484.1">TensorFlow</span></strong><span class="koboSpan" id="kobo.485.1"> offers both low and high levels of abstractions through their APIs, giving users lots of flexibility when it comes to developing models. </span><span class="koboSpan" id="kobo.485.2">On the other hand, </span><strong class="bold"><span class="koboSpan" id="kobo.486.1">Keras</span></strong><span class="koboSpan" id="kobo.487.1"> offers high-level APIs for developing neural networks, which run using </span><strong class="bold"><span class="koboSpan" id="kobo.488.1">TensorFlow</span></strong><span class="koboSpan" id="kobo.489.1">. </span><span class="koboSpan" id="kobo.489.2">The high-level nature of this library allows users to begin developing and training complex neural networks with only a few lines of Python code. </span><span class="koboSpan" id="kobo.489.3">Keras is generally regarded as user-friendly, modular, and extendable. </span><span class="koboSpan" id="kobo.489.4">A third library exists that is commonly used in </span><a id="_idIndexMarker844"/><span class="koboSpan" id="kobo.490.1">the deep learning space known as </span><strong class="bold"><span class="koboSpan" id="kobo.491.1">PyTorch</span></strong><span class="koboSpan" id="kobo.492.1">. </span><strong class="bold"><span class="koboSpan" id="kobo.493.1">PyTorch</span></strong><span class="koboSpan" id="kobo.494.1"> is a low-level API known for its remarkable speed and optimization in the model training process. </span><span class="koboSpan" id="kobo.494.2">The architectures within this library are generally complex and not appropriate for introductory material, so they are not within the scope of this book. </span><span class="koboSpan" id="kobo.494.3">However, it is worth mentioning as it is one of the most common libraries in the machine learning space that you will likely encounter. </span><span class="koboSpan" id="kobo.494.4">Let's take a closer look at all three:</span></p>
			<div>
				<div id="_idContainer287" class="IMG---Figure">
					<span class="koboSpan" id="kobo.495.1"><img src="image/013.jpg" alt="Figure 8.18 – A comparison between three of the most common deep learning frameworks "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.496.1">Figure 8.18 – A comparison between three of the most common deep learning frameworks</span></p>
			<p><span class="koboSpan" id="kobo.497.1">There are </span><a id="_idIndexMarker845"/><span class="koboSpan" id="kobo.498.1">pros and cons for each of these libraries and you should </span><a id="_idIndexMarker846"/><span class="koboSpan" id="kobo.499.1">select one of these libraries based on the task you set out to accomplish. </span><span class="koboSpan" id="kobo.499.2">Given that we are exploring the development of deep learning models for the first time, we will focus on using the Keras library.</span></p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor122"/><span class="koboSpan" id="kobo.500.1">Getting started with Keras and ANNs</span></h2>
			<p><span class="koboSpan" id="kobo.501.1">Before we </span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.502.1">move on to a full tutorial, let's look at an example of using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">Keras</span></strong><span class="koboSpan" id="kobo.504.1"> library since </span><a id="_idIndexMarker848"/><span class="koboSpan" id="kobo.505.1">we have not explored its functionality and code:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.506.1">First, we will need some sample data to use. </span><span class="koboSpan" id="kobo.506.2">Let's take advantage of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.507.1">make_blobs</span></strong><span class="koboSpan" id="kobo.508.1"> class in </span><strong class="source-inline"><span class="koboSpan" id="kobo.509.1">sklearn</span></strong><span class="koboSpan" id="kobo.510.1"> to create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.511.1">classification</span></strong><span class="koboSpan" id="kobo.512.1"> dataset. </span></li>
				<li><span class="koboSpan" id="kobo.513.1">We will specify the need for two classes (binary classification) and a cluster standard deviation of </span><strong class="source-inline"><span class="koboSpan" id="kobo.514.1">5</span></strong><span class="koboSpan" id="kobo.515.1"> to ensure that the two clusters overlap, making it a more difficult dataset to work with:</span><p class="source-code"><span class="koboSpan" id="kobo.516.1">from sklearn.datasets import make_blobs</span></p><p class="source-code"><span class="koboSpan" id="kobo.517.1">X, y = make_blobs(n_samples=2000, centers=2, n_features=4, random_state=1, cluster_std=5)</span></p></li>
				<li><span class="koboSpan" id="kobo.518.1">Next, we can scale the data using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.519.1">MinMaxScaler()</span></strong><span class="koboSpan" id="kobo.520.1"> class:</span><p class="source-code"><span class="koboSpan" id="kobo.521.1">from sklearn.preprocessing import MinMaxScaler</span></p><p class="source-code"><span class="koboSpan" id="kobo.522.1">scalar = MinMaxScaler()</span></p><p class="source-code"><span class="koboSpan" id="kobo.523.1">scalar.fit(X)</span></p><p class="source-code"><span class="koboSpan" id="kobo.524.1">X_scaled = scalar.transform(X)</span></p></li>
				<li><span class="koboSpan" id="kobo.525.1">Following this transformation, we can split the data into training and testing sets, similar to how we have done previously:</span><p class="source-code"><span class="koboSpan" id="kobo.526.1">from sklearn.model_selection import train_test_split</span></p><p class="source-code"><span class="koboSpan" id="kobo.527.1">X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25)</span></p></li>
				<li><span class="koboSpan" id="kobo.528.1">Let's go </span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.529.1">ahead and convert the array into a DataFrame to check </span><a id="_idIndexMarker850"/><span class="koboSpan" id="kobo.530.1">the first few rows of data beforehand:</span><p class="source-code"><span class="koboSpan" id="kobo.531.1">dfx_train = pd.DataFrame(X_train, columns=["Feature1", "Feature2", "Feature3", "Feature4"])</span></p><p class="source-code"><span class="koboSpan" id="kobo.532.1">dfx_train.head()</span></p><p><span class="koboSpan" id="kobo.533.1">This will render the following table:</span></p><div id="_idContainer288" class="IMG---Figure"><span class="koboSpan" id="kobo.534.1"><img src="image/B17761_08_019.jpg" alt="Figure 8.19 – An example of the data within the DataFrame of features "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.535.1">Figure 8.19 – An example of the data within the DataFrame of features</span></p></li>
				<li><span class="koboSpan" id="kobo.536.1">We can check the overlap of the two clusters by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.537.1">seaborn</span></strong><span class="koboSpan" id="kobo.538.1"> library to plot the first two of the four features of the training dataset:</span><p class="source-code"><span class="koboSpan" id="kobo.539.1">sns.scatterplot(x=dfx_train.Feature1, y=dfx_train.Feature2, hue=y_train)</span></p><p><span class="koboSpan" id="kobo.540.1"> The following diagram shows the preceding code's output:</span></p><div id="_idContainer289" class="IMG---Figure"><span class="koboSpan" id="kobo.541.1"><img src="image/B17761_08_020.jpg" alt="Figure 8.20 – A scatterplot of the dataset showing the overlapping nature of the two classes "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.542.1">Figure 8.20 – A scatterplot of the dataset showing the overlapping nature of the two classes</span></p><p><span class="koboSpan" id="kobo.543.1">Here, we can see that the data is quite well blended, making it difficult for some of the machine learning models we have explored to be able to separate the two classes with a high degree of </span><strong class="bold"><span class="koboSpan" id="kobo.544.1">accuracy</span></strong><span class="koboSpan" id="kobo.545.1">.</span></p></li>
				<li><span class="koboSpan" id="kobo.546.1">With the </span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.547.1">data ready, we can go ahead and use the Keras library. </span><span class="koboSpan" id="kobo.547.2">One </span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.548.1">of the most popular methods for setting up a model is using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.549.1">Sequential()</span></strong><span class="koboSpan" id="kobo.550.1"> class from </span><strong class="source-inline"><span class="koboSpan" id="kobo.551.1">Keras</span></strong><span class="koboSpan" id="kobo.552.1">. </span><span class="koboSpan" id="kobo.552.2">Let's go ahead and import the class and instantiate a new model:</span><p class="source-code"><span class="koboSpan" id="kobo.553.1">from keras.models import Sequential</span></p><p class="source-code"><span class="koboSpan" id="kobo.554.1">model = Sequential()</span></p></li>
				<li><span class="koboSpan" id="kobo.555.1">With the model now instantiated, we can add a new layer to our model using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">Dense</span></strong><span class="koboSpan" id="kobo.557.1"> class. </span><span class="koboSpan" id="kobo.557.2">We can also specify the number of </span><strong class="source-inline"><span class="koboSpan" id="kobo.558.1">nodes</span></strong><span class="koboSpan" id="kobo.559.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">4</span></strong><span class="koboSpan" id="kobo.561.1">), </span><strong class="source-inline"><span class="koboSpan" id="kobo.562.1">input_shape</span></strong><span class="koboSpan" id="kobo.563.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.564.1">4</span></strong><span class="koboSpan" id="kobo.565.1"> for the four features), </span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">activation</span></strong><span class="koboSpan" id="kobo.567.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">relu</span></strong><span class="koboSpan" id="kobo.569.1">), and a unique </span><strong class="source-inline"><span class="koboSpan" id="kobo.570.1">name</span></strong><span class="koboSpan" id="kobo.571.1"> for the layer:</span><p class="source-code"><span class="koboSpan" id="kobo.572.1">from keras.layers import Dense</span></p><p class="source-code"><span class="koboSpan" id="kobo.573.1">model.add(Dense(4, input_shape=(4,), activation='relu', name="DenseLayer1")) </span></p></li>
				<li><span class="koboSpan" id="kobo.574.1">To review </span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.575.1">the model we have built so far, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.576.1">summary()</span></strong><span class="koboSpan" id="kobo.577.1"> function:</span><p class="source-code"><span class="koboSpan" id="kobo.578.1">model.summary()</span></p><p><span class="koboSpan" id="kobo.579.1">This will </span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.580.1">give us some information and details about the model so far:</span></p><div id="_idContainer290" class="IMG---Figure"><span class="koboSpan" id="kobo.581.1"><img src="image/B17761_08_021.jpg" alt="Figure 8.21 – The sample output of the model's summary "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.582.1">Figure 8.21 – The sample output of the model's summary</span></p></li>
				<li><span class="koboSpan" id="kobo.583.1">We can add a few more layers to our model by simply using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">model.add()</span></strong><span class="koboSpan" id="kobo.585.1"> function again after the fact, perhaps even with a different number of nodes:</span><p class="source-code"><span class="koboSpan" id="kobo.586.1">model.add(Dense(8, activation='relu', name="DenseLayer2"))</span></p></li>
				<li><span class="koboSpan" id="kobo.587.1">Since we </span><a id="_idIndexMarker855"/><span class="koboSpan" id="kobo.588.1">are developing a </span><strong class="bold"><span class="koboSpan" id="kobo.589.1">binary classification</span></strong><span class="koboSpan" id="kobo.590.1"> model with two classes, </span><strong class="source-inline"><span class="koboSpan" id="kobo.591.1">0</span></strong><span class="koboSpan" id="kobo.592.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.593.1">1</span></strong><span class="koboSpan" id="kobo.594.1">, we can only have a single output value come from the model. </span><span class="koboSpan" id="kobo.594.2">Therefore, we will need to add one more layer that reduces the number of nodes from 8 to 1. </span><span class="koboSpan" id="kobo.594.3">In addition, we will change the activation to </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">sigmoid</span></strong><span class="koboSpan" id="kobo.596.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.597.1">model.add(Dense(1, activation='sigmoid', name="DenseLayer3"))</span></p></li>
				<li><span class="koboSpan" id="kobo.598.1">Now that our model's general architecture has been set up, we will need to use the compile function and specify our loss. </span><span class="koboSpan" id="kobo.598.2">Since we are creating a binary classifier, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.599.1">binary_crossentropy</span></strong><span class="koboSpan" id="kobo.600.1"> loss and specify accuracy as our main metric of interest:</span><p class="source-code"><span class="koboSpan" id="kobo.601.1">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=["accuracy"])</span></p><p><span class="koboSpan" id="kobo.602.1">With the model ready, let's use the summary function to check it once more:</span></p><div id="_idContainer291" class="IMG---Figure"><span class="koboSpan" id="kobo.603.1"><img src="image/B17761_08_022.jpg" alt="Figure 8.22 – The sample output of the model's summary "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.604.1">Figure 8.22 – The sample output of the model's summary</span></p><p><span class="koboSpan" id="kobo.605.1">So far, the </span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.606.1">model is quite simple. </span><span class="koboSpan" id="kobo.606.2">It will intake a dataset with four features in the first layer, expand that to eight nodes in the second layer, and </span><a id="_idIndexMarker857"/><span class="koboSpan" id="kobo.607.1">then reduce it down to a single output in the third layer. </span><span class="koboSpan" id="kobo.607.2">With the model all set, we can go ahead and train it. </span></p><p><span class="koboSpan" id="kobo.608.1">We can train a model by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.609.1">model.fit()</span></strong><span class="koboSpan" id="kobo.610.1"> function and by specifying the </span><strong class="source-inline"><span class="koboSpan" id="kobo.611.1">X_train</span></strong><span class="koboSpan" id="kobo.612.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.613.1">y_train</span></strong><span class="koboSpan" id="kobo.614.1"> sets. </span><span class="koboSpan" id="kobo.614.2">In addition, we will specify 50 </span><strong class="source-inline"><span class="koboSpan" id="kobo.615.1">epochs</span></strong><span class="koboSpan" id="kobo.616.1"> to train over. </span><strong class="bold"><span class="koboSpan" id="kobo.617.1">Epochs</span></strong><span class="koboSpan" id="kobo.618.1"> are simply </span><a id="_idIndexMarker858"/><span class="koboSpan" id="kobo.619.1">the number of passes or iterations. </span><span class="koboSpan" id="kobo.619.2">We can also control the verbosity of the model, allowing us to control the amount of output data we want to see in the training process.</span></p></li>
				<li><span class="koboSpan" id="kobo.620.1">Recall that, in our earlier machine learning models, we only used the training data to train the model and kept the testing data to test the model after the training was completed. </span><span class="koboSpan" id="kobo.620.2">We will use the same methodology here as well; however, we will take advantage of the high-level nature of </span><strong class="bold"><span class="koboSpan" id="kobo.621.1">Keras</span></strong><span class="koboSpan" id="kobo.622.1"> and specify a </span><strong class="source-inline"><span class="koboSpan" id="kobo.623.1">validation split</span></strong><span class="koboSpan" id="kobo.624.1"> to be used in the training process. </span><span class="koboSpan" id="kobo.624.2">Deep learning models will almost always overfit your data. </span><span class="koboSpan" id="kobo.624.3">Using a validation split in the training process can help mitigate this: </span><p class="source-code"><span class="koboSpan" id="kobo.625.1">history = model.fit(X_train, y_train, epochs=50, verbose=1, validation_split=0.2)</span></p><p><span class="koboSpan" id="kobo.626.1">As the model begins the training process, it will begin to produce the following output. </span><span class="koboSpan" id="kobo.626.2">You </span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.627.1">can monitor the performance here by looking at the </span><a id="_idIndexMarker860"/><span class="koboSpan" id="kobo.628.1">number of </span><strong class="bold"><span class="koboSpan" id="kobo.629.1">epochs</span></strong><span class="koboSpan" id="kobo.630.1"> on the left and the </span><strong class="bold"><span class="koboSpan" id="kobo.631.1">metrics</span></strong><span class="koboSpan" id="kobo.632.1"> on the right. </span><span class="koboSpan" id="kobo.632.2">When training a model, our objective is to ensure that the </span><strong class="bold"><span class="koboSpan" id="kobo.633.1">loss</span></strong><span class="koboSpan" id="kobo.634.1"> metric is constantly decreasing, whereas the accuracy is increasing. </span><span class="koboSpan" id="kobo.634.2">We can see an example of this in the following screenshot:</span></p><div id="_idContainer292" class="IMG---Figure"><span class="koboSpan" id="kobo.635.1"><img src="image/B17761_08_023.jpg" alt="Figure 8.23 – A sample of a model's output "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.636.1">Figure 8.23 – A sample of a model's output</span></p></li>
				<li><span class="koboSpan" id="kobo.637.1">With the model trained, let's quickly examine the classification metrics, as we did previously, to get a sense of the performance. </span><span class="koboSpan" id="kobo.637.2">We can begin by making predictions using the testing data and using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.638.1">classification_report</span></strong><span class="koboSpan" id="kobo.639.1"> to calculate our metric. </span><span class="koboSpan" id="kobo.639.2">Note that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.640.1">predict()</span></strong><span class="koboSpan" id="kobo.641.1"> method does not return a class but </span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.642.1">a probability that needs to be rounded to either </span><strong class="source-inline"><span class="koboSpan" id="kobo.643.1">0</span></strong><span class="koboSpan" id="kobo.644.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.645.1">1</span></strong><span class="koboSpan" id="kobo.646.1">, given that this is a </span><strong class="bold"><span class="koboSpan" id="kobo.647.1">binary classification</span></strong><span class="koboSpan" id="kobo.648.1"> problem:</span><p class="source-code"><span class="koboSpan" id="kobo.649.1">y_pred = (model.predict(X_test) &gt; 0.5).astype("int32").ravel()</span></p><p class="source-code"><span class="koboSpan" id="kobo.650.1">from sklearn.metrics import classification_report</span></p><p class="source-code"><span class="koboSpan" id="kobo.651.1">print(classification_report(y_pred, y_test))</span></p><p><span class="koboSpan" id="kobo.652.1">Upon printing the report, we will get the following results:</span></p><div id="_idContainer293" class="IMG---Figure"><span class="koboSpan" id="kobo.653.1"><img src="image/B17761_08_024.jpg" alt="Figure 8.24 – The results of the model "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.654.1">Figure 8.24 – The results of the model</span></p></li>
				<li><span class="koboSpan" id="kobo.655.1">We can see that the </span><strong class="bold"><span class="koboSpan" id="kobo.656.1">precision</span></strong><span class="koboSpan" id="kobo.657.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.658.1">recall</span></strong><span class="koboSpan" id="kobo.659.1"> are quite high – not too bad for our first attempt! </span><span class="koboSpan" id="kobo.659.2">We can take a look at the model's performance from a different perspective </span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.660.1">using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.661.1">history</span></strong><span class="koboSpan" id="kobo.662.1"> variable, which contains the model's </span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.663.1">training history:</span><p class="source-code"><span class="koboSpan" id="kobo.664.1">fig = plt.figure(figsize=(10,10))</span></p><p class="source-code"><span class="koboSpan" id="kobo.665.1"># total_rows, total_columns, subplot_index(1st, 2nd, etc..)</span></p><p class="source-code"><span class="koboSpan" id="kobo.666.1">plt.subplot(2, 2, 1)</span></p><p class="source-code"><span class="koboSpan" id="kobo.667.1">plt.title("Accuracy", fontsize=15)</span></p><p class="source-code"><span class="koboSpan" id="kobo.668.1">plt.xlabel("Epochs", fontsize=15)</span></p><p class="source-code"><span class="koboSpan" id="kobo.669.1">plt.ylabel("Accuracy (%)", fontsize=15)</span></p><p class="source-code"><span class="koboSpan" id="kobo.670.1">plt.plot(history.history["val_accuracy"], label='Validation Accuracy', linestyle='dashed')</span></p><p class="source-code"><span class="koboSpan" id="kobo.671.1">plt.plot(history.history["accuracy"], label='Training Accuracy')</span></p><p class="source-code"><span class="koboSpan" id="kobo.672.1">plt.legend(["Validation", "Training"], loc="lower right")</span></p><p class="source-code"><span class="koboSpan" id="kobo.673.1">plt.subplot(2, 2, 2)</span></p><p class="source-code"><span class="koboSpan" id="kobo.674.1">plt.title("Loss", fontsize=15)</span></p><p class="source-code"><span class="koboSpan" id="kobo.675.1">plt.xlabel("Epochs", fontsize=15)</span></p><p class="source-code"><span class="koboSpan" id="kobo.676.1">plt.ylabel("Loss", fontsize=15)</span></p><p class="source-code"><span class="koboSpan" id="kobo.677.1">plt.plot(history.history["val_loss"], label='Validation loss', linestyle='dashed')</span></p><p class="source-code"><span class="koboSpan" id="kobo.678.1">plt.plot(history.history["loss"], label='Training loss')</span></p><p class="source-code"><span class="koboSpan" id="kobo.679.1">plt.legend(["Validation", "Training"], loc="lower left")</span></p><p><span class="koboSpan" id="kobo.680.1">Upon executing </span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.681.1">this code, we will receive the following diagram, which </span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.682.1">shows the change in accuracy and loss over the course of the model training process:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer294" class="IMG---Figure">
					<span class="koboSpan" id="kobo.683.1"><img src="image/B17761_08_025.jpg" alt="Figure 8.25 – The accuracy and loss of the model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.684.1">Figure 8.25 – The accuracy and loss of the model</span></p>
			<p><span class="koboSpan" id="kobo.685.1">When training a model, recall that the main objective is to ensure that the loss is decreasing, and never increasing over time. </span><span class="koboSpan" id="kobo.685.2">In addition, our secondary objective is to ensure that the accuracy of our model slowly and steadily increases. </span><span class="koboSpan" id="kobo.685.3">When trying to diagnose a model that is not performing well, the first step is to generate graphs such as these to get a sense of any potential problems before altering the model in an attempt to improve the metrics.</span></p>
			<p><span class="koboSpan" id="kobo.686.1">When we worked with most of the machine learning models in the previous chapters, we learned that we could alter these metrics by doing the following:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.687.1">Improving our data preprocessing.</span></li>
				<li><span class="koboSpan" id="kobo.688.1">Tuning our hyperparameters or changing the model. </span><span class="koboSpan" id="kobo.688.2">Within the confines of deep learning, in addition to the options mentioned previously, there are a few more tools we have to change to suit our needs. </span><span class="koboSpan" id="kobo.688.3">For instance, we can change the overall architecture by adding or removing layers and nodes. </span><span class="koboSpan" id="kobo.688.4">In addition, we can change the activation functions within each of the layers to whatever would complement our problem statement the most. </span><span class="koboSpan" id="kobo.688.5">We can also change the optimizer or the learning rate of our optimizer (Adam, in this model). </span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.689.1">With so many </span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.690.1">changes that can be made that could have high impacts on </span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.691.1">any given model, we will need to organize our work. </span><span class="koboSpan" id="kobo.691.2">We could either create numerous models and record our metrics manually in a spreadsheet, or we could take advantage of a library specifically designed to handle use cases such as these: </span><strong class="bold"><span class="koboSpan" id="kobo.692.1">MLflow</span></strong><span class="koboSpan" id="kobo.693.1">. </span><span class="koboSpan" id="kobo.693.2">We will take a closer look at </span><strong class="bold"><span class="koboSpan" id="kobo.694.1">MLflow</span></strong><span class="koboSpan" id="kobo.695.1"> in the next section.</span></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor123"/><span class="koboSpan" id="kobo.696.1">Tutorial – protein sequence classification via LSTMs using Keras and MLflow</span></h1>
			<p><span class="koboSpan" id="kobo.697.1">Deep </span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.698.1">learning has gained a surge </span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.699.1">of popularity </span><a id="_idIndexMarker870"/><span class="koboSpan" id="kobo.700.1">in recent </span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.701.1">years, prompting many scientists to turn to the field as a new means for solving and optimizing scientific problems. </span><span class="koboSpan" id="kobo.701.2">One of the most popular applications for deep learning within the biotechnology space involves </span><strong class="bold"><span class="koboSpan" id="kobo.702.1">protein sequence</span></strong><span class="koboSpan" id="kobo.703.1"> data. </span><span class="koboSpan" id="kobo.703.2">So far within this book, we have focused our efforts on developing predictive models when it comes to </span><strong class="bold"><span class="koboSpan" id="kobo.704.1">structured</span></strong><span class="koboSpan" id="kobo.705.1"> data. </span><span class="koboSpan" id="kobo.705.2">We will now turn our attention to data that's </span><strong class="bold"><span class="koboSpan" id="kobo.706.1">sequential</span></strong><span class="koboSpan" id="kobo.707.1"> in the sense that the elements within a sequence bear some relation to their previous element. </span><span class="koboSpan" id="kobo.707.2">Within this tutorial, we will attempt to develop a protein </span><strong class="bold"><span class="koboSpan" id="kobo.708.1">sequence classification</span></strong><span class="koboSpan" id="kobo.709.1"> model in which we will classify protein sequences </span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.710.1">based on their known family accession using the </span><strong class="bold"><span class="koboSpan" id="kobo.711.1">Pfam</span></strong><span class="koboSpan" id="kobo.712.1"> (</span><a href="https://pfam.xfam.org/"><span class="koboSpan" id="kobo.713.1">https://pfam.xfam.org/</span></a><span class="koboSpan" id="kobo.714.1">) dataset. </span></p>
			<p class="callout-heading"><span class="koboSpan" id="kobo.715.1">Important note</span></p>
			<p class="callout"><strong class="source-inline"><span class="koboSpan" id="kobo.716.1">Pfam</span></strong><span class="koboSpan" id="kobo.717.1"> dataset: Pfam: The </span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.718.1">protein families database in 2021 J. </span><span class="koboSpan" id="kobo.718.2">Mistry, S. </span><span class="koboSpan" id="kobo.718.3">Chuguransky, L. </span><span class="koboSpan" id="kobo.718.4">Williams, M. </span><span class="koboSpan" id="kobo.718.5">Qureshi, G.A. </span><span class="koboSpan" id="kobo.718.6">Salazar, E.L.L. </span><span class="koboSpan" id="kobo.718.7">Sonnhammer, S.C.E. </span><span class="koboSpan" id="kobo.718.8">Tosatto, L. </span><span class="koboSpan" id="kobo.718.9">Paladin, S. </span><span class="koboSpan" id="kobo.718.10">Raj, L.J. </span><span class="koboSpan" id="kobo.718.11">Richardson, R.D. </span><span class="koboSpan" id="kobo.718.12">Finn, A. </span><span class="koboSpan" id="kobo.718.13">BatemanNucleic Acids Research (2020) doi: 10.1093/nar/gkaa913 (</span><strong class="source-inline"><span class="koboSpan" id="kobo.719.1">Pfam: The protein families database in 2021</span></strong><span class="koboSpan" id="kobo.720.1">).</span></p>
			<p><span class="koboSpan" id="kobo.721.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.722.1">Pfam</span></strong><span class="koboSpan" id="kobo.723.1"> dataset consists of several columns, as follows:</span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.724.1">Family_id</span></strong><span class="koboSpan" id="kobo.725.1">: The name </span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.726.1">of the family that the sequence belongs to (for example, filamin)</span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.727.1">Family Accession</span></strong><span class="koboSpan" id="kobo.728.1">: The </span><a id="_idIndexMarker875"/><span class="koboSpan" id="kobo.729.1">class or output that our model will aim to predict</span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.730.1">Sequence</span></strong><span class="koboSpan" id="kobo.731.1">: The </span><a id="_idIndexMarker876"/><span class="koboSpan" id="kobo.732.1">amino acid sequence we will use as input for our model</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.733.1">Throughout this tutorial, we will use the sequence data to develop several predictive models </span><a id="_idIndexMarker877"/><span class="koboSpan" id="kobo.734.1">to determine </span><a id="_idIndexMarker878"/><span class="koboSpan" id="kobo.735.1">each sequence's </span><a id="_idIndexMarker879"/><span class="koboSpan" id="kobo.736.1">associated family accession. </span><span class="koboSpan" id="kobo.736.2">The sequences are in their raw state with different lengths and </span><a id="_idIndexMarker880"/><span class="koboSpan" id="kobo.737.1">sizes. </span><span class="koboSpan" id="kobo.737.2">We will need to pre-process the data and structure it in such a way as to prepare it for sequence classification. </span><span class="koboSpan" id="kobo.737.3">When it comes to the labels, we will develop a model using a </span><strong class="bold"><span class="koboSpan" id="kobo.738.1">balanced</span></strong><span class="koboSpan" id="kobo.739.1"> set of different labels to ensure the model does not learn any particular bias. </span></p>
			<p><span class="koboSpan" id="kobo.740.1">As we begin to develop our ideal classification model, we will need to alter the many possible parameters to maximize the performance. </span><span class="koboSpan" id="kobo.740.2">To keep track of these changes, we will make </span><a id="_idIndexMarker881"/><span class="koboSpan" id="kobo.741.1">use of the MLflow (</span><a href="https://mlflow.org"><span class="koboSpan" id="kobo.742.1">https://mlflow.org</span></a><span class="koboSpan" id="kobo.743.1">) library. </span><span class="koboSpan" id="kobo.743.2">There are four main </span><a id="_idIndexMarker882"/><span class="koboSpan" id="kobo.744.1">components within </span><strong class="bold"><span class="koboSpan" id="kobo.745.1">MLflow</span></strong><span class="koboSpan" id="kobo.746.1">: </span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.747.1">MLflow Tracking</span></strong><span class="koboSpan" id="kobo.748.1">: Allows users to record and query experiments</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.749.1">MLflow Projects</span></strong><span class="koboSpan" id="kobo.750.1">: Packages data science code</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.751.1">MLflow Models</span></strong><span class="koboSpan" id="kobo.752.1">: Deploys trained machine learning models</span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.753.1">MLflow Registry</span></strong><span class="koboSpan" id="kobo.754.1">: Stores and manages your models</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.755.1">Within this tutorial, we will explore how to use MLflow tracking to track and manage the development of a protein sequence classification model. </span><span class="koboSpan" id="kobo.755.2">With these items in mind, let's get started.</span></p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor124"/><span class="koboSpan" id="kobo.756.1">Importing the necessary libraries and datasets</span></h2>
			<p><span class="koboSpan" id="kobo.757.1">We will </span><a id="_idIndexMarker883"/><span class="koboSpan" id="kobo.758.1">begin with the standard set of library imports, followed by the dataset in the format of a CSV document: </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.759.1">import pandas as pd</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.760.1">import numpy as np</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.761.1">from tensorflow.keras.utils import to_categorical</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.762.1">import matplotlib.pyplot as plt</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.763.1">import seaborn as sns</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.764.1">sns.set_style("darkgrid")</span></p>
			<p><span class="koboSpan" id="kobo.765.1">With the libraries now imported, we can import the dataset as well. </span><span class="koboSpan" id="kobo.765.2">We will begin by specifying the path, and then concatenate the dataset using a </span><strong class="source-inline"><span class="koboSpan" id="kobo.766.1">for</span></strong><span class="koboSpan" id="kobo.767.1"> loop:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.768.1">PATH = "../../../datasets/dataset_pfam/"</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.769.1">files = []</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.770.1">for i in range(8):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.771.1">    df = pd.read_csv(PATH+f"dataset_pfam_seq_sd{i+1}.csv", index_col=None, header=0)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.772.1">    files.append(df)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.773.1">    </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.774.1">df = pd.concat(files, axis=0, ignore_index=True)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.775.1">df.shape</span></p>
			<p><span class="koboSpan" id="kobo.776.1">Upon </span><a id="_idIndexMarker884"/><span class="koboSpan" id="kobo.777.1">importing the dataset, we immediately notice that it contains five columns and ~1.3 million rows of data – slightly larger than what we have worked with so far. </span><span class="koboSpan" id="kobo.777.2">We can take a quick glimpse at the dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.778.1">.head()</span></strong><span class="koboSpan" id="kobo.779.1"> function:</span></p>
			<div>
				<div id="_idContainer295" class="IMG---Figure">
					<span class="koboSpan" id="kobo.780.1"><img src="image/B17761_08_026.jpg" alt="Figure 8.26 – A sample of the data from the protein sequence dataset "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.781.1">Figure 8.26 – A sample of the data from the protein sequence dataset</span></p>
			<p><span class="koboSpan" id="kobo.782.1">With the dataset successfully imported, let's go ahead and explore the dataset in more detail.</span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor125"/><span class="koboSpan" id="kobo.783.1">Checking the dataset</span></h2>
			<p><span class="koboSpan" id="kobo.784.1">We can </span><a id="_idIndexMarker885"/><span class="koboSpan" id="kobo.785.1">confirm the completeness of the data in this DataFrame using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.786.1">isna()</span></strong><span class="koboSpan" id="kobo.787.1"> function, followed by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.788.1">sum()</span></strong><span class="koboSpan" id="kobo.789.1"> function to summarize by column:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.790.1">df.isna().sum()</span></p>
			<p><span class="koboSpan" id="kobo.791.1">Now, let's take a closer look at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.792.1">family_accession</span></strong><span class="koboSpan" id="kobo.793.1"> column (our model's output) of this dataset. </span><span class="koboSpan" id="kobo.793.2">We can check the total number of instances by grouping the column and using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.794.1">value_counts()</span></strong><span class="koboSpan" id="kobo.795.1"> function, followed by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.796.1">n_largest()</span></strong><span class="koboSpan" id="kobo.797.1"> function, to get the top 10 most common entries in this column:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.798.1">df["family_accession"].groupby(df["family_accession"]).value_counts().nlargest(10)</span></p>
			<p><span class="koboSpan" id="kobo.799.1">Grouping the data will yield the following results:</span></p>
			<div>
				<div id="_idContainer296" class="IMG---Figure">
					<span class="koboSpan" id="kobo.800.1"><img src="image/B17761_08_027.jpg" alt="Figure 8.27 – A summary of the classes in the dataset with value counts higher than 1,200 "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.801.1">Figure 8.27 – A summary of the classes in the dataset with value counts higher than 1,200</span></p>
			<p><span class="koboSpan" id="kobo.802.1">Here, we can see that 1,500 entries seems to be the cutoff point for the top 10 values. </span><span class="koboSpan" id="kobo.802.2">We can </span><a id="_idIndexMarker886"/><span class="koboSpan" id="kobo.803.1">also take a closer look at the sequence column (our model's input) by getting a sense of the average lengths of the sequences. </span><span class="koboSpan" id="kobo.803.2">We can plot the count of each sequence length using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.804.1">displot()</span></strong><span class="koboSpan" id="kobo.805.1"> function from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.806.1">seaborn</span></strong><span class="koboSpan" id="kobo.807.1"> library:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.808.1">sns.displot(df["sequence"].apply(lambda x: len(x)), bins=75, height=4, aspect=2) </span></p>
			<p><span class="koboSpan" id="kobo.809.1">Executing this code will yield the following results:</span></p>
			<div>
				<div id="_idContainer297" class="IMG---Figure">
					<span class="koboSpan" id="kobo.810.1"><img src="image/B17761_08_028.jpg" alt="Figure 8.28 – A histogram of the counts of the sequence lengths in the dataset "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.811.1">Figure 8.28 – A histogram of the counts of the sequence lengths in the dataset</span></p>
			<p><span class="koboSpan" id="kobo.812.1">From this graph, as well as by using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.813.1">mean()</span></strong><span class="koboSpan" id="kobo.814.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.815.1">median()</span></strong><span class="koboSpan" id="kobo.816.1"> functions, we can see that the average and most common lengths are approximately 155 and 100 amino acids. </span><span class="koboSpan" id="kobo.816.2">We will use these numbers later when determining what the cutoff should be for the input sequences.</span></p>
			<p><span class="koboSpan" id="kobo.817.1">Now that we have gained a better sense of the data, it is time to prepare the dataset for our classification models. </span><span class="koboSpan" id="kobo.817.2">We could theoretically train the model on the dataset as a whole without limits – however, the models would require a much longer duration to train. </span><span class="koboSpan" id="kobo.817.3">In addition, by training across all the data without accounting for balance, we may introduce </span><a id="_idIndexMarker887"/><span class="koboSpan" id="kobo.818.1">bias within the model. </span><span class="koboSpan" id="kobo.818.2">To mitigate both of these situations, let's reduce this dataset by filtering for the classifications with at least 1,200 </span><strong class="bold"><span class="koboSpan" id="kobo.819.1">observations</span></strong><span class="koboSpan" id="kobo.820.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.821.1">df_filt = df.groupby("family_accession").filter(lambda x: len(x) &gt; 1200)</span></p>
			<p><span class="koboSpan" id="kobo.822.1">Given that some classes have significantly more than 1,200 observations, we can randomly select exactly 1,200 observations using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.823.1">sample()</span></strong><span class="koboSpan" id="kobo.824.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.825.1">df_bal = df_filt.groupby('family_accession').apply(lambda x: x.sample(1200))</span></p>
			<p><span class="koboSpan" id="kobo.826.1">We can now check the filtered and balanced dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.827.1">head()</span></strong><span class="koboSpan" id="kobo.828.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.829.1">df_red = df_bal[["family_accession", "sequence"]].reset_index(drop=True)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.830.1">df_red.head()</span></p>
			<p><span class="koboSpan" id="kobo.831.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.832.1">head()</span></strong><span class="koboSpan" id="kobo.833.1"> function will yield the following results:</span></p>
			<div>
				<div id="_idContainer298" class="IMG---Figure">
					<span class="koboSpan" id="kobo.834.1"><img src="image/B17761_08_029.jpg" alt="Figure 8.29 – A sample of the data in the form of a DataFrame "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.835.1">Figure 8.29 – A sample of the data in the form of a DataFrame</span></p>
			<p><span class="koboSpan" id="kobo.836.1">We can </span><a id="_idIndexMarker888"/><span class="koboSpan" id="kobo.837.1">check the number of classes we will have in this dataset by checking the length of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.838.1">value_counts()</span></strong><span class="koboSpan" id="kobo.839.1"> function:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.840.1">num_classes = len(df_red.family_accession.value_counts())</span></p>
			<p><span class="koboSpan" id="kobo.841.1">If we check the </span><strong class="source-inline"><span class="koboSpan" id="kobo.842.1">num_classes</span></strong><span class="koboSpan" id="kobo.843.1"> variable, we will see that we have 28 possible classes in total.</span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.844.1">Splitting the dataset</span></h2>
			<p><span class="koboSpan" id="kobo.845.1">With the </span><a id="_idIndexMarker889"/><span class="koboSpan" id="kobo.846.1">data prepared, our next step will be to split the dataset into training, testing, and validation sets. </span><span class="koboSpan" id="kobo.846.2">We will once again make use of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.847.1">train_test_split</span></strong><span class="koboSpan" id="kobo.848.1"> function from </span><strong class="source-inline"><span class="koboSpan" id="kobo.849.1">sklearn</span></strong><span class="koboSpan" id="kobo.850.1"> to accomplish this:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.851.1">from sklearn.model_selection import train_test_split</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.852.1">X_train, X_test = train_test_split(df_red, test_size=0.25)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.853.1">X_val, X_test = train_test_split(X_test, test_size=0.50)</span></p>
			<p><span class="koboSpan" id="kobo.854.1">With the data now split, let's go ahead and preprocess it.</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.855.1">Preprocessing the data</span></h2>
			<p><span class="koboSpan" id="kobo.856.1">With the data split up, we need to preprocess the datasets to use on our neural network models. </span><span class="koboSpan" id="kobo.856.2">First, we </span><a id="_idIndexMarker890"/><span class="koboSpan" id="kobo.857.1">will need to reduce the sequences down to the 20 most common amino acids and convert the sequences into integers. </span><span class="koboSpan" id="kobo.857.2">This will speed up the training process. </span><span class="koboSpan" id="kobo.857.3">First, we will create a dictionary of amino acids that contains their corresponding values:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.858.1">aa_seq_dict = {'A': 1,'C': 2,'D': 3,'E': 4,'F': 5,'G': 6,'H': 7,'I': 8,'K': 9,'L': 10,'M': 11,'N': 12,'P': 13,'Q': 14,'R': 15,'S': 16,'T': 17,'V': 18,'W': 19,'Y': 20}</span></p>
			<p><span class="koboSpan" id="kobo.859.1">Next, we can iterate over the sequences and convert the string values into their corresponding integers. </span><span class="koboSpan" id="kobo.859.2">Note that we will complete this for the training, testing, and validation sets:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.860.1">def aa_seq_encoder(data):</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.861.1">    full_sequence_list = []</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.862.1">    for i in data['sequence'].values:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.863.1">        row_sequence_list = []</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.864.1">        for j in i:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.865.1">            row_sequence_list.append(aa_seq_dict.get(j, 0))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.866.1">        full_sequence_list.append(np.array(row_sequence_list))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.867.1">    return full_sequence_list</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.868.1">  </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.869.1">X_train_encode = aa_seq_encoder(X_train) </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.870.1">X_val_encode = aa_seq_encoder(X_val) </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.871.1">X_test_encode = aa_seq_encoder(X_test)</span></p>
			<p><span class="koboSpan" id="kobo.872.1">Next, we will </span><a id="_idIndexMarker891"/><span class="koboSpan" id="kobo.873.1">need to pad the sequences to ensure they are all of an equal length. </span><span class="koboSpan" id="kobo.873.2">To accomplish this, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.874.1">pad_sequences</span></strong><span class="koboSpan" id="kobo.875.1"> function from </span><strong class="source-inline"><span class="koboSpan" id="kobo.876.1">keras</span></strong><span class="koboSpan" id="kobo.877.1">. </span><span class="koboSpan" id="kobo.877.2">We will specify </span><strong class="source-inline"><span class="koboSpan" id="kobo.878.1">max_length</span></strong><span class="koboSpan" id="kobo.879.1"> for each of the sequences as 100, given that it approximates the median value we saw earlier. </span><span class="koboSpan" id="kobo.879.2">In addition, we will pad the sequences with </span><strong class="source-inline"><span class="koboSpan" id="kobo.880.1">'post'</span></strong><span class="koboSpan" id="kobo.881.1"> to ensure that we pad them at the end instead of the front: </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.882.1">from keras.preprocessing.sequence import pad_sequences</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.883.1">max_length = 100</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.884.1">X_train_padded = pad_sequences(X_train_encode, maxlen=max_length, padding='post', truncating='post')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.885.1">X_val_padded = pad_sequences(X_val_encode, maxlen=max_length, padding='post', truncating='post')</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.886.1">X_test_padded = pad_sequences(X_test_encode, maxlen=max_length, padding='post', truncating='post')</span></p>
			<p><span class="koboSpan" id="kobo.887.1">We can take a quick glance of the changes we have made using one of the sequences. </span><span class="koboSpan" id="kobo.887.2">First, we have the raw sequence as a </span><strong class="source-inline"><span class="koboSpan" id="kobo.888.1">string</span></strong><span class="koboSpan" id="kobo.889.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.890.1">X_train.sequence[1][:30]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.891.1">'LRDLRHFLAVAEEGHIGRAAARLHLSQPPL'</span></p>
			<p><span class="koboSpan" id="kobo.892.1">Next, we can encode the sequence to remove uncommon amino acids and convert the string to a list of integers:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.893.1">X_train_encode[1][:30]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.894.1">array([ 7, 10, 15, 18, 10,  3, 18, 16, 14, 17, 15,  5, 12, 10,  7, 16, 15, 12, 12,  8, 18,  4, 14,  5, 17,  4,  2])</span></p>
			<p><span class="koboSpan" id="kobo.895.1">Finally, we can </span><a id="_idIndexMarker892"/><span class="koboSpan" id="kobo.896.1">limit the lengths of the sequences to either truncate them at 100 elements or </span><strong class="bold"><span class="koboSpan" id="kobo.897.1">pad</span></strong><span class="koboSpan" id="kobo.898.1"> them with zeros to reach 100 elements:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.899.1">X_train_padded[1][:30]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.900.1">array([ 7, 10, 15, 18, 10,  3, 18, 16, 14, 17, 15,  5, 12, 10,  7, 16, 15, 12, 12,  8, 18,  4, 14,  5, 17,  4,  2,  0,  0,  0])</span></p>
			<p><span class="koboSpan" id="kobo.901.1">Now that we have preprocessed the input data, we will need to preprocess the output values as well. </span><span class="koboSpan" id="kobo.901.2">We can do so using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.902.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.903.1"> class from </span><strong class="source-inline"><span class="koboSpan" id="kobo.904.1">sklearn</span></strong><span class="koboSpan" id="kobo.905.1">. </span><span class="koboSpan" id="kobo.905.2">Our main objective here will be to transform the values from a list of labels in a dataframe column into an encoded list:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.906.1">from sklearn.preprocessing import LabelEncoder</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.907.1">le = LabelEncoder()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.908.1">y_train_enc = le.fit_transform(X_train['family_accession'])</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.909.1">y_val_enc = le.transform(X_val['family_accession'])</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.910.1">y_test_enc = le.transform(X_test['family_accession'])</span></p>
			<p><span class="koboSpan" id="kobo.911.1">Finally, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.912.1">to_categorical</span></strong><span class="koboSpan" id="kobo.913.1"> function from </span><strong class="source-inline"><span class="koboSpan" id="kobo.914.1">sklearn</span></strong><span class="koboSpan" id="kobo.915.1"> to transform a class vector into a binary class matrix:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.916.1">from tensorflow.keras.utils import to_categorical</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.917.1">y_train = to_categorical(y_train_enc)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.918.1">y_val = to_categorical(y_val_enc)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.919.1">y_test = to_categorical(y_test_enc)</span></p>
			<p><span class="koboSpan" id="kobo.920.1">To review the changes we've made here, we can use a single column in a </span><strong class="source-inline"><span class="koboSpan" id="kobo.921.1">DataFrame</span></strong><span class="koboSpan" id="kobo.922.1">:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.923.1">X_train['family_accession']</span></p>
			<p><span class="koboSpan" id="kobo.924.1">We can </span><a id="_idIndexMarker893"/><span class="koboSpan" id="kobo.925.1">see the results of this in the following diagram:</span></p>
			<div>
				<div id="_idContainer299" class="IMG---Figure">
					<span class="koboSpan" id="kobo.926.1"><img src="image/B17761_08_030.jpg" alt="Figure 8.30 – A list of the classes "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.927.1">Figure 8.30 – A list of the classes</span></p>
			<p><span class="koboSpan" id="kobo.928.1">Next, we must encode the classes into a list of numerical values, with each value representing a specific class: </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.929.1">y_train_enc</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.930.1">array([ 4,  3, 21, ..., 10, 15, 12], dtype=int64)</span></p>
			<p><span class="koboSpan" id="kobo.931.1">Finally, we must </span><a id="_idIndexMarker894"/><span class="koboSpan" id="kobo.932.1">convert the structure into a </span><strong class="bold"><span class="koboSpan" id="kobo.933.1">binary class matrix</span></strong><span class="koboSpan" id="kobo.934.1"> so that each row consists of a list of 27 values of zero, and one value of 1, representing the class it belongs to:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.935.1">y_train[5]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.936.1">array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)</span></p>
			<p><span class="koboSpan" id="kobo.937.1">With that, our datasets have been fully preprocessed and ready to be used in the model development phase of the tutorial.</span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.938.1">Developing models with Keras and MLflow</span></h2>
			<p><span class="koboSpan" id="kobo.939.1">Similar </span><a id="_idIndexMarker895"/><span class="koboSpan" id="kobo.940.1">to our previous </span><a id="_idIndexMarker896"/><span class="koboSpan" id="kobo.941.1">example </span><a id="_idIndexMarker897"/><span class="koboSpan" id="kobo.942.1">of developing </span><a id="_idIndexMarker898"/><span class="koboSpan" id="kobo.943.1">models with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.944.1">Keras</span></strong><span class="koboSpan" id="kobo.945.1"> library, we will once again be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.946.1">Sequential</span></strong><span class="koboSpan" id="kobo.947.1"> class:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.948.1">We will begin by importing the layers and other items we will need from Keras:</span><p class="source-code"><span class="koboSpan" id="kobo.949.1">import tensorflow as tf</span></p><p class="source-code"><span class="koboSpan" id="kobo.950.1">from keras.models import Sequential</span></p><p class="source-code"><span class="koboSpan" id="kobo.951.1">from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Input, Bidirectional, LSTM, Dropout</span></p><p class="source-code"><span class="koboSpan" id="kobo.952.1">from keras.layers.embeddings import Embedding</span></p><p class="source-code"><span class="koboSpan" id="kobo.953.1">from keras.regularizers import l2</span></p><p class="source-code"><span class="koboSpan" id="kobo.954.1">from keras.models import Model</span></p><p class="source-code"><span class="koboSpan" id="kobo.955.1">import mlflow</span></p><p class="source-code"><span class="koboSpan" id="kobo.956.1">import mlflow.keras</span></p></li>
				<li><span class="koboSpan" id="kobo.957.1">Now, we can create a new instance of a model using the sequential class and begin populating it by adding a few layers of interest. </span><span class="koboSpan" id="kobo.957.2">We will start by adding an embedding layer to convert positive integers into dense vectors. </span><span class="koboSpan" id="kobo.957.3">We will specify an </span><strong class="source-inline"><span class="koboSpan" id="kobo.958.1">input_dim</span></strong><span class="koboSpan" id="kobo.959.1"> of </span><strong class="source-inline"><span class="koboSpan" id="kobo.960.1">21</span></strong><span class="koboSpan" id="kobo.961.1"> to represent the size of the amino acid index + 1, and an </span><strong class="source-inline"><span class="koboSpan" id="kobo.962.1">output_dim</span></strong><span class="koboSpan" id="kobo.963.1"> of 32. </span><span class="koboSpan" id="kobo.963.2">In addition, we will assign an </span><strong class="source-inline"><span class="koboSpan" id="kobo.964.1">input_length</span></strong><span class="koboSpan" id="kobo.965.1"> equal to that of </span><strong class="source-inline"><span class="koboSpan" id="kobo.966.1">max_length</span></strong><span class="koboSpan" id="kobo.967.1"> – the length of the sequences:</span><p class="source-code"><span class="koboSpan" id="kobo.968.1">model = Sequential()</span></p><p class="source-code"><span class="koboSpan" id="kobo.969.1">model.add(Embedding(21, 8, input_length=max_length, name="EmbeddingLayer"))</span></p></li>
				<li><span class="koboSpan" id="kobo.970.1">Next, we will add an LSTM layer, wrapped in a </span><strong class="source-inline"><span class="koboSpan" id="kobo.971.1">Bidirectional</span></strong><span class="koboSpan" id="kobo.972.1"> layer, to run inputs in both directions – from past to future and from future to past:</span><p class="source-code"><span class="koboSpan" id="kobo.973.1">model.add(Bidirectional(LSTM(8), name="BidirectionalLayer"))</span></p></li>
				<li><span class="koboSpan" id="kobo.974.1">Next, we will add a </span><strong class="source-inline"><span class="koboSpan" id="kobo.975.1">Dropout</span></strong><span class="koboSpan" id="kobo.976.1"> layer to help prevent the model from overfitting:</span><p class="source-code"><span class="koboSpan" id="kobo.977.1">model.add(Dropout(0.2, name="DropoutLayer"))</span></p></li>
				<li><span class="koboSpan" id="kobo.978.1">Finally, we will end with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.979.1">Dense</span></strong><span class="koboSpan" id="kobo.980.1"> layer and set the number of nodes to </span><strong class="source-inline"><span class="koboSpan" id="kobo.981.1">28</span></strong><span class="koboSpan" id="kobo.982.1"> so that this </span><a id="_idIndexMarker899"/><span class="koboSpan" id="kobo.983.1">corresponds with the shape of the outputs. </span><span class="koboSpan" id="kobo.983.2">Notice that we use a Softmax activation here:</span><p class="source-code"><span class="koboSpan" id="kobo.984.1">model.add(Dense(28, activation='softmax', name="DenseLayer"))</span></p></li>
				<li><span class="koboSpan" id="kobo.985.1">With </span><a id="_idIndexMarker900"/><span class="koboSpan" id="kobo.986.1">the model's architecture prepared, we can assign an optimizer (Adam), compile the model, and check the summary:</span><p class="source-code"><span class="koboSpan" id="kobo.987.1">opt = tf.keras.optimizers.Adam(learning_rate=0.1)</span></p><p class="source-code"><span class="koboSpan" id="kobo.988.1">model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])</span></p></li>
				<li><span class="koboSpan" id="kobo.989.1">Now, let's </span><a id="_idIndexMarker901"/><span class="koboSpan" id="kobo.990.1">go ahead and train our model using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.991.1">fit()</span></strong><span class="koboSpan" id="kobo.992.1"> function and assign 30 epochs. </span><span class="koboSpan" id="kobo.992.2">Notice from our previous tutorial that training deep learning models can be </span><a id="_idIndexMarker902"/><span class="koboSpan" id="kobo.993.1">very time-consuming and expensive, so training a model that is not learning can be a major waste of time. </span><span class="koboSpan" id="kobo.993.2">To mitigate situations such as these, we can implement what is known as a callback in the sense that Keras can end the training period when a model is no longer learning (that is, the loss is no longer decreasing):</span><p class="source-code"><span class="koboSpan" id="kobo.994.1">from keras.callbacks import EarlyStopping</span></p><p class="source-code"><span class="koboSpan" id="kobo.995.1">es = EarlyStopping(monitor='val_loss', patience=5, verbose=1)</span></p></li>
				<li><span class="koboSpan" id="kobo.996.1">Finally, we can go ahead and log our new run in MLflow by calling the </span><strong class="source-inline"><span class="koboSpan" id="kobo.997.1">autolog()</span></strong><span class="koboSpan" id="kobo.998.1"> function and fitting the model, as we did previously. </span><span class="koboSpan" id="kobo.998.2">MLflow offers </span><a id="_idIndexMarker903"/><span class="koboSpan" id="kobo.999.1">many different methods to log both parameters and metrics, and you are not limited </span><a id="_idIndexMarker904"/><span class="koboSpan" id="kobo.1000.1">to using just </span><strong class="source-inline"><span class="koboSpan" id="kobo.1001.1">autolog()</span></strong><span class="koboSpan" id="kobo.1002.1">:</span><p class="source-code"><span class="koboSpan" id="kobo.1003.1">mlflow.keras.autolog()</span></p><p class="source-code"><span class="koboSpan" id="kobo.1004.1">history = model.fit(</span></p><p class="source-code"><span class="koboSpan" id="kobo.1005.1">    X_train_padded, y_train,</span></p><p class="source-code"><span class="koboSpan" id="kobo.1006.1">    epochs=30, batch_size=256,</span></p><p class="source-code"><span class="koboSpan" id="kobo.1007.1">    validation_data=(X_val_padded, y_val),</span></p><p class="source-code"><span class="koboSpan" id="kobo.1008.1">    callbacks=[es]</span></p><p class="source-code"><span class="koboSpan" id="kobo.1009.1">    )</span></p><p><span class="koboSpan" id="kobo.1010.1">Assuming </span><a id="_idIndexMarker905"/><span class="koboSpan" id="kobo.1011.1">you followed these steps correctly, the model will print a note stating that </span><a id="_idIndexMarker906"/><span class="koboSpan" id="kobo.1012.1">MLflow is being used, and you should see a new directory appear next to your current notebook. </span><span class="koboSpan" id="kobo.1012.2">Upon completing the training process, you can plot the results, as we did previously, to arrive at the following diagram:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer300" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1013.1"><img src="image/B17761_08_031.jpg" alt="Figure 8.31 – The accuracy and results of the first iteration of this model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1014.1">Figure 8.31 – The accuracy and results of the first iteration of this model</span></p>
			<p><span class="koboSpan" id="kobo.1015.1">Here, we can see that the accuracy seems to remain stagnant at around 80-85%, while </span><a id="_idIndexMarker907"/><span class="koboSpan" id="kobo.1016.1">the loss </span><a id="_idIndexMarker908"/><span class="koboSpan" id="kobo.1017.1">remains stagnant at 0.6 to 0.8. </span><span class="koboSpan" id="kobo.1017.2">We can see that the model is not learning. </span><span class="koboSpan" id="kobo.1017.3">Perhaps </span><a id="_idIndexMarker909"/><span class="koboSpan" id="kobo.1018.1">a change </span><a id="_idIndexMarker910"/><span class="koboSpan" id="kobo.1019.1">of parameters is needed? </span><span class="koboSpan" id="kobo.1019.2">Let's go ahead and change the number of nodes from </span><strong class="source-inline"><span class="koboSpan" id="kobo.1020.1">8</span></strong><span class="koboSpan" id="kobo.1021.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1022.1">12</span></strong><span class="koboSpan" id="kobo.1023.1"> and the learning rate from </span><strong class="source-inline"><span class="koboSpan" id="kobo.1024.1">0.1</span></strong><span class="koboSpan" id="kobo.1025.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1026.1">0.01</span></strong><span class="koboSpan" id="kobo.1027.1">. </span><span class="koboSpan" id="kobo.1027.2">Upon compiling the new model, calling the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1028.1">autolog()</span></strong><span class="koboSpan" id="kobo.1029.1"> function, and training the new dataset, we will arrive at a new diagram:</span></p>
			<div>
				<div id="_idContainer301" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1030.1"><img src="image/B17761_08_032.jpg" alt="Figure 8.32 – The accuracy and results of the next iteration of this model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1031.1">Figure 8.32 – The accuracy and results of the next iteration of this model</span></p>
			<p><span class="koboSpan" id="kobo.1032.1">Here, we can see that the model's loss, both for training and validation, decreased </span><a id="_idIndexMarker911"/><span class="koboSpan" id="kobo.1033.1">quite nicely until the callback stopped the training at around 30 epochs in. </span><span class="koboSpan" id="kobo.1033.2">Alternatively, the accuracy shows a sharp increase at the beginning, followed by a </span><a id="_idIndexMarker912"/><span class="koboSpan" id="kobo.1034.1">stable increase toward the end, also stopping at 30 epochs into the process. </span><span class="koboSpan" id="kobo.1034.2">We can </span><a id="_idIndexMarker913"/><span class="koboSpan" id="kobo.1035.1">keep making our changes and calling the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1036.1">autolog()</span></strong><span class="koboSpan" id="kobo.1037.1"> function over and over, allowing </span><a id="_idIndexMarker914"/><span class="koboSpan" id="kobo.1038.1">the system to log the changes and the resulting metrics on our behalf. </span><span class="koboSpan" id="kobo.1038.2">After several iterations, we can review the performance of our models using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1039.1">mlflow ui</span></strong><span class="koboSpan" id="kobo.1040.1">. </span><span class="koboSpan" id="kobo.1040.2">Within the notebook itself, enter the following command:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1041.1">!mlflow ui</span></p>
			<p><span class="koboSpan" id="kobo.1042.1">Next, navigate to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1043.1">http://localhost:5000/</span></strong><span class="koboSpan" id="kobo.1044.1">. </span><span class="koboSpan" id="kobo.1044.2">There, you will be able to see the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1045.1">MLflow</span></strong><span class="koboSpan" id="kobo.1046.1"> UI, where you will be able to view the models, their parameters, and their associated metrics:</span></p>
			<div>
				<div id="_idContainer302" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1047.1"><img src="image/B17761_08_033.jpg" alt="Figure 8.33 – An example of the MLflow UI "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1048.1">Figure 8.33 – An example of the MLflow UI</span></p>
			<p><span class="koboSpan" id="kobo.1049.1">With </span><a id="_idIndexMarker915"/><span class="koboSpan" id="kobo.1050.1">that, you can select </span><a id="_idIndexMarker916"/><span class="koboSpan" id="kobo.1051.1">the best model </span><a id="_idIndexMarker917"/><span class="koboSpan" id="kobo.1052.1">and move forward </span><a id="_idIndexMarker918"/><span class="koboSpan" id="kobo.1053.1">with your project.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.1054.1">Reviewing the model's performance</span></h2>
			<p><span class="koboSpan" id="kobo.1055.1">Now </span><a id="_idIndexMarker919"/><span class="koboSpan" id="kobo.1056.1">that the best-performing model has been selected, let's get a better sense of its associated </span><strong class="bold"><span class="koboSpan" id="kobo.1057.1">metrics</span></strong><span class="koboSpan" id="kobo.1058.1">. </span><span class="koboSpan" id="kobo.1058.2">We can gain a better sense of the results by using a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1059.1">classification_report</span></strong><span class="koboSpan" id="kobo.1060.1">, as we did previously, showing almost 99% for both precision and recall. </span><span class="koboSpan" id="kobo.1060.2">Alternatively, we can use a confusion matrix to get a better sense of the data, given that we have 28 classes in total:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1061.1">from sklearn.metrics import confusion_matrix</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1062.1">y_pred = model.predict(X_test_padded)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1063.1">cf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))</span></p>
			<p><span class="koboSpan" id="kobo.1064.1">With the confusion matrix calculated, we can use a heatmap to visualize the results:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1065.1">import seaborn as sns</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1066.1">plt.figure(figsize=(15,10))</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.1067.1">sns.heatmap(cf_matrix, annot=True, fmt='', cmap='Blues')</span></p>
			<p><span class="koboSpan" id="kobo.1068.1">Upon executing this, we will get the following diagram:</span></p>
			<div>
				<div id="_idContainer303" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1069.1"><img src="image/B17761_08_034.jpg" alt="Figure 8.34 – A confusion matrix of the results of the model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1070.1">Figure 8.34 – A confusion matrix of the results of the model</span></p>
			<p><span class="koboSpan" id="kobo.1071.1">Here, we </span><a id="_idIndexMarker920"/><span class="koboSpan" id="kobo.1072.1">can see that the performance of the model is quite robust as it is giving us great results! </span><span class="koboSpan" id="kobo.1072.2">Keras, TensorFlow, and PyTorch are great packages that can help us develop robust and high-impact models to solve specific solutions. </span><span class="koboSpan" id="kobo.1072.3">Often, we will find that there may be a model (or set of models) that already exists through AWS that can solve our complex problem with little to no code. </span><span class="koboSpan" id="kobo.1072.4">We will explore an example of this in the next section.</span></p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.1073.1">Tutorial – anomaly detection in manufacturing using AWS Lookout for Vision</span></h1>
			<p><span class="koboSpan" id="kobo.1074.1">In the previous section, we prepared and trained a deep learning model to classify proteins </span><a id="_idIndexMarker921"/><span class="koboSpan" id="kobo.1075.1">in their given categories. </span><span class="koboSpan" id="kobo.1075.2">We went through the process of preprocessing our data, developing a model, testing the parameters, editing the architecture, and selecting a combination that </span><a id="_idIndexMarker922"/><span class="koboSpan" id="kobo.1076.1">maximized our metrics of interest. </span><span class="koboSpan" id="kobo.1076.2">While this process can generally produce good results, we can sometimes utilize platform architectures such as those from AWS to automatically develop models on our </span><a id="_idIndexMarker923"/><span class="koboSpan" id="kobo.1077.1">behalf. </span><span class="koboSpan" id="kobo.1077.2">Within this tutorial, we will take advantage of a tool known </span><strong class="bold"><span class="koboSpan" id="kobo.1078.1">AWS Lookout for Vision</span></strong><span class="koboSpan" id="kobo.1079.1"> (</span><a href="https://aws.amazon.com/lookout-for-vision/"><span class="koboSpan" id="kobo.1080.1">https://aws.amazon.com/lookout-for-vision/</span></a><span class="koboSpan" id="kobo.1081.1">) to help us prepare a model capable of detecting anomalies within a dataset.</span></p>
			<p><span class="koboSpan" id="kobo.1082.1">Throughout this tutorial, we will be working with a dataset consisting of images concerned </span><a id="_idIndexMarker924"/><span class="koboSpan" id="kobo.1083.1">with manufacturing a of </span><strong class="bold"><span class="koboSpan" id="kobo.1084.1">Drug Product</span></strong><span class="koboSpan" id="kobo.1085.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1086.1">DP</span></strong><span class="koboSpan" id="kobo.1087.1">). </span><span class="koboSpan" id="kobo.1087.2">Each of the images consists of a vial whose image was captured at the end of the manufacturing cycle. </span><span class="koboSpan" id="kobo.1087.3">Most of the vials are clean and don't have any impurities. </span><span class="koboSpan" id="kobo.1087.4">However, some of the vials contain minor impurities, as illustrated in the following diagram:</span></p>
			<div>
				<div id="_idContainer304" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1088.1"><img src="image/B17761_08_035.jpg" alt="Figure 8.35 – An example of an accepted vial versus a damaged vial "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1089.1">Figure 8.35 – An example of an accepted vial versus a damaged vial</span></p>
			<p><span class="koboSpan" id="kobo.1090.1">The process of rejecting damaged or impure vials is often done manually and can be quite time-consuming. </span><span class="koboSpan" id="kobo.1090.2">We have been tasked with implementing an automated solution to this problem and we only have a few days to do so. </span><span class="koboSpan" id="kobo.1090.3">Rather than developing our own custom deep learning model for detecting anomalies in images, we can utilize </span><strong class="bold"><span class="koboSpan" id="kobo.1091.1">Amazon Lookout for Vision</span></strong><span class="koboSpan" id="kobo.1092.1">. </span><span class="koboSpan" id="kobo.1092.2">In this tutorial, we will begin by uploading our dataset of images to S3, importing </span><a id="_idIndexMarker925"/><span class="koboSpan" id="kobo.1093.1">the images into the framework, and begin training our model. </span><span class="koboSpan" id="kobo.1093.2">With that in mind, let's go ahead and get started!</span></p>
			<p><span class="koboSpan" id="kobo.1094.1">Within </span><a id="_idIndexMarker926"/><span class="koboSpan" id="kobo.1095.1">this book's GitHub repository, you can find a directory called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1096.1">vials_input_dataset_s3</span></strong><span class="koboSpan" id="kobo.1097.1">, which contains a collection of both </span><a id="_idIndexMarker927"/><span class="koboSpan" id="kobo.1098.1">normal and damaged vials. </span><span class="koboSpan" id="kobo.1098.2">If we </span><a id="_idIndexMarker928"/><span class="koboSpan" id="kobo.1099.1">take a closer look at our dataset, we will notice that it is constructed using a directory hierarchy, as shown in the following diagram:</span></p>
			<div>
				<div id="_idContainer305" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1100.1"><img src="image/B17761_08_036.jpg" alt="Figure 8.36 – An example of an accepted vial versus a damaged vial "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1101.1">Figure 8.36 – An example of an accepted vial versus a damaged vial</span></p>
			<p><span class="koboSpan" id="kobo.1102.1">We will begin by importing the images into the same S3 bucket we have been working with throughout this book:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.1103.1">First, navigate to S3 from within the AWS console and select the bucket of interest. </span><span class="koboSpan" id="kobo.1103.2">In this case, I will select </span><strong class="bold"><span class="koboSpan" id="kobo.1104.1">biotech-machine-learning</span></strong><span class="koboSpan" id="kobo.1105.1">.</span></li>
				<li><span class="koboSpan" id="kobo.1106.1">Next, click the orange </span><strong class="bold"><span class="koboSpan" id="kobo.1107.1">Upload</span></strong><span class="koboSpan" id="kobo.1108.1"> button, select the </span><strong class="bold"><span class="koboSpan" id="kobo.1109.1">vials_input_dataset_s3</span></strong><span class="koboSpan" id="kobo.1110.1"> folder, and click </span><strong class="bold"><span class="koboSpan" id="kobo.1111.1">Upload</span></strong><span class="koboSpan" id="kobo.1112.1">. </span><span class="koboSpan" id="kobo.1112.2">This process may take a few moments, depending on your internet connection.</span></li>
				<li><span class="koboSpan" id="kobo.1113.1">Now, click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1114.1">Copy S3 URI</span></strong><span class="koboSpan" id="kobo.1115.1"> button at the top right-hand side of the page. </span><span class="koboSpan" id="kobo.1115.2">We will need this URI in a few moments.</span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.1116.1">Now, our </span><a id="_idIndexMarker929"/><span class="koboSpan" id="kobo.1117.1">data is available for use to </span><a id="_idIndexMarker930"/><span class="koboSpan" id="kobo.1118.1">use in our S3 bucket. </span><span class="koboSpan" id="kobo.1118.2">Next, we can focus on getting the data imported with the model and start the model training process:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.1119.1">To begin, navigate to Amazon Lookout for Vision, which is located in the AWS console. </span><span class="koboSpan" id="kobo.1119.2">Then, click the </span><strong class="bold"><span class="koboSpan" id="kobo.1120.1">Get started</span></strong><span class="koboSpan" id="kobo.1121.1"> button:</span><div id="_idContainer306" class="IMG---Figure"><span class="koboSpan" id="kobo.1122.1"><img src="image/B17761_08_037.jpg" alt="Figure 8.37 – The front page of Amazon Lookout for Vision "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1123.1">Figure 8.37 – The front page of Amazon Lookout for Vision</span></p></li>
				<li><span class="koboSpan" id="kobo.1124.1">Click on the </span><strong class="bold"><span class="koboSpan" id="kobo.1125.1">Create project</span></strong><span class="koboSpan" id="kobo.1126.1"> button on the right-hand side of the page and give your project a name.</span></li>
				<li><span class="koboSpan" id="kobo.1127.1">Once the project has been created, go ahead and click the </span><strong class="bold"><span class="koboSpan" id="kobo.1128.1">Create dataset</span></strong><span class="koboSpan" id="kobo.1129.1"> button on the left-hand side of the page.</span></li>
				<li><span class="koboSpan" id="kobo.1130.1">Select the second option to </span><strong class="bold"><span class="koboSpan" id="kobo.1131.1">Create a training dataset and test dataset</span></strong><span class="koboSpan" id="kobo.1132.1">:</span><div id="_idContainer307" class="IMG---Figure"><span class="koboSpan" id="kobo.1133.1"><img src="image/B17761_08_038.jpg" alt="Figure 8.38 – Creating a dataset in AWS Lookout for Vision "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1134.1">Figure 8.38 – Creating a dataset in AWS Lookout for Vision</span></p></li>
				<li><span class="koboSpan" id="kobo.1135.1">Next, within </span><a id="_idIndexMarker931"/><span class="koboSpan" id="kobo.1136.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.1137.1">Training dataset details</span></strong><span class="koboSpan" id="kobo.1138.1"> section, select the option to </span><strong class="bold"><span class="koboSpan" id="kobo.1139.1">Import images from S3 bucket</span></strong><span class="koboSpan" id="kobo.1140.1"> and paste the URI you copied previously into the S3 URI field. </span><span class="koboSpan" id="kobo.1140.2">Since this </span><a id="_idIndexMarker932"/><span class="koboSpan" id="kobo.1141.1">section pertains to the training dataset, we will add the word </span><strong class="source-inline"><span class="koboSpan" id="kobo.1142.1">training</span></strong><span class="koboSpan" id="kobo.1143.1"> to the path, as shown in the following screenshot:</span><div id="_idContainer308" class="IMG---Figure"><span class="koboSpan" id="kobo.1144.1"><img src="image/B17761_08_039.jpg" alt="Figure 8.39 – Creating a dataset in AWS Lookout for Vision "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.1145.1">Figure 8.39 – Creating a dataset in AWS Lookout for Vision</span></p></li>
				<li><span class="koboSpan" id="kobo.1146.1">In addition, be sure to select the </span><strong class="bold"><span class="koboSpan" id="kobo.1147.1">Automatic labeling</span></strong><span class="koboSpan" id="kobo.1148.1"> option to ensure our labels are taken in by AWS.</span></li>
				<li><span class="koboSpan" id="kobo.1149.1">Repeat this same process for the test dataset but be sure to add the word </span><strong class="source-inline"><span class="koboSpan" id="kobo.1150.1">validation</span></strong><span class="koboSpan" id="kobo.1151.1"> instead of training in the S3 URI path. </span><span class="koboSpan" id="kobo.1151.2">Then, click on </span><strong class="bold"><span class="koboSpan" id="kobo.1152.1">Create dataset</span></strong><span class="koboSpan" id="kobo.1153.1">. </span></li>
				<li><span class="koboSpan" id="kobo.1154.1">Once the </span><a id="_idIndexMarker933"/><span class="koboSpan" id="kobo.1155.1">dataset has been </span><a id="_idIndexMarker934"/><span class="koboSpan" id="kobo.1156.1">created, you will be taken to a new page where you can visually inspect the dataset's readiness. </span><span class="koboSpan" id="kobo.1156.2">Then, you can click the </span><strong class="bold"><span class="koboSpan" id="kobo.1157.1">Train model</span></strong><span class="koboSpan" id="kobo.1158.1"> button located in the top right-hand corner of the page to begin the model training process. </span><span class="koboSpan" id="kobo.1158.2">This process can be time-consuming and may take a few hours:</span></li>
			</ol>
			<div>
				<div id="_idContainer309" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1159.1"><img src="image/B17761_08_040.jpg" alt="Figure 8.40 – The dataset before training the model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1160.1">Figure 8.40 – The dataset before training the model</span></p>
			<p><span class="koboSpan" id="kobo.1161.1">By doing this, you will be presented with the final results for the model, which will show you the precision, recall, and F1 score, as shown in the following screenshot:</span></p>
			<div>
				<div id="_idContainer310" class="IMG---Figure">
					<span class="koboSpan" id="kobo.1162.1"><img src="image/B17761_08_041.jpg" alt="Figure 8.41 – The Model performance metrics page "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.1163.1">Figure 8.41 – The Model performance metrics page</span></p>
			<p><span class="koboSpan" id="kobo.1164.1">With that </span><a id="_idIndexMarker935"/><span class="koboSpan" id="kobo.1165.1">final step completed, we have </span><a id="_idIndexMarker936"/><span class="koboSpan" id="kobo.1166.1">successfully developed a robust model capable of detecting anomalies in the manufacturing process! </span><span class="koboSpan" id="kobo.1166.2">Not only were we able to create the models in just a few hours, but we managed to do so without any code!</span></p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.1167.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.1168.1">Throughout this chapter, we made a major stride to cover a respectable portion of the </span><em class="italic"><span class="koboSpan" id="kobo.1169.1">must-know</span></em><span class="koboSpan" id="kobo.1170.1"> elements of deep learning and neural networks. </span><span class="koboSpan" id="kobo.1170.2">First, we investigated the roots of neural networks and how they came about and then dove into the idea of a perceptron and its basic form of functionality. </span><span class="koboSpan" id="kobo.1170.3">We then embarked on a journey to explore four of the most common neural networks out there: MLP, CNN, RNN, and LSTM. </span><span class="koboSpan" id="kobo.1170.4">We gained a better sense of how to select activation functions, measure loss, and implement our understandings using the Keras library.</span></p>
			<p><span class="koboSpan" id="kobo.1171.1">Next, we took a less theoretical and much more hands-on approach as we tackled our first dataset that was sequential nature. </span><span class="koboSpan" id="kobo.1171.2">We spent a considerable amount of time preprocessing our data, developing our model, getting our model development organized with MLflow, and reviewing its performance. </span><span class="koboSpan" id="kobo.1171.3">Following these steps allowed us to create a custom and well-suited model for the problem at hand. </span><span class="koboSpan" id="kobo.1171.4">Finally, we took a no-code approach by using AWS Lookout for Vision to train a model capable of detecting anomalies in images of vials.</span></p>
			<p><span class="koboSpan" id="kobo.1172.1">Deep learning and the application of neural networks have most certainly seen a major surge over the last few years, and in the next chapter, we will see an application of deep learning as it relates to natural language processing.</span></p>
		</div>
	</body></html>