["```py\na = [[1,2,1], [3,2], [4,9,1,0,2]]\n\n```", "```py\nsums = map(sum, a)\n\n```", "```py\nsums = []\nfor sublist in a:\n    results = sum(sublist)\n    sums.append(results)\n\n```", "```py\ndef add(a, b): \n    return a + b\n\n```", "```py\nfrom functools import reduce \nprint(reduce(add, sums, 0))\n\n```", "```py\ninitial = 0 \ncurrent_result = initial \nfor element in sums: \n    current_result = add(current_result, element)\n\n```", "```py\nfrom collections import defaultdict\ndef map_word_count(document_id, document):\n    counts = defaultdict(int) \n    for word in document.split(): \n        counts[word] += 1\n    for word in counts: \n        yield (word, counts[word])\n\n```", "```py\ndef shuffle_words(results):\n    records = defaultdict(list)\n    for results in results_generators: \n        for word, count in results: \n            records[word].append(count)\n    for word in records: \n        yield (word, records[word])\n\n```", "```py\ndef reduce_counts(word, list_of_counts): \n    return (word, sum(list_of_counts))\n\n```", "```py\nfrom sklearn.datasets import fetch_20newsgroups \ndataset = fetch_20newsgroups(subset='train') \ndocuments = dataset.data\n\n```", "```py\nmap_results = map(map_word_count, enumerate(documents))\n\n```", "```py\nshuffle_results = shuffle_words(map_results)\n\n```", "```py\n1005545.male.25.Engineering.Sagittarius.xml\n\n```", "```py\nimport os \nfilename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"blogs\", \"1005545.male.25.Engineering.Sagittarius.xml\")\n\n```", "```py\nall_posts = []\n\n```", "```py\nwith open(filename) as inf:\n    post_start = False\n    post = []\n    for line in inf: \n        line = line.strip()\n        if line == \"<post>\":\n            # Found a new post\n            post_start = True \n        elif line == \"</post>\":\n            # End of the current post, append to our list of posts and start a new one\n            post_start = False\n            all_posts.append(\"n\".join(post))\n            post = []\n        elif post_start:\n            # In a current post, add the line to the text of the post\n            post.append(line)\n\n```", "```py\nprint(all_posts[0])\n\n```", "```py\nprint(len(all_posts))\n\n```", "```py\nimport os \nimport re\n\n```", "```py\nfrom mrjob.job import MRJob\n\n```", "```py\nclass ExtractPosts(MRJob):\n    post_start = False \n    post = []\n\n    def mapper(self, key, line):\n        filename = os.environ[\"map_input_file\"]\n        # split the filename to get the gender (which is the second token)\n        gender = filename.split(\".\")[1]\n        line = line.strip()\n        if line == \"<post>\":\n            self.post_start = True\n        elif line == \"</post>\":\n            self.post_start = False\n            yield gender, repr(\"n\".join(self.post))\n            self.post = []\n        elif self.post_start:\n            self.post.append(line)\n\n```", "```py\nif __name__ == '__main__': \n ExtractPosts.run()\n\n```", "```py\n$ python extract_posts.py <your_data_folder>/blogs/51* \n --output-dir=<your_data_folder>/blogposts --no-output\n\n```", "```py\n\"'ailleurs\" {\"female\": 0.003205128205128205}\n\"'air\" {\"female\": 0.003205128205128205}\n\"'an\" {\"male\": 0.0030581039755351682, \"female\": 0.004273504273504274}\n\"'angoisse\" {\"female\": 0.003205128205128205}\n\"'apprendra\" {\"male\": 0.0013047113868622459, \"female\": 0.0014172668603481887}\n\"'attendent\" {\"female\": 0.00641025641025641}\n\"'autistic\" {\"male\": 0.002150537634408602}\n\"'auto\" {\"female\": 0.003205128205128205}\n\"'avais\" {\"female\": 0.00641025641025641}\n\"'avait\" {\"female\": 0.004273504273504274}\n\"'behind\" {\"male\": 0.0024390243902439024} \n\"'bout\" {\"female\": 0.002034152292059272}\n\n```", "```py\nimport os \nimport re \nimport numpy as np \nfrom mrjob.job import MRJob \nfrom operator import itemgetter\n\n```", "```py\nfrom mrjob.step import MRStep\n\n```", "```py\nword_search_re = re.compile(r\"[w']+\") \n\n```", "```py\nclass NaiveBayesTrainer(MRJob):\n\n    def steps(self):\n    return [\n            MRStep(mapper=self.extract_words_mapping,\n                   reducer=self.reducer_count_words),\n            MRStep(reducer=self.compare_words_reducer),\n    ]\n\n    def extract_words_mapping(self, key, value):\n        tokens = value.split()\n        gender = eval(tokens[0])\n        blog_post = eval(\" \".join(tokens[1:]))\n        all_words = word_search_re.findall(blog_post)\n        all_words = [word.lower() for word in all_words]\n        for word in all_words:\n            # Occurence probability\n            yield (gender, word), 1\\. / len(all_words)\n\n    def reducer_count_words(self, key, counts):\n        s = sum(counts)\n        gender, word = key #.split(\":\")\n        yield word, (gender, s)\n\n    def compare_words_reducer(self, word, values):\n        per_gender = {}\n        for value in values:\n            gender, s = value\n            per_gender[gender] = s\n            yield word, per_gender\n\n    def ratio_mapper(self, word, value):\n        counts = dict(value)\n        sum_of_counts = float(np.mean(counts.values()))\n        maximum_score = max(counts.items(), key=itemgetter(1))\n        current_ratio = maximum_score[1] / sum_of_counts\n        yield None, (word, sum_of_counts, value)\n\n    def sorter_reducer(self, key, values):\n        ranked_list = sorted(values, key=itemgetter(1), reverse=True)\n        n_printed = 0\n        for word, sum_of_counts, scores in ranked_list:\n            if n_printed < 20:\n                print((n_printed + 1), word, scores)\n            n_printed += 1\n        yield word, dict(scores)\n\n```", "```py\nclass NaiveBayesTrainer(MRJob):\n\n```", "```py\n    def steps(self):\n        return [\n            MRStep(mapper=self.extract_words_mapping,\n                   reducer=self.reducer_count_words),\n            MRStep(reducer=self.compare_words_reducer),\n        ]\n\n```", "```py\n    def extract_words_mapping(self, key, value):\n        tokens = value.split()\n        gender = eval(tokens[0])\n        blog_post = eval(\" \".join(tokens[1:]))\n        all_words = word_search_re.findall(blog_post)\n        all_words = [word.lower() for word in all_words]\n        for word in all_words:\n            # Occurence probability\n            yield (gender, word), 1\\. / len(all_words)\n\n```", "```py\n    def reducer_count_words(self, key, counts):\n        s = sum(counts)\n        gender, word = key #.split(\":\")\n        yield word, (gender, s)\n\n```", "```py\n    def compare_words_reducer(self, word, values):\n        per_gender = {}\n        for value in values:\n            gender, s = value\n            per_gender[gender] = s\n            yield word, per_gender\n\n```", "```py\nif __name__ == '__main__': \n NaiveBayesTrainer.run()\n\n```", "```py\n$ python nb_train.py <your_data_folder>/blogposts/ \n --output-dir=<your_data_folder>/models/ --no-output\n\n```", "```py\ncat * > model.txt\n\n```", "```py\nimport os \nimport re\nimport numpy as np \nfrom collections import defaultdict \nfrom operator import itemgetter\n\n```", "```py\nword_search_re = re.compile(r\"[w']+\")\n\n```", "```py\ndef load_model(model_filename):\n    model = defaultdict(lambda: defaultdict(float))\n    with open(model_filename) as inf: \n        for line in inf:\n            word, values = line.split(maxsplit=1) \n            word = eval(word) \n            values = eval(values)\n            model[word] = values\n    return model\n\n```", "```py\nmodel_filename = os.path.join(os.path.expanduser(\"~\"), \"models\", \"part-00000\") \nmodel = load_model(model_filename)\n\n```", "```py\nmodel[\"i\"][\"male\"], model[\"i\"][\"female\"]\n\n```", "```py\ndef nb_predict(model, document):\n    probabilities = defaultdict(lambda : 1)\n    words = word_search_re.findall(document)\n    for word in set(words): \n        probabilities[\"male\"] += np.log(model[word].get(\"male\", 1e-15)) \n        probabilities[\"female\"] += np.log(model[word].get(\"female\", 1e-15))\n        most_likely_genders = sorted(probabilities.items(), key=itemgetter(1), reverse=True) \n    return most_likely_genders[0][0]\n\n```", "```py\nnew_post = \"\"\" Every day should be a half day. Took the afternoon off to hit the dentist, and while I was out I managed to get my oil changed, too. Remember that business with my car dealership this winter? Well, consider this the epilogue. The friendly fellas at the Valvoline Instant Oil Change on Snelling were nice enough to notice that my dipstick was broken, and the metal piece was too far down in its little dipstick tube to pull out. Looks like I'm going to need a magnet. Damn you, Kline Nissan, daaaaaaammmnnn yooouuuu.... Today I let my boss know that I've submitted my Corps application. The news has been greeted by everyone in the company with a level of enthusiasm that really floors me. The back deck has finally been cleared off by the construction company working on the place. This company, for anyone who's interested, consists mainly of one guy who spends his days cursing at his crew of Spanish-speaking laborers. Construction of my deck began around the time Nixon was getting out of office.\n\"\"\"\n\n```", "```py\nnb_predict(model, new_post)\n\n```", "```py\nmkdir blogs_train\n\n```", "```py\ncp blogs/4* blogs_train/ \ncp blogs/8* blogs_train/\n\n```", "```py\nmkdir blogs_test\n\n```", "```py\ncp blogs/6* blogs_test/ \ncp blogs/7* blogs_test/\n\n```", "```py\n$ python extract_posts.py ~/Data/blogs_train --output-dir=/home/bob/Data/blogposts_train --no-output\n\n```", "```py\n$ python nb_train.py ~/Data/blogposts_train/ --output-dir=/home/bob/models/ --no-output\n\n```", "```py\npython extract_posts.py ~/Data/blogs_test --output-dir=/home/bob/Data/blogposts_test --no-output\n\n```", "```py\ntesting_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"blogposts_testing\") \ntesting_filenames = [] \nfor filename in os.listdir(testing_folder): \n    testing_filenames.append(os.path.join(testing_folder, filename))\n\n```", "```py\ndef nb_predict_many(model, input_filename): \n    with open(input_filename) as inf: # remove leading and trailing whitespace \n    for line in inf: \n        tokens = line.split() \n        actual_gender = eval(tokens[0]) \n        blog_post = eval(\" \".join(tokens[1:])) \n        yield actual_gender, nb_predict(model, blog_post)\n\n```", "```py\ny_true = []\ny_pred = [] \nfor actual_gender, predicted_gender in nb_predict_many(model, testing_filenames[0]):                    \n    y_true.append(actual_gender == \"female\")   \n    y_pred.append(predicted_gender == \"female\") \n    y_true = np.array(y_true, dtype='int') \n    y_pred = np.array(y_pred, dtype='int')\n\n```", "```py\nfrom sklearn.metrics import f1_score \nprint(\"f1={:.4f}\".format(f1_score(y_true, y_pred, pos_label=None)))\n\n```", "```py\nsudo pip install awscli\n\n```", "```py\ncp -R ~/Data/blogs ~/Data/blogs_train_large \nrm ~/Data/blogs_train_large/blogs/6* \nrm ~/Data/blogs_train_large/blogs/7*\n\n```", "```py\naws s3 cp ~/Data/blogs_train_large/ s3://ch12/blogs_train_large --recursive --exclude \"*\" \n--include \"*.xml\"\n\n```", "```py\n$ python extract_posts.py -r emr s3://ch12gender/blogs_train_large/ \n--output-dir=s3://ch12/blogposts_train/ --no-output \n$ python nb_train.py -r emr s3://ch12/blogposts_train/ --output-dir=s3://ch12/model/ --o-output\n\n```", "```py\n$ python extract_posts.py -r emr s3://chapter12/blogs_train_large/blogs/ --output-dir=s3://chapter12/blogposts_train/ --no-output  --instance-type c1.medium --num-core-instances 16\n\n```", "```py\nws_model_filename = os.path.join(os.path.expanduser(\"~\"), \"models\", \"aws_model\")\naws_model = load_model(aws_model_filename) \ny_true = [] \ny_pred = [] \nfor actual_gender, predicted_gender in nb_predict_many(aws_model, testing_filenames[0]):\n    y_true.append(actual_gender == \"female\") \n    y_pred.append(predicted_gender == \"female\") \ny_true = np.array(y_true, dtype='int') \ny_pred = np.array(y_pred, dtype='int') \nprint(\"f1={:.4f}\".format(f1_score(y_true, y_pred, pos_label=None)))\n\n```"]