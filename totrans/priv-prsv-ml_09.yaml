- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Federated Learning and Implementing FL Using Open Source Frameworks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习与使用开源框架实现FL
- en: In this chapter, you will learn about **Federated Learning** (**FL**) and how
    to implement it using open source frameworks. We will cover why it is needed and
    how to preserve data privacy. We will also look at the definition of FL, as well
    as its characteristics and the steps involved in it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解**联邦学习**（**FL**）以及如何使用开源框架实现它。我们将涵盖为什么需要它以及如何保护数据隐私。我们还将探讨FL的定义、其特性和涉及的步骤。
- en: 'We will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主要主题：
- en: FL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL
- en: FL algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL算法
- en: The steps involved in implementing FL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现FL涉及的步骤
- en: Open source frameworks for implementing FL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现FL的开源框架
- en: An end-to-end use case of implementing fraud detection using FL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用FL实现欺诈检测的端到端用例
- en: FL with differential privacy
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有差分隐私的FL
- en: By exploring these topics, you will gain a comprehensive understanding of the
    need for FL and the open source frameworks for implementing FL.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探讨这些主题，您将全面了解FL的需求以及实现FL的开源框架。
- en: Federated learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习
- en: FL has emerged as a solution to address the challenges of traditional centralized
    **Machine Learning** (**ML**) approaches in scenarios where data privacy and data
    locality are of paramount importance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: FL已成为解决在数据隐私和数据本地性至关重要的场景中传统集中式**机器学习**（**ML**）方法挑战的解决方案。
- en: 'The key reasons that we need FL are as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要FL的关键原因如下：
- en: '**Preserving data privacy**: In many situations, data is sensitive and cannot
    be shared due to legal, ethical, or privacy concerns. FL enables you to train
    models directly on distributed data sources without sharing the raw data, ensuring
    privacy protection. By keeping data local and performing model updates locally,
    FL minimizes the risk of exposing sensitive information.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保护数据隐私**：在许多情况下，数据是敏感的，由于法律、伦理或隐私问题无法共享。FL允许您在分布式数据源上直接训练模型，而不共享原始数据，确保隐私保护。通过保持数据本地化并在本地执行模型更新，FL最大限度地降低了暴露敏感信息的风险。'
- en: '**Data localization and regulatory compliance**: FL allows organizations to
    comply with data localization requirements and regulations. Instead of transferring
    data to a central server, data remains within the jurisdiction where it is generated
    or collected, addressing concerns related to cross-border data transfers.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据本地化和法规遵从性**：FL允许组织遵守数据本地化要求和法规。无需将数据传输到中央服务器，数据保持在生成或收集数据的管辖范围内，解决了与跨境数据传输相关的担忧。'
- en: '**Scalability and efficiency**: Centralized machine learning approaches often
    face challenges when dealing with large volumes of data, as aggregating and processing
    data from various sources can be time-consuming and resource-intensive. FL distributes
    the training process, allowing data to remain decentralized while benefiting from
    the collective intelligence of all participating devices or data sources. This
    decentralized approach improves scalability and computational efficiency.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和效率**：集中式机器学习方法在处理大量数据时往往面临挑战，因为从各种来源汇总和处理数据可能耗时且资源密集。FL将训练过程分散化，允许数据保持去中心化，同时从所有参与设备或数据源的集体智慧中受益。这种去中心化方法提高了可扩展性和计算效率。'
- en: '**Access to diverse data**: FL facilitates the pooling of data from multiple
    sources, enabling models to learn from diverse datasets without the need for direct
    data sharing. This is particularly beneficial in scenarios where data sources
    have distinct characteristics, such as different demographics, geographical regions,
    or user preferences. Access to a diverse range of data enhances the generalization
    and robustness of ML models.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问多样化的数据**：FL促进了来自多个来源的数据汇集，使模型能够在不直接共享数据的情况下从多样化的数据集中学习。这在数据来源具有不同特征的情况下特别有益，例如不同的人口统计、地理区域或用户偏好。访问多样化的数据范围增强了机器学习模型的泛化能力和鲁棒性。'
- en: '**Enhanced security and resilience**: With FL, the data remains distributed
    across devices or edge nodes, reducing the risk of a single point of failure or
    vulnerability. This distributed nature enhances the security and resilience of
    the overall system, making it less susceptible to attacks or breaches.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强的安全性和弹性**：使用FL，数据分布在设备或边缘节点上，降低了单点故障或漏洞的风险。这种分布式特性增强了整个系统的安全性和弹性，使其对攻击或入侵的抵抗力降低。'
- en: '**User empowerment and inclusion**: FL offers opportunities for user participation
    and control over their data. Instead of relinquishing data ownership and control
    to a centralized authority, users can actively contribute to the learning process
    while retaining control over their personal information. This empowers individuals
    and promotes a sense of inclusion and transparency.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户赋权和包容性**：联邦学习（FL）为用户参与和控制他们的数据提供了机会。用户不必将数据所有权和控制权交给中央权威机构，他们可以在保留对个人信息的控制的同时积极参与学习过程。这赋予了个人权力，并促进了包容性和透明度。'
- en: The need for FL arises from the critical requirements of preserving data privacy,
    complying with regulatory frameworks, achieving scalability and efficiency, accessing
    diverse data sources, ensuring security, and empowering users.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 需要联邦学习（FL）的原因在于保护数据隐私、遵守监管框架、实现可扩展性和效率、访问多样化的数据源、确保安全以及赋予用户权力等关键要求。
- en: By leveraging FL, organizations can overcome the limitations of centralized
    approaches and unlock the potential of distributed data for training robust and
    privacy-preserving ML models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用联邦学习（FL），组织可以克服集中式方法的局限性，并释放分布式数据用于训练强大且保护隐私的机器学习模型。
- en: Preserving privacy
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护隐私
- en: Let’s consider the case of ARDHA Bank (a fictional bank for illustration purposes
    only). ARDHA Bank is a financial institution that has been operating in the United
    States for several years, adhering to country-specific regulations. The bank offers
    a range of services to its customers, including fraud prevention, loyalty programs,
    and digital payments. Initially, ARDHA Bank employed static rule-based systems
    to detect and prevent fraudulent activities. However, recognizing the need for
    more advanced approaches, they transitioned to utilizing ML algorithms for enhanced
    fraud detection and prevention.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑 ARDHA 银行（仅用于说明目的的虚构银行）的案例。ARDHA 银行是一家在美国运营多年的金融机构，遵守特定国家的法规。该银行向客户提供一系列服务，包括欺诈预防、忠诚度计划和数字支付。最初，ARDHA
    银行采用基于静态规则的系统来检测和预防欺诈活动。然而，意识到需要更先进的方法，他们转向利用机器学习算法来增强欺诈检测和预防。
- en: With access to a comprehensive dataset comprising historical and current transaction
    data, ARDHA Bank developed ML and **Deep Learning** (**DL**) algorithms specifically
    tailored to their operations. These algorithms were trained on this extensive
    dataset, allowing the bank to effectively identify and prevent financial fraud
    with exceptional accuracy. By leveraging the power of ML and DL techniques, ARDHA
    Bank significantly improved its ability to detect and mitigate fraudulent digital
    transactions, thereby safeguarding its customers’ financial interests.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问包含历史和当前交易数据的综合数据集，ARDHA 银行开发了针对其运营的机器学习和**深度学习**（**DL**）算法。这些算法在这个广泛的数据集上进行了训练，使银行能够以极高的准确性有效地识别和预防金融欺诈。通过利用机器学习和深度学习技术的力量，ARDHA
    银行显著提高了其检测和减轻欺诈性数字交易的能力，从而保护了客户的财务利益。
- en: '![Figure 6.1 – A simple ML model in a financial bank](img/B16573_06_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 金融银行中的简单机器学习模型](img/B16573_06_01.jpg)'
- en: Figure 6.1 – A simple ML model in a financial bank
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 金融银行中的简单机器学习模型
- en: ARDHA Bank, having experienced success in the **United States** (**US**), made
    the strategic decision to expand its business and establish branches in two additional
    countries – France (for Europe) and India. With the expansion, ARDHA Bank aimed
    to offer the same suite of services to its customers in both regions. To provide
    digital payment services in France and India, one option considered by ARDHA Bank
    was to transmit periodic transaction data from both countries to their US servers.
    The US servers would then serve as the central location to run the ML models.
    After training the ML models on the combined data from all regions, the trained
    models would be deployed to the regional servers in France and India. By adopting
    this approach, ARDHA Bank sought to leverage the infrastructure of its US servers
    to process and analyze the transaction data efficiently. The centralized training
    of ML models allowed for a unified approach to fraud detection and prevention,
    ensuring consistency and accuracy across different regions. This strategy enabled
    ARDHA Bank to provide reliable and effective digital payment services in Europe
    and India while maintaining data security and privacy. By utilizing regional servers
    and deploying the trained ML models locally, the bank ensured swift and localized
    decision-making, catering to the specific needs and regulatory requirements of
    each region.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ARDHA银行在**美国**取得了成功之后，做出了战略决策，扩大其业务并在两个额外的国家设立分支机构——法国（针对欧洲）和印度。随着扩张，ARDHA银行旨在为其两个地区的客户提供相同的服务套件。为了在法国和印度提供数字支付服务，ARDHA银行考虑的一个选项是将两个国家的定期交易数据传输到其美国服务器。然后，美国服务器将作为运行机器学习模型的中央位置。在结合所有地区的综合数据上训练机器学习模型后，训练好的模型将被部署到法国和印度的区域服务器上。通过采用这种方法，ARDHA银行旨在利用其美国服务器的基础设施来高效地处理和分析交易数据。集中训练机器学习模型允许采取统一的方法进行欺诈检测和预防，确保在不同地区的一致性和准确性。这一策略使ARDHA银行能够在欧洲和印度提供可靠有效的数字支付服务，同时保持数据安全和隐私。通过利用区域服务器并在本地部署训练好的机器学习模型，银行确保了快速和本地化的决策制定，满足每个地区的特定需求和监管要求。
- en: '![Figure 6.2 – A simple ML model in a financial bank in three locations](img/B16573_06_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 在三个地点的金融银行中的简单机器学习模型](img/B16573_06_02.jpg)'
- en: Figure 6.2 – A simple ML model in a financial bank in three locations
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 在三个地点的金融银行中的简单机器学习模型
- en: The proposed solution, which involves transferring data to a central server
    and running ML models on that data, faces challenges due to privacy regulations
    and data localization laws in Europe and India. These regulations, such as the
    **General Data Protection Regulation** (**GDPR**) in Europe and India’s data localization
    requirements, stipulate that data generated within these countries must be stored
    within local data centers. Data must remain within the borders of the country
    where it was created.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的解决方案，涉及将数据传输到中央服务器并在该数据上运行机器学习模型，由于欧洲和印度的隐私法规和数据本地化法律而面临挑战。这些法规，如欧洲的**通用数据保护条例**（**GDPR**）和印度的数据本地化要求，规定在这些国家生成的数据必须存储在本地的数据中心。数据必须保持在创建它的国家境内。
- en: Given these privacy and localization constraints, an alternative approach is
    necessary. One possible alternative is to run ML models locally at each branch
    or location of the bank. This approach entails deploying client models that utilize
    the local data available at each location. The local models would process the
    data within the boundaries of the respective country, ensuring compliance with
    privacy regulations. To implement this alternative, only the model weights and
    parameters, not the transaction data used by customers, would be shared with a
    central server. The central server, hosted in any country, would be responsible
    for running a global model using the aggregated model weights and parameters from
    each location. The resulting global model could then be regularly distributed
    back to the local clients in each country. This approach enables the bank to leverage
    the benefits of ML models while adhering to privacy regulations and data localization
    laws. By conducting ML computations locally and sharing only model-related information,
    the bank ensures compliance, data security, and privacy. Additionally, this distributed
    approach allows for local adaptation and customization while still benefiting
    from the insights gained through the global model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些隐私和本地化限制，需要一种替代方法。一个可能的替代方案是在银行的每个分支或地点本地运行机器学习（ML）模型。这种方法包括部署利用每个地点可用的本地数据的客户端模型。本地模型将在各自国家的边界内处理数据，确保符合隐私法规。为了实施这种替代方案，只有模型权重和参数，而不是客户使用的交易数据，会与中央服务器共享。任何国家的中央服务器将负责运行一个全局模型，使用来自每个位置的聚合模型权重和参数。然后，生成的全局模型可以定期分发给每个国家的本地客户端。这种方法使银行能够利用机器学习模型的好处，同时遵守隐私法规和数据本地化法律。通过在本地进行机器学习计算并仅共享模型相关信息，银行确保了合规性、数据安全和隐私。此外，这种分布式方法允许进行本地适应和定制，同时仍然受益于全局模型获得的见解。
- en: '![Figure 6.3 – Local model interactions with the global model](img/B16573_06_03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 本地模型与全局模型的交互](img/B16573_06_03.jpg)'
- en: Figure 6.3 – Local model interactions with the global model
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 本地模型与全局模型的交互
- en: This approach is known as **Federated Machine Learning**, or FL. In FL, the
    traditional paradigm of moving data to a central location is reversed. Instead,
    the model and computation are brought to the data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**联邦机器学习**，或FL。在FL中，将数据移动到中央位置的传统范式被颠倒。相反，模型和计算被带到数据那里。
- en: In FL, the ML model is deployed and executed directly on the local data sources
    or devices where the data resides. This eliminates the need to transfer raw data
    to a central server, addressing privacy concerns and regulatory requirements.
    The model is trained locally using the data on each device, and only the model
    updates, such as gradients or weights, are securely transmitted to a central aggregator.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在联邦学习中，机器学习（ML）模型直接部署并在数据所在的本地区域或设备上执行。这消除了将原始数据传输到中央服务器的需求，解决了隐私问题和监管要求。模型使用每个设备上的数据进行本地训练，并且只有模型更新，如梯度或权重，才会安全地传输到中央聚合器。
- en: By keeping the data decentralized and performing computations locally, FL ensures
    data privacy and reduces the risks associated with data transfer. It allows organizations
    to leverage the collective knowledge and insights from distributed data sources
    without compromising individual data privacy. This approach is particularly beneficial
    in scenarios where data cannot be easily shared due to legal, regulatory, or privacy
    constraints.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保持数据去中心化和本地执行计算，联邦学习（FL）确保了数据隐私并减少了与数据传输相关的风险。它允许组织利用分布式数据源的知识和洞察力，同时不损害个人数据隐私。这种方法在数据因法律、监管或隐私限制而难以共享的场景中尤其有益。
- en: FL represents a paradigm shift in ML, enabling collaborative and privacy-preserving
    model training. It promotes a distributed approach where data remains under the
    control of the data owners while contributing to a shared model. This decentralized
    and privacy-conscious framework opens up possibilities to harness the power of
    large-scale data without sacrificing privacy and security.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习代表了机器学习（ML）的一个范式转变，它实现了协作和隐私保护模型训练。它促进了一种分布式方法，其中数据保持在数据所有者的控制之下，同时为共享模型做出贡献。这种去中心化和注重隐私的框架为利用大规模数据的力量打开了可能性，同时不牺牲隐私和安全。
- en: FL definition
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FL定义
- en: 'The following is the formal definition of FL proposed as per the *Advances
    and Open Problems in Federated Learning* paper published at arxiv/1912.04977:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是根据在 arxiv/1912.04977 发表的 *Federated Learning 的进展和开放问题* 论文提出的 FL 的正式定义：
- en: “Federated learning is a machine learning setting where multiple entities (clients)
    collaborate in solving a machine learning problem, under the coordination of a
    central server or service provider. Each client’s raw data is stored locally and
    not exchanged or transferred; instead, focused updates intended for immediate
    aggregation are used to achieve the learning objective”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: “联邦学习是一种机器学习设置，其中多个实体（客户端）在中央服务器或服务提供商的协调下协作解决机器学习问题。每个客户端的原始数据都存储在本地，不进行交换或传输；相反，用于立即聚合的聚焦更新用于实现学习目标”
- en: 'As per this definition, these are the characteristics of FL:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个定义，以下是 FL 的特点：
- en: Multiple clients (entities) collaborate to solve an ML problem.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个客户端（实体）协作解决机器学习问题。
- en: A service provider or central server coordinates with these entities.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务提供商或中央服务器与这些实体协调。
- en: Raw data (data with samples) is stored locally at each client location and is
    not transferred to the servers.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始数据（带有样本的数据）存储在每个客户端位置，并且不会传输到服务器。
- en: The learning objective (or loss function) is defined. To minimize the loss (predictions
    versus actual), focused updates (weights and biases) are sent to the server from
    clients, the aggregation of weights (either average or dynamic aggregation) is
    done at the server, and these updates are sent back to clients.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义了学习目标（或损失函数）。为了最小化损失（预测与实际之间的差异），客户端将聚焦更新（权重和偏差）发送到服务器，在服务器上进行权重的聚合（平均或动态聚合），然后将这些更新发送回客户端。
- en: Let’s delve further into each one of these in detail to understand them better.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步详细探讨每一个，以更好地理解它们。
- en: Characteristics of FL
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FL 的特点
- en: The following subsections will cover the characteristics of FL in depth.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将深入探讨 FL 的特点。
- en: Multiple clients (entities) collaborate to solve an ML problem
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多个客户端（实体）协作解决机器学习问题
- en: In FL, the participation requirement typically involves a minimum of two clients,
    while the maximum number of clients can vary based on the specific use cases and
    client types.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FL 中，参与要求通常涉及至少两个客户端，而客户端的最大数量可以根据特定的用例和客户端类型而变化。
- en: 'Clients participating in FL can be broadly classified into two categories –
    cross-device and cross-silo:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参与 FL 的客户端可以大致分为两类 – 跨设备和跨数据存储库：
- en: '**Cross-device** clients are individual devices, such as smartphones, laptops,
    or IoT devices, that contribute their local data for model training. These devices
    act as clients in the FL framework, allowing their data to be utilized while preserving
    privacy.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨设备**客户端是个人设备，如智能手机、笔记本电脑或物联网设备，它们为模型训练贡献本地数据。这些设备在 FL 框架中作为客户端，允许其数据被利用同时保护隐私。'
- en: '**Cross-silo** clients, on the other hand, represent data sources that are
    distributed across different organizational silos or entities. These silos can
    be different departments within an organization, separate institutions, or even
    distinct geographical regions. Each silo acts as a client, contributing its local
    data for collaborative model training.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨数据存储库**客户端，另一方面，代表分布在不同的组织数据存储库或实体中的数据源。这些数据存储库可以是组织内部的不同部门，独立的机构，甚至是不同的地理区域。每个数据存储库作为客户端，为其协作模型训练贡献本地数据。'
- en: '![Figure 6.4 – A classification of FL clients](img/B16573_06_04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – FL 客户端的分类](img/B16573_06_04.jpg)'
- en: Figure 6.4 – A classification of FL clients
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – FL 客户端的分类
- en: The maximum number of clients in an FL setup depends on the specific use cases
    and the scale of the distributed data sources. For instance, in scenarios where
    multiple organizations collaborate to build a global model while maintaining data
    privacy, the number of participating clients can be substantial. On the other
    hand, in more focused or localized use cases, the number of clients may be limited
    to a smaller group.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: FL 设置中客户端的最大数量取决于特定的用例和分布式数据源的规模。例如，在多个组织协作构建全球模型同时保持数据隐私的场景中，参与客户端的数量可能很大。另一方面，在更专注或本地化的用例中，客户端的数量可能限制在一个较小的群体。
- en: Cross-silo FL clients
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨数据存储库 FL 客户端
- en: 'Cross-silo clients are entities such as financial banks, institutions, hospitals,
    and pharmacy companies. These clients can be further categorized into two groups:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 跨部门客户端包括金融机构、机构、医院和制药公司等实体。这些客户端可以进一步分为两组：
- en: '**Different clients within the same institution**: This includes different
    branches of the same bank, different branches within a hospital network, and similar
    setups where multiple branches or divisions of a single institution participate
    in FL.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同一机构内的不同客户端**：这包括同一银行的分支机构、医院网络内的不同分支机构，以及类似设置，其中多个分支机构或一个机构的多个部门参与联邦学习。'
- en: '**Different clients across different institutions**: This involves different
    organizations, such as different banks or hospitals, collaborating and contributing
    their data to the FL process. These clients represent inter-institutional collaborations.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同机构间的不同客户端**：这涉及不同组织，如不同银行或医院，进行合作并向联邦学习过程贡献数据。这些客户端代表机构间的合作。'
- en: The maximum number of clients in the cross-silo category can vary based on the
    specific use case, but typically, it ranges from tens to hundreds. The number
    of participating clients is usually limited due to the nature of collaborations
    and the scale of the institutions involved.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 跨部门类别中的最大客户端数量可以根据具体用例而变化，但通常在数十到数百之间。由于合作性质和参与机构的规模，参与客户端的数量通常受到限制。
- en: Cross-device FL clients
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨设备联邦学习客户端
- en: Cross-device clients, on the other hand, encompass various devices that participate
    as clients or nodes in FL. These devices can be either homogenous or heterogenous,
    and examples include devices such as Apple iPhones, Google phones, and the Brave
    browser.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，跨设备客户端包括作为联邦学习中的客户端或节点的各种设备。这些设备可以是同质或异质的，例如苹果iPhone、谷歌手机和Brave浏览器等设备。
- en: In the case of cross-device FL clients, each device runs its own ML model based
    on the local data available on that specific device. Only the model weights and
    biases are transmitted to the server based on device conditions and other configuration
    settings.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨设备联邦学习客户端的情况下，每个设备都根据该特定设备上可用的本地数据运行自己的机器学习模型。仅根据设备条件和其他配置设置将模型权重和偏差传输到服务器。
- en: In this scenario, the maximum number of clients can reach thousands or even
    millions, as it encompasses a wide range of devices participating in FL across
    different locations and user bases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，客户端的最大数量可以达到数千甚至数百万，因为它涵盖了不同地点和用户基础中参与联邦学习的广泛设备。
- en: By accommodating both cross-silo and cross-device clients, FL enables collaboration
    and knowledge sharing while respecting data privacy and ensuring scalable participation
    across institutions and devices.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过同时容纳跨部门和跨设备客户端，联邦学习实现了协作和知识共享，同时尊重数据隐私并确保机构间和设备间的可扩展参与。
- en: A service provider or central server coordinates with these entities
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务提供商或中央服务器与这些实体进行协调
- en: The server in FL makes decisions based on the network topology of the participating
    clients and the total number of clients involved in the process. The server determines
    when to distribute the initial model or updated models to the clients, considering
    factors such as the network structure and the specific number of participating
    clients. It decides whether to send the model updates to all clients or only a
    subset of them, based on the requirements of the learning task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在联邦学习中，服务器根据参与客户端的网络拓扑和参与过程的总客户端数量做出决策。服务器确定何时向客户端分发初始模型或更新模型，考虑因素包括网络结构和具体参与客户端的数量。它根据学习任务的要求，决定是否向所有客户端或仅向其中一部分发送模型更新。
- en: After the clients receive the model, they compute and update the weights and
    biases based on their local data. The clients then send these updated weights
    and biases back to the server. The server aggregates the received data and performs
    computations using an objective function to minimize the loss or optimize the
    learning objective.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端接收到模型后，根据其本地数据计算并更新权重和偏差。然后，客户端将这些更新的权重和偏差发送回服务器。服务器汇总接收到的数据，并使用目标函数进行计算，以最小化损失或优化学习目标。
- en: Based on the aggregated information, the server generates an updated model.
    It decides which clients need to be updated with the new model and which clients
    can continue running the existing model without any changes. This decision is
    based on factors such as the learning progress, the need for updates, or the compatibility
    of clients with the updated model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据汇总信息，服务器生成一个更新的模型。它决定哪些客户端需要更新为新模型，哪些客户端可以继续运行现有模型而不做任何更改。这个决定基于学习进度、更新的需要或客户端与更新模型的兼容性等因素。
- en: By carefully orchestrating these steps, the server manages the distribution
    of models, collects client updates, aggregates data, and ultimately, sends back
    the updated model to the appropriate clients. This iterative process in FL ensures
    collaborative model improvement while accounting for the individual requirements
    and capabilities of the participating clients.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过精心安排这些步骤，服务器管理模型的分布，收集客户端更新，汇总数据，并最终将更新的模型发送回适当的客户端。在联邦学习（FL）中，这个迭代过程确保了协作模型改进，同时考虑到参与客户端的个别需求和能力。
- en: Raw data (data with samples) is stored locally at each client location and is
    not transferred to the servers
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原始数据（带有样本的数据）在每个客户端位置本地存储，并且不会传输到服务器。
- en: In FL, raw data is stored locally at each client location instead of being centralized
    in a single server. This decentralized approach ensures that the data remains
    under the control and ownership of the respective clients, preserving privacy
    and complying with data regulations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL中，原始数据在每个客户端位置本地存储，而不是集中存储在单个服务器上。这种去中心化的方法确保数据始终处于各自客户端的控制和所有权之下，保护隐私并符合数据法规。
- en: The data at each client location exhibits a specific distribution, which can
    vary across different clients. The distribution of the data refers to the statistical
    characteristics and patterns present within the dataset. The data samples within
    a client’s dataset can be independent of each other, meaning that they are unrelated
    or do not rely on each other for their values or properties. Alternatively, the
    data samples can be dependent, indicating that there is some form of correlation
    or relationship between them.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每个客户端位置的数据都表现出特定的分布，这种分布可能在不同客户端之间有所不同。数据的分布指的是数据集中存在的统计特性和模式。客户端数据集中的数据样本可能相互独立，这意味着它们之间没有关系或它们的价值或属性不依赖于彼此。或者，数据样本可能是相关的，表明它们之间存在某种形式的关联或关系。
- en: Furthermore, the data distribution can be either identical or non-identical
    among the clients. Identical data distribution implies that the statistical properties
    of the datasets are the same across different clients. On the other hand, non-identical
    data distribution suggests that the datasets exhibit variations in their statistical
    characteristics, such as mean, variance, or other relevant parameters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据分布可以在客户端之间是相同的或不同的。相同的数据分布意味着不同客户端的数据集的统计特性是相同的。另一方面，不同的数据分布表明数据集在统计特性上存在差异，例如均值、方差或其他相关参数。
- en: The presence of diverse data distributions, whether independent or dependent,
    identical or non-identical, introduces challenges and complexities in FL. Nevertheless,
    FL methods are designed to handle these variations and enable collaborative model
    training across decentralized data sources, leveraging the collective knowledge
    while respecting data privacy and distribution characteristics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 独立或相关、相同或不同的数据分布的存在，在FL中引入了挑战和复杂性。尽管如此，FL方法被设计来处理这些变化，并使跨去中心化数据源进行协作模型训练成为可能，同时利用集体知识，尊重数据隐私和分布特性。
- en: Datasets with IID and non-IID data
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 具有独立同分布（IID）和非独立同分布（non-IID）数据的集合
- en: Independent and identically distributed (IID) data refers to a dataset in which
    the data samples are independent of each other, and the distribution of the data
    is identical across all samples. In this case, the outcomes of each data sample
    are not dependent on previous samples, and the statistical properties of the data
    remain consistent.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 独立同分布（IID）数据指的是数据样本之间相互独立，且所有样本的数据分布相同的数据集。在这种情况下，每个数据样本的结果不依赖于先前样本，数据的统计特性保持一致。
- en: For example, consider a dataset where a coin is tossed five times and the number
    of times it turns up heads is recorded. In this scenario, each coin toss is independent
    of the previous tosses, and the probability of getting heads is identical for
    each toss. This results in an IID dataset where the distribution of outcomes is
    the same for every coin toss.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个数据集，其中抛掷硬币五次并记录出现正面的次数。在这种情况下，每次抛掷硬币都是独立的，每次抛掷出现正面的概率是相同的。这导致了一个IID数据集，其中每次抛掷硬币的结果分布是相同的。
- en: In FL, the data across different clients may exhibit **non-IID** characteristics.
    This means that the data samples are not identically distributed, and they may
    also be dependent on each other. Various factors can contribute to non-IID data,
    such as variations in the amount of labeled data, differences in the features
    present in the samples, data drift, concept drift, or imbalanced data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL中，不同客户端之间的数据可能表现出**非-IID**特征。这意味着数据样本不是同质分布的，它们也可能相互依赖。各种因素可能导致非-IID数据，例如标签数据的数量变化、样本中存在的特征差异、数据漂移、概念漂移或不平衡数据。
- en: For example, in the case of cross-silo entities within a company, each client
    may have the same kind of features and labels for classification. However, the
    number of data samples at each location may vary, resulting in imbalanced data.
    Additionally, each location may not have data for all classes or may exhibit different
    distributions of examples.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在公司内部的跨部门实体中，每个客户端可能具有相同的特征和标签用于分类。然而，每个位置的数据样本数量可能不同，导致数据不平衡。此外，每个位置可能没有所有类别的数据，或者表现出不同的实例分布。
- en: Raw data in cross-silo entities in FL
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FL中跨部门实体的原始数据
- en: 'When dealing with cross-silo entities in FL, the raw data exhibits certain
    characteristics. Specifically, in the case of intra-company scenarios, the following
    can be observed:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理FL中的跨部门实体时，原始数据表现出某些特征。具体来说，在内部公司场景中，可以观察到以下情况：
- en: Each client within the cross-silo entities will possess the same kind of features.
    This means that the types of data attributes or variables available for analysis
    will be consistent across all clients.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨部门实体中的每个客户端都将拥有相同类型的特征。这意味着可用于分析的数据属性或变量的类型将在所有客户端中保持一致。
- en: The labels or classes used for classification tasks will also be the same among
    the clients. This ensures that the target categories or outcomes for classification
    are consistent throughout the participating entities.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类任务的标签或类别在客户端之间也将是相同的。这确保了参与实体中的分类目标类别或结果的一致性。
- en: The number of data samples at each client location may vary. This implies that
    the amount of available data may differ across different locations or branches
    within the same company. Some clients may have more extensive datasets, while
    others may have fewer samples.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个客户端位置的数据样本数量可能不同。这意味着可用的数据量可能在不同的位置或同一公司内的不同分支之间有所不同。一些客户端可能有更广泛的数据集，而其他客户端可能有较少的样本。
- en: Not all classes or categories may be represented in each client’s data. This
    results in imbalanced data, where certain classes may be overrepresented or underrepresented
    compared to others. Such imbalances can pose challenges for model training and
    evaluation.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有类别或类别可能在每个客户端的数据中都有表示。这导致数据不平衡，某些类别可能相对于其他类别被过度或不足表示。这种不平衡可能会对模型训练和评估造成挑战。
- en: The distribution of examples may not be the same across all clients. This means
    that the statistical characteristics, such as the mean, variance, or other properties,
    may vary between different client locations. Each client’s data may exhibit unique
    distributional patterns, which need to be accounted for during the FL process.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例的分布可能不会在所有客户端中相同。这意味着统计特征，如均值、方差或其他属性，可能在不同的客户端位置之间有所不同。每个客户端的数据可能表现出独特的分布模式，这些模式需要在FL过程中予以考虑。
- en: Considering these characteristics, FL techniques must address the variability
    in data samples, imbalanced class distributions, and divergent data distributions
    across the cross-silo entities.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些特征，FL技术必须解决数据样本的变异性、类别分布的不平衡以及跨部门实体间的数据分布差异。
- en: In the context of the banking example we discussed, since it is the same bank
    operating in different countries, the features (such as customer ID, amount, transaction
    date, source account, destination account, and address) and labels (*fraud* or
    *non-fraud*) will be the same. However, the distribution of data samples and labels
    may vary at each location, based on factors such as the number of customers and
    the types of transactions. This introduces non-IID characteristics to the data,
    requiring careful handling in FL approaches.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论的银行示例的背景下，由于它是同一银行在不同国家运营，因此特征（如客户ID、金额、交易日期、源账户、目标账户和地址）和标签（*欺诈*或*非欺诈*）将是相同的。然而，数据样本和标签的分布可能因每个位置的客户数量和交易类型等因素而异。这给数据引入了非独立同分布（non-IID）的特性，需要在联邦学习方法中谨慎处理。
- en: '![Figure 6.5 – The ML model in a financial bank in three locations](img/B16573_06_05.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 三地金融银行中的机器学习模型](img/B16573_06_05.jpg)'
- en: Figure 6.5 – The ML model in a financial bank in three locations
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 三地金融银行中的机器学习模型
- en: Data is distributed in the following way to each client. There is skewness in
    the label data but samples with all features exist in each location/client/entity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以以下方式分配给每个客户端。标签数据存在偏斜，但每个位置/客户端/实体中均存在具有所有特征的样本。
- en: '| Data at different clients | FeaturesX={ X1, X2, X3, …Xn} | Labely = { y1,
    y2…, ym} |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 不同客户端的数据 | 特征X={ X1, X2, X3, …Xn} | 标签y = { y1, y2…, ym} |'
- en: '| X1 | X2 | X3 | X4 | Fraud data label counts |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| X1 | X2 | X3 | X4 | 欺诈数据标签计数 |'
- en: '| Europe(Client 1) | yes | yes | yes | yes | Fraud count = N, Non-fraud = 0
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 欧洲（客户端1） | 是 | 是 | 是 | 是 | 欺诈计数 = N，非欺诈 = 0 |'
- en: '| US(Client 2 | yes | yes | yes | yes | Fraud count = 0, Non-fraud = N |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 美国（客户端2） | 是 | 是 | 是 | 是 | 欺诈计数 = 0，非欺诈 = N |'
- en: '| India(Client 3) | yes | yes | yes | yes | Fraud count = N/2, Non-fraud =
    N/4 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 印度（客户端3） | 是 | 是 | 是 | 是 | 欺诈计数 = N/2，非欺诈 = N/4 |'
- en: Table 6.1 – Label data skewness
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – 标签数据偏斜
- en: 'In the case of intra-institutions, where different institutions within the
    same industry participate in FL to offer similar ML services, the data may exhibit
    the following characteristics:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在机构内部的情况下，即同一行业内不同机构参与联邦学习（FL）以提供类似的机器学习（ML）服务时，数据可能表现出以下特征：
- en: Each client, representing a different institution, may or may not have the same
    kind of features. This means that the available data attributes or variables may
    differ between institutions, based on their specific contexts or data collection
    practices.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个客户端，代表不同的机构，可能具有或不具有相同类型的特征。这意味着根据它们的具体背景或数据收集实践，不同机构之间可用的数据属性或变量可能不同。
- en: The number of data samples at each client location may vary. This indicates
    that the amount of data available for analysis could differ between different
    institutions. Some institutions may have larger datasets, while others may have
    relatively smaller ones.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个客户端位置的数据样本数量可能不同。这表明不同机构可用于分析的数据量可能不同。一些机构可能拥有更大的数据集，而其他机构可能拥有相对较小的数据集。
- en: Not all classes or categories may be present in each client’s data. This can
    result in imbalanced data, where certain classes may be underrepresented or missing
    altogether in some institutions’ datasets. Handling imbalanced data is an important
    consideration in the FL process.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有类别或类别都可能在每个客户端的数据中存在。这可能导致数据不平衡，某些类别可能在某些机构的数据集中代表性不足或完全缺失。在联邦学习过程中处理不平衡数据是一个重要的考虑因素。
- en: The distribution of examples may also differ among the participating institutions.
    Each institution’s data may have its own unique distributional patterns, including
    variations in mean, variance, or other statistical properties. These differences
    need to be taken into account during the collaborative model training process.![Figure
    6.6 – FL client and server communication (send and receive) model parameters](img/B16573_06_06.jpg)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参与机构之间的示例分布也可能不同。每个机构的数据可能具有其独特的分布模式，包括均值、方差或其他统计特性的变化。这些差异需要在协作模型训练过程中予以考虑。![图6.6
    – FL客户端和服务器通信（发送和接收）模型参数](img/B16573_06_06.jpg)
- en: Figure 6.6 – FL client and server communication (send and receive) model parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – FL客户端和服务器通信（发送和接收）模型参数
- en: Data is distributed in the following way to each client. In this scenario, there
    is skewness in features and label data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以以下方式分配给每个客户端。在此场景中，特征和标签数据存在偏斜。
- en: '| Data at different clients | FeaturesX={ X1, X2, X3, …Xn} | Labely = { y1,
    y2…, yn} |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 不同客户端的数据 | 特征X={ X1, X2, X3, …Xn} | 标签y = { y1, y2…, yn} |'
- en: '| --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| X1 | X2 | X3 | X4 | Fraud data label counts |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| Client 1 | yes |  | yes |  | No (Non-Fraud)= 70% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| Client 2 |  | yes |  | yes | Yes (Fraud)= 100% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| Client 3 | yes | yes | yes | yes | Yes (Fraud) = 50%, No (Non-Fraud)=50%
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: Table 6.2 – Feature and label data skewness at different clients
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Learning objective
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In FL, the central server takes on the responsibility of executing the learning
    objective and minimizing the loss function. It achieves this by leveraging the
    model weights (*Wt*) and biases received from the participating clients. The server
    determines the number of rounds of data it needs from the clients and the specific
    clients that need to participate.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an example where there are three clients involved in the FL
    process. Each client sends its respective model weights and biases to the central
    server. The server then performs the following objective or learning function
    to minimize the loss:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '*minimize loss (**Wt, biases)*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: The objective of the server is to optimize the model parameters, represented
    by the weights (*Wt*) and biases, to minimize the loss function. By utilizing
    the received weights and biases from the participating clients, the server performs
    iterative updates to refine the model and improve its performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The specific details of the learning objective and loss function depend on the
    specific ML algorithm and the task at hand. The central server orchestrates the
    aggregation of client updates, manages the training process, and sends back the
    updated model to the clients. This collaborative approach enables the clients
    to collectively contribute their local knowledge while benefiting from the improved
    global model provided by the server.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the objective function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Min f(w) = ∑ i=1 n   £i * Fi(w)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Here, *w* is the model parameters (weights, and so on), *f(w)* is the objective
    function, and *n* is the number of clients participating in FL.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: A few more mathematical terms will be used in the next section including the
    following.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '*Wt*: Model weights in the communication round *t* (client to server)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wt k*: Model weights in the communication round on client *k*'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*C*: The number of clients participating in each round to update the model
    and compute the weights'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*B*: The local clients’ batch size of the data samples'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pk*: The set of data samples at client *k*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*nk*: The number of data points at client *k*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*fi (w)*: *loss L ( xi, yi, w)* – the loss function'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the server side, various objective functions can be implemented, depending
    on the specific requirements and goals of the FL process.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: FL algorithms
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: FL algorithms, such as FedSGD, FedAvg, and Adaptive Federated Optimization,
    play a crucial role in the distributed training of ML models while ensuring privacy
    and security. In this section, we will explore these algorithms and their key
    characteristics.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: FedSGD
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Federated stochastic gradient descent** (**FedSGD**) is a fundamental algorithm
    used in FL. It extends the traditional SGD optimization method to the federated
    setting. In FedSGD, each client (entity) computes the gradients on its local data
    and sends them to the central server. The server aggregates the gradients and
    updates the global model parameters accordingly. FedSGD is efficient for large-scale
    distributed training but may suffer from issues related to non-IID data and communication
    efficiency.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦随机梯度下降**（**FedSGD**）是FL中使用的根本算法。它将传统的SGD优化方法扩展到联邦设置。在FedSGD中，每个客户端（实体）在其本地数据上计算梯度，并将它们发送到中央服务器。服务器聚合梯度并相应地更新全局模型参数。FedSGD适用于大规模分布式训练，但可能受到与非-IID数据和通信效率相关的问题的影响。'
- en: '![Figure 6.7 – The FedSGD model weights exchange with the server](img/B16573_06_07.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – FedSGD模型权重与服务器交换](img/B16573_06_07.jpg)'
- en: Figure 6.7 – The FedSGD model weights exchange with the server
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – FedSGD模型权重与服务器交换
- en: 'Let’s look at the FedSGD algorithm:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看FedSGD算法：
- en: '| **Server-side algorithm** | **Client-side algorithm** |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **服务器端算法** | **客户端算法** |'
- en: '| Initialize weights (w0)for each round t = 1,2, …m = max (C, K, 1)st = random
    set of m clientsfor client k in st,wt+1 = client-side function (k, wt)wt+1 = average
    of weights | Client-side function (k, w):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '| 初始化每个轮次t = 1,2, …m = max (C, K, 1)st = 随机选择的m个客户端for客户端k在st中，wt+1 = 客户端函数(k,
    wt)wt+1 = 权重的平均值 | 客户端函数(k, w):'
- en: Split the data in *k* batches, with each batch based on the batch size *B* (complete
    local dataset)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分成*k*个批次，每个批次基于批次大小*B*（完整本地数据集）
- en: 'For each batch:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个批次：
- en: fi (w) = loss L (xi, yi, w)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fi (w) = 损失 L (xi, yi, w)
- en: w = w – learning rate * loss
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: w = w – 学习率 * 损失
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 6.3 - FedSGD Algorithem
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 - FedSGD算法
- en: 'On the client side, each participating client performs the following steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端，每个参与客户端执行以下步骤：
- en: '**Data partitioning**: Clients have their own local datasets and partition
    them into smaller subsets to ensure privacy.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据分区**：客户端拥有自己的本地数据集，并将它们分成更小的子集以确保隐私。'
- en: '**Local model training**: Each client independently trains the shared model
    using its local data. This involves computing the gradients of the model parameters
    (weights and biases) on the local dataset using SGD or a variant.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**本地模型训练**：每个客户端独立使用其本地数据训练共享模型。这涉及到使用SGD或其变体在本地数据集上计算模型参数（权重和偏差）的梯度。'
- en: '**Model update**: After the local model training, the client sends the computed
    gradients to the server for aggregation.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型更新**：在本地模型训练后，客户端将计算的梯度发送到服务器进行聚合。'
- en: 'On the server side, the central server performs the following steps:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器端，中央服务器执行以下步骤：
- en: '**Aggregation**: The server receives the gradients from all participating clients
    and aggregates them using various aggregation techniques, such as averaging or
    weighted averaging.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聚合**：服务器从所有参与客户端接收梯度，并使用各种聚合技术（如平均或加权平均）进行聚合。'
- en: '**Model update**: The aggregated gradients are used to update the global model’s
    parameters. The server applies the received gradients to the global model, adjusting
    its weights and biases to reflect the collective knowledge from all clients.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型更新**：聚合的梯度用于更新全局模型参数。服务器将接收到的梯度应用于全局模型，调整其权重和偏差以反映所有客户端的集体知识。'
- en: '**Model distribution**: The updated global model is then sent back to the clients
    for the next round of training, ensuring that each client benefits from the collective
    knowledge while preserving data privacy.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型分发**：然后，更新后的全局模型被发送回客户端进行下一轮训练，确保每个客户端都能从集体知识中受益，同时保护数据隐私。'
- en: FedSGD aims to minimize the communication overhead between the clients and the
    server by exchanging only the model gradients rather than the raw data. This allows
    for distributed model training while maintaining data privacy and security. However,
    it is important to address challenges such as data heterogeneity and non-IID data
    distribution, which can impact the convergence and performance of the FL process.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: FedSGD旨在通过仅交换模型梯度而不是原始数据来最小化客户端和服务器之间的通信开销。这允许在保持数据隐私和安全的同时进行分布式模型训练。然而，解决诸如数据异质性和非-IID数据分布等挑战非常重要，这些挑战可能会影响FL过程的收敛性和性能。
- en: Overall, FedSGD enables collaborative model training in a decentralized manner,
    leveraging the computational resources of multiple clients while preserving data
    privacy. It serves as a foundational algorithm for FL and has paved the way for
    more advanced techniques to improve the efficiency and effectiveness of distributed
    ML.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，FedSGD以去中心化的方式实现了协作模型训练，利用了多个客户端的计算资源，同时保护了数据隐私。它是联邦学习的基础算法，为更高级的技术的开发铺平了道路，以改进分布式机器学习的效率和效果。
- en: FedAvg
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FedAvg
- en: '**Federated averaging** (**FedAvg**) is a widely adopted FL algorithm designed
    to address the challenges of non-IID data and communication efficiency. In FedAvg,
    similar to FedSGD, each client computes the gradients on its local data. However,
    instead of directly updating the global model with the individual gradients, FedAvg
    employs weighted averaging to combine the client models’ parameters. This approach
    allows for better handling of data heterogeneity and reduces the communication
    overhead between the clients and the server.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦平均**（**FedAvg**）是一种广泛采用的联邦学习算法，旨在解决非独立同分布数据和非高效通信的挑战。在FedAvg中，类似于FedSGD，每个客户端在其本地数据上计算梯度。然而，FedAvg不是直接使用单个梯度更新全局模型，而是采用加权平均来组合客户端模型的参数。这种方法可以更好地处理数据异构性，并减少客户端与服务器之间的通信开销。'
- en: '![Figure 6.8 – The FedAvg model weights exchange with the server](img/B16573_06_08.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – FedAvg模型与服务器之间的权重交换](img/B16573_06_08.jpg)'
- en: Figure 6.8 – The FedAvg model weights exchange with the server
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – FedAvg模型与服务器之间的权重交换
- en: 'Let’s look at the FedAvg algorithm:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看FedAvg算法：
- en: '| **Server-side algorithm** | **Client-side algorithm** |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| **服务器端算法** | **客户端算法** |'
- en: '| Initialize weights (w0)for each round t = 1,2, …m = max (C, K, 1)st = random
    set of m clientsfor client k in st,wt+1 = client-side function (k, wt)wt+1 = average
    of gradients | Client-side function (k, w):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '| 初始化每轮t = 1,2, …m = max (C, K, 1)的权重(w0)st = 随机选择m个客户端for客户端k在st中，wt+1 = 客户端函数(k,
    wt)wt+1 = 梯度的平均值 | 客户端函数(k, w):'
- en: Split the data into *k* batches, with each batch based on the batch size *B*
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分成*k*个批次，每个批次基于批次大小*B*
- en: For each epoch in training E
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于训练中的每个epoch E
- en: 'For each batch:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个批次：
- en: fi (w) = loss L (xi, yi, w)
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: fi (w) = 损失 L (xi, yi, w)
- en: w = w – learning rate * loss
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: w = w – 学习率 * 损失
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 6.4 – FedAVG Algorithm
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4 – FedAVG算法
- en: FedAvg leverages the concept of averaging to combine the locally trained models
    of different clients, which helps mitigate the impact of data heterogeneity and
    non-IID data distribution. By averaging the model parameters, FedAvg effectively
    creates a global model that captures insights from all participating clients while
    preserving the privacy of individual data. The iterative nature of FedAvg allows
    the shared model to progressively improve with each round of training. As the
    process continues, the global model becomes more refined and represents the collective
    knowledge of all clients.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: FedAvg利用平均的概念来组合不同客户端的本地训练模型，这有助于减轻数据异构性和非独立同分布数据分布的影响。通过平均模型参数，FedAvg有效地创建了一个全局模型，该模型捕捉了所有参与客户端的见解，同时保护了个人数据的隐私。FedAvg的迭代性质允许共享模型在每一轮训练中逐步改进。随着过程的继续，全局模型变得更加精细，代表了所有客户端的集体知识。
- en: Overall, FedAvg enables collaborative training of a shared model in a privacy-preserving
    manner. It addresses challenges associated with data privacy and distribution,
    allowing multiple clients to contribute to the model’s improvement without sharing
    their raw data. FedAvg has been instrumental in advancing the field of FL, enabling
    applications in various domains while maintaining data privacy and security.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，FedAvg以保护隐私的方式实现了共享模型的协作训练。它解决了与数据隐私和分布相关的挑战，允许多个客户端在不共享原始数据的情况下为模型改进做出贡献。FedAvg在联邦学习的领域中发挥了重要作用，使得在各个领域中的应用成为可能，同时保持了数据隐私和安全。
- en: Fed Adaptative Optimization
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fed Adaptative Optimization
- en: In cross-device FL, a multitude of clients communicate with a central server,
    and each client possesses a unique set of data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨设备联邦学习中，众多客户端与一个中央服务器通信，每个客户端拥有独特的数据集。
- en: For instance, in the context of next-word prediction on phones, different users’
    phones contain distinct word sets based on factors such as country, region, and
    language. However, traditional FL algorithms such as FedSGD and FedAvg may not
    perform optimally when confronted with heterogeneous data from diverse clients.
    The challenge arises from the inherent differences in data distribution and characteristics
    among the clients. Heterogeneous data introduces complexities that can impact
    the convergence and performance of FL algorithms. As a result, handling heterogeneous
    data poses a considerable obstacle compared to scenarios where the clients have
    homogeneous data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在手机上的下一个单词预测的背景下，不同用户的手机根据国家、地区和语言等因素包含不同的单词集。然而，当面对来自不同客户端的异构数据时，传统的 FL
    算法如 FedSGD 和 FedAvg 可能无法表现最佳。挑战源于客户端之间数据分布和特性的固有差异。异构数据引入的复杂性可能会影响 FL 算法的收敛性和性能。因此，与客户端具有同质数据的情况相比，处理异构数据构成了相当大的障碍。
- en: Efforts are being made to address the challenges associated with heterogeneous
    data in cross-device FL.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正在努力解决跨设备联邦学习（FL）中与异构数据相关联的挑战。
- en: In order to overcome this, researchers at Google (*Sashank J. Reddi et al.,
    2021*) proposed new adaptative optimizations in the research paper published at
    arxiv.org/abs/2003.00295.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一点，谷歌的研究人员（Sashank J. Reddi 等人，2021年）在 arxiv.org/abs/2003.00295 发表的论文中提出了新的自适应优化方法。
- en: 'Here is the detailed algorithm (the image is sourced from the preceding URL):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是详细的算法（图片来源于前面的 URL）：
- en: '![Figure 6.9 – The Fed Adaptive Optimization algorithm proposed by Google researchers](img/B16573_06_09.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 谷歌研究人员提出的 Fed Adaptive Optimization 算法](img/B16573_06_09.jpg)'
- en: Figure 6.9 – The Fed Adaptive Optimization algorithm proposed by Google researchers
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 谷歌研究人员提出的 Fed Adaptive Optimization 算法
- en: Please refer to the article for a detailed explanation of the Adaptive Optimization
    algorithm. In a nutshell, the idea is to optimize the communication cost like
    FEDAVG and work in cross-device settings.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅文章以获取自适应优化算法的详细解释。简而言之，这个想法是优化通信成本，类似于 FEDAVG，并在跨设备环境中工作。
- en: The steps involved in implementing FL
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施联邦学习所涉及的步骤
- en: 'The following are the five steps that are typically followed to implement FL.
    There can be alternatives/changes to these steps, but initially, these are the
    steps that need to be followed:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下通常是实现联邦学习（FL）所遵循的五个步骤。这些步骤可能有替代方案/变化，但最初，这些是需要遵循的步骤：
- en: '**The server side – the initialization of the global model**: In this step,
    the server starts and accepts the client requests. Before actually starting the
    server, the model on the server side will be initiated with model parameters.
    Typically, model parameters will be initiated with zeros or from the previous
    checkpoint model.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务器端 – 全局模型的初始化**：在这个步骤中，服务器启动并接受客户端请求。在实际启动服务器之前，服务器端的模型将使用模型参数进行初始化。通常，模型参数将初始化为零或从之前的检查点模型中获取。'
- en: '**The server sends model parameters to all or a subset of clients**: In this
    step, the server sends the initial model parameters to all clients (for cross-silo
    FL clients, they will be within the same institutions and may only be numbered
    in the tens) or a subset of clients (in the case of cross-device FL where devices
    are in the millions, the server decides to select only a subset from the total
    devices). Each client will make use of these initial model parameters for the
    local training of the model.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务器将模型参数发送到所有或部分客户端**：在这个步骤中，服务器将初始模型参数发送到所有客户端（对于跨沙盒 FL 客户端，它们将位于同一机构内，可能只有几十个）或部分客户端（在跨设备
    FL 的情况下，设备数以百万计，服务器决定只从总数中选择一部分）。每个客户端将使用这些初始模型参数进行模型的本地训练。'
- en: '**The clients train the model and send the model weights/parameters back to
    the server**: In this step, each client will train the model with their local
    data, making use of the entire local data in one shot, dividing the data into
    several batches, or splitting the data randomly and making use of the different
    splits for different rounds (a multiple rounds of exchanges of model parameters
    between the client and server). The clients will send the model parameters or
    weights only to the server.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户端训练模型并将模型权重/参数发送回服务器**：在这个步骤中，每个客户端将使用其本地数据训练模型，一次性使用全部本地数据，将数据分成几个批次，或者随机分割数据并利用不同轮次（客户端和服务器之间多次交换模型参数）的不同分割。客户端只会将模型参数或权重发送到服务器。'
- en: '**The server executes one of the FL algorithms, updates the global model, and
    sends the updated weights to the client for the next round**: In this step, the
    server will run one of the FL algorithms and make use of the weights received
    by the clients to update the global model. In the case of FedAvg, it will calculate
    the weighted average of the weights received from clients and send the updated
    weights back to the client for the next round.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务器执行一个FL算法，更新全局模型，并将更新的权重发送给客户端进行下一轮**：在这一步中，服务器将运行一个FL算法，并利用从客户端接收到的权重来更新全局模型。在FedAvg的情况下，它将计算从客户端接收到的权重的加权平均值，并将更新的权重发送回客户端进行下一轮。'
- en: '**Repeat steps 2 to 4 based on the number of rounds configured**: Repeat *steps
    2* to *4* for each round. If five rounds are configured, then repeat *steps 2*
    to *4* five times, and after the last round, clients will make use of the weights
    received by the server for the final ML model. Clients can make use of these model
    weights either at the end of the last round or in each round and evaluate the
    model’s accuracy with the test data.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**根据配置的轮数重复步骤2到4**：对于每一轮重复步骤2到4。如果配置了五轮，那么重复步骤2到4五次，在最后一轮之后，客户端将利用从服务器接收到的权重来使用最终的机器学习模型。客户端可以在最后一轮结束时或每一轮中使用这些模型权重，并使用测试数据评估模型的准确性。'
- en: 'The following sequence diagram shows these steps in detail:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下序列图详细展示了这些步骤：
- en: '![Figure 6.10 – The steps in the FL sequence diagram](img/B16573_06_10.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – FL序列图中的步骤](img/B16573_06_10.jpg)'
- en: Figure 6.10 – The steps in the FL sequence diagram
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – FL序列图中的步骤
- en: The sequence diagram shows the detailed interactions between the server and
    the clients participating in the FL, performing four high-level steps as explained
    in this section.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 序列图显示了参与联邦学习（FL）的服务器和客户端之间的详细交互，按照本节所述执行四个高级步骤。
- en: Open source frameworks to implement FL
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现FL的开源框架
- en: There are a few open source frameworks to implement FL at scale. The following
    are some of the most popular.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个开源框架可以用于大规模实现FL。以下是一些最受欢迎的。
- en: '**PySyft** ([https://github.com/OpenMined/PySyft](https://github.com/OpenMined/PySyft)),
    developed by OpenMined, is an open source stack that offers secure and private
    data science capabilities in Python. It introduces a separation between private
    data and model training, enabling functionalities such as FL, differential privacy,
    and encrypted computation. Initially, PySyft utilized the Opacus framework to
    support differential privacy, as discussed in the Differential privacy chapter.
    However, the latest version of PySyft incorporates its own differential privacy
    component to provide enhanced functionality and efficiency in preserving privacy
    while performing data analysis tasks.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**PySyft**（[https://github.com/OpenMined/PySyft](https://github.com/OpenMined/PySyft)），由OpenMined开发，是一个开源栈，它提供了在Python中安全且私有的数据科学能力。它引入了私有数据和模型训练之间的分离，使得FL、差分隐私和加密计算等功能成为可能。最初，PySyft利用Opacus框架来支持差分隐私，如差分隐私章节所述。然而，PySyft的最新版本集成了自己的差分隐私组件，以提供在执行数据分析任务时保护隐私的同时增强功能和效率。'
- en: TensorFlow Federated
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Federated
- en: '**TensorFlow Federated** (**TFF**) is a library developed by Google that facilitates
    the training of shared ML models across multiple clients using their local data
    ([https://www.tensorflow.org/federated](https://www.tensorflow.org/federated)).
    TFF consists of two layers – the Federated Core API and the Federated Learning
    API.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow Federated**（**TFF**）是由谷歌开发的一个库，它通过使用客户端的本地数据来简化跨多个客户端训练共享机器学习模型的过程（[https://www.tensorflow.org/federated](https://www.tensorflow.org/federated)）。TFF由两层组成——联邦核心API和联邦学习API。'
- en: The Federated Core API offers low-level interfaces for tasks such as data serialization,
    distribution communication between the server and clients, and implementation
    of FL algorithms. It provides the foundational components necessary to build FL
    systems.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦核心API提供了低级接口，用于数据序列化、服务器和客户端之间的分布式通信以及FL算法的实现。它提供了构建FL系统所需的基础组件。
- en: Conversely, the Federated Learning API provides a higher-level interface that
    allows users to easily construct FL models or wrap existing models as FL models.
    It offers a set of APIs for training and evaluating models using federated computations
    and datasets. This higher-level interface abstracts away some of the complexities
    involved in building and training FL models, making it more accessible and convenient
    for developers.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，联邦学习API提供了一个高级接口，使用户能够轻松构建联邦学习模型或将现有模型包装为联邦学习模型。它提供了一套API，用于使用联邦计算和数据集进行模型训练和评估。这个高级接口抽象了一些构建和训练联邦学习模型所涉及到的复杂性，使得它对开发者来说更加易于访问和方便。
- en: By providing these two layers, TFF empowers researchers and developers to leverage
    the power of FL in their projects. It simplifies the process of building and training
    models on decentralized data while ensuring privacy and data security.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供这两层，TFF使研究人员和开发者能够在其项目中利用联邦学习的力量。它简化了在去中心化数据上构建和训练模型的过程，同时确保隐私和数据安全。
- en: Flower
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flower
- en: '**Flower** ([https://flower.dev/](https://flower.dev/)) is an open source framework
    that aims to provide a user-friendly experience. It supports ML and DL models
    developed using various frameworks, such as scikit-learn, TensorFlow, PyTorch,
    PyTorch Lightning, MXNet, and JAX. Flower makes it easy to convert these models
    into FL models.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**Flower** ([https://flower.dev/](https://flower.dev/)) 是一个开源框架，旨在提供用户友好的体验。它支持使用各种框架开发的机器学习和深度学习模型，例如scikit-learn、TensorFlow、PyTorch、PyTorch
    Lightning、MXNet和JAX。Flower使得将这些模型转换为联邦学习模型变得容易。'
- en: One of the key features of Flower is its communication implementation, which
    is built on top of bidirectional gRPC streams. This enables an efficient and seamless
    exchange of multiple messages between clients and the server without the need
    to establish a new connection for each message request.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Flower的一个关键特性是其通信实现，它建立在双向gRPC流之上。这允许客户端和服务器之间高效且无缝地交换多个消息，而无需为每个消息请求建立新的连接。
- en: Flower offers a range of strategies and implements several FL algorithms on
    the server side. These algorithms include FedAvg, FedSGD, Fault Tolerance FedAvg,
    FedProxy, and FedOptim (which consists of FedAdagrad, FedYogi, and FedAdam). These
    algorithms provide different approaches to model aggregation and training in FL
    scenarios.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Flower提供了一系列策略，并在服务器端实现了几个联邦学习算法。这些算法包括FedAvg、FedSGD、容错FedAvg、FedProxy和FedOptim（包括FedAdagrad、FedYogi和FedAdam）。这些算法在联邦学习场景中提供了不同的模型聚合和训练方法。
- en: '![Figure 6.11 – The Flower framework architecture diagram (simplified)](img/B16573_06_11.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – Flower框架架构图（简化）](img/B16573_06_11.jpg)'
- en: Figure 6.11 – The Flower framework architecture diagram (simplified)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – Flower框架架构图（简化）
- en: To validate its performance, Flower has been extensively benchmarked. The framework
    has demonstrated the ability to scale up to 15 million clients using only two
    GPU servers. These experiments were compared with FedScale, another FL engine
    and benchmark suite, to evaluate Flower’s performance and efficiency in large-scale
    FL settings.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证其性能，Flower已经进行了广泛的基准测试。该框架已经证明了仅使用两个GPU服务器就能扩展到1500万客户端的能力。这些实验与另一个联邦学习引擎和基准测试套件FedScale进行了比较，以评估Flower在大型联邦学习环境中的性能和效率。
- en: An end-to-end use case of implementing fraud detection using FL
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用联邦学习实现欺诈检测的端到端用例
- en: Fraud detection is a critical task for many industries, including finance, e-commerce,
    and healthcare. Traditional fraud detection methods often rely on centralized
    data collection, where sensitive customer information is gathered and analyzed
    in a single location. However, this approach raises concerns about data privacy
    and security, as well as compliance with regulations such as the GDPR.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测对许多行业，包括金融、电子商务和医疗保健，都是一个关键任务。传统的欺诈检测方法通常依赖于集中式数据收集，其中敏感的客户信息在单一地点收集和分析。然而，这种方法引发了关于数据隐私和安全以及遵守GDPR等法规的担忧。
- en: FL offers a promising solution to address these challenges. By leveraging the
    power of distributed computing and collaborative learning, FL enables fraud detection
    models to be trained directly on the devices or local servers of individual institutions,
    without the need for data sharing. This decentralized approach ensures that sensitive
    customer data remains private and secure, as it never leaves the local environment.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: FL为解决这些挑战提供了一个有希望的解决方案。通过利用分布式计算和协作学习的力量，FL使得欺诈检测模型可以直接在单个机构的设备或本地服务器上训练，无需共享数据。这种去中心化的方法确保敏感客户数据保持私密和安全，因为它从未离开本地环境。
- en: Implementing fraud detection using FL involves several key steps. Firstly, a
    consortium of institutions or organizations, such as banks or e-commerce platforms,
    need to establish an FL framework that enables them to collaborate on model training
    while preserving data privacy. This may involve the adoption of FL libraries or
    platforms such as TFF or Flower.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用联邦学习（FL）实现欺诈检测涉及几个关键步骤。首先，由银行或电子商务平台等机构或组织组成的联盟需要建立一个FL框架，使他们能够在保护数据隐私的同时协作进行模型训练。这可能涉及采用FL库或平台，如TFF或Flower。
- en: Next, the participating institutions define a common fraud detection objective
    and develop a shared model architecture. Each institution then trains its local
    model using its own private data, which may include transaction records, user
    behavior patterns, and other relevant features. The models are trained locally,
    ensuring that sensitive data remains under the control of the respective institutions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，参与机构定义一个共同的欺诈检测目标并开发共享的模型架构。然后，每个机构使用自己的私有数据训练其本地模型，这些数据可能包括交易记录、用户行为模式和其他相关特征。模型在本地进行训练，确保敏感数据始终处于相应机构的控制之下。
- en: To facilitate collaborative learning, the institutions periodically share model
    updates with a central server. These updates, which typically include model weights
    and parameters, are aggregated using federated averaging or other aggregation
    techniques to create a global model that captures insights from all participants,
    while preserving the privacy of individual data.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进协作学习，机构定期将模型更新与中央服务器共享。这些更新通常包括模型权重和参数，通过联邦平均或其他聚合技术进行汇总，以创建一个全局模型，该模型能够捕捉到所有参与者的洞察，同时保护个人数据的隐私。
- en: The central server, which oversees the aggregation process, ensures that the
    global model is refined based on the collective knowledge of the participating
    institutions. This process allows the model to learn from a diverse range of fraud
    patterns and adapt to evolving fraudulent activities while maintaining data privacy
    and compliance with regulations.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 负责监督聚合过程的中央服务器确保全局模型基于参与机构的集体知识进行优化。这个过程允许模型从各种欺诈模式中学习，并适应不断发展的欺诈活动，同时保持数据隐私并符合法规。
- en: Implementing fraud detection using FL offers several advantages. It allows institutions
    to leverage a larger and more diverse dataset, leading to improved fraud detection
    accuracy. It also reduces the risks associated with data breaches or unauthorized
    access, since sensitive data remains under the control of the respective institutions.
    Additionally, FL enables real-time updates and faster model deployment, allowing
    institutions to respond quickly to emerging fraud patterns.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FL实现欺诈检测提供了几个优势。它允许机构利用更大、更多样化的数据集，从而提高欺诈检测的准确性。它还降低了数据泄露或未经授权访问的风险，因为敏感数据始终处于相应机构的控制之下。此外，FL可以实现实时更新和更快的模型部署，使机构能够迅速应对新出现的欺诈模式。
- en: Implementing fraud detection using FL offers a privacy-preserving and collaborative
    approach to combat fraud in various industries. By combining the power of distributed
    computing and shared learning, organizations can enhance fraud detection capabilities
    while safeguarding sensitive customer data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FL实现欺诈检测为各种行业中的欺诈提供了隐私保护和协作的方法。通过结合分布式计算和共享学习的力量，组织可以增强欺诈检测能力，同时保护敏感客户数据。
- en: Let’s implement this use case using the Flower framework and the open source
    dataset.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Flower框架和开源数据集来实现这个用例。
- en: Developing an FL model for fraud detection using the Flower framework
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Flower框架开发用于欺诈检测的FL模型
- en: In this example, we will leverage the Flower framework to develop an FL model
    for fraud detection. The implementation will involve both server-side and client-side
    components. To illustrate the process, we will set up one server and two clients.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The communication between the server and clients will occur over several rounds,
    with the exchange of weights and parameters. The exact number of rounds may vary
    depending on the specific scenario, but typically, the communication continues
    until the weights converge or a predetermined convergence criterion is met.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: On the server side, we will implement the FedAvg algorithm to aggregate the
    weights received from the clients. FedAvg is a widely used algorithm in FL that
    combines the knowledge from multiple clients to create a global model.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: For the fraud detection task, we will develop an actual linear regression model
    using the scikit-learn library. This model will be trained using the data available
    at each client, which consists of transaction records and relevant features. The
    goal is to classify whether a transaction is fraudulent or not.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The client-side implementation will involve training the local linear regression
    models using the respective client’s data. The clients will then communicate with
    the server, exchanging their model weights and parameters over the predefined
    rounds. This collaborative learning process allows the clients to contribute their
    local insights to the global model while preserving the privacy of their data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The server will receive the model updates from the clients and perform the aggregation
    step using the FedAvg algorithm. This aggregation process ensures that the global
    model incorporates the knowledge learned from all the participating clients, resulting
    in an enhanced fraud detection capability.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the implementation, the Flower framework provides the necessary infrastructure
    for the communication between the server and clients. It abstracts the underlying
    complexities of distributed computing and handles the synchronization of model
    updates. By developing an FL model for fraud detection, we can leverage the distributed
    knowledge and data from multiple clients to improve the accuracy of fraud classification.
    The federated approach also addresses privacy concerns by keeping the sensitive
    transaction data local to each client.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: In summary, this project demonstrates the implementation of an FL model using
    the Flower framework. The server and clients collaborate to train a global model
    for fraud detection, exchanging model weights and parameters over multiple communication
    rounds. By aggregating the client models using FedAvg, we can leverage the collective
    intelligence of multiple participants while ensuring data privacy.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used in the example
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Lopez, Elmir, and Axelsson* developed a mobile money dataset for fraud detection,
    and it is featured on Kaggle as well (E. A. Lopez-Rojas, A. Elmir, and S. Axelsson,
    *PaySim: A financial mobile money simulator for fraud detection*, 28th European
    Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus. 2016).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: We will make use of this dataset for the detection of fraud using FL, but the
    same can be extended to anti-money laundering use cases as well, with minor changes
    to the model. The dataset can be found at [https://github.com/EdgarLopezPhD/PaySim](https://github.com/EdgarLopezPhD/PaySim).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Download this dataset and keep the file in the [*Chapter 6*](B16573_06_split_000.xhtml#_idTextAnchor120)
    directory with the name `PS_20174392719_1491204439457_log.csv`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset consists of 6.3 million records of transactions and has the following
    features:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '| **Field** | **Data type** | **Details** |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| `Step` | Numerical | The unit of time in the real world. One step is 1 hour.
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| `Type` | Object | `CASH-IN`, `CASH-OUT`, `DEBIT`, `PAYMENT`, and `TRANSFER`.
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| `Amount` | Numerical | The amount of the transaction. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| `nameOrig` | Object | The customer who started the transaction. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| `nameDest` | Object | The recipient ID of the transaction. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| `oldbalanceOrg` | Numerical | The initial balance before the transaction.
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| `newbalanceOrig` | Numerical | The customer’s balance after the transaction.
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| `oldbalanceDest` | Numerical | The initial recipient’s balance before the
    transaction. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: '| `newbalanceDest` | Numerical | The recipient’s balance after the transaction.
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
- en: '| `isFraud` | Boolean | Identifies fraudulent (`1`) and non-fraudulent (`0`)
    transactions. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
- en: '| `isFlaggedFraud` | Boolean | Flags illegal attempts to transfer more than
    200,000 amount in a single transaction. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
- en: Table 6.5 – The dataset features
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The installation of Flower
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Install Flower using the `python -m pip install` `flwr` command.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of a server
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The server-side implementation of the FL model for fraud detection involves
    several high-level steps. We will utilize the sample code provided by the Flower
    framework and extend it to fit our specific use case.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps outline the process:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialize the** **model parameters**:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial model weights to **0** and initialize the intercept as **0**
    (since we are working with a regression model).
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the number of classes or labels for classification.
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the number of features used in the model.
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the number of participating clients.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define the supporting functions**: Develop additional functions to load the
    data from clients, define the loss function, and evaluate the model’s performance.
    These functions will help facilitate data handling, calculate the loss during
    training, and assess the model’s accuracy.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Choose the server-side strategy**: Select the FedAvg algorithm as the strategy
    to aggregate the weights received from the clients. FedAvg is a popular choice
    to combine model updates from multiple clients and generate an updated global
    model.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Start** **the server**:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initiate the server-side component, which will orchestrate the FL process.
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The server will communicate with the participating clients, receive their model
    updates, and aggregate the weights using the FedAvg algorithm.
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It will also handle the synchronization of the model updates between the clients
    and ensure the convergence of the global model.
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By following these steps, we can implement the server-side functionality of
    our FL model. The initialization of model parameters, definition of supporting
    functions, selection of the server-side strategy (FedAvg), and starting the server
    itself are crucial in facilitating the collaborative training process among the
    clients. Through this implementation, the server will act as the central coordinator,
    receiving and aggregating the model updates from the clients. It plays a crucial
    role in ensuring the model’s convergence and generating an updated global model
    that incorporates the knowledge from all participating clients.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the following code as `FL_AML_Server.py`:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This results in the following output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Dataset information with few rows and columns](img/B16573_06_12.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Dataset information with few rows and columns
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using all 6 million records, we will use only the first 25,000 records
    in this example implementation:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This results in the following output:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s find the data types of each field in the dataset:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Encode the object data type fields as labels using `LabelEncoder`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Open a terminal and run this program (`python3 FL_AML_Server.py`)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'This results in the following output:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Server startup logs](img/B16573_06_13.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Server startup logs
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Server will run and wait for data from clients to process.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of clients
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The client-side implementation of the FL model for fraud detection involves
    the following steps. We will utilize the provided `NumPyClient` from the Flower
    samples. The steps are as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '**Load** **the data**:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the relevant data to train and test the fraud detection model.
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure the data is properly formatted and available for processing.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Split** **the data**:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the loaded data into training and testing sets. This division allows you
    to evaluate the model’s performance on unseen data.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Shuffle/partition** **the data**:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shuffle or partition the training data into batches.
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly select a partition for each round of communication with the server.
    This ensures that different subsets of the training data are used in each round.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create the linear** **regression model**:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop a simple linear regression model using the chosen framework (for example,
    scikit-learn).
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Configure the model with appropriate settings for the fraud detection task.
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Establish a connection with** **the server**:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establish a connection with the server to send and receive model weights.
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Utilize the provided communication protocol (for example, gRPC) to exchange
    information.
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train** **the model**:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the model with the initial weights received from the server.
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model using the client’s local data and the weights updated by the
    server for each round.
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply appropriate optimization techniques (for example, gradient descent) to
    update the model parameters.
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test** **the model**:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the trained model using the testing data to assess its performance.
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate relevant metrics such as accuracy, precision, recall, or F1 score.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the model’s effectiveness in detecting fraudulent transactions.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By following these steps, we can implement the client-side functionality of
    the FL model. The client will load and partition the data, create the linear regression
    model, establish a connection with the server, train the model using local data
    and updated weights, and evaluate its performance. The client’s role is crucial
    in contributing local knowledge while preserving data privacy. By training on
    their respective local data and participating in the FL process, clients collectively
    improve the global fraud detection model without sharing sensitive information.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Creating a non-IID dataset
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To transform the dataset into a non-IID setting, we can apply the following
    approach:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '**First client (****client 1)**:'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the **Synthetic Minority Oversampling Technique** (**SMOTE**) to oversample
    the fraud transactions
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This technique generates synthetic examples of the minority class (fraudulent
    transactions) to balance the dataset
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, client 1 will have a training dataset with 50,000 samples, consisting
    of 25,000 original transactions and 25,000 synthetic fraud examples created using
    SMOTE
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The distribution of fraud versus non-fraud transactions will be balanced at
    50% for each class
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Second client (****client 2)**:'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leave the transactions as they are without any oversampling or modification
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Client 2 will have a training dataset with the last 25,000 transactions
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The class distribution will reflect the original distribution, with only 2%
    of the transactions classified as fraud
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By employing this approach, we introduce non-identical and imbalanced datasets
    across the two clients. Client 1 will have a balanced dataset with equal representation
    of fraud and non-fraud transactions, while client 2 will have a dataset that mirrors
    the original distribution.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: This non-IID setup allows us to simulate real-world scenarios where different
    clients may have varying distributions of data. Through FL, both clients can contribute
    their local knowledge while training their models on distinct datasets, ultimately
    improving the overall fraud detection model.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Client 1 | Client 2 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| Original transactions | 25,000 | 25,000 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| Transactions generated using SMOTE | 25,000 | 0 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| Total transactions | 50,000 | 25,000 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| Fraud versus non-fraud | 50% and 50% | Fraud: 2.43%(608)Non-fraud: 97.57%
    (24,392) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| Train and test split | 70:30 | 70:30 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| Number of partitions after shuffling the train data with equal size in each
    partition | 10 | 10 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: Table 6.6 – Training data distribution on the client side as non-IID data
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for client 1\. Save this code as `FL_AML_Client1.py`:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This results in the following output:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this dataset, fraudulent transactions account for 0.33% of the total data,
    indicating a highly imbalanced dataset. This imbalance is typical in real-world
    scenarios, where fraud transactions are much less frequent compared to genuine
    (non-fraud) transactions.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Encode the object types as labels using sci-kit learn’s `LabelEncoder`:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Apply SMOTE to generate synthetic data:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in the following output:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Open a second terminal and run the client 1 code (`python3 FL_AML_Client1.py`)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'This results in the following output:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – The execution of client 1 and the logs](img/B16573_06_14.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – The execution of client 1 and the logs
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the code for client 2\. Save this code as `FL_AML_Client2.py`.
    The client 2 code will be the same as client 1, but fraud transactions are not
    increased using the SMOTE method. For thoroughness, here is the complete code
    for the second client:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following output:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Encode the object types as labels using sci-kit learn’s `LabelEncoder`:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This results in the following output:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Open another terminal and run the client 2 code (`python3 FL_AML_Client2.py`):'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Running client 2 and the logs](img/B16573_06_15.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Running client 2 and the logs
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you run the client 2 code, pay close attention to the log statements on
    the server side. The server will initiate communication with both clients, enabling
    the exchange of the initial parameters and, subsequently, the updated weights
    for each round. Monitoring the server logs will provide insights into the progress
    of the FL process and the information shared between the clients and the server:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – The server-side logs](img/B16573_06_16.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – The server-side logs
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe the logs on both clients as well as the server side. These metrics
    provide an overview of the loss (indicating the model’s performance) and accuracy
    (representing the model’s correctness) for each client across multiple rounds
    of the FL process:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '| **Server** | **Client 1** | **Client 2** |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| INFO flwr 2023-03-14 17:46:49,202 &#124; app.py:139 &#124; Starting Flower
    server, config: ServerConfig(num_rounds=5, round_ timeout=None)INFO flwr 2023-03-14
    17:51:21,810 &#124; server.py:101 &#124; FL starting |  |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '|  | DEBUG flwr 2023-03-14 17:51:21,778 &#124; connection.py:38 &#124; ChannelConnectivity.READY
    |  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '|  |  | ChannelConnectivity. DEBUG flwr 2023-03-14 17:53:46,338 &#124; connection.py:38
    &#124; ChannelConnectivity.READY |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| DEBUG flwr 2023-03-14 17:53:46,338 &#124; server.py:215 &#124; fit_round
    1: strategy sampled 2 clients (out of 2)DEBUG flwr 2023-03-14 17:53:46,351 &#124;
    server.py:229 &#124; fit_round 1 received 2 results and 0 failuresWARNING flwr
    2023-03-14 17:53:46,354 &#124; fedavg.py:242 &#124; No fit_metrics_aggregation_fn
    providedINFO flwr 2023-03-14 17:53:46,362 &#124; server.py:116 &#124; fit progress:
    (1, 0.06756539217831908, {‘accuracy’: 0.9962666666666666}, 144.549737353)DEBUG
    flwr 2023-03-14 17:53:46,363 &#124; server.py:165 &#124; evaluate_round 1: strategy
    sampled 2 clients (out of 2)DEBUG flwr 2023-03-14 17:53:46,377 &#124; server.py:179
    &#124; evaluate_round 1 received 2 results and 0 failures |  |  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| INFO flwr 2023-03-14 17:53:46,400 &#124; server.py:116 &#124; fit progress:
    (2, 0.40485776608772656, {‘accuracy’: 0.9633333333333334}, 144.58791799899996)
    |  |  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| INFO flwr 2023-03-14 17:53:46,432 &#124; server.py:116 &#124; fit progress:
    (3, 0.11833075507570899, {‘accuracy’: 0.9962666666666666}, 144.61946266499996)
    |  |  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| INFO flwr 2023-03-14 17:53:46,465 &#124; server.py:116 &#124; fit progress:
    (4, 0.1145626928425223, {‘accuracy’: 0.9962666666666666}, 144.65267561899998)
    |  |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| INFO flwr 2023-03-14 17:53:46,497 &#124; server.py:116 &#124; fit progress:
    (5, 0.27867744042157033, {‘accuracy’: 0.9861333333333333}, 144.68508043599996)
    |  |  |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| INFO flwr 2023-03-14 17:53:46,511 &#124; app.py:202 &#124; app_fit: losses_distributed
    [(1, 0.4398987330496311), (2, 0.4606742262840271), (3, 0.5105149038136005), (4,
    0.5070083439350128), (5, 0.5951354652643204)] |  |  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '|  | Training finished for round 1:0.06756539217831908 0.9962666666666666Training
    finished for round 2:0.40485776608772656 0.9633333333333334Training finished for
    round 3:0.11833075507570899 0.9962666666666666Training finished for round 4:0.1145626928425223
    0.9962666666666666Training finished for round 5:0.27867744042157033 0.9861333333333333
    | Training finished for round 1:0.8122320748323023 0.9745333333333334Training
    finished for round 2:0.5164906830160562 0.9541333333333334Training finished for
    round 3:0.9026990471833415 0.9745333333333334Training finished for round 4:0.8994540131249842
    0.9745333333333334Training finished for round 5:0.9115935132282235 0.9736 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '|  | DEBUG flwr 2023-03-14 17:53:46,521 &#124; connection.py:109 &#124; gRPC
    channel closedINFO flwr 2023-03-14 17:53:46,522 &#124; app.py:153 &#124; Disconnect
    and shut down | DEBUG flwr 2023-03-14 17:53:46,521 &#124; connection.py:109 &#124;
    gRPC channel closedINFO flwr 2023-03-14 17:53:46,522 &#124; app.py:153 &#124;
    Disconnect and shut down |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: Table 6.8 – Log data at Server and Clients
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: As per the log, there are 5 rounds of communication between clients and the
    server, and in each round, accuracy results and loss change based on the weights.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '| **Client 1** | **Loss** | **Accuracy** |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| Round 1 | 0.06756539217831908 | 0.9962666666666666 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| Round 2 | 0.40485776608772656 | 0.9633333333333334 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| Round 3 | 0.11833075507570899 | 0.9962666666666666 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| Round 4 | 0.1145626928425223 | 0.9962666666666666 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| Round 5 | 0.27867744042157033 | 0.9861333333333333 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: Table 6.9 – Accuracy and Loss metrics at Client 1
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: As per the debug logs, loss and accuracy vary on client 1\. Let’s observe the
    loss and accuracy results on Client 2 as well.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '| **Client 2** | **Loss** | **Accuracy** |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| Round 1 | 0.8122320748323023 | 0.9745333333333334 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| Round 2 | 0.5164906830160562 | 0.9541333333333334 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| Round 3 | 0.9026990471833415 | 0.9745333333333334 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| Round 4 | 0.8994540131249842 | 0.9745333333333334 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| Round 5 | 0.9115935132282235 | 0.9736 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: Table 6.10 – Accuracy and Loss metrics at Client 2
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented a sample fraud detection application using Federated Learning
    and made use of open-source frameworks like Flower. In the next section, let’s
    try to learn and implement federated learning using differential privacy.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: FL with differential privacy
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Federated Learning with Differential Privacy** (**FL-DP**) is an approach
    that combines the principles of FL and **Differential Privacy** (**DP**) to ensure
    privacy and security in distributed ML systems. FL-DP aims to protect sensitive
    data while enabling collaborative model training across multiple devices or entities.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: The goal of FL-DP is to achieve accurate model training without compromising
    the privacy of individual data contributors. It addresses the challenge of preventing
    data leakage during the aggregation of model updates from different participants.
    By incorporating DP techniques, FL-DP provides strong privacy guarantees by adding
    noise or perturbation to the model updates or gradients before aggregating them.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: There are different approaches to implementing FL-DP. One common approach involves
    each client training a local ML model using their own data. The client applies
    techniques such as clipping and noise addition to the gradients or weights of
    the model. The client then sends the updated data to the server. On the server
    side, the updates are aggregated while preserving privacy using techniques such
    as secure aggregation or privacy-preserving FL algorithms. This ensures that individual
    client data remains private while enabling collaborative model training.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: FL-DP algorithms may vary depending on the specific differential privacy mechanisms
    used, such as Gaussian noise addition, subsampling, or advanced techniques such
    as **Private Aggregation of Teacher Ensembles** (**PATE**). The choice of techniques
    depends on the level of privacy required and the characteristics of the distributed
    dataset.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Implementing FL-DP requires careful consideration of privacy, accuracy, and
    computational overhead. It involves striking a balance between preserving privacy
    and maintaining model utility. Various frameworks and libraries, such as Flower
    and TensorFlow Privacy, provide tools and techniques to facilitate the implementation
    of FL-DP.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: FL-DP has the potential to unlock the benefits of collaborative ML in scenarios
    where data privacy and security are paramount. By preserving privacy, FL-DP enables
    organizations and individuals to collaborate on model training while safeguarding
    sensitive information.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: FL-DP provides a way to implement privacy-preserving techniques in the FL process,
    ensuring that client-side data remains protected.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore two general approaches to implementing FL-DP,
    although specific frameworks and implementations may have slight variations.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Approach one
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This approach shares similarities with **Differentially Private Federated Averaging**
    (**DP-FedAvg**), which was introduced by the Google research team. By following
    these approaches, FL-DP allows you to train ML models on client data while preserving
    privacy through techniques such as clipping and noise addition.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'Each client does the following:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Trains an ML/DL model using its local data.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computes gradients/weights using a standard SGD algorithm.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applies clipping to the weights to limit their sensitivity.
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adds noise to the weights to introduce randomness and privacy.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sends the modified weights to the server.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The server does the following:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Computes the average of the weights received from each client.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Broadcasts back the updated weights to the clients.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alternatively to step 1, applies clipping and adds noise to the final weights
    before broadcasting.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.17 – The DP-FedAvg model weights exchanged with the server](img/B16573_06_17.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – The DP-FedAvg model weights exchanged with the server
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: In this approach, each client trains its model on local data and work on the
    updated average weights sent by the server.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Approach two
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this approach, each client trains its model on local data and applies privacy-preserving
    techniques to compute the gradients/weights. The server then incorporates these
    noisy weights and performs aggregation using the FD-SGD algorithm, ensuring privacy
    is maintained throughout the FL process.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'Each client does the following:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Trains an ML model using its local data.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computes the gradients/weights using either noisy SGD or DP-SGD (DP stochastic
    gradient) algorithms, which incorporate noise during gradient computation to preserve
    privacy.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sends the weights to the server.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The server does the following:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Utilizes the noisy weights received from the clients.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Follows the **Federated Differential SGD** (**FD-SGD**) algorithm, which incorporates
    privacy-preserving techniques during the aggregation process on the server.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.18 – The DP-FedSGD model weights exchanged with the server](img/B16573_06_18.jpg)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – The DP-FedSGD model weights exchanged with the server
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: There are various variants of Differential Privacy Federated Learning (FL-DP)
    algorithms designed to address different scenarios, such as cross-device and cross-silo
    FL, with both homogeneous and heterogeneous data. In our implementation, we will
    apply FL-DP to the same example as before, ensuring privacy preservation throughout
    the FL process.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: A sample application using FL-DP
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing, the Flower framework (version 1.3) currently offers
    experimental support for FL-DP. It provides a strategy class (similar to FedAvg,
    FedYogi, and so on) specifically designed to support FL-DP. The class name designed
    to support this is `DPFedAvg`.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: The `DPFedAvg` class in the Flower framework is a component specifically designed
    to support FL-DP. It extends the functionality of the FedAvg algorithm by incorporating
    differential privacy techniques to protect the privacy of individual client data
    during model aggregation.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '`DPFedAvg` implements a privacy-preserving mechanism that ensures the privacy
    of client updates while enabling collaborative model training. It achieves this
    by adding noise or perturbation to the model updates or gradients received from
    each client, before aggregating them on the server side.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'The key features and functionalities of the `DPFedAvg` class include the following:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '**DP**: **DPFedAvg** integrates DP techniques into the FL process, ensuring
    that the privacy of individual client data is preserved during model training.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noise addition**: **DPFedAvg** applies noise to the gradients or model updates
    received from each client before aggregating them. The amount of noise added is
    determined based on privacy parameters and privacy budget allocation.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy budget management**: **DPFedAvg** incorporates mechanisms to manage
    and allocate the privacy budget effectively, ensuring that the desired privacy
    guarantees are maintained throughout the training process.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy parameters**: **DPFedAvg** allows users to customize the privacy
    parameters such as privacy budget, noise distribution, and sensitivity of the
    model updates. These parameters enable fine-grained control over the level of
    privacy and utility trade-off.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model aggregation**: **DPFedAvg** performs the aggregation of client updates
    using the DP averaging algorithm. This ensures that the privacy of individual
    updates is preserved while generating an updated global model.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compatibility with the Flower framework**: **DPFedAvg** is designed to seamlessly
    integrate with the Flower framework, allowing users to incorporate DP into their
    FL pipelines using the existing Flower infrastructure.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using the `DPFedAvg` class in the Flower framework, developers and ML engineers
    can implement FL-DP straightforwardly and efficiently. It provides a powerful
    tool to ensure privacy in distributed ML scenarios while maintaining the collaborative
    benefits of FL.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through the Flower-provided class in detail.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: The DPFedAvgFixed class
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This class is a wrapper class and adds clipping and Gaussian noise to the weights.
    The constructor of this class supports parameters to set server-side noise, a
    clip norm value, and the noise multiplier.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use this class on the server side. The server code is as follows:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The source code of the Jupyter notebooks for the server and clients is located
    in the [*Chapter* *6*](B16573_06_split_000.xhtml#_idTextAnchor120) folder:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 'Server code: **Fed-DP-AML-Server.ipynb**'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Client 1 code: **DP-FL-AML_Client1.ipynb**'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Client 2 code: **DP-FL-AML-Client2.ipynb**'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at the accuracy of the model for the clients and server:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '| **Client 1** | **Client 2** | **Server** |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
- en: '| 0.99626666666666660.71626666666666670.98760.93720.7714666666666666 | 0.97453333333333340.59786666666666670.96933333333333340.94480.7708
    | 0.99626666666666660.71626666666666670.98760.93720.7714666666666666 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
- en: Table 6.11 – Accracy results at Sever and Clients
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Applying DP to FL introduces some overhead in terms of computational cost, communication
    overhead, and potentially reduced model performance. In our example case, by the
    fourth round, the accuracy was 93%, but in the fifth round, the accuracy suddenly
    dropped. This tells us that we need to monitor the accuracy during training to
    help us decide on the number of rounds each client needs to participate in and
    stop further rounds when the accuracy drops.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explained why FL is needed and looked at its definition
    and characteristics in detail. We covered the steps involved in implementing FL
    and discussed IID and non-IID datasets and FL algorithms. We implemented a sample
    application using an open source FL framework. Finally, we converted the same
    application using DP.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about FL benchmarks and look at key start-ups
    that are working on or already have FL products.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
