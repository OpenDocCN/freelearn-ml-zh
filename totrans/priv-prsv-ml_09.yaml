- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Federated Learning and Implementing FL Using Open Source Frameworks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习与使用开源框架实现FL
- en: In this chapter, you will learn about **Federated Learning** (**FL**) and how
    to implement it using open source frameworks. We will cover why it is needed and
    how to preserve data privacy. We will also look at the definition of FL, as well
    as its characteristics and the steps involved in it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解**联邦学习**（**FL**）以及如何使用开源框架实现它。我们将涵盖为什么需要它以及如何保护数据隐私。我们还将探讨FL的定义、其特性和涉及的步骤。
- en: 'We will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主要主题：
- en: FL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL
- en: FL algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FL算法
- en: The steps involved in implementing FL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现FL涉及的步骤
- en: Open source frameworks for implementing FL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现FL的开源框架
- en: An end-to-end use case of implementing fraud detection using FL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用FL实现欺诈检测的端到端用例
- en: FL with differential privacy
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有差分隐私的FL
- en: By exploring these topics, you will gain a comprehensive understanding of the
    need for FL and the open source frameworks for implementing FL.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探讨这些主题，您将全面了解FL的需求以及实现FL的开源框架。
- en: Federated learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习
- en: FL has emerged as a solution to address the challenges of traditional centralized
    **Machine Learning** (**ML**) approaches in scenarios where data privacy and data
    locality are of paramount importance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: FL已成为解决在数据隐私和数据本地性至关重要的场景中传统集中式**机器学习**（**ML**）方法挑战的解决方案。
- en: 'The key reasons that we need FL are as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要FL的关键原因如下：
- en: '**Preserving data privacy**: In many situations, data is sensitive and cannot
    be shared due to legal, ethical, or privacy concerns. FL enables you to train
    models directly on distributed data sources without sharing the raw data, ensuring
    privacy protection. By keeping data local and performing model updates locally,
    FL minimizes the risk of exposing sensitive information.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保护数据隐私**：在许多情况下，数据是敏感的，由于法律、伦理或隐私问题无法共享。FL允许您在分布式数据源上直接训练模型，而不共享原始数据，确保隐私保护。通过保持数据本地化并在本地执行模型更新，FL最大限度地降低了暴露敏感信息的风险。'
- en: '**Data localization and regulatory compliance**: FL allows organizations to
    comply with data localization requirements and regulations. Instead of transferring
    data to a central server, data remains within the jurisdiction where it is generated
    or collected, addressing concerns related to cross-border data transfers.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据本地化和法规遵从性**：FL允许组织遵守数据本地化要求和法规。无需将数据传输到中央服务器，数据保持在生成或收集数据的管辖范围内，解决了与跨境数据传输相关的担忧。'
- en: '**Scalability and efficiency**: Centralized machine learning approaches often
    face challenges when dealing with large volumes of data, as aggregating and processing
    data from various sources can be time-consuming and resource-intensive. FL distributes
    the training process, allowing data to remain decentralized while benefiting from
    the collective intelligence of all participating devices or data sources. This
    decentralized approach improves scalability and computational efficiency.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和效率**：集中式机器学习方法在处理大量数据时往往面临挑战，因为从各种来源汇总和处理数据可能耗时且资源密集。FL将训练过程分散化，允许数据保持去中心化，同时从所有参与设备或数据源的集体智慧中受益。这种去中心化方法提高了可扩展性和计算效率。'
- en: '**Access to diverse data**: FL facilitates the pooling of data from multiple
    sources, enabling models to learn from diverse datasets without the need for direct
    data sharing. This is particularly beneficial in scenarios where data sources
    have distinct characteristics, such as different demographics, geographical regions,
    or user preferences. Access to a diverse range of data enhances the generalization
    and robustness of ML models.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问多样化的数据**：FL促进了来自多个来源的数据汇集，使模型能够在不直接共享数据的情况下从多样化的数据集中学习。这在数据来源具有不同特征的情况下特别有益，例如不同的人口统计、地理区域或用户偏好。访问多样化的数据范围增强了机器学习模型的泛化能力和鲁棒性。'
- en: '**Enhanced security and resilience**: With FL, the data remains distributed
    across devices or edge nodes, reducing the risk of a single point of failure or
    vulnerability. This distributed nature enhances the security and resilience of
    the overall system, making it less susceptible to attacks or breaches.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强的安全性和弹性**：使用FL，数据分布在设备或边缘节点上，降低了单点故障或漏洞的风险。这种分布式特性增强了整个系统的安全性和弹性，使其对攻击或入侵的抵抗力降低。'
- en: '**User empowerment and inclusion**: FL offers opportunities for user participation
    and control over their data. Instead of relinquishing data ownership and control
    to a centralized authority, users can actively contribute to the learning process
    while retaining control over their personal information. This empowers individuals
    and promotes a sense of inclusion and transparency.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户赋权和包容性**：联邦学习（FL）为用户参与和控制他们的数据提供了机会。用户不必将数据所有权和控制权交给中央权威机构，他们可以在保留对个人信息的控制的同时积极参与学习过程。这赋予了个人权力，并促进了包容性和透明度。'
- en: The need for FL arises from the critical requirements of preserving data privacy,
    complying with regulatory frameworks, achieving scalability and efficiency, accessing
    diverse data sources, ensuring security, and empowering users.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 需要联邦学习（FL）的原因在于保护数据隐私、遵守监管框架、实现可扩展性和效率、访问多样化的数据源、确保安全以及赋予用户权力等关键要求。
- en: By leveraging FL, organizations can overcome the limitations of centralized
    approaches and unlock the potential of distributed data for training robust and
    privacy-preserving ML models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用联邦学习（FL），组织可以克服集中式方法的局限性，并释放分布式数据用于训练强大且保护隐私的机器学习模型。
- en: Preserving privacy
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护隐私
- en: Let’s consider the case of ARDHA Bank (a fictional bank for illustration purposes
    only). ARDHA Bank is a financial institution that has been operating in the United
    States for several years, adhering to country-specific regulations. The bank offers
    a range of services to its customers, including fraud prevention, loyalty programs,
    and digital payments. Initially, ARDHA Bank employed static rule-based systems
    to detect and prevent fraudulent activities. However, recognizing the need for
    more advanced approaches, they transitioned to utilizing ML algorithms for enhanced
    fraud detection and prevention.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑 ARDHA 银行（仅用于说明目的的虚构银行）的案例。ARDHA 银行是一家在美国运营多年的金融机构，遵守特定国家的法规。该银行向客户提供一系列服务，包括欺诈预防、忠诚度计划和数字支付。最初，ARDHA
    银行采用基于静态规则的系统来检测和预防欺诈活动。然而，意识到需要更先进的方法，他们转向利用机器学习算法来增强欺诈检测和预防。
- en: With access to a comprehensive dataset comprising historical and current transaction
    data, ARDHA Bank developed ML and **Deep Learning** (**DL**) algorithms specifically
    tailored to their operations. These algorithms were trained on this extensive
    dataset, allowing the bank to effectively identify and prevent financial fraud
    with exceptional accuracy. By leveraging the power of ML and DL techniques, ARDHA
    Bank significantly improved its ability to detect and mitigate fraudulent digital
    transactions, thereby safeguarding its customers’ financial interests.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问包含历史和当前交易数据的综合数据集，ARDHA 银行开发了针对其运营的机器学习和**深度学习**（**DL**）算法。这些算法在这个广泛的数据集上进行了训练，使银行能够以极高的准确性有效地识别和预防金融欺诈。通过利用机器学习和深度学习技术的力量，ARDHA
    银行显著提高了其检测和减轻欺诈性数字交易的能力，从而保护了客户的财务利益。
- en: '![Figure 6.1 – A simple ML model in a financial bank](img/B16573_06_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 金融银行中的简单机器学习模型](img/B16573_06_01.jpg)'
- en: Figure 6.1 – A simple ML model in a financial bank
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 金融银行中的简单机器学习模型
- en: ARDHA Bank, having experienced success in the **United States** (**US**), made
    the strategic decision to expand its business and establish branches in two additional
    countries – France (for Europe) and India. With the expansion, ARDHA Bank aimed
    to offer the same suite of services to its customers in both regions. To provide
    digital payment services in France and India, one option considered by ARDHA Bank
    was to transmit periodic transaction data from both countries to their US servers.
    The US servers would then serve as the central location to run the ML models.
    After training the ML models on the combined data from all regions, the trained
    models would be deployed to the regional servers in France and India. By adopting
    this approach, ARDHA Bank sought to leverage the infrastructure of its US servers
    to process and analyze the transaction data efficiently. The centralized training
    of ML models allowed for a unified approach to fraud detection and prevention,
    ensuring consistency and accuracy across different regions. This strategy enabled
    ARDHA Bank to provide reliable and effective digital payment services in Europe
    and India while maintaining data security and privacy. By utilizing regional servers
    and deploying the trained ML models locally, the bank ensured swift and localized
    decision-making, catering to the specific needs and regulatory requirements of
    each region.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ARDHA银行在**美国**取得了成功之后，做出了战略决策，扩大其业务并在两个额外的国家设立分支机构——法国（针对欧洲）和印度。随着扩张，ARDHA银行旨在为其两个地区的客户提供相同的服务套件。为了在法国和印度提供数字支付服务，ARDHA银行考虑的一个选项是将两个国家的定期交易数据传输到其美国服务器。然后，美国服务器将作为运行机器学习模型的中央位置。在结合所有地区的综合数据上训练机器学习模型后，训练好的模型将被部署到法国和印度的区域服务器上。通过采用这种方法，ARDHA银行旨在利用其美国服务器的基础设施来高效地处理和分析交易数据。集中训练机器学习模型允许采取统一的方法进行欺诈检测和预防，确保在不同地区的一致性和准确性。这一策略使ARDHA银行能够在欧洲和印度提供可靠有效的数字支付服务，同时保持数据安全和隐私。通过利用区域服务器并在本地部署训练好的机器学习模型，银行确保了快速和本地化的决策制定，满足每个地区的特定需求和监管要求。
- en: '![Figure 6.2 – A simple ML model in a financial bank in three locations](img/B16573_06_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 在三个地点的金融银行中的简单机器学习模型](img/B16573_06_02.jpg)'
- en: Figure 6.2 – A simple ML model in a financial bank in three locations
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 在三个地点的金融银行中的简单机器学习模型
- en: The proposed solution, which involves transferring data to a central server
    and running ML models on that data, faces challenges due to privacy regulations
    and data localization laws in Europe and India. These regulations, such as the
    **General Data Protection Regulation** (**GDPR**) in Europe and India’s data localization
    requirements, stipulate that data generated within these countries must be stored
    within local data centers. Data must remain within the borders of the country
    where it was created.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的解决方案，涉及将数据传输到中央服务器并在该数据上运行机器学习模型，由于欧洲和印度的隐私法规和数据本地化法律而面临挑战。这些法规，如欧洲的**通用数据保护条例**（**GDPR**）和印度的数据本地化要求，规定在这些国家生成的数据必须存储在本地的数据中心。数据必须保持在创建它的国家境内。
- en: Given these privacy and localization constraints, an alternative approach is
    necessary. One possible alternative is to run ML models locally at each branch
    or location of the bank. This approach entails deploying client models that utilize
    the local data available at each location. The local models would process the
    data within the boundaries of the respective country, ensuring compliance with
    privacy regulations. To implement this alternative, only the model weights and
    parameters, not the transaction data used by customers, would be shared with a
    central server. The central server, hosted in any country, would be responsible
    for running a global model using the aggregated model weights and parameters from
    each location. The resulting global model could then be regularly distributed
    back to the local clients in each country. This approach enables the bank to leverage
    the benefits of ML models while adhering to privacy regulations and data localization
    laws. By conducting ML computations locally and sharing only model-related information,
    the bank ensures compliance, data security, and privacy. Additionally, this distributed
    approach allows for local adaptation and customization while still benefiting
    from the insights gained through the global model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些隐私和本地化限制，需要一种替代方法。一个可能的替代方案是在银行的每个分支或地点本地运行机器学习（ML）模型。这种方法包括部署利用每个地点可用的本地数据的客户端模型。本地模型将在各自国家的边界内处理数据，确保符合隐私法规。为了实施这种替代方案，只有模型权重和参数，而不是客户使用的交易数据，会与中央服务器共享。任何国家的中央服务器将负责运行一个全局模型，使用来自每个位置的聚合模型权重和参数。然后，生成的全局模型可以定期分发给每个国家的本地客户端。这种方法使银行能够利用机器学习模型的好处，同时遵守隐私法规和数据本地化法律。通过在本地进行机器学习计算并仅共享模型相关信息，银行确保了合规性、数据安全和隐私。此外，这种分布式方法允许进行本地适应和定制，同时仍然受益于全局模型获得的见解。
- en: '![Figure 6.3 – Local model interactions with the global model](img/B16573_06_03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 本地模型与全局模型的交互](img/B16573_06_03.jpg)'
- en: Figure 6.3 – Local model interactions with the global model
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 本地模型与全局模型的交互
- en: This approach is known as **Federated Machine Learning**, or FL. In FL, the
    traditional paradigm of moving data to a central location is reversed. Instead,
    the model and computation are brought to the data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**联邦机器学习**，或FL。在FL中，将数据移动到中央位置的传统范式被颠倒。相反，模型和计算被带到数据那里。
- en: In FL, the ML model is deployed and executed directly on the local data sources
    or devices where the data resides. This eliminates the need to transfer raw data
    to a central server, addressing privacy concerns and regulatory requirements.
    The model is trained locally using the data on each device, and only the model
    updates, such as gradients or weights, are securely transmitted to a central aggregator.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在联邦学习中，机器学习（ML）模型直接部署并在数据所在的本地区域或设备上执行。这消除了将原始数据传输到中央服务器的需求，解决了隐私问题和监管要求。模型使用每个设备上的数据进行本地训练，并且只有模型更新，如梯度或权重，才会安全地传输到中央聚合器。
- en: By keeping the data decentralized and performing computations locally, FL ensures
    data privacy and reduces the risks associated with data transfer. It allows organizations
    to leverage the collective knowledge and insights from distributed data sources
    without compromising individual data privacy. This approach is particularly beneficial
    in scenarios where data cannot be easily shared due to legal, regulatory, or privacy
    constraints.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保持数据去中心化和本地执行计算，联邦学习（FL）确保了数据隐私并减少了与数据传输相关的风险。它允许组织利用分布式数据源的知识和洞察力，同时不损害个人数据隐私。这种方法在数据因法律、监管或隐私限制而难以共享的场景中尤其有益。
- en: FL represents a paradigm shift in ML, enabling collaborative and privacy-preserving
    model training. It promotes a distributed approach where data remains under the
    control of the data owners while contributing to a shared model. This decentralized
    and privacy-conscious framework opens up possibilities to harness the power of
    large-scale data without sacrificing privacy and security.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习代表了机器学习（ML）的一个范式转变，它实现了协作和隐私保护模型训练。它促进了一种分布式方法，其中数据保持在数据所有者的控制之下，同时为共享模型做出贡献。这种去中心化和注重隐私的框架为利用大规模数据的力量打开了可能性，同时不牺牲隐私和安全。
- en: FL definition
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FL定义
- en: 'The following is the formal definition of FL proposed as per the *Advances
    and Open Problems in Federated Learning* paper published at arxiv/1912.04977:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是根据在 arxiv/1912.04977 发表的 *Federated Learning 的进展和开放问题* 论文提出的 FL 的正式定义：
- en: “Federated learning is a machine learning setting where multiple entities (clients)
    collaborate in solving a machine learning problem, under the coordination of a
    central server or service provider. Each client’s raw data is stored locally and
    not exchanged or transferred; instead, focused updates intended for immediate
    aggregation are used to achieve the learning objective”
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: “联邦学习是一种机器学习设置，其中多个实体（客户端）在中央服务器或服务提供商的协调下协作解决机器学习问题。每个客户端的原始数据都存储在本地，不进行交换或传输；相反，用于立即聚合的聚焦更新用于实现学习目标”
- en: 'As per this definition, these are the characteristics of FL:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个定义，以下是 FL 的特点：
- en: Multiple clients (entities) collaborate to solve an ML problem.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个客户端（实体）协作解决机器学习问题。
- en: A service provider or central server coordinates with these entities.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务提供商或中央服务器与这些实体协调。
- en: Raw data (data with samples) is stored locally at each client location and is
    not transferred to the servers.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始数据（带有样本的数据）存储在每个客户端位置，并且不会传输到服务器。
- en: The learning objective (or loss function) is defined. To minimize the loss (predictions
    versus actual), focused updates (weights and biases) are sent to the server from
    clients, the aggregation of weights (either average or dynamic aggregation) is
    done at the server, and these updates are sent back to clients.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义了学习目标（或损失函数）。为了最小化损失（预测与实际之间的差异），客户端将聚焦更新（权重和偏差）发送到服务器，在服务器上进行权重的聚合（平均或动态聚合），然后将这些更新发送回客户端。
- en: Let’s delve further into each one of these in detail to understand them better.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步详细探讨每一个，以更好地理解它们。
- en: Characteristics of FL
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FL 的特点
- en: The following subsections will cover the characteristics of FL in depth.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将深入探讨 FL 的特点。
- en: Multiple clients (entities) collaborate to solve an ML problem
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多个客户端（实体）协作解决机器学习问题
- en: In FL, the participation requirement typically involves a minimum of two clients,
    while the maximum number of clients can vary based on the specific use cases and
    client types.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FL 中，参与要求通常涉及至少两个客户端，而客户端的最大数量可以根据特定的用例和客户端类型而变化。
- en: 'Clients participating in FL can be broadly classified into two categories –
    cross-device and cross-silo:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 参与 FL 的客户端可以大致分为两类 – 跨设备和跨数据存储库：
- en: '**Cross-device** clients are individual devices, such as smartphones, laptops,
    or IoT devices, that contribute their local data for model training. These devices
    act as clients in the FL framework, allowing their data to be utilized while preserving
    privacy.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨设备**客户端是个人设备，如智能手机、笔记本电脑或物联网设备，它们为模型训练贡献本地数据。这些设备在 FL 框架中作为客户端，允许其数据被利用同时保护隐私。'
- en: '**Cross-silo** clients, on the other hand, represent data sources that are
    distributed across different organizational silos or entities. These silos can
    be different departments within an organization, separate institutions, or even
    distinct geographical regions. Each silo acts as a client, contributing its local
    data for collaborative model training.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨数据存储库**客户端，另一方面，代表分布在不同的组织数据存储库或实体中的数据源。这些数据存储库可以是组织内部的不同部门，独立的机构，甚至是不同的地理区域。每个数据存储库作为客户端，为其协作模型训练贡献本地数据。'
- en: '![Figure 6.4 – A classification of FL clients](img/B16573_06_04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – FL 客户端的分类](img/B16573_06_04.jpg)'
- en: Figure 6.4 – A classification of FL clients
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – FL 客户端的分类
- en: The maximum number of clients in an FL setup depends on the specific use cases
    and the scale of the distributed data sources. For instance, in scenarios where
    multiple organizations collaborate to build a global model while maintaining data
    privacy, the number of participating clients can be substantial. On the other
    hand, in more focused or localized use cases, the number of clients may be limited
    to a smaller group.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: FL 设置中客户端的最大数量取决于特定的用例和分布式数据源的规模。例如，在多个组织协作构建全球模型同时保持数据隐私的场景中，参与客户端的数量可能很大。另一方面，在更专注或本地化的用例中，客户端的数量可能限制在一个较小的群体。
- en: Cross-silo FL clients
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨数据存储库 FL 客户端
- en: 'Cross-silo clients are entities such as financial banks, institutions, hospitals,
    and pharmacy companies. These clients can be further categorized into two groups:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 跨部门客户端包括金融机构、机构、医院和制药公司等实体。这些客户端可以进一步分为两组：
- en: '**Different clients within the same institution**: This includes different
    branches of the same bank, different branches within a hospital network, and similar
    setups where multiple branches or divisions of a single institution participate
    in FL.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同一机构内的不同客户端**：这包括同一银行的分支机构、医院网络内的不同分支机构，以及类似设置，其中多个分支机构或一个机构的多个部门参与联邦学习。'
- en: '**Different clients across different institutions**: This involves different
    organizations, such as different banks or hospitals, collaborating and contributing
    their data to the FL process. These clients represent inter-institutional collaborations.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同机构间的不同客户端**：这涉及不同组织，如不同银行或医院，进行合作并向联邦学习过程贡献数据。这些客户端代表机构间的合作。'
- en: The maximum number of clients in the cross-silo category can vary based on the
    specific use case, but typically, it ranges from tens to hundreds. The number
    of participating clients is usually limited due to the nature of collaborations
    and the scale of the institutions involved.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 跨部门类别中的最大客户端数量可以根据具体用例而变化，但通常在数十到数百之间。由于合作性质和参与机构的规模，参与客户端的数量通常受到限制。
- en: Cross-device FL clients
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨设备联邦学习客户端
- en: Cross-device clients, on the other hand, encompass various devices that participate
    as clients or nodes in FL. These devices can be either homogenous or heterogenous,
    and examples include devices such as Apple iPhones, Google phones, and the Brave
    browser.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，跨设备客户端包括作为联邦学习中的客户端或节点的各种设备。这些设备可以是同质或异质的，例如苹果iPhone、谷歌手机和Brave浏览器等设备。
- en: In the case of cross-device FL clients, each device runs its own ML model based
    on the local data available on that specific device. Only the model weights and
    biases are transmitted to the server based on device conditions and other configuration
    settings.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨设备联邦学习客户端的情况下，每个设备都根据该特定设备上可用的本地数据运行自己的机器学习模型。仅根据设备条件和其他配置设置将模型权重和偏差传输到服务器。
- en: In this scenario, the maximum number of clients can reach thousands or even
    millions, as it encompasses a wide range of devices participating in FL across
    different locations and user bases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，客户端的最大数量可以达到数千甚至数百万，因为它涵盖了不同地点和用户基础中参与联邦学习的广泛设备。
- en: By accommodating both cross-silo and cross-device clients, FL enables collaboration
    and knowledge sharing while respecting data privacy and ensuring scalable participation
    across institutions and devices.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过同时容纳跨部门和跨设备客户端，联邦学习实现了协作和知识共享，同时尊重数据隐私并确保机构间和设备间的可扩展参与。
- en: A service provider or central server coordinates with these entities
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务提供商或中央服务器与这些实体进行协调
- en: The server in FL makes decisions based on the network topology of the participating
    clients and the total number of clients involved in the process. The server determines
    when to distribute the initial model or updated models to the clients, considering
    factors such as the network structure and the specific number of participating
    clients. It decides whether to send the model updates to all clients or only a
    subset of them, based on the requirements of the learning task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在联邦学习中，服务器根据参与客户端的网络拓扑和参与过程的总客户端数量做出决策。服务器确定何时向客户端分发初始模型或更新模型，考虑因素包括网络结构和具体参与客户端的数量。它根据学习任务的要求，决定是否向所有客户端或仅向其中一部分发送模型更新。
- en: After the clients receive the model, they compute and update the weights and
    biases based on their local data. The clients then send these updated weights
    and biases back to the server. The server aggregates the received data and performs
    computations using an objective function to minimize the loss or optimize the
    learning objective.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端接收到模型后，根据其本地数据计算并更新权重和偏差。然后，客户端将这些更新的权重和偏差发送回服务器。服务器汇总接收到的数据，并使用目标函数进行计算，以最小化损失或优化学习目标。
- en: Based on the aggregated information, the server generates an updated model.
    It decides which clients need to be updated with the new model and which clients
    can continue running the existing model without any changes. This decision is
    based on factors such as the learning progress, the need for updates, or the compatibility
    of clients with the updated model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 根据汇总信息，服务器生成一个更新的模型。它决定哪些客户端需要更新为新模型，哪些客户端可以继续运行现有模型而不做任何更改。这个决定基于学习进度、更新的需要或客户端与更新模型的兼容性等因素。
- en: By carefully orchestrating these steps, the server manages the distribution
    of models, collects client updates, aggregates data, and ultimately, sends back
    the updated model to the appropriate clients. This iterative process in FL ensures
    collaborative model improvement while accounting for the individual requirements
    and capabilities of the participating clients.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过精心安排这些步骤，服务器管理模型的分布，收集客户端更新，汇总数据，并最终将更新的模型发送回适当的客户端。在联邦学习（FL）中，这个迭代过程确保了协作模型改进，同时考虑到参与客户端的个别需求和能力。
- en: Raw data (data with samples) is stored locally at each client location and is
    not transferred to the servers
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原始数据（带有样本的数据）在每个客户端位置本地存储，并且不会传输到服务器。
- en: In FL, raw data is stored locally at each client location instead of being centralized
    in a single server. This decentralized approach ensures that the data remains
    under the control and ownership of the respective clients, preserving privacy
    and complying with data regulations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL中，原始数据在每个客户端位置本地存储，而不是集中存储在单个服务器上。这种去中心化的方法确保数据始终处于各自客户端的控制和所有权之下，保护隐私并符合数据法规。
- en: The data at each client location exhibits a specific distribution, which can
    vary across different clients. The distribution of the data refers to the statistical
    characteristics and patterns present within the dataset. The data samples within
    a client’s dataset can be independent of each other, meaning that they are unrelated
    or do not rely on each other for their values or properties. Alternatively, the
    data samples can be dependent, indicating that there is some form of correlation
    or relationship between them.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每个客户端位置的数据都表现出特定的分布，这种分布可能在不同客户端之间有所不同。数据的分布指的是数据集中存在的统计特性和模式。客户端数据集中的数据样本可能相互独立，这意味着它们之间没有关系或它们的价值或属性不依赖于彼此。或者，数据样本可能是相关的，表明它们之间存在某种形式的关联或关系。
- en: Furthermore, the data distribution can be either identical or non-identical
    among the clients. Identical data distribution implies that the statistical properties
    of the datasets are the same across different clients. On the other hand, non-identical
    data distribution suggests that the datasets exhibit variations in their statistical
    characteristics, such as mean, variance, or other relevant parameters.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据分布可以在客户端之间是相同的或不同的。相同的数据分布意味着不同客户端的数据集的统计特性是相同的。另一方面，不同的数据分布表明数据集在统计特性上存在差异，例如均值、方差或其他相关参数。
- en: The presence of diverse data distributions, whether independent or dependent,
    identical or non-identical, introduces challenges and complexities in FL. Nevertheless,
    FL methods are designed to handle these variations and enable collaborative model
    training across decentralized data sources, leveraging the collective knowledge
    while respecting data privacy and distribution characteristics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 独立或相关、相同或不同的数据分布的存在，在FL中引入了挑战和复杂性。尽管如此，FL方法被设计来处理这些变化，并使跨去中心化数据源进行协作模型训练成为可能，同时利用集体知识，尊重数据隐私和分布特性。
- en: Datasets with IID and non-IID data
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 具有独立同分布（IID）和非独立同分布（non-IID）数据的集合
- en: Independent and identically distributed (IID) data refers to a dataset in which
    the data samples are independent of each other, and the distribution of the data
    is identical across all samples. In this case, the outcomes of each data sample
    are not dependent on previous samples, and the statistical properties of the data
    remain consistent.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 独立同分布（IID）数据指的是数据样本之间相互独立，且所有样本的数据分布相同的数据集。在这种情况下，每个数据样本的结果不依赖于先前样本，数据的统计特性保持一致。
- en: For example, consider a dataset where a coin is tossed five times and the number
    of times it turns up heads is recorded. In this scenario, each coin toss is independent
    of the previous tosses, and the probability of getting heads is identical for
    each toss. This results in an IID dataset where the distribution of outcomes is
    the same for every coin toss.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个数据集，其中抛掷硬币五次并记录出现正面的次数。在这种情况下，每次抛掷硬币都是独立的，每次抛掷出现正面的概率是相同的。这导致了一个IID数据集，其中每次抛掷硬币的结果分布是相同的。
- en: In FL, the data across different clients may exhibit **non-IID** characteristics.
    This means that the data samples are not identically distributed, and they may
    also be dependent on each other. Various factors can contribute to non-IID data,
    such as variations in the amount of labeled data, differences in the features
    present in the samples, data drift, concept drift, or imbalanced data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL中，不同客户端之间的数据可能表现出**非-IID**特征。这意味着数据样本不是同质分布的，它们也可能相互依赖。各种因素可能导致非-IID数据，例如标签数据的数量变化、样本中存在的特征差异、数据漂移、概念漂移或不平衡数据。
- en: For example, in the case of cross-silo entities within a company, each client
    may have the same kind of features and labels for classification. However, the
    number of data samples at each location may vary, resulting in imbalanced data.
    Additionally, each location may not have data for all classes or may exhibit different
    distributions of examples.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在公司内部的跨部门实体中，每个客户端可能具有相同的特征和标签用于分类。然而，每个位置的数据样本数量可能不同，导致数据不平衡。此外，每个位置可能没有所有类别的数据，或者表现出不同的实例分布。
- en: Raw data in cross-silo entities in FL
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FL中跨部门实体的原始数据
- en: 'When dealing with cross-silo entities in FL, the raw data exhibits certain
    characteristics. Specifically, in the case of intra-company scenarios, the following
    can be observed:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理FL中的跨部门实体时，原始数据表现出某些特征。具体来说，在内部公司场景中，可以观察到以下情况：
- en: Each client within the cross-silo entities will possess the same kind of features.
    This means that the types of data attributes or variables available for analysis
    will be consistent across all clients.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨部门实体中的每个客户端都将拥有相同类型的特征。这意味着可用于分析的数据属性或变量的类型将在所有客户端中保持一致。
- en: The labels or classes used for classification tasks will also be the same among
    the clients. This ensures that the target categories or outcomes for classification
    are consistent throughout the participating entities.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类任务的标签或类别在客户端之间也将是相同的。这确保了参与实体中的分类目标类别或结果的一致性。
- en: The number of data samples at each client location may vary. This implies that
    the amount of available data may differ across different locations or branches
    within the same company. Some clients may have more extensive datasets, while
    others may have fewer samples.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个客户端位置的数据样本数量可能不同。这意味着可用的数据量可能在不同的位置或同一公司内的不同分支之间有所不同。一些客户端可能有更广泛的数据集，而其他客户端可能有较少的样本。
- en: Not all classes or categories may be represented in each client’s data. This
    results in imbalanced data, where certain classes may be overrepresented or underrepresented
    compared to others. Such imbalances can pose challenges for model training and
    evaluation.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有类别或类别可能在每个客户端的数据中都有表示。这导致数据不平衡，某些类别可能相对于其他类别被过度或不足表示。这种不平衡可能会对模型训练和评估造成挑战。
- en: The distribution of examples may not be the same across all clients. This means
    that the statistical characteristics, such as the mean, variance, or other properties,
    may vary between different client locations. Each client’s data may exhibit unique
    distributional patterns, which need to be accounted for during the FL process.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例的分布可能不会在所有客户端中相同。这意味着统计特征，如均值、方差或其他属性，可能在不同的客户端位置之间有所不同。每个客户端的数据可能表现出独特的分布模式，这些模式需要在FL过程中予以考虑。
- en: Considering these characteristics, FL techniques must address the variability
    in data samples, imbalanced class distributions, and divergent data distributions
    across the cross-silo entities.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些特征，FL技术必须解决数据样本的变异性、类别分布的不平衡以及跨部门实体间的数据分布差异。
- en: In the context of the banking example we discussed, since it is the same bank
    operating in different countries, the features (such as customer ID, amount, transaction
    date, source account, destination account, and address) and labels (*fraud* or
    *non-fraud*) will be the same. However, the distribution of data samples and labels
    may vary at each location, based on factors such as the number of customers and
    the types of transactions. This introduces non-IID characteristics to the data,
    requiring careful handling in FL approaches.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论的银行示例的背景下，由于它是同一银行在不同国家运营，因此特征（如客户ID、金额、交易日期、源账户、目标账户和地址）和标签（*欺诈*或*非欺诈*）将是相同的。然而，数据样本和标签的分布可能因每个位置的客户数量和交易类型等因素而异。这给数据引入了非独立同分布（non-IID）的特性，需要在联邦学习方法中谨慎处理。
- en: '![Figure 6.5 – The ML model in a financial bank in three locations](img/B16573_06_05.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 三地金融银行中的机器学习模型](img/B16573_06_05.jpg)'
- en: Figure 6.5 – The ML model in a financial bank in three locations
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 三地金融银行中的机器学习模型
- en: Data is distributed in the following way to each client. There is skewness in
    the label data but samples with all features exist in each location/client/entity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以以下方式分配给每个客户端。标签数据存在偏斜，但每个位置/客户端/实体中均存在具有所有特征的样本。
- en: '| Data at different clients | FeaturesX={ X1, X2, X3, …Xn} | Labely = { y1,
    y2…, ym} |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 不同客户端的数据 | 特征X={ X1, X2, X3, …Xn} | 标签y = { y1, y2…, ym} |'
- en: '| X1 | X2 | X3 | X4 | Fraud data label counts |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| X1 | X2 | X3 | X4 | 欺诈数据标签计数 |'
- en: '| Europe(Client 1) | yes | yes | yes | yes | Fraud count = N, Non-fraud = 0
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 欧洲（客户端1） | 是 | 是 | 是 | 是 | 欺诈计数 = N，非欺诈 = 0 |'
- en: '| US(Client 2 | yes | yes | yes | yes | Fraud count = 0, Non-fraud = N |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 美国（客户端2） | 是 | 是 | 是 | 是 | 欺诈计数 = 0，非欺诈 = N |'
- en: '| India(Client 3) | yes | yes | yes | yes | Fraud count = N/2, Non-fraud =
    N/4 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 印度（客户端3） | 是 | 是 | 是 | 是 | 欺诈计数 = N/2，非欺诈 = N/4 |'
- en: Table 6.1 – Label data skewness
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – 标签数据偏斜
- en: 'In the case of intra-institutions, where different institutions within the
    same industry participate in FL to offer similar ML services, the data may exhibit
    the following characteristics:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在机构内部的情况下，即同一行业内不同机构参与联邦学习（FL）以提供类似的机器学习（ML）服务时，数据可能表现出以下特征：
- en: Each client, representing a different institution, may or may not have the same
    kind of features. This means that the available data attributes or variables may
    differ between institutions, based on their specific contexts or data collection
    practices.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个客户端，代表不同的机构，可能具有或不具有相同类型的特征。这意味着根据它们的具体背景或数据收集实践，不同机构之间可用的数据属性或变量可能不同。
- en: The number of data samples at each client location may vary. This indicates
    that the amount of data available for analysis could differ between different
    institutions. Some institutions may have larger datasets, while others may have
    relatively smaller ones.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个客户端位置的数据样本数量可能不同。这表明不同机构可用于分析的数据量可能不同。一些机构可能拥有更大的数据集，而其他机构可能拥有相对较小的数据集。
- en: Not all classes or categories may be present in each client’s data. This can
    result in imbalanced data, where certain classes may be underrepresented or missing
    altogether in some institutions’ datasets. Handling imbalanced data is an important
    consideration in the FL process.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有类别或类别都可能在每个客户端的数据中存在。这可能导致数据不平衡，某些类别可能在某些机构的数据集中代表性不足或完全缺失。在联邦学习过程中处理不平衡数据是一个重要的考虑因素。
- en: The distribution of examples may also differ among the participating institutions.
    Each institution’s data may have its own unique distributional patterns, including
    variations in mean, variance, or other statistical properties. These differences
    need to be taken into account during the collaborative model training process.![Figure
    6.6 – FL client and server communication (send and receive) model parameters](img/B16573_06_06.jpg)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参与机构之间的示例分布也可能不同。每个机构的数据可能具有其独特的分布模式，包括均值、方差或其他统计特性的变化。这些差异需要在协作模型训练过程中予以考虑。![图6.6
    – FL客户端和服务器通信（发送和接收）模型参数](img/B16573_06_06.jpg)
- en: Figure 6.6 – FL client and server communication (send and receive) model parameters
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – FL客户端和服务器通信（发送和接收）模型参数
- en: Data is distributed in the following way to each client. In this scenario, there
    is skewness in features and label data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以以下方式分配给每个客户端。在此场景中，特征和标签数据存在偏斜。
- en: '| Data at different clients | FeaturesX={ X1, X2, X3, …Xn} | Labely = { y1,
    y2…, yn} |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 不同客户端的数据 | 特征X={ X1, X2, X3, …Xn} | 标签y = { y1, y2…, yn} |'
- en: '| --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| X1 | X2 | X3 | X4 | Fraud data label counts |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| X1 | X2 | X3 | X4 | 欺诈数据标签计数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Client 1 | yes |  | yes |  | No (Non-Fraud)= 70% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 客户端1 | 是 |  | 是 |  | 否（非欺诈）= 70% |'
- en: '| Client 2 |  | yes |  | yes | Yes (Fraud)= 100% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 客户端2 |  | 是 |  | 是 | 是（欺诈）= 100% |'
- en: '| Client 3 | yes | yes | yes | yes | Yes (Fraud) = 50%, No (Non-Fraud)=50%
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 客户端3 | 是 | 是 | 是 | 是 | 是（欺诈）= 50%，否（非欺诈）=50% |'
- en: Table 6.2 – Feature and label data skewness at different clients
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 – 不同客户端的特征和标签数据偏斜
- en: Learning objective
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习目标
- en: In FL, the central server takes on the responsibility of executing the learning
    objective and minimizing the loss function. It achieves this by leveraging the
    model weights (*Wt*) and biases received from the participating clients. The server
    determines the number of rounds of data it needs from the clients and the specific
    clients that need to participate.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL中，中央服务器承担执行学习目标和最小化损失函数的责任。它通过利用从参与客户端接收到的模型权重（*Wt*）和偏差来实现这一点。服务器确定需要从客户端获取的数据轮数以及需要参与的特定客户端。
- en: 'Let’s consider an example where there are three clients involved in the FL
    process. Each client sends its respective model weights and biases to the central
    server. The server then performs the following objective or learning function
    to minimize the loss:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，其中涉及三个参与FL过程的客户端。每个客户端将其各自的模型权重和偏差发送到中央服务器。然后，服务器执行以下目标或学习函数以最小化损失：
- en: '*minimize loss (**Wt, biases)*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*minimize loss (**Wt, biases)*'
- en: The objective of the server is to optimize the model parameters, represented
    by the weights (*Wt*) and biases, to minimize the loss function. By utilizing
    the received weights and biases from the participating clients, the server performs
    iterative updates to refine the model and improve its performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器的目标是优化由权重（*Wt*）和偏差表示的模型参数，以最小化损失函数。通过利用从参与客户端接收到的权重和偏差，服务器执行迭代更新以细化模型并提高其性能。
- en: The specific details of the learning objective and loss function depend on the
    specific ML algorithm and the task at hand. The central server orchestrates the
    aggregation of client updates, manages the training process, and sends back the
    updated model to the clients. This collaborative approach enables the clients
    to collectively contribute their local knowledge while benefiting from the improved
    global model provided by the server.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 学习目标和损失函数的具体细节取决于特定的ML算法和手头的任务。中央服务器协调客户端更新的聚合，管理训练过程，并将更新的模型发送回客户端。这种协作方法使客户端能够集体贡献其本地知识，同时从服务器提供的改进的全球模型中受益。
- en: 'This is the objective function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是目标函数：
- en: Min f(w) = ∑ i=1 n   £i * Fi(w)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Min f(w) = ∑ i=1 n   £i * Fi(w)
- en: Here, *w* is the model parameters (weights, and so on), *f(w)* is the objective
    function, and *n* is the number of clients participating in FL.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w* 是模型参数（权重等），*f(w)* 是目标函数，*n* 是参与FL的客户端数量。
- en: A few more mathematical terms will be used in the next section including the
    following.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，将使用一些数学术语，包括以下内容。
- en: '*Wt*: Model weights in the communication round *t* (client to server)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wt*: 在通信轮次 *t* 中的模型权重（客户端到服务器）'
- en: '*Wt k*: Model weights in the communication round on client *k*'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Wt k*: 在客户端 *k* 的通信轮次中的模型权重'
- en: '*C*: The number of clients participating in each round to update the model
    and compute the weights'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*C*: 每轮更新模型和计算权重的参与客户端数量'
- en: '*B*: The local clients’ batch size of the data samples'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*B*: 本地客户端数据样本的批量大小'
- en: '*Pk*: The set of data samples at client *k*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pk*: 客户端 *k* 的数据样本集'
- en: '*nk*: The number of data points at client *k*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*nk*: 客户端 *k* 的数据点数量'
- en: '*fi (w)*: *loss L ( xi, yi, w)* – the loss function'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*fi (w)*: *loss L ( xi, yi, w)* – 损失函数'
- en: On the server side, various objective functions can be implemented, depending
    on the specific requirements and goals of the FL process.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器端，可以根据FL过程的特定需求和目标实现各种目标函数。
- en: FL algorithms
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FL算法
- en: FL algorithms, such as FedSGD, FedAvg, and Adaptive Federated Optimization,
    play a crucial role in the distributed training of ML models while ensuring privacy
    and security. In this section, we will explore these algorithms and their key
    characteristics.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: FL算法，如FedSGD、FedAvg和自适应联邦优化，在确保隐私和安全的同时，在机器学习模型的分布式训练中发挥着关键作用。在本节中，我们将探讨这些算法及其关键特性。
- en: FedSGD
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FedSGD
- en: '**Federated stochastic gradient descent** (**FedSGD**) is a fundamental algorithm
    used in FL. It extends the traditional SGD optimization method to the federated
    setting. In FedSGD, each client (entity) computes the gradients on its local data
    and sends them to the central server. The server aggregates the gradients and
    updates the global model parameters accordingly. FedSGD is efficient for large-scale
    distributed training but may suffer from issues related to non-IID data and communication
    efficiency.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦随机梯度下降**（**FedSGD**）是FL中使用的根本算法。它将传统的SGD优化方法扩展到联邦设置。在FedSGD中，每个客户端（实体）在其本地数据上计算梯度，并将它们发送到中央服务器。服务器聚合梯度并相应地更新全局模型参数。FedSGD适用于大规模分布式训练，但可能受到与非-IID数据和通信效率相关的问题的影响。'
- en: '![Figure 6.7 – The FedSGD model weights exchange with the server](img/B16573_06_07.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – FedSGD模型权重与服务器交换](img/B16573_06_07.jpg)'
- en: Figure 6.7 – The FedSGD model weights exchange with the server
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – FedSGD模型权重与服务器交换
- en: 'Let’s look at the FedSGD algorithm:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看FedSGD算法：
- en: '| **Server-side algorithm** | **Client-side algorithm** |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| **服务器端算法** | **客户端算法** |'
- en: '| Initialize weights (w0)for each round t = 1,2, …m = max (C, K, 1)st = random
    set of m clientsfor client k in st,wt+1 = client-side function (k, wt)wt+1 = average
    of weights | Client-side function (k, w):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '| 初始化每个轮次t = 1,2, …m = max (C, K, 1)st = 随机选择的m个客户端for客户端k在st中，wt+1 = 客户端函数(k,
    wt)wt+1 = 权重的平均值 | 客户端函数(k, w):'
- en: Split the data in *k* batches, with each batch based on the batch size *B* (complete
    local dataset)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分成*k*个批次，每个批次基于批次大小*B*（完整本地数据集）
- en: 'For each batch:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个批次：
- en: fi (w) = loss L (xi, yi, w)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fi (w) = 损失 L (xi, yi, w)
- en: w = w – learning rate * loss
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: w = w – 学习率 * 损失
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 6.3 - FedSGD Algorithem
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 - FedSGD算法
- en: 'On the client side, each participating client performs the following steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端，每个参与客户端执行以下步骤：
- en: '**Data partitioning**: Clients have their own local datasets and partition
    them into smaller subsets to ensure privacy.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据分区**：客户端拥有自己的本地数据集，并将它们分成更小的子集以确保隐私。'
- en: '**Local model training**: Each client independently trains the shared model
    using its local data. This involves computing the gradients of the model parameters
    (weights and biases) on the local dataset using SGD or a variant.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**本地模型训练**：每个客户端独立使用其本地数据训练共享模型。这涉及到使用SGD或其变体在本地数据集上计算模型参数（权重和偏差）的梯度。'
- en: '**Model update**: After the local model training, the client sends the computed
    gradients to the server for aggregation.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型更新**：在本地模型训练后，客户端将计算的梯度发送到服务器进行聚合。'
- en: 'On the server side, the central server performs the following steps:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器端，中央服务器执行以下步骤：
- en: '**Aggregation**: The server receives the gradients from all participating clients
    and aggregates them using various aggregation techniques, such as averaging or
    weighted averaging.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聚合**：服务器从所有参与客户端接收梯度，并使用各种聚合技术（如平均或加权平均）进行聚合。'
- en: '**Model update**: The aggregated gradients are used to update the global model’s
    parameters. The server applies the received gradients to the global model, adjusting
    its weights and biases to reflect the collective knowledge from all clients.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型更新**：聚合的梯度用于更新全局模型参数。服务器将接收到的梯度应用于全局模型，调整其权重和偏差以反映所有客户端的集体知识。'
- en: '**Model distribution**: The updated global model is then sent back to the clients
    for the next round of training, ensuring that each client benefits from the collective
    knowledge while preserving data privacy.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型分发**：然后，更新后的全局模型被发送回客户端进行下一轮训练，确保每个客户端都能从集体知识中受益，同时保护数据隐私。'
- en: FedSGD aims to minimize the communication overhead between the clients and the
    server by exchanging only the model gradients rather than the raw data. This allows
    for distributed model training while maintaining data privacy and security. However,
    it is important to address challenges such as data heterogeneity and non-IID data
    distribution, which can impact the convergence and performance of the FL process.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: FedSGD旨在通过仅交换模型梯度而不是原始数据来最小化客户端和服务器之间的通信开销。这允许在保持数据隐私和安全的同时进行分布式模型训练。然而，解决诸如数据异质性和非-IID数据分布等挑战非常重要，这些挑战可能会影响FL过程的收敛性和性能。
- en: Overall, FedSGD enables collaborative model training in a decentralized manner,
    leveraging the computational resources of multiple clients while preserving data
    privacy. It serves as a foundational algorithm for FL and has paved the way for
    more advanced techniques to improve the efficiency and effectiveness of distributed
    ML.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，FedSGD以去中心化的方式实现了协作模型训练，利用了多个客户端的计算资源，同时保护了数据隐私。它是联邦学习的基础算法，为更高级的技术的开发铺平了道路，以改进分布式机器学习的效率和效果。
- en: FedAvg
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FedAvg
- en: '**Federated averaging** (**FedAvg**) is a widely adopted FL algorithm designed
    to address the challenges of non-IID data and communication efficiency. In FedAvg,
    similar to FedSGD, each client computes the gradients on its local data. However,
    instead of directly updating the global model with the individual gradients, FedAvg
    employs weighted averaging to combine the client models’ parameters. This approach
    allows for better handling of data heterogeneity and reduces the communication
    overhead between the clients and the server.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦平均**（**FedAvg**）是一种广泛采用的联邦学习算法，旨在解决非独立同分布数据和非高效通信的挑战。在FedAvg中，类似于FedSGD，每个客户端在其本地数据上计算梯度。然而，FedAvg不是直接使用单个梯度更新全局模型，而是采用加权平均来组合客户端模型的参数。这种方法可以更好地处理数据异构性，并减少客户端与服务器之间的通信开销。'
- en: '![Figure 6.8 – The FedAvg model weights exchange with the server](img/B16573_06_08.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – FedAvg模型与服务器之间的权重交换](img/B16573_06_08.jpg)'
- en: Figure 6.8 – The FedAvg model weights exchange with the server
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – FedAvg模型与服务器之间的权重交换
- en: 'Let’s look at the FedAvg algorithm:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看FedAvg算法：
- en: '| **Server-side algorithm** | **Client-side algorithm** |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| **服务器端算法** | **客户端算法** |'
- en: '| Initialize weights (w0)for each round t = 1,2, …m = max (C, K, 1)st = random
    set of m clientsfor client k in st,wt+1 = client-side function (k, wt)wt+1 = average
    of gradients | Client-side function (k, w):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '| 初始化每轮t = 1,2, …m = max (C, K, 1)的权重(w0)st = 随机选择m个客户端for客户端k在st中，wt+1 = 客户端函数(k,
    wt)wt+1 = 梯度的平均值 | 客户端函数(k, w):'
- en: Split the data into *k* batches, with each batch based on the batch size *B*
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分成*k*个批次，每个批次基于批次大小*B*
- en: For each epoch in training E
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于训练中的每个epoch E
- en: 'For each batch:'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个批次：
- en: fi (w) = loss L (xi, yi, w)
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: fi (w) = 损失 L (xi, yi, w)
- en: w = w – learning rate * loss
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: w = w – 学习率 * 损失
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 6.4 – FedAVG Algorithm
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4 – FedAVG算法
- en: FedAvg leverages the concept of averaging to combine the locally trained models
    of different clients, which helps mitigate the impact of data heterogeneity and
    non-IID data distribution. By averaging the model parameters, FedAvg effectively
    creates a global model that captures insights from all participating clients while
    preserving the privacy of individual data. The iterative nature of FedAvg allows
    the shared model to progressively improve with each round of training. As the
    process continues, the global model becomes more refined and represents the collective
    knowledge of all clients.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: FedAvg利用平均的概念来组合不同客户端的本地训练模型，这有助于减轻数据异构性和非独立同分布数据分布的影响。通过平均模型参数，FedAvg有效地创建了一个全局模型，该模型捕捉了所有参与客户端的见解，同时保护了个人数据的隐私。FedAvg的迭代性质允许共享模型在每一轮训练中逐步改进。随着过程的继续，全局模型变得更加精细，代表了所有客户端的集体知识。
- en: Overall, FedAvg enables collaborative training of a shared model in a privacy-preserving
    manner. It addresses challenges associated with data privacy and distribution,
    allowing multiple clients to contribute to the model’s improvement without sharing
    their raw data. FedAvg has been instrumental in advancing the field of FL, enabling
    applications in various domains while maintaining data privacy and security.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，FedAvg以保护隐私的方式实现了共享模型的协作训练。它解决了与数据隐私和分布相关的挑战，允许多个客户端在不共享原始数据的情况下为模型改进做出贡献。FedAvg在联邦学习的领域中发挥了重要作用，使得在各个领域中的应用成为可能，同时保持了数据隐私和安全。
- en: Fed Adaptative Optimization
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fed Adaptative Optimization
- en: In cross-device FL, a multitude of clients communicate with a central server,
    and each client possesses a unique set of data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨设备联邦学习中，众多客户端与一个中央服务器通信，每个客户端拥有独特的数据集。
- en: For instance, in the context of next-word prediction on phones, different users’
    phones contain distinct word sets based on factors such as country, region, and
    language. However, traditional FL algorithms such as FedSGD and FedAvg may not
    perform optimally when confronted with heterogeneous data from diverse clients.
    The challenge arises from the inherent differences in data distribution and characteristics
    among the clients. Heterogeneous data introduces complexities that can impact
    the convergence and performance of FL algorithms. As a result, handling heterogeneous
    data poses a considerable obstacle compared to scenarios where the clients have
    homogeneous data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在手机上的下一个单词预测的背景下，不同用户的手机根据国家、地区和语言等因素包含不同的单词集。然而，当面对来自不同客户端的异构数据时，传统的 FL
    算法如 FedSGD 和 FedAvg 可能无法表现最佳。挑战源于客户端之间数据分布和特性的固有差异。异构数据引入的复杂性可能会影响 FL 算法的收敛性和性能。因此，与客户端具有同质数据的情况相比，处理异构数据构成了相当大的障碍。
- en: Efforts are being made to address the challenges associated with heterogeneous
    data in cross-device FL.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正在努力解决跨设备联邦学习（FL）中与异构数据相关联的挑战。
- en: In order to overcome this, researchers at Google (*Sashank J. Reddi et al.,
    2021*) proposed new adaptative optimizations in the research paper published at
    arxiv.org/abs/2003.00295.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一点，谷歌的研究人员（Sashank J. Reddi 等人，2021年）在 arxiv.org/abs/2003.00295 发表的论文中提出了新的自适应优化方法。
- en: 'Here is the detailed algorithm (the image is sourced from the preceding URL):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是详细的算法（图片来源于前面的 URL）：
- en: '![Figure 6.9 – The Fed Adaptive Optimization algorithm proposed by Google researchers](img/B16573_06_09.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 谷歌研究人员提出的 Fed Adaptive Optimization 算法](img/B16573_06_09.jpg)'
- en: Figure 6.9 – The Fed Adaptive Optimization algorithm proposed by Google researchers
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 谷歌研究人员提出的 Fed Adaptive Optimization 算法
- en: Please refer to the article for a detailed explanation of the Adaptive Optimization
    algorithm. In a nutshell, the idea is to optimize the communication cost like
    FEDAVG and work in cross-device settings.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅文章以获取自适应优化算法的详细解释。简而言之，这个想法是优化通信成本，类似于 FEDAVG，并在跨设备环境中工作。
- en: The steps involved in implementing FL
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施联邦学习所涉及的步骤
- en: 'The following are the five steps that are typically followed to implement FL.
    There can be alternatives/changes to these steps, but initially, these are the
    steps that need to be followed:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下通常是实现联邦学习（FL）所遵循的五个步骤。这些步骤可能有替代方案/变化，但最初，这些是需要遵循的步骤：
- en: '**The server side – the initialization of the global model**: In this step,
    the server starts and accepts the client requests. Before actually starting the
    server, the model on the server side will be initiated with model parameters.
    Typically, model parameters will be initiated with zeros or from the previous
    checkpoint model.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务器端 – 全局模型的初始化**：在这个步骤中，服务器启动并接受客户端请求。在实际启动服务器之前，服务器端的模型将使用模型参数进行初始化。通常，模型参数将初始化为零或从之前的检查点模型中获取。'
- en: '**The server sends model parameters to all or a subset of clients**: In this
    step, the server sends the initial model parameters to all clients (for cross-silo
    FL clients, they will be within the same institutions and may only be numbered
    in the tens) or a subset of clients (in the case of cross-device FL where devices
    are in the millions, the server decides to select only a subset from the total
    devices). Each client will make use of these initial model parameters for the
    local training of the model.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务器将模型参数发送到所有或部分客户端**：在这个步骤中，服务器将初始模型参数发送到所有客户端（对于跨沙盒 FL 客户端，它们将位于同一机构内，可能只有几十个）或部分客户端（在跨设备
    FL 的情况下，设备数以百万计，服务器决定只从总数中选择一部分）。每个客户端将使用这些初始模型参数进行模型的本地训练。'
- en: '**The clients train the model and send the model weights/parameters back to
    the server**: In this step, each client will train the model with their local
    data, making use of the entire local data in one shot, dividing the data into
    several batches, or splitting the data randomly and making use of the different
    splits for different rounds (a multiple rounds of exchanges of model parameters
    between the client and server). The clients will send the model parameters or
    weights only to the server.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户端训练模型并将模型权重/参数发送回服务器**：在这个步骤中，每个客户端将使用其本地数据训练模型，一次性使用全部本地数据，将数据分成几个批次，或者随机分割数据并利用不同轮次（客户端和服务器之间多次交换模型参数）的不同分割。客户端只会将模型参数或权重发送到服务器。'
- en: '**The server executes one of the FL algorithms, updates the global model, and
    sends the updated weights to the client for the next round**: In this step, the
    server will run one of the FL algorithms and make use of the weights received
    by the clients to update the global model. In the case of FedAvg, it will calculate
    the weighted average of the weights received from clients and send the updated
    weights back to the client for the next round.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**服务器执行一个FL算法，更新全局模型，并将更新的权重发送给客户端进行下一轮**：在这一步中，服务器将运行一个FL算法，并利用从客户端接收到的权重来更新全局模型。在FedAvg的情况下，它将计算从客户端接收到的权重的加权平均值，并将更新的权重发送回客户端进行下一轮。'
- en: '**Repeat steps 2 to 4 based on the number of rounds configured**: Repeat *steps
    2* to *4* for each round. If five rounds are configured, then repeat *steps 2*
    to *4* five times, and after the last round, clients will make use of the weights
    received by the server for the final ML model. Clients can make use of these model
    weights either at the end of the last round or in each round and evaluate the
    model’s accuracy with the test data.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**根据配置的轮数重复步骤2到4**：对于每一轮重复步骤2到4。如果配置了五轮，那么重复步骤2到4五次，在最后一轮之后，客户端将利用从服务器接收到的权重来使用最终的机器学习模型。客户端可以在最后一轮结束时或每一轮中使用这些模型权重，并使用测试数据评估模型的准确性。'
- en: 'The following sequence diagram shows these steps in detail:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下序列图详细展示了这些步骤：
- en: '![Figure 6.10 – The steps in the FL sequence diagram](img/B16573_06_10.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – FL序列图中的步骤](img/B16573_06_10.jpg)'
- en: Figure 6.10 – The steps in the FL sequence diagram
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – FL序列图中的步骤
- en: The sequence diagram shows the detailed interactions between the server and
    the clients participating in the FL, performing four high-level steps as explained
    in this section.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 序列图显示了参与联邦学习（FL）的服务器和客户端之间的详细交互，按照本节所述执行四个高级步骤。
- en: Open source frameworks to implement FL
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现FL的开源框架
- en: There are a few open source frameworks to implement FL at scale. The following
    are some of the most popular.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个开源框架可以用于大规模实现FL。以下是一些最受欢迎的。
- en: '**PySyft** ([https://github.com/OpenMined/PySyft](https://github.com/OpenMined/PySyft)),
    developed by OpenMined, is an open source stack that offers secure and private
    data science capabilities in Python. It introduces a separation between private
    data and model training, enabling functionalities such as FL, differential privacy,
    and encrypted computation. Initially, PySyft utilized the Opacus framework to
    support differential privacy, as discussed in the Differential privacy chapter.
    However, the latest version of PySyft incorporates its own differential privacy
    component to provide enhanced functionality and efficiency in preserving privacy
    while performing data analysis tasks.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**PySyft**（[https://github.com/OpenMined/PySyft](https://github.com/OpenMined/PySyft)），由OpenMined开发，是一个开源栈，它提供了在Python中安全且私有的数据科学能力。它引入了私有数据和模型训练之间的分离，使得FL、差分隐私和加密计算等功能成为可能。最初，PySyft利用Opacus框架来支持差分隐私，如差分隐私章节所述。然而，PySyft的最新版本集成了自己的差分隐私组件，以提供在执行数据分析任务时保护隐私的同时增强功能和效率。'
- en: TensorFlow Federated
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Federated
- en: '**TensorFlow Federated** (**TFF**) is a library developed by Google that facilitates
    the training of shared ML models across multiple clients using their local data
    ([https://www.tensorflow.org/federated](https://www.tensorflow.org/federated)).
    TFF consists of two layers – the Federated Core API and the Federated Learning
    API.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow Federated**（**TFF**）是由谷歌开发的一个库，它通过使用客户端的本地数据来简化跨多个客户端训练共享机器学习模型的过程（[https://www.tensorflow.org/federated](https://www.tensorflow.org/federated)）。TFF由两层组成——联邦核心API和联邦学习API。'
- en: The Federated Core API offers low-level interfaces for tasks such as data serialization,
    distribution communication between the server and clients, and implementation
    of FL algorithms. It provides the foundational components necessary to build FL
    systems.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦核心API提供了低级接口，用于数据序列化、服务器和客户端之间的分布式通信以及FL算法的实现。它提供了构建FL系统所需的基础组件。
- en: Conversely, the Federated Learning API provides a higher-level interface that
    allows users to easily construct FL models or wrap existing models as FL models.
    It offers a set of APIs for training and evaluating models using federated computations
    and datasets. This higher-level interface abstracts away some of the complexities
    involved in building and training FL models, making it more accessible and convenient
    for developers.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，联邦学习API提供了一个高级接口，使用户能够轻松构建联邦学习模型或将现有模型包装为联邦学习模型。它提供了一套API，用于使用联邦计算和数据集进行模型训练和评估。这个高级接口抽象了一些构建和训练联邦学习模型所涉及到的复杂性，使得它对开发者来说更加易于访问和方便。
- en: By providing these two layers, TFF empowers researchers and developers to leverage
    the power of FL in their projects. It simplifies the process of building and training
    models on decentralized data while ensuring privacy and data security.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供这两层，TFF使研究人员和开发者能够在其项目中利用联邦学习的力量。它简化了在去中心化数据上构建和训练模型的过程，同时确保隐私和数据安全。
- en: Flower
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flower
- en: '**Flower** ([https://flower.dev/](https://flower.dev/)) is an open source framework
    that aims to provide a user-friendly experience. It supports ML and DL models
    developed using various frameworks, such as scikit-learn, TensorFlow, PyTorch,
    PyTorch Lightning, MXNet, and JAX. Flower makes it easy to convert these models
    into FL models.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**Flower** ([https://flower.dev/](https://flower.dev/)) 是一个开源框架，旨在提供用户友好的体验。它支持使用各种框架开发的机器学习和深度学习模型，例如scikit-learn、TensorFlow、PyTorch、PyTorch
    Lightning、MXNet和JAX。Flower使得将这些模型转换为联邦学习模型变得容易。'
- en: One of the key features of Flower is its communication implementation, which
    is built on top of bidirectional gRPC streams. This enables an efficient and seamless
    exchange of multiple messages between clients and the server without the need
    to establish a new connection for each message request.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Flower的一个关键特性是其通信实现，它建立在双向gRPC流之上。这允许客户端和服务器之间高效且无缝地交换多个消息，而无需为每个消息请求建立新的连接。
- en: Flower offers a range of strategies and implements several FL algorithms on
    the server side. These algorithms include FedAvg, FedSGD, Fault Tolerance FedAvg,
    FedProxy, and FedOptim (which consists of FedAdagrad, FedYogi, and FedAdam). These
    algorithms provide different approaches to model aggregation and training in FL
    scenarios.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Flower提供了一系列策略，并在服务器端实现了几个联邦学习算法。这些算法包括FedAvg、FedSGD、容错FedAvg、FedProxy和FedOptim（包括FedAdagrad、FedYogi和FedAdam）。这些算法在联邦学习场景中提供了不同的模型聚合和训练方法。
- en: '![Figure 6.11 – The Flower framework architecture diagram (simplified)](img/B16573_06_11.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – Flower框架架构图（简化）](img/B16573_06_11.jpg)'
- en: Figure 6.11 – The Flower framework architecture diagram (simplified)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – Flower框架架构图（简化）
- en: To validate its performance, Flower has been extensively benchmarked. The framework
    has demonstrated the ability to scale up to 15 million clients using only two
    GPU servers. These experiments were compared with FedScale, another FL engine
    and benchmark suite, to evaluate Flower’s performance and efficiency in large-scale
    FL settings.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证其性能，Flower已经进行了广泛的基准测试。该框架已经证明了仅使用两个GPU服务器就能扩展到1500万客户端的能力。这些实验与另一个联邦学习引擎和基准测试套件FedScale进行了比较，以评估Flower在大型联邦学习环境中的性能和效率。
- en: An end-to-end use case of implementing fraud detection using FL
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用联邦学习实现欺诈检测的端到端用例
- en: Fraud detection is a critical task for many industries, including finance, e-commerce,
    and healthcare. Traditional fraud detection methods often rely on centralized
    data collection, where sensitive customer information is gathered and analyzed
    in a single location. However, this approach raises concerns about data privacy
    and security, as well as compliance with regulations such as the GDPR.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测对许多行业，包括金融、电子商务和医疗保健，都是一个关键任务。传统的欺诈检测方法通常依赖于集中式数据收集，其中敏感的客户信息在单一地点收集和分析。然而，这种方法引发了关于数据隐私和安全以及遵守GDPR等法规的担忧。
- en: FL offers a promising solution to address these challenges. By leveraging the
    power of distributed computing and collaborative learning, FL enables fraud detection
    models to be trained directly on the devices or local servers of individual institutions,
    without the need for data sharing. This decentralized approach ensures that sensitive
    customer data remains private and secure, as it never leaves the local environment.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: FL为解决这些挑战提供了一个有希望的解决方案。通过利用分布式计算和协作学习的力量，FL使得欺诈检测模型可以直接在单个机构的设备或本地服务器上训练，无需共享数据。这种去中心化的方法确保敏感客户数据保持私密和安全，因为它从未离开本地环境。
- en: Implementing fraud detection using FL involves several key steps. Firstly, a
    consortium of institutions or organizations, such as banks or e-commerce platforms,
    need to establish an FL framework that enables them to collaborate on model training
    while preserving data privacy. This may involve the adoption of FL libraries or
    platforms such as TFF or Flower.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用联邦学习（FL）实现欺诈检测涉及几个关键步骤。首先，由银行或电子商务平台等机构或组织组成的联盟需要建立一个FL框架，使他们能够在保护数据隐私的同时协作进行模型训练。这可能涉及采用FL库或平台，如TFF或Flower。
- en: Next, the participating institutions define a common fraud detection objective
    and develop a shared model architecture. Each institution then trains its local
    model using its own private data, which may include transaction records, user
    behavior patterns, and other relevant features. The models are trained locally,
    ensuring that sensitive data remains under the control of the respective institutions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，参与机构定义一个共同的欺诈检测目标并开发共享的模型架构。然后，每个机构使用自己的私有数据训练其本地模型，这些数据可能包括交易记录、用户行为模式和其他相关特征。模型在本地进行训练，确保敏感数据始终处于相应机构的控制之下。
- en: To facilitate collaborative learning, the institutions periodically share model
    updates with a central server. These updates, which typically include model weights
    and parameters, are aggregated using federated averaging or other aggregation
    techniques to create a global model that captures insights from all participants,
    while preserving the privacy of individual data.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进协作学习，机构定期将模型更新与中央服务器共享。这些更新通常包括模型权重和参数，通过联邦平均或其他聚合技术进行汇总，以创建一个全局模型，该模型能够捕捉到所有参与者的洞察，同时保护个人数据的隐私。
- en: The central server, which oversees the aggregation process, ensures that the
    global model is refined based on the collective knowledge of the participating
    institutions. This process allows the model to learn from a diverse range of fraud
    patterns and adapt to evolving fraudulent activities while maintaining data privacy
    and compliance with regulations.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 负责监督聚合过程的中央服务器确保全局模型基于参与机构的集体知识进行优化。这个过程允许模型从各种欺诈模式中学习，并适应不断发展的欺诈活动，同时保持数据隐私并符合法规。
- en: Implementing fraud detection using FL offers several advantages. It allows institutions
    to leverage a larger and more diverse dataset, leading to improved fraud detection
    accuracy. It also reduces the risks associated with data breaches or unauthorized
    access, since sensitive data remains under the control of the respective institutions.
    Additionally, FL enables real-time updates and faster model deployment, allowing
    institutions to respond quickly to emerging fraud patterns.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FL实现欺诈检测提供了几个优势。它允许机构利用更大、更多样化的数据集，从而提高欺诈检测的准确性。它还降低了数据泄露或未经授权访问的风险，因为敏感数据始终处于相应机构的控制之下。此外，FL可以实现实时更新和更快的模型部署，使机构能够迅速应对新出现的欺诈模式。
- en: Implementing fraud detection using FL offers a privacy-preserving and collaborative
    approach to combat fraud in various industries. By combining the power of distributed
    computing and shared learning, organizations can enhance fraud detection capabilities
    while safeguarding sensitive customer data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FL实现欺诈检测为各种行业中的欺诈提供了隐私保护和协作的方法。通过结合分布式计算和共享学习的力量，组织可以增强欺诈检测能力，同时保护敏感客户数据。
- en: Let’s implement this use case using the Flower framework and the open source
    dataset.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Flower框架和开源数据集来实现这个用例。
- en: Developing an FL model for fraud detection using the Flower framework
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Flower框架开发用于欺诈检测的FL模型
- en: In this example, we will leverage the Flower framework to develop an FL model
    for fraud detection. The implementation will involve both server-side and client-side
    components. To illustrate the process, we will set up one server and two clients.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将利用Flower框架开发用于欺诈检测的FL模型。实现将涉及服务器端和客户端组件。为了说明这个过程，我们将设置一个服务器和两个客户端。
- en: The communication between the server and clients will occur over several rounds,
    with the exchange of weights and parameters. The exact number of rounds may vary
    depending on the specific scenario, but typically, the communication continues
    until the weights converge or a predetermined convergence criterion is met.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器和客户端之间的通信将在多个轮次中进行，涉及权重和参数的交换。具体的轮次数量可能因具体场景而异，但通常，通信会持续到权重收敛或达到预定的收敛标准。
- en: On the server side, we will implement the FedAvg algorithm to aggregate the
    weights received from the clients. FedAvg is a widely used algorithm in FL that
    combines the knowledge from multiple clients to create a global model.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器端，我们将实现FedAvg算法来聚合从客户端接收到的权重。FedAvg是FL中广泛使用的算法，它结合了多个客户端的知识来创建一个全局模型。
- en: For the fraud detection task, we will develop an actual linear regression model
    using the scikit-learn library. This model will be trained using the data available
    at each client, which consists of transaction records and relevant features. The
    goal is to classify whether a transaction is fraudulent or not.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于欺诈检测任务，我们将使用scikit-learn库开发一个实际的线性回归模型。此模型将使用每个客户端可用的数据进行训练，这些数据包括交易记录和相关特征。目标是分类交易是否为欺诈。
- en: The client-side implementation will involve training the local linear regression
    models using the respective client’s data. The clients will then communicate with
    the server, exchanging their model weights and parameters over the predefined
    rounds. This collaborative learning process allows the clients to contribute their
    local insights to the global model while preserving the privacy of their data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端实现将涉及使用各自客户端的数据训练本地线性回归模型。然后，客户端将与服务器通信，在预定义的轮次中交换他们的模型权重和参数。这种协作学习过程允许客户端将其本地见解贡献给全局模型，同时保护其数据的隐私。
- en: The server will receive the model updates from the clients and perform the aggregation
    step using the FedAvg algorithm. This aggregation process ensures that the global
    model incorporates the knowledge learned from all the participating clients, resulting
    in an enhanced fraud detection capability.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器将从客户端接收模型更新，并使用FedAvg算法执行聚合步骤。此聚合过程确保全局模型结合了所有参与客户端学习到的知识，从而增强了欺诈检测能力。
- en: Throughout the implementation, the Flower framework provides the necessary infrastructure
    for the communication between the server and clients. It abstracts the underlying
    complexities of distributed computing and handles the synchronization of model
    updates. By developing an FL model for fraud detection, we can leverage the distributed
    knowledge and data from multiple clients to improve the accuracy of fraud classification.
    The federated approach also addresses privacy concerns by keeping the sensitive
    transaction data local to each client.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个实现过程中，Flower框架为服务器和客户端之间的通信提供了必要的基础设施。它抽象了分布式计算的底层复杂性，并处理模型更新的同步。通过开发用于欺诈检测的FL模型，我们可以利用多个客户端的分布式知识和数据来提高欺诈分类的准确性。联邦方法还通过将敏感的交易数据保留在每个客户端本地来解决隐私问题。
- en: In summary, this project demonstrates the implementation of an FL model using
    the Flower framework. The server and clients collaborate to train a global model
    for fraud detection, exchanging model weights and parameters over multiple communication
    rounds. By aggregating the client models using FedAvg, we can leverage the collective
    intelligence of multiple participants while ensuring data privacy.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这个项目展示了使用Flower框架实现FL模型的过程。服务器和客户端协作训练一个用于欺诈检测的全局模型，在多个通信轮次中交换模型权重和参数。通过使用FedAvg聚合客户端模型，我们可以利用多个参与者的集体智慧，同时确保数据隐私。
- en: The dataset used in the example
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本例中使用的数据集
- en: '*Lopez, Elmir, and Axelsson* developed a mobile money dataset for fraud detection,
    and it is featured on Kaggle as well (E. A. Lopez-Rojas, A. Elmir, and S. Axelsson,
    *PaySim: A financial mobile money simulator for fraud detection*, 28th European
    Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus. 2016).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*Lopez, Elmir, 和Axelsson* 开发了一个用于欺诈检测的移动货币数据集，并在Kaggle上也有展示（E. A. Lopez-Rojas,
    A. Elmir, 和S. Axelsson, *PaySim: A financial mobile money simulator for fraud
    detection*, 第28届欧洲建模与仿真研讨会-EMSS，拉纳卡，塞浦路斯。2016）。'
- en: We will make use of this dataset for the detection of fraud using FL, but the
    same can be extended to anti-money laundering use cases as well, with minor changes
    to the model. The dataset can be found at [https://github.com/EdgarLopezPhD/PaySim](https://github.com/EdgarLopezPhD/PaySim).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用此数据集通过FL进行欺诈检测，但也可以通过修改模型以适应反洗钱用例，数据集可在[https://github.com/EdgarLopezPhD/PaySim](https://github.com/EdgarLopezPhD/PaySim)找到。
- en: Download this dataset and keep the file in the [*Chapter 6*](B16573_06_split_000.xhtml#_idTextAnchor120)
    directory with the name `PS_20174392719_1491204439457_log.csv`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下载此数据集，并将文件保存在[*第6章*](B16573_06_split_000.xhtml#_idTextAnchor120)目录中，文件名为`PS_20174392719_1491204439457_log.csv`。
- en: 'This dataset consists of 6.3 million records of transactions and has the following
    features:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集包含630万条交易记录，具有以下特征：
- en: '| **Field** | **Data type** | **Details** |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| **字段** | **数据类型** | **详细信息** |'
- en: '| --- | --- | --- |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `Step` | Numerical | The unit of time in the real world. One step is 1 hour.
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `Step` | 数值 | 现实世界中的时间单位。一步是1小时。 |'
- en: '| `Type` | Object | `CASH-IN`, `CASH-OUT`, `DEBIT`, `PAYMENT`, and `TRANSFER`.
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `Type` | 对象 | `CASH-IN`，`CASH-OUT`，`DEBIT`，`PAYMENT`和`TRANSFER`。 |'
- en: '| `Amount` | Numerical | The amount of the transaction. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `Amount` | 数值 | 交易金额。 |'
- en: '| `nameOrig` | Object | The customer who started the transaction. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `nameOrig` | 对象 | 开始交易的客户。 |'
- en: '| `nameDest` | Object | The recipient ID of the transaction. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `nameDest` | 对象 | 交易的接收者ID。 |'
- en: '| `oldbalanceOrg` | Numerical | The initial balance before the transaction.
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `oldbalanceOrg` | 数值 | 交易前的初始余额。 |'
- en: '| `newbalanceOrig` | Numerical | The customer’s balance after the transaction.
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `newbalanceOrig` | 数值 | 交易后客户的余额。 |'
- en: '| `oldbalanceDest` | Numerical | The initial recipient’s balance before the
    transaction. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `oldbalanceDest` | 数值 | 交易前初始接收者的余额。 |'
- en: '| `newbalanceDest` | Numerical | The recipient’s balance after the transaction.
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `newbalanceDest` | 数值 | 交易后接收者的余额。 |'
- en: '| `isFraud` | Boolean | Identifies fraudulent (`1`) and non-fraudulent (`0`)
    transactions. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `isFraud` | 布尔值 | 识别欺诈（`1`）和非欺诈（`0`）交易。 |'
- en: '| `isFlaggedFraud` | Boolean | Flags illegal attempts to transfer more than
    200,000 amount in a single transaction. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `isFlaggedFraud` | 布尔值 | 标记非法尝试在单笔交易中转账超过20万金额。 |'
- en: Table 6.5 – The dataset features
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.5 – 数据集特征
- en: The installation of Flower
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Flower的安装
- en: Install Flower using the `python -m pip install` `flwr` command.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`python -m pip install` `flwr`命令安装Flower。
- en: The implementation of a server
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务器的实现
- en: The server-side implementation of the FL model for fraud detection involves
    several high-level steps. We will utilize the sample code provided by the Flower
    framework and extend it to fit our specific use case.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测的FL模型的服务器端实现涉及几个高级步骤。我们将利用Flower框架提供的示例代码，并将其扩展以适应我们的特定用例。
- en: 'The following steps outline the process:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤概述了该过程：
- en: '**Initialize the** **model parameters**:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化模型参数**：'
- en: Set the initial model weights to **0** and initialize the intercept as **0**
    (since we are working with a regression model).
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始模型权重设置为**0**，并将截距初始化为**0**（因为我们正在使用回归模型）。
- en: Determine the number of classes or labels for classification.
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定分类的类别或标签数量。
- en: Determine the number of features used in the model.
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定模型中使用的特征数量。
- en: Determine the number of participating clients.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定参与客户端的数量。
- en: '**Define the supporting functions**: Develop additional functions to load the
    data from clients, define the loss function, and evaluate the model’s performance.
    These functions will help facilitate data handling, calculate the loss during
    training, and assess the model’s accuracy.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义支持函数**：开发额外的函数以从客户端加载数据，定义损失函数，并评估模型的性能。这些函数将有助于数据处理，计算训练过程中的损失，并评估模型的准确性。'
- en: '**Choose the server-side strategy**: Select the FedAvg algorithm as the strategy
    to aggregate the weights received from the clients. FedAvg is a popular choice
    to combine model updates from multiple clients and generate an updated global
    model.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择服务器端策略**：选择FedAvg算法作为从客户端接收权重的聚合策略。FedAvg是结合多个客户端的模型更新并生成更新全局模型的一个流行选择。'
- en: '**Start** **the server**:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**启动** **服务器**：'
- en: Initiate the server-side component, which will orchestrate the FL process.
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动服务器端组件，该组件将协调FL过程。
- en: The server will communicate with the participating clients, receive their model
    updates, and aggregate the weights using the FedAvg algorithm.
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务器将与参与客户端通信，接收他们的模型更新，并使用FedAvg算法汇总权重。
- en: It will also handle the synchronization of the model updates between the clients
    and ensure the convergence of the global model.
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它还将处理客户端之间的模型更新同步，并确保全局模型的收敛。
- en: By following these steps, we can implement the server-side functionality of
    our FL model. The initialization of model parameters, definition of supporting
    functions, selection of the server-side strategy (FedAvg), and starting the server
    itself are crucial in facilitating the collaborative training process among the
    clients. Through this implementation, the server will act as the central coordinator,
    receiving and aggregating the model updates from the clients. It plays a crucial
    role in ensuring the model’s convergence and generating an updated global model
    that incorporates the knowledge from all participating clients.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤，我们可以实现FL模型的服务器端功能。模型参数的初始化、支持函数的定义、服务器端策略（FedAvg）的选择以及启动服务器本身对于促进客户端之间的协作训练过程至关重要。通过此实现，服务器将作为中央协调者，接收并汇总来自客户端的模型更新。它在确保模型收敛和生成包含所有参与客户端知识的更新全局模型中发挥着关键作用。
- en: 'Save the following code as `FL_AML_Server.py`:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下代码保存为`FL_AML_Server.py`：
- en: '[PRE0]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This results in the following output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 6.12 – Dataset information with few rows and columns](img/B16573_06_12.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – 少行少列的dataset信息](img/B16573_06_12.jpg)'
- en: Figure 6.12 – Dataset information with few rows and columns
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 少行少列的dataset信息
- en: 'Instead of using all 6 million records, we will use only the first 25,000 records
    in this example implementation:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例实现中，我们不会使用全部600万条记录，而只会使用前25,000条记录：
- en: '[PRE1]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This results in the following output:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE2]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s find the data types of each field in the dataset:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找出数据集中每个字段的类型：
- en: '[PRE3]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE4]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Encode the object data type fields as labels using `LabelEncoder`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`LabelEncoder`将对象数据类型字段编码为标签：
- en: '[PRE5]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Open a terminal and run this program (`python3 FL_AML_Server.py`)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 打开终端并运行此程序（`python3 FL_AML_Server.py`）
- en: 'This results in the following output:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 6.13 – Server startup logs](img/B16573_06_13.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – 服务器启动日志](img/B16573_06_13.jpg)'
- en: Figure 6.13 – Server startup logs
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 服务器启动日志
- en: Server will run and wait for data from clients to process.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器将运行并等待来自客户端的数据进行处理。
- en: The implementation of clients
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户端的实现
- en: 'The client-side implementation of the FL model for fraud detection involves
    the following steps. We will utilize the provided `NumPyClient` from the Flower
    samples. The steps are as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测FL模型的客户端实现涉及以下步骤。我们将利用Flower示例中提供的`NumPyClient`。步骤如下：
- en: '**Load** **the data**:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载** **数据**：'
- en: Load the relevant data to train and test the fraud detection model.
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载相关数据以训练和测试欺诈检测模型。
- en: Ensure the data is properly formatted and available for processing.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保数据格式正确且可用于处理。
- en: '**Split** **the data**:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分割** **数据**：'
- en: Split the loaded data into training and testing sets. This division allows you
    to evaluate the model’s performance on unseen data.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将加载的数据分割成训练集和测试集。这种划分允许你在未见过的数据上评估模型的性能。
- en: '**Shuffle/partition** **the data**:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**洗牌/分区** **数据**：'
- en: Shuffle or partition the training data into batches.
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据洗牌或分区成批次。
- en: Randomly select a partition for each round of communication with the server.
    This ensures that different subsets of the training data are used in each round.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在与服务器通信的每一轮中随机选择一个分区。这确保了在每一轮中使用不同的训练数据子集。
- en: '**Create the linear** **regression model**:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建线性** **回归模型**：'
- en: Develop a simple linear regression model using the chosen framework (for example,
    scikit-learn).
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所选框架（例如，scikit-learn）开发一个简单的线性回归模型。
- en: Configure the model with appropriate settings for the fraud detection task.
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为欺诈检测任务配置模型适当的设置。
- en: '**Establish a connection with** **the server**:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**与服务器建立连接**：'
- en: Establish a connection with the server to send and receive model weights.
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与服务器建立连接以发送和接收模型权重。
- en: Utilize the provided communication protocol (for example, gRPC) to exchange
    information.
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用提供的通信协议（例如，gRPC）交换信息。
- en: '**Train** **the model**:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练模型**：'
- en: Initialize the model with the initial weights received from the server.
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从服务器接收的初始权重初始化模型。
- en: Train the model using the client’s local data and the weights updated by the
    server for each round.
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用客户端的本地数据和服务器在每个轮次更新的权重来训练模型。
- en: Apply appropriate optimization techniques (for example, gradient descent) to
    update the model parameters.
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用适当的优化技术（例如，梯度下降）来更新模型参数。
- en: '**Test** **the model**:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试模型**：'
- en: Evaluate the trained model using the testing data to assess its performance.
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试数据评估训练模型的性能。
- en: Calculate relevant metrics such as accuracy, precision, recall, or F1 score.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相关指标，如准确率、精确率、召回率或F1分数。
- en: Determine the model’s effectiveness in detecting fraudulent transactions.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定模型在检测欺诈交易方面的有效性。
- en: By following these steps, we can implement the client-side functionality of
    the FL model. The client will load and partition the data, create the linear regression
    model, establish a connection with the server, train the model using local data
    and updated weights, and evaluate its performance. The client’s role is crucial
    in contributing local knowledge while preserving data privacy. By training on
    their respective local data and participating in the FL process, clients collectively
    improve the global fraud detection model without sharing sensitive information.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤，我们可以实现FL模型的客户端功能。客户端将加载数据并对其进行分区，创建线性回归模型，与服务器建立连接，使用本地数据和更新的权重训练模型，并评估其性能。客户端在贡献本地知识的同时保护数据隐私的作用至关重要。通过在各自本地数据上训练并参与FL过程，客户端共同提高全局欺诈检测模型，而不共享敏感信息。
- en: Creating a non-IID dataset
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个非IID数据集
- en: 'To transform the dataset into a non-IID setting, we can apply the following
    approach:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据集转换为非IID设置，我们可以采用以下方法：
- en: '**First client (****client 1)**:'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一个客户端（**客户端1**）**：'
- en: Apply the **Synthetic Minority Oversampling Technique** (**SMOTE**) to oversample
    the fraud transactions
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对欺诈交易应用**合成少数类过采样技术**（**SMOTE**）以增加样本量
- en: This technique generates synthetic examples of the minority class (fraudulent
    transactions) to balance the dataset
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种技术生成少数类（欺诈交易）的合成示例以平衡数据集
- en: As a result, client 1 will have a training dataset with 50,000 samples, consisting
    of 25,000 original transactions and 25,000 synthetic fraud examples created using
    SMOTE
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，客户端1将有一个包含50,000个样本的训练数据集，其中包括25,000个原始交易和25,000个使用SMOTE创建的合成欺诈示例
- en: The distribution of fraud versus non-fraud transactions will be balanced at
    50% for each class
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈与非欺诈交易的分布将在每个类别中平衡至50%
- en: '**Second client (****client 2)**:'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二个客户端（**客户端2**）**：'
- en: Leave the transactions as they are without any oversampling or modification
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不对交易进行任何过采样或修改
- en: Client 2 will have a training dataset with the last 25,000 transactions
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端2将有一个包含最后25,000个交易的训练数据集
- en: The class distribution will reflect the original distribution, with only 2%
    of the transactions classified as fraud
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别分布将反映原始分布，只有2%的交易被分类为欺诈
- en: By employing this approach, we introduce non-identical and imbalanced datasets
    across the two clients. Client 1 will have a balanced dataset with equal representation
    of fraud and non-fraud transactions, while client 2 will have a dataset that mirrors
    the original distribution.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这种方法，我们在两个客户端之间引入了非相同和不平衡的数据集。客户端1将有一个平衡的数据集，欺诈和非欺诈交易的代表性相等，而客户端2将有一个与原始分布相似的数据集。
- en: This non-IID setup allows us to simulate real-world scenarios where different
    clients may have varying distributions of data. Through FL, both clients can contribute
    their local knowledge while training their models on distinct datasets, ultimately
    improving the overall fraud detection model.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这种非IID设置使我们能够模拟现实世界场景，其中不同的客户端可能具有不同的数据分布。通过联邦学习（FL），两个客户端都可以贡献其本地知识，同时在不同的数据集上训练模型，从而最终提高整体欺诈检测模型的性能。
- en: '|  | Client 1 | Client 2 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  | 客户端1 | 客户端2 |'
- en: '| --- | --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Original transactions | 25,000 | 25,000 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 原始交易 | 25,000 | 25,000 |'
- en: '| Transactions generated using SMOTE | 25,000 | 0 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 使用 SMOTE 生成的事务 | 25,000 | 0 |'
- en: '| Total transactions | 50,000 | 25,000 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 总交易数 | 50,000 | 25,000 |'
- en: '| Fraud versus non-fraud | 50% and 50% | Fraud: 2.43%(608)Non-fraud: 97.57%
    (24,392) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 欺诈与非欺诈 | 50% 和 50% | 欺诈：2.43%（608）非欺诈：97.57%（24,392） |'
- en: '| Train and test split | 70:30 | 70:30 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 训练和测试分割 | 70:30 | 70:30 |'
- en: '| Number of partitions after shuffling the train data with equal size in each
    partition | 10 | 10 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 在每个分区大小相等的情况下，对训练数据进行洗牌后的分区数 | 10 | 10 |'
- en: Table 6.6 – Training data distribution on the client side as non-IID data
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.6 – 客户端侧的非 IID 训练数据分布
- en: 'Here is the code for client 1\. Save this code as `FL_AML_Client1.py`:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '这里是客户端 1 的代码。将此代码保存为 `FL_AML_Client1.py`:'
- en: '[PRE6]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This results in the following output:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE7]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this dataset, fraudulent transactions account for 0.33% of the total data,
    indicating a highly imbalanced dataset. This imbalance is typical in real-world
    scenarios, where fraud transactions are much less frequent compared to genuine
    (non-fraud) transactions.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，欺诈交易占总数据的 0.33%，表明数据集高度不平衡。这种不平衡在现实场景中很典型，欺诈交易比真实（非欺诈）交易少得多。
- en: '[PRE8]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE9]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Encode the object types as labels using sci-kit learn’s `LabelEncoder`:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 sci-kit learn 的 `LabelEncoder` 将对象类型编码为标签：
- en: '[PRE10]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Apply SMOTE to generate synthetic data:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 SMOTE 生成合成数据：
- en: '[PRE11]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in the following output:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE12]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Open a second terminal and run the client 1 code (`python3 FL_AML_Client1.py`)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 打开第二个终端并运行客户端 1 的代码（`python3 FL_AML_Client1.py`）
- en: 'This results in the following output:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 6.14 – The execution of client 1 and the logs](img/B16573_06_14.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 客户端 1 的执行及其日志](img/B16573_06_14.jpg)'
- en: Figure 6.14 – The execution of client 1 and the logs
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 客户端 1 的执行及其日志
- en: 'Now, let’s look at the code for client 2\. Save this code as `FL_AML_Client2.py`.
    The client 2 code will be the same as client 1, but fraud transactions are not
    increased using the SMOTE method. For thoroughness, here is the complete code
    for the second client:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看客户端 2 的代码。将此代码保存为 `FL_AML_Client2.py`。客户端 2 的代码将与客户端 1 相同，但不会使用 SMOTE
    方法增加欺诈交易。为了全面性，以下是第二个客户端的完整代码：
- en: '[PRE13]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following output:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE14]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Encode the object types as labels using sci-kit learn’s `LabelEncoder`:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 sci-kit learn 的 `LabelEncoder` 将对象类型编码为标签：
- en: '[PRE15]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This results in the following output:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE16]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Open another terminal and run the client 2 code (`python3 FL_AML_Client2.py`):'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个终端中运行客户端 2 的代码（`python3 FL_AML_Client2.py`）：
- en: '![Figure 6.15 – Running client 2 and the logs](img/B16573_06_15.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – 运行客户端 2 及其日志](img/B16573_06_15.jpg)'
- en: Figure 6.15 – Running client 2 and the logs
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 运行客户端 2 及其日志
- en: 'Once you run the client 2 code, pay close attention to the log statements on
    the server side. The server will initiate communication with both clients, enabling
    the exchange of the initial parameters and, subsequently, the updated weights
    for each round. Monitoring the server logs will provide insights into the progress
    of the FL process and the information shared between the clients and the server:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行客户端 2 的代码，请密切关注服务器端的日志语句。服务器将与两个客户端进行通信，启用初始参数的交换，并在随后的每一轮中更新每个客户端的权重。监控服务器日志将提供关于联邦学习过程进展和客户端与服务器之间共享信息的见解：
- en: '![Figure 6.16 – The server-side logs](img/B16573_06_16.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.16 – 服务器端日志](img/B16573_06_16.jpg)'
- en: Figure 6.16 – The server-side logs
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 服务器端日志
- en: 'Observe the logs on both clients as well as the server side. These metrics
    provide an overview of the loss (indicating the model’s performance) and accuracy
    (representing the model’s correctness) for each client across multiple rounds
    of the FL process:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 观察客户端和服务器端的日志。这些指标提供了关于每个客户端在联邦学习过程的多个回合中损失（表示模型的性能）和准确度（表示模型的正确性）的概述：
- en: '| **Server** | **Client 1** | **Client 2** |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| **服务器** | **客户端 1** | **客户端 2** |'
- en: '| --- | --- | --- |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| INFO flwr 2023-03-14 17:46:49,202 &#124; app.py:139 &#124; Starting Flower
    server, config: ServerConfig(num_rounds=5, round_ timeout=None)INFO flwr 2023-03-14
    17:51:21,810 &#124; server.py:101 &#124; FL starting |  |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| INFO flwr 2023-03-14 17:46:49,202 &#124; app.py:139 &#124; Starting Flower
    server, config: ServerConfig(num_rounds=5, round_ timeout=None)INFO flwr 2023-03-14
    17:51:21,810 &#124; server.py:101 &#124; FL starting |  |  |'
- en: '|  | DEBUG flwr 2023-03-14 17:51:21,778 &#124; connection.py:38 &#124; ChannelConnectivity.READY
    |  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | DEBUG flwr 2023-03-14 17:51:21,778 &#124; connection.py:38 &#124; ChannelConnectivity.READY
    |  |'
- en: '|  |  | ChannelConnectivity. DEBUG flwr 2023-03-14 17:53:46,338 &#124; connection.py:38
    &#124; ChannelConnectivity.READY |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ChannelConnectivity. DEBUG flwr 2023-03-14 17:53:46,338 &#124; connection.py:38
    &#124; ChannelConnectivity.READY |'
- en: '| DEBUG flwr 2023-03-14 17:53:46,338 &#124; server.py:215 &#124; fit_round
    1: strategy sampled 2 clients (out of 2)DEBUG flwr 2023-03-14 17:53:46,351 &#124;
    server.py:229 &#124; fit_round 1 received 2 results and 0 failuresWARNING flwr
    2023-03-14 17:53:46,354 &#124; fedavg.py:242 &#124; No fit_metrics_aggregation_fn
    providedINFO flwr 2023-03-14 17:53:46,362 &#124; server.py:116 &#124; fit progress:
    (1, 0.06756539217831908, {‘accuracy’: 0.9962666666666666}, 144.549737353)DEBUG
    flwr 2023-03-14 17:53:46,363 &#124; server.py:165 &#124; evaluate_round 1: strategy
    sampled 2 clients (out of 2)DEBUG flwr 2023-03-14 17:53:46,377 &#124; server.py:179
    &#124; evaluate_round 1 received 2 results and 0 failures |  |  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| DEBUG flwr 2023-03-14 17:53:46,338 &#124; server.py:215 &#124; fit_round
    1: strategy sampled 2 clients (out of 2)DEBUG flwr 2023-03-14 17:53:46,351 &#124;
    server.py:229 &#124; fit_round 1 received 2 results and 0 failuresWARNING flwr
    2023-03-14 17:53:46,354 &#124; fedavg.py:242 &#124; No fit_metrics_aggregation_fn
    providedINFO flwr 2023-03-14 17:53:46,362 &#124; server.py:116 &#124; fit progress:
    (1, 0.06756539217831908, {‘accuracy’: 0.9962666666666666}, 144.549737353)DEBUG
    flwr 2023-03-14 17:53:46,363 &#124; server.py:165 &#124; evaluate_round 1: strategy
    sampled 2 clients (out of 2)DEBUG flwr 2023-03-14 17:53:46,377 &#124; server.py:179
    &#124; evaluate_round 1 received 2 results and 0 failures |  |  |'
- en: '| INFO flwr 2023-03-14 17:53:46,400 &#124; server.py:116 &#124; fit progress:
    (2, 0.40485776608772656, {‘accuracy’: 0.9633333333333334}, 144.58791799899996)
    |  |  |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| INFO flwr 2023-03-14 17:53:46,400 &#124; server.py:116 &#124; fit progress:
    (2, 0.40485776608772656, {‘accuracy’: 0.9633333333333334}, 144.58791799899996)
    |  |  |'
- en: '| INFO flwr 2023-03-14 17:53:46,432 &#124; server.py:116 &#124; fit progress:
    (3, 0.11833075507570899, {‘accuracy’: 0.9962666666666666}, 144.61946266499996)
    |  |  |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| INFO flwr 2023-03-14 17:53:46,432 &#124; server.py:116 &#124; fit progress:
    (3, 0.11833075507570899, {‘accuracy’: 0.9962666666666666}, 144.61946266499996)
    |  |  |'
- en: '| INFO flwr 2023-03-14 17:53:46,465 &#124; server.py:116 &#124; fit progress:
    (4, 0.1145626928425223, {‘accuracy’: 0.9962666666666666}, 144.65267561899998)
    |  |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| INFO flwr 2023-03-14 17:53:46,465 &#124; server.py:116 &#124; fit progress:
    (4, 0.1145626928425223, {‘accuracy’: 0.9962666666666666}, 144.65267561899998)
    |  |  |'
- en: '| INFO flwr 2023-03-14 17:53:46,497 &#124; server.py:116 &#124; fit progress:
    (5, 0.27867744042157033, {‘accuracy’: 0.9861333333333333}, 144.68508043599996)
    |  |  |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| INFO flwr 2023-03-14 17:53:46,497 &#124; server.py:116 &#124; fit progress:
    (5, 0.27867744042157033, {‘accuracy’: 0.9861333333333333}, 144.68508043599996)
    |  |  |'
- en: '| INFO flwr 2023-03-14 17:53:46,511 &#124; app.py:202 &#124; app_fit: losses_distributed
    [(1, 0.4398987330496311), (2, 0.4606742262840271), (3, 0.5105149038136005), (4,
    0.5070083439350128), (5, 0.5951354652643204)] |  |  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| INFO flwr 2023-03-14 17:53:46,511 &#124; app.py:202 &#124; app_fit: losses_distributed
    [(1, 0.4398987330496311), (2, 0.4606742262840271), (3, 0.5105149038136005), (4,
    0.5070083439350128), (5, 0.5951354652643204)] |  |  |'
- en: '|  | Training finished for round 1:0.06756539217831908 0.9962666666666666Training
    finished for round 2:0.40485776608772656 0.9633333333333334Training finished for
    round 3:0.11833075507570899 0.9962666666666666Training finished for round 4:0.1145626928425223
    0.9962666666666666Training finished for round 5:0.27867744042157033 0.9861333333333333
    | Training finished for round 1:0.8122320748323023 0.9745333333333334Training
    finished for round 2:0.5164906830160562 0.9541333333333334Training finished for
    round 3:0.9026990471833415 0.9745333333333334Training finished for round 4:0.8994540131249842
    0.9745333333333334Training finished for round 5:0.9115935132282235 0.9736 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | 第 1 轮训练完成：0.06756539217831908 0.9962666666666666 第 2 轮训练完成：0.40485776608772656
    0.9633333333333334 第 3 轮训练完成：0.11833075507570899 0.9962666666666666 第 4 轮训练完成：0.1145626928425223
    0.9962666666666666 第 5 轮训练完成：0.27867744042157033 0.9861333333333333 第 1 轮训练完成：0.8122320748323023
    0.9745333333333334 第 2 轮训练完成：0.5164906830160562 0.9541333333333334 第 3 轮训练完成：0.9026990471833415
    0.9745333333333334 第 4 轮训练完成：0.8994540131249842 0.9745333333333334 第 5 轮训练完成：0.9115935132282235
    0.9736 |'
- en: '|  | DEBUG flwr 2023-03-14 17:53:46,521 &#124; connection.py:109 &#124; gRPC
    channel closedINFO flwr 2023-03-14 17:53:46,522 &#124; app.py:153 &#124; Disconnect
    and shut down | DEBUG flwr 2023-03-14 17:53:46,521 &#124; connection.py:109 &#124;
    gRPC channel closedINFO flwr 2023-03-14 17:53:46,522 &#124; app.py:153 &#124;
    Disconnect and shut down |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | DEBUG flwr 2023-03-14 17:53:46,521 &#124; connection.py:109 &#124; gRPC
    channel closedINFO flwr 2023-03-14 17:53:46,522 &#124; app.py:153 &#124; Disconnect
    and shut down | DEBUG flwr 2023-03-14 17:53:46,521 &#124; connection.py:109 &#124;
    gRPC channel closedINFO flwr 2023-03-14 17:53:46,522 &#124; app.py:153 &#124;
    Disconnect and shut down |'
- en: Table 6.8 – Log data at Server and Clients
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.8 – 服务器和客户端的日志数据
- en: As per the log, there are 5 rounds of communication between clients and the
    server, and in each round, accuracy results and loss change based on the weights.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 根据日志，客户端和服务器之间有 5 轮通信，在每一轮中，准确率和损失根据权重变化。
- en: '| **Client 1** | **Loss** | **Accuracy** |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| **客户端 1** | **损失** | **准确率** |'
- en: '| --- | --- | --- |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Round 1 | 0.06756539217831908 | 0.9962666666666666 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 第一轮 | 0.06756539217831908 | 0.9962666666666666 |'
- en: '| Round 2 | 0.40485776608772656 | 0.9633333333333334 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 第二轮 | 0.40485776608772656 | 0.9633333333333334 |'
- en: '| Round 3 | 0.11833075507570899 | 0.9962666666666666 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 第三轮 | 0.11833075507570899 | 0.9962666666666666 |'
- en: '| Round 4 | 0.1145626928425223 | 0.9962666666666666 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 第四轮 | 0.1145626928425223 | 0.9962666666666666 |'
- en: '| Round 5 | 0.27867744042157033 | 0.9861333333333333 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 第五轮 | 0.27867744042157033 | 0.9861333333333333 |'
- en: Table 6.9 – Accuracy and Loss metrics at Client 1
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.9 – 客户端1的准确度和损失指标
- en: As per the debug logs, loss and accuracy vary on client 1\. Let’s observe the
    loss and accuracy results on Client 2 as well.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 根据调试日志，客户端1的损失和准确度有所变化。让我们也观察一下客户端2的损失和准确度结果。
- en: '| **Client 2** | **Loss** | **Accuracy** |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| **客户端2** | **损失** | **准确度** |'
- en: '| --- | --- | --- |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Round 1 | 0.8122320748323023 | 0.9745333333333334 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| 第一轮 | 0.8122320748323023 | 0.9745333333333334 |'
- en: '| Round 2 | 0.5164906830160562 | 0.9541333333333334 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| 第二轮 | 0.5164906830160562 | 0.9541333333333334 |'
- en: '| Round 3 | 0.9026990471833415 | 0.9745333333333334 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 第三轮 | 0.9026990471833415 | 0.9745333333333334 |'
- en: '| Round 4 | 0.8994540131249842 | 0.9745333333333334 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 第四轮 | 0.8994540131249842 | 0.9745333333333334 |'
- en: '| Round 5 | 0.9115935132282235 | 0.9736 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 第五轮 | 0.9115935132282235 | 0.9736 |'
- en: Table 6.10 – Accuracy and Loss metrics at Client 2
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.10 – 客户端2的准确度和损失指标
- en: We have implemented a sample fraud detection application using Federated Learning
    and made use of open-source frameworks like Flower. In the next section, let’s
    try to learn and implement federated learning using differential privacy.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用联邦学习实现了一个样本欺诈检测应用，并使用了Flower等开源框架。在下一节中，让我们尝试学习和实现使用差分隐私的联邦学习。
- en: FL with differential privacy
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FL与差分隐私
- en: '**Federated Learning with Differential Privacy** (**FL-DP**) is an approach
    that combines the principles of FL and **Differential Privacy** (**DP**) to ensure
    privacy and security in distributed ML systems. FL-DP aims to protect sensitive
    data while enabling collaborative model training across multiple devices or entities.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦学习与差分隐私**（**FL-DP**）是一种结合了FL和**差分隐私**（**DP**）原则的方法，以确保分布式机器学习系统中的隐私和安全。FL-DP旨在保护敏感数据，同时允许在多个设备或实体之间进行协作模型训练。'
- en: The goal of FL-DP is to achieve accurate model training without compromising
    the privacy of individual data contributors. It addresses the challenge of preventing
    data leakage during the aggregation of model updates from different participants.
    By incorporating DP techniques, FL-DP provides strong privacy guarantees by adding
    noise or perturbation to the model updates or gradients before aggregating them.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: FL-DP的目标是在不损害个别数据贡献者隐私的情况下实现准确的模型训练。它解决了在聚合来自不同参与者的模型更新时防止数据泄露的挑战。通过在聚合之前对模型更新或梯度添加噪声或扰动，FL-DP通过DP技术提供了强大的隐私保证。
- en: There are different approaches to implementing FL-DP. One common approach involves
    each client training a local ML model using their own data. The client applies
    techniques such as clipping and noise addition to the gradients or weights of
    the model. The client then sends the updated data to the server. On the server
    side, the updates are aggregated while preserving privacy using techniques such
    as secure aggregation or privacy-preserving FL algorithms. This ensures that individual
    client data remains private while enabling collaborative model training.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 实现联邦差分隐私（FL-DP）的方法有多种。一种常见的方法是每个客户端使用自己的数据训练一个本地机器学习模型。客户端会对模型的梯度或权重应用剪枝和噪声添加等技术。然后，客户端将更新后的数据发送到服务器。在服务器端，使用诸如安全聚合或隐私保护FL算法等技术对更新进行聚合，同时保护隐私。这确保了个人客户端数据保持隐私，同时允许协作模型训练。
- en: FL-DP algorithms may vary depending on the specific differential privacy mechanisms
    used, such as Gaussian noise addition, subsampling, or advanced techniques such
    as **Private Aggregation of Teacher Ensembles** (**PATE**). The choice of techniques
    depends on the level of privacy required and the characteristics of the distributed
    dataset.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: FL-DP算法可能因所使用的特定差分隐私机制而异，例如高斯噪声添加、子采样或如**私有教师集成聚合**（**PATE**）等高级技术。技术选择取决于所需的隐私级别和分布式数据集的特征。
- en: Implementing FL-DP requires careful consideration of privacy, accuracy, and
    computational overhead. It involves striking a balance between preserving privacy
    and maintaining model utility. Various frameworks and libraries, such as Flower
    and TensorFlow Privacy, provide tools and techniques to facilitate the implementation
    of FL-DP.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 实施FL-DP需要对隐私、准确性和计算开销进行仔细考虑。它涉及在保护隐私和保持模型效用之间取得平衡。各种框架和库，如Flower和TensorFlow
    Privacy，提供了工具和技术，以促进FL-DP的实施。
- en: FL-DP has the potential to unlock the benefits of collaborative ML in scenarios
    where data privacy and security are paramount. By preserving privacy, FL-DP enables
    organizations and individuals to collaborate on model training while safeguarding
    sensitive information.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: FL-DP有可能在数据隐私和安全至关重要的场景中释放协作机器学习的优势。通过保护隐私，FL-DP使组织和个人能够在模型训练中协作，同时保护敏感信息。
- en: FL-DP provides a way to implement privacy-preserving techniques in the FL process,
    ensuring that client-side data remains protected.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: FL-DP提供了一种在FL过程中实施隐私保护技术的方法，确保客户端数据保持安全。
- en: In this section, we will explore two general approaches to implementing FL-DP,
    although specific frameworks and implementations may have slight variations.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨实现FL-DP的两种通用方法，尽管具体的框架和实现可能略有不同。
- en: Approach one
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一种方法
- en: This approach shares similarities with **Differentially Private Federated Averaging**
    (**DP-FedAvg**), which was introduced by the Google research team. By following
    these approaches, FL-DP allows you to train ML models on client data while preserving
    privacy through techniques such as clipping and noise addition.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与谷歌研究团队引入的**差分隐私联邦平均**（**DP-FedAvg**）类似。通过遵循这些方法，FL-DP允许您在客户端数据上训练机器学习模型，同时通过剪裁和添加噪声等技术来保护隐私。
- en: 'Each client does the following:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 每个客户端执行以下操作：
- en: Trains an ML/DL model using its local data.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用其本地数据训练机器学习/深度学习模型。
- en: Computes gradients/weights using a standard SGD algorithm.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准SGD算法计算梯度/权重。
- en: Applies clipping to the weights to limit their sensitivity.
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对权重应用剪裁以限制其敏感性。
- en: Adds noise to the weights to introduce randomness and privacy.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向权重添加噪声以引入随机性和隐私。
- en: Sends the modified weights to the server.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将修改后的权重发送到服务器。
- en: 'The server does the following:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器执行以下操作：
- en: Computes the average of the weights received from each client.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算从每个客户端接收到的权重的平均值。
- en: Broadcasts back the updated weights to the clients.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将更新的权重广播回客户端。
- en: Alternatively to step 1, applies clipping and adds noise to the final weights
    before broadcasting.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，在步骤1中，在广播之前对最终权重应用剪裁并添加噪声。
- en: '![Figure 6.17 – The DP-FedAvg model weights exchanged with the server](img/B16573_06_17.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 与服务器交换的DP-FedAvg模型权重](img/B16573_06_17.jpg)'
- en: Figure 6.17 – The DP-FedAvg model weights exchanged with the server
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 与服务器交换的DP-FedAvg模型权重
- en: In this approach, each client trains its model on local data and work on the
    updated average weights sent by the server.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，每个客户端在其本地数据上训练其模型，并处理服务器发送的更新后的平均权重。
- en: Approach two
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二种方法
- en: In this approach, each client trains its model on local data and applies privacy-preserving
    techniques to compute the gradients/weights. The server then incorporates these
    noisy weights and performs aggregation using the FD-SGD algorithm, ensuring privacy
    is maintained throughout the FL process.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，每个客户端在其本地数据上训练其模型，并应用隐私保护技术来计算梯度/权重。然后服务器将这些噪声权重合并，并使用FD-SGD算法进行聚合，确保在整个FL过程中保持隐私。
- en: 'Each client does the following:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 每个客户端执行以下操作：
- en: Trains an ML model using its local data.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用其本地数据训练机器学习模型。
- en: Computes the gradients/weights using either noisy SGD or DP-SGD (DP stochastic
    gradient) algorithms, which incorporate noise during gradient computation to preserve
    privacy.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用带有噪声的SGD或DP-SGD（DP随机梯度）算法计算梯度/权重，这些算法在梯度计算过程中引入噪声以保护隐私。
- en: Sends the weights to the server.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重发送到服务器。
- en: 'The server does the following:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器执行以下操作：
- en: Utilizes the noisy weights received from the clients.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用从客户端接收到的噪声权重。
- en: Follows the **Federated Differential SGD** (**FD-SGD**) algorithm, which incorporates
    privacy-preserving techniques during the aggregation process on the server.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遵循**联邦差分SGD**（**FD-SGD**）算法，该算法在服务器上的聚合过程中采用了隐私保护技术。
- en: '![Figure 6.18 – The DP-FedSGD model weights exchanged with the server](img/B16573_06_18.jpg)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – 与服务器交换的DP-FedSGD模型权重](img/B16573_06_18.jpg)'
- en: Figure 6.18 – The DP-FedSGD model weights exchanged with the server
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – 与服务器交换的DP-FedSGD模型权重
- en: There are various variants of Differential Privacy Federated Learning (FL-DP)
    algorithms designed to address different scenarios, such as cross-device and cross-silo
    FL, with both homogeneous and heterogeneous data. In our implementation, we will
    apply FL-DP to the same example as before, ensuring privacy preservation throughout
    the FL process.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 设计了各种不同的差分隐私联邦学习（FL-DP）算法变体，旨在解决不同的场景，如跨设备和跨存储库的FL，以及同质化和异质化数据。在我们的实现中，我们将应用FL-DP到之前相同的示例中，确保在整个FL过程中保持隐私保护。
- en: A sample application using FL-DP
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个使用FL-DP的示例应用
- en: At the time of writing, the Flower framework (version 1.3) currently offers
    experimental support for FL-DP. It provides a strategy class (similar to FedAvg,
    FedYogi, and so on) specifically designed to support FL-DP. The class name designed
    to support this is `DPFedAvg`.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Flower框架（版本1.3）目前提供对FL-DP的实验性支持。它提供了一个策略类（类似于FedAvg、FedYogi等），专门设计来支持FL-DP。设计来支持这一功能的类名为`DPFedAvg`。
- en: The `DPFedAvg` class in the Flower framework is a component specifically designed
    to support FL-DP. It extends the functionality of the FedAvg algorithm by incorporating
    differential privacy techniques to protect the privacy of individual client data
    during model aggregation.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: Flower框架中的`DPFedAvg`类是一个专门设计来支持FL-DP的组件。它通过结合差分隐私技术来扩展FedAvg算法的功能，以保护在模型聚合过程中个人客户端数据的隐私。
- en: '`DPFedAvg` implements a privacy-preserving mechanism that ensures the privacy
    of client updates while enabling collaborative model training. It achieves this
    by adding noise or perturbation to the model updates or gradients received from
    each client, before aggregating them on the server side.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '`DPFedAvg`实现了一种隐私保护机制，确保客户端更新的隐私性，同时允许协作模型训练。它通过在聚合到服务器端之前向每个客户端收到的模型更新或梯度添加噪声或扰动来实现这一点。'
- en: 'The key features and functionalities of the `DPFedAvg` class include the following:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '`DPFedAvg`类的关键特性和功能包括以下内容：'
- en: '**DP**: **DPFedAvg** integrates DP techniques into the FL process, ensuring
    that the privacy of individual client data is preserved during model training.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DP**：**DPFedAvg**将差分隐私技术集成到FL过程中，确保在模型训练过程中个人客户端数据的隐私得到保护。'
- en: '**Noise addition**: **DPFedAvg** applies noise to the gradients or model updates
    received from each client before aggregating them. The amount of noise added is
    determined based on privacy parameters and privacy budget allocation.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声添加**：**DPFedAvg**在聚合之前将对每个客户端收到的梯度或模型更新应用噪声。添加的噪声量基于隐私参数和隐私预算分配。'
- en: '**Privacy budget management**: **DPFedAvg** incorporates mechanisms to manage
    and allocate the privacy budget effectively, ensuring that the desired privacy
    guarantees are maintained throughout the training process.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私预算管理**：**DPFedAvg**包含管理和有效分配隐私预算的机制，确保在整个训练过程中保持所需的隐私保证。'
- en: '**Privacy parameters**: **DPFedAvg** allows users to customize the privacy
    parameters such as privacy budget, noise distribution, and sensitivity of the
    model updates. These parameters enable fine-grained control over the level of
    privacy and utility trade-off.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私参数**：**DPFedAvg**允许用户自定义隐私参数，如隐私预算、噪声分布和模型更新的敏感性。这些参数使得对隐私和效用之间的权衡可以进行精细控制。'
- en: '**Model aggregation**: **DPFedAvg** performs the aggregation of client updates
    using the DP averaging algorithm. This ensures that the privacy of individual
    updates is preserved while generating an updated global model.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型聚合**：**DPFedAvg**使用DP平均算法执行客户端更新的聚合。这确保了个人更新的隐私得到保护，同时生成一个更新的全局模型。'
- en: '**Compatibility with the Flower framework**: **DPFedAvg** is designed to seamlessly
    integrate with the Flower framework, allowing users to incorporate DP into their
    FL pipelines using the existing Flower infrastructure.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与Flower框架的兼容性**：**DPFedAvg**被设计成可以无缝集成到Flower框架中，使用户能够通过现有的Flower基础设施将DP纳入他们的FL管道。'
- en: By using the `DPFedAvg` class in the Flower framework, developers and ML engineers
    can implement FL-DP straightforwardly and efficiently. It provides a powerful
    tool to ensure privacy in distributed ML scenarios while maintaining the collaborative
    benefits of FL.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在Flower框架中使用`DPFedAvg`类，开发者和机器学习工程师可以简单高效地实现FL-DP。它提供了一个强大的工具，以确保在分布式机器学习场景中的隐私性，同时保持联邦学习的协作优势。
- en: Let’s walk through the Flower-provided class in detail.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下Flower提供的类。
- en: The DPFedAvgFixed class
  id: totrans-446
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DPFedAvgFixed类
- en: This class is a wrapper class and adds clipping and Gaussian noise to the weights.
    The constructor of this class supports parameters to set server-side noise, a
    clip norm value, and the noise multiplier.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类是一个包装类，它向权重添加裁剪和高斯噪声。该类的构造函数支持设置服务器端噪声、裁剪范数值和噪声乘数参数。
- en: 'Let’s use this class on the server side. The server code is as follows:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在服务器端使用这个类。服务器代码如下：
- en: '[PRE17]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The source code of the Jupyter notebooks for the server and clients is located
    in the [*Chapter* *6*](B16573_06_split_000.xhtml#_idTextAnchor120) folder:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器和客户端的Jupyter笔记本源代码位于[*第6章*](B16573_06_split_000.xhtml#_idTextAnchor120)文件夹中：
- en: 'Server code: **Fed-DP-AML-Server.ipynb**'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器代码：**Fed-DP-AML-Server.ipynb**
- en: 'Client 1 code: **DP-FL-AML_Client1.ipynb**'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端1代码：**DP-FL-AML_Client1.ipynb**
- en: 'Client 2 code: **DP-FL-AML-Client2.ipynb**'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端2代码：**DP-FL-AML-Client2.ipynb**
- en: 'Let’s look at the accuracy of the model for the clients and server:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看客户端和服务器模型的准确率：
- en: '| **Client 1** | **Client 2** | **Server** |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| **客户端1** | **客户端2** | **服务器** |'
- en: '| --- | --- | --- |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0.99626666666666660.71626666666666670.98760.93720.7714666666666666 | 0.97453333333333340.59786666666666670.96933333333333340.94480.7708
    | 0.99626666666666660.71626666666666670.98760.93720.7714666666666666 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 0.99626666666666660.71626666666666670.98760.93720.7714666666666666 | 0.97453333333333340.59786666666666670.96933333333333340.94480.7708
    | 0.99626666666666660.71626666666666670.98760.93720.7714666666666666 |'
- en: Table 6.11 – Accracy results at Sever and Clients
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '| 表6.11 – 服务器和客户端的准确率结果 |'
- en: Applying DP to FL introduces some overhead in terms of computational cost, communication
    overhead, and potentially reduced model performance. In our example case, by the
    fourth round, the accuracy was 93%, but in the fifth round, the accuracy suddenly
    dropped. This tells us that we need to monitor the accuracy during training to
    help us decide on the number of rounds each client needs to participate in and
    stop further rounds when the accuracy drops.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 将DP应用于FL会在计算成本、通信开销以及可能降低模型性能方面引入一些开销。在我们的示例案例中，到第四轮时，准确率为93%，但在第五轮时，准确率突然下降。这告诉我们，我们需要在训练过程中监控准确率，以帮助我们决定每个客户端需要参与的轮数，并在准确率下降时停止进一步的轮次。
- en: Summary
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explained why FL is needed and looked at its definition
    and characteristics in detail. We covered the steps involved in implementing FL
    and discussed IID and non-IID datasets and FL algorithms. We implemented a sample
    application using an open source FL framework. Finally, we converted the same
    application using DP.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解释了为什么需要联邦学习（FL），并详细探讨了其定义和特性。我们涵盖了实现FL的步骤，讨论了独立同分布（IID）和非独立同分布（non-IID）数据集以及FL算法。我们使用开源FL框架实现了一个示例应用。最后，我们使用差分隐私（DP）将同一应用进行了转换。
- en: In the next chapter, we will learn about FL benchmarks and look at key start-ups
    that are working on or already have FL products.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习FL基准测试，并了解正在开发或已经拥有FL产品的关键初创公司。
