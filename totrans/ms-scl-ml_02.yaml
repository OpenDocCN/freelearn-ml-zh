- en: Chapter 2. Data Pipelines and Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have looked at basic hands-on tools for exploring the data in the previous
    chapter, thus we now can delve into more complex topics of statistical model building
    and optimal control or science-driven tools and problems. I will go ahead and
    say that we will only touch on some topics in optimal control since this book
    really is just about ML in Scala and not the theory of data-driven business management,
    which might be an exciting topic for a book on its own.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, I will stay away from specific implementations in Scala and
    discuss the problem of building a data-driven enterprise at a high level. Later
    chapters will address how to solve these smaller pieces of the puzzle. A special
    emphasis will be given to handing uncertainty. Uncertainty usually comes in several
    favors: first, there can be noise in the information we are provided with. Secondly,
    the information can be incomplete. The system may have some degree of freedom
    in filling the missing pieces, which results in uncertainty. Finally, there may
    be variations in the interpretation of the models and the resulting metrics. The
    final point is subtle, as most classic textbooks assume that we can measure things
    directly. Not only the measurements may be noisy, but the definition of the measure
    may change in time—try measuring satisfaction or happiness. Certainly, we can
    avoid the ambiguity by saying that we can optimize only measurable metrics, as
    people usually do, but it will significantly limit the application domain in practice.
    Nothing prevents the scientific machinery from handling the uncertainty in the
    interpretation into account as well.'
  prefs: []
  type: TYPE_NORMAL
- en: The predictive models are often built just for data understanding. From the
    linguistic derivation, model is a simplified representation of the actual complex
    buildings or processes for exactly the purpose of making a point and convincing
    people, one or another way. The ultimate goal for predictive modeling, the modeling
    I am concerned about in this book and this chapter specifically, is to optimize
    the business processes by taking the most important factors into account in order
    to make the world a better place. This was certainly a sentence with a lot of
    uncertainty entrenched, but at least it looks like a much better goal than optimizing
    a click-through rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a traditional business decision-making process: a traditional
    business might involve a set of C-level executives making decisions based on information
    that is usually obtained from a set of dashboards with graphical representation
    of the data in one or several DBs. The promise of an automated data-driven business
    is to be able to automatically make most of the decisions provided the uncertainties
    eliminating human bias. This is not to say that we no longer need C-level executives,
    but the C-level executives will be busy helping the machines to make the decisions
    instead of the other way around.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Going through the basics of influence diagrams as a tool for decision making
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at variations of the pure decision making optimization in the context
    of adaptive **Markov Decision** making process and **Kelly Criterion**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting familiar with at least three different practical strategies for exploration-exploitation
    trade-off
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describing the architecture of a data-driven enterprise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing major architectural components of a decision-making pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting familiar with standard tools for building data pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Influence diagrams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the decision making process can have multiple facets, a book about decision
    making under uncertainty would be incomplete without mentioning influence diagrams
    (*Influence Diagrams for Team Decision Analysis*, Decision Analysis 2 (4): 207–228),
    which help the analysis and understanding of the decision-making process. The
    decision may be as mundane as selection of the next news article to show to a
    user in a personalized environment or a complex one as detecting malware on an
    enterprise network or selecting the next research project.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the weather she can try and go on a boat trip. We can represent
    the decision-making process as a diagram. Let''s decide whether to take a river
    boat tour during her stay in Portland, Oregon:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Influence diagrams](img/B04935_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 02-1\. A simple vacation influence diagram to represent a simple decision-making
    process. The diagram contains decision nodes such as Vacation Activity, observable
    and unobservable information nodes such as Weather Forecast and Weather, and finally
    the value node such as Satisfaction
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram represents this situation. The decision whether to participate
    in the activity is clearly driven by the potential to get certain satisfaction,
    which is a function of the decision itself and the weather at the time of the
    activity. While the actual weather conditions are unknown at the time of the trip
    planning, we believe there is a certain correlation between the weather forecast
    and the actual weather experienced during the trip, which is represented by the
    edge between the **Weather** and **Weather Forecast** nodes. The **Vacation Activity**
    node is the decision node, it has only one parent as the decision is made solely
    based on **Weather Forecast**. The final node in the DAG is **Satisfaction**,
    which is a function of the actual whether and the decision we made during the
    trip planning—obviously, *yes + good weather* and *no + bad weather* are likely
    to have the highest scores. The *yes + bad weather* and *no + good weather* would
    be a bad outcome—the latter case is probably just a missed opportunity, but not
    necessarily a bad decision, provided an inaccurate weather forecast.
  prefs: []
  type: TYPE_NORMAL
- en: The absence of an edge carries an independence assumption. For example, we believe
    that **Satisfaction** should not depend on **Weather Forecast**, as the latter
    becomes irrelevant once we are on the boat. Once the vacation plan is finalized,
    the actual weather during the boating activity can no longer affect the decision,
    which was made solely based on the weather forecast; at least in our simplified
    model, where we exclude the option of buying a trip insurance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph shows different stages of decision making and the flow of information
    (we will provide an actual graph implementation in Scala in [Chapter 7](ch07.xhtml
    "Chapter 7. Working with Graph Algorithms"), *Working with Graph Algorithms*).
    There is only one piece of information required to make the decision in our simplified
    diagram: the weather forecast. Once the decision is made, we can no longer change
    it, even if we have information about the actual weather at the time of the trip.
    The weather and the decision data can be used to model her satisfaction with the
    decision she has made.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s map this approach to an advertising problem as an illustration: the
    ultimate goal is to get user satisfaction with the targeted ads, which results
    in additional revenue for an advertiser. The satisfaction is the function of user-specific
    environmental state, which is unknown at the time of decision making. Using machine
    learning algorithms, however, we can forecast this state based on the user''s
    recent Web visit history and other information that we can gather, such as geolocation,
    browser-agent string, time of day, category of the ad, and so on (refer to *Figure
    02-2*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While we are unlikely to measure the level of dopamine in the user''s brain,
    which will certainly fall under the realm of measurable metrics and probably reduce
    the uncertainty, we can measure the user satisfaction indirectly by the user''s
    actions, either the fact that they responded to the ad or even the measure of
    time the user spent between the clicks to browse relevant information, which can
    be used to estimate the effectiveness of our modeling and algorithms. Here is
    an influence diagram, similar to the one for "vacation", adjusted for the advertising
    decision-making process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Influence diagrams](img/B04935_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 02-2\. The vacation influence diagram adjusted to the online advertising
    decision-making case. The decisions for online advertising can be made thousand
    times per second
  prefs: []
  type: TYPE_NORMAL
- en: The actual process might be more complex, representing a chain of decisions,
    each one depending on a few previous time slices. For example, the so-called **Markov
    Chain Decision Process**. In this case, the diagram might be repeated over multiple
    time slices.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another example might be Enterprise Network Internet malware analytics system.
    In this case, we try to detect network connections indicative of either **command
    and control** (**C2**), lateral movement, or data exfiltration based on the analysis
    of network packets flowing through the enterprise switches. The goal is to minimize
    the potential impact of an outbreak with minimum impact on the functioning systems.
  prefs: []
  type: TYPE_NORMAL
- en: One of the decisions we might take is to reimage a subset of nodes or to at
    least isolate them. The data we collect may contain uncertainty—many benign software
    packages may send traffic in suspicious ways, and the models need to differentiate
    between them based on the risk and potential impact. One of the decisions in this
    specific case may be to collect additional information.
  prefs: []
  type: TYPE_NORMAL
- en: I will leave it to the reader to map this and other potential business cases
    to the corresponding diagram as an exercise. Let's consider a more complex optimization
    problem now.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential trials and dealing with risk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What if my preferences for making an extra few dollars outweigh the risk of
    losing the same amount? I will stop on why one''s preferences might be asymmetric
    in a little while in this section, and there is scientific evidence that this
    asymmetry is ingrained in our minds for evolutionary reasons, but you are right,
    I have to optimize the expected value of the asymmetric function of the parameterized
    utility now, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/B04935_02_01F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Why would an asymmetric function surface in the analysis? One example is repeated
    bets or re-investments, also known as the Kelly Criterion problem. Although originally,
    the Kelly Criterion was developed for a specific case of binary outcome as in
    a gambling machine and the optimization of the fraction of money to bet in each
    round (*A New Interpretation of Information Rate*, Bell System Technical Journal
    35 (4): 917–926, 1956), a more generic formulation as an re-investment problem
    involves a probabilistic distribution of possible returns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The return over multiple bets is a product of individual return rates on each
    of the bets—the return rate is the ratio between the bankroll after the bet to
    the original bankroll before each individual bet, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/B04935_02_02F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This does not help us much to optimize the total return as we don''t know how
    to optimize the product of *i.i.d*. random variables. However, we can convert
    the product to a sum using log transformation and apply the **central limit theorem**
    (**CLT**) to approximate the sum of *i.i.d*. variables (provided that the distribution
    of *r* *[i]* is subect to CLT conditions, for example, has a finite mean and variance),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/B04935_02_03F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the cumulative result of making *N* bets would look like the result of
    making *N* bets with expected return of ![Sequential trials and dealing with risk](img/B04935_02_04F.jpg),
    and not ![Sequential trials and dealing with risk](img/B04935_02_05F.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'As I mentioned before, the problem is most often applied for the case of binary
    bidding, although it can be easily generalized, in which case there is an additional
    parameter: *x*, the amount of money to bid in each round. Let''s say I make a
    profit of *W* with probability *p* or completely lose my bet otherwise with the
    probability *(1-p)*. Optimizing the expected return with respect to the following
    additional parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/B04935_02_06F.jpg)![Sequential
    trials and dealing with risk](img/B04935_02_07F.jpg)![Sequential trials and dealing
    with risk](img/B04935_02_08F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The last equation is the Kelly Criterion ratio and gives you the optimal amount
    to bet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason that one might bet less than the total amount is that even if the
    average return is positive, there is still a possibility to lose the whole bankroll,
    particularly, in highly skewed situations. For example, even if the probability
    of making *10 x* on your bet is *0.105* (*W = 10*, the expected return is *5%)*,
    the combinatorial analysis show that even after *60* bets, there is roughly a
    *50%* chance that the overall return will be negative, and there is an *11%* chance,
    in particular, of losing *(57 - 10 x 3) = 27* times your bet or more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that to recover the *27 x* amount, one would need to play only ![Sequential
    trials and dealing with risk](img/B04935_02_09F.jpg) additional rounds on average
    with these favourable odds, but one must have something to bet to start with.
    The Kelly Criterion provides that the optimal is to bet only *1.55%* of our bankroll.
    Note that if I bet the whole bankroll, I would lose all my money with 89.5% certainty
    in the first round (the probability of a win is only *0.105*). If I bet only a
    fraction of the bankroll, the chances of staying in the game are infinitely better,
    but the overall returns are smaller. The plot of expected log of return is shown
    in *Figure 02-3* as a function of the portions of the bankroll to bet, *x*, and
    possible distribution of outcomes in 60 bets that I just computed. In 24% of the
    games we''ll do worse than the lower curve, in 39% worse than the next curve,
    in about half—44%—a gambler we''ll do the same or better than the black curve
    in the middle, and in 30% of cases better than the top one. The optimal Kelly
    Criterion value for *x* is *0.0155*, which will eventually optimize the overall
    return over infinitely many rounds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/B04935_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 02-3\. The expected log of return as a function of the bet amount and
    possible outcomes in 60 rounds (see equation (2.2))
  prefs: []
  type: TYPE_NORMAL
- en: The Kelly Criterion has been criticized for being both too aggressive (gamblers
    tend to overestimate their winning potential/ratio and underestimate the probability
    of a ruin), as well as for being too conservative (the value at risk should be
    the total available capital, not just the bankroll), but it demonstrates one of
    the examples where we need to compensate our intuitive understanding of the "benefit"
    with some additional transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the financial point of view, the Kelly Criterion is a much better description
    of risk than the standard definition as volatility or variance of the returns.
    For a generic parametrized payoff distribution, *y(z)*, with a probability distribution
    function, *f(z)*, the equation (2.3) can be reformulated as follows. after the
    substitution *r(x) = 1 + x y(z)*, where *x* is still the amount to bet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/B04935_02_10F.jpg)![Sequential
    trials and dealing with risk](img/B04935_02_11F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It can also be written in the following manner in the discrete case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sequential trials and dealing with risk](img/B04935_02_12F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the denominator emphasizes the contributions from the regions with negative
    payoffs. Specifically, the possibility of losing all your bankroll is exactly
    where the denominator ![Sequential trials and dealing with risk](img/B04935_02_13F.jpg)
    is zero.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned before, interestingly, risk aversion is engrained in our intuitions
    and there seems to be a natural risk-aversion system of preferences encoded in
    both humans and primates (*A Monkey Economy as Irrational as Ours* by Laurie Santos,
    TED talk, 2010). Now enough about monkeys and risk, let's get into another rather
    controversial subject—the exploration-exploitation trade-off, where one might
    not even know the payoff trade-offs initially.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration and exploitation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The exploration-exploitation trade-off is another problem that has its apparent
    origin within gambling, even though the real applications range from allocation
    of funding to research projects to self-driving cars. The traditional formulation
    is a multi-armed bandit problem, which refers to an imaginary slot machine with
    one or more arms. Sequential plays of each arm generate *i.i.d* `.` returns with
    unknown probabilities for each arm; the successive plays are independent in the
    simplified models. The rewards are assumed to be independent across the arms.
    The goal is to maximize the reward—for example, the amount of money won, and to
    minimize the learning loss, or the amount spend on the arms with less than optimal
    winning rate, provided an agreed upon arm selection policy. The obvious trade-off
    is between the **exploration** in search of an arm that produces the best return
    and **exploitation** of the best-known arm with optimal return:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploration and exploitation](img/B04935_02_14F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The **pseudo-regret** is then the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploration and exploitation](img/B04935_02_15F.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Exploration and exploitation](img/B04935_02_16F.jpg) is the *i^(th)*
    arm selection out of *N* trials. The multi-armed bandit problem was extensively
    studied in the 1930s and again during the early 2000s, with the application in
    finance and ADTECH. While in general, due to stochastic nature of the problem,
    it is not possible to provide a bound on the expected regret better than the square
    root of *N*, the pseudo-regret can be controlled so that we are able to bound
    it by a log of *N* (*Regret Analysis of Stochastic and Nonstochastic Multi-armed
    Bandit Problems* by Sebastien Bubeck and Nicolo Cesa-Bianchi, [http://arxiv.org/pdf/1204.5721.pdf](http://arxiv.org/pdf/1204.5721.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common strategies used in practice is epsilon strategies, where
    the optimal arm is chosen with the probability of ![Exploration and exploitation](img/B04935_02_17F.jpg)
    and one of the other arms with the remaining probability. The drawback of this
    approach is that we might spend a lot of exploration resources on the arms that
    are never going to provide any rewards. The UCB strategy improves the epsilon
    strategy by choosing an arm with the largest estimate of the return, plus some
    multiple or fraction of the standard deviation of the return estimates. The approach
    needs the recomputation of the best arm to pull at each round and suffers from
    approximations made to estimate the mean and standard deviation. Besides, UCB
    requires the recomputation of the estimates for each successive pull, which might
    be a scalability problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the Thompson sampling strategy uses a fixed random sample from Beta-Bernoulli
    posterior estimates and assigns the next arm to the one that gives the minimal
    expected regret, for which real data can be used to avoid parameter recomputation.
    Although the specific numbers may depend on the assumptions, one available comparison
    for these model performances is provided in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploration and exploitation](img/B04935_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 02-3\. The simulation results for different exploration exploitation
    strategies for K = 5, one-armed bandits, and different strategies.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 02-3* shows simulation results for different strategies (taken from
    the Rich Relevance website at [http://engineering.richrelevance.com/recommendations-thompson-sampling](http://engineering.richrelevance.com/recommendations-thompson-sampling)).
    The **Random** strategy just allocates the arms at random and corresponds to pure
    exploration. The **Naive** strategy is random up to a certain threshold and than
    switches to pure Exxploitation mode. **Upper Confidence Bound** (**UCB**) with
    95% confidence level. UCB1 is a modification of UCB to take into account the log-normality
    of the distributions. Finally the Thompson sampling strategy makes a random sample
    from actual posterior distribution to optimize the regret.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploration/exploitation models are known to be very sensitive to the initial
    conditions and outliers, particularly on the low-response side. One can spend
    enormous amount of trials on the arms that are essentially dead.
  prefs: []
  type: TYPE_NORMAL
- en: Other improvements on the strategies are possible by estimating better priors
    based on additional information, such as location, or limiting the set of arms
    to explore—*K*—due to such additional information, but these aspects are more
    domain-specific (such as personalization or online advertising).
  prefs: []
  type: TYPE_NORMAL
- en: Unknown unknowns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unknown unknowns have been largely made famous due to a phrase from a response
    the United States Secretary of Defense, Donald Rumsfeld, gave to a question at
    a United States **Department of Defense** (**DoD**) news briefing on February
    12, 2002 about the lack of evidence linking the government of Iraq with the supply
    of weapons of mass destruction to terrorist groups, and books by Nassim Taleb
    (*The Black Swan: The Impact of the Highly Improbable* by Nassim Taleb, Random
    House, 2007).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Turkey paradox**'
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, the unknown unknown is better explained by the turkey paradox. Suppose
    you have a family of turkeys playing in the backyard and enjoying protection and
    free food. Across the fence, there is another family of turkeys. This all works
    day after day, and month after month, until Thanksgiving comes—Thanksgiving Day
    is a national holiday celebrated in Canada and the United States, where it's customary
    to roast the turkeys in an oven. The turkeys are very likely to be harvested and
    consumed at this point, although from the turkey's point of view, there is no
    discernable signal that anything will happen on the second Monday of October in
    Canada and the fourth Thursday of November in the United States. No amount of
    modeling on the within-the-year data can fix this prediction problem from the
    turkey's point of view besides the additional year-over-year information.
  prefs: []
  type: TYPE_NORMAL
- en: The unknown unknown is something that is not in the model and cannot be anticipated
    to be in the model. In reality, the only unknown unknowns that are of interest
    are the ones that affect the model so significantly that the results that were
    previously virtually impossible, or possible with infinitesimal probability, now
    become the reality. Given that most of the practical distributions are from exponential
    family with really thin tails, the deviation from normal does not have to be more
    than a few sigmas to have devastating results on the standard model assumptions.
    While one has still to come up with an actionable strategy of how to include the
    unknown factors in the model—a few ways have been proposed, including fractals,
    but few if any are actionable—the practitioners have to be aware of the risks,
    and here the definition of the risk is exactly the possibility of delivering the
    models useless. Of course, the difference between the known unknown and unknown
    unknown is exactly that we understand the risks and what needs to be explored.
  prefs: []
  type: TYPE_NORMAL
- en: As we looked at the basic scope of problems that the decision-making systems
    are facing, let's look at the data pipelines, the software systems that provide
    information for making the decisions, and more practical aspects of designing
    the data pipeline for a data-driven system.
  prefs: []
  type: TYPE_NORMAL
- en: Basic components of a data-driven system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In short, a data-driven architecture contains the following components—at least
    all the systems I''ve seen have them—or can be reduced to these components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingest**: We need to collect the data from systems and devices. Most
    of the systems have logs, or at least an option to write files into a local filesystem.
    Some can have capabilities to report information to network-based interfaces such
    as syslog, but the absence of persistence layer usually means potential data loss,
    if not absence of audit information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation layer**: It was also historically called **extract, transform,
    and load** (**ETL**). Today the data transformation layer can also be used to
    have real-time processing, where the aggregates are computed on the most recent
    data. The data transformation layer is also traditionally used to reformat and
    index the data to be efficiently accessed by a UI component of algorithms down
    the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data analytics and machine learning engine**: The reason this is not part
    of the standard data transformation layer is usually that this layer requires
    quite different skills. The mindset of people who build reasonable statistical
    models is usually different from people who make terabytes of data move fast,
    even though occasionally I can find people with both skills. Usually, these unicorns
    are called data scientists, but the skills in any specific field are usually inferior
    to ones who specialize in a particular field. We need more of either, though.
    Another reason is that machine learning, and to a certain extent, data analysis,
    requires multiple aggregations and passes over the same data, which as opposed
    to a more stream-like ETL transformations, requires a different engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UI component**: Yes, UI stands for user interface, which most often is a
    set of components that allow you to communicate with the system via a browser
    (it used to be a native GUI, but these days the web-based JavaScript or Scala-based
    frameworks are much more powerful and portable). From the data pipeline and modeling
    perspective, this component offers an API to access internal representation of
    data and models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions engine**: This is usually a configurable rules engine to optimize
    the provided metrics based on insights. The actions may be either real-time, like
    in online advertising, in which case the engine should be able to supply real-time
    scoring information, or a recommendation for a user action, which can take the
    form of an e-mail alert.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlation engine**: This is an emerging component that may analyze the
    output of data analysis and machine learning engine to infer additional insights
    into data or model behavior. The actions might also be triggered by an output
    from this layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**: This is a complex system will be incomplete without logging,
    monitoring, and some way to change system parameters. The purpose of monitoring
    is to have a nested decision-making system regarding the optimal health of the
    system and either to mitigate the problem(*s*) automatically or to alert the system
    administrators about the problem(*s*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's discuss each of the components in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the proliferation of smart devices, information gathering has become less
    of a problem and more of a necessity for any business that does more than a type-written
    text. For the purpose of this chapter, I will assume that the device or devices
    are connected to the Internet or have some way of passing this information via
    home dialing or direct network connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major purpose of this component is to collect all relevant information
    that can be relevant for further data-driven decision making. The following table
    provides details on the most common implementations of the data ingest:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | When used | Comments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Syslog** | Syslog is one of the most common standards to pass messages
    between the machines on Unix. Syslog usually listens on port 514 and the transport
    protocol can be configured either with UDP (unreliable) or with TCP. The latest
    enhanced implementation on CentOS and Red Hat Linux is rsyslog, which includes
    many advanced options such as regex-based filtering that is useful for system-performance
    tuning and debugging. Apart from slightly inefficient raw message representation—plain
    text, which might be inefficient for long messages with repeated strings—the syslog
    system can support tens of thousands of messages per second. | Syslog is one of
    the oldest protocols developed in the 1980s by Eric Allman as part of Sendmail.
    While it does not guarantee delivery or durability, particularly for distributed
    systems, it is one of the most widespread protocols for message passing. Some
    of the later frameworks, such as Flume and Kafka, have syslog interfaces as well.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Rsync** | Rsync is a younger framework developed in the 1990s. If the data
    is put in the flat files on a local filesystem, rsync might be an option. While
    rsync is more traditionally used to synchronize two directories, it also can be
    run periodically to transfer log data in batches. Rsync uses a recursive algorithm
    invented by an Australian computer programmer, Andrew Tridgell, for efficiently
    detecting the differences and transmitting a structure (such as a file) across
    a communication link when the receiving computer already has a similar, but not
    identical, version of the same structure. While it incurs extra communication,
    it is better from the point of durability, as the original copy can always be
    retrieved. It is particularly appropriate if the log data is known to arrive in
    batches in the first place (such as uploads or downloads). | Rsync has been known
    to be hampered by network bottlenecks, as it ultimately passes more information
    over the network when comparing the directory structures. However, the transferred
    files may be compressed when passed over the network. The network bandwidth can
    be limited per command-line flags. |'
  prefs: []
  type: TYPE_TB
- en: '| **Flume** | Flume is one of the youngest frameworks developed by Cloudera
    in 2009-2011 and open sourced. Flume—we refer to the more popular flume-ng implementation
    as Flume as opposed to an older regular Flume—consists of sources, pipes, and
    sinks that may be configured on multiple nodes for high availability and redundancy
    purposes. Flume was designed to err on the reliability side at the expense of
    possible duplication of data. Flume passes the messages in the **Avro** format,
    which is also open sourced and the transfer protocol, as well as messages can
    be encoded and compressed. | While Flume originally was developed just to ship
    records from a file or a set of files, it can also be configured to listen to
    a port, or even grab the records from a database. Flume has multiple adapters
    including the preceding syslog. |'
  prefs: []
  type: TYPE_TB
- en: '| **Kafka** | Kafka is the latest addition to the log-processing framework
    developed by LinkedIn and is open sourced. Kafka, compared to the previous frameworks,
    is more like a distributed reliable message queue. Kafka keeps a partitioned,
    potentially between multiple distributed machines; buffer and one can subscribe
    to or unsubscribe from getting messages for a particular topic. Kafka was built
    with strong reliability guarantees in mind, which is achieved through replication
    and consensus protocol. | Kafka might not be appropriate for small systems (<
    five nodes) as the benefits of the fully distributed system might be evident only
    at larger scales. Kafka is commercially supported by Confluent. |'
  prefs: []
  type: TYPE_TB
- en: The transfer of information usually occurs in batches, or micro batches if the
    requirements are close to real time. Usually the information first ends up in
    a file, traditionally called log, in a device's local filesystem, and then is
    transferred to a central location. Recently developed Kafka and Flume are often
    used to manage these transfers, together with a more traditional syslog, rsync,
    or netcat. Finally, the data can be placed into a local or distributed storage
    such as HDFS, Cassandra, or Amazon S3.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After the data ends up in HDFS or other storage, the data needs to be made
    available for processing. Traditionally, the data is processed on a schedule and
    ends up partitioned by time-based buckets. The processing can happen daily or
    hourly, or even on a sub-minute basis with the new Scala streaming framework,
    depending on the latency requirements. The processing may involve some preliminary
    feature construction or vectorization, even though it is traditionally considered
    a machine-learning task. The following table summarizes some available frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | When used | Comments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Oozie** | This is one of the oldest open source frameworks developed by
    Yahoo. This has good integration with big data Hadoop tools. It has limited UI
    that lists the job history. | The whole workflow is put into one big XML file,
    which might be considered a disadvantage from the modularity point of view. |'
  prefs: []
  type: TYPE_TB
- en: '| **Azkaban** | This is an alternative open source workflow-scheduling framework
    developed by LinkedIn. Compared to Oozie, this arguably has a better UI. The disadvantage
    is that all high-level tasks are executed locally, which might present a scalability
    problem. | The idea behind Azkaban is to create a fully modularized drop-in architecture
    where the new jobs/tasks can be added with as few modifications as possible. |'
  prefs: []
  type: TYPE_TB
- en: '| **StreamSets** | StreamSets is the latest addition build by the former Informix
    and Cloudera developers. It has a very developed UI and supports a much richer
    set of input sources and output destinations. | This is a fully UI-driven tool
    with an emphasis on data curation, for example, constantly monitoring the data
    stream for problems and abnormalities. |'
  prefs: []
  type: TYPE_TB
- en: Separate attention should be given to stream-processing frameworks, where the
    latency requirements are reduced to one or a few records at a time. First, stream
    processing usually requires much more resources dedicated to processing, as it
    is more expensive to process individual records at a time as opposed to batches
    of records, even if it is tens or hundreds of records. So, the architect needs
    to justify the additional costs based on the value of more recent result, which
    is not always warranted. Second, stream processing requires a few adjustments
    to the architecture as handling the more recent data becomes a priority; for example,
    a delta architecture where the more recent data is handled by a separate substream
    or a set of nodes became very popular recently with systems such as **Druid**
    ([http://druid.io](http://druid.io)).
  prefs: []
  type: TYPE_NORMAL
- en: Data analytics and machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the purpose of this chapter, **Machine Learning** (**ML**) is any algorithm
    that can compute aggregates or summaries that are actionable. We will cover more
    complex algorithms from [Chapter 3](ch03.xhtml "Chapter 3. Working with Spark
    and MLlib"), *Working with Spark and MLlib* to [Chapter 6](ch06.xhtml "Chapter 6. Working
    with Unstructured Data"), *Working with Unstructured Data*, but in some cases,
    a simple sliding-window average and deviation from the average may be sufficient
    signal for taking an action. In the past few years, it just works in A/B testing
    somehow became a convincing argument for model building and deployment. I am not
    speculating that solid scientific principles might or might not apply, but many
    fundamental assumptions such as *i.i.d.*, balanced designs, and the thinness of
    the tail just fail to hold for many big data situation. Simpler models tend to
    be faster and to have better performance and stability.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in online advertising, one might just track average performance
    of a set of ads over a certain similar properties over times to make a decision
    whether to have this ad displayed. The information about anomalies, or deviation
    from the previous behavior, may be a signal a new unknown unknown, which signals
    that the old data no longer applies, in which case, the system has no choice but
    to start the new exploration cycle.
  prefs: []
  type: TYPE_NORMAL
- en: I will talk about more complex non-structured, graph, and pattern mining later
    in [Chapter 6](ch06.xhtml "Chapter 6. Working with Unstructured Data"), *Working
    with Unstructured Data*, [Chapter 8](ch08.xhtml "Chapter 8. Integrating Scala
    with R and Python"), *Integrating Scala with R and Python* and [Chapter 9](ch09.xhtml
    "Chapter 9. NLP in Scala"), *NLP in Scala*.
  prefs: []
  type: TYPE_NORMAL
- en: UI component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Well, UI is for wimps! Just joking...maybe it's too harsh, but in reality, UI
    usually presents a syntactic sugar that is necessary to convince the population
    beyond the data scientists. A good analyst should probably be able to figure out
    t-test probabilities by just looking at a table with numbers.
  prefs: []
  type: TYPE_NORMAL
- en: However, one should probably apply the same methodologies we used at the beginning
    of the chapter, assessing the usefulness of different components and the amount
    of cycles put into them. The presence of a good UI is often justified, but depends
    on the target audience.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, there are a number of existing UIs and reporting frameworks. Unfortunately,
    most of them are not aligned with the functional programming methodologies. Also,
    the presence of complex/semi-structured data, which I will describe in [Chapter
    6](ch06.xhtml "Chapter 6. Working with Unstructured Data"), *Working with Unstructured
    Data* in more detail, presents a new twist that many frameworks are not ready
    to deal with without implementing some kind of DSL. Here are a few frameworks
    for building the UI in a Scala project that I find particularly worthwhile:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | When used | Comments |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Scala Swing** | If you used Swing components in Java and are proficient
    with them, Scala Swing is a good choice for you. Swing component is arguably the
    least portable component of Java, so your mileage can vary on different platforms.
    | The `Scala.swing` package uses the standard Java Swing library under the hood,
    but it has some nice additions. Most notably, as it''s made for Scala, it can
    be used in a much more concise way than the standard Swing. |'
  prefs: []
  type: TYPE_TB
- en: '| **Lift** | Lift is a secure, developer-centric, scalable, and interactive
    framework written in Scala. Lift is open sourced under Apache 2.0 license. | The
    open source Lift framework was launched in 2007 by David Polak, who was dissatisfied
    with certain aspects of the Ruby on Rails framework. Any existing Java library
    and web container can be used in running Lift applications. Lift web applications
    are thus packaged as WAR files and deployed on any servlet 2.4 engine (for example,
    Tomcat 5.5.xx, Jetty 6.0, and so on). Lift programmers may use the standard Scala/Java
    development toolchain, including IDEs such as Eclipse, NetBeans, and IDEA. Dynamic
    web content is authored via templates using standard HTML5 or XHTML editors. Lift
    applications also benefit from native support for advanced web development techniques,
    such as Comet and Ajax. |'
  prefs: []
  type: TYPE_TB
- en: '| **Play** | Play is arguably better aligned with Scala as a functional language
    than any other platform—it is officially supported by Typesafe, the commercial
    company behind Scala. The Play framework 2.0 builds on Scala, Akka, and sbt to
    deliver superior asynchronous request handling, fast and reliable. Typesafe templates,
    and a powerful build system with flexible deployment options. Play is open sourced
    under Apache 2.0 license. | The open source Play framework was created in 2007
    by Guillaume Bort, who sought to bring a fresh web development experience inspired
    by modern web frameworks like Ruby on Rails to the long-suffering Java web development
    community. Play follows a familiar stateless model-view-controller architectural
    pattern, with a philosophy of convention-over-configuration and an emphasis on
    developer productivity. Unlike traditional Java web frameworks with their tedious
    compile-package-deploy-restart cycles, updates to Play applications are instantly
    visible with a simple browser refresh. |'
  prefs: []
  type: TYPE_TB
- en: '| **Dropwizard** | The dropwizard ([www.dropwizard.io](http://www.dropwizard.io))
    project is an attempt to build a generic RESTful framework in both Java and Scala,
    even though one might end up using more Java than Scala. What is nice about this
    framework is that it is flexible enough to be used with arbitrary complex data
    (including semi-structured).This is licensed under Apache License 2.0. | RESTful
    API assumes state, while functional languages shy away from using state. Unless
    you are flexible enough to deviate from a pure functional approach, this framework
    is probably not good enough for you. |'
  prefs: []
  type: TYPE_TB
- en: '| **Slick** | While Slick is not a UI component, it is Typesafe''s modern database
    query and access library for Scala, which can serve as a UI backend. It allows
    you to work with the stored data almost as if you were using Scala collections,
    while at the same time, giving you full control over when a database access occurs
    and what data is transferred. You can also use SQL directly. Use it if all of
    your data is purely relational. This is open sourced under BSD-Style license.
    | Slick was started in 2012 by Stefan Zeiger and maintained mainly by Typesafe.
    It is useful for mostly relational data. |'
  prefs: []
  type: TYPE_TB
- en: '| **NodeJS** | Node.js is a JavaScript runtime, built on Chrome''s V8 JavaScript
    engine. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight
    and efficient. Node.js'' package ecosystem, npm, is the largest ecosystem of open
    source libraries in the world. It is open sourced under MIT License. | Node.js
    was first introduced in 2009 by Ryan Dahl and other developers working at Joyent.
    Originally Node.js supported only Linux, but now it runs on OS X and Windows.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **AngularJS** | AngularJS ([https://angularjs.org](https://angularjs.org))
    is a frontend development framework, built to simplify development of one-page
    web applications. This is open sourced under MIT License. | AngularJS was originally
    developed in 2009 by Misko Hevery at Brat Tech LLC. AngularJS is mainly maintained
    by Google and by a community of individual developers and corporations, and thus
    is specifically for Android platform (support for IE8 is dropped in versions 1.3
    and later). |'
  prefs: []
  type: TYPE_TB
- en: Actions engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this is the heart of the data-oriented system pipeline, it is also arguably
    the easiest one. Once the system of metrics and values is known, the system decides,
    based on the known equations, whether to take a certain set of actions or not,
    based on the information provided. While the triggers based on a threshold is
    the most common implementation, the significance of probabilistic approaches that
    present the user with a set of possibilities and associated probabilities is emerging—or
    just presenting the user with the top *N* relevant choices like a search engine
    does.
  prefs: []
  type: TYPE_NORMAL
- en: The management of the rules might become pretty involved. It used to be that
    managing the rules with a rule engine, such as **Drools** ([http://www.drools.org](http://www.drools.org)),
    was sufficient. However, managing complex rules becomes an issue that often requires
    development of a DSL (*Domain-Specific Languages* by Martin Fowler, Addison-Wesley,
    2010). Scala is particularly fitting language for the development of such an actions
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The more complex the decision-making system is, the more it requires a secondary
    decision-making system to optimize its management. DevOps is turning into DataOps
    (*Getting Data Right* by Michael Stonebraker et al., Tamr, 2015). Data collected
    about the performance of a data-driven system are used to detect anomalies and
    semi-automated maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Models are often subject to time drift, where the performance might deteriorate
    either due to the changes in the data collection layers or the behavioral changes
    in the population (I will cover model drift in [Chapter 10](ch10.xhtml "Chapter 10. Advanced
    Model Monitoring"), *Advanced Model Monitoring*). Another aspect of model management
    is to track model performance, and in some cases, use "collective" intelligence
    of the models by various consensus schemes.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring a system involves collecting information about system performance
    either for audit, diagnostic, or performance-tuning purposes. While it is related
    to the issues raised in the previous sections, monitoring solution often incorporates
    diagnostic and historical storage solutions and persistence of critical data,
    such as a black box on an airplane. In the Java and, thus, Scala world, a popular
    tool of choice is Java performance beans, which can be monitored in the Java Console.
    While Java natively supports MBean for exposing JVM information over JMX, **Kamon**
    ([http://kamon.io](http://kamon.io)) is an open source library that uses this
    mechanism to specifically expose Scala and Akka metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Some other popular monitoring open source solutions are **Ganglia** ([http://ganglia.sourceforge.net/](http://ganglia.sourceforge.net/))
    and **Graphite** ([http://graphite.wikidot.com](http://graphite.wikidot.com)).
  prefs: []
  type: TYPE_NORMAL
- en: I will stop here, as I will address system and model monitoring in more detail
    in [Chapter 10](ch10.xhtml "Chapter 10. Advanced Model Monitoring"), *Advanced
    Model Monitoring*.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization and interactivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While the data collected can be just used for understanding the business, the
    final goal of any data-driven business is to optimize the business behavior by
    automatically making data-based and model-based decisions. We want to reduce human
    intervention to minimum. The following simplified diagram can be depicted as a
    cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optimization and interactivity](img/B04935_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 02-4\. The predictive model life cycle
  prefs: []
  type: TYPE_NORMAL
- en: The cycle is repeated over and over for new information coming into the system.
    The parameters of the system may be tuned to improve the overall system performance.
  prefs: []
  type: TYPE_NORMAL
- en: Feedback loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While humans are still likely to be kept in the loop for most of the systems,
    last few years saw an emergence of systems that can manage the complete feedback
    loop on their own—ranging from advertisement systems to self-driving cars.
  prefs: []
  type: TYPE_NORMAL
- en: The classical formulation of this problem is the optimal control theory, which
    is also an optimization problem to minimize cost functional, given a set of differential
    equations describing the system. An optimal control is a set of control policies
    to minimize the cost functional given constraints. For example, the problem might
    be to find a way to drive the car to minimize its fuel consumption, given that
    it must complete a given course in a time not exceeding some amount. Another control
    problem is to maximize profit for showing ads on a website, provided the inventory
    and time constraints. Most software packages for optimal control are written in
    other languages such as C or MATLAB (PROPT, SNOPT, RIOTS, DIDO, DIRECT, and GPOPS),
    but can be interfaced with Scala.
  prefs: []
  type: TYPE_NORMAL
- en: However, in many cases, the parameters for the optimization or the state transition,
    or differential equations, are not known with certainty. **Markov Decision Processes**
    (**MDPs**) provide a mathematical framework to model decision making in situations
    where outcomes are partly random and partly under the control of the decision
    maker. In MDPs, we deal with a discrete set of possible states and a set of actions.
    The "rewards" and state transitions depend both on the state and actions. MDPs
    are useful for studying a wide range of optimization problems solved via dynamic
    programming and reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I described a high-level architecture and approach to design
    a data-driven enterprise. I also introduced you to influence diagrams, a tool
    for understanding how the decisions are made in traditional and data-driven enterprises.
    I stopped on a few key models, such as Kelly Criterion and multi-armed bandit,
    essential to demonstrate the issues from the mathematical point of view. I built
    on top of this to introduce some Markov decision process approaches where we deal
    with decision policies based on the results of the previous decisions and observations.
    I delved into more practical aspects of building a data pipeline for decision-making,
    describing major components and frameworks that can be used to built them. I also
    discussed the issues of communicating the data and modeling results between different
    stages and nodes, presenting the results to the user, feedback loop, and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, I will describe MLlib, a library for machine learning over
    distributed set of nodes written in Scala.
  prefs: []
  type: TYPE_NORMAL
