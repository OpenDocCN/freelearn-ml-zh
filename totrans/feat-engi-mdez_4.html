<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Feature Construction</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we worked with the <span><kbd>Pima Indian Diabetes Prediction</kbd> dataset to get a better understanding of which</span> given featu<span>res in our dataset are most valuable. Working with the features that were available to us, we identified missing values within our columns and employed techniques of dropping missing values, imputing, and normalizing/standardizing our data to improve the accuracy of our machine learning model.</span></p>
<p>It is important to note that, up to this point, we have only worked with features that are quantitative. We will now shift into dealing with categorical data, in addition to the quantitative data that has missing values. Our main focus will be to work with our given features to construct entirely new features for our models to learn from. </p>
<p>There are various methods we can utilize to construct our features, with the most basic starting with the pandas library in Python to scale an existing feature by a multiples. We will be diving into some more mathematically intensive methods, and will employ various packages available to us through the scikit-learn library; we will also create our own custom classes. We will go over these classes <span>in detail</span> as we get into the code. </p>
<p>We will be covering the following topics in our discussions:</p>
<ul>
<li>Examining our dataset</li>
<li>Imputing categorical features</li>
<li>Encoding categorical variables</li>
<li>Extending numerical features</li>
<li>Text-specific feature construction</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examining our dataset</h1>
                </header>
            
            <article>
                
<p>For demonstrative purposes, in this chapter, we will utilize a dataset that we have created, so that we can showcase a variety of data levels and types. Let's set up our DataFrame and dive into our data.</p>
<p>We will use pandas to create the DataFrame we will work with, as this is the primary data structure in pandas. The advantage of a pandas DataFrame is that there are several attributes and methods available for us to perform on our data. This allows us to logically manipulate the data to develop a thorough understanding of what we are working with, and how best to structure our machine learning models:</p>
<ol>
<li class="mce-root">First, let's import <kbd>pandas</kbd>:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># import pandas as pd</pre>
<ol start="2">
<li>Now, we can set up our <kbd>DataFrame X</kbd>. To do this, we will utilize the <kbd>DataFrame</kbd> method in pandas, which creates a tabular data structure (table with rows and columns). This method can take in a few types of data (NumPy arrays or dictionaries, to name a couple). Here, we will be passing-in a dictionary with keys as column headers and values as lists, with each list representing a column: </li>
</ol>
<pre style="padding-left: 60px"><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">'city'</span><span class="p">:[</span><span class="s1">'tokyo'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="s1">'london'</span><span class="p">,</span> <span class="s1">'seattle'</span><span class="p">,</span> <span class="s1">'san francisco'</span><span class="p">,</span> <span class="s1">'tokyo'</span><span class="p">],</span> 
                  <span class="s1">'boolean'</span><span class="p">:[</span><span class="s1">'yes'</span><span class="p">,</span> <span class="s1">'no'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="s1">'no'</span><span class="p">,</span> <span class="s1">'no'</span><span class="p">,</span> <span class="s1">'yes'</span><span class="p">],</span> 
                  <span class="s1">'ordinal_column'</span><span class="p">:[</span><span class="s1">'somewhat like'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'somewhat like'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'somewhat like'</span><span class="p">,</span> <span class="s1">'dislike'</span><span class="p">],</span> 
                  <span class="s1">'quantitative_column'</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="o">-.</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="mi">20</span><span class="p">]})</span></pre>
<ol start="3">
<li>This will give us a DataFrame with four columns and six rows. Let's print our DataFrame <kbd>X</kbd> and take a look at the data:</li>
</ol>
<pre style="padding-left: 60px">print X</pre>
<p>We get the output as follows:</p>
<table style="width: 726px;height: 315px">
<tbody>
<tr>
<td>
<p> </p>
</td>
<td>
<p><strong>boolean</strong></p>
</td>
<td>
<p><strong>city</strong></p>
</td>
<td>
<p><strong>ordinal_column</strong></p>
</td>
<td>
<p><strong>quantitative_column</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>1.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>None</p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>11.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>None</p>
</td>
<td>
<p>london</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>-0.5</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>seattle</p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>10.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>san francisco</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>NaN</p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p><span>dislike</span></p>
</td>
<td>
<p>20.0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Let's take a look at our columns and identify our data levels and types:</p>
<ul>
<li><kbd>boolean</kbd>: This column is represented by binary categorical data (yes/no), and is at the nominal level</li>
<li><kbd>city</kbd>: This column is represented by categorical data, also at the nominal level</li>
<li><kbd>ordinal_column</kbd>: As you may have guessed by the column name, this column is represented by ordinal data, at the ordinal level</li>
<li><kbd>quantitative_column</kbd>: This column is represented by integers at the ratio level</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Imputing categorical features</h1>
                </header>
            
            <article>
                
<p>Now that we have an understanding of the data we are working with, let's take a look at our missing values:</p>
<ul>
<li><span>To do this, we can use the <kbd>isnull</kbd> method available to us in pandas for DataFrames. This method returns a <kbd>boolean</kbd> same-sized object indicating if the values are null.</span></li>
<li><span>We will then <kbd>sum</kbd> these to see which columns have missing data:</span></li>
</ul>
<pre style="padding-left: 60px">X.isnull().sum()<br/>&gt;&gt;&gt;&gt;<br/>boolean                1
city                   1
ordinal_column         0
quantitative_column    1
dtype: int64</pre>
<p class="mce-root">Here, we can see that three of our columns are missing values. Our course of action will be to impute these missing values.</p>
<p class="mce-root">If you recall, we implemented scikit-learn's <kbd>Imputer</kbd> class in a previous chapter to fill in numerical data. <kbd>Imputer</kbd> does have a categorical option, <kbd>most_frequent</kbd>, however it only works on categorical data that has been encoded as integers.</p>
<p class="mce-root">We may not always want to transform our categorical data this way, as it can change how we interpret the categorical information, so we will build our own transformer. By transformer, we mean a method by which a column will impute missing values. </p>
<p>In fact, we will build several custom transformers in this chapter, as they are quite useful for making transformations to our data, and give us options that are not readily available in pandas or scikit-learn.</p>
<p>Let's start with our categorical column, <kbd>city</kbd>. <span>Just as we have the strategy of imputing the mean value to fill missing rows for numerical data, we have a similar method for categorical data. T</span>o impute values for categorical data, fill missing rows with the most common category. </p>
<p>To do so, we will need to find out what the most common category is in our <kbd>city</kbd> column:</p>
<div class="packt_infobox">Note that we need to specify the column we are working with to employ a method called <kbd>value_counts</kbd>. This will return an <span>object that will be in descending order so that the first element is the most frequently-occurring element.</span></div>
<p><span>We will grab only the first element in the object: </span></p>
<pre class="mce-root" style="padding-left: 30px"># Let's find out what our most common category is in our city column<br/>X['city'].value_counts().index[0]<br/><br/>&gt;&gt;&gt;&gt;<br/>'tokyo'</pre>
<p>We can see that <kbd>tokyo</kbd> appears to be the most common city. Now that we know which value to use to impute our missing rows, let's fill these slots. There is a <kbd>fillna</kbd> function that allows us to specify exactly how we want to fill missing values:</p>
<pre><span class="n"># fill empty slots with most common category<br/>X</span><span class="p">[</span><span class="s1">'city'</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s1">'city'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">])<br/></span></pre>
<p>The <kbd>city</kbd> column now looks like this:</p>
<pre>0            tokyo
1            tokyo
2           london
3          seattle
4    san francisco
5            tokyo
Name: city, dtype: object</pre>
<p>Great, now our <kbd>city</kbd> column no longer has missing values. However, our other categorical column, <kbd>boolean</kbd>, still does. Rather than going through the same method, let's build a custom imputer that will be able to handle imputing all categorical data. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom imputers</h1>
                </header>
            
            <article>
                
<p> Before we jump into the code, let's have a quick refresher of pipelines:</p>
<ul>
<li>Pipelines allow us to s<span>equentially apply a list of transforms and a final estimator</span></li>
<li><span>Intermediate steps of the pipeline must be <strong>transforms</strong>, meaning they must implement <kbd>fit</kbd> and <kbd>transform</kbd> methods</span></li>
<li><span>The final estimator only needs to implement <kbd>fit</kbd></span></li>
</ul>
<p><span>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. Once we have built our custom transformers for each column that needs imputing, we will pass them all through a pipeline so that our data can be transformed in one go. Let's build our custom category imputer to start. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom category imputer</h1>
                </header>
            
            <article>
                
<p><span>First, we will utilize the scikit-learn <kbd>TransformerMixin</kbd> base class to create our own custom categorical imputer. This transformer (and all other custom transformers in this chapter) will work as an element in a pipeline with a fit and <kbd>transform</kbd> method.</span></p>
<p><span>The following code block will become very familiar throughout this chapter, so we will go over each line in detail:</span></p>
<pre><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">TransformerMixin</span>

<span class="k">class</span> <span class="nc">CustomCategoryImputer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cols</span> <span class="o">=</span> <span class="n">cols</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cols</span><span class="p">:</span>
            <span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span></pre>
<p><span>There is a lot happening in this code block, so let's break it down by line: </span></p>
<ol>
<li>First, we have a new <kbd>import</kbd> statement:</li>
</ol>
<pre style="padding-left: 60px"><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">TransformerMixin</span></pre>
<ol start="2">
<li>We will inherit the <kbd>TransformerMixin</kbd> class from scikit-learn, which includes a <kbd>.fit_transform</kbd> method that calls upon the <kbd>.fit</kbd> and <kbd>.transform</kbd> methods we will create. This allows us to maintain a similar structure in our transformer to that of scikit-learn. Let's initialize our custom class:</li>
</ol>
<pre style="padding-left: 60px"><span class="k">class</span> <span class="nc">CustomCategoryImputer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cols</span> <span class="o">=</span> <span class="n">cols</span></pre>
<ol start="3">
<li>We have now instantiated our custom class and have our <kbd>__init__</kbd> method that initializes our attributes. In our case, we only need to initialize one instance attribute, <kbd>self.cols</kbd> (which will be the columns that we specify as a parameter). Now, we can build our <kbd>fit</kbd> and <kbd>transform</kbd> methods:</li>
</ol>
<pre style="padding-left: 60px"><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cols</span><span class="p">:</span>
            <span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span></pre>
<ol start="4">
<li>Here, we have our <kbd>transform</kbd> method. It takes in a DataFrame, and the first step is to copy and rename the DataFrame to <kbd>X</kbd>. Then, we will iterate over the columns we have specified in our <kbd>cols</kbd> parameter to fill in the missing slots. The <kbd>fillna</kbd> portion may feel familiar, as it is the function we employed in our first example. We are using the same function and setting it up so that our custom categorical imputer can work across several columns at once. After the missing values have been filled, we return our filled DataFrame. Next comes our <kbd>fit</kbd> method: </li>
</ol>
<pre style="padding-left: 60px"><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span></pre>
<p>We have set up our <kbd>fit</kbd> method to simply <kbd>return self</kbd>, as is the standard of <kbd>.fit</kbd> methods in scikit-learn.</p>
<ol start="5">
<li>Now we have a custom method that allows us to impute our categorical data! Let's see it in action with our two categorical columns, <kbd>city</kbd> and <kbd>boolean</kbd>:</li>
</ol>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<pre style="padding-left: 60px"><span class="n"># Implement our custom categorical imputer on our categorical columns.<br/><br/>cci</span> <span class="o">=</span> <span class="n">CustomCategoryImputer</span><span class="p">(</span><span class="n">cols</span><span class="o">=</span><span class="p">[</span><span class="s1">'city'</span><span class="p">,</span> <span class="s1">'boolean'</span><span class="p">])<br/></span></pre>
<ol start="6">
<li>We have initialized our custom categorical imputer, and we now need to <kbd>fit_transform</kbd> this imputer to our dataset:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">cci</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></pre></div>
</div>
</div>
</div>
<p class="mce-root">Our dataset now looks like this:</p>
<table style="width: 726px;height: 315px">
<tbody>
<tr>
<td/>
<td>
<p><strong>boolean</strong></p>
</td>
<td>
<p><strong>city</strong></p>
</td>
<td>
<p><strong>ordinal_column</strong></p>
</td>
<td>
<p><strong>quantitative_column</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>1.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>11.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>london</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>-0.5</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>seattle</p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>10.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>san francisco</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>NaN</p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p><span>dislike</span></p>
</td>
<td>
<p>20.0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p><span>Great! Our</span> <kbd>city</kbd> <span>and</span> <kbd>boolean</kbd> <span>columns are no longer missing values. However, our quantitative column still has null values. Since the default imputer cannot select columns, let's make another custom one. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom quantitative imputer</h1>
                </header>
            
            <article>
                
<p>We will use the same structure as our custom category imputer. The main difference here is that we will utilize scikit-learn's <kbd>Imputer</kbd> class to actually make the transformation on our columns:</p>
<pre><span class="c1"># Lets make an imputer that can apply a strategy to select columns by name</span>
<br/>from sklearn.preprocessing import Imputer<br/><span class="k">class</span> <span class="nc">CustomQuantitativeImputer</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">'mean'</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cols</span> <span class="o">=</span> <span class="n">cols</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">strategy</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">impute</span> <span class="o">=</span> <span class="n">Imputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">strategy</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cols</span><span class="p">:</span>
            <span class="n">X</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">impute</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">[[</span><span class="n">col</span><span class="p">]])</span>
        <span class="k">return</span> <span class="n">X</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span></pre>
<p>For our <kbd>CustomQuantitativeImputer</kbd>, we have added a <kbd>strategy</kbd> parameter that will allow us to specify exactly how we want to impute missing values for our quantitative data. Here, we have selected the <kbd>mean</kbd> to replace missing values and still employ the <kbd>transform</kbd> and <kbd>fit</kbd> methods. </p>
<p>Once again, in order to impute our data, we will call the <kbd>fit_transform</kbd> method, this time specifying both the column and the <kbd>strategy</kbd> to use to impute: </p>
<pre><span class="n">cqi</span> <span class="o">=</span> <span class="n">CustomQuantitativeImputer</span><span class="p">(</span><span class="n">cols</span><span class="o">=</span><span class="p">[</span><span class="s1">'quantitative_column'</span><span class="p">],</span> <span class="n">strategy</span><span class="o">=</span><span class="s1">'mean'</span><span class="p">)</span>

<span class="n">cqi</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></pre>
<p class="mce-root">Alternatively, rather than calling and <kbd>fit_transforming</kbd> our <kbd>CustomCategoryImputer</kbd> and our <kbd>CustomQuantitativeImputer</kbd> separately, we can also set them up in a pipeline so that we can transform our dataset in one go. Let's see how:</p>
<ol>
<li>Start with our <kbd>import</kbd> statement:</li>
</ol>
<pre style="padding-left: 60px"><span class="kn"># import Pipeline from sklearn<br/>from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span></pre>
<ol start="2">
<li>Now, we can pass through our custom imputers: </li>
</ol>
<pre style="padding-left: 60px"><span class="n">imputer</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">'quant'</span><span class="p">,</span> <span class="n">cqi</span><span class="p">),</span> <span class="p">(</span><span class="s1">'category'</span><span class="p">,</span> <span class="n">cci</span><span class="p">)])</span> <span class="n">imputer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></pre>
<p>Let's see what our dataset looks like after our pipeline transformations: </p>
<table style="width: 726px;height: 315px">
<tbody>
<tr>
<td/>
<td>
<p><strong>boolean</strong></p>
</td>
<td>
<p><strong>city</strong></p>
</td>
<td>
<p><strong>ordinal_column</strong></p>
</td>
<td>
<p><strong>quantitative_column</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>1.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>11.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>london</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>-0.5</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>seattle</p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>10.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>san francisco</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>8.3</p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p><span>dislike</span></p>
</td>
<td>
<p>20.0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now we have a dataset with no missing values to work with!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding categorical variables</h1>
                </header>
            
            <article>
                
<p>To recap, thus far we have successfully imputed our dataset—both our categorical and quantitative columns. At this point, you may be wondering, <em>how do we utilize the categorical data with a machine learning algorithm?</em></p>
<p>Simply put, we need to transform this categorical data into numerical data. So far, we have ensured that the most common category was used to fill the missing values. Now that this is done, we need to take it a step further. </p>
<p>Any machine learning algorithm, whether it is a linear-regression or a KNN-utilizing Euclidean distance, requires numerical input features to learn from. There are several methods we can rely on to transform our categorical data into numerical data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding at the nominal level</h1>
                </header>
            
            <article>
                
<p>Let's begin with data at the nominal level. The main method we have is to transform our categorical data into dummy variables. We have two options to do this:</p>
<ul>
<li>Utilize pandas to automatically find the categorical variables and dummy code them</li>
<li>Create our own custom transformer using dummy variables to work in a pipeline </li>
</ul>
<p>Before we delve into these options, let's go over exactly what dummy variables are. </p>
<p>Dummy variables <span>take the value zero or one to indicate the absence or presence of a category. They are proxy variables, or numerical stand-ins, for qualitative data.</span></p>
<p><span>Consider a simple regression analysis for wage determination. Say we are given gender, which is qualitative, and years of education, which is quantitative. In order to see if gender has an effect on wages, we would dummy code when the person is a female to female = 1, and female = 0 when the person is male. </span></p>
<p>When working with dummy variables, it is important to be aware of and avoid the dummy variable trap. The dummy variable trap is when you have independent variables that are multicollinear, or highly correlated. Simply put, these variables can be predicted from each other. So, in our gender example, the dummy variable trap would be if we include both female as (0|1) and male as (0|1), essentially creating a duplicate category. It can be inferred that a 0 female value indicates a male.</p>
<p>To avoid the dummy variable trap, simply leave out the constant term or one of the dummy categories. The left out dummy can become the base category to which the rest are compared to.</p>
<p>Let's come back to our dataset and employ some methods to encode our categorical data into dummy variables. pandas has a handy <kbd>get_dummies</kbd> method that actually finds all of the categorical variables and dummy codes them for us:</p>
<pre><span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> 
               <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'city'</span><span class="p">,</span> <span class="s1">'boolean'</span><span class="p">],</span>  <span class="c1"># which columns to dummify</span>
               <span class="n">prefix_sep</span><span class="o">=</span><span class="s1">'__'</span><span class="p">)</span>  <span class="c1"># the separator between the prefix (column name) and cell value<br/></span></pre>
<p><span>We have to be sure to specify which columns we want to apply this to because it will also dummy code the ordinal columns, and this wouldn't make much sense. We will take a more in-depth look into why dummy coding ordinal data doesn't makes sense shortly.</span></p>
<p>Our data, with our dummy coded columns, now looks like this:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>ordinal_column</strong></p>
</td>
<td>
<p><strong>quantitative_column</strong></p>
</td>
<td>
<p><strong>city__london</strong></p>
</td>
<td>
<p><strong>city_san francisco</strong></p>
</td>
<td>
<p><strong>city_seattle</strong></p>
</td>
<td>
<p><strong>city_tokyo</strong></p>
</td>
<td>
<p><strong>boolean_no</strong></p>
</td>
<td>
<p><strong>boolean_yes</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p><span>1.0</span></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>11.0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>-0.5</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>10.0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>NaN</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p><span>dislike</span></p>
</td>
<td>
<p>20.0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p><span>Our other option for dummy coding our data is to create our own custom dummifier. Creating this allows us to set up a pipeline to transform our whole dataset in one go. </span></p>
<p>Once again, we will use the same structure as our previous two custom imputers. Here, our <kbd>transform</kbd> method will use the handy pandas <kbd>get_dummies</kbd> method to create dummy variables for specified columns. The only parameter we have in this custom dummifier is <kbd>cols</kbd>:</p>
<pre><span class="k"># create our custom dummifier<br/>class</span> <span class="nc">CustomDummifier</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cols</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cols</span> <span class="o">=</span> <span class="n">cols</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cols</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self<br/></span></pre>
<p class="mce-root">Our custom dummifier mimics scikit-learn's <kbd>OneHotEncoding</kbd>, but with the added advantage of working on our entire DataFrame. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding at the ordinal level</h1>
                </header>
            
            <article>
                
<p>Now, let's take a look at our ordinal columns. There is still useful information here, however, we need to transform the strings into numerical data. At the ordinal level, since there is meaning in the data having a specific order, it does not make sense to use dummy variables. To maintain the order, we will use a label encoder.</p>
<p>By a label encoder, we mean that each label in our ordinal data will have a numerical value associated to it. In our example, this means that the ordinal column values (<kbd>dislike</kbd>, <kbd>somewhat like</kbd>, and <kbd>like</kbd>) will be represented as <kbd>0</kbd>, <kbd>1</kbd>, and <kbd>2</kbd>. </p>
<p>In the simplest form, the code is as follows:</p>
<pre><span class="n"># set up a list with our ordinal data corresponding the list index<br/>ordering</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'dislike'</span><span class="p">,</span> <span class="s1">'somewhat like'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">]</span>  <span class="c1"># 0 for dislike, 1 for somewhat like, and 2 for like<br/></span># before we map our ordering to our ordinal column, let's take a look at the column<br/><br/>print X['ordinal_column']<br/>&gt;&gt;&gt;&gt;<br/><span>0 somewhat like <br/>1 like <br/>2 somewhat like <br/>3 like <br/>4 somewhat like <br/>5 dislike <br/>Name: ordinal_column, dtype: object</span></pre>
<p>Here, we have set up a list for ordering our labels. This is key, as we will be utilizing the index of our list to transform the labels to numerical data. </p>
<p>Here, we will implement a function called <kbd>map</kbd> on our column, that allows us to specify the function we want to implement on the column. We specify this function using a construct called <kbd>lambda</kbd>, which essentially allows us to create an anonymous function, or one that is not bound to a name: </p>
<pre class="mce-root">lambda x: ordering.index(x)</pre>
<p>This specific code is creating a function that will apply the index of our list called <kbd>ordering</kbd> to each element. Now, we map this to our ordinal column:</p>
<pre class="mce-root"># now map our ordering to our ordinal column:<br/>print X['ordinal_column'].map(lambda x: ordering.index(x))<br/>&gt;&gt;&gt;&gt;<br/>0    1
1    2
2    1
3    2
4    1
5    0
Name: ordinal_column, dtype: int64</pre>
<p>Our ordinal column is now represented as labeled data. </p>
<p><span>Note that scikit-learn has a <kbd>LabelEncoder</kbd>, but we are not using this method because it does not include the ability to order categories (<kbd>0</kbd> for dislike, <kbd>1</kbd> for somewhat like, <kbd>2</kbd> for like) as we have done previously. Rather, the default is a sorting method, which is not what we want to use here.</span></p>
<p>Once again, let us make a custom label encoder that will fit into our pipeline:</p>
<pre><span class="k">class</span> <span class="nc">CustomEncoder</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">ordering</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ordering</span> <span class="o">=</span> <span class="n">ordering</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">col</span> <span class="o">=</span> <span class="n">col</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ordering</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">X</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span></pre>
<p>We have maintained the structure of the other custom transformers in this chapter. Here, we have utilized the <kbd>map</kbd> and <kbd>lambda</kbd> functions detailed previously to transform the specified columns. Note the key parameter, <kbd>ordering</kbd>, which will determine which numerical values the labels will be encoding into. </p>
<p>Let's call our custom encoder:</p>
<pre><span class="n">ce</span> <span class="o">=</span> <span class="n">CustomEncoder</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="s1">'ordinal_column'</span><span class="p">,</span> <span class="n">ordering</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'dislike'</span><span class="p">,</span> <span class="s1">'somewhat like'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">])</span>

<span class="n">ce</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></pre>
<p>Our dataset after these transformations looks like the following:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>boolean</strong></p>
</td>
<td>
<p><strong>city</strong></p>
</td>
<td>
<p><strong>ordinal_column</strong></p>
</td>
<td>
<p><strong>quantitative_column</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>None</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>11.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>None</p>
</td>
<td>
<p>london</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>-0.5</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>seattle</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>10.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p><span>san francisco</span></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>NaN</p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p><span>tokyo</span></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>20.0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Our ordinal column is now labeled.</p>
<p>Up to this point, we have transformed the following columns accordingly:</p>
<ul>
<li><kbd>boolean</kbd>, <kbd>city</kbd>: dummy encoding</li>
<li><kbd>ordinal_column</kbd>: label encoding</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bucketing continuous features into categories</h1>
                </header>
            
            <article>
                
<p class="mce-root">Sometimes, when you have continuous numerical data, it may make sense to transform a continuous variable into a categorical variable. For example, say you have ages, but it would be more useful to work with age ranges. </p>
<p>pandas has a useful function called <kbd>cut</kbd> that will bin your data for you. By binning, we mean it will create the ranges for your data.</p>
<p>Let's see how this function could work on our <kbd>quantitative_column</kbd>:</p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="c1"># name of category is the bin by default</span>
<span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s1">'quantitative_column'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span></pre>
<p>The output of the <kbd>cut</kbd> function for our quantitative column looks like this: </p>
</div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<pre>0     (-0.52, 6.333]
1    (6.333, 13.167]
2     (-0.52, 6.333]
3    (6.333, 13.167]
4                NaN
5     (13.167, 20.0]
Name: quantitative_column, dtype: category
Categories (3, interval[float64]): [(-0.52, 6.333] &lt; (6.333, 13.167] &lt; (13.167, 20.0]]</pre>
<p>When we specify <kbd>bins</kbd> to be an integer (<kbd>bins = 3</kbd>), it defines the number of equal–width bins in the range of <kbd>X</kbd>. However, in this case, the range of <kbd>X</kbd> is extended by .1% on each side to include the min or max values of <kbd>X</kbd>.</p>
<p>We can also set labels to <kbd>False</kbd>, which will return only integer indicators of the <kbd>bins</kbd>:</p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="c1"># using no labels</span>
<span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="s1">'quantitative_column'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></pre>
<p>Here is what the integer indicators look like for our <kbd>quantitative_column</kbd>:</p>
</div>
</div>
</div>
</div>
<pre>0    0.0
1    1.0
2    0.0
3    1.0
4    NaN
5    2.0
Name: quantitative_column, dtype: float64</pre>
<p class="output_area">Seeing our options with the <kbd>cut</kbd> function, we can also build our own <kbd>CustomCutter</kbd> for our pipeline. Once again, we will mimic the structure of our transformers. Our <kbd>transform</kbd> method will use the <kbd>cut</kbd> function, and so we will need to set <kbd>bins</kbd> and <kbd>labels</kbd> as parameters:</p>
<pre><span class="k">class</span> <span class="nc">CustomCutter</span><span class="p">(</span><span class="n">TransformerMixin</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bins</span> <span class="o">=</span> <span class="n">bins</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">col</span> <span class="o">=</span> <span class="n">col</span>
        
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">col</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span></pre>
<p class="output_area">Note that we have set the default labels parameter to <kbd>False</kbd>. Initialize our <kbd>CustomCutter</kbd>, specifying the column to transform and the number of bins to use:</p>
<pre><span class="n">cc</span> <span class="o">=</span> <span class="n">CustomCutter</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="s1">'quantitative_column'</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">cc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></pre>
<p class="prompt output_prompt">With our <kbd>CustomCutter</kbd> transforming our <kbd>quantitative_column</kbd>, our data now looks like this:</p>
<table>
<tbody>
<tr>
<td/>
<td><strong>boolean</strong></td>
<td><strong>city</strong></td>
<td><strong>ordinal_column</strong></td>
<td><strong>quantitative_column</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td>yes</td>
<td>tokyo</td>
<td><span>somewhat like</span></td>
<td>1.0</td>
</tr>
<tr>
<td><strong>1</strong></td>
<td>no</td>
<td>None</td>
<td><span>like</span></td>
<td>11.0</td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>None</td>
<td>london</td>
<td><span>somewhat like</span></td>
<td>-0.5</td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>no</td>
<td>seattle</td>
<td><span>like</span></td>
<td>10.0</td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>no</td>
<td><span>san francisco</span></td>
<td><span>somewhat like</span></td>
<td>NaN</td>
</tr>
<tr>
<td><strong>5</strong></td>
<td>yes</td>
<td><span>tokyo</span></td>
<td><span>dislike</span></td>
<td>20.0</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="output_text output_subarea output_execute_result">Note that our <kbd>quantitative_column</kbd> is now ordinal, and so there is no need to dummify the data. </p>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating our pipeline</h1>
                </header>
            
            <article>
                
<p>To review, we have transformed the columns in our dataset in the following ways thus far:</p>
<ul>
<li><kbd>boolean, city</kbd>: dummy encoding</li>
<li><kbd>ordinal_column</kbd>: label encoding</li>
<li><kbd>quantitative_column</kbd>: ordinal level data</li>
</ul>
<p>Since we now have transformations for all of our columns, let's put everything together in a pipeline. </p>
<p>Start with importing our <kbd>Pipeline</kbd> class from scikit-learn:</p>
<pre><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline<br/></span></pre>
<p>We will bring together each of the custom transformers that we have created. Here is the order we will follow in our pipeline:</p>
<ol>
<li>First, we will utilize the <kbd>imputer</kbd> to fill in missing values</li>
<li>Next, we will dummify our categorical columns</li>
<li>Then, we will encode the <kbd>ordinal_column</kbd></li>
<li>Finally, we will bucket the <kbd>quantitative_column</kbd></li>
</ol>
<p>Let's set up our pipeline as follows:</p>
<pre><span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s2">"imputer"</span><span class="p">,</span> <span class="n">imputer</span><span class="p">),</span> <span class="p">(</span><span class="s1">'dummify'</span><span class="p">,</span> <span class="n">cd</span><span class="p">),</span> <span class="p">(</span><span class="s1">'encode'</span><span class="p">,</span> <span class="n">ce</span><span class="p">),</span> <span class="p">(</span><span class="s1">'cut'</span><span class="p">,</span> <span class="n">cc</span><span class="p">)])</span>
<span class="c1"># will use our initial imputer</span>
<span class="c1"># will dummify variables first</span>
<span class="c1"># then encode the ordinal column</span>
<span class="c1"># then bucket (bin) the quantitative column</span></pre>
<p>In order to see the full transformation of our data using our pipeline, let's take a look at our data with zero transformations:</p>
<pre><span class="n"># take a look at our data before fitting our pipeline<br/>print X </span></pre>
<p class="prompt output_prompt">This is what our data looked like in the beginning before any transformations were made:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>boolean</strong></p>
</td>
<td>
<p><strong>city</strong></p>
</td>
<td>
<p><strong>ordinal_column</strong></p>
</td>
<td>
<p><strong>quantitative_column</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p>tokyo</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>1.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>None</p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>11.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>None</p>
</td>
<td>
<p>london</p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>-0.5</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p>seattle</p>
</td>
<td>
<p><span>like</span></p>
</td>
<td>
<p>10.0</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>no</p>
</td>
<td>
<p><span>san francisco</span></p>
</td>
<td>
<p><span>somewhat like</span></p>
</td>
<td>
<p>NaN</p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p>yes</p>
</td>
<td>
<p><span>tokyo</span></p>
</td>
<td>
<p><span>dislike</span></p>
</td>
<td>
<p>20.0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We can now <kbd>fit</kbd> our pipeline:</p>
<pre># now fit our pipeline<br/>pipe.fit(X)<br/><br/>&gt;&gt;&gt;&gt;<br/>Pipeline(memory=None,
     steps=[('imputer', Pipeline(memory=None,
     steps=[('quant', &lt;__main__.CustomQuantitativeImputer object at 0x128bf00d0&gt;), ('category', &lt;__main__.CustomCategoryImputer object at 0x13666bf50&gt;)])), ('dummify', &lt;__main__.CustomDummifier object at 0x128bf0ed0&gt;), ('encode', &lt;__main__.CustomEncoder object at 0x127e145d0&gt;), ('cut', &lt;__main__.CustomCutter object at 0x13666bc90&gt;)])</pre>
<p>We have created our pipeline object, let's transform our DataFrame: </p>
<pre><span class="n">pipe</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span></pre>
<p>Here is what our final dataset looks like after undergoing all of the appropriate transformations by column:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>ordinal_column</strong></p>
</td>
<td>
<p><strong>quantitative_column</strong></p>
</td>
<td>
<p><strong>boolean_no</strong></p>
</td>
<td>
<p><strong>boolean_yes</strong></p>
</td>
<td>
<p><strong>city_london</strong></p>
</td>
<td>
<p><strong>city_san francisco</strong></p>
</td>
<td>
<p><strong>city_seattle</strong></p>
</td>
<td>
<p><strong>city_tokyo</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p><strong>5</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extending numerical features</h1>
                </header>
            
            <article>
                
<p>Numerical features can undergo various methods to create extended features from them. Previously, we saw how we can transform continuous numerical data into ordinal data. Now, we will dive into extending our numerical features further. </p>
<p>Before we go any deeper into these methods, we will introduce a new dataset to work with.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activity recognition from the Single Chest-Mounted Accelerometer dataset</h1>
                </header>
            
            <article>
                
<p><span>This dataset collects data from a wearable accelerometer, mounted on the chest, collected from fifteen participants performing seven activities. The sampling frequency of the accelerometer is 52 Hz and the accelerometer data is uncalibrated. </span></p>
<p><span>The dataset is separated by participant and contains the following:</span></p>
<ul>
<li><span>Sequential number</span></li>
<li><span>x acceleration</span></li>
<li><span>y acceleration</span></li>
<li><span>z acceleration</span></li>
<li><span>Label</span></li>
</ul>
<p>Labels are codified by numbers and represent an activity, as follows:</p>
<ul>
<li class="normal">Working at a computer<span> </span></li>
<li class="normal">Standing up, walking, and going up/down stairs<span> </span></li>
<li class="normal">Standing<span> </span></li>
<li class="normal">Walking<span> </span></li>
<li class="normal">Going up/down stairs<span> </span></li>
<li class="normal">Walking and talking with someone<span> </span></li>
<li class="normal">Talking while standing</li>
</ul>
<p>Further information on this dataset<span> is available on the UCI <em>Machine Learning Repository</em> at:</span></p>
<p><span><a href="https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer" target="_blank">https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer</a></span></p>
<p>Let's take a look at our data. First, we need to load in our CSV file and set our column headers:</p>
<pre><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'../data/activity_recognizer/1.csv'</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'index'</span><span class="p">,</span> <span class="s1">'x'</span><span class="p">,</span> <span class="s1">'y'</span><span class="p">,</span> <span class="s1">'z'</span><span class="p">,</span> <span class="s1">'activity'</span><span class="p">]</span></pre>
<p>Now, let's examine the first few rows with the <kbd>.head</kbd> method, which will default to the first five rows, unless we specify how many rows to show: </p>
<pre><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></pre>
<p>This shows us:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>index</strong></p>
</td>
<td>
<p><strong>x</strong></p>
</td>
<td>
<p><strong>y</strong></p>
</td>
<td>
<p><strong>z</strong></p>
</td>
<td>
<p><strong>activity</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p><span>0.0</span></p>
</td>
<td>
<p>1502</p>
</td>
<td>
<p>2215</p>
</td>
<td>
<p>2153</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>1.0</p>
</td>
<td>
<p>1667</p>
</td>
<td>
<p>2072</p>
</td>
<td>
<p>2047</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>2.0</p>
</td>
<td>
<p>1611</p>
</td>
<td>
<p>1957</p>
</td>
<td>
<p>1906</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>3.0</p>
</td>
<td>
<p>1601</p>
</td>
<td>
<p>1939</p>
</td>
<td>
<p>1831</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>4.0</p>
</td>
<td>
<p>1643</p>
</td>
<td>
<p>1965</p>
</td>
<td>
<p>1879</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p>This dataset is meant to train models to recognize a user's current physical activity given an accelerometer's <kbd>x</kbd>, <kbd>y</kbd>, and <kbd>z</kbd> position on a device such as a smartphone. According to the website, the options for the <kbd>activity</kbd> column are:</p>
<ul>
<li>
<p><span><strong>1</strong>: Working at a computer </span></p>
</li>
<li>
<p><span><strong>2</strong>: Standing Up and Going updown stairs </span></p>
</li>
<li>
<p><span><strong>3</strong>: Standing </span></p>
</li>
<li>
<p><span><strong>4</strong>: Walking </span></p>
</li>
<li>
<p><span><strong>5</strong>: Going UpDown Stairs </span></p>
</li>
<li>
<p><span><strong>6</strong>: Walking and Talking with Someone </span></p>
</li>
<li>
<p><span><strong>7</strong>: Talking while Standing</span></p>
</li>
</ul>
<p>The <kbd>activity</kbd> column will be the target variable we will be trying to predict, using the other columns. Let's determine the null accuracy to beat in our machine learning model. To do this, we will invoke the <kbd>value_counts</kbd> method with the <kbd>normalize</kbd> option set to <kbd>True</kbd> to give us the most commonly occurring activity as a percentage:</p>
<pre class="mce-root"><span class="n">df</span><span class="p">[</span><span class="s1">'activity'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)<br/><br/></span>7    0.515369
1    0.207242
4    0.165291
3    0.068793
5    0.019637
6    0.017951
2    0.005711
0    0.000006
Name: activity, dtype: float64</pre>
<p>The null accuracy to beat is 51.53%, meaning that if we guessed seven (talking while standing), then we would be right over half of the time. Now, let's do some machine learning! Let's step through, line by line, setting up our model.</p>
<p>First, we have our <kbd>import</kbd> statements:</p>
<pre><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span></pre>
<p><span>You may be familiar with these import statements from last chapter. Once again, we will be utilizing scikit-learn's <strong>K-Nearest Neighbors</strong> (<strong>KNN</strong>) classification model. We will also use the grid search module that automatically finds the best combination of parameters for the KNN model that best fits our data with respect to cross-validated accuracy. Next, we create a feature matrix (<kbd>X</kbd>) and a response variable (<kbd>y</kbd>) for our predictive model: </span></p>
<pre><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">'x'</span><span class="p">,</span> <span class="s1">'y'</span><span class="p">,</span> <span class="s1">'z'</span><span class="p">]]</span>
<span class="c1"># create our feature matrix by removing the response variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">'activity'</span><span class="p">]</span></pre>
<p>Once our <kbd>X</kbd> and <kbd>y</kbd> are set up, <span>we can introduce the variables and instances we need to successfully run a grid search:</span></p>
<pre><span class="c1"># our grid search variables and instances</span>

<span class="c1"># KNN parameters to try</span>
<span class="n">knn_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'n_neighbors'</span><span class="p">:[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]}</span></pre>
<p>Next, we will instantiate a KNN model and <span>a grid search module and fit it to our feature matrix and response variable:</span></p>
<pre><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">knn_params</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></pre>
<p>Now, we can <kbd>print</kbd> the best accuracy and parameters that were used to learn from: </p>
<pre>print grid.best_score_, grid.best_params_<br/><br/>0.720752487677 {'n_neighbors': 5}</pre>
<p><span>Using five neighbors as its parameter, our KNN model was able to achieve a 72.07% accuracy, much better than our null accuracy of around 51.53%! Perhaps we can utilize another method to get our accuracy up even more. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Polynomial features</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>A key method of working with numerical data and creating more features is through scikit-learn's <kbd>PolynomialFeatures</kbd> class. In its simplest form, this constructor will create new columns that are products of existing columns to capture feature interactions.</span></p>
<p>More specifically, this class will g<span>enerate a new feature matrix with all of the polynomial combinations of the features with a degree less than or equal to the specified degree. Meaning that, if your input sample is two-dimensional, like so: [a, b], then the degree-2 polynomial features are as follows: [1, a, b, a^2, ab, b^2]. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parameters</h1>
                </header>
            
            <article>
                
<p>When instantiating polynomial features, there are three parameters to keep in mind:</p>
<ul>
<li>degree</li>
<li><kbd>interaction_only</kbd></li>
<li><kbd>include_bias</kbd></li>
</ul>
<p>Degree corresponds to the degree of the polynomial features, with the default set to two.</p>
<p><kbd>interaction_only</kbd> is a boolean that, when true, <span>only interaction features are produced, meaning features that are products of degree distinct features. The default for <kbd>interaction_only</kbd> is false.</span></p>
<p><span><kbd>include_bias</kbd> is also a boolean that, when true (default), includes a <kbd>bias</kbd> column, the feature in which all polynomial powers are zero, adding a column of all ones.</span></p>
<p>Let's set up a polynomial feature instance by first importing the class and instantiating with our parameters. At first, let's take a look at what features we get when setting <kbd>interaction_only</kbd> to <kbd>False</kbd>:</p>
<pre><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">interaction_only</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span></pre>
<p>Now, we can <kbd>fit_transform</kbd> these polynomial features to our dataset and look at the <kbd>shape</kbd> of our extended dataset:</p>
<pre><span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_poly</span><span class="o">.</span><span class="n">shape<br/><br/></span>(162501, 9)</pre>
<p>Our dataset has now expanded to <kbd>162501</kbd> rows and <kbd>9</kbd> columns.</p>
<p>Let's place our data into a DataFrame, setting the column headers to the <kbd>feature_names</kbd>, and taking a look at the first few rows:</p>
<pre><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">poly</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></pre>
<p>This shows us:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>x0</strong></p>
</td>
<td>
<p><strong>x1</strong></p>
</td>
<td>
<p><strong>x2</strong></p>
</td>
<td>
<p><strong>x0^2</strong></p>
</td>
<td>
<p><strong>x0 x1</strong></p>
</td>
<td>
<p><strong>x0 x2</strong></p>
</td>
<td>
<p><strong>x1^2</strong></p>
</td>
<td>
<p><strong>x1 x2</strong></p>
</td>
<td>
<p><strong>x2^2</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p><span>1502.0</span></p>
</td>
<td>
<p><span>2215.0</span></p>
</td>
<td>
<p><span>2153.0</span></p>
</td>
<td>
<p><span>2256004.0</span></p>
</td>
<td>
<p><span>3326930.0</span></p>
</td>
<td>
<p><span>3233806.0</span></p>
</td>
<td>
<p><span>4906225.0</span></p>
</td>
<td>
<p><span>4768895.0</span></p>
</td>
<td>
<p><span>4635409.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p><span>1667.0</span></p>
</td>
<td>
<p><span>2072.0</span></p>
</td>
<td>
<p><span>2047.0</span></p>
</td>
<td>
<p><span>2778889.0</span></p>
</td>
<td>
<p><span>3454024.0</span></p>
</td>
<td>
<p><span>3412349.0</span></p>
</td>
<td>
<p><span>4293184.0</span></p>
</td>
<td>
<p><span>4241384.0</span></p>
</td>
<td>
<p><span>4190209.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p><span>1611.0</span></p>
</td>
<td>
<p><span>1957.0</span></p>
</td>
<td>
<p><span>1906.0</span></p>
</td>
<td>
<p><span>2595321.0</span></p>
</td>
<td>
<p><span>3152727.0</span></p>
</td>
<td>
<p><span>3070566.0</span></p>
</td>
<td>
<p><span>3829849.0</span></p>
</td>
<td>
<p><span>3730042.0</span></p>
</td>
<td>
<p><span>3632836.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p><span>1601.0</span></p>
</td>
<td>
<p><span>1939.0</span></p>
</td>
<td>
<p><span>1831.0</span></p>
</td>
<td>
<p><span>2563201.0</span></p>
</td>
<td>
<p><span>3104339.0</span></p>
</td>
<td>
<p><span>2931431.0</span></p>
</td>
<td>
<p><span>3759721.0</span></p>
</td>
<td>
<p><span>3550309.0</span></p>
</td>
<td>
<p><span>3352561.0</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p><span>1643.0</span></p>
</td>
<td>
<p><span>1965.0</span></p>
</td>
<td>
<p><span>1879.0</span></p>
</td>
<td>
<p><span>2699449.0</span></p>
</td>
<td>
<p><span>3228495.0</span></p>
</td>
<td>
<p><span>3087197.0</span></p>
</td>
<td>
<p><span>3861225.0</span></p>
</td>
<td>
<p><span>3692235.0</span></p>
</td>
<td>
<p><span>3530641.0</span></p>
</td>
</tr>
</tbody>
</table>
<div class="output_html rendered_html output_subarea output_execute_result"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploratory data analysis</h1>
                </header>
            
            <article>
                
<p>Now we can conduct some exploratory data analysis. Since the purpose of polynomial features is to get a better sense of feature interaction in the original data, the best way to visualize this is through a correlation <kbd>heatmap</kbd>. </p>
<p>We need to import a data visualization tool that will allow us to create a <kbd>heatmap</kbd>:</p>
<pre><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span></pre>
<p>Matplotlib and Seaborn are popular data visualization tools. We can now visualize our correlation <kbd>heatmap</kbd> as follows:</p>
<pre><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">poly</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span><span class="o">.</span><span class="n">corr</span><span class="p">())</span></pre>
<p><kbd>.corr</kbd> is a function we can call on our DataFrame that gives us a correlation matrix of our features. Let's take a look at our feature interactions:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="277" src="assets/d80a9fbb-3ed3-461d-add6-513e6fb79d8f.png" width="467"/></div>
<p>The colors on the <kbd>heatmap</kbd> are based on pure values; the darker the color, the greater the correlation of the features. </p>
<p>So far, we have looked at our polynomial features with our <kbd>interaction_only</kbd> parameter set to <kbd>False</kbd>. Let's set this to <kbd>True</kbd> and see what our features look like without repeat variables. </p>
<p>We will set up this polynomial feature instance the same as we did previously. Note the only difference is that <kbd>interaction_only</kbd> is now <kbd>True</kbd>:</p>
<pre><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">interaction_only</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">print</span> <span class="n">X_poly</span><span class="o">.</span><span class="n">shape</span><br/> (162501, 6)</pre>
<p>We now have <kbd>162501</kbd> rows by <kbd>6</kbd> columns. Let's take a look:</p>
<pre><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">poly</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></pre>
<p>The DataFrame now looks as follows:</p>
<table>
<tbody>
<tr>
<td/>
<td><strong>x0</strong></td>
<td><strong>x1</strong></td>
<td><strong>x2</strong></td>
<td><strong>x0 x1</strong></td>
<td><strong>x0 x2</strong></td>
<td><strong>x1 x2</strong></td>
</tr>
<tr>
<td><strong>0</strong></td>
<td><span>1502.0</span></td>
<td><span>2215.0</span></td>
<td><span>2153.0</span></td>
<td><span>3326930.0</span></td>
<td><span>3233806.0</span></td>
<td><span>4768895.0</span></td>
</tr>
<tr>
<td><strong>1</strong></td>
<td><span>1667.0</span></td>
<td><span>2072.0</span></td>
<td><span>2047.0</span></td>
<td><span>3454024.0</span></td>
<td><span>3412349.0</span></td>
<td><span>4241384.0</span></td>
</tr>
<tr>
<td><strong>2</strong></td>
<td><span>1611.0</span></td>
<td><span>1957.0</span></td>
<td><span>1906.0</span></td>
<td><span>3152727.0</span></td>
<td><span>3070566.0</span></td>
<td><span>3730042.0</span></td>
</tr>
<tr>
<td><strong>3</strong></td>
<td><span>1601.0</span></td>
<td><span>1939.0</span></td>
<td><span>1831.0</span></td>
<td><span>3104339.0</span></td>
<td><span>2931431.0</span></td>
<td><span>3550309.0</span></td>
</tr>
<tr>
<td><strong>4</strong></td>
<td><span>1643.0</span></td>
<td><span>1965.0</span></td>
<td><span>1879.0</span></td>
<td><span>3228495.0</span></td>
<td><span>3087197.0</span></td>
<td><span>3692235.0</span></td>
</tr>
</tbody>
</table>
<div class="output_html rendered_html output_subarea output_execute_result"/>
<p> </p>
<p>Since <kbd>interaction_only</kbd> has been set to <kbd>True</kbd> this time, <kbd>x0^2</kbd>, <kbd>x1^2</kbd>, and <kbd>x2^2</kbd> <span class="c1">have disappeared since they were repeat variables. Let's see what our correlation matrix looks like now:</span></p>
<pre><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,<br/></span><span class="n">columns</span><span class="o">=</span><span class="n">poly</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span><span class="o">.</span><span class="n">corr</span><span class="p">())</span></pre>
<p class="mce-root"><span class="p">We get the following result:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="p"><img height="266" src="assets/17488850-dc5c-4e56-8aed-7882370d9704.png" width="439"/><br/></span></div>
<p>We are able to see how the features interact with each other. We can also perform a grid search of our KNN model with the new polynomial features, which can also be grid searched in a pipeline:</p>
<ol>
<li>Let's set up our pipeline parameters first:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">pipe_params = {'poly_features__degree':[1, 2, 3], 'poly_features__interaction_only':[True, False], 'classify__n_neighbors':[3, 4, 5, 6]}</pre>
<ol start="2">
<li>Now, instantiate our <kbd>Pipeline</kbd>:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span class="n">pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">'poly_features'</span><span class="p">,</span> <span class="n">poly</span><span class="p">),</span> <span class="p">(</span><span class="s1">'classify'</span><span class="p">,</span> <span class="n">knn</span><span class="p">)])</span></pre>
<ol start="3">
<li>From here, we can set up our grid search and print the best score and parameters to learn from:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">pipe_params</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span> <span class="n">grid</span><span class="o">.</span><span class="n">best_params_<br/><br/></span>0.721189408065 {'poly_features__degree': 2, 'poly_features__interaction_only': True, 'classify__n_neighbors': 5}</pre>
<p>Our accuracy is now 72.12%, which is an improvement from our accuracy without expanding our features using polynomial features!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text-specific feature construction</h1>
                </header>
            
            <article>
                
<p>Until this point, we have been working with categorical and numerical data. While our categorical data has come in the form of a string, the text has been part of a single category. We will now dive deeper into longer—form text data. This form of text data is much more complex than single—category text, because we now have a series of categories, or tokens. </p>
<p>Before we get any further into working with text data, let's make sure we have a good understanding of what we mean when we refer to text data. Consider a service like Yelp, where users write up reviews of restaurants and businesses to share their thoughts on their experience. These reviews, all written in text format, contain a wealth of information that would be useful for machine learning purposes, for example, in predicting the best restaurant to visit. </p>
<p>In general, a large part of how we communicate in today's world is through written text, whether in messaging services, social media, or email. As a result, so much can be garnered from this information through modeling. For example, we can conduct a sentiment analysis from Twitter data. </p>
<p>This type of work can be referred to as <strong>natural language processing</strong> (<strong>NLP</strong>). This is a field primarily concerned with interactions between computers and humans, specifically where computers can be programmed to process natural language. </p>
<p>Now, as we've mentioned before, it's important to note that all machine learning models require numerical inputs, so we have to be creative and think strategically when we work with text and convert such data into numerical features. There are several options for doing so, so let's get started. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bag of words representation</h1>
                </header>
            
            <article>
                
<p>The scikit-learn has a handy module called <kbd>feature_extraction</kbd> that allows us to, as the name suggests, extract features for data such as text in a format supported by machine learning algorithms. This module has methods for us to utilize when working with text. </p>
<p>Going forward, we may refer to our text data as a corpus, specifically meaning an aggregate of text content or documents. </p>
<p>The most common method to transform a corpus into a numerical representation, a process known as vectorization, is through a method called <strong>bag-of-words</strong>. The basic idea behind the bag of words approach is that documents are described by word occurrences while completely ignoring the positioning of words in the document. In its simplest form, text is represented as a <strong>bag</strong>,<strong> </strong>without regard for grammar or word order, and is maintained as a set, with importance given to multiplicity. A bag of words representation is achieved in the following three steps:</p>
<ul>
<li>Tokenizing</li>
<li>Counting</li>
<li>Normalizing</li>
</ul>
<p>Let's start with tokenizing. This process uses white spaces and punctuation to separate words from each other, turning them into tokens. Each possible token is given an integer ID.</p>
<p>Next comes counting. This step simply counts the occurrences of tokens within a document.</p>
<p>Last comes normalizing, meaning that tokens are weighted with diminishing importance when they occur in the majority of documents. </p>
<p>Let's consider a couple more methods for vectorizing. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CountVectorizer</h1>
                </header>
            
            <article>
                
<p><kbd>CountVectorizer</kbd> is the most commonly used method to convert text data into their vector representations. It is similar to dummy variables, in the sense that <kbd>CountVectorizer</kbd> converts text columns into matrices where columns are tokens and cell values are counts of occurrences of each token in each document. The resulting matrix is referred to as a <strong>document-term matrix</strong> because each row will represent a <strong>document</strong> (in this case, a tweet) and each column represents a <strong>term</strong> (a word).</p>
<p>Let's take a look at a new dataset, and see how <kbd>CountVectorizer</kbd> works. The Twitter Sentiment Analysis dataset <span>contains 1,578,627 classified tweets, and each row is marked as one for positive sentiment and zero for negative sentiment.</span></p>
<p><span>Further information on this dataset can be found at <a href="http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/" target="_blank">http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/.</a></span></p>
<p>Let's load in our data using pandas' <kbd>read_csv</kbd> method. Note that we are specifying an <kbd>encoding</kbd> as an optional parameter to ensure that we handle all special characters in the tweets properly:</p>
<pre class="mce-root"><span class="n">tweets</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'../data/twitter_sentiment.csv'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'latin1'</span><span class="p">)</span></pre>
<p>This allows us to load in our data in a specific format and map text characters appropriately.</p>
<p>Take a look at the first few rows of data:</p>
<pre class="mce-root"><span class="n">tweets</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></pre>
<p>We get the following data:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>ItemID</strong></p>
</td>
<td>
<p><strong>Sentiment</strong></p>
</td>
<td>
<p><strong>SentimentText</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p><span>is so sad for my APL frie...</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p><span>I missed the New Moon trail...</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>3</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p><span>omg its already 7:30 :O</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>4</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p><span>.. Omgaga. Im sooo im gunna CRy. I'...</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>5</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p><span>i think mi bf is cheating on me!!! ...</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We are only concerned with the <kbd>Sentiment</kbd> and <kbd>SentimentText</kbd> columns, so we will delete the <kbd>ItemID</kbd> column for now: </p>
<pre class="mce-root"><span class="k">del</span> <span class="n">tweets</span><span class="p">[</span><span class="s1">'ItemID'</span><span class="p">]</span></pre>
<p>Our data looks as follows:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>Sentiment</strong></p>
</td>
<td>
<p><strong>SentimentText</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>0</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p><span>is so sad for my APL frie...</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>1</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p><span>I missed the New Moon trail...</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>2</strong></p>
</td>
<td>
<p>1</p>
</td>
<td>
<p><span>omg its already 7:30 :O</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>3</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p><span>.. Omgaga. Im sooo im gunna CRy. I'...</span></p>
</td>
</tr>
<tr>
<td>
<p><strong>4</strong></p>
</td>
<td>
<p>0</p>
</td>
<td>
<p><span>i think mi bf is cheating on me!!! ...</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now, we can import <kbd>CountVectorizer</kbd> and get a better understanding of the text we are working with:</p>
<pre class="mce-root"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span></pre>
<p>Let's set up our <kbd>X</kbd> and <kbd>y</kbd>:</p>
<pre class="mce-root"><span class="n">X</span> <span class="o">=</span> <span class="n">tweets</span><span class="p">[</span><span class="s1">'SentimentText'</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tweets</span><span class="p">[</span><span class="s1">'Sentiment'</span><span class="p">]</span></pre>
<p>The <kbd>CountVectorizer</kbd> class works very similarly to the custom transformers we have been working with so far, and has a <kbd>fit_transform</kbd> function to manipulate the data: </p>
<pre class="mce-root"><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span> <span class="n">_</span><span class="o">.</span><span class="n">shape<br/><br/></span><span>(99989, 105849)</span></pre>
<p>After our <kbd>CountVectorizer</kbd> has transformed our data, we have 99,989 rows and 105,849 columns. </p>
<p><kbd>CountVectorizer</kbd> has many different parameters that can change the number of features that are constructed. Let's go over a few of these parameters to get a better sense of how these features are created. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CountVectorizer parameters</h1>
                </header>
            
            <article>
                
<p>A few parameters that we will go over include:</p>
<ul>
<li><kbd>stop_words</kbd></li>
<li><kbd>min_df</kbd></li>
<li><kbd>max_df</kbd></li>
<li><kbd>ngram_range</kbd></li>
<li><kbd>analyzer</kbd></li>
</ul>
<p><kbd>stop_words</kbd> is a frequently used parameter in <kbd>CountVectorizer</kbd>. You can pass in the string <kbd>english</kbd> to this parameter, and a built-in stop word list for English is used. You can also specify a list of words yourself. These words will then be removed from the tokens and will not appear as features in your data.</p>
<p>Here is an example:</p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<pre class=" highlight hl-ipython2"><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">'english'</span><span class="p">)</span>  <span class="c1"># removes a set of english stop words (if, a, the, etc)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span> <span class="n">_</span><span class="o">.</span><span class="n">shape<br/><br/></span>(99989, 105545)</pre></div>
</div>
</div>
<p>You can see that the feature columns have gone down from 105,849 when stop words were not used, to 105,545 when English stop words have been set. The idea behind using stop words is to remove noise within your features and take out words that occur so often that there won't be much meaning to garner from them in your models. </p>
<p>Another parameter is called <kbd>min_df</kbd>. This parameter is used to skim the number of features, by ignoring terms that have a document frequency lower than the given threshold or cut-off. </p>
<p>Here is an implementation of our <kbd>CountVectorizer</kbd> with <kbd>min_df</kbd>:</p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<pre class=" highlight hl-ipython2"><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">min_df</span><span class="o">=.</span><span class="mo">05</span><span class="p">)</span>  <span class="c1"># only includes words that occur in at least 5% of the corpus documents</span>
<span class="c1"># used to skim the number of features</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span> <span class="n">_</span><span class="o">.</span><span class="n">shape</span>
<br/>(99989, 31)</pre></div>
</div>
</div>
<p>This is a method that is utilized to significantly reduce the number of features created. </p>
<p>There is also a parameter called <kbd>max_df</kbd>:</p>
<pre class="mce-root"><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_df</span><span class="o">=.</span><span class="mi">8</span><span class="p">)</span>  <span class="c1"># only includes words that occur at most 80% of the documents</span>
<span class="c1"># used to "Deduce" stop words</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span> <span class="n">_</span><span class="o">.</span><span class="n">shape<br/><br/></span>(99989, 105849)</pre>
<p>This is similar to trying to understand what stop words exist in the document. </p>
<p>Next, let's look at the <kbd>ngram_range</kbd> parameter. This parameter takes in a tuple where th<span>e lower and upper boundary of the range of n-values indicates the number of different n-grams to be extracted. N-grams represent phrases, so a value of one would represent one token, however a value of two would represent two tokens together. As you can imagine, this will expand our feature set quite significantly: </span></p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<pre class=" highlight hl-ipython2"><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>  <span class="c1"># also includes phrases up to 5 words</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span> <span class="n">_</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># explodes the number of features<br/><br/></span>(99989, 3219557)</pre></div>
</div>
</div>
<p>See, we now have 3,219,557 features. Since sets of words (phrases) can sometimes have more meaning, using n-gram ranges can be useful for modeling.</p>
<p>You can also set an analyzer as a parameter in <kbd>CountVectorizer</kbd>. The analyzer determines w<span>hether the feature should be made of word or character n-grams. Word is the default: </span></p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<pre class=" highlight hl-ipython2"><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s1">'word'</span><span class="p">)</span>  <span class="c1"># default analyzer, decides to split into words</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span> <span class="n">_</span><span class="o">.</span><span class="n">shape<br/><br/></span>(99989, 105849)</pre></div>
</div>
</div>
<p>Given that word is the default, our feature column number doesn't change much from the original.</p>
<p class="mce-root">We can even create our own custom analyzer. Conceptually, words are built from root words, or stems, and we can construct a custom analyzer that accounts for this.</p>
<p class="mce-root">Stemming is a common natural language processing method that allows us to stem our vocabulary, or make it smaller by converting words to their roots. There is a natural language toolkit, known as NLTK, that has several packages that allow us to perform operations on text data. One such package is a <kbd>stemmer</kbd>. </p>
<p>Let's see how it works:</p>
<ol>
<li>First, import our <kbd>stemmer</kbd> and then initialize it:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span class="kn">from</span> <span class="nn">nltk.stem.snowball</span> <span class="kn">import</span> <span class="n">SnowballStemmer</span>
<span class="n">stemmer</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s1">'english'</span><span class="p">)</span></pre>
<ol start="2">
<li>Now, let's see how some words are stemmed: </li>
</ol>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<pre class=" highlight hl-ipython2" style="padding-left: 60px"><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s1">'interesting'</span><span class="p">)<br/></span>u'interest'</pre></div>
</div>
</div>
<ol start="3">
<li>So, the word <kbd>interesting</kbd> can be reduced to the root stem. We can now use this to create a function that will allow us to tokenize words into their stems:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span class="c1"># define a function that accepts text and returns a list of lemmas</span>
<span class="k">def</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s1">'lemma'</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">' '</span><span class="p">)</span>  <span class="c1"># tokenize into words</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span></pre>
<ol start="4">
<li>Let's see what our function outputs:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><span class="n">word_tokenize</span><span class="p">(</span><span class="s2">"hello you are very interesting"</span><span class="p">)<br/><br/></span>[u'hello', u'you', u'are', u'veri', u'interest']</pre>
<ol start="5">
<li>We can now place this tokenizer function into our analyzer parameter:</li>
</ol>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<pre class=" highlight hl-ipython2" style="padding-left: 60px"><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="n">word_tokenize</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span> <span class="n">_</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># fewer features as stemming makes words smaller<br/><br/></span>(99989, 154397)</pre></div>
</div>
</div>
<p>This yields us fewer features, which intuitively makes sense since our vocabulary has reduced with stemming.</p>
<p><kbd>CountVectorizer</kbd> is a very useful tool to help us expand our features and convert text to numerical features. There is another common vectorizer that we will look into. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Tf-idf vectorizer</h1>
                </header>
            
            <article>
                
<p>A <kbd>Tf-idfVectorizer</kbd> can be broken down into two components. First, the <em>tf</em> part, which represents <strong>term frequency</strong>, and the <em>idf</em> part, meaning <strong>inverse document frequency</strong>. It is a term—weighting method that has applications in information—retrieval and clustering.</p>
<p>A weight is given to evaluate how important a word is to a document in a corpus. Let's look into each part a little more: </p>
<ul>
<li><span><strong>tf: term frequency</strong></span>: <span>Measures how frequently a term occurs in a document. Since documents can be different in length, it is possible that a term would appear many more times in longer documents than shorter ones. Thus, the term frequency is often divided by the document length, or the total number of terms in the document, as a way of normalization.</span></li>
<li><strong>idf: i</strong><span><strong>nverse document frequency</strong></span>: <span>Measures how important a term is. While computing term frequency, all terms are considered equally important. However, certain terms, such as <em>is</em>, <em>of</em>, and <em>that</em>, may appear a lot of times but have little importance. So, we need to weight the frequent terms less, while we scale up the rare ones.</span></li>
</ul>
<p>To re-emphasize, a <kbd>TfidfVectorizer</kbd> is the same as <kbd>CountVectorizer</kbd>, in that it constructs features from tokens, but it takes a step further and normalizes counts to frequency of occurrences across a corpus. Let's see an example of this in action.</p>
<p>First, our import:</p>
<pre class="mce-root">from sklearn.feature_extraction.text import TfidfVectorizer</pre>
<p>To bring up some code from before, a plain vanilla <kbd>CountVectorizer</kbd> will output a document-term matrix:</p>
<pre class="mce-root">vect = CountVectorizer()<br/>_ = vect.fit_transform(X)<br/>print _.shape, _[0,:].mean()<br/><br/>(99989, 105849) 6.61319426731e-05</pre>
<p>Our <span> </span><kbd>TfidfVectorizer</kbd><span> can be set up as follows:</span></p>
<pre class="mce-root">vect = TfidfVectorizer()<br/>_ = vect.fit_transform(X)<br/>print _.shape, _[0,:].mean() # same number of rows and columns, different cell values<br/><br/>(99989, 105849) 2.18630609758e-05</pre>
<p>We can see that both vectorizers output the same number of rows and columns, but produce different values in each cell. This is because <kbd>TfidfVectorizer</kbd> and <kbd>CountVectorizer</kbd> are both used to transform text data into quantitative data, but the way in which they fill in cell values differ.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using text in machine learning pipelines</h1>
                </header>
            
            <article>
                
<p>Of course, the ultimate goal of our vectorizers is to use them to make text data ingestible for our machine learning pipelines. Because <kbd>CountVectorizer</kbd> and <kbd>TfidfVectorizer</kbd> act like any other transformer we have been working with in this book, we will have to utilize a scikit-learn pipeline to ensure accuracy and honesty in our machine learning pipeline. In our example, we are going to be working with a large number of columns (in the hundreds of thousands), so I will use a classifier that is known to be more efficient in this case, a Naive Bayes model:</p>
<pre class="mce-root">from sklearn.naive_bayes import MultinomialNB # for faster predictions with large number of features...</pre>
<p>Before we start building our pipelines, let's get our null accuracy of the response column, which is either zero (negative) or one (positive):</p>
<pre class="mce-root"><span># get the null accuracy</span><br/> <span>y.value_counts(normalize=True)</span><br/> <br/> 1 0.564632 0 0.435368 Name: Sentiment, dtype: float64</pre>
<p>Making the accuracy beat 56.5%. Now, let's create a pipeline with two steps:</p>
<ul>
<li><kbd>CountVectorizer</kbd> to featurize the tweets</li>
<li><kbd>MultiNomialNB</kbd> Naive Bayes model to classify between positive and negative sentiment</li>
</ul>
<p>First let's start with setting up our pipeline parameters as follows, and then instantiate our grid search as follows:</p>
<pre class="mce-root"># set our pipeline parameters<br/> pipe_params = {'vect__ngram_range':[(1, 1), (1, 2)], 'vect__max_features':[1000, 10000], 'vect__stop_words':[None, 'english']}<br/> <br/> # instantiate our pipeline<br/> pipe = Pipeline([('vect', CountVectorizer()), ('classify', MultinomialNB())])<br/> <br/> # instantiate our gridsearch object<br/> grid = GridSearchCV(pipe, pipe_params)<br/> # fit the gridsearch object<br/> grid.fit(X, y)<br/> <br/> # get our results<br/> print grid.best_score_, grid.best_params_<br/> <br/> <br/> 0.755753132845 {'vect__ngram_range': (1, 2), 'vect__stop_words': None, 'vect__max_features': 10000}</pre>
<div class="cell code_cell rendered unselected">
<p class="inner_cell"><span>And we got 75.6%, which is great! Now, let's kick things into high-gear and incorporate the</span> <kbd>TfidfVectorizer</kbd><span>. Instead of rebuilding the pipeline using tf-idf instead of</span> <kbd>CountVectorizer</kbd><span>, let's try using something a bit different. The scikit-learn has a </span><kbd>FeatureUnion</kbd> <span>module that facilitates horizontal stacking of features (side-by-side). This allows us to use multiple types of text featurizers in the same pipeline.</span></p>
<div class="input_area">
<p>For example, we can build a <kbd>featurizer</kbd> that runs both a <kbd>TfidfVectorizer</kbd> and a <kbd>CountVectorizer</kbd> on our tweets and concatenates them horizontally (keeping the same number of rows but increasing the number of columns):</p>
<pre>from sklearn.pipeline import FeatureUnion<br/># build a separate featurizer object<br/>featurizer = FeatureUnion([('tfidf_vect', TfidfVectorizer()), ('count_vect', CountVectorizer())])</pre></div>
</div>
<div class="cell code_cell rendered unselected">
<p><span>Once we build the</span> <kbd>featurizer</kbd><span>, we can use it to see how it affects the shape of our data:</span></p>
<div class="input">
<pre class="inner_cell">_ = featurizer.fit_transform(X)<br/> print _.shape # same number of rows , but twice as many columns as either CV or TFIDF<br/> <br/> (99989, 211698)</pre>
<p>We can see that unioning the two featurizers results in a dataset with the same number of rows, but doubles the number of either the <kbd>CountVectorizer</kbd> or the <kbd>TfidfVectorizer</kbd>. This is because the resulting dataset is literally both datasets side-by-side. This way, our machine learning models may learn from both sets of data simultaneously. Let's change the <kbd>params</kbd> of our <kbd>featurizer</kbd> object slightly and see what difference it makes:</p>
<pre>featurizer.set_params(tfidf_vect__max_features=100, count_vect__ngram_range=(1, 2),<br/> count_vect__max_features=300)<br/> # the TfidfVectorizer will only keep 100 words while the CountVectorizer will keep 300 of 1 and 2 word phrases <br/> _ = featurizer.fit_transform(X)<br/> print _.shape # same number of rows , but twice as many columns as either CV or TFIDF<br/> (99989, 400)</pre>
<div class="inner_cell">
<div class="cell code_cell rendered running unselected">
<div class="input">
<div class="cell code_cell rendered running unselected">
<p>Let's build a much more comprehensive pipeline that incorporates the feature union of both of our vectorizers:</p>
<pre>pipe_params = {'featurizer__count_vect__ngram_range':[(1, 1), (1, 2)], 'featurizer__count_vect__max_features':[1000, 10000], 'featurizer__count_vect__stop_words':[None, 'english'],<br/> 'featurizer__tfidf_vect__ngram_range':[(1, 1), (1, 2)], 'featurizer__tfidf_vect__max_features':[1000, 10000], 'featurizer__tfidf_vect__stop_words':[None, 'english']}<br/> pipe = Pipeline([('featurizer', featurizer), ('classify', MultinomialNB())])<br/> grid = GridSearchCV(pipe, pipe_params)<br/> grid.fit(X, y)<br/> print grid.best_score_, grid.best_params_<br/> 0.758433427677 {'featurizer__tfidf_vect__max_features': 10000, 'featurizer__tfidf_vect__stop_words': 'english', 'featurizer__count_vect__stop_words': None, 'featurizer__count_vect__ngram_range': (1, 2), 'featurizer__count_vect__max_features': 10000, 'featurizer__tfidf_vect__ngram_range': (1, 1)}</pre></div>
</div>
</div>
</div>
</div>
<p>Nice, even better than just <kbd>CountVectorizer</kbd> alone! It is also interesting to note that the best <kbd>ngram_range</kbd> for the <kbd>CountVectorizer</kbd> was <kbd>(1, 2)</kbd>, while it was <kbd>(1, 1)</kbd> for the <kbd>TfidfVectorizer</kbd>, implying that word occurrences alone were not as important as two-word phrase occurrences.</p>
</div>
<div class="packt_infobox">By this point, it should be obvious that we could have made our pipeline much more complicated by:<br/>
<ul>
<li style="list-style-type: none">
<ul>
<li>Grid searching across dozens of parameters for each vectorizer</li>
</ul>
</li>
</ul>
<ul>
<li style="list-style-type: none">
<ul>
<li>Adding in more steps to our pipeline such as polynomial feature construction</li>
</ul>
</li>
</ul>
But this would have been very cumbersome for this text and would take hours to run on most commercial laptops. Feel free to expand on this pipeline and beat our score!</div>
<p>Phew, that was a lot. Text can be difficult to work with. Between sarcasm, misspellings, and vocabulary size, data scientists and machine learning engineers have their hands full. This introduction to working with text will allow you, the reader, to experiment with your own large text datasets and obtain your own results!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Thus far, we have gone over several methods of imputing missing values in our categorical and numerical data, encoding our categorical variables, and creating custom transformers to fit into a pipeline. We also dove into several feature construction methods for both numerical data and text-based data. </p>
<p>In the next chapter, we will take a look at the features we have constructed, and consider appropriate methods of selecting the right features to use for our machine learning models. </p>


            </article>

            
        </section>
    </body></html>