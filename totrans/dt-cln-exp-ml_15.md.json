["```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import OneHotEncoder\n    from imblearn.pipeline import make_pipeline\n    from sklearn.compose import ColumnTransformer\n    from sklearn.model_selection import RandomizedSearchCV\n    from imblearn.over_sampling import SMOTENC\n    from sklearn.tree import DecisionTreeClassifier, plot_tree\n    from scipy.stats import randint\n    import sklearn.metrics as skmet\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    from preprocfunc import MakeOrdinal,\\\n      ReplaceVals\n    ```", "```py\n    healthinfo = pd.read_csv(\"data/healthinfosample.csv\")\n    healthinfo.set_index(\"personid\", inplace=True)\n    healthinfo.heartdisease.value_counts()\n    No     27467\n    Yes     2533\n    Name: heartdisease, dtype: int64\n    healthinfo['heartdisease'] = \\\n      np.where(healthinfo.heartdisease=='No',0,1).\\\n      astype('int')\n    healthinfo.heartdisease.value_counts()\n    0    27467\n    1     2533\n    Name: heartdisease, dtype: int64\n    ```", "```py\n    healthinfo.agecategory.value_counts().\\\n      sort_index().reset_index()\n              index  agecategory\n    0         18-24         1973\n    1         25-29         1637\n    2         30-34         1688\n    3         35-39         1938\n    4         40-44         2007\n    5         45-49         2109\n    6         50-54         2402\n    7         55-59         2789\n    8         60-64         3122\n    9         65-69         3191\n    10        70-74         2953\n    11        75-79         2004\n    12  80 or older         2187\n    ```", "```py\n    num_cols = ['bmi','physicalhealthbaddays',\n       'mentalhealthbaddays','sleeptimenightly']\n    binary_cols = ['smoking','alcoholdrinkingheavy',\n      'stroke','walkingdifficult','physicalactivity',\n      'asthma','kidneydisease','skincancer']\n    cat_cols = ['gender','ethnicity']\n    spec_cols1 = ['agecategory']\n    spec_cols2 = ['genhealth','diabetic']\n    rep_dict = {\n      'genhealth': {'Poor':0,'Fair':1,'Good':2,\n        'Very good':3,'Excellent':4},\n      'diabetic': {'No':0,\n        'No, borderline diabetes':0,'Yes':1,\n        'Yes (during pregnancy)':1}           \n    }\n    ```", "```py\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(healthinfo[num_cols + \n        binary_cols + cat_cols + spec_cols1 +\n        spec_cols2],\\\n      healthinfo[['heartdisease']], test_size=0.2,\n        random_state=0)\n    ```", "```py\nohe = OneHotEncoder(drop='first', sparse=False)\nspectrans1 = make_pipeline(MakeOrdinal())\nspectrans2 = make_pipeline(ReplaceVals(rep_dict))\nbintrans = make_pipeline(ohe)\ncattrans = make_pipeline(ohe)\ncoltrans = ColumnTransformer(\n  transformers=[\n    (\"bin\", bintrans, binary_cols),\n    (\"cat\", cattrans, cat_cols),\n    (\"spec1\", spectrans1, spec_cols1),\n    (\"spec2\", spectrans2, spec_cols2),\n  ],\n    remainder = 'passthrough'\n)\n```", "```py\n    coltrans.fit(X_train.sample(1000))\n    new_binary_cols = \\\n      coltrans.\\\n      named_transformers_['bin'].\\\n      named_steps['onehotencoder'].\\\n      get_feature_names(binary_cols)\n    new_cat_cols = \\\n      coltrans.\\\n      named_transformers_['cat'].\\\n      named_steps['onehotencoder'].\\\n      get_feature_names(cat_cols)\n    ```", "```py\n    new_cols = np.concatenate((new_binary_cols, \n      new_cat_cols, np.array(spec_cols1 + spec_cols2 +\n      num_cols)))\n    new_cols\n    array(['smoking_Yes', 'alcoholdrinkingheavy_Yes',\n           'stroke_Yes', 'walkingdifficult_Yes',\n           'physicalactivity_Yes', 'asthma_Yes',\n           'kidneydisease_Yes', 'skincancer_Yes',\n           'gender_Male', 'ethnicity_Asian',\n           'ethnicity_Black', 'ethnicity_Hispanic',\n           'ethnicity_Other', 'ethnicity_White',\n           'agecategory', 'genhealth', 'diabetic', 'bmi',\n           'physicalhealthbaddays', 'mentalhealthbaddays',\n           'sleeptimenightly'], dtype=object)\n    ```", "```py\ncatcolscnt = new_binary_cols.shape[0] + \\\n  new_cat_cols.shape[0]\nsmotenc = \\\n  SMOTENC(categorical_features=np.arange(0,catcolscnt),\n  random_state=0)\ndtc_example = DecisionTreeClassifier(\n  min_samples_leaf=5, max_depth=2)\npipe0 = make_pipeline(coltrans, smotenc, dtc_example)\npipe0.fit(X_train, y_train.values.ravel())\n```", "```py\n    feature_imp = \\\n      pipe0.named_steps['decisiontreeclassifier'].\\\n      tree_.compute_feature_importances(normalize=False)\n    feature_impgt0 = feature_imp>0\n    feature_implabs = np.column_stack((feature_imp.\\\n      ravel(), new_cols))\n    feature_implabs[feature_impgt0]\n    array([[0.10241844433036575, 'agecategory'],\n           [0.04956947743193013, 'genhealth'],\n           [0.012777650193266089, 'diabetic']],\n          dtype=object)\n    ```", "```py\n    plot_tree(pipe0.named_steps['decisiontreeclassifier'],\n      feature_names=new_cols, \n      class_names=['No Disease','Disease'], fontsize=10)\n    ```", "```py\n    pred = pipe0.predict(X_test)\n    print(\"accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f\"  %\n      (skmet.accuracy_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred,\n        pos_label=0),\n      skmet.precision_score(y_test.values.ravel(), pred)))\n    accuracy: 0.72, sensitivity: 0.70, specificity: 0.73, precision: 0.19\n    ```", "```py\n    dtc = DecisionTreeClassifier(random_state=0)\n    pipe1 = make_pipeline(coltrans, smotenc, dtc)\n    dtc_params = {\n     'decisiontreeclassifier__min_samples_leaf': randint(100, 1200),\n     'decisiontreeclassifier__max_depth': randint(2, 11)\n    }\n    ```", "```py\n    rs = RandomizedSearchCV(pipe1, dtc_params, cv=5,\n      n_iter=20, scoring=\"roc_auc\")\n    rs.fit(X_train, y_train.values.ravel())\n    rs.best_params_\n    {'decisiontreeclassifier__max_depth': 9,\n     'decisiontreeclassifier__min_samples_leaf': 954}\n    rs.best_score_\n    0.7964540832005679\n    ```", "```py\n    results = \\\n      pd.DataFrame(rs.cv_results_['mean_test_score'], \\\n        columns=['meanscore']).\\\n      join(pd.DataFrame(rs.cv_results_['params'])).\\\n      sort_values(['meanscore'], ascending=False).\\\n      rename(columns=\\\n        {'decisiontreeclassifier__max_depth':'maxdepth',\n         'decisiontreeclassifier__min_samples_leaf':\\\n         'samples'})\n    ```", "```py\n    meanscore  maxdepth  samples\n15      0.796         9      954\n13      0.796         8      988\n4       0.795         7      439\n19      0.795         9      919\n12      0.794         9      856\n3       0.794         9      510\n2       0.794         9     1038\n5       0.793         8      575\n0       0.793        10     1152\n10      0.793         7     1080\n6       0.793         6     1013\n8       0.793        10      431\n17      0.793         6      896\n14      0.792         6      545\n16      0.784         5      180\n1       0.778         4      366\n11      0.775         4      286\n9       0.773         4      138\n18      0.768         3      358\n7       0.765         3      907 \n```", "```py\n    pred2 = rs.predict(X_test)\n    cm = skmet.confusion_matrix(y_test, pred2)\n    cmplot = \\\n      skmet.ConfusionMatrixDisplay(confusion_matrix=cm,\n      display_labels=['Negative', 'Positive'])\n    cmplot.plot()\n    cmplot.ax_.\\\n      set(title='Heart Disease Prediction Confusion Matrix', \n      xlabel='Predicted Value', ylabel='Actual Value') \n    ```", "```py\n    print(\"accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f\"  %\n      (skmet.accuracy_score(y_test.values.ravel(), pred2),\n       skmet.recall_score(y_test.values.ravel(), pred2),\n       skmet.recall_score(y_test.values.ravel(), pred2,\n         pos_label=0),\n       skmet.precision_score(y_test.values.ravel(), \n         pred2)))\n    accuracy: 0.77, sensitivity: 0.63, specificity: 0.79, precision: 0.21\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from imblearn.pipeline import make_pipeline\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import RandomizedSearchCV\n    from scipy.stats import randint\n    import sklearn.metrics as skmet\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    import healthinfo as hi\n    ```", "```py\n    X_train = hi.X_train\n    X_test = hi.X_test\n    y_train = hi.y_train\n    y_test = hi.y_test\n    ```", "```py\n    rfc = RandomForestClassifier(random_state=0)\n    pipe1 = make_pipeline(hi.coltrans, hi.smotenc, rfc)\n    rfc_params = {\n     'randomforestclassifier__min_samples_leaf':\n        randint(100, 1200),\n     'randomforestclassifier__max_depth': \n        randint(2, 11),\n     'randomforestclassifier__n_estimators': \n        randint(100, 3000),\n     'randomforestclassifier__criterion': \n        ['gini','entropy']\n    }\n    rs = RandomizedSearchCV(pipe1, rfc_params, cv=5, \n      n_iter=20, scoring=\"roc_auc\")\n    rs.fit(X_train, y_train.values.ravel())\n    ```", "```py\nrs.best_params_\n{'randomforestclassifier__criterion': 'gini',\n 'randomforestclassifier__max_depth': 9,\n 'randomforestclassifier__min_samples_leaf': 667,\n 'randomforestclassifier__n_estimators': 1023}\nrs.best_score_\n0.8210934290375318\n```", "```py\n    feature_imp = \\\n      rs.best_estimator_['randomforestclassifier'].\\\n      feature_importances_\n    feature_implabs = np.column_stack((feature_imp.\\\n      ravel(), hi.new_cols))\n    pd.DataFrame(feature_implabs,\n      columns=['importance','feature']).\\\n      sort_values(['importance'], ascending=False)\n             importance       feature\n    14       0.321            agecategory\n    15       0.269            genhealth\n    16       0.159            diabetic\n    13       0.058            ethnicity_White\n    0        0.053            smoking_Yes\n    18       0.033            physicalhealthbaddays\n    8        0.027            gender_Male\n    3        0.024            walkingdifficult_Yes\n    20       0.019            sleeptimenightly\n    11       0.010            ethnicity_Hispanic\n    17       0.007            bmi\n    19       0.007            mentalhealthbaddays\n    1        0.007            alcoholdrinkingheavy_Yes\n    5        0.003            asthma_Yes\n    10       0.002            ethnicity_Black\n    4        0.001            physicalactivity_Yes\n    7        0.001            skincancer_Yes\n    2        0.000            stroke_Yes\n    6        0.000            kidneydisease_Yes\n    9        0.000            ethnicity_Asian\n    12       0.000            ethnicity_Other\n    ```", "```py\n    print(\"accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f\"  %\n      (skmet.accuracy_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred,\n        pos_label=0),\n      skmet.precision_score(y_test.values.ravel(), pred)))\n    accuracy: 0.77, sensitivity: 0.69, specificity: 0.78, precision: 0.22\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from imblearn.pipeline import make_pipeline\n    from sklearn.model_selection import RandomizedSearchCV\n    from sklearn.ensemble import GradientBoostingClassifier\n    import sklearn.metrics as skmet\n    from scipy.stats import uniform\n    from scipy.stats import randint\n    import matplotlib.pyplot as plt\n    import os\n    import sys\n    sys.path.append(os.getcwd() + \"/helperfunctions\")\n    import healthinfo as hi\n    ```", "```py\n    X_train = hi.X_train\n    X_test = hi.X_test\n    y_train = hi.y_train\n    y_test = hi.y_test\n    ```", "```py\ngbc = GradientBoostingClassifier(random_state=0)\npipe1 = make_pipeline(hi.coltrans, hi.smotenc, gbc)\ngbc_params = {\n 'gradientboostingclassifier__min_samples_leaf':\n     randint(100, 1200),\n 'gradientboostingclassifier__max_depth':\n     randint(2, 20),\n 'gradientboostingclassifier__learning_rate':\n     uniform(loc=0.02, scale=0.25),\n 'gradientboostingclassifier__n_estimators':\n     randint(100, 1200)\n}\n```", "```py\n    rs = RandomizedSearchCV(pipe1, gbc_params, cv=5, \n      n_iter=7, scoring=\"roc_auc\")\n    rs.fit(X_train, y_train.values.ravel())\n    rs.best_params_\n    {'gradientboostingclassifier__learning_rate': 0.2528,\n     'gradientboostingclassifier__max_depth': 3,\n     'gradientboostingclassifier__min_samples_leaf': 565,\n     'gradientboostingclassifier__n_estimators': 308}\n    rs.best_score_\n    0.8162378796382679\n    ```", "```py\n    feature_imp = pd.Series(rs.\\\n      best_estimator_['gradientboostingclassifier'].\\\n      feature_importances_, index=hi.new_cols)\n    feature_imp.loc[feature_imp>0.01].\\\n        plot(kind='barh')\n    plt.tight_layout()    \n    plt.title('Gradient Boosting Feature Importance')\n    ```", "```py\n    pred = rs.predict(X_test)\n    print(\"accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f\"  %\n      (skmet.accuracy_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred),\n      skmet.recall_score(y_test.values.ravel(), pred,\n        pos_label=0),\n      skmet.precision_score(y_test.values.ravel(), pred)))\n    accuracy: 0.91, sensitivity: 0.19, specificity: 0.97, precision: 0.40\n    ```"]