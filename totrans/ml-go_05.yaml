- en: Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: When many people think about machine learning or artificial intelligence, they
    probably first think about machine learning to solve classification problems.
    These are problems where we want to train a model to predict one of a finite number
    of distinct categories. For example, we may want to predict if a financial transaction
    is fraudulent or not fraudulent, or we may want to predict whether an image contains
    a hot dog, airplane, cat, and so on, or none of those things.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当许多人思考机器学习或人工智能时，他们可能首先想到的是机器学习来解决分类问题。这些问题是我们希望训练一个模型来预测有限数量的不同类别之一。例如，我们可能想要预测一笔金融交易是欺诈还是非欺诈，或者我们可能想要预测一张图片是否包含热狗、飞机、猫等，或者都不是这些。
- en: The categories that we try to predict could number from two to many hundreds
    or thousands. In addition, we could be making our predictions based on only a
    few attributes or many attributes. All of the scenarios arising from these combinations
    lead to a host of models with a corresponding host of assumptions, advantages,
    and disadvantages.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图预测的类别数量可能从两个到数百或数千不等。此外，我们可能只基于几个属性或许多属性进行预测。所有这些组合产生的场景都导致了一系列具有相应假设、优点和缺点的模型。
- en: We will cover some of these models in this chapter and later in the book, but
    there are many that we will skip for brevity. However, as with any of the problems
    that we tackle in this book, simplicity and integrity should be a major concern
    as we choose a type of model for our use cases. There are highly sophisticated
    and complicated models that solve certain problems very well, but these models
    are not necessary for many use cases. Applying simple and interpretable classification
    models should continue to be one of our goals.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章和本书的后续部分介绍一些这些模型，但为了简洁起见，我们将跳过许多模型。然而，正如我们在本书中解决任何问题时一样，简单性和完整性在选择适用于我们用例的模型时应该是一个主要关注点。有一些非常复杂和高级的模型可以很好地解决某些问题，但这些模型对于许多用例来说并不是必要的。应用简单且可解释的分类模型应该继续成为我们的目标之一。
- en: Understanding classification model jargon
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分类模型术语
- en: 'As with regression, classification problems come with their own set of jargon.
    There is some overlap with terms used in regression, but there are also some new
    terms specific to classification:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 与回归一样，分类问题也有其一套术语。这些术语与回归中使用的术语有一些重叠，但也有一些是特定于分类的新术语：
- en: '**Categories**, **labels**, or **classes**: These terms are used interchangeably
    to represent the various distinct choices for our prediction. For example, we
    could have a fraud class and a not fraud class, or we could have sitting, standing,
    running, and walking categories.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别**、**标签**或**类别**：这些术语可以互换使用，以表示我们预测的各种不同选择。例如，我们可以有一个欺诈类别和一个非欺诈类别，或者我们可以有坐着、站着、跑步和行走类别。'
- en: '**Binary classification**: This type of classification is one with only two
    categories or classes, such as yes/no or fraud/not fraud.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元分类**：这种分类类型只有两个类别或类别，例如是/否或欺诈/非欺诈。'
- en: '**Multi-class classification**: This type of classification is one with more
    than two classes, such as a classification trying to assign one of hot dog, airplane,
    cat, and so on, to an image.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类分类**：这种分类类型具有超过两个类别，例如尝试将热狗、飞机、猫等中的一个分配给图像的分类。'
- en: '**Labeled data** or **annotated data**: Real-world observations or records
    that have been paired with their corresponding class. For example, if we are predicting
    fraud via transaction time, this data would include a bunch of measured transaction
    times along with a corresponding label indicating whether they were fraudulent
    or not.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记数据**或**标注数据**：与它们对应的类别配对的真实世界观察或记录。例如，如果我们通过交易时间预测欺诈，这些数据将包括一系列测量的交易时间以及一个相应的标签，指示它们是否是欺诈的。'
- en: Logistic regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: The first classification model that we are going to explore is called **logistic
    regression**. As you can tell from the name, this method is based on a regression,
    which we discussed in more detail in the previous chapter. However, this particular
    regression uses a function that is particularly well suited to classification
    problems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探索的第一个分类模型被称为**逻辑回归**。从名称上可以看出，这种方法基于回归，我们在上一章中详细讨论了回归。然而，这种特定的回归使用了一个特别适合分类问题的函数。
- en: This is also a simple and interpretable model, which makes it a great first
    choice when solving classification problems. There are a variety of existing Go
    packages that implement logistic regression for you, including `github.com/xlvector/hector`,
    `github.com/cdipaolo/goml`, and `github.com/sjwhitworth/golearn`. However, in
    our example, we will implement logistic regression from scratch, so that you can
    both form a full understanding of what goes into training a model and understand
    the simplicity of logistic regression. Further, in some cases, you may want to
    utilize a from-scratch implementation as illustrated in the following section
    to avoid extra dependencies in your code base.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一个简单且易于理解的模型，因此在解决分类问题时，它是一个非常好的首选。目前有各种现有的Go包实现了逻辑回归，包括`github.com/xlvector/hector`、`github.com/cdipaolo/goml`和`github.com/sjwhitworth/golearn`。然而，在我们的例子中，我们将从头开始实现逻辑回归，这样你既可以全面了解模型训练的过程，也可以理解逻辑回归的简单性。此外，在某些情况下，你可能希望利用以下章节中所示的自定义实现来避免在代码库中引入额外的依赖。
- en: Overview of logistic regression
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归概述
- en: 'Let''s say that we have two classes, *A* and *B,* that we are trying to predict.
    Let''s also suppose that we are trying to predict *A* or *B* based on a variable
    *x*. Classes *A* and *B* might look something like this when plotted against *x*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个类别*A*和*B*，我们正在尝试预测。让我们还假设我们正在根据变量*x*来预测*A*或*B*。当与*x*绘制时，类别*A*和*B*可能看起来像这样：
- en: '![](img/e025a17c-35a1-48f6-b99f-6acac1b654c5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e025a17c-35a1-48f6-b99f-6acac1b654c5.png)'
- en: Although we could draw a line modeling this behavior, this is clearly not linear
    behavior and does not fit the assumptions of linear regression. The shape of the
    data is more of a step from one class to another class as a function of *x*. What
    we really need is some function that goes to and stays at *A* for low values of
    *x,* and goes to and stays at *B* for higher values of *x.*
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以绘制一条线来模拟这种行为，但这显然不是线性行为，并且不符合线性回归的假设。数据的形状更像是一个从一类到另一类的阶梯，作为*x*的函数。我们真正需要的是一个函数，它在*x*的较低值时趋近并保持在*A*，而在*x*的较高值时趋近并保持在*B*。
- en: 'Well, we are in luck! There is such a function. The function is called the
    **logistic function**, and it gives logistic regression its name. It has the following
    form:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们很幸运！确实存在这样一个函数。这个函数被称为**逻辑函数**，它为逻辑回归提供了其名称。它具有以下形式：
- en: '![](img/00e1e244-3cb7-445a-aff7-af8b3d72f391.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00e1e244-3cb7-445a-aff7-af8b3d72f391.jpg)'
- en: 'Implemented in Go, this looks as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在Go中实现如下：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s plot the logistic function with `gonum.org/v1/plot` to see what it looks
    like:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`gonum.org/v1/plot`来绘制逻辑函数，看看它是什么样子：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Compiling and running this plotting code creates the following graph:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行此绘图代码将创建以下图表：
- en: '![](img/aaac3a8d-b90c-421d-83d9-0505f845a417.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaac3a8d-b90c-421d-83d9-0505f845a417.png)'
- en: As you can see, this function has the step-like behavior that we are looking
    for to model the steps between classes *A* and *B* (imagining that *A* corresponds
    to *0.0* and *B* corresponds to *1.0*).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个函数具有我们所寻找的阶梯状行为，可以用来建模类别*A*和*B*之间的步骤（假设*A*对应于*0.0*，而*B*对应于*1.0*）。
- en: 'Not only that, the logistic function has some really convenient properties
    that we can take advantage of while doing classification. To see this, let''s
    take a step back and consider how we might model *p*, the probability of one of
    the classes *A* or *B*, occurring. One way to do this would be to model the *log*
    of the **odds** **ratio**, *log(* *p / (1 - p) )* linearly, where the odds ratio
    tells us how the presence or absence of class *A* has an effect on the presence
    or absence of class *B*. The reason for using this strange *log* (referred to
    as **logit**) will make sense shortly, but for now, let''s just assume that we
    want to model this linearly as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，逻辑函数还有一些非常方便的性质，我们可以在分类过程中利用这些性质。为了看到这一点，让我们退一步，考虑我们如何可能建模*p*，即类别*A*或*B*发生的概率。一种方法是将**odds
    ratio**（优势比）的**log**（对数）线性化，即*log(*p / (1 - p)**)，其中优势比告诉我们类别*A*的存在或不存在如何影响类别*B*的存在或不存在。使用这种奇怪的*log*（称为**logit**）的原因很快就会变得有意义，但现在，我们只需假设我们想要如下线性化地建模这个：
- en: '![](img/12a20c36-d29d-4179-9462-60aae6534f4a.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12a20c36-d29d-4179-9462-60aae6534f4a.jpg)'
- en: 'Now, if we take the exponential of this odds ratio, we get the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们取这个优势比的指数，我们得到以下结果：
- en: '![](img/8832e141-9447-4913-a625-f96e1ea61cd0.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8832e141-9447-4913-a625-f96e1ea61cd0.jpg)'
- en: 'This is what we get when we simplify the preceding equation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们简化前面的方程时，我们得到以下结果：
- en: '![](img/08fee5a9-69d0-458a-8c82-14fd462b2640.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/08fee5a9-69d0-458a-8c82-14fd462b2640.jpg)'
- en: 'If you look at the right-hand side of this equation, you will see that our
    logistic function has popped up. This equation then gives some formal underpinning
    to our assumption that the logistic function would be good for modeling the separation
    between two classes: *A* and *B*. If, for instance, we take *p* to be the probability
    of observing *B* and we fit the logistic function to our data, we could get a
    model that predicts the probability of *B* as a function of *x* (and thus one
    minus the probability of predicting *A*). This is pictured in the following plot,
    in which we have formalized our original plot of *A* and *B* and overlaid the
    logistic function that models probability:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看看这个方程的右侧，你会看到我们的逻辑函数出现了。这个方程为我们的假设提供了正式的依据，即逻辑函数适合于模拟两个类别*A*和*B*之间的分离。例如，如果我们把*p*看作是观察到*B*的概率，并将逻辑函数拟合到我们的数据上，我们就可以得到一个模型，该模型将*B*的概率作为*x*的函数来预测（从而预测*A*的概率为1减去该概率）。这在上面的图中得到了体现，我们在其中正式化了*A*和*B*的原始图，并叠加了模拟概率的逻辑函数：
- en: '![](img/ed4153cb-34b8-4084-8435-3bbb0606c82a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ed4153cb-34b8-4084-8435-3bbb0606c82a.png)'
- en: Thus, creating a logistic regression model involves finding the logistic function
    that maximizes the number of observations that we can predict with the logistic
    function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，创建逻辑回归模型涉及找到最大化我们能够用逻辑函数预测的观测数目的逻辑函数。
- en: Note that one advantage of logistic regression is that it remains simple and
    interpretable. However, the coefficients *m* and *b* in the model do not have
    the same interpretation as in linear regression. The coefficient *m* (or coefficients
    *m[1]*, *m[2]*, and so on, if we have multiple independent variables) has an exponential
    relationship with the odds ratio. Thus, if you have a coefficient *m* of *0.5*,
    this is related to the odds ratio via *exp(0.5 x)*. If we had two coefficients
    *exp(0.5 x[1] + 1.0 x[2])*, we could conclude that the odds of the modeled class
    for *x[1]* is *exp(0.5) = 1.65* when compared to *exp(1.0) = 2.72* for *x[2]*.
    In other words, we cannot directly compare the coefficients. We need to keep them
    in the context of the exponential.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，逻辑回归的一个优点是它保持简单且可解释。然而，模型中的系数*m*和*b*在解释上并不像线性回归中的那样。系数*m*（或者如果有多个独立变量，系数*m[1]*、*m[2]*等）与似然比有指数关系。因此，如果你有一个*m*系数为*0.5*，这通过*exp(0.5
    x)*与似然比相关。如果我们有两个系数*exp(0.5 x[1] + 1.0 x[2])*，我们可以得出结论，对于*x[1]*，模型类别的似然比是*exp(0.5)
    = 1.65*，而*x[2]*的似然比是*exp(1.0) = 2.72*。换句话说，我们不能直接比较系数。我们需要在指数的上下文中保持它们。
- en: Logistic regression assumptions and pitfalls
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归的假设和陷阱
- en: 'Remember the long list of assumptions that applied to linear regression? Well,
    logistic regression is not limited by those same assumptions. However, there are
    still some important assumptions that we make when using logistic regression:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 记得之前应用到线性回归上的那些长长的假设列表吗？嗯，逻辑回归并不受那些相同假设的限制。然而，当我们使用逻辑回归时，仍然有一些重要的假设：
- en: '**Linear relationship with the log odds**: As we discussed earlier, underlying
    logistic regression is an assumption that we can model the log of the odds ratio
    with a line.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与对数似然比之间的线性关系**：正如我们之前讨论的，逻辑回归的潜在假设是我们可以用一条线来模拟对数似然比。'
- en: '**Encoding of dependent variable**: When we set up our model earlier, we assumed
    that we were trying to predict the probability of *B*, where a probability of
    1.0 corresponded to a positive *B* example. Thus, we need to prepare our data
    with this type of encoding. This will be demonstrated in the following example.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因变量的编码**：在我们之前设置模型时，我们假设我们正在尝试预测*B*的概率，其中概率为1.0对应于正的*B*例子。因此，我们需要用这种类型的编码准备我们的数据。这将在下面的例子中演示。'
- en: '**Independence of observations**: Each of the examples of *x* in our data must
    be independent. That is, we have to avoid things like including the same example
    several times.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观测的独立性**：我们数据中*x*的每一个例子都必须是独立的。也就是说，我们必须避免诸如多次包含相同例子这样的情况。'
- en: 'Also, some common pitfalls of logistic regression to keep in mind are as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以下是一些需要记住的逻辑回归常见陷阱：
- en: Logistic regression can be more sensitive to outliers than other classification
    techniques. Keep this in mind and try to profile your data accordingly.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归可能比其他分类技术对异常值更敏感。请记住这一点，并相应地尝试分析你的数据。
- en: As logistic regression relies on an exponential function that never truly goes
    to *0.0* or *1.0* (except at +/- infinity), you may have very small degradations
    in your evaluation metric.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于逻辑回归依赖于一个永远不会真正达到*0.0*或*1.0*（除了在正负无穷大时）的指数函数，你可能会在评估指标中看到非常小的下降。
- en: All being said, logistic regression is a fairly robust method that remains interpretable.
    It is a flexible model that should be at the top of your list when considering
    how you might solve classification problems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，逻辑回归是一种相当稳健的方法，且易于解释。它是一个灵活的模型，在考虑如何解决分类问题时，应该排在你的首选列表中。
- en: Logistic regression example
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归示例
- en: 'The dataset that we are going to use to illustrate logistic regression is data
    corresponding to loans issues by LendingClub. LendingClub publishes this data
    quarterly and it can be found in its original form at [https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action).
    We will work with a trimmed down and simplified version of this data (available
    in the code bundles that come with this book) including only two columns, `FICO.Range`
    (indicating a loan applicants credit score as given by Fair, Isaac and Company,
    or FICO) and `Interest.Rate` (indicating the interest rate of a loan granted to
    the loan applicant). The data looks like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要用来展示逻辑回归的数据集是LendingClub发布的贷款数据。LendingClub每季度发布这些数据，其原始形式可以在[https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action)找到。我们将使用这本书附带代码包中的简化版数据（只包含两列），即`FICO.Range`（表示贷款申请人的信用评分，由Fair,
    Isaac and Company提供，或称FICO）和`Interest.Rate`（表示授予贷款申请人的利率）。数据看起来是这样的：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Our goal for this exercise will be to create a logistic regression model that
    will tell us, for a given credit score, if we can get a loan at or below a certain
    interest rate. For example, let's say that we are interested in getting an interest
    rate below 12%. Our model would tell us that we could (yes, or class one) or could
    not (no, class two) get a loan, given a certain credit score.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这个练习的目标是创建一个逻辑回归模型，它将告诉我们，对于给定的信用评分，我们能否以或低于某个利率获得贷款。例如，假设我们感兴趣的是利率低于12%。我们的模型将告诉我们，在给定的信用评分下，我们能否（是的，或类别一）或不能（不，类别二）获得贷款。
- en: Cleaning and profiling the data
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洗和描述数据
- en: 'Looking at the preceding sample of the loan data, we can see that it is not
    exactly in the form that we need for our classification. Specifically, we need
    to do the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前述的贷款数据样本，我们可以看到它并不完全是我们需要的分类形式。具体来说，我们需要做以下几步：
- en: Remove non-numerical characters from the interest rate and FICO score columns.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从利率和FICO评分列中移除非数值字符。
- en: Encode our interest rate into two classes for a given interest rate threshold.
    We will use *1.0* to represent our first class (yes, we can get the loan with
    that interest rate) and *0.0* to represent our second class (no, we cannot get
    the loan with that interest rate).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将利率编码为两个类别，针对给定的利率阈值。我们将使用*1.0*来表示第一个类别（是的，我们可以以该利率获得贷款）和*0.0*来表示第二个类别（不，我们不能以该利率获得贷款）。
- en: Select a single value for the FICO credit score. We are given a range of credit
    scores, but we need a single value. The average, minimum, or maximum score are
    natural choices and, in our example, we will use the minimum value (to be conservative).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择FICO信用评分的单个值。我们给出了一个信用评分的范围，但我们需要一个单一值。平均值、最小值或最大值是自然的选择，在我们的例子中，我们将使用最小值（为了保守起见）。
- en: In this case, we are going to **standardize** our FICO scores (by subtracting
    the minimum score value from each score and then dividing by the score range).
    This will spread out our score values between *0.0* and *1.0*. We need to have
    a justification for this as it is making our data less readable. However, there
    is a good justification. We will be training our logistic regression using a gradient
    descent method that can perform better with normalized data. In fact, when running
    the same example with non-normalized data, there are convergence issues.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将**标准化**我们的FICO评分（通过从每个评分中减去最小评分值然后除以评分范围）。这将使评分值分布在*0.0*到*1.0*之间。我们需要对此进行合理的解释，因为它会使我们的数据不那么易读。然而，有一个合理的解释。我们将使用梯度下降法来训练逻辑回归，这种方法在标准化数据上表现更好。实际上，当使用非标准化数据运行相同的示例时，会出现收敛问题。
- en: 'Let''s write a Go program that will clean our data for a given interest rate
    (12% for our example). We will read in the data from a given file, parse the values
    using `encoding/csv`, and put the cleaned data in an output file called `clean_loan_data.csv`.
    During the cleaning of the data we will make use of the following minimum and
    maximum values, that we define as constants:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个Go程序，该程序将为我们给定利率（例如12%）的数据进行清理。我们将从指定的文件中读取数据，使用`encoding/csv`解析值，并将清理后的数据放入名为`clean_loan_data.csv`的输出文件中。在数据清理过程中，我们将使用以下最小和最大值，我们将它们定义为常量：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, the actual cleaning functionality is shown in the following code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，实际的清理功能如下所示：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Compiling this and running it confirms our desired output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行它确认了我们的预期输出：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Great! We have our data in the desired format. Now, let''s gain a little more
    intuition about our data by creating histograms for the FICO score and interest
    rate data and calculating summary statistics. We will utilize `github.com/kniren/gota/dataframe`
    to calculate summary statistics and `gonum.org/v1/plot` to generate histograms:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们的数据已经以所需的格式存在。现在，让我们通过创建FICO评分和利率数据的直方图以及计算摘要统计来对我们的数据有更多的直观了解。我们将使用`github.com/kniren/gota/dataframe`来计算摘要统计，并使用`gonum.org/v1/plot`来生成直方图：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Running this results in the following output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码将产生以下输出：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see that the average credit score is pretty high at *706.1* and there
    is a pretty good balance between the classes one and zero, as indicated by a mean
    near *0.5*. However, there appears to be more class zero examples (which corresponds
    to not receiving a loan with an interest rate of 12% or lower). Also, the `*.png`
    histogram graphs look as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，平均信用评分相当高，为 *706.1*，并且一和零类之间有一个相当好的平衡，这从接近 *0.5* 的平均值中可以看出。然而，似乎有更多的零类示例（这对应于没有以12%或以下利率获得贷款）。此外，`*.png`直方图图看起来如下：
- en: '![](img/5ef325e8-a76e-45db-a77f-2b727bbd76cb.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ef325e8-a76e-45db-a77f-2b727bbd76cb.png)'
- en: This confirms our suspicions about the balance between the classes and shows
    us that the FICO scores are skewed a bit to lower values.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这证实了我们对类别之间平衡的怀疑，并显示FICO评分略偏向较低值。
- en: Creating our training and test sets
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的训练和测试集
- en: 'Similar to our examples in the previous chapter, we need to split our data
    into a training and test set. We will once again use `github.com/kniren/gota/dataframe`
    to do this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一章中的示例类似，我们需要将我们的数据分为训练集和测试集。我们再次使用`github.com/kniren/gota/dataframe`来完成此操作：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Compiling and running this results in two files with our training and test
    examples:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行此代码将产生两个文件，包含我们的训练和测试示例：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Training and testing the logistic regression model
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和测试逻辑回归模型
- en: 'Now, let''s create a function that trains a logistic regression model. This
    function needs to perform the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个函数来训练逻辑回归模型。这个函数需要执行以下操作：
- en: Accept our FICO score data as an independent variable.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的FICO评分数据作为独立变量接受。
- en: Add an intercept to our model.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的模型中添加一个截距。
- en: Initialize and optimize the coefficients (or weights) of the logistic regression
    model.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化并优化逻辑回归模型的系数（或权重）。
- en: Return the optimized weights which define our trained model.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回定义我们的训练模型的优化权重。
- en: To optimize the coefficient/weights, we will use a technique called **stochastic
    gradient descent**. This technique will be covered in greater detail in the Appendix,
    *Algorithms/Techniques Related to Machine Learning.* For now, suffice it to say
    that we are trying to make predictions with some non-optimized weights, calculating
    an error for those weights, and then updating them iteratively to maximize the
    likelihood of making a correct prediction.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化系数/权重，我们将使用一种称为**随机梯度下降**的技术。这种技术将在附录*与机器学习相关的算法/技术*中更详细地介绍。现在，只需说我们正在尝试使用一些未优化的权重进行预测，计算这些权重的错误，然后迭代地更新它们以最大化正确预测的可能性。
- en: 'An implementation of this optimization is as follows. The function takes the
    following as input:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对这种优化的实现。该函数接受以下输入：
- en: '`features`: A pointer to a gonum `mat64.Dense` matrix. This matrix includes
    a column for any independent variable that we are using (FICO score, in our case)
    along with columns of 1.0s representing an intercept.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`features`：一个指向gonum `mat64.Dense`矩阵的指针。这个矩阵包括一个用于任何独立变量（在我们的例子中是FICO评分）的列，以及表示截距的1.0列。'
- en: '`labels`: A slice of floats including all of the class labels corresponding
    to our `features`.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`：包含所有对应于我们的`features`的类标签的浮点数切片。'
- en: '`numSteps`: A maximum number of iterations for the optimization.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numSteps`：优化的最大迭代次数。'
- en: '`learningRate`: An adjustable parameter that helps with the convergence of
    the optimization.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learningRate`：一个可调整的参数，有助于优化的收敛。'
- en: 'The function then outputs the optimized weights for the logistic regression
    model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后该函数输出逻辑回归模型的优化权重：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, this function is relatively compact and simple. This will keep
    our code readable and allow people on our team to quickly understand what is happening
    in our model without hiding things in a black box.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个函数相对紧凑且简单。这将使我们的代码易于阅读，并允许我们团队的人快速理解模型中的情况，而不会将事物隐藏在黑盒中。
- en: Despite the popularity of R and Python in machine learning, you can see that
    machine learning algorithms can be implemented quickly and compactly in Go. Moreover,
    these implementations immediately achieve a level of integrity that far surpasses
    naive implementations in other languages.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管R和Python在机器学习中的流行，您可以看到机器学习算法可以在Go中快速且紧凑地实现。此外，这些实现立即达到了远远超过其他语言中天真实现的完整性水平。
- en: 'To train our logistic regression model on our training dataset, we will parse
    our training file with `encoding/csv` and then supply the necessary parameters
    to our `logisticRegression` function. This process is as follows, along with some
    code to output our trained logistic formula to `stdout`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要在我们的训练数据集上训练我们的逻辑回归模型，我们将使用`encoding/csv`解析我们的训练文件，然后向`logisticRegression`函数提供必要的参数。这个过程如下，以及一些代码，将我们的训练好的逻辑公式输出到`stdout`：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Compiling and running this training functionality results in the following
    trained logistic regression formula:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行这个训练功能，得到以下训练好的逻辑回归公式：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can then utilize this formula directly to make predictions. Remember, however,
    that this model makes a prediction for the probability of getting the loan (at
    the interest rate of 12%). As such, we need to utilize a threshold for the probability
    in making predictions. For example, we can say that any *p* of *0.5+* will be
    deemed positive (class one, or getting the loan) and any lower *p* values will
    be deemed negative. This type of prediction is implemented in the following function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以直接使用这个公式进行预测。但是，请记住，这个模型预测的是获得贷款（利率为12%）的概率。因此，在做出预测时，我们需要使用概率的阈值。例如，我们可以说任何*p*大于或等于*0.5*的将被视为正值（类别一，或获得贷款），任何更低的*p*值将被视为负值。这种预测在以下函数中实现：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using this `predict` function, we can evaluate our trained logistic regression
    model using one of the evaluation metrics introduced earlier in the book. In this
    case, let''s use accuracy, as shown in the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个`predict`函数，我们可以使用书中前面介绍的评价指标之一来评估我们训练好的逻辑回归模型。在这种情况下，让我们使用准确率，如下面的代码所示：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Running this test on our data results in the following accuracy:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据上运行这个测试，得到的准确率如下：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Nice! 83% accuracy is not that bad for a machine learning model that we implemented
    in about 30 lines of Go. With this simple model, we were able to predict whether,
    given a certain credit score, loan applicants would be accepted for a loan with
    less than or equal to 12% interest. Not only that, we did it with real-world messy
    data from a real company.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！83%的准确率对于一个我们用大约30行Go语言实现的机器学习模型来说并不差。使用这个简单的模型，我们能够预测，给定一个特定的信用评分，贷款申请人是否会获得利率低于或等于12%的贷款。不仅如此，我们使用的是来自真实公司的真实世界混乱数据。
- en: k-nearest neighbors
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-最近邻
- en: Moving on from logistic regression, let's try our first non-regression model,
    **k-nearest neighbors** (**kNN**). kNN is also a simple classification model,
    and it's one of the easiest model algorithms to grasp. It follows on from the
    basic premise that if I want to classify a record, I should consider other similar
    records.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从逻辑回归转向，让我们尝试我们的第一个非回归模型，**k-最近邻**（**kNN**）。kNN也是一个简单的分类模型，并且是掌握起来最容易的模型算法之一。它遵循这样一个基本前提：如果我想对一条记录进行分类，我应该考虑其他类似的记录。
- en: kNN is implemented in multiple existing Go packages including `github.com/sjwhitworth/golearn`,
    `github.com/rikonor/go-ann`, `github.com/akreal/knn`, and `github.com/cdipaolo/goml`.
    We will be using the `github.com/sjwhitworth/golearn` implementation, which will
    serve as a great introduction to using `github.com/sjwhitworth/golearn` in general.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: kNN在多个现有的Go包中实现，包括`github.com/sjwhitworth/golearn`、`github.com/rikonor/go-ann`、`github.com/akreal/knn`和`github.com/cdipaolo/goml`。我们将使用`github.com/sjwhitworth/golearn`实现，这将作为使用`github.com/sjwhitworth/golearn`的绝佳介绍。
- en: Overview of kNN
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kNN概述
- en: As mentioned, kNN operates on the principle that we should classify records
    according to similar records. There are some details to be dealt with in defining
    similar. However, kNN does not have the complexity of parameters and options that
    come with many models.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine again that we have two classes *A* and *B*. This time, however, suppose
    that we are wanting to classify based on two features, *x[1]* and *x[2]*. Visually,
    this looks something like the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76976c71-4716-4ed4-8567-a855e8195ca8.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'Now, suppose we have a new point of data with an unknown class. This new point
    of data will sit somewhere in this space. kNN says that, to classify this new
    point, we should perform the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Find the *k* nearest point to the new point according to some measure of nearness
    (straight distance in this space of *x[1]* and *x[2]*, for example).
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine how many of those *k* nearest neighbors are of class *A* and how many
    are of class *B.*
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classify the new point as the dominant class among the *k* nearest neighbors.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, if we choose *k* to be four, this would look something as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a6bf21d-8fa5-46a6-aa44-dcd480c6e8e6.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Our mystery point has three *A* nearest neighbors and only one *B* nearest neighbor.
    Thus, we would classify the new mystery point as a class *A* point.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: There are many measures of similarity that you can use to determine the *k*
    nearest neighbors. The most common of these is the Euclidean distance*,* which
    is just the straight line distance from one point to the next in the space made
    up of your features (*x[1]* and *x[2]* in our example). Others include **Manhattan
    distance**, **Minkowski** **distance**, **cosine similarity**, and **Jaccard Similarity**.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: As with evaluation metrics, there are a whole host of ways that distance or
    similarity can be measured. When using kNN, you should look into the advantages
    and disadvantages of these measures and choose one that fits your use case and
    data. When in doubt, however, you can try starting with the Euclidean distance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: kNN assumptions and pitfalls
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Due to its simplicity, kNN does not have too many assumptions. However, there
    are some common pitfalls that you should be aware of as you apply kNN:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: kNN is evaluated lazily. By this, we mean that the distances or similarities
    are calculated when we need to make a prediction. There is not really anything
    to train or fit prior to making a prediction. This has some advantages, but the
    calculation and search over points can be slow when you have many data points.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of *k* is up to you, but you should put some formalism around choosing
    *k* and provide justification for the *k* that you choose. A common technique
    to choose *k* is just to search over a range of *k* values. You could, for example,
    start with *k = 2*. Then, you could start increasing *k*, and for each *k*, evaluate
    on a test set.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kNN does not take into consideration which features are more important than
    other features. Moreover, if the scale of certainty of your features is much larger
    than other features, this could unnaturally weight the importance of those larger
    features.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kNN没有考虑哪些特征比其他特征更重要。此外，如果你的特征的确定性尺度比其他特征大得多，这可能会不自然地增加这些较大特征的重要性。
- en: kNN example
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kNN示例
- en: 'For this, and the remaining examples in this chapter, we are going to solve
    a classic classification problem using a dataset about **iris flowers**. The dataset
    looks like this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一点，以及本章剩余的示例，我们将使用关于**鸢尾花**的数据集来解决一个经典的分类问题。数据集看起来是这样的：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first four columns are various measurements of iris flowers and the last
    column is a corresponding species label. The goal of this example will be to create
    a kNN classifier that is able to predict the species of an iris flower from a
    set of measurements. There are three species of flowers, or three classes, making
    this a multi-class classification (in contrast to the binary classification that
    we did with logistic regression).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 前四列是鸢尾花的各种测量值，最后一列是对应的物种标签。本例的目标是创建一个kNN分类器，能够从一组测量值中预测鸢尾花的物种。有三种花类，或三个类别，这使得这是一个多类别分类（与我们在逻辑回归中进行的二进制分类相反）。
- en: You may remember that we already profiled the iris dataset in detail in [Chapter
    2](5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml), *Matrices, Probability, and Statistics.*
    We will not reprofile the data here. However, it is still important that we have
    intuition about our data as we develop the kNN model. Make sure you flip back
    to [Chapter 2](5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml), *Matrices, Probability,
    and Statistics*, to remind yourself about the distributions of variables in this
    dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，我们在[第2章](5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml)，*矩阵、概率和统计学*中已经详细分析了鸢尾花数据集。我们在这里不会重新分析数据。然而，在我们开发kNN模型时，对数据有直观的了解仍然很重要。确保你翻回到[第2章](5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml)，*矩阵、概率和统计学*，以提醒自己关于这个数据集中变量分布的情况。
- en: We will be using `github.com/sjwhitworth/golearn` in this example. `github.com/sjwhitworth/golearn`
    implements a variety of machine learning models, including kNN and some others
    that we will explore shortly. `github.com/sjwhitworth/golearn` also implements
    cross-validation. We will take advantage of cross-validation here to perform training,
    testing, and validation, which is convenient and lets us avoid performing a manual
    split between training and test sets.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用`github.com/sjwhitworth/golearn`。`github.com/sjwhitworth/golearn`实现了多种机器学习模型，包括kNN和一些我们很快将要探索的其他模型。`github.com/sjwhitworth/golearn`还实现了交叉验证。我们将利用交叉验证在这里进行训练、测试和验证，这既方便又让我们避免了手动在训练集和测试集之间进行分割。
- en: 'To use any of `github.com/sjwhitworth/golearn`''s models, we must first convert
    the data into a `github.com/sjwhitworth/golearn` internal format called **instances**.
    For the iris data, we can do this as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`github.com/sjwhitworth/golearn`的任何模型，我们首先必须将数据转换为`github.com/sjwhitworth/golearn`内部格式，称为**实例**。对于鸢尾花数据，我们可以这样做：
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, initializing our kNN model and performing the cross-validation is quick
    and simple:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，初始化我们的kNN模型并进行交叉验证是快速且简单的：
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we can get the average accuracy across the five folds of the cross-validation
    and output that accuracy to `stdout`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以得到交叉验证的五次折叠的平均准确率，并将该准确率输出到`stdout`：
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Compiling and running all of this together gives the following output:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些编译并运行，会得到以下输出：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: After some benign logging from the package during the cross-validation, we can
    see that kNN (*k = 2*) was able to predict iris flower species with 95% accuracy!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证期间，从包中输出的良性日志显示，kNN（k = 2）能够以95%的准确率预测鸢尾花物种！
- en: The next step here would be to try this model out with a variety of different
    *k* values. Actually, it would be a good exercise to plot the accuracy versus
    *k* values to see which *k* value gives you the best performance.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步将是尝试使用不同的*k*值来测试这个模型。实际上，绘制准确率与*k*值的对比图，以查看哪个*k*值能给出最佳性能，将是一个很好的练习。
- en: Decision trees and random forests
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和随机森林
- en: Tree-based models are very different from the previous types of models that
    we have discussed, but they are widely utilized and very powerful. You can think
    about a **decision tree** model like a series of `if-then` statements applied
    to your data. When you train this type of model, you are constructing a series
    of control flow statements that eventually allow you to classify records.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are implemented in `github.com/sjwhitworth/golearn` and `github.com/xlvector/hector`,
    among others, and random forests are implemented in `github.com/sjwhitworth/golearn`,
    `github.com/xlvector/hector`, and `github.com/ryanbressler/CloudForest`, among
    others. We will utilize `github.com/sjwhitworth/golearn` again in our examples
    shown in the following section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Overview of decision trees and random forests
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, consider our classes *A* and *B*. In this case, suppose that we have
    one feature, *x[1],* that ranges from *0.0* to *1.0*, and we have another feature,
    *x[2]* that is categorical and can take on one of two values, *a[1]* and *a[2]*
    (this could be something like male/female or red/blue). A decision tree to classify
    a new data point might look something like the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c0fa849-b74d-4d02-a1c4-b8a767b82300.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: There are a variety of ways to choose how decision trees are constructed, split,
    and so on. One of the most common ways to determine how decision trees are constructed
    is using a quantity called **entropy**. This entropy-based approach is discussed
    in more detail in the *Appendix*, but, basically, we construct the tree and splits
    based on which features give us the most information about the problem we are
    solving. The more important features are prioritized higher on the tree.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: This prioritization of important features and a natural looking structure makes
    decision trees very interpretable. This makes decision trees important for applications
    in which you might have to explain your inferences (for compliance reasons, for
    example).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: However, a single decision tree can be unstable with respect to changes in your
    training data. In other words, the structure of the tree may change significantly
    with even small changes in your training data. This can be challenging to manage
    both operationally and cognitively, and this is one of the reasons why the **random
    forest** model was created.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: A random forest is a collection of decision trees that work together to make
    a prediction. Random forests are more stable when compared to single decision
    trees, and they are more robust against overfitting. In fact, this idea of combining
    models in an **ensemble** is prevalent throughout machine learning both to improve
    the performance of simple classifiers (such as decision trees) and to help prevent
    overfitting.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: To construct a random forest, we choose *N* random subsets of features and construct
    *N* separate decision trees based on these subsets. When making a prediction,
    we can then have each of these *N* decision trees make a prediction. To obtain
    a final prediction, we can take the majority vote of these *N* predictions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree and random forest assumptions and pitfalls
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tree-based approaches are non-statistical approaches without many of the assumptions
    that come along with things like regression. However, there are some pitfalls
    to keep in mind:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Single decision tree models can easily overfit to your data, especially if you
    do not limit the depth of the trees. Most implementations allow you to limit this
    depth via a parameter (or **prune** your decision trees). A pruning parameter
    will often allow you to remove sections of the tree that have little influence
    on the predictions, and, thus, reduce the overall complexity of the model.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we start talking about ensemble models, such as random forest, we are getting
    into models that are somewhat opaque. It's very hard to gain intuition about an
    ensemble of models and you have to treat it like a black box to some degree. Less
    interpretable models like these should be applied only when necessary.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although decision trees themselves are very computationally efficient, random
    forests can be very computationally inefficient depending on how many features
    you have and how many trees are in your random forest.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree example
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will again utilize the iris dataset for this example. You have already learned
    how to handle this dataset in `github.com/sjwhitworth/golearn`, and we can follow
    a similar pattern again. We will again use cross-validation. However, this time
    we will fit a decision tree model:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Compiling and running this decision tree model gives the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 94% accuracy this time. Slightly worse than our kNN model, but still very respectable.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Random forest example
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`github.com/sjwhitworth/golearn` also implements random forests. To utilize
    random forest in solving the iris problem, we simply swap our decision tree model
    for the random forests. We will need to tell the package how many trees we want
    to build and with how many randomly chosen features per tree.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'A sane default for the number of features per tree is the square root of the
    number of total features, which in our case would be two. We will see that this
    choice for our small dataset does not produce good results because we need all
    of the features to make a good prediction here. However, we will illustrate random
    forest with the sane default to see how it works:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Running this gives an accuracy that is worse than the single decision tree.
    If we change the number of features per tree back up to four, we will recreate
    the accuracy of the single decision tree. This means that every tree is being
    trained with the same information as the single decision tree, and thus produces
    the same results.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Random forest is likely to overkill here and cannot be justified with any performance
    gains, so it would be best to stick with the single decision tree. This single
    decision tree is also more interpretable and more efficient.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Naive bayes
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final model that we will cover here for classification is called **Naive
    bayes**. In [Chapter 2](5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml), *Matrices,
    Probability, and Statistics,* we discussed the Bayes rule, which forms the basis
    of this technique. Naive Bayes is a probability-based method like logistic regression,
    but its basic ideas and assumptions are different.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes is also implemented in `github.com/sjwhitworth/golearn`, which will
    allow us to easily try it out. However, there are a variety of other Go implementations
    including `github.com/jbrukh/bayesian`, `github.com/lytics/multibayes`, and `github.com/cdipaolo/goml`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Overview of naive bayes and its big assumption
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive bayes operates under one large assumption. This assumption says that the
    probability of the classes and the presence or absence of a certain feature in
    our dataset is independent of the presence or absence of other features in our
    dataset. This allows us to write a very simple formula for the probability of
    a certain class, given the presence or absence of certain features.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example to make this more concrete. Again, let''s say that we
    are trying to predict two classes of emails, *A* and *B* (spam and not spam, for
    example), based on words in the emails. Naive bayes would assume that the presence/absence
    of a certain word is independent of other words. If we make this assumption, the
    probability that a certain class contains certain words is proportional to all
    of the individual conditional probabilities multiplied together. Using this, the
    Bayes rule, some chain rules, and our independent assumptions, we can write the
    conditional probability of a certain class as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6a0ccd5-a99e-4a30-aecd-12ca929d01bd.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Everything on the right-hand side we can calculate by counting the occurrences
    of the features and labels in our training set, which is what is done when training
    the model. Predictions can then be made by stringing together chains of these
    probabilities.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Actually, in practice, a little trick is used to avoid having to string together
    a bunch of numbers that are close to zero. We can take the log of the probabilities,
    add these, and then take the exponential of the expression. This process is generally
    better in practice.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Naive bayes example
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Circling back to our loan dataset for a final time, we are going to use `github.com/sjwhitworth/golearn`
    to solve the same loan acceptance problem with Naive bayes. We will utilize the
    same training and test sets that we used in the logistic regression example. However,
    we need to convert the labels in the datasets to a binary classifier format used
    in `github.com/sjwhitworth/golearn`. We can write a simple function to perform
    this conversion:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once we have that, we can train and test our naive bayes model, as shown in
    the following code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Compiling and running this gives the following accuracy:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This is not quite as good as our from-scratch logistic regression. However,
    there is still some predictive power here. A good exercise would be to add some
    of the other features from the LendingClub dataset to this model, especially some
    of the categorical variables. This would likely improve the naive bayes result.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'General classification:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '`github.com/sjwhitworth/golearn` docs: [https://godoc.org/github.com/sjwhitworth/golearn](https://godoc.org/github.com/sjwhitworth/golearn)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a variety of classification models including logistic regression,
    k-nearest neighbors, decision trees, random forest, and naive bayes. In fact,
    we even implemented logistic regression from scratch. All of these models have
    their different strengths and weaknesses, which we have discussed. However, they
    should provide you with a good set of tools to start doing classification with
    Go.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss yet another type of machine learning called
    **clustering**. This is the first unsupervised technique that we will discuss,
    and we will try a few different approaches.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
