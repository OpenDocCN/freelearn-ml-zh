- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When many people think about machine learning or artificial intelligence, they
    probably first think about machine learning to solve classification problems.
    These are problems where we want to train a model to predict one of a finite number
    of distinct categories. For example, we may want to predict if a financial transaction
    is fraudulent or not fraudulent, or we may want to predict whether an image contains
    a hot dog, airplane, cat, and so on, or none of those things.
  prefs: []
  type: TYPE_NORMAL
- en: The categories that we try to predict could number from two to many hundreds
    or thousands. In addition, we could be making our predictions based on only a
    few attributes or many attributes. All of the scenarios arising from these combinations
    lead to a host of models with a corresponding host of assumptions, advantages,
    and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover some of these models in this chapter and later in the book, but
    there are many that we will skip for brevity. However, as with any of the problems
    that we tackle in this book, simplicity and integrity should be a major concern
    as we choose a type of model for our use cases. There are highly sophisticated
    and complicated models that solve certain problems very well, but these models
    are not necessary for many use cases. Applying simple and interpretable classification
    models should continue to be one of our goals.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding classification model jargon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with regression, classification problems come with their own set of jargon.
    There is some overlap with terms used in regression, but there are also some new
    terms specific to classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categories**, **labels**, or **classes**: These terms are used interchangeably
    to represent the various distinct choices for our prediction. For example, we
    could have a fraud class and a not fraud class, or we could have sitting, standing,
    running, and walking categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binary classification**: This type of classification is one with only two
    categories or classes, such as yes/no or fraud/not fraud.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class classification**: This type of classification is one with more
    than two classes, such as a classification trying to assign one of hot dog, airplane,
    cat, and so on, to an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labeled data** or **annotated data**: Real-world observations or records
    that have been paired with their corresponding class. For example, if we are predicting
    fraud via transaction time, this data would include a bunch of measured transaction
    times along with a corresponding label indicating whether they were fraudulent
    or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first classification model that we are going to explore is called **logistic
    regression**. As you can tell from the name, this method is based on a regression,
    which we discussed in more detail in the previous chapter. However, this particular
    regression uses a function that is particularly well suited to classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: This is also a simple and interpretable model, which makes it a great first
    choice when solving classification problems. There are a variety of existing Go
    packages that implement logistic regression for you, including `github.com/xlvector/hector`,
    `github.com/cdipaolo/goml`, and `github.com/sjwhitworth/golearn`. However, in
    our example, we will implement logistic regression from scratch, so that you can
    both form a full understanding of what goes into training a model and understand
    the simplicity of logistic regression. Further, in some cases, you may want to
    utilize a from-scratch implementation as illustrated in the following section
    to avoid extra dependencies in your code base.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say that we have two classes, *A* and *B,* that we are trying to predict.
    Let''s also suppose that we are trying to predict *A* or *B* based on a variable
    *x*. Classes *A* and *B* might look something like this when plotted against *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e025a17c-35a1-48f6-b99f-6acac1b654c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Although we could draw a line modeling this behavior, this is clearly not linear
    behavior and does not fit the assumptions of linear regression. The shape of the
    data is more of a step from one class to another class as a function of *x*. What
    we really need is some function that goes to and stays at *A* for low values of
    *x,* and goes to and stays at *B* for higher values of *x.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, we are in luck! There is such a function. The function is called the
    **logistic function**, and it gives logistic regression its name. It has the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00e1e244-3cb7-445a-aff7-af8b3d72f391.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Implemented in Go, this looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the logistic function with `gonum.org/v1/plot` to see what it looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this plotting code creates the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaac3a8d-b90c-421d-83d9-0505f845a417.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, this function has the step-like behavior that we are looking
    for to model the steps between classes *A* and *B* (imagining that *A* corresponds
    to *0.0* and *B* corresponds to *1.0*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Not only that, the logistic function has some really convenient properties
    that we can take advantage of while doing classification. To see this, let''s
    take a step back and consider how we might model *p*, the probability of one of
    the classes *A* or *B*, occurring. One way to do this would be to model the *log*
    of the **odds** **ratio**, *log(* *p / (1 - p) )* linearly, where the odds ratio
    tells us how the presence or absence of class *A* has an effect on the presence
    or absence of class *B*. The reason for using this strange *log* (referred to
    as **logit**) will make sense shortly, but for now, let''s just assume that we
    want to model this linearly as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12a20c36-d29d-4179-9462-60aae6534f4a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we take the exponential of this odds ratio, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8832e141-9447-4913-a625-f96e1ea61cd0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is what we get when we simplify the preceding equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08fee5a9-69d0-458a-8c82-14fd462b2640.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you look at the right-hand side of this equation, you will see that our
    logistic function has popped up. This equation then gives some formal underpinning
    to our assumption that the logistic function would be good for modeling the separation
    between two classes: *A* and *B*. If, for instance, we take *p* to be the probability
    of observing *B* and we fit the logistic function to our data, we could get a
    model that predicts the probability of *B* as a function of *x* (and thus one
    minus the probability of predicting *A*). This is pictured in the following plot,
    in which we have formalized our original plot of *A* and *B* and overlaid the
    logistic function that models probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed4153cb-34b8-4084-8435-3bbb0606c82a.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, creating a logistic regression model involves finding the logistic function
    that maximizes the number of observations that we can predict with the logistic
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Note that one advantage of logistic regression is that it remains simple and
    interpretable. However, the coefficients *m* and *b* in the model do not have
    the same interpretation as in linear regression. The coefficient *m* (or coefficients
    *m[1]*, *m[2]*, and so on, if we have multiple independent variables) has an exponential
    relationship with the odds ratio. Thus, if you have a coefficient *m* of *0.5*,
    this is related to the odds ratio via *exp(0.5 x)*. If we had two coefficients
    *exp(0.5 x[1] + 1.0 x[2])*, we could conclude that the odds of the modeled class
    for *x[1]* is *exp(0.5) = 1.65* when compared to *exp(1.0) = 2.72* for *x[2]*.
    In other words, we cannot directly compare the coefficients. We need to keep them
    in the context of the exponential.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression assumptions and pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Remember the long list of assumptions that applied to linear regression? Well,
    logistic regression is not limited by those same assumptions. However, there are
    still some important assumptions that we make when using logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear relationship with the log odds**: As we discussed earlier, underlying
    logistic regression is an assumption that we can model the log of the odds ratio
    with a line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoding of dependent variable**: When we set up our model earlier, we assumed
    that we were trying to predict the probability of *B*, where a probability of
    1.0 corresponded to a positive *B* example. Thus, we need to prepare our data
    with this type of encoding. This will be demonstrated in the following example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence of observations**: Each of the examples of *x* in our data must
    be independent. That is, we have to avoid things like including the same example
    several times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, some common pitfalls of logistic regression to keep in mind are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression can be more sensitive to outliers than other classification
    techniques. Keep this in mind and try to profile your data accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As logistic regression relies on an exponential function that never truly goes
    to *0.0* or *1.0* (except at +/- infinity), you may have very small degradations
    in your evaluation metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All being said, logistic regression is a fairly robust method that remains interpretable.
    It is a flexible model that should be at the top of your list when considering
    how you might solve classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset that we are going to use to illustrate logistic regression is data
    corresponding to loans issues by LendingClub. LendingClub publishes this data
    quarterly and it can be found in its original form at [https://www.lendingclub.com/info/download-data.action](https://www.lendingclub.com/info/download-data.action).
    We will work with a trimmed down and simplified version of this data (available
    in the code bundles that come with this book) including only two columns, `FICO.Range`
    (indicating a loan applicants credit score as given by Fair, Isaac and Company,
    or FICO) and `Interest.Rate` (indicating the interest rate of a loan granted to
    the loan applicant). The data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Our goal for this exercise will be to create a logistic regression model that
    will tell us, for a given credit score, if we can get a loan at or below a certain
    interest rate. For example, let's say that we are interested in getting an interest
    rate below 12%. Our model would tell us that we could (yes, or class one) or could
    not (no, class two) get a loan, given a certain credit score.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and profiling the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Looking at the preceding sample of the loan data, we can see that it is not
    exactly in the form that we need for our classification. Specifically, we need
    to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove non-numerical characters from the interest rate and FICO score columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode our interest rate into two classes for a given interest rate threshold.
    We will use *1.0* to represent our first class (yes, we can get the loan with
    that interest rate) and *0.0* to represent our second class (no, we cannot get
    the loan with that interest rate).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a single value for the FICO credit score. We are given a range of credit
    scores, but we need a single value. The average, minimum, or maximum score are
    natural choices and, in our example, we will use the minimum value (to be conservative).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, we are going to **standardize** our FICO scores (by subtracting
    the minimum score value from each score and then dividing by the score range).
    This will spread out our score values between *0.0* and *1.0*. We need to have
    a justification for this as it is making our data less readable. However, there
    is a good justification. We will be training our logistic regression using a gradient
    descent method that can perform better with normalized data. In fact, when running
    the same example with non-normalized data, there are convergence issues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s write a Go program that will clean our data for a given interest rate
    (12% for our example). We will read in the data from a given file, parse the values
    using `encoding/csv`, and put the cleaned data in an output file called `clean_loan_data.csv`.
    During the cleaning of the data we will make use of the following minimum and
    maximum values, that we define as constants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the actual cleaning functionality is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling this and running it confirms our desired output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! We have our data in the desired format. Now, let''s gain a little more
    intuition about our data by creating histograms for the FICO score and interest
    rate data and calculating summary statistics. We will utilize `github.com/kniren/gota/dataframe`
    to calculate summary statistics and `gonum.org/v1/plot` to generate histograms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the average credit score is pretty high at *706.1* and there
    is a pretty good balance between the classes one and zero, as indicated by a mean
    near *0.5*. However, there appears to be more class zero examples (which corresponds
    to not receiving a loan with an interest rate of 12% or lower). Also, the `*.png`
    histogram graphs look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ef325e8-a76e-45db-a77f-2b727bbd76cb.png)'
  prefs: []
  type: TYPE_IMG
- en: This confirms our suspicions about the balance between the classes and shows
    us that the FICO scores are skewed a bit to lower values.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our training and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similar to our examples in the previous chapter, we need to split our data
    into a training and test set. We will once again use `github.com/kniren/gota/dataframe`
    to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this results in two files with our training and test
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Training and testing the logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s create a function that trains a logistic regression model. This
    function needs to perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Accept our FICO score data as an independent variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an intercept to our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize and optimize the coefficients (or weights) of the logistic regression
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the optimized weights which define our trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To optimize the coefficient/weights, we will use a technique called **stochastic
    gradient descent**. This technique will be covered in greater detail in the Appendix,
    *Algorithms/Techniques Related to Machine Learning.* For now, suffice it to say
    that we are trying to make predictions with some non-optimized weights, calculating
    an error for those weights, and then updating them iteratively to maximize the
    likelihood of making a correct prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'An implementation of this optimization is as follows. The function takes the
    following as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '`features`: A pointer to a gonum `mat64.Dense` matrix. This matrix includes
    a column for any independent variable that we are using (FICO score, in our case)
    along with columns of 1.0s representing an intercept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels`: A slice of floats including all of the class labels corresponding
    to our `features`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numSteps`: A maximum number of iterations for the optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learningRate`: An adjustable parameter that helps with the convergence of
    the optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function then outputs the optimized weights for the logistic regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this function is relatively compact and simple. This will keep
    our code readable and allow people on our team to quickly understand what is happening
    in our model without hiding things in a black box.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the popularity of R and Python in machine learning, you can see that
    machine learning algorithms can be implemented quickly and compactly in Go. Moreover,
    these implementations immediately achieve a level of integrity that far surpasses
    naive implementations in other languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train our logistic regression model on our training dataset, we will parse
    our training file with `encoding/csv` and then supply the necessary parameters
    to our `logisticRegression` function. This process is as follows, along with some
    code to output our trained logistic formula to `stdout`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this training functionality results in the following
    trained logistic regression formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then utilize this formula directly to make predictions. Remember, however,
    that this model makes a prediction for the probability of getting the loan (at
    the interest rate of 12%). As such, we need to utilize a threshold for the probability
    in making predictions. For example, we can say that any *p* of *0.5+* will be
    deemed positive (class one, or getting the loan) and any lower *p* values will
    be deemed negative. This type of prediction is implemented in the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this `predict` function, we can evaluate our trained logistic regression
    model using one of the evaluation metrics introduced earlier in the book. In this
    case, let''s use accuracy, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this test on our data results in the following accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Nice! 83% accuracy is not that bad for a machine learning model that we implemented
    in about 30 lines of Go. With this simple model, we were able to predict whether,
    given a certain credit score, loan applicants would be accepted for a loan with
    less than or equal to 12% interest. Not only that, we did it with real-world messy
    data from a real company.
  prefs: []
  type: TYPE_NORMAL
- en: k-nearest neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Moving on from logistic regression, let's try our first non-regression model,
    **k-nearest neighbors** (**kNN**). kNN is also a simple classification model,
    and it's one of the easiest model algorithms to grasp. It follows on from the
    basic premise that if I want to classify a record, I should consider other similar
    records.
  prefs: []
  type: TYPE_NORMAL
- en: kNN is implemented in multiple existing Go packages including `github.com/sjwhitworth/golearn`,
    `github.com/rikonor/go-ann`, `github.com/akreal/knn`, and `github.com/cdipaolo/goml`.
    We will be using the `github.com/sjwhitworth/golearn` implementation, which will
    serve as a great introduction to using `github.com/sjwhitworth/golearn` in general.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of kNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, kNN operates on the principle that we should classify records
    according to similar records. There are some details to be dealt with in defining
    similar. However, kNN does not have the complexity of parameters and options that
    come with many models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine again that we have two classes *A* and *B*. This time, however, suppose
    that we are wanting to classify based on two features, *x[1]* and *x[2]*. Visually,
    this looks something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76976c71-4716-4ed4-8567-a855e8195ca8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, suppose we have a new point of data with an unknown class. This new point
    of data will sit somewhere in this space. kNN says that, to classify this new
    point, we should perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the *k* nearest point to the new point according to some measure of nearness
    (straight distance in this space of *x[1]* and *x[2]*, for example).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine how many of those *k* nearest neighbors are of class *A* and how many
    are of class *B.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classify the new point as the dominant class among the *k* nearest neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, if we choose *k* to be four, this would look something as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a6bf21d-8fa5-46a6-aa44-dcd480c6e8e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Our mystery point has three *A* nearest neighbors and only one *B* nearest neighbor.
    Thus, we would classify the new mystery point as a class *A* point.
  prefs: []
  type: TYPE_NORMAL
- en: There are many measures of similarity that you can use to determine the *k*
    nearest neighbors. The most common of these is the Euclidean distance*,* which
    is just the straight line distance from one point to the next in the space made
    up of your features (*x[1]* and *x[2]* in our example). Others include **Manhattan
    distance**, **Minkowski** **distance**, **cosine similarity**, and **Jaccard Similarity**.
  prefs: []
  type: TYPE_NORMAL
- en: As with evaluation metrics, there are a whole host of ways that distance or
    similarity can be measured. When using kNN, you should look into the advantages
    and disadvantages of these measures and choose one that fits your use case and
    data. When in doubt, however, you can try starting with the Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: kNN assumptions and pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Due to its simplicity, kNN does not have too many assumptions. However, there
    are some common pitfalls that you should be aware of as you apply kNN:'
  prefs: []
  type: TYPE_NORMAL
- en: kNN is evaluated lazily. By this, we mean that the distances or similarities
    are calculated when we need to make a prediction. There is not really anything
    to train or fit prior to making a prediction. This has some advantages, but the
    calculation and search over points can be slow when you have many data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of *k* is up to you, but you should put some formalism around choosing
    *k* and provide justification for the *k* that you choose. A common technique
    to choose *k* is just to search over a range of *k* values. You could, for example,
    start with *k = 2*. Then, you could start increasing *k*, and for each *k*, evaluate
    on a test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kNN does not take into consideration which features are more important than
    other features. Moreover, if the scale of certainty of your features is much larger
    than other features, this could unnaturally weight the importance of those larger
    features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kNN example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this, and the remaining examples in this chapter, we are going to solve
    a classic classification problem using a dataset about **iris flowers**. The dataset
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first four columns are various measurements of iris flowers and the last
    column is a corresponding species label. The goal of this example will be to create
    a kNN classifier that is able to predict the species of an iris flower from a
    set of measurements. There are three species of flowers, or three classes, making
    this a multi-class classification (in contrast to the binary classification that
    we did with logistic regression).
  prefs: []
  type: TYPE_NORMAL
- en: You may remember that we already profiled the iris dataset in detail in [Chapter
    2](5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml), *Matrices, Probability, and Statistics.*
    We will not reprofile the data here. However, it is still important that we have
    intuition about our data as we develop the kNN model. Make sure you flip back
    to [Chapter 2](5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml), *Matrices, Probability,
    and Statistics*, to remind yourself about the distributions of variables in this
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using `github.com/sjwhitworth/golearn` in this example. `github.com/sjwhitworth/golearn`
    implements a variety of machine learning models, including kNN and some others
    that we will explore shortly. `github.com/sjwhitworth/golearn` also implements
    cross-validation. We will take advantage of cross-validation here to perform training,
    testing, and validation, which is convenient and lets us avoid performing a manual
    split between training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use any of `github.com/sjwhitworth/golearn`''s models, we must first convert
    the data into a `github.com/sjwhitworth/golearn` internal format called **instances**.
    For the iris data, we can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, initializing our kNN model and performing the cross-validation is quick
    and simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can get the average accuracy across the five folds of the cross-validation
    and output that accuracy to `stdout`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running all of this together gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: After some benign logging from the package during the cross-validation, we can
    see that kNN (*k = 2*) was able to predict iris flower species with 95% accuracy!
  prefs: []
  type: TYPE_NORMAL
- en: The next step here would be to try this model out with a variety of different
    *k* values. Actually, it would be a good exercise to plot the accuracy versus
    *k* values to see which *k* value gives you the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees and random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tree-based models are very different from the previous types of models that
    we have discussed, but they are widely utilized and very powerful. You can think
    about a **decision tree** model like a series of `if-then` statements applied
    to your data. When you train this type of model, you are constructing a series
    of control flow statements that eventually allow you to classify records.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are implemented in `github.com/sjwhitworth/golearn` and `github.com/xlvector/hector`,
    among others, and random forests are implemented in `github.com/sjwhitworth/golearn`,
    `github.com/xlvector/hector`, and `github.com/ryanbressler/CloudForest`, among
    others. We will utilize `github.com/sjwhitworth/golearn` again in our examples
    shown in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of decision trees and random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Again, consider our classes *A* and *B*. In this case, suppose that we have
    one feature, *x[1],* that ranges from *0.0* to *1.0*, and we have another feature,
    *x[2]* that is categorical and can take on one of two values, *a[1]* and *a[2]*
    (this could be something like male/female or red/blue). A decision tree to classify
    a new data point might look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c0fa849-b74d-4d02-a1c4-b8a767b82300.png)'
  prefs: []
  type: TYPE_IMG
- en: There are a variety of ways to choose how decision trees are constructed, split,
    and so on. One of the most common ways to determine how decision trees are constructed
    is using a quantity called **entropy**. This entropy-based approach is discussed
    in more detail in the *Appendix*, but, basically, we construct the tree and splits
    based on which features give us the most information about the problem we are
    solving. The more important features are prioritized higher on the tree.
  prefs: []
  type: TYPE_NORMAL
- en: This prioritization of important features and a natural looking structure makes
    decision trees very interpretable. This makes decision trees important for applications
    in which you might have to explain your inferences (for compliance reasons, for
    example).
  prefs: []
  type: TYPE_NORMAL
- en: However, a single decision tree can be unstable with respect to changes in your
    training data. In other words, the structure of the tree may change significantly
    with even small changes in your training data. This can be challenging to manage
    both operationally and cognitively, and this is one of the reasons why the **random
    forest** model was created.
  prefs: []
  type: TYPE_NORMAL
- en: A random forest is a collection of decision trees that work together to make
    a prediction. Random forests are more stable when compared to single decision
    trees, and they are more robust against overfitting. In fact, this idea of combining
    models in an **ensemble** is prevalent throughout machine learning both to improve
    the performance of simple classifiers (such as decision trees) and to help prevent
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: To construct a random forest, we choose *N* random subsets of features and construct
    *N* separate decision trees based on these subsets. When making a prediction,
    we can then have each of these *N* decision trees make a prediction. To obtain
    a final prediction, we can take the majority vote of these *N* predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree and random forest assumptions and pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tree-based approaches are non-statistical approaches without many of the assumptions
    that come along with things like regression. However, there are some pitfalls
    to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Single decision tree models can easily overfit to your data, especially if you
    do not limit the depth of the trees. Most implementations allow you to limit this
    depth via a parameter (or **prune** your decision trees). A pruning parameter
    will often allow you to remove sections of the tree that have little influence
    on the predictions, and, thus, reduce the overall complexity of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we start talking about ensemble models, such as random forest, we are getting
    into models that are somewhat opaque. It's very hard to gain intuition about an
    ensemble of models and you have to treat it like a black box to some degree. Less
    interpretable models like these should be applied only when necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although decision trees themselves are very computationally efficient, random
    forests can be very computationally inefficient depending on how many features
    you have and how many trees are in your random forest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will again utilize the iris dataset for this example. You have already learned
    how to handle this dataset in `github.com/sjwhitworth/golearn`, and we can follow
    a similar pattern again. We will again use cross-validation. However, this time
    we will fit a decision tree model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this decision tree model gives the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 94% accuracy this time. Slightly worse than our kNN model, but still very respectable.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`github.com/sjwhitworth/golearn` also implements random forests. To utilize
    random forest in solving the iris problem, we simply swap our decision tree model
    for the random forests. We will need to tell the package how many trees we want
    to build and with how many randomly chosen features per tree.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A sane default for the number of features per tree is the square root of the
    number of total features, which in our case would be two. We will see that this
    choice for our small dataset does not produce good results because we need all
    of the features to make a good prediction here. However, we will illustrate random
    forest with the sane default to see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Running this gives an accuracy that is worse than the single decision tree.
    If we change the number of features per tree back up to four, we will recreate
    the accuracy of the single decision tree. This means that every tree is being
    trained with the same information as the single decision tree, and thus produces
    the same results.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest is likely to overkill here and cannot be justified with any performance
    gains, so it would be best to stick with the single decision tree. This single
    decision tree is also more interpretable and more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Naive bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final model that we will cover here for classification is called **Naive
    bayes**. In [Chapter 2](5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml), *Matrices,
    Probability, and Statistics,* we discussed the Bayes rule, which forms the basis
    of this technique. Naive Bayes is a probability-based method like logistic regression,
    but its basic ideas and assumptions are different.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes is also implemented in `github.com/sjwhitworth/golearn`, which will
    allow us to easily try it out. However, there are a variety of other Go implementations
    including `github.com/jbrukh/bayesian`, `github.com/lytics/multibayes`, and `github.com/cdipaolo/goml`.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of naive bayes and its big assumption
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive bayes operates under one large assumption. This assumption says that the
    probability of the classes and the presence or absence of a certain feature in
    our dataset is independent of the presence or absence of other features in our
    dataset. This allows us to write a very simple formula for the probability of
    a certain class, given the presence or absence of certain features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example to make this more concrete. Again, let''s say that we
    are trying to predict two classes of emails, *A* and *B* (spam and not spam, for
    example), based on words in the emails. Naive bayes would assume that the presence/absence
    of a certain word is independent of other words. If we make this assumption, the
    probability that a certain class contains certain words is proportional to all
    of the individual conditional probabilities multiplied together. Using this, the
    Bayes rule, some chain rules, and our independent assumptions, we can write the
    conditional probability of a certain class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6a0ccd5-a99e-4a30-aecd-12ca929d01bd.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Everything on the right-hand side we can calculate by counting the occurrences
    of the features and labels in our training set, which is what is done when training
    the model. Predictions can then be made by stringing together chains of these
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, in practice, a little trick is used to avoid having to string together
    a bunch of numbers that are close to zero. We can take the log of the probabilities,
    add these, and then take the exponential of the expression. This process is generally
    better in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Naive bayes example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Circling back to our loan dataset for a final time, we are going to use `github.com/sjwhitworth/golearn`
    to solve the same loan acceptance problem with Naive bayes. We will utilize the
    same training and test sets that we used in the logistic regression example. However,
    we need to convert the labels in the datasets to a binary classifier format used
    in `github.com/sjwhitworth/golearn`. We can write a simple function to perform
    this conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have that, we can train and test our naive bayes model, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and running this gives the following accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This is not quite as good as our from-scratch logistic regression. However,
    there is still some predictive power here. A good exercise would be to add some
    of the other features from the LendingClub dataset to this model, especially some
    of the categorical variables. This would likely improve the naive bayes result.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'General classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '`github.com/sjwhitworth/golearn` docs: [https://godoc.org/github.com/sjwhitworth/golearn](https://godoc.org/github.com/sjwhitworth/golearn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a variety of classification models including logistic regression,
    k-nearest neighbors, decision trees, random forest, and naive bayes. In fact,
    we even implemented logistic regression from scratch. All of these models have
    their different strengths and weaknesses, which we have discussed. However, they
    should provide you with a good set of tools to start doing classification with
    Go.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss yet another type of machine learning called
    **clustering**. This is the first unsupervised technique that we will discuss,
    and we will try a few different approaches.
  prefs: []
  type: TYPE_NORMAL
