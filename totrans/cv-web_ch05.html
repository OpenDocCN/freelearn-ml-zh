<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" style="font-size:1.200rem;">
<head><title>Chapter&#160;5.&#160;May JS Be with You! Control Your Browser with Motion</title>
<link rel="stylesheet" href="../Styles/style0001.css" type="text/css"/>
<meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
</head>
<body id="page">
<div class="chapter" title="Chapter&#160;5.&#160;May JS Be with You! Control Your Browser with Motion" id="aid-10DJ41"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"></a>Chapter&#160;5.&#160;May JS Be with You! Control Your Browser with Motion</h1>
</div>
</div>
</div>
<p>Imagine how exciting it would be to be able to control your browser using neither the keyboard nor mouse. There are many fields in computer science that tend to create a good human interface. One of those fields is Computer Vision. It provides outstanding methods that can help you to create something useful rapidly and you do not even need devices such as Kinect! The human interface in Computer Vision highly depends on object tracking. In the previous chapter, we already saw some object tracking examples, such as Camshift. Here, we will introduce more algorithms to play with. First, we will start with the basic tracking algorithms, which do not have any assumptions about an object from a previous frame. Next, we will move <a id="id210" class="indexterm"></a>on to <span class="strong"><strong>Head-coupled perspective</strong></span>; this is a technique that uses the head (or eye) position to simulate a 3D environment on a screen. This will be covered by the headtrackr library, which we have seen in the<a id="id211" class="indexterm"></a> previous chapter (<a class="ulink" href="https://github.com/auduno/headtrackr">https://github.com/auduno/headtrackr</a>). Finally, we will move on to the optical flow algorithms, with the help of which you can track many different objects in your application and even create programs which can be controlled by gestures. We will create an interesting example that uses that type of control. Here, we will introduce a new library (<a class="ulink" href="https://github.com/anvaka/oflow">https://github.com/anvaka/oflow</a>), which provides an excellent way to generate the<a id="id212" class="indexterm"></a> optical flow of an image. To track multiple points at once, we will use the JSFeat (<a class="ulink" href="http://inspirit.github.io/jsfeat/">http://inspirit.github.io/jsfeat/</a>) library. Let's get started!</p>
<p>We will cover the following topics in this chapter:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Basic tracking with tracking.js</li>
<li class="listitem">Controlling objects with head motion</li>
<li class="listitem">Optical flow for motion estimation</li>
</ul>
</div>
<div class="section" title="Basic tracking with tracking.js"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"></a>Basic tracking with tracking.js</h1>
</div>
</div>
</div>
<p>In this section, we will refresh our knowledge about object detection and create a sample project, which will show how object detection can be presented as object tracking. This is a relatively simple <a id="id213" class="indexterm"></a>topic but, in some cases, it can outperform other methods. Remember that object detection deals with detecting instances of objects, while tracking deals with locating moving objects over time. If you have only one unique object and you assume it will still be unique on the next frames, then you can calculate its location over time. In that case, we do not need to worry about tracking techniques because the tracking can be done using object detection. Here, we will focus mostly on the<a id="id214" class="indexterm"></a> tracking.js library (<a class="ulink" href="http://trackingjs.com">http://trackingjs.com</a>) since it provides the easiest way to do that.</p>
<div class="section" title="An example of an object tracking application"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec35"></a>An example of an object tracking application</h2>
</div>
</div>
</div>
<p>We already <a id="id215" class="indexterm"></a>mentioned the idea of using object detection as tracking. The idea is using a unique object on a scene. This can be a colored ball, your head, hand, or anything that has something special and can help to distinguish it from other objects in a scene. When you have that object, you just detect it on a frame and calculate its centroid to get its position.</p>
<p>To explore this concept, we will use the tracking.js library. We will draw a small ball with its center at the centroid of the detected object.</p>
<p>First, we will place the necessary tags for a video and the ball scene:</p>
<div class="informalexample"><pre class="programlisting">&lt;video id='video' width='640' height='480' preload autoplay&gt;
    &lt;source src="/path/to/your/video.mp4"&gt;
&lt;/video&gt;
&lt;canvas id='ballCanvas' width='640px' height='480px'&gt;&lt;/canvas&gt;</pre>
</div>
<p>We need to get the ball's context and its parameters to be able to draw on it:</p>
<div class="informalexample"><pre class="programlisting">var ballCanvas = document.getElementById('ballCanvas'),
        ballContext = ballCanvas.getContext('2d'),
        ballSceneW = ballCanvas.width,
        ballSceneH = ballCanvas.height;</pre>
</div>
<p>The object we want to track is just a simple colored object, so we will use <code class="literal">ColorTracker</code>. To remove noise, we set the minimum dimensions of a detected object to <code class="literal">20</code>:</p>
<div class="informalexample"><pre class="programlisting">var tracker = new tracking.ColorTracker(['yellow']);
tracker.setMinDimension(20);</pre>
</div>
<p>When we detect an object, we need to clear the context that contains the ball. In addition, we take the first detected object and use it to move the ball to a new position:</p>
<div class="informalexample"><pre class="programlisting">tracker.on('track', function (event) {   
    ballContext.clearRect(0, 0, ballSceneW, ballSceneH);
    if (event.data.length &gt; 0)
        move(event.data[0]);
});</pre>
</div>
<p>The <code class="literal">move</code> function is defined in the following code:</p>
<div class="informalexample"><pre class="programlisting">function move(rect) {
    ballContext.beginPath();
    var ballX = rect.x + rect.width / 2;
    var ballY = rect.y + rect.height / 2;
    ballContext.arc(ballX, ballY, 30, 0, 2 * Math.PI, false);
    ballContext.fillStyle = 'yellow';
    ballContext.fill();
    ballContext.stroke();
}</pre>
</div>
<p>As we can<a id="id216" class="indexterm"></a> see, we take the center of a detected rectangle and draw a ball using it.</p>
<p>To start the tracker, we just initiate it using the <code class="literal">track</code> function on the <code class="literal">&lt;video&gt;</code> tag:</p>
<div class="informalexample"><pre class="programlisting">tracking.track('#video', tracker);</pre>
</div>
<p>Here, we did not place the code for displaying the tracked object on a video because we already covered it in <a class="link" title="Chapter&#160;3.&#160;Easy Object Detection for Everyone" href="cv-web_ch03.html#aid-MSDG1">Chapter 3</a>, <span class="emphasis"><em>Easy Object Detection for Everyone</em></span>.</p>
<p>Here is what we get:</p>
<div class="mediaobject"><img src="../Images/image00129.jpeg" alt="An example of an object tracking application"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>The preceding image shows the detection of a bird in different frames and the estimated positions of the corresponding ball below the detections. As we can see, our main assumption about the object's (bird) uniqueness is observed. The only question is about the tracking—the bird's head is not perfectly detected.</p>
<p>We saw the<a id="id217" class="indexterm"></a> basic object detection, which is one step closer to tracking but it is not 100 percent. What we need to do is to remember the old coordinates of a previous object position and compute the motion vector. Let's move on!</p>
</div>
</div>
</div>


<div class="section" title="Controlling objects with the head motion"><div class="titlepage" id="aid-11C3M2"><div><div><h1 class="title"><a id="ch05lvl1sec33"></a>Controlling objects with the head motion</h1>
</div>
</div>
</div>
<p>Creating a human interface in Computer Vision is not an easy task. One of the exciting fields is Head-coupled perspective. This technique is used for rendering the scene on the screen, which<a id="id218" class="indexterm"></a> responds naturally to changes in the <a id="id219" class="indexterm"></a>head position of a viewer relative to the display. Simply put, the technology creates a 3D display without using any additional devices except the camera.</p>
<p>In the previous chapter, we saw how to track a head with the headtrackr library. It was done using the Camshift algorithm. In this section, we will explain the background of the function for Head-coupled perspective and how to use it in your projects to create an amazing human interface. To present a scene, the headtrackr library uses one of the most popular JavaScript libraries for <a id="id220" class="indexterm"></a>3D modeling—three.js (<a class="ulink" href="http://threejs.org">http://threejs.org</a>). We will begin with an explanation of the core function and then see an example of its usage.</p>
<div class="section" title="The Head-coupled perspective"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec36"></a>The Head-coupled perspective</h2>
</div>
</div>
</div>
<p>As we mentioned earlier, the headtrackr library works with three.js. The three.js library provides a<a id="id221" class="indexterm"></a> clear API and exceptional functionality for<a id="id222" class="indexterm"></a> 3D modeling. If you want, you can switch to another library, but in that case, you will need to rewrite some code from the headtrackr library.</p>
<p>The headtrackr library<a id="id223" class="indexterm"></a> provides a good explanation of the whole algorithm; you can refer to it at <a class="ulink" href="http://auduno.com/post/25125149521/head-tracking-with-webrtc">http://auduno.com/post/25125149521/head-tracking-with-webrtc</a>. To help you better understand the whole process, and in case you want to modify the functionality of the head tracking or use other libraries for the 3D modeling, here we will focus on the code of the core function.</p>
<p>The function itself is called:</p>
<div class="informalexample"><pre class="programlisting">headtrackr.controllers.three.realisticAbsoluteCameraControl</pre>
</div>
<p>To change the perspective of a scene appropriately, we need to know the movement of a head in three directions: <span class="emphasis"><em>X</em></span>, <span class="emphasis"><em>Y</em></span>, and <span class="emphasis"><em>Z</em></span>. To do this, we need to assume some scene attributes. The core assumption this method makes is that, at the first initialization of the algorithm, the distance between the user who sits in front of the screen and the camera is 60cm. In addition to this, the method defines the screen height, which is 20cm by default. Using these parameters, we can find <a id="id224" class="indexterm"></a>the <span class="strong"><strong>Field of View</strong></span> (<span class="strong"><strong>fov</strong></span>):</p>
<div class="mediaobject"><img src="../Images/image00130.jpeg" alt="The Head-coupled perspective"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>In the scene, we<a id="id225" class="indexterm"></a> will create a camera that will represent a<a id="id226" class="indexterm"></a> user (your head); we will call it <span class="strong"><strong>camera on a scene</strong></span>. Do not confuse it with the camera that is used to capture your face, for example, the laptop's camera.</p>
<p>The bigger the fov angle, the more objects fit on a screen and the further they appear to be. The Fov is computed in the first frame where a head is detected, taking into account previously mentioned assumptions.</p>
<p>There are several parameters that the headtrackr function uses:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>camera</strong></span>: This <a id="id227" class="indexterm"></a>is the <code class="literal">PerspectiveCamera</code> object from the three.js library.</li>
<li class="listitem"><span class="strong"><strong>scaling</strong></span>: This is<a id="id228" class="indexterm"></a> the vertical size of a screen in a 3D model. Basically, it scales the whole scene by the constant you define.</li>
<li class="listitem"><span class="strong"><strong>fixedPosition</strong></span>: This<a id="id229" class="indexterm"></a> is the initial position of a scene camera.</li>
<li class="listitem"><span class="strong"><strong>lookAt</strong></span>: This is <a id="id230" class="indexterm"></a>the position of the object you look at, which should be THREE.Vector3 which contains the 3 coordinates of an object.</li>
<li class="listitem"><span class="strong"><strong>params</strong></span>: This<a id="id231" class="indexterm"></a> includes a <code class="literal">screenHeight</code> field in centimeters. This is an optional parameter and it defines the height of your monitor.</li>
</ul>
</div>
<p>During the algorithm initialization, first, we set the scene camera position and the position of the object that this camera should point at:</p>
<div class="informalexample"><pre class="programlisting">camera.position.x = fixedPosition[0];
camera.position.y = fixedPosition[1];
camera.position.z = fixedPosition[2];
camera.lookAt(lookAt);</pre>
</div>
<p>Next, we define the screen width and height using the camera aspect ratio and scaling parameters:</p>
<div class="informalexample"><pre class="programlisting">var wh = screenHeight_cms * scaling;
var ww = wh * camera.aspect;</pre>
</div>
<p>To get a head position in each frame, we need to add a listener to <code class="literal">headtrackingEvent</code>:</p>
<div class="informalexample"><pre class="programlisting">document.addEventListener('headtrackingEvent', function (event) {
...
// function body
...
});</pre>
</div>
<p>The headtrackr library returns an estimated position of a head in each frame. The event contains the <code class="literal">x</code>, <code class="literal">y</code>, and <code class="literal">z</code> fields.</p>
<p>Since our<a id="id232" class="indexterm"></a> camera represents our head position, to <a id="id233" class="indexterm"></a>update its parameters, we need to change the position of the camera with respect to event data; do not forget about scaling:</p>
<div class="informalexample"><pre class="programlisting">camera.position.x = fixedPosition[0] + event.x * scaling;
camera.position.y = fixedPosition[1] + event.y * scaling;
camera.position.z = fixedPosition[2] + event.z * scaling;</pre>
</div>
<p>You need to keep an object in the center of the screen. To do so, the method sets the view offset using the <code class="literal">setViewOffset</code> method. The first two parameters define the size of the whole view, the last four parameters are the parameters of a view rectangle:</p>
<div class="informalexample"><pre class="programlisting">var xOffset = event.x &gt; 0 ? 0 : -event.x * 2 * scaling;
var yOffset = event.y &lt; 0 ? 0 : event.y * 2 * scaling;
camera.setViewOffset(ww + Math.abs(event.x * 2 * scaling), wh + Math.abs(event.y * 2 * scaling), xOffset, yOffset, ww, wh);</pre>
</div>
<p>The last attribute we want to update is the field of view, for which we use the <code class="literal">atan2</code> function. It returns the result from <code class="literal">-PI</code> to <code class="literal">PI</code> in radians; we need to convert it to degrees and multiply it by <code class="literal">2</code>, since we use only half of a screen in our computation:</p>
<div class="informalexample"><pre class="programlisting">camera.fov = Math.atan2(wh / 2 + Math.abs(event.y * scaling), Math.abs(event.z * scaling)) * 180 * 2 / Math.PI;</pre>
</div>
<p>After this, we update the camera parameters:</p>
<div class="informalexample"><pre class="programlisting">camera.updateProjectionMatrix();</pre>
</div>
<p>As we saw, all we need to do is to work with the scene camera. If you want modify the code or use another library, it should not be a problem for you now.</p>
</div>
<div class="section" title="Controlling a simple box"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec37"></a>Controlling a simple box</h2>
</div>
</div>
</div>
<p>The example which is provided by the headtrackr library uses an old version of three.js but the good <a id="id234" class="indexterm"></a>point is that it can be applied to new versions too! We will follow the cube example, which is available at <a class="ulink" href="http://threejs.org/examples/#canvas_geometry_cube">http://threejs.org/examples/#canvas_geometry_cube</a>. You can copy and paste the whole code from there; we<a id="id235" class="indexterm"></a> will do only basic modifications.</p>
<p>First, you need to update the scripts section and add the headtrackr library:</p>
<div class="informalexample"><pre class="programlisting">&lt;script src="js/three.min.js"&gt;&lt;/script&gt;
&lt;script src="js/Projector.js"&gt;&lt;/script&gt;
&lt;script src="js/CanvasRenderer.js"&gt;&lt;/script&gt;
&lt;script src="js/headtrackr.js"&gt;&lt;/script&gt;</pre>
</div>
<p>To track your face, you will need to define the video and canvas tags, which will be used by the headtrackr library:</p>
<div class="informalexample"><pre class="programlisting">&lt;canvas id="compare" width="320" height="240" style="display:none"&gt;&lt;/canvas&gt;
&lt;video id="vid" autoplay loop&gt;&lt;/video&gt;</pre>
</div>
<p>Far plane of a scene camera in the three.js example is too close for us, we better set it a bit further away:</p>
<div class="informalexample"><pre class="programlisting">camera.far = 10000;</pre>
</div>
<p>Initialization of the tracking process is done by the function that we reviewed in the previous section; take a look at the third parameter of a function below - camera position. The cube location in the example is <code class="literal">[0, 150, 0]</code> and its dimensions are 200 pixels. We set the camera initialization position at the cube plane:</p>
<div class="informalexample"><pre class="programlisting">headtrackr.controllers.three.realisticAbsoluteCameraControl(camera, 20, [0, 150, 100], cube.position);</pre>
</div>
<p>Next, we create a tracker:</p>
<div class="informalexample"><pre class="programlisting">var htracker = new headtrackr.Tracker();</pre>
</div>
<p>In the previous section, we reviewed the parameters that can be used while you track a face. Now, let's see what you can use for the head position estimation:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>cameraOffset</strong></span>: This is the distance from your laptop camera to the center of a screen, which is 11.5cm (half of the height of a regular laptop screen) by default.</li>
<li class="listitem"><span class="strong"><strong>fov</strong></span>: This is the horizontal field of view used by the camera in degrees. By default, the algorithm automatically estimates this.</li>
</ul>
</div>
<p>Now, we get the video and canvas on the JavaScript side. Then, we get the initialization and start the tracker:</p>
<div class="informalexample"><pre class="programlisting">var videoInput = document.getElementById('vid');
var canvasInput = document.getElementById('compare');
videoInput.style.position = 'absolute';
videoInput.style.top = '50px';
videoInput.style.zIndex = '100001';
videoInput.style.display = 'block';
htracker.init(videoInput, canvasInput);
htracker.start();</pre>
</div>
<p>Here is a rough<a id="id236" class="indexterm"></a> sketch of what you will see while using the application:</p>
<div class="mediaobject"><img src="../Images/image00131.jpeg" alt="Controlling a simple box"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>Keep in mind that when you move your head to the left, the camera will display the movement to the right. You can move your head in any direction, the only issue with that method is that it does not calculate the position of your eyes and because of this, the image will not be as perfect as a 3D model. This can be solved using more advanced techniques that involve tracking eyes, but in that case, the performance will not be in real-time for JavaScript applications.</p>
</div>
</div>


<div class="section" title="Optical flow for motion estimation"><div class="titlepage" id="aid-12AK82"><div><div><h1 class="title"><a id="ch05lvl1sec34"></a>Optical flow for motion estimation</h1>
</div>
</div>
</div>
<p>We saw how<a id="id237" class="indexterm"></a> to track different objects in a scene and how<a id="id238" class="indexterm"></a> to make a human interface using them, but we did not see a more generalized approach. When an object changes its position, it moves through a scene and it is interesting to estimate the overall movement of the scene. Here, we will introduce the concept of optical flow, and will see how to use it for object tracking. In the first part, we will focus on the theory and then present two<a id="id239" class="indexterm"></a> wonderful examples of the optical flow usage. Finally, we <a id="id240" class="indexterm"></a>will create a simple gesture application.</p>
<div class="section" title="The Lucas-Kanade optical flow"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec38"></a>The Lucas-Kanade optical flow</h2>
</div>
</div>
</div>
<p>There are<a id="id241" class="indexterm"></a> many definitions of optical flow, the main one is: it <a id="id242" class="indexterm"></a>is the change in structured intensities of an<a id="id243" class="indexterm"></a> image due to relative motion between the eyeball (camera) and the scene (<a class="ulink" href="http://www.scholarpedia.org/article/Optic_flow">http://www.scholarpedia.org/article/Optic_flow</a>). According to another definition, it is the distribution of the apparent velocities of objects in an image (<a class="ulink" href="http://www.mathworks.com/discovery/optical-flow.html">http://www.mathworks.com/discovery/optical-flow.html</a>). To get the idea, look at the following image:</p>
<div class="mediaobject"><img src="../Images/image00132.jpeg" alt="The Lucas-Kanade optical flow"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>Here, we are just moving a box and the arrows in the third picture show this movement. Simply put, the optical flow shows the displacement of objects. It can be used not only for object tracking, but also for video compression and stabilization. Furthermore, you can get a structure of a scene using the optical flow. For example, if you record a still environment with a moving camera, the objects that are closer to the camera change their destination faster than objects that are far from the camera.</p>
<p>The optical flow can be computed in many ways. The basic assumption of optical flow algorithms is that the object intensities of neighboring frames do not change rapidly. The most popular method is the Lucas-Kanade method. In addition to the previous assumption, it states that the displacement of objects in nearby frames is not large. Moreover, the method takes an <span class="emphasis"><em>NxN</em></span> patch, typically 3 x 3, around each pixel and it assumes that the motion of all these pixels is the same. Using these assumptions and the knowledge of changes in pixel intensities (gradient) around each pixel of the patch, the algorithm calculates its displacement. The changes in intensities are computed in <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> dimensions and, in time. Here, by time, we mean the difference between the previous and a current frame.</p>
<p>It's only a 3x 3 patch? What should we do with the fast moving objects? This problem can be solved using image pyramids. In this case, we will downsample an image and look for the same movement as that of a 3 x 3 patch, but at a lower resolution.</p>
<p>The other improvement is the iterative Lucas-Kanade method. After getting a flow vector for each pixel, we move pixels by those vectors and try to match the previous and current images. In an ideal situation, those images would be matched, but with real videos there  might be errors due to changes in pixels brightness. To avoid an error, we reiterate the process before we get a small error or<a id="id244" class="indexterm"></a> exceed the maximum number of iterations.</p>
<p>We discussed the<a id="id245" class="indexterm"></a> theoretical part, now let's move to the two amazing libraries that can provide the implementation of the optical flow. They can be used for different purposes. The core of both libraries is the Lucas-Kanade method.</p>
</div>
<div class="section" title="Optical flow map with oflow"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec39"></a>Optical flow map with oflow</h2>
</div>
</div>
</div>
<p>We start with a small library—oflow (<a class="ulink" href="https://github.com/anvaka/oflow">https://github.com/anvaka/oflow</a>). This is a simple library that just calculates<a id="id246" class="indexterm"></a> displacement vectors of each patch and returns the overall movement of a scene. We will use this movement to control the ball that we already <a id="id247" class="indexterm"></a>used in this chapter. Unfortunately, the library does not use an image pyramid to calculate optical flow and because of that, it is<a id="id248" class="indexterm"></a> better suited for getting the whole scene movement than object tracking.</p>
<p>We start by defining the library in our project:</p>
<div class="informalexample"><pre class="programlisting">&lt;script src="js/oflow.js"&gt;&lt;/script&gt;</pre>
</div>
<p>As we did previously, we create a video input and a ball canvas. In addition to this, we add a canvas for displaying the optical flow map with <code class="literal">id='flow'</code>:</p>
<div class="informalexample"><pre class="programlisting">&lt;div&gt;
    &lt;video id='videoOut' width='640' height='360' autoplay&gt;
        &lt;source src="/path/to/your/video.mp4"&gt;
    &lt;/video&gt;
    &lt;canvas id='ballScene' width='320' height='190'&gt;&lt;/canvas&gt;
&lt;/div&gt;
&lt;canvas id='flow' width='640' height='360'&gt;&lt;/canvas&gt;</pre>
</div>
<p>Next, we define an object that will be used for optical flow calculation. You can create it not only for a video as shown here, but also for a web camera (<code class="literal">WebFlow</code>) and a canvas (<code class="literal">CanvasFlow</code>). The <code class="literal">zoneSize</code> variable defines the half dimension of a patch, which is set to <code class="literal">8</code> by default:</p>
<div class="informalexample"><pre class="programlisting">var zoneSize = 8,
        videoElement = document.getElementById('videoOut');
var webCamFlow = new oflow.VideoFlow(videoElement, zoneSize),</pre>
</div>
<p>Here is a short example of what we receive in the end—the video on the left and the directions of optical flow on the right:</p>
<div class="mediaobject"><img src="../Images/image00133.jpeg" alt="Optical flow map with oflow"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>Each arrow shows the movement direction of a patch, of course, there is some noise, but most of the directions show the correct result. How do we receive the result?</p>
<p>After the<a id="id249" class="indexterm"></a> computation is done for each frame, we <a id="id250" class="indexterm"></a>need to handle the result. In the following, we receive directions for each patch. Then, we draw arrows that point to the direction of a patch displacement; we multiply that displacement by four so we can see the result in a better manner. You can choose any other multiplier, since it is used only for displaying the optical flow and not for the actual calculation:</p>
<div class="informalexample"><pre class="programlisting">webCamFlow.onCalculated(function (direciton) {
    flowContext.clearRect(0, 0, sceneWidth, sceneHeight);
    for (var i = 0; i &lt; direciton.zones.length; ++i) {
        var zone = direciton.zones[i];
        drawArrow(flowContext, zone, {x: zone.x + zone.u * 4, y: zone.y + zone.v * 4}, 2);
    }</pre>
</div>
<p>To calculate the overall displacement, the library just adds together all the vectors. Using the common vector, we draw a ball on its context. If the ball exceeds the screen dimensions, we draw it on the opposite side. The overall direction is returned using the <code class="literal">u</code> and <code class="literal">v</code> fields of the result:</p>
<div class="informalexample"><pre class="programlisting">    ballContext.clearRect(0, 0, ballSceneW, ballSceneH);
    ballContext.beginPath();
    ballX -= direciton.u;
    ballY += direciton.v;
    if (ballX &lt; 0) {
        ballX = ballSceneW;
    }
    if (ballX &gt; ballSceneW) {
        ballX = 0;
    }
    if (ballY &lt; 0) {
        ballY = ballSceneH;
    }
    if (ballY &gt; ballSceneH) {
        ballY = 0;
    }
    ballContext.arc(ballX, ballY, 10, 0, 2 * Math.PI, false);
    ballContext.fillStyle = 'yellow';
    ballContext.fill();
    ballContext.stroke();
});</pre>
</div>
<p>To start<a id="id251" class="indexterm"></a> the computational process, just call the following <a id="id252" class="indexterm"></a>function:</p>
<div class="informalexample"><pre class="programlisting">webCamFlow.startCapture();</pre>
</div>
<p>After several frames of the video, we get the following result:</p>
<div class="mediaobject"><img src="../Images/image00134.jpeg" alt="Optical flow map with oflow"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>This is where the ball placement is in the first and last frames, respectively:</p>
<div class="mediaobject"><img src="../Images/image00135.jpeg" alt="Optical flow map with oflow"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>Using this<a id="id253" class="indexterm"></a> library, it is easy to create a simple game<a id="id254" class="indexterm"></a> that is controlled by gestures, for example. The library usage is not limited to that and you can create something different very fast.</p>
</div>
<div class="section" title="Track points with JSFeat"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec40"></a>Track points with JSFeat</h2>
</div>
</div>
</div>
<p>The JSFeat library extends the functionality of optical flow and it can even track image points. You <a id="id255" class="indexterm"></a>can use those points to track objects and control your browser. The implementation of optical flow in JSFeat library uses the iterative Lucas-Kanade method with pyramids and the result it provides is very smooth.</p>
<p>In JSFeat, to work with a video, we need to include an additional JavaScript file, which is provided by this library:</p>
<div class="informalexample"><pre class="programlisting">&lt;script src="js/compatibility.js"&gt;&lt;/script&gt;</pre>
</div>
<p>Next, we define a video to be processed and a canvas for displaying the content:</p>
<div class="informalexample"><pre class="programlisting">&lt;video id="vid" width="640" height="360" autoplay style="display: none"&gt;
    &lt;source src="/path/to/your/video.mp4"/&gt;
&lt;/video&gt;
&lt;div&gt;
    &lt;canvas id="canvas" width="640" height="360"&gt;&lt;/canvas&gt;
&lt;/div&gt;</pre>
</div>
<p>There are a lot of variables that need to be defined:</p>
<div class="informalexample"><pre class="programlisting">var video = document.getElementById('vid');
var canvas = document.getElementById('canvas');
var context, canvasWidth, canvasHeight;
var curr_pyr, prev_pyr, count, status, prev_xy, curr_xy;</pre>
</div>
<p>The last row, from left to right shows: current image pyramid, image pyramid from the previous level, number of points that are tracked, and status of points. The status indicates whether a point has its representation on a new frame; if there is no such point, then the method assumes that the tracking of this point was lost and it is removed from the tracking process. The last two variables contain the point coordinates of the previous and current frames.</p>
<div class="note" title="Note"><h3 class="title"><a id="note02"></a>Note</h3>
<p>We do not provide functions to select original points here, but you can see them in the <a id="id256" class="indexterm"></a>JSFeat example:</p>
<p>
<a class="ulink" href="https://github.com/inspirit/jsfeat/blob/gh-pages/sample_oflow_lk.html">https://github.com/inspirit/jsfeat/blob/gh-pages/sample_oflow_lk.html</a>.</p>
</div>
<p>The following function initializes all necessary variables:</p>
<div class="informalexample"><pre class="programlisting">function init(videoWidth, videoHeight) {
    canvasWidth = canvas.width;
    canvasHeight = canvas.height;
    context = canvas.getContext('2d');
    context.fillStyle = "rgb(0,255,0)";
    curr_pyr = new jsfeat.pyramid_t(3);
    prev_pyr = new jsfeat.pyramid_t(3);
    curr_pyr.allocate(canvasWidth, canvasHeight, jsfeat.U8C1_t);
    prev_pyr.allocate(canvasWidth, canvasHeight, jsfeat.U8C1_t);
    count = 0;
    status = new Uint8Array(100);
    prev_xy = new Float32Array(100 * 2);
    curr_xy = new Float32Array(100 * 2);
}</pre>
</div>
<p>To start <a id="id257" class="indexterm"></a>getting video frames, we need to call the following function:</p>
<div class="informalexample"><pre class="programlisting">compatibility.requestAnimationFrame(process);</pre>
</div>
<p>It uses the <code class="literal">process</code> function to work with each video frame:</p>
<div class="informalexample"><pre class="programlisting">function process() {
    compatibility.requestAnimationFrame(process);
    if (video.readyState === video.HAVE_ENOUGH_DATA) {
        context.drawImage(video, 0, 0, canvasWidth, canvasHeight);
        var imageData = context.getImageData(0, 0, canvasWidth, canvasHeight);</pre>
</div>
<p>We copy the points and pyramid variables from the previous frame with the <code class="literal">curr_</code> prefix to variables with the <code class="literal">prev_ </code>prefix:</p>
<div class="informalexample"><pre class="programlisting">        var _points = prev_xy;
        prev_xy = curr_xy;
        curr_xy = _points;
        var _pyr = prev_pyr;
        prev_pyr = curr_pyr;
        curr_pyr = _pyr;</pre>
</div>
<p>Next, we compute an image pyramid for the current frame:</p>
<div class="informalexample"><pre class="programlisting">        jsfeat.imgproc.grayscale(imageData.data, canvasWidth, canvasHeight, curr_pyr.data[0]);
        curr_pyr.build(curr_pyr.data[0], true);</pre>
</div>
<p>To call a function for optical flow, we introduce four more variables that this function needs. The first variable is <code class="literal">win_size</code>, which is the size of a patch for searching the flow vector; the <code class="literal">max_iter</code> variable is the maximum number of iterations; the <code class="literal">eps</code> variable is the algorithm, which stops updating a point when the movement is less than <code class="literal">eps</code>; and the <code class="literal">min_eigen_threshold</code> variable, which is the threshold for removing bad points. The <code class="literal">process</code> function computes new point coordinates on a new frame. After this, we call the <code class="literal">prune_oflow_points</code> function. If you continue a video, you can probably loose some points on a future frame. In that case, they will not be tracked anymore and will be removed from the <code class="literal">curr_xy</code> variable by this function:</p>
<div class="informalexample"><pre class="programlisting">        var win_size = 20;
        var max_iter = 30;
        var eps = 0.01;
        var min_eigen_threshold = 0.001;
        jsfeat.optical_flow_lk.track(prev_pyr, curr_pyr, prev_xy, curr_xy, count, win_size, max_iter, status, eps, min_eigen_threshold);
        prune_oflow_points(context);
    }
}</pre>
</div>
<p>Here is the result we received:</p>
<div class="mediaobject"><img src="../Images/image00136.jpeg" alt="Track points with JSFeat"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>As we can<a id="id258" class="indexterm"></a> see, all points were successfully tracked; this is an ideal example, where the object was moving smoothly. In many cases, points can be tracked correctly, especially if a video is not edited.</p>
<p>The functionality of the library provides an excellent opportunity to track an object by defining its unique points, for example, you can predefine those points using FAST corner detection from <a class="link" title="Chapter&#160;3.&#160;Easy Object Detection for Everyone" href="cv-web_ch03.html#aid-MSDG1">Chapter 3</a>, <span class="emphasis"><em>Easy Object Detection for Everyone</em></span>. In addition to that, you can stabilize a video in real time and do other amazing things with it.</p>
</div>
<div class="section" title="Zooming with gestures"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec41"></a>Zooming with gestures</h2>
</div>
</div>
</div>
<p>What if we <a id="id259" class="indexterm"></a>want to extend the functionality<a id="id260" class="indexterm"></a> a bit? Suppose we want to add a simple zooming feature to our website. We can use the optical flow for that.</p>
<p>To start, we create our content tag we want to zoom with the following style:</p>
<div class="informalexample"><pre class="programlisting">#content {
    position: absolute;
    margin: auto;
    top: 0;
    right: 0;
    bottom: 0;
    left: 0;
    width: 100px;
    height: 100px;
    line-height: 100px;
    font-size: 10pt;
    text-align: center;
    vertical-align: middle;
}
&lt;div id="content"&gt;Content&lt;/div&gt;</pre>
</div>
<p>To use a webcam with JSFeat, run the following function from compatibility.js, which simply initializes your camera:</p>
<div class="informalexample"><pre class="programlisting">compatibility.getUserMedia({video: true}, function (stream) {
    try {
        video.src = compatibility.URL.createObjectURL(stream);
    } catch (error) {
        video.src = stream;
    }
    setTimeout(function () {
        video.play();
    }, 500);
}, function (error) {
});</pre>
</div>
<p>For zooming, we need only two points. So, after receiving the result from the optical flow algorithm, we check whether there are two points and if so, we call the zoom method:</p>
<div class="informalexample"><pre class="programlisting">jsfeat.optical_flow_lk.track(prev_pyr, curr_pyr, prev_xy, curr_xy, count, win_size, max_iter, status, eps, min_eigen_threshold);
if (count == 2)
    zoom(prev_xy, curr_xy);</pre>
</div>
<p>The method itself is very simple. We save the original size and change it based on the information we receive from the points of optical flow. We check the distance between two points and if it changed, we change the <code class="literal">&lt;div&gt;</code> content with respect to it:</p>
<div class="informalexample"><pre class="programlisting">var size = 100.0;
var content = document.getElementById('content');
function zoom(prev_xy, curr_xy) {
    var prev_d = dist2(prev_xy);
    var curr_d = dist2(curr_xy);
    size = Math.round(size * Math.sqrt(curr_d / prev_d));
    content.style.width = size + 'px';
    content.style.height = size + 'px';
    content.style['line-height'] = size + 'px';
    content.style['font-size'] = (size / 10) + 'pt';
}

function dist2(arr) {
    return Math.pow(arr[0] - arr[2], 2) + Math.pow(arr[1] - arr[3], 2);
}</pre>
</div>
<p>Here is an <a id="id261" class="indexterm"></a>example of zooming, where <a id="id262" class="indexterm"></a>we used the Canny edge detector in addition to the whole process:</p>
<div class="mediaobject"><img src="../Images/image00137.jpeg" alt="Zooming with gestures"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>There is no function to find your fingers on a video, so you need to select them before using the zoom function. If you want to find them by yourself, it is all in your hands! Probably, you could create a new era in web browser user experience.</p>
</div>
</div>


<div class="section" title="Summary" id="aid-1394Q1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec35"></a>Summary</h1>
</div>
</div>
</div>
<p>This is the last chapter of this book. Here, we saw examples that aggregate many techniques from the previous chapters. We covered object detection and tracking by color, which you can use to create your first tracking application very quickly. We explored the power of Head-coupled perspective, which is a new way to present the content on your websites in a fresh manner or create funny browser games with a human interface. In addition to that, the optical flow provides many extensions to this field too. It provides you with an excellent way to track points and objects. Moreover, now you can create a simple application that uses gestures to zoom the web content. The usage of optical flow is not limited to that and is very flexible, and it can be combined with many techniques.</p>
</div>
</body>
</html>