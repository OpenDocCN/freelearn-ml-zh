<html><head></head><body>
		<div id="_idContainer314">
			<h1 id="_idParaDest-69"><a id="_idTextAnchor074"/><em class="italic">Chapter 8</em>: Hyperparameter Tuning via Hyperopt</h1>
			<p><strong class="bold">Hyperopt</strong> is an optimization package in Python that provides several implementations of hyperparameter tuning methods, including <strong class="bold">Random Search</strong>, <strong class="bold">Simulated Annealing</strong> (<strong class="bold">SA</strong>), <strong class="bold">Tree-Structured Parzen Estimators</strong> (<strong class="bold">TPE</strong>), and <strong class="bold">Adaptive TPE</strong> (<strong class="bold">ATPE</strong>). It also supports various types of hyperparameters with ranging types of sampling distributions. </p>
			<p>In this chapter, we’ll introduce the <strong class="source-inline">Hyperopt</strong> package, starting with its capabilities and limitations, how to utilize it to perform hyperparameter tuning, and all the other important things you need to know about <strong class="source-inline">Hyperopt</strong>. We’ll learn not only how to utilize <strong class="source-inline">Hyperopt</strong> to perform hyperparameter tuning with its default configurations but also discuss the available configurations, along with their usage. Moreover, we’ll discuss how the implementation of the hyperparameter tuning methods is related to the theory that we learned about in the previous chapters, since there some minor differences or adjustments may have been made in the implementation.</p>
			<p>By the end of this chapter, you will be able to understand all the important things you need to know about <strong class="source-inline">Hyperopt</strong> and be able to implement various hyperparameter tuning methods available in this package. You’ll also be able to understand each of the important parameters of their classes and how they are related to the theory that we learned about in the previous chapters. Finally, equipped with the knowledge from previous chapters, you will be able to understand what’s happening if there are errors or unexpected results, as well as how to set up the method configuration so that it matches your specific problem.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Introducing Hyperopt</li>
				<li>Implementing Random Search</li>
				<li>Implementing Tree-Structured Parzen Estimators</li>
				<li>Implementing Adaptive Tree-Structured Parzen Estimators</li>
				<li>Implementing simulated annealing</li>
			</ul>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor075"/>Technical requirements</h1>
			<p>In this chapter, we will learn how to implement various hyperparameter tuning methods with Hyperopt. To ensure that you can reproduce the code examples in this chapter, you will require the following:</p>
			<ul>
				<li>Python 3 (version 3.7 or above)</li>
				<li>The <strong class="source-inline">pandas</strong> package (version 1.3.4 or above)</li>
				<li>The <strong class="source-inline">NumPy</strong> package (version 1.21.2 or above)</li>
				<li>The <strong class="source-inline">Matplotlib</strong> package (version 3.5.0 or above)</li>
				<li>The <strong class="source-inline">scikit-learn</strong> package (version 1.0.1 or above)</li>
				<li>The <strong class="source-inline">Hyperopt</strong> package (version 0.2.7 or above)</li>
				<li>The <strong class="source-inline">LightGBM</strong> package (version 3.3.2 or above)</li>
			</ul>
			<p>All the code examples for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python">https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python</a>.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor076"/>Introducing Hyperopt</h1>
			<p>All of the implemented <a id="_idIndexMarker386"/>optimization methods in the <strong class="source-inline">Hyperopt</strong> package assume we are working with a <em class="italic">minimization problem</em>. If your objective function is categorized as a maximization problem, for example, when you are using accuracy as the objective function score, you must <em class="italic">add a negative sign to your objective function</em>.</p>
			<p>Utilizing the <strong class="source-inline">Hyperopt</strong> package to perform hyperparameter tuning is very simple. The following steps show how to perform any hyperparameter tuning methods provided in the <strong class="source-inline">Hyperopt</strong> package. More detailed steps, including the code implementation, will be given through various examples in the upcoming sections:</p>
			<ol>
				<li>Define the objective function to be minimized. </li>
				<li>Define the hyperparameter space.</li>
				<li>(<em class="italic">Optional</em>) Initiate the <strong class="source-inline">Trials()</strong> object and pass it to the <strong class="source-inline">fmin()</strong> function.</li>
				<li>Perform hyperparameter tuning by calling the <strong class="source-inline">fmin()</strong> function.</li>
				<li>Train the model on full training data using the best set of hyperparameters that have been found from the output of the <strong class="source-inline">fmin()</strong> function.</li>
				<li>Test the final trained model on the test data.</li>
			</ol>
			<p>The simplest case <a id="_idIndexMarker387"/>of the objective function is when we only return the floating type of objective function score. However, we can also add other additional information to the output of the objective function, for example, the evaluation time or any other statistics we want to get for further analysis. When we add additional information to the output of the objective function score, <strong class="source-inline">Hyperopt</strong> expects the output of the objective function to be in the form of a Python dictionary that has at least two mandatory key-value pairs – that is, <strong class="source-inline">status</strong> and <strong class="source-inline">loss</strong>. The former key stores the status value of the run, while the latter key stores the objective function that we want to minimize. </p>
			<p>The simplest type of hyperparameter space in Hyperopt is in the form of a Python dictionary, where the keys refer to the name of the hyperparameters and the values contain the distribution of the hyperparameters to be sampled from. The following example shows how we can define a very simple hyperparameter space in <strong class="source-inline">Hyperopt</strong>:</p>
			<pre class="source-code">import numpy as np</pre>
			<pre class="source-code">from hyperopt import hp</pre>
			<pre class="source-code">hyperparameter_space = {</pre>
			<pre class="source-code">“criterion”: <strong class="bold">hp.choice</strong>(“criterion”, [“gini”, “entropy”]),</pre>
			<pre class="source-code">“n_estimators”: <strong class="bold">5</strong> + <strong class="bold">hp.randint</strong>(“n_estimators”, 195),</pre>
			<pre class="source-code">“min_samples_split” : <strong class="bold">hp.loguniform</strong>(“min_samples_split”, np.log(0.0001), np.log(0.5))</pre>
			<pre class="source-code">}</pre>
			<p>As you can see, the values of the <strong class="source-inline">hyperparameter_space</strong> dictionary are the distributions that accompany each of the hyperparameters we have in the space. <strong class="source-inline">Hyperopt</strong> provides a lot of sampling distributions that we can utilize, such as <strong class="source-inline">hp.choice</strong>, <strong class="source-inline">hp.randint</strong>, <strong class="source-inline">hp.uniform</strong>, <strong class="source-inline">hp.loguniform</strong>, <strong class="source-inline">hp.normal</strong>, and <strong class="source-inline">hp.lognormal</strong>. The <strong class="source-inline">hp.choice</strong> distribution will randomly choose one option from the several given options. The <strong class="source-inline">hp.randint</strong> distribution will randomly choose an integer within the range of <strong class="source-inline">[0, high)</strong>, where <strong class="source-inline">high</strong> is the input given by us. In the previous example, we passed <strong class="source-inline">195</strong> as the <strong class="source-inline">high</strong> value and added a value of <strong class="source-inline">5</strong>. This means <strong class="source-inline">Hyperopt</strong> will randomly choose an integer within the range of <strong class="source-inline">[5,200)</strong>. </p>
			<p>The rest of the <a id="_idIndexMarker388"/>distributions are dedicated to real/floating hyperparameter values. Note that Hyperopt also provides distributions dedicated to integer hyperparameter values that mimic the distribution of those four distributions – that is, <strong class="source-inline">hp.quniform</strong>, <strong class="source-inline">hp.qloguniform</strong>, <strong class="source-inline">hp.qnormal</strong>, and <strong class="source-inline">hp.qlognormal</strong>. For more information regarding the sampling distributions provided by Hyperopt, please refer to its official wiki page (<a href="https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions">https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions</a>).</p>
			<p>It is worth <a id="_idIndexMarker389"/>noting that Hyperopt enables us to define a <strong class="bold">conditional hyperparameter space</strong> (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Bayesian Optimization</em>) that suits our needs. The following code example shows how we can define such a search space:</p>
			<pre class="source-code">hyperparameter_space =</pre>
			<pre class="source-code"><strong class="bold">hp.choice</strong>(“class_weight_type”, [</pre>
			<pre class="source-code">{“class_weight”: None,</pre>
			<pre class="source-code">“n_estimators”: 5 + hp.randint(“<strong class="bold">none_n_estimators</strong>”, 45),</pre>
			<pre class="source-code">},</pre>
			<pre class="source-code">{“class_weight”: “balanced”,</pre>
			<pre class="source-code">“n_estimators”: 5 + hp.randint(“<strong class="bold">balanced_n_estimators</strong>”, 195),</pre>
			<pre class="source-code">}</pre>
			<pre class="source-code">])</pre>
			<p>As you can see, the only difference between a conditional hyperparameter space and a non-conditional one is that we add <strong class="source-inline">hp.choice</strong> before defining the hyperparameters for each condition. In this example, when <strong class="source-inline">class_weight</strong> is <strong class="source-inline">None</strong>, we will only search for the best <strong class="source-inline">n_estimators</strong> hyperparameters within the range <strong class="source-inline">[5,50)</strong>. On the other hand, when <strong class="source-inline">class_weight</strong> is <strong class="source-inline">“balanced”</strong>, the range becomes <strong class="source-inline">[5,200)</strong>.  </p>
			<p>Once the hyperparameter space <a id="_idIndexMarker390"/>is defined, we can start the hyperparameter tuning process via the <strong class="source-inline">fmin()</strong> function. The output of this function is the best set of hyperparameters that has been found from the tuning process. There are several important parameters available in this function that you need to know about. The <strong class="source-inline">fn</strong> parameter refers to the objective function we are trying to minimize, the <strong class="source-inline">space</strong> parameter refers <a id="_idIndexMarker391"/>to the hyperparameter space that will be used in our experiment, the <strong class="source-inline">algo</strong> parameter refers to the hyperparameter tuning algorithm that we want to utilize, the <strong class="source-inline">rstate</strong> parameter refers to the random seed for the tuning process, the <strong class="source-inline">max_evals</strong> parameter refers to the stopping criterion of the tuning process based on the number of trials, and the <strong class="source-inline">timeout</strong> parameter refers to the stopping criterion based on the time limit in seconds. Another important parameter is the <strong class="source-inline">trials</strong> parameter, which expects to receive the <strong class="source-inline">Hyperopt</strong> <strong class="source-inline">Trials()</strong> object.</p>
			<p>The <strong class="source-inline">Trials()</strong> object in <strong class="source-inline">Hyperopt</strong> logs all the relevant information during the tuning process. This object is also responsible for storing all of the additional information we put in the dictionary output of the objective function. We can utilize this object for debugging purposes or to pass it directly to the built-in plotting module in <strong class="source-inline">Hyperopt</strong>. </p>
			<p>Several built-in plotting modules are implemented in the <strong class="source-inline">Hyperopt</strong> package, such as <strong class="source-inline">main_plot_history</strong>, <strong class="source-inline">main_plot_histogram</strong>, and <strong class="source-inline">main_plot_vars modules</strong>. The first plotting module can help us understand the relationship between the loss values and the execution time. The second plotting module shows the histogram of all of the losses in all trials. The third plotting module is useful for understanding more about the heatmap of each hyperparameter in the space relative to the loss values.</p>
			<p>Last but not least, it is worth noting that Hyperopt also supports parallel search processes <a id="_idIndexMarker392"/>by utilizing <strong class="bold">MongoDB</strong> or <strong class="bold">Spark</strong>. To utilize the parallel resources via MongoDB, we can <a id="_idIndexMarker393"/>simply change the trial database from <strong class="source-inline">Trials()</strong> to<strong class="source-inline"> MongoTrials()</strong>. We can change from <strong class="source-inline">Trials()</strong> to <strong class="source-inline">SparkTrials()</strong> if we want to utilize Spark instead of MongoDB. Please refer to the official documentation of Hyperopt for more information about parallel computations (<a href="https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB ">https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB </a>and <a href="http://hyperopt.github.io/hyperopt/scaleout/spark/">http://hyperopt.github.io/hyperopt/scaleout/spark/</a>).</p>
			<p>In this section, you <a id="_idIndexMarker394"/>were introduced to the overall capability of the <strong class="source-inline">Hyperopt</strong> package, along with the general steps to perform hyperparameter tuning with this package. In the next few sections, we will learn how to implement each of the hyperparameter tuning methods available in <strong class="source-inline">Hyperopt </strong>through examples. </p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor077"/>Implementing Random Search</h1>
			<p>To implement Random Search (see <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a>) in Hyperopt, we can simply follow the steps explained <a id="_idIndexMarker395"/>in the previous section and pass the <strong class="source-inline">rand.suggest</strong> object to the <strong class="source-inline">algo</strong> parameter in the <strong class="source-inline">fmin()</strong> function. Let’s learn how we can utilize the <strong class="source-inline">Hyperopt</strong> package to perform Random Search. We will use the same data and <strong class="source-inline">sklearn</strong> pipeline definition as in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a><em class="italic">, Hyperparameter Tuning via Scikit</em>, but with a slightly different definition of the hyperparameter space. Let’s follow the steps that were introduced in the previous section:</p>
			<ol>
				<li value="1">Define the objective function to be minimized. Here, we are utilizing the defined pipeline, <strong class="source-inline">pipe</strong>, to calculate the <em class="italic">5-fold cross-validation</em> score by utilizing the <strong class="source-inline">cross_val_score</strong> function from <strong class="source-inline">sklearn</strong>. We will use the <em class="italic">F1 score</em> as the evaluation metric:<p class="source-code">import numpy as np</p><p class="source-code">from sklearn.base import clone</p><p class="source-code">from sklearn.model_selection import cross_val_score</p><p class="source-code">from hyperopt import STATUS_OK</p><p class="source-code">def objective(<strong class="bold">space</strong>):</p><p class="source-code">    estimator_clone = clone(<strong class="bold">pipe</strong>).set_params(**space)</p><p class="source-code">    return {<strong class="bold">‘loss’</strong>: <strong class="bold">-1 *</strong> np.mean(cross_val_score(estimator_clone, X_train_full, y_train, cv=5, scoring=’f1’, n_jobs=-1)), </p><p class="source-code">            <strong class="bold">‘status’</strong>: STATUS_OK}</p></li>
			</ol>
			<p>Note that the defined <strong class="source-inline">objective</strong> function only receives one input, which is the predefined hyperparameter space, <strong class="source-inline">space</strong>, and outputs a dictionary that contains two mandatory key-value pairs – that is, <strong class="source-inline">status</strong> and <strong class="source-inline">loss</strong>.  It is also worth noting that the reason why we multiply the average cross-validation score output with <strong class="source-inline">–1</strong> is that <strong class="source-inline">Hyperopt</strong> always assumes that we are working with a minimization problem, while we are not in this example.</p>
			<ol>
				<li value="2">Define the hyperparameter space. Since we are using the <strong class="source-inline">sklearn</strong> pipeline as our estimator, we still need to follow the naming convention of the hyperparameters <a id="_idIndexMarker396"/>within the defined space (see <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>). Note that the naming convention just needs to be applied to the hyperparameter names in the keys of the search space dictionary, not to the names within the sampling distribution objects:<p class="source-code">from hyperopt import hp</p><p class="source-code">hyperparameter_space = { </p><p class="source-code">“model__n_estimators”: 5 + <strong class="bold">hp.randint</strong>(“n_estimators”, 195), </p><p class="source-code">“model__criterion”: <strong class="bold">hp.choice</strong>(“criterion”, [“gini”, “entropy”]),</p><p class="source-code">“model__class_weight”: <strong class="bold">hp.choice</strong>(“class_weight”, [“balanced”,”balanced_subsample”]),</p><p class="source-code">“model__min_samples_split”: <strong class="bold">hp.loguniform</strong>(“min_samples_split”, np.log(0.0001), np.log(0.5))</p><p class="source-code">}</p></li>
				<li>Initiate the <strong class="source-inline">Trials()</strong> object. In this example, we will utilize this object for plotting purposes after the tuning process has been done:<p class="source-code">from hyperopt import Trials</p><p class="source-code">trials = Trials()</p></li>
				<li>Perform hyperparameter tuning by calling the <strong class="source-inline">fmin()</strong> function. Here, we are performing a Random Search by passing the defined objective function and hyperparameter space. We have set the <strong class="source-inline">algo</strong> parameter with the <strong class="source-inline">rand.suggest</strong> object and set the number of trials to <strong class="source-inline">100</strong> as the stopping criterion. We also <a id="_idIndexMarker397"/>set the random state to ensure reproducibility. Last but not least, we passed the defined <strong class="source-inline">Trials()</strong> object to the <strong class="source-inline">trials</strong> parameter:<p class="source-code">from hyperopt import fmin, rand</p><p class="source-code">best = fmin(objective,</p><p class="source-code">            space=hyperparameter_space,</p><p class="source-code">            algo=rand.suggest,</p><p class="source-code">            max_evals=100,</p><p class="source-code">            rstate=np.random.default_rng(0),</p><p class="source-code">            trials=trials</p><p class="source-code">           )</p><p class="source-code">print(best)</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">-0.621</strong> of the objective function score, which refers to <strong class="source-inline">0.621</strong> of the average 5-fold cross-validation F--score. We also get a dictionary consisting of the best set of hyperparameters, as follows:</p>
			<p class="source-code">{‘class_weight’: <strong class="bold">0</strong>, ‘criterion’: <strong class="bold">1</strong>, ‘min_samples_split’: 0.00047017001935242104, ‘n_estimators’: 186}</p>
			<p>As can be seen, <strong class="source-inline">Hyperopt</strong> will only return the index of the hyperparameter values when we use <strong class="source-inline">hp.choice</strong> as the sampling distribution (see the <strong class="source-inline">class_weight</strong> and <strong class="source-inline">criterion</strong> hyperparameters). Here, by referring to the predefined hyperparameter space, <strong class="source-inline">0</strong> for <strong class="source-inline">class_weight</strong> refers to <em class="italic">balanced</em> and <strong class="source-inline">1</strong> for <strong class="source-inline">criterion</strong> refers to <em class="italic">entropy</em>. Thus, the best set of hyperparameters is <strong class="source-inline">{‘model__class_weight’: ‘balanced’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.0004701700193524210, ‘model__n_estimators’: 186}</strong>.</p>
			<ol>
				<li value="5">Train the model on the full training data using the best set of hyperparameters that have been found in the output of the <strong class="source-inline">fmin()</strong> function:<p class="source-code">pipe = pipe.<strong class="bold">set_params</strong>(**{‘model__class_weight’: “balanced”,</p><p class="source-code">‘model__criterion’: “entropy”,</p><p class="source-code">‘model__min_samples_split’: 0.00047017001935242104,</p><p class="source-code">‘model__n_estimators’: 186})</p><p class="source-code">pipe.<strong class="bold">fit</strong>(X_train_full,y_train)</p></li>
				<li>Test the <a id="_idIndexMarker398"/>final trained model on the test data:<p class="source-code">from sklearn.metrics import f1_score</p><p class="source-code">y_pred = pipe.<strong class="bold">predict</strong>(X_test_full)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.624</strong> for the F1-score when testing our final trained Random Forest model with the best set of hyperparameters on the test set.</p>
			<ol>
				<li value="7">Last but not least, we can also utilize the built-in plotting modules implemented in <strong class="source-inline">Hyperopt</strong>. The following code shows how to do this. Note that we need to pass the <strong class="source-inline">trials</strong> object from the tuning process to the plotting modules since all of the tuning process logs are in there:<p class="source-code">from hyperopt import <strong class="bold">plotting</strong></p></li>
			</ol>
			<p>Now, we must plot the relationship between the loss values and the execution time:</p>
			<p class="source-code">plotting.<strong class="bold">main_plot_history</strong>(trials)</p>
			<p>We will get the following output:</p>
			<div>
				<div id="_idContainer310" class="IMG---Figure">
					<img src="image/B18753_08_001.jpg" alt="Figure 8.1 – Relationship between the loss values and the execution time&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Relationship between the loss values and the execution time</p>
			<p>Now, we must <a id="_idIndexMarker399"/>plot the histogram of all of the objective function scores from all the trials:</p>
			<p class="source-code">plotting.<strong class="bold">main_plot_histogram</strong>(trials)</p>
			<p>We will get the following output.</p>
			<div>
				<div id="_idContainer311" class="IMG---Figure">
					<img src="image/B18753_08_002.jpg" alt="Figure 8.2 – Histogram of all of the objective function scores from all trials&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Histogram of all of the objective function scores from all trials</p>
			<p>Now, we must <a id="_idIndexMarker400"/>plot the heatmap of each hyperparameter in the space relative to the loss values:</p>
			<p class="source-code">Plotting.<strong class="bold">main_plot_vars</strong>(trials)</p>
			<p>We will get the following output.</p>
			<div>
				<div id="_idContainer312" class="IMG---Figure">
					<img src="image/B18753_08_003.jpg" alt="Figure 8.3 – Heatmap of each hyperparameter in the space relative to the loss values (the darker, the better)&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Heatmap of each hyperparameter in the space relative to the loss values (the darker, the better)</p>
			<p>In this section, we learned how to perform Random Search in <strong class="source-inline">Hyperopt</strong> by looking at an example similar <a id="_idIndexMarker401"/>example to the one shown in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a><em class="italic">, Hyperparameter Tuning via Scikit</em>. We also saw what kind of figures we can get from utilizing the built-in plotting modules in Hyperopt. </p>
			<p>It is worth noting that we are not bounded to using only the <strong class="source-inline">sklearn</strong> implementation of models to perform hyperparameter tuning with <strong class="source-inline">Hyperopt</strong>. We can also use implementations from other packages, such as <strong class="source-inline">PyTorch</strong>, <strong class="source-inline">Tensorflow</strong>, and so on. One thing that needs to be kept in mind is to be careful with the <em class="italic">data leakage issue</em> (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Evaluating Machine Learning Models</em>) when performing cross-validation. We must fit all of the data preprocessing methods on the training data and apply the fitted preprocessors to the validation data. </p>
			<p>In the next section, we will learn how to utilize <strong class="source-inline">Hyperopt</strong> to perform hyperparameter tuning with one of the available Bayesian Optimization methods.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor078"/>Implementing Tree-structured Parzen Estimators</h1>
			<p><strong class="bold">Tree-Structured Parzen Estimators</strong> (<strong class="bold">TPE</strong>) is one of the variants of the Bayesian Optimization hyperparameter tuning group (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>) that is also implemented in the <strong class="source-inline">Hyperopt</strong> package. To perform hyperparameter tuning with this method, we can follow a <a id="_idIndexMarker402"/>similar procedure as in the previous section by only changing the <strong class="source-inline">algo</strong> parameter to <strong class="source-inline">tpe.suggest</strong> in <em class="italic">Step 4</em>. The following code shows how to perform hyperparameter tuning with TPE in <strong class="source-inline">Hyperopt</strong>:</p>
			<pre class="source-code">from hyperopt import fmin, <strong class="bold">tpe</strong></pre>
			<pre class="source-code">best = fmin(objective, </pre>
			<pre class="source-code">            space=hyperparameter_space, </pre>
			<pre class="source-code">            algo=<strong class="bold">tpe</strong>.suggest, </pre>
			<pre class="source-code">            max_evals=100, </pre>
			<pre class="source-code">            rstate=np.random.default_rng(0), </pre>
			<pre class="source-code">            trials=trials </pre>
			<pre class="source-code">           )</pre>
			<pre class="source-code">print(best)</pre>
			<p>Using the same data, hyperparameter space, and parameters for the <strong class="source-inline">fmin()</strong> function, we get around <strong class="source-inline">-0.620</strong> for the objective function score, which refers to <strong class="source-inline">0.620</strong> of the average 5-fold cross-validation F1-score. We also get a dictionary consisting of the best set of hyperparameters, as follows:</p>
			<pre class="source-code">{‘class_weight’: 1, ‘criterion’: 1, ‘min_samples_split’: 0.0005245304932726025, ‘n_estimators’: 138}</pre>
			<p>Once the model has been trained on the full data using the best set of hyperparameters, we get around <strong class="source-inline">0.621</strong> in terms of the F1-score when we test the final Random Forest model that’s been trained on the test data.</p>
			<p>In this section, we learned how to perform hyperparameter tuning using the TPE method with <strong class="source-inline">Hyperopt</strong>. In the next section, we will learn how to implement a variant of TPE called Adaptive TPE with the <strong class="source-inline">Hyperopt</strong> package.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor079"/>Implementing Adaptive TPE</h1>
			<p><strong class="bold">Adaptive TPE</strong> (<strong class="bold">ATPE</strong>) is a variant of the TPE hyperparameter tuning method that is developed based on several improvements compared to TPE, such as automatically tuning several <a id="_idIndexMarker403"/>hyperparameters of the TPE method based on the data that we have. For more information about this method, please refer to the original white papers. These can be found in the GitHub repository of the author (<a href="https://github.com/electricbrainio/hypermax">https://github.com/electricbrainio/hypermax</a>). </p>
			<p>While you can experiment with this method directly using the original GitHub repository of ATPE, <strong class="source-inline">Hyperopt</strong> has also included this method as part of the package. You can simply follow a similar procedure as in the <em class="italic">Implementing Random Search</em> section by only changing the <strong class="source-inline">algo</strong> parameter to <strong class="source-inline">atpe.suggest</strong> in <em class="italic">Step 4</em>. The following code shows how to perform hyperparameter tuning with ATPE in <strong class="source-inline">Hyperopt</strong>. Please note that ATPE utilizes the <strong class="bold">LightGBM</strong> model to <a id="_idIndexMarker404"/>predict each of the ATPE parameters. That’s why we need to have the <strong class="source-inline">LightGBM</strong> package installed before we can start to perform hyperparameter tuning with ATPE in <strong class="source-inline">Hyperopt</strong>:</p>
			<pre class="source-code">from hyperopt import fmin, <strong class="bold">atpe</strong></pre>
			<pre class="source-code">best = fmin(objective, </pre>
			<pre class="source-code">            space=hyperparameter_space, </pre>
			<pre class="source-code">            algo=<strong class="bold">atpe</strong>.suggest, </pre>
			<pre class="source-code">            max_evals=100, </pre>
			<pre class="source-code">            rstate=np.random.default_rng(0), </pre>
			<pre class="source-code">            trials=trials </pre>
			<pre class="source-code">           )</pre>
			<pre class="source-code">print(best)</pre>
			<p>Using the same data, hyperparameter space, and parameters for the <strong class="source-inline">fmin()</strong> function, we get around <strong class="source-inline">-0.621</strong> for the objective function score, which refers to <strong class="source-inline">0.621</strong> of the average 5-fold cross-validation F1-score. We also get a dictionary consisting of the best set of hyperparameters, as follows:</p>
			<pre class="source-code">{‘class_weight’: 1, ‘criterion’: 1, ‘min_samples_split’: 0.0005096354197481012, ‘n_estimators’: 157}</pre>
			<p>Once the model has been trained on the full data using the best set of hyperparameters, we get around <strong class="source-inline">0.622</strong> in terms of the F1 score when we test the final Random Forest model that was trained on the test data.</p>
			<p>In this section, we learned <a id="_idIndexMarker405"/>how to perform hyperparameter tuning using the ATPE method with <strong class="source-inline">Hyperopt</strong>. In the next section, we will learn how to implement a hyperparameter tuning method that is part of the Heuristic Search group with the <strong class="source-inline">Hyperopt</strong> package.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor080"/>Implementing simulated annealing</h1>
			<p><strong class="bold">Simulated annealing</strong> (<strong class="bold">SA</strong>) is part of the Heuristic Search hyperparameter tuning group (see <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a><em class="italic">, Exploring Heuristic Search</em>), which is also implemented in the <strong class="source-inline">Hyperopt</strong> package. Similar to TPE and ATPE, to <a id="_idIndexMarker406"/>perform hyperparameter tuning with this method, we can simply follow the procedure shown in the <em class="italic">Implementing Random Search</em> section; we only need to change the <strong class="source-inline">algo</strong> parameter to <strong class="source-inline">anneal.suggest</strong> in <em class="italic">Step 4</em>. The following code shows how to perform hyperparameter tuning with SA in <strong class="source-inline">Hyperopt</strong>:</p>
			<pre class="source-code">from hyperopt import fmin, <strong class="bold">anneal</strong></pre>
			<pre class="source-code">best = fmin(objective, </pre>
			<pre class="source-code">            space=hyperparameter_space, </pre>
			<pre class="source-code">            algo=<strong class="bold">anneal</strong>.suggest, </pre>
			<pre class="source-code">            max_evals=100, </pre>
			<pre class="source-code">            rstate=np.random.default_rng(0), </pre>
			<pre class="source-code">            trials=trials </pre>
			<pre class="source-code">           )</pre>
			<pre class="source-code">print(best)</pre>
			<p>Using the same data, hyperparameter space, and parameters for the <strong class="source-inline">fmin()</strong> function, we get around <strong class="source-inline">-0.620</strong> for the objective function score, which refers to <strong class="source-inline">0.620</strong> of the average 5-fold cross-validation F1-score. We also get a dictionary consisting of the best set of hyperparameters, as follows:</p>
			<pre class="source-code">{‘class_weight’: 1, ‘criterion’: 1, ‘min_samples_split’: 0.00046660708302994583, ‘n_estimators’: 189}</pre>
			<p>Once the model has been trained on the full data using the best set of hyperparameters, we get around <strong class="source-inline">0.625</strong> in terms of the F1-score when we test the final Random Forest model that was trained on the test data.</p>
			<p>While <strong class="source-inline">Hyperopt</strong> has built-in plotting modules, we can also create a customized plotting function by utilizing the <strong class="source-inline">Trials()</strong> object. The following code shows how to visualize the <a id="_idIndexMarker407"/>distribution of each hyperparameter over the number of trials:</p>
			<ol>
				<li value="1">Get the value of each hyperparameter in each of the trials:<p class="source-code">plotting_data = np.array([[x[‘result’][‘loss’],</p><p class="source-code">x[‘misc’][‘vals’][‘class_weight’][0],</p><p class="source-code">x[‘misc’][‘vals’][‘criterion’][0],</p><p class="source-code">x[‘misc’][‘vals’][‘min_samples_split’][0],</p><p class="source-code">x[‘misc’][‘vals’][‘n_estimators’][0],</p><p class="source-code">] for x in trials.trials])</p></li>
				<li>Convert the values into a pandas DataFrame:<p class="source-code">import pandas as pd</p><p class="source-code">plotting_data = pd.DataFrame(plotting_data,</p><p class="source-code">columns=[‘score’, ‘class_weight’, ‘criterion’, ‘min_samples_split’,’n_estimators’])</p></li>
				<li>Plot the relationship between each hyperparameter’s distribution and the number of trials:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">plotting_data.plot(subplots=True,figsize=(12, 12))</p><p class="source-code">plt.xlabel(“Iterations”)</p><p class="source-code">plt.show()</p></li>
			</ol>
			<p>Based on the preceding code, we will get the following output:</p>
			<div>
				<div id="_idContainer313" class="IMG---Figure">
					<img src="image/B18753_08_004.jpg" alt="Figure 8.4 – Relationship between each hyperparameter’s distribution and the number of trials &#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Relationship between each hyperparameter’s distribution and the number of trials </p>
			<p>In this <a id="_idIndexMarker408"/>section, we learned how to implement SA in <strong class="source-inline">Hyperopt</strong> by using the same example as in the <em class="italic">Implementing Random Search</em> section. We also learned how to create a custom plotting function to visualize the relationship between each hyperparameter’s distribution and the number of trials.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor081"/>Summary</h1>
			<p>In this chapter, we learned all the important things about the <strong class="source-inline">Hyperopt</strong> package, including its capabilities and limitations, and how to utilize it to perform hyperparameter tuning. We saw that <strong class="source-inline">Hyperopt</strong> supports various types of sampling distribution methods but can only work with a minimization problem. We also learned how to implement various hyperparameter tuning methods with the help of this package, which has helped us understand each of the important parameters of the classes and how are they related to the theory that we learned about in the previous chapters. At this point, you should be able to utilize <strong class="source-inline">Hyperopt</strong> to implement your chosen hyperparameter tuning method and, ultimately, boost the performance of your ML model. Equipped with the knowledge from <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a>, to <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>, you should be able to understand what’s happening if there are errors or unexpected results, as well as understand how to set up the method configuration so that it matches your specific problem.</p>
			<p>In the next chapter, we will learn about the <strong class="source-inline">Optuna</strong> package and how to utilize it to perform various hyperparameter tuning methods. The goal of the next chapter is similar to this chapter – that is, being able to utilize the package for hyperparameter tuning purposes and understanding each of the parameters of the implemented classes.</p>
		</div>
		<div>
			<div id="_idContainer315">
			</div>
		</div>
	</body></html>