["```py\ninclude <iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n//Definition of kernel function to add two variables\n__global__ void gpuAdd(int d_a, int d_b, int *d_c) \n{\n   *d_c = d_a + d_b;\n}\n```", "```py\n\n int main(void) \n{\n //Defining host variable to store answer\n   int h_c;\n //Defining device pointer\n   int *d_c;\n //Allocating memory for device pointer\n   cudaMalloc((void**)&d_c, sizeof(int));\n //Kernel call by passing 1 and 4 as inputs and storing answer in d_c\n //<< <1,1> >> means 1 block is executed with 1 thread per block\n   gpuAdd << <1, 1 >> > (1, 4, d_c);\n //Copy result from device memory to host memory\n   cudaMemcpy(&h_c, d_c, sizeof(int), cudaMemcpyDeviceToHost);\n   printf(\"1 + 4 = %d\\n\", h_c);\n //Free up memory\n   cudaFree(d_c);\n   return 0;\n}\n```", "```py\nkernel << <number of blocks, number of threads per block, size of shared memory > >> (parameters for kernel)\n```", "```py\ngpuAdd << <1,1> >> (1 , 4, d_c)\n```", "```py\ngpuAdd<< <1,500> >> (1,4, d_c)\n```", "```py\nmykernel<< <dim3(Nbx, Nby,Nbz), dim3(Ntx, Nty,Ntz) > >> ()  \n```", "```py\nmykernel << <dim3(16,16),dim3(16,16)> >> ()\n```", "```py\ncudaMalloc(void ** d_pointer, size_t size)\nExample: cudaMalloc((void**)&d_c, sizeof(int));\n```", "```py\ncudaMemcpy ( void * dst_ptr, const void * src_ptr, size_t size, enum cudaMemcpyKind kind )\nExample: cudaMemcpy(&h_c, d_c, sizeof(int), cudaMemcpyDeviceToHost);\n```", "```py\ncudaFree ( void * d_ptr )\nExample: cudaFree(d_c)\n```", "```py\ngpuAdd << <1,1> >>(1,4,d_c)\n```", "```py\n__global__  gpuAdd(int d_a, int d_b, int *d_c) \n```", "```py\ngpuAdd << <1,1> >>(a,b,d_c)\n```", "```py\n#include <iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n//Kernel function to add two variables, parameters are passed by reference\n __global__ void gpuAdd(int *d_a, int *d_b, int *d_c) \n{\n  *d_c = *d_a + *d_b;\n}\n```", "```py\nint main(void) \n{\n  //Defining host and variables\n  int h_a,h_b, h_c;\n  int *d_a,*d_b,*d_c;\n  //Initializing host variables\n  h_a = 1;\n  h_b = 4;\n  //Allocating memory for Device Pointers\n  cudaMalloc((void**)&d_a, sizeof(int));\n  cudaMalloc((void**)&d_b, sizeof(int));\n  cudaMalloc((void**)&d_c, sizeof(int));\n  //Coping value of host variables in device memory\n  cudaMemcpy(d_a, &h_a, sizeof(int), cudaMemcpyHostToDevice);\n  cudaMemcpy(d_b, &h_b, sizeof(int), cudaMemcpyHostToDevice);\n  //Calling kernel with one thread and one block with parameters passed by reference\n  gpuAdd << <1, 1 >> > (d_a, d_b, d_c);\n  //Coping result from device memory to host\n  cudaMemcpy(&h_c, d_c, sizeof(int), cudaMemcpyDeviceToHost);\n  printf(\"Passing Parameter by Reference Output: %d + %d = %d\\n\", h_a, h_b, h_c);\n  //Free up memory\n  cudaFree(d_a);\n  cudaFree(d_b);\n  cudaFree(d_c);\n  return 0;\n }\n```", "```py\n#include <iostream>\n#include <stdio.h>\n__global__ void myfirstkernel(void) \n{\n  //blockIdx.x gives the block number of current kernel\n   printf(\"Hello!!!I'm thread in block: %d\\n\", blockIdx.x);\n}\nint main(void) \n{\n   //A kernel call with 16 blocks and 1 thread per block\n   myfirstkernel << <16,1>> >();\n\n   //Function used for waiting for all kernels to finish\n   cudaDeviceSynchronize();\n\n   printf(\"All threads are finished!\\n\");\n   return 0;\n}\n```", "```py\n#include <memory>\n#include <iostream>\n#include <cuda_runtime.h>\n// Main Program \nint main(void)\n{\n  int device_Count = 0;\n  cudaGetDeviceCount(&device_Count);\n  // This function returns count of number of CUDA enable devices and 0 if there are no CUDA capable devices.\n  if (device_Count == 0)\n  {\n     printf(\"There are no available device(s) that support CUDA\\n\");\n  }\n  else\n  {\n     printf(\"Detected %d CUDA Capable device(s)\\n\", device_Count);\n  }\n}\n```", "```py\ncudaDeviceProp device_Property;\ncudaGetDeviceProperties(&device_Property, device);\nprintf(\"\\nDevice %d: \\\"%s\\\"\\n\", device, device_Property.name);\ncudaDriverGetVersion(&driver_Version);\ncudaRuntimeGetVersion(&runtime_Version);\nprintf(\" CUDA Driver Version / Runtime Version %d.%d / %d.%d\\n\", driver_Version / 1000, (driver_Version % 100) / 10, runtime_Version / 1000, (runtime_Version % 100) / 10);\nprintf( \" Total amount of global memory: %.0f MBytes (%llu bytes)\\n\",\n (float)device_Property.totalGlobalMem / 1048576.0f, (unsigned long long) device_Property.totalGlobalMem);\n printf(\" (%2d) Multiprocessors\", device_Property.multiProcessorCount );\nprintf(\"  GPU Max Clock rate: %.0f MHz (%0.2f GHz)\\n\", device_Property.clockRate * 1e-3f, device_Property.clockRate * 1e-6f);\n```", "```py\nprintf( \" Total amount of global memory: %.0f MBytes (%llu bytes)\\n\",\n(float)device_Property.totalGlobalMem / 1048576.0f, (unsigned long long) device_Property.totalGlobalMem);\nprintf(\" Memory Clock rate: %.0f Mhz\\n\", device_Property.memoryClockRate * 1e-3f);\nprintf(\" Memory Bus Width: %d-bit\\n\", device_Property.memoryBusWidth);\nif (device_Property.l2CacheSize)\n{\n    printf(\" L2 Cache Size: %d bytes\\n\", device_Property.l2CacheSize);\n}\nprintf(\" Total amount of constant memory: %lu bytes\\n\",         device_Property.totalConstMem);\nprintf(\" Total amount of shared memory per block: %lu bytes\\n\", device_Property.sharedMemPerBlock);\nprintf(\" Total number of registers available per block: %d\\n\", device_Property.regsPerBlock);\n```", "```py\nprintf(\" Maximum number of threads per multiprocessor: %d\\n\",              device_Property.maxThreadsPerMultiProcessor);\nprintf(\" Maximum number of threads per block: %d\\n\",         device_Property.maxThreadsPerBlock);\nprintf(\" Max dimension size of a thread block (x,y,z): (%d, %d, %d)\\n\",\n    device_Property.maxThreadsDim[0],\n    device_Property.maxThreadsDim[1],\n    device_Property.maxThreadsDim[2]);\nprintf(\" Max dimension size of a grid size (x,y,z): (%d, %d, %d)\\n\",\n    device_Property.maxGridSize[0],\n    device_Property.maxGridSize[1],\n    device_Property.maxGridSize[2]);\n```", "```py\n#include <memory>\n#include <iostream>\n#include <cuda_runtime.h>\n// Main Program\nint main(void)\n{\nint device;\ncudaDeviceProp device_property;\ncudaGetDevice(&device);\nprintf(\"ID of device: %d\\n\", device);\nmemset(&device_property, 0, sizeof(cudaDeviceProp));\ndevice_property.major = 1;\ndevice_property.minor = 3;\ncudaChooseDevice(&device, &device_property);\nprintf(\"ID of device which supports double precision is: %d\\n\", device);\ncudaSetDevice(device);\n}\n```", "```py\n#include \"stdio.h\"\n#include<iostream>\n //Defining Number of elements in Array\n#define N 5\n //Defining vector addition function for CPU\nvoid cpuAdd(int *h_a, int *h_b, int *h_c) \n{\n     int tid = 0;\n     while (tid < N)\n     {\n         h_c[tid] = h_a[tid] + h_b[tid];\n         tid += 1;\n     }\n }\n```", "```py\nint main(void) \n{\n   int h_a[N], h_b[N], h_c[N];\n   //Initializing two arrays for addition\n   for (int i = 0; i < N; i++) \n   {\n     h_a[i] = 2 * i*i;\n     h_b[i] = i;\n     }\n   //Calling CPU function for vector addition\n   cpuAdd (h_a, h_b, h_c);\n   //Printing Answer\n   printf(\"Vector addition on CPU\\n\");\n   for (int i = 0; i < N; i++) \n   {\n     printf(\"The sum of %d element is %d + %d = %d\\n\", i, h_a[i], h_b[i],             h_c[i]);\n   }\n   return 0;\n }\n```", "```py\n#include \"stdio.h\"\n#include<iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n //Defining number of elements in Array\n#define N 5\n //Defining Kernel function for vector addition\n__global__ void gpuAdd(int *d_a, int *d_b, int *d_c) \n{\n //Getting block index of current kernel\n     int tid = blockIdx.x; // handle the data at this index\n     if (tid < N)\n     d_c[tid] = d_a[tid] + d_b[tid];\n }\n```", "```py\nint main(void) \n{\n //Defining host arrays\n int h_a[N], h_b[N], h_c[N];\n //Defining device pointers\n int *d_a, *d_b, *d_c;\n // allocate the memory\n cudaMalloc((void**)&d_a, N * sizeof(int));\n cudaMalloc((void**)&d_b, N * sizeof(int));\n cudaMalloc((void**)&d_c, N * sizeof(int));\n //Initializing Arrays\n for (int i = 0; i < N; i++) \n    {\n     h_a[i] = 2*i*i;\n     h_b[i] = i ;\n     }\n\n// Copy input arrays from host to device memory\n cudaMemcpy(d_a, h_a, N * sizeof(int), cudaMemcpyHostToDevice);\n cudaMemcpy(d_b, h_b, N * sizeof(int), cudaMemcpyHostToDevice);\n\n//Calling kernels with N blocks and one thread per block, passing device pointers as parameters\ngpuAdd << <N, 1 >> >(d_a, d_b, d_c);\n //Copy result back to host memory from device memory\ncudaMemcpy(h_c, d_c, N * sizeof(int), cudaMemcpyDeviceToHost);\nprintf(\"Vector addition on GPU \\n\");\n //Printing result on console\nfor (int i = 0; i < N; i++) \n{\n     printf(\"The sum of %d element is %d + %d = %d\\n\", i, h_a[i], h_b[i],             h_c[i]);\n}\n //Free up memory\n cudaFree(d_a);\n cudaFree(d_b);\n cudaFree(d_c);\n return 0;\n}\n```", "```py\nclock_t start_d = clock();\nprintf(\"Doing GPU Vector add\\n\");\ngpuAdd << <N, 1 >> >(d_a, d_b, d_c);\ncudaThreadSynchronize();\nclock_t end_d = clock();\ndouble time_d = (double)(end_d - start_d) / CLOCKS_PER_SEC;\nprintf(\"No of Elements in Array:%d \\n Device time %f seconds \\n host time %f Seconds\\n\", N, time_d, time_h);\n```", "```py\n#include \"stdio.h\"\n#include<iostream>\n#include <cuda.h>\n#include <cuda_runtime.h>\n //Defining number of elements in Array\n#define N 5\n//Kernel function for squaring number\n__global__ void gpuSquare(float *d_in, float *d_out) \n{\n     //Getting thread index for current kernel\n     int tid = threadIdx.x; // handle the data at this index\n     float temp = d_in[tid];\n     d_out[tid] = temp*temp;\n }\n```", "```py\nint main(void) \n{\n //Defining Arrays for host\n     float h_in[N], h_out[N];\n     float *d_in, *d_out;\n// allocate the memory on the cpu\n     cudaMalloc((void**)&d_in, N * sizeof(float));\n     cudaMalloc((void**)&d_out, N * sizeof(float));\n //Initializing Array\n     for (int i = 0; i < N; i++) \n    {\n         h_in[i] = i;\n     }\n //Copy Array from host to device\n     cudaMemcpy(d_in, h_in, N * sizeof(float), cudaMemcpyHostToDevice);\n //Calling square kernel with one block and N threads per block\n     gpuSquare << <1, N >> >(d_in, d_out);\n //Coping result back to host from device memory\n     cudaMemcpy(h_out, d_out, N * sizeof(float), cudaMemcpyDeviceToHost);\n //Printing result on console\n     printf(\"Square of Number on GPU \\n\");\n     for (int i = 0; i < N; i++) \n     {\n         printf(\"The square of %f is %f\\n\", h_in[i], h_out[i]);\n     }\n //Free up memory\n     cudaFree(d_in);\n     cudaFree(d_out);\n     return 0;\n }\n```", "```py\nd_out[i] = d_in[i] * 2\n```", "```py\nout[i] = (in [i-1] + in[i] + in[i+1])/3\n```", "```py\nout[i-1] += 2 * in[i] and out[i+1] += 3*in[i]  \n```", "```py\nout[i+j*128] = in [j +i*128]\n```"]