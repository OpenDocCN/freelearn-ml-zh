- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navigating the Generative AI Project Lifecycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As briefly mentioned in *Chapter 3*, *Exploring ML Algorithms*, generative AI
    represents a category of AI focused on generating new data, such as text, images,
    videos, music, or other content, based on input data. This technology has the
    potential to transform numerous industries, offering capabilities previously unattainable.
    From entertainment to healthcare to financial services, generative AI exhibits
    a wide range of practical applications capable of solving intricate problems and
    creating innovative solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will embark on a practical journey, guiding you through
    the process of turning a generative AI project from a business concept to deployment.
    We will delve into the various stages of a generative AI project’s lifecycle,
    exploring different generative technologies, methodologies, and best practices.
    Specifically, we will cover the following key topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The advancement and economic impact of generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What industries are doing with generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lifecycle of a generative AI project and the core technology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations and challenges of generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advancement and economic impact of generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the past decade, there has been remarkable progress in the field of generative
    AI, which involves the creation of realistic images, audio, video, and text. This
    advancement has been driven by increased computational power, access to vast internet
    datasets, and advancement in ML algorithms. Both open-source communities and commercial
    entities have played pivotal roles in pushing the boundaries of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Prominent organizations like OpenAI, Stability AI, Meta, Google, the **Technology
    Innovation Institute** (**TII**), Hugging Face, and EleutherAI have contributed
    by open sourcing models such as GPT-2, OPT, LlaMA, Falcon, BLOOM, and GPT-J, fostering
    innovation within the community. On the commercial front, companies like OpenAI,
    Anthropics, Cohere, Amazon, and Google have made substantial investments in proprietary
    models like GPT-4, Claude, Cohere, Titan, and PaLM, leveraging cutting-edge transformer
    architectures and massive computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: The pace of development in generative AI is unprecedented. For instance, in
    November 2022, OpenAI released ChatGPT, a conversational chatbot based on LLM
    GPT3.5 turbo. Four months later, they released GPT-4, showcasing significant advancements.
    Similarly, Anthropics’ generative AI model, Claude, expanded its text processing
    capabilities from around 9,000 tokens per single API call when it debuted in March
    2023 to processing 100,000 tokens by May 2023, and to 200,000 tokens in November
    2023\. In the open-source realm, Meta launched Llama 2 in July 2023, building
    upon the success of LLaMA introduced in February 2023\. TII introduced its Falcon
    model with 40 billion parameters in May 2023, followed by a more advanced 180
    billion parameters model in September 2023, demonstrating a continuous evolution.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI is poised to have a profound impact across various industry sectors,
    potentially contributing trillions of dollars to the global economy. Industries
    such as banking, high tech, and life sciences stand to benefit significantly,
    with generative AI playing a substantial role in their revenue streams.
  prefs: []
  type: TYPE_NORMAL
- en: While the excitement surrounding generative AI is palpable, its full potential
    will take time to realize. Leaders in both business and society face substantial
    challenges, including managing the inherent risks associated with generative AI,
    identifying the new skills and capabilities required by the workforce, and reevaluating
    core business processes. It’s also essential to acknowledge that while generative
    AI is a rapidly advancing technology, ML continues to account for the majority
    of the overall potential value within the field of AI.
  prefs: []
  type: TYPE_NORMAL
- en: What industries are doing with generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enterprises across diverse sectors are actively engaging in the exploration
    of potential applications for generative AI technology, even though it is still
    early days in the adoption of generative AI. These enterprises are looking into
    this innovative technology to drive tangible business outcomes including increased
    productivity, enhanced customer experiences, novel business insights, and the
    creation of new products and services. With all the excitement surrounding this
    technology, it is also important to understand what’s practical and what is aspirational.
    With that in mind, let’s delve into some active areas of exploration of the adoption
    of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Financial services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As leaders in technology adoption, financial services firms are actively exploring
    generative AI use cases across banking, capital markets, insurance, and financial
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The majority of current generative AI applications focus on document analysis,
    knowledge search, insight generation, and content creation. For example, some
    financial services firms are building generative-AI-powered financial research
    applications to rapidly analyze public and proprietary data to identify investment
    opportunities and risks. Other financial firms are using generative AI to create
    summaries of vast amounts of proprietary research reports for a quick understanding
    of key investment insights. Insurance companies are piloting generative AI to
    extract required information from various sources to streamline underwriting and
    claims processing and provide underwriters the ability to interactively query
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI is proving valuable in investigating financial fraud scenarios.
    Payment companies, for instance, are applying these models to streamline fraud
    alert validation. For example, when an internal system flags a suspicious transaction,
    the generative model can rapidly correlate this alert with relevant external data.
    This may involve scanning the news and public records to uncover negative events
    related to the transacting entities. Moreover, the model can uncover hidden relationships
    in the transaction path that suggest illegitimate activity. By augmenting fraud
    analysts with an AI assistant that can quickly surface supporting contextual insights
    from large, disparate sources, cases can be prioritized and validated more efficiently.
    This allows a faster response to prevent fraudulent transactions while reducing
    false positives and manual review overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Financial services institutions are deploying conversational AI and generative
    models to enhance customer support interactions. Virtual assistants powered by
    these models can understand customer queries and automatically provide answers
    to common questions. They can also generate personalized product or service recommendations
    based on customer needs and transaction history. For complex customer inquiries,
    generative models help point users to relevant articles or offer next-best actions
    to resolve issues. For task fulfillment, these AI agents can guide customers through
    processes, collect necessary information, and complete end-to-end fulfillment.
  prefs: []
  type: TYPE_NORMAL
- en: Leading-edge financial institutions are piloting the use of generative AI to
    automatically formulate new market hypotheses and trading strategies. By analyzing
    a large volume of historical market data, research, and event narratives, these
    models can help identify hidden relationships, patterns, and insights. The generated
    hypotheses can highlight promising new signals, strategies, and relationships
    that complement conventional quantitative analysis. This enables institutions
    to combine ML with human intelligence to create innovative, differentiated investing
    and trading approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Healthcare and life sciences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI holds tremendous potential across healthcare and life sciences,
    from providers to payers, and pharmaceutical **research and development** (**R&D**)
    to medical device makers. Its unique capabilities are enabling innovations in
    drug discovery, clinical care, customer engagement, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Pharmaceutical companies are exploring generative AI to accelerate and enhance
    drug development in various ways. Powerful protein folding algorithms such as
    AlphaFold enable predicting protein structures directly from amino acid sequences.
    These 3D protein models provide insights to guide targeted drug design. Generative
    models can also propose completely novel molecular structures and compounds with
    desired pharmaceutical properties. This expands the drug candidate space for testing
    beyond incremental tweaks to existing therapies. Additionally, by reading, comprehending,
    and summarizing massive volumes of biomedical research, generative AI can assist
    researchers in extracting relevant findings and knowledge from research literature.
    This augmented intelligence helps inform R&D strategy and drug discovery by synthesizing
    insights from across huge corpora of domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Healthcare providers are exploring numerous applications of generative AI to
    augment clinical workflows and care. In diagnosis, these models can analyze medical
    scans, lab tests, and patient history to provide condition assessments and triage
    recommendations. Generative models can even summarize doctor-patient conversation
    details into structured medical notes for easy maintenance and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: To assist physicians at the point of care, AI assistants can respond to medical
    questions by searching knowledge bases and research to retrieve helpful information.
    Generative models also show potential for automated report writing, such as synthesizing
    patient discharge summaries.
  prefs: []
  type: TYPE_NORMAL
- en: Health insurance payers are assessing generative AI applications to improve
    customer and claims processing workflows. Virtual assistants and chatbots can
    understand customer queries and provide conversational support to promptly resolve
    inquiries. Generative models are also being tested to automate elements of claims
    adjudication. By analyzing claim forms, attached documentation, provider info,
    and payer guidelines, these models can extract relevant details to validate claims
    and determine appropriate payment. This could significantly reduce manual review
    and speed up claim settlement timelines.
  prefs: []
  type: TYPE_NORMAL
- en: Medical device and pharmaceutical manufacturers are piloting the use of generative
    AI for automated manufacturing and production oversight. By analyzing written
    standard operating procedures and process documentation, generative models can
    validate that critical manufacturing processes adhere to regulatory compliance
    standards and internal policies. Any deviations or missing steps can be flagged
    to ensure protocols meet requirements before reaching inspection. This proactive
    auditing can identify compliance gaps upstream and enable corrective actions sooner.
    With the ability to thoroughly scan extensive documentation and compare them to
    guidelines at scale, generative AI can strengthen quality assurance and streamline
    production oversight in medical product manufacturing.
  prefs: []
  type: TYPE_NORMAL
- en: Media and entertainment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The media and entertainment sector presents tremendous opportunities to apply
    generative AI across the entire content value chain and consumer touchpoints.
  prefs: []
  type: TYPE_NORMAL
- en: For content production, media companies are exploring the use of generative
    models to autonomously synthesize completely new images, videos, and other multimedia
    from textual prompts. These models can also meaningfully enhance existing assets,
    such as increasing image and video resolution, colorizing black-and-white content,
    or restoring corrupted and damaged files.
  prefs: []
  type: TYPE_NORMAL
- en: In content distribution, generative AI can unlock capabilities like automated
    metadata tagging, hyper-relevant search, and customized recommendations that can
    substantially improve media discovery and engagement. Marketing campaigns can
    also leverage dynamically generated, personalized content tailored to individual
    user interests and localized preferences. With contextually relevant experiences
    powered by generative AI, media companies can deepen audience relationships, improve
    retention, and better monetize content catalogs.
  prefs: []
  type: TYPE_NORMAL
- en: Media companies are piloting the use of generative AI to enrich customer experience
    such as automating live sports commentary and reporting. By ingesting real-time
    data and narratives around games, generative models can provide customized play-by-play
    and analysis as engaging, conversational outputs. When applied to customer service,
    conversational AI interfaces leveraging these models could deliver highly responsive,
    natural interactions to resolve subscriber issues and queries.
  prefs: []
  type: TYPE_NORMAL
- en: Automotive and manufacturing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The automotive and manufacturing industries are exploring generative AI across
    customer experience, product engineering, and smart manufacturing use cases.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, some automakers are evaluating conversational AI to power interactive
    digital owner’s manuals in the car or on mobile devices. This would enable voice-guided
    vehicle troubleshooting and contextual search for repair procedures. Generative
    AI call center analytics can also help summarize transcripts to address customer
    issues faster and improve agent training.
  prefs: []
  type: TYPE_NORMAL
- en: In product engineering, generative models are being explored to ideate exterior
    and interior styling concepts balanced with considerations like aerodynamics,
    space utilization, and ergonomics. These models can also predict simulation outcomes
    to complement physics-based testing.
  prefs: []
  type: TYPE_NORMAL
- en: For smart manufacturing, generative AI can assist by producing detailed machine
    troubleshooting guides using maintenance manuals, issue patterns, and repair procedures.
    This can enable self-guided maintenance and reduced downtime.
  prefs: []
  type: TYPE_NORMAL
- en: With the potential benefit and impact promised by generative AI, what does it
    take to turn an idea into a practical generative AI solution? How do we navigate
    the various stages of the generative AI project lifecycle? What are the different
    science and technology options available for consideration? What challenges and
    risks should we be keeping a watchful eye on? In this next section, we will explore
    and try to answer these questions.
  prefs: []
  type: TYPE_NORMAL
- en: The lifecycle of a generative AI project and the core technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The lifecycle for developing and deploying generative AI solutions spans multiple
    stages, with some variations from traditional ML projects, such as model customization
    and model evaluation. While certain phases like use case definition and data preparation
    align closely, stages including model development, training, evaluation, and adaptation
    take on unique characteristics for generative models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_15_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: Generative AI project lifecycle'
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, a generative AI project consists of a series of stages, including
    identification of business use cases, model selection or pre-training, domain
    adaptation and model customization, post-customization model evaluation, and model
    deployment. It’s important to recognize that while a generative AI project places
    significant emphasis on the capabilities and quality of the model itself, the
    model constitutes just one facet within the broader development of a generative
    AI solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before delving into the lifecycle details, it’s crucial to grasp the various
    adoption approaches that different organizations take, as they significantly impact
    project execution. Based on their business objectives, organizations typically
    fall into one of three categories for generative AI adoption:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model consumers**: Direct consumers of **foundation models** (**FMs**) typically
    leverage them as-is to address specific business challenges. While they may employ
    techniques like prompt engineering for customization, they don’t invest resources
    in teaching the model new domains or tasks. Their primary focus is on solving
    immediate business problems for internal or external customers seeking end-user
    applications rather than building foundational technology blocks. Additionally,
    these organizations generally don’t prioritize enhancing existing FMs with proprietary
    datasets. An example of a model consumer is a generative AI application developer
    who builds a customer support chatbot that directly consumes the OpenAI GPT model
    or Claude model from Anthropic via application APIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model tuners**: FM tuners are organizations aiming to fine-tune existing
    FMs for specific business purposes. This refinement can involve domain adaptation,
    enriching the model with domain-specific data (e.g., finance or medicine), or
    teaching the model new tasks (e.g., writing in a specific style). These organizations
    typically have distinct business goals, including generating revenue, reducing
    costs, improving productivity, or enhancing customer experiences. They possess
    proprietary datasets that offer a competitive edge, as well as the scientific
    and engineering expertise needed to tailor an existing model to meet their unique
    business needs. An example of a model tuner could be a financial services organization
    enriching an open-source LLM with their proprietary dataset such as a financial
    research report, so the model can perform better with research report summarization
    tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model developers**: FM developers are organizations dedicated to constructing
    FMs from the ground up. These models are subsequently provided to other organizations
    for either commercial purposes or for contribution to the open-source community,
    promoting the advancement and widespread adoption of this technology. Notable
    examples of FM developers include OpenAI, Anthropic, Google, Meta, Amazon, open-source
    communities, and government entities. These models typically serve as fundamental
    building blocks for a variety of general-purpose capabilities, including text
    generation, summarization, text-to-image generation, question answering, mathematics,
    planning, and reasoning. The primary audience for these FMs consists of other
    organizations and developers aiming to create applications powered by generative
    AI technology, in addition to their internal use. Organizations in this category
    are characterized by their substantial expertise in data sciences and ML engineering,
    as well as robust financial backing for these endeavors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While there are three distinct user personas in generation adoption, it is worth
    noting that many organizations can take on more than one persona. For example,
    while a model tuner might tune some existing FMs with its proprietary dataset
    for specific needs or competitive advantage, it might also just use an existing
    model as it is for some other needs.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to highlight that the generative AI project lifecycle varies
    among the personas mentioned earlier, based on distinct business objectives associated
    with each approach. Next, let’s delve into the specifics of each key step, starting
    with business use case selection.
  prefs: []
  type: TYPE_NORMAL
- en: Business use case selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the first step in a generative AI project. In this pivotal stage, organizations
    typically chart the course for their generative AI endeavors by selecting the
    right business case by aligning technology with specific business objectives.
    The selection of use cases not only shapes the trajectory of the project but also
    determines the impact on internal and external stakeholders. Choosing the right
    business use case for a generative AI initiative involves several key considerations.
    Here are some factors to weigh when deciding which business use cases to pursue:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business value and ROI assessment**: Like any AI initiative, generative AI
    projects require clearly defined business objectives and metrics to measure value.
    In the excitement of new possibilities, organizations should pragmatically validate
    that generative AI products and services deliver tangible benefits. Despite many
    opportunities, not all generative AI applications can translate to positive business
    impact. With proper goal-setting and outcome-driven guidance, enterprises can
    strategically unlock real business value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical capability assessment**: When selecting use cases, companies must
    consider their technical capabilities. For instance, training a novel FM from
    scratch promises potentially high value but requires skills and computational
    and data resources that many organizations may lack. Tailoring use cases to build
    upon existing competencies is key for successful execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data availability consideration**: An organization also needs to assess what
    dataset it has to determine whether a certain use case is feasible. For example,
    an organization may consider fine-tuning FMs with unique knowledge to be competitive,
    but if the organization does not have access to a proprietary dataset, then it
    is also not a feasible use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory and compliance consideration**: While generative AI enables many
    new product possibilities, companies must evaluate potential regulatory constraints
    and compliance risks. For instance, investment advice applications may require
    specific licensing, preventing unrestrained deployment. A pragmatic assessment
    of the regulatory landscape for each use case is prudent to avoid pitfalls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethics consideration**: Ethics should guide use case selection. Applications
    should avoid disenfranchising groups or causing harm. Generative AI’s responsibilities
    extend beyond business value to societal impact.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risk assessment**: Organizations should carefully evaluate the risks that
    might arise from the implementation of generative AI applications. For example,
    consider the risk the solution may cause to a patient if generative AI makes a
    wrong decision for medical diagnosis. If mitigations are lacking for high-severity
    risks, it may be advisable to avoid certain use cases altogether.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated decision versus assistive augmentation**: Organizations should
    weigh whether use cases require fully automated decisions versus AI assistance
    where humans retain control. Limitations can be mitigated by keeping the human
    in the loop for final decisions rather than fully autonomous generative AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI is still an emerging field. Most organizations are still evaluating
    and doing **proofs of concept** (**POCs**) for different business use cases to
    assess the production deployment readiness for real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: FM selection and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FMs are large, pre-trained, and/or tuned ML models designed to adapt to various
    downstream tasks like translation, summarization, question answering, and image
    generation. These models are pre-trained using self-supervised training on very
    large datasets with trillions of tokens, including internet text and images, encoding
    a wealth of knowledge in their parameters.
  prefs: []
  type: TYPE_NORMAL
- en: This knowledge can be fine-tuned for different tasks, allowing for versatile
    reuse. Notable examples of FMs include GPT, LLaMA, and Stable Diffusion. Since
    FMs serve as the core components of generative AI applications, choosing the appropriate
    FMs for your chosen use case becomes a crucial next step in your generative AI
    project lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'For organizations new to FM adoption, choosing the right FMs can be challenging
    due to numerous proprietary and open-source options. At a high level, there are
    five key focus areas for FM quality evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Factuality**: This is one of the most important model qualities to be evaluated.
    A high-quality model should return factually accurate information with or without
    a given context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task completion**: A model should be able to complete the desired tasks when
    provided with clear instructions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responsible AI enforcement**: Does the model exhibit unresponsible behaviors
    such as bias and harmful content?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning/logical thinking**: A high-quality model should be able to perform
    complex analyses with sound logical reasoning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creativity**: How creative is the response when completing a task with specific
    instruction?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, non-model quality factors like inference latency and hosting costs
    must be weighed as part of the overall model selection decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s explore the essential dimensions of the model evaluation process
    and techniques. There are four primary stages of model selection at a high level:
    initial screening via manual assessment, automated model evaluation, human expert
    evaluation, and AI risk assessment. Let’s discuss them in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Initial screening via manual assessment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The primary goal of this stage is to come up with a short list of FMs for further
    evaluation. There are many existing open-source and proprietary FMs and new ones
    are being created continuously. For example, in the Hugging Face platform alone,
    there are over 120K open-source models currently, of which many are FMs, and the
    number is expected to grow much larger. Furthermore, many proprietary model developers,
    including Amazon, Google, Anthropics, Cohere, and AI21, also offer proprietary
    FMs for commercial use.
  prefs: []
  type: TYPE_NORMAL
- en: To create a shortlist from available models, establish selection criteria based
    on factors like modality (e.g., text, image, video, code, etc.), model size, supported
    use cases (e.g., summarization, question answering, reasoning, etc.), training
    data (e.g., general-purpose or domain-specific), and performance expectations.
    Hugging Face and proprietary providers provide FM model cards with these details.
  prefs: []
  type: TYPE_NORMAL
- en: Public benchmarks (e.g., **Holistic Evaluation of Language Models** (**HELM**)
    for task-specific performance and HumanEval for code generation correctness) and
    leaderboards (e.g., Hugging Face LLM leaderboard) offer valuable information.
    Combine model card data and benchmark insights to compile a shortlist of suitable
    FMs. You can also run HELM to run benchmarks on FMs directly. HELM supports multi-metric
    measurement including accuracy, calibration, robustness, fairness, bias, toxicity,
    and efficiency for a set of scenarios. HELM provides command-line tools for running
    benchmarks (helm-run), summarizing results (helm-summarize), and visualizing results
    (helm-server).
  prefs: []
  type: TYPE_NORMAL
- en: With this initial list, you should create a small testing dataset consisting
    of input-output pairs and conduct a manual assessment of the FMs. Hugging Face
    and proprietary model providers offer model playgrounds or **software development
    kits** (**SDKs**) that facilitate this assessment process. Alternatively, you
    can deploy these models in your own environment for testing purposes. Through
    this manual testing phase, the goal is to identify a manageable number of FMs
    for the next stage of evaluation. If you intend to fine-tune the FMs using your
    own data, ensure that the FMs can support further fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Automated model evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The aim of this stage is to perform extensive automated testing of the short-listed
    FMs using evaluation metrics to identify the final two to three models for human
    expert evaluation before adoption. It is important to know that each evaluation
    metric only assesses one aspect of a model. As FMs can often perform many different
    tasks, it is recommended to evaluate metrics holistically for the final decision.
  prefs: []
  type: TYPE_NORMAL
- en: FMs present a unique challenge in automated model evaluation, particularly for
    generative tasks like text generation. Unlike traditional supervised ML, where
    ground truth labels and training data distribution are known, FMs often lack visibility
    into their training data distribution and lack ground truth for output. This raises
    questions about which metrics to use for accuracy, factual correctness, tone,
    and style assessment of generated text, as well as considerations like creativity
    and output format. While public benchmarks provide useful insights, they may not
    cover specific data and workflows. So, let’s address these challenges by categorizing
    them based on task types, objectives, and data availability. Specifically, we
    will cover tasks with discrete outputs and tasks with continuous text outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks with discrete outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tasks with discrete outputs involve generating or predicting categorical or
    discrete values as the output, as opposed to continuous values (discussed in the
    upcoming section). Common examples of tasks with discrete outputs in the NLP domain
    include text classification, named entity extraction, intent recognition, part-of-speech
    tagging, and spam detection. These may also include text generation tasks that
    produce specific items (e.g., an exact textual answer to a question) such as words,
    characters, or tokens.
  prefs: []
  type: TYPE_NORMAL
- en: For these types of tasks, the objective is to have FMs produce responses that
    match the expected labels precisely. Therefore, the recommended evaluation method
    involves creating a test dataset consisting of input-output label pairs and assessing
    the FMs’ performance using established metrics like accuracy and F1\. There are
    also public benchmarks and datasets available for the evaluation of specific NLP
    tasks such as entity resolution. For example, the **General Language Understanding
    Evaluation** (**GLUE**) benchmark consists of a collection of nine representative
    NLP tasks, including sentence classification, sentiment analysis, and question
    answering. Each task in the benchmark comes with a training set, a development
    set for fine-tuning the models, and an evaluation set for testing the performance
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks with continuous text outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tasks with continuous text outputs involve generating text as the output, which
    can be a sequence of words, characters, or tokens, rather than discrete labels
    or categories. Some examples of tasks with continuous text outputs include text
    summarization, machine translation, image captioning, question answering, and
    text generation tasks for creative stories and poems. For tasks of this nature,
    the main objective is to generate coherent and contextually relevant text that
    is factually correct. To measure the performance of this type of task, conventional
    evaluation NLP metrics such as **Bilingual Evaluation Understudy** (**BLEU**)
    and **Recall-Oriented Understudy for Gisting Evaluation** (**ROUGE**) remain relevant
    for FMs, assuming the availability of a suitable testing dataset. There are public
    datasets available for the evaluation of NLP tasks with continuous outputs. For
    example, the **Stanford Question Answering Dataset** (**SQuAD**) is a reading
    comprehension dataset that can be used for question-answering tasks. However,
    while these metrics help measure the similarity between the machine-generated
    text and human-generated reference text, these metrics focus on n-gram overlapping
    and matching, and they lack semantic understanding of the generated text, sensitivity
    of word order, and consideration of the overall text quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address certain limitations of conventional metrics, the approach of utilizing
    other more powerful LLMs to assist in the automated evaluation of the target FMs
    has been explored. Specifically, the approach involves employing LLMs in the following
    evaluation processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic similarity assessment**: Powerful LLMs like GPT4 and Claude are
    known for having a strong semantic understanding of text. This capability can
    be used to assess the semantic similarity between machine-generated text and human-generated
    reference text. For example, you can ask an LLM to measure the semantic similarity
    using a cosine similarity score or return a response of a yes or no for similarity
    measure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language coherence assessment**: Powerful LLMs are known for having a strong
    ability to analyze the certain structure of text such as consistency, relevance,
    transition, and clarity. As such, they can help assess the coherence of the generated
    text. For example, you can directly ask an LLM to rate the coherence of a generated
    text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ranking of generated responses**: One approach to assess the quality of a
    generated text is to compare it with another piece of text. LLMs can be used to
    rank the quality of generated text with reference text using different criteria
    such as clarity or overall text quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test data generation**: Powerful LLMs have the capability to generate input-output
    test data for specific language tasks when given specific instructions. If necessary,
    these pieces of test data should be subsequently refined or adjusted to align
    with specific requirements. For example, you can design a prompt to ask an LLM
    to generate a list of questions and answers from a body of input text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The open-source community has been actively creating automated evaluators utilizing
    LLMs. One such example is AlpacaEval, which employs LLMs to evaluate instruction-following
    language models. It is crucial to know that while employing LLMs for automated
    assessment has demonstrated utility based on empirical experiments, it does not
    have the same precision as conventional metrics. Therefore, it is imperative to
    recognize its limitations and employ additional human evaluation when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Human evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the final candidate FMs have been selected from the automated evaluation
    stage, the subsequent phase in the FM selection and evaluation process involves
    engaging human evaluators for a more comprehensive assessment, addressing aspects
    that may not have been adequately covered by manual screening and automated assessments
    for the target use case. It’s important to emphasize that organizations have the
    flexibility to choose human evaluation independently of automated evaluation methods,
    depending on their specific requirements. In the case of FM evaluation, the human
    evaluator plays a critical role in the selection of a high-quality model. Here
    are some scenarios where human expert evaluation can be particularly valuable:'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of testing data for automated evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of robust evaluation metrics for automated evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factual correctness assessment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessment of creativity, tone, style, and fluency of the generated content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessment of soundness of reasoning and logical thinking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessment of ethical consideration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human behavior imitation in performing certain tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cybersecurity risk exposure assessment such as red teaming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Establishing and executing a human evaluation workflow can be a complicated
    process. In addition to providing the right tooling and recruiting the right evaluators,
    there are other process-related tasks to complete, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining evaluation goals**: During this step, it is essential to specify
    the aspects of model performance that will be assessed by humans. For instance,
    in the case of language FMs, these aspects may encompass language coherence, fluency,
    bias, factual accuracy, style and tone, logical reasoning, or the presence of
    toxic content. In the context of image-based FMs, the focus might be on evaluating
    bias and accuracy in representing textual inputs. It is also important to define
    the downstream tasks and evaluate the FMs against the downstream tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defining clear rating schemes and rubrics**: To ensure consistency across
    human evaluators, it is essential to define clear rating schemes and rubrics to
    score model outputs for the different evaluation criteria such as factual correctness
    or language coherence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Designing diverse prompts for evaluation**: Design prompts with varying contexts,
    knowledge domains, and user perspectives to help ensure FMs can handle diverse
    requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collecting feedback and assessing model performance**: Collect and aggregate
    feedback from evaluators to assess intended aspects of model performance. Incorporate
    human feedback into potential model fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While human evaluation is extremely important to model selection, it comes with
    its own set of challenges. Human evaluation is slow and costly and can be difficult
    to implement at scale. In addition, individual evaluators can have subjective
    viewpoints, which can lead to skewed evaluation results. Different human evaluators
    might assess output differently even with clear rating rubrics, resulting in discrepancies
    in their evaluation. Furthermore, recruiting diverse evaluators covering different
    demographics, cultural backgrounds, and expertise can be difficult due to the
    scarcity of these resources.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing AI risks for FMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Assessing the functional aspects of FMs represents just one facet of the comprehensive
    evaluation. What is equally important is ensuring that FMs exhibit behaviors in
    accordance with AI risk considerations, which can include operational risks such
    as FM hallucination and generating irrelevant answers, ethics risks such as producing
    harmful output and bias, and security risks such as data privacy leakage and FM
    input (a.k.a. prompt) manipulation. AI risk management is a large topic, and it
    is covered in greater detail in *Chapter 12*, *AI Risk Management*. In this section,
    however, we will take a high-level approach to understanding how to automate risk
    detection for FMs, which mainly revolves around test prompt generation and output
    validation. The goal is to assess whether an FM exhibits unethical behaviors,
    an operational risk, or a security risk in its response when presented with different
    input prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test prompt generation**: To create effective prompts to test the FMs, you
    need to determine what risks to assess and design the prompt appropriately. For
    example, if you want to detect harmful content in the output of an FM, then the
    prompt should instruct the FM to perform this specific task. If you want to detect
    bias in the output, then you want to design your prompt with different terms and
    keywords, such as the names of certain ethnic groups, and see how the response
    will be different. These test prompts can be used to generate outputs from the
    FMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output validation**: This is mainly about building ML models or rule engines
    that detect specific risks using the test prompt generated. For example, you can
    train a harmful content detection model using a hate speech and offensive language
    dataset and use the model on output generated from a specifically designed test
    prompt for harmful content. If you would like to detect bias in the output, then
    you can build an ML model to detect whether the FM produces an output that is
    disproportionally biased against a certain group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to know that AI risk detection for FMs is still an ongoing research
    area, and there are still many unknowns and gaps. For example, hallucination and
    lack of interpretability remain a challenge with the adoption of FMs. Furthermore,
    you need to balance the need for AI risk detection and the specific use case requirements.
    For example, some use cases might require less restrictive guardrails on offensive
    language due to its actual applications such as movie script generation.
  prefs: []
  type: TYPE_NORMAL
- en: Other evaluation consideration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond assessing the functional capabilities and quality of a model, several
    other factors come into play when determining which model to deploy in production.
    Considerations such as cost and inference latency also play a crucial role in
    this decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying large FMs can be cost prohibitive due to the substantial infrastructure
    needed to host them. As a result, there may be a need to opt for smaller and less
    resource-intensive models in production to strike a balance between cost and model
    performance. Additionally, certain applications, like those related to fraud detection,
    demand low inference latency. This requirement further narrows down the pool of
    models that are suitable for production deployment, as low-latency models are
    preferred in such scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, model selection can be a highly iterative process based on different
    considerations and potential changes in requirements. It is important, therefore,
    to consider different models and sizes for different needs based on model performance,
    running cost, and latency for the different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Building FMs from scratch via pre-training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Organizations that want to build their own FMs would skip the model selection
    process and follow a model training process called pre-training, which is the
    process of training an ML model on a large dataset. It is a key technique in developing
    FMs and it requires large datasets, significant compute resources, and advanced
    model training techniques. The primary objective of pre-training is to acquire
    robust, general representations of the data that can be effectively applied to
    other tasks subsequently. Pre-training is typically done in a self-supervised
    manner on unlabeled data. The datasets used are very large, usually hundreds of
    gigabytes to terabytes, to teach comprehensive representations. The following
    are several techniques for LLM pre-training:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Causal language modeling**: Causal language modeling is a technique employed
    in training generative language models like GPT-3, characterized by predicting
    the next token solely based on the preceding context without access to future
    tokens. This ensures that the model learns meaningful sequential dependencies,
    promoting coherent and logical text generation during inference. During training,
    the previous context is limited by a fixed window length, and the model must predict
    tokens sequentially in a forward direction. This differs from standard bidirectional
    language modeling, where both the left and right context is seen. While more challenging,
    causal modeling enhances generation quality and equips the model to handle open-ended
    text generation tasks effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Masked language model**: Masked language model pre-training is a common technique
    for training extensive language models such as BERT. This method involves taking
    a text dataset intended for training and randomly masking a portion of tokens,
    usually around 15%, within each training example. These masked tokens are substituted
    with a distinct `[MASK]` token. The model then processes this altered input and
    is trained to forecast the original identities of the masked words. This prediction
    leverages the contextual information from nearby unmasked tokens. The model’s
    optimization is guided by a loss function that incentivizes accurate predictions
    of the originally masked words. Consequently, the model’s parameters are iteratively
    adjusted to minimize this prediction loss, leading to improved language understanding
    and generation capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next sentence prediction**: Next sentence prediction pre-training is a technique
    used in natural language processing, particularly in the training of language
    models like BERT. The objective of this technique is to enhance the model’s understanding
    of sentence relationships and context. In this approach, the model is trained
    to predict whether two consecutive sentences in a text corpus are logically connected
    or not. During training, pairs of sentences are sampled, and the model learns
    to predict whether the second sentence follows the first one coherently. This
    task encourages the model to capture semantic relationships between sentences
    and understand discourse flow. By exposing the model to this binary classification
    task, it gains the ability to comprehend sentence-level context and relationships,
    which in turn improves its performance on a wide range of downstream tasks, such
    as question answering, sentiment analysis, and text classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diffusion**: In this technique, a dataset of images paired with text captions
    is used. The model consists of an encoder and decoder, and images are incrementally
    corrupted with noise during multiple iterations. At each step, the encoder encodes
    the noisy image, and the decoder attempts to reverse the noise and recover the
    original image, facilitating denoising autoencoder training for valuable image
    representations. The process involves hyperparameters like the number of diffusion
    steps and noise levels, often trained through numerous denoising cycles. The encoder
    learns semantic image representations from noise, while the decoder learns to
    generate clean pixels from these representations. Post pre-training, the decoder
    enables image generation, and the encoder encodes images into a latent space for
    manipulation, often guided by text captions for conditional input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After FMs are pre-trained with a vast amount of training data, they would display
    a number of abilities in solving text-based or image-based problems such as fluent
    text generation or image generation.
  prefs: []
  type: TYPE_NORMAL
- en: Language models, post pre-training, accumulate substantial world knowledge and
    encode a vast range of information, concepts, and relationships from their pre-training
    data. They possess a deep understanding of language, facilitating the analysis
    of syntax, semantics, and text structure. These models excel in generating meaningful
    text with strong coherence and logical consistency. Another notable feature of
    LLMs is their capacity to efficiently tackle diverse tasks using a single model
    with various conditioning inputs. Additionally, they are adaptable to downstream
    tasks by transferring their acquired foundational knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: For image-based FMs after pre-training on large image datasets, diffusion models
    like DALL-E and Stable Diffusion have exhibited capabilities in many image tasks.
    For example, these models can be used for synthesizing highly realistic, coherent
    images that match the semantics of the prompts. You can also use these models
    to provide fine-grained control over image attributes and composition, and creatively
    recombine different concepts into novel images. Other image-related capabilities
    such as inpainting (replacing or restoring missing sections of an image), outpainting
    (expanding and filling missing parts of an image), and style transfer (tuning
    the style of images by manipulating the text inputs) also become available after
    pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training an FM demands complex engineering effort as well as significant
    compute resources and the availability of a large amount of training. Training
    these models can take weeks or months with modern compute and data infrastructures.
    The following diagram shows the high-level flow of model pre-training for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a model  Description automatically generated](img/B20836_15_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: Pre-training an LLM'
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, Pre-training involves the following key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection and preprocessing**: Gather massive text corpora from diverse
    sources like books, common crawl, web pages, and Wikipedia. Consider a mixture
    of general-purpose data and specialized data. Specialized data such as scientific
    data and code data give the model specific problem-solving capabilities. A legal
    review of collected datasets might be required to ensure compliance with any license
    or IP restrictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data preprocessing**: Raw data needs to be preprocessed to ensure high quality
    as it is pivotal to the ultimate performance of the FM. The steps normally include
    data quality checks and filtering, deduplication, privacy redaction (PII), and
    tokenization, which are important steps in data preprocessing. Unlike training
    for smaller models, it is crucial to have high-quality data before the Pre-training
    process starts, as it is very expensive to repeatedly pre-train FMs due to high
    compute resource requirements and the long time pre-training takes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model architecture selecting**: Design a transformer-based architecture like
    BERT or GPT for LLMs pre-training. There are three main variations of transformer
    architecture to consider, including encoder-only architecture, decoder-only architecture,
    and encode-decoder architecture. Each architecture has its own benefits, limitations,
    and targeted use cases. So, it is important to consider these architectures based
    on your intended objectives. For an image-based model, consider a diffusion-based
    architecture.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pre-training technique selection**: Pick a pre-training technique such as
    casual language modeling, masked language modeling, or diffusion modeling depending
    on the model type.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training infrastructure provisioning and distributed training setup**: Provision
    TPUs/GPU clusters for accelerated parallel training. Split data over many machines
    and devices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training loop**: Iterate through data batches, apply masking, predict targets,
    compute loss, and update weights. Periodically save model parameters throughout
    training. Track validation performance and stop if overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluation**: Assess pre-trained models on metrics such as perplexity and
    entropy for LLMs, and CLIP score and Fréchet inception distance for text-to-image
    diffusion models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iteration**: Repeat this process until you achieve the desired outcome.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are many commercial and open-source efforts in building pre-trained FMs,
    covering a wide range of domains. While most of the FMs are general purpose FMs
    to solve general purpose problems, some organizations are realizing the value
    of domain-specific FMs and building FMs for a particular domain such as medicine
    and finance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of sample pre-trained open-source FMs that have been
    adopted by various organizations for building generative AI solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model name** | **Description** | **Modality** | **Provider** |'
  prefs: []
  type: TYPE_TB
- en: '| T5 | **T5** (**Text-to-Text Transfer Transformer**) was developed by Google.
    It is an encoder-decoder Transformer model trained on a multi-task mixture of
    unsupervised and supervised data.T5 converts all language tasks into a unified
    text-to-text format, which simplifies model training and inference.It uses a scaled-down
    transformer architecture compared to predecessors like BERT. T5 comes in different
    sizes as large as 11 B. It is trained on the **Colossal Clean Crawled Corpus**
    (**C4**) dataset, containing hundreds of gigabytes of text from the web. It permits
    transfer learning by fine-tuning downstream tasks using standard text-to-text
    formatting. | Language | Google |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion | Stable Diffusion is an open-source text-to-image generative
    model developed by Stability AI. It is based on a convolutional autoencoder with
    latent diffusion-based sampling. Stable Diffusion can generate realistic images
    and art from text descriptions and prompts. The model was trained on LAION-5B,
    a large dataset of image-text pairs from the internet. | Image | Stability AI
    |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon | Falcon is an LLM with 40 billion parameters trained on one trillion
    tokens. Pre-training data was collected from public crawls of the web. To broaden
    Falcon’s abilities, this dataset was then extended with a few curated sources
    such as research papers and conversations from social media. | Language | TII
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2 7B | Llama 2 is an LLM developed by Meta. Llama 2 was pre-trained
    on publicly available online data sources using transformer architecture. It comes
    in different sizes from 7B to 70B parameters. It is an open-source model that
    can be used for commercial usage. | Language | Meta |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-J 6B | GPT-J is a transformer-based model with 6 billion parameters.
    It is an autoregressive decoder-only model for NLP tasks. | Language | EleutherAI
    |'
  prefs: []
  type: TYPE_TB
- en: '| Segment Anything Model (SAM) | A model that can cut out objects in any image.
    | Computer vision | Meta |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15.1: Example pre-trained FMs'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pre-training process allows models to learn representations of language
    as well as encode world knowledge within their parameters. As this model is specifically
    trained to predict the next token given an input text, its primary capability
    lies in completing sentences with a high likelihood. As a result, it is already
    capable of completing certain tasks such as completing a sentence and answering
    some questions. The following is an example of input and out using the Llama 2
    model with 7B parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, while the model provides the correct answer, Paris, it also
    generates additional unasked-for text. So, the question is, how can we make a
    pre-trained model perform a task with more precision? We will try to answer this
    question in the section on *Instruction fine-tuning*.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the high cost associated with pre-training really large FMs and the demand
    for specialized engineering and scientific expertise, only a limited number of
    organizations have the resources to undertake the pre-training of foundational
    models. Consequently, many organizations opt for adaptation and customization
    approaches to align the model with their requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptation and customization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While pre-trained large FMs already come with many capabilities that meet diverse
    requirements, they are mainly trained using general-purpose datasets and might
    not have knowledge about niche domains, such as medicine and legal, or they might
    not know how to perform a specific task. In addition, you might have your own
    proprietary data and workflow that you need the model to be aware of. In these
    cases, you will need to refine models by incorporating domain or proprietary expertise
    or improving their effectiveness in specific tasks. To accomplish this, four primary
    options are available:'
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning with human feedback
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore each of these alternatives in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation pre-training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pre-trained language models are trained on broad universal language data and
    they only hold general knowledge, as such they might underperform in niche domains
    such as finance or medicine. For example, these models might not understand niche
    vocabulary, jargon, or name entities in a highly technical or esoteric domain.
    The technique to teach model learn new knowledge in a new domain is called domain
    adaptation pre-trained. The following example shows the results from a GPT-J 6B
    model before domain adaptation and after domain adaption using SEC 10K filings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As evident, domain adaptation makes the model more familiar with financial terminologies,
    and able to come back with more contextual and coherent responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of domain adaptation is very similar to that of pre-training, also
    following the *self-supervised learning* approach using an unlabeled dataset.
    Here are the general stages encompassed in the process of domain adaptation fine-tuning
    for an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection for target domain**: Acquire relevant text data that accurately
    represents the target domain.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data preprocessing**: Clean, deduplicate, tokenize, normalize special characters,
    and redact privacy information within the collected data. This is similar to the
    pre-training process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Base pre-trained model selection**: Choose an appropriate pre-trained model
    architecture as the foundation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model initialization**: Initialize the model’s weights using a pre-trained
    checkpoint.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Domain-specific training**: Train the model using the domain-specific data
    for a predetermined number of epochs and assess training performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**In-domain testing**: Evaluate the trained model on a test dataset from the
    same domain.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative optimization**: Reiterate the training process to progressively
    enhance model performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Self-supervised learning is an ML paradigm where a model learns to generate
    its own labels from the input data, allowing it to learn meaningful representations
    and features without relying on externally provided labeled datasets.
  prefs: []
  type: TYPE_NORMAL
- en: There are many examples of domain-adapted pre-trained models. For example, FinBERT
    is the result of domain adaption of BERT for the finance domain, and LEGAL-BERT
    is a family of BERT models for the legal domain, intended to assist legal NLP
    research, computational law, and legal technology applications. Some of the more
    recent domain-adapted models include Med-PaLM 2 from Google for medical NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Although domain adaptation can enhance a model’s comprehension of a new target
    domain, it might lead to diminished performance in broader, general-purpose domains.
    To address some of these constraints, strategies like importance sampling of data
    and multi-stage domain adaptation have been investigated to mitigate such drawbacks.
    An in-depth exploration of these techniques is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FMs such as Llama 2, GPT, and Falcon are trained on diverse datasets to acquire
    general representations rather than being specialized for specific tasks. Fine-tuning
    is the process of adapting these FMs to the specific nuances and patterns present
    in particular datasets and tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This approach capitalizes on the extensive knowledge gained during the initial
    pre-training of FMs and produces models that excel at target tasks beyond the
    general capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will discuss instruction fine-tuning and parameter-efficient
    fine-tuning. Let’s get into it!
  prefs: []
  type: TYPE_NORMAL
- en: Instruction fine-tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One technique to enhance an FM’s ability to perform new tasks or improve its
    ability to perform an existing task by teaching it to follow an instruction. This
    is where instruction fine-tuning comes in. Instruction fine-tuning teaches a pre-trained
    model to perform an existing task better or learn a new task such as summarization
    or reasoning. Compared to pre-training, instruction fine-tuning requires significantly
    less data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instruction fine-tuning is a supervised approach where you need to provide
    a labeled training dataset in the form of a prompt and the expected output from
    the prompt. The following is an example of instruction fine-tuning a dataset called
    `Dolly from Databricks`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can see the complete Dolly dataset at [https://huggingface.co/datasets/databricks/databricks-dolly-15k.](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
  prefs: []
  type: TYPE_NORMAL
- en: 'After instruction fine-tuning with a labeled dataset such as Dolly-15K, the
    model should be able to generate responses with greater precision for the same
    question. For example, if we fine-tune the Llama 2 7B model with the Dolly-15k
    dataset, you will get the following output for the same text completion task from
    the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The process of instruction fine-tuning is very similar to any supervised ML,
    where you provide labeled training data, and the model learns to predict the output
    and minimize the loss between the predicted value and labels. You can also perform
    additional fine-tuning on models already fine-tuned to further improve their performance.
    The following diagram illustrates the flow of instruction fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a model  Description automatically generated](img/B20836_15_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: Instruction fine-tuning of pre-trained LLM model'
  prefs: []
  type: TYPE_NORMAL
- en: Several existing public datasets are available for instruction fine-tuning,
    such as Dolly and TriviaQA. However, when dealing with proprietary knowledge and
    specific capabilities, it becomes necessary to curate and prepare custom datasets
    for instruction fine-tuning. Depending on the domains and use cases, subject-matter
    experts may be required to assist in assembling these datasets, including crafting
    domain-specific prompts and defining desired answers. To ensure the high quality
    and standards of these datasets, a combination of human expert evaluation and
    automated scoring mechanisms should be employed. This approach ensures that the
    datasets meet rigorous criteria for accuracy and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of automated scoring and evaluation of fine-tuned models, one can leverage
    powerful models like Claude from Anthropics or GPT-4\. These models contribute
    to the robustness and precision of the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning an LLM is not without complexities. Challenges include issues such
    as overfitting, where the model excels on the training data but struggles to apply
    its knowledge to unfamiliar data, as well as the risk of catastrophic forgetting,
    which involves the model erasing its original pre-training knowledge. In order
    to tackle these obstacles, various strategies can be employed to alleviate the
    potential constraints associated with conventional fine-tuning for LLMs. These
    include implementing cautious regularization techniques such as dropout, L2 normalization,
    and early stopping to curb overfitting tendencies. The approach of gradual unfreezing
    can be adopted as well, involving a gradual release of lower layers over time
    to keep the model’s pre-existing knowledge. Engaging in multi-task training offers
    another avenue, where simultaneous fine-tuning is carried out across multiple
    datasets or objectives, fostering broader adaptability. Additionally, the method
    of continual pre-training can be integrated, supplementing fine-tuning batches
    with the initial pre-training objective to sustain foundational learning.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning can provide differentiating capabilities for organizations seeking
    a competitive edge with custom datasets and unique knowledge, without the heavy
    investment of training models from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter-efficient fine-tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regular instruction fine-tuning requires the full pre-trained model to be fine-tuned
    and all its parameters need to be available for potential updates. This requires
    significant compute resources, especially when the models are large. To address
    this challenge, a new technique called **parameter-efficient fine-tuning** (**PEFT**)
    has been introduced. PEFT refers to techniques to adapt large pre-trained language
    models to downstream tasks by introducing a small new set of trainable parameters
    instead of updating the original parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: This approach significantly reduces both computational requirements and storage
    demands. Additionally, PEFT mitigates the challenges posed by catastrophic forgetting—a
    phenomenon encountered during comprehensive LLM fine-tuning, whereby an LLM fails
    to perform tasks that it knew how to perform previously after fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several PEFT techniques available, including **Low-Rank Adaptation**
    (**LoRA**), prefix tuning, and prompt tuning. You can find out how these techniques
    work by visiting the related online resources directly. With these libraries,
    performing PEFT is a very straightforward process. For example, using LoRA for
    PEFT involves adding the following three main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary library packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a configuration corresponding to the PEFT method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wrap the base model by calling `get_peft_model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The rest of the model training steps are the same as regular training. After
    the model is fine-tuned, you can save the model by calling the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will only save the incremental PEFT weights that were trained. During inference
    time, you use the **PeftModel** package to load the base model and PEFT weights
    together as the combined model for serving. You can also merge the PEFT weights
    with the base model prior to deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With domain adaption and instruction fine-tuning, LLMs can generate compelling
    and diverse text from input prompts. However, how does an LLM know that it has
    generated a good response?
  prefs: []
  type: TYPE_NORMAL
- en: What makes a text good is hard to define as it is subjective and context dependent.
    Loss functions, such as cross-entropy, measure the accuracy of a model predicting
    the next token; however, it cannot tell whether the overall response is aligned
    with human preferences. Other metrics such as ROUGE and BLEU are designed to better
    capture human preferences by optimizing the overlap of n-grams with human-generated
    reference text, however, it does not capture semantic context.
  prefs: []
  type: TYPE_NORMAL
- en: To address the limitation of human alignment for the generated text from an
    LLM, **reinforcement learning from human feedback** (**RLHF**) was introduced
    as an additional tuning paradigm to help an LLM align with human preferences and
    values. RLHF also helps address the scaling challenges associated with human-generated
    training data for instruction fine-tuning, as it is easier for a human to rate/rank
    responses to prompts than create new prompt and response pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'With RLHF, human evaluators rank or vote on the response generated by the target
    LLM for a prompt. The ratings collected from a human can be used to train a reward
    model (usually based on the LLM model) to score the model response (a scalar value
    indicating how good a response is), and this model is then incorporated into the
    fine-tuning of the model to achieve performance that’s more aligned to human value
    or preferences. This helps with the tone, style, and creativity of output, and
    it can be used to detect ethical issues such as harmful language in the responses.
    The following diagram illustrates the flow of RLHF:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a model  Description automatically generated](img/B20836_15_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: RLHF flow'
  prefs: []
  type: TYPE_NORMAL
- en: There are many examples of RLHF-tuned FMs, including ChatGPT from OpenAI, Claude
    from Anthropic, and LLAMA-2-Chat from Meta. As many of us have experienced, these
    models have all produced compelling and human-aligned results on a wide range
    of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF is a complex and iterative process that requires careful design and implementation
    to effectively leverage human feedback for training. Some of the known challenges
    include bias in human feedback, lack of subject-matter expertise in feedback providers,
    difficulty in designing rewards for the reward model, and challenges in combining
    multiple pieces of feedback. As this is highly human-effort dependent, it is also
    a very expensive and time-consuming process.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With FMs fine-tuned, they can perform a multitude of tasks such as summarization,
    question answering, and entity extraction tasks when appropriate inputs (a.k.a.
    prompts) are provided. The following are some examples of prompts for performing
    different tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question answering**: What is the capital of France?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summarization**: `<text to summarize>` Summarize the proceeding text into
    one sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: `<text to predict>` Predict the sentiment of the proceeding
    sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mathematics**: How much does 2 + 2 equal to?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning/Logical thinking**: `<text that describes a problem>` Solve this
    problem and provide a step-by-step explanation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text generation**: Write a blog about AI/ML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: Write a piece of Python code to sort a list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, since different FMs are trained differently using different techniques
    and datasets, they could react to prompts differently. Poor prompts can result
    in models generating inaccurate, biased, or nonsensical text. In this section,
    we will focus our discussion on prompt engineering for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'A prompt consists of several key components that together influence how a model
    will react. There are several core components that make up a prompt, including
    action, context, input, and output indicators:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Action**: This is the directive given to the model that details what is expected
    in terms of the task to be performed. This could range from “classify the text
    into positive or negative” to “generate a list of ideas for a vacation in Europe.”
    Depending on the model, the instruction is usually the first part or the last
    part of the prompt and sets the overall task for the model to perform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context**: This element supplies additional information to guide the model’s
    response. For instance, in a text summarization task, you might provide some background
    on the text to be summarized (like it’s a text from an academic research paper).
    The context can help the model understand the style, tone, and specifics of the
    input data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input data:** This refers to the actual data that the model will be working
    with. In a summarization task, this would be the text to be summarized. In a question-answering
    task, this would be text from which questions are being asked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output indicator**: This element instructs the model on which format of the
    output should be used. For instance, you might specify that you want the model’s
    response in the form of a list, a paragraph, a single sentence, or any other specific
    structure. This can help narrow down the model’s output and guide it towards more
    useful responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following example shows a prompt with all these core components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on the task and the intended result, not all components are needed
    in every prompt. For example, a basic prompt might only need action and input
    data such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the prompt, many models also have support for different settings
    to help configure their output, such as the **Temperature**, `Top_p`, and `Top_k`
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The **Temperature parameter** controls the randomness of the model’s output.
    Lower values make the model’s output more deterministic, favoring the most probable
    next token. This is useful for tasks requiring precise and factual answers, like
    a fact-based question-answer system. On the other hand, increasing the **Temperature**
    value induces more randomness in the model’s responses, allowing for more creative
    and diverse results. This is beneficial for creative tasks like poem generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Top_p` **parameter** is used in the sampling technique by the model. It
    influences the determinism of the model’s response. It tells the model to include
    only possible outputs whose combined probability does not exceed the probability
    specified by `Top_p`. A lower `Top_p` value results in more exact and factual
    answers, while a higher value increases the diversity of the responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Top_k` **parameter** is also used to influence the determinism of the model’s
    response. It tells the model to include only possible outputs in the top k number
    determined by their probability. Similar to `Top_p`, a lower value of `Top_k`
    also results in more exact and factual answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to performing tasks that an LLM has been trained or tuned for, instruction
    fine-tuned LLMs can also perform new tasks without explicitly being trained on
    or learn to perform new tasks by dynamically providing them with examples. In
    the following sections, we will discuss zero-shot prompting/learning and few-shot
    prompting/learning.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot prompting/learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ability for instruction fine-tuned LLMs to perform new tasks without explicitly
    being trained is referred to as zero-shot learning/prompting. LLMs can display
    this capability because it has already acquired extensive knowledge and learned
    how to perform a multitude of tasks from specific fine-tuning. This is a very
    important capability of LLMs because it is not always feasible to train LLMs on
    all different kinds of tasks. An LLM’s ability to perform zero-shot learning is
    often a strong indicator of the LLM’s overall capability. To use zero-shot, you
    simply provide a prompt to an LLM to perform a new task that it has not been trained
    on. However, due to a lack of prior training in specific tasks, the performance
    of zero-shot prompting/learning might be limited.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot prompting/learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another capability of many LLMs is the ability to learn from examples that
    are directly provided in the prompts. For example, in addition to telling the
    model to perform an action, you can include a few examples of how it should be
    done in the context section, and LLMs would be able to learn from these examples
    and learn to perform the action on the actual input data. This is formally known
    as few-shot prompt/learning. It is also referred to as in-context learning. The
    following is an example of teaching an LLM to perform sentiment analysis by providing
    some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Few-shot learning, a key feature of LLMs, enables them to perform new tasks
    without extensive fine-tuning. However, it does have limitations, including reduced
    performance due to a small number of examples provided in each prompt, challenges
    in handling complex tasks accurately, high compute cost (examples are needed in
    every call), and potential struggles in highly specialized domains.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering best practices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prompt engineering is the process of crafting the prompts using different structures,
    phrases, contexts, and modifiers to achieve the best possible output from the
    models. It is a science as much as it is an art. Knowing the specific capability
    of a model and the data used for training and tuning is often very important when
    it comes to designing the prompts for the different models. Next, let’s take a
    look at some of the general best practices as well as LLM-specific techniques
    for designing effective prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Be very specific with the instruction**: LLM models are very capable, but
    they are also imperfect and can misinterpret the prompt if it is vague. Always
    be very specific about the length (e.g., number of words, sentences) and format
    of the output (e.g., list, table, paragraph) and the actions (e.g., summarize,
    classify, analyze) to be performed. Incorporate specific keywords relevant to
    the task domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Utilize context**: Incorporate contextual details within your prompts to
    enable the model to comprehensively grasp your inquiries. Contextual prompts can
    encompass factors like emulating a persona or providing background insights on
    the input data. By establishing a specific tone (e.g., formal, conversational,
    etc.) and perspective for the AI model, you’re essentially providing it with a
    framework that outlines the desired tone, style, and specialized expertise. This
    practice can elevate the relevance and efficacy of the generated output. If the
    context has all the information, you can explicitly instruct the model to use
    the knowledge from the context in response generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provide examples**: When formulating prompts for AI models, incorporating
    examples proves very helpful. This is because prompts serve as directives for
    the model, and examples help the model understand your requirements. The following
    is an instance of providing examples for sentiment analysis:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Experiment with prompts to learn model behaviors**: Different models have
    different capabilities and may interpret prompts differently. A well-crafted prompt
    might work well for one model, but it may not transfer well with other models.
    Try out model behaviors with different action words, sentence structures, and
    modifiers to discover how a particular model would behave. This is the art aspect
    of prompt engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ask the models to explain steps**: Some models can produce individual steps
    when providing responses to a prompt. This is also known as **chain-of-thought**
    (**CoT**) prompting. Breaking down a problem into individual steps can help improve
    the correctness of the responses. This is especially useful for reasoning and
    mathematical tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Split a complex task into a collection of smaller tasks**: If a task request
    is overly complex (i.e., having multiple tasks), a FM might not handle it effectively.
    Consider creating a number of simpler tasks and completing them separately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ask the model to use known knowledge**: Instruct the model not to return
    anything if it does not know the answer. This helps with issues such as hallucination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instruct the model for clarification**: Sometimes, the model might not truly
    understand the instruction in the prompt and return an incorrect response. Instruct
    the model to respond with clarifying questions if it does not understand the instruction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use variation to test consistency**: Use different rephrases of a prompt
    to check model output consistency. Inconsistent outputs across different prompt
    variations indicate an error or factual inaccuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Start simple**: Start with simple prompts to evaluate the responses before
    adding more elements or contexts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build an inventory of templates**: Create an inventory of curated prompt
    templates that have proven to be useful and effective for the rest of the organization
    to share and reuse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these common best practices, there are also model-specific best
    practices which are usually provided by different model providers. For example,
    you can learn more about prompt engineering guidance for Anthropic at [https://docs.anthropic.com/claude/docs/guide-to-anthropics-prompt-engineering-resources](https://docs.anthropic.com/claude/docs/guide-to-anthropics-prompt-engineering-resources).
    OpenAI also provides its prompt engineering guide at [https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api).
  prefs: []
  type: TYPE_NORMAL
- en: So, in essence, prompt engineering is an empirical, iterative process of crafting
    instructions tuned to each model and application. The human plays a key role in
    actively honing prompts. In addition, a number of commercial and free tools have
    been developed for automated prompt optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As with many ML technologies, generative AI and its prompting approaches have
    potential vulnerabilities from adversarial attacks. Bad actors can intentionally
    manipulate prompts to exploit vulnerabilities or biases in language models, resulting
    in unintended or harmful outputs. This is also known as adversarial prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are a few known examples of adversarial prompting techniques
    that can exploit vulnerabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt injection** is a technique used in adversarial prompting where additional
    instructions or content are inserted into the prompt to influence the model’s
    behavior. By injecting specific keywords, phrases, or instructions, the model’s
    output can be manipulated to produce desired or undesired outcomes. Prompt injection
    can be used to introduce biases, generate offensive or harmful content, or manipulate
    the model’s understanding of the task. The following is an example of prompt injection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Prompt leaking** occurs when sensitive or confidential information unintentionally
    gets exposed in the model’s response. This can happen when the model incorporates
    parts of the prompt, including personally identifiable information, into its generated
    output. Prompt leaking poses privacy and security risks, as it may disclose sensitive
    data to unintended recipients or expose vulnerabilities in the model’s handling
    of input prompts. The following is an example of prompt leaking:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Jailbreaking**, in the context of prompt engineering, refers to bypassing
    or overriding safety mechanisms put in place to restrict or regulate the behavior
    of language models. It involves manipulating the prompt in a way that allows the
    model to generate outputs that may be inappropriate, unethical, or against the
    intended guidelines. Jailbreaking can lead to the generation of offensive content,
    misinformation, or other undesirable outcomes. The following is an example jailbreaking
    prompt:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Overall, adversarial prompting techniques like prompt injection, prompt leaking,
    and jailbreaking highlight the importance of responsible and ethical prompt engineering
    practices. It is essential to be aware of the potential risks and vulnerabilities
    associated with language models and to take precautions to mitigate these risks
    such as adversarial prompt detectors while ensuring the safe and responsible use
    of these powerful AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Model management and deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the generative AI model trained, tuned, tested, and the potential risks
    mitigated or accepted, the next step is to place it under proper model management
    and deploy the model for application and user consumption. The management for
    generative AI models is largely similar to that of traditional ML models, with
    some new process and management considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Process for capturing additional data**: FMs are designed for downstream
    tasks as well as direct consumption. Consequently, it is imperative to capture
    additional information, such as a dataset for pre-trained, data for fine-tuning,
    a dataset for testing, and the associated model performance metrics (both automated
    and human evaluation) across various tasks. Other information such as intended
    usage and limitations of these FMs need to be captured and documented. This will
    help with the model selection process for the different downstream tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Process for usage review and approval**: The usage of powerful FMs should
    be governed for proper use to avoid unintended risks. An enhanced or new FM model
    review and approval process should be established.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**New technology capability**: New or enhanced technology capabilities such
    as model registry need to be implemented to support the technical management of
    FMs and human processes and workflows. For example, FMs and their respective PEFT-tuned
    adapters need to be properly stored and tracked, and, if needed, a technical capability
    to combine an FM and an adapter can be implemented to support FM distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the requirements for accelerated computation and large GPU memory, hosting
    generative AI models presents unique challenges across several dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Given the large size of many of the generative models (hundreds of GBs in size),
    they require expensive hardware with a large amount of memory and compute resources.
    Hence, it could get very expensive to run these models. Also, many of these models
    cannot fit into a single GPU or single node, so they will need to be split up
    across multiple GPU devices. In addition, large models are also slower with inferences
    in general.
  prefs: []
  type: TYPE_NORMAL
- en: To help address these challenges, several engineering approaches have been developed
    to support the deployment of these large models.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed in *Chapter 10*, *Advanced ML Engineering*, model size can be
    reduced through techniques such as pruning (selectively removing non-critical
    structures in a model), model weights quantization (e.g., reducing the precision
    from 32-bit to 16-bit), distillation (training a smaller model to mimic the behaviors
    of a large model), and model optimization. The goal of this approach is to fit
    the model into a single GPU or a smaller number of GPUs with a reduced size. While
    all these approaches can be used, the post-training quantization method is the
    most popular one due to its broad support in various ML frameworks and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: If the model with a reduced size still does not fit into a single GPU memory,
    then another deployment approach is to split the model across multiple GPU devices
    in a single node. This is also referred to as tensor parallelism. SageMaker **large
    model inference** (**LMI**) **deep learning containers** (**DLCs**) can help with
    hosting LLMs across multiple devices. LMI DLCs are a complete end-to-end solution
    for hosting LLMs. At the frontend, they include a high-performance model server
    (DJL Serving) designed for large model inference with features such as token streaming
    and automatic model replication within an instance to increase throughput. On
    the backend, LMI DLCs also include several high-performance model parallel engines,
    such as DeepSpeed and FasterTransformer, which can shard and manage model parameters
    across multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: These engines also include optimized kernels for popular transformer models,
    which can accelerate inference by up to three times faster. Another popular technology
    for hosting large models is the **Text Generation Interference** (**TGI**) from
    Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: With the ability to fine-tune large FMs using techniques such as PEFT, it is
    now also possible to dynamically attach different fine-tuned adapters to common
    pre-trained FMs to reduce hosting costs.
  prefs: []
  type: TYPE_NORMAL
- en: The limitations, risks, and challenges of adopting generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As powerful as generative AI technology is, it comes with its own set of limitations
    and challenges across multiple dimensions. In this section, we will delve into
    some of these concerns.
  prefs: []
  type: TYPE_NORMAL
- en: As most generative AI technologies such as LLMs generate responses based on
    conditioned probabilities, the outputs can be factually inaccurate or self-contradictory.
    They can even generate factually inaccurate responses with fluency and convincing
    tones, leading to difficulty in detecting misinformation by humans. This can create
    a multitude of problems, including erroneous decision making and negative social
    influence from misinformation. Moreover, it’s challenging to determine the source
    documents for the responses generated by generative AI models, which leads to
    difficulties in verifying facts and providing proper attribution.
  prefs: []
  type: TYPE_NORMAL
- en: Despite their impressive performance on standardized tests like BAR or SAT,
    LLMs have limitations in certain aspects of cognitive abilities. One notable limitation
    is their inability to engage in complex reasoning and long-range strategic planning.
    While they excel at processing and generating text based on patterns and existing
    knowledge, they struggle with tasks that require a deep understanding of context
    and the ability to make nuanced decisions. These models also lack the fundamental
    understanding of common-sense knowledge that humans take for granted. As a result,
    these models may perform well in structured assessments but fall short when faced
    with real-world scenarios that demand higher-order thinking and contextual reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI technologies have also raised many ethical and social concerns
    such as copyrights and displacement of jobs. Ownership and copyright of synthetic
    media generated are legally ambiguous. Since generative AI models are pre-trained
    with vast amounts of data, including potentially copyrighted data from the internet,
    the content generated by generative models can potentially raise copyright concerns.
    It is also challenging to attribute the generated text to the original training
    data. Just like traditional AI/ML technology, generative AI has the potential
    to displace many known jobs such as content creators and document analysts. Moreover,
    generative AI can contain personal data and can potentially output that data,
    leading to privacy leaks.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, training, fine-tuning, and performing inferences with large generative
    AI models can be costly due to their substantial computational resource requirements.
    However, there have been advancements in developing techniques to enhance their
    efficiency. As generative AI is so new and evolving quickly, both public and private
    policies and governance are lagging behind to effectively and appropriately guide
    the development and deployment of generative AI technology and solutions. In addition,
    generative AI technologies do not provide good ways to deal with interpretability.
    It is very difficult, if not impossible, to know how generative AI comes to certain
    responses and decisions. This limits what kind of use cases generative AI can
    be applied to.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the limits and challenges are yet to have practical solutions. So, it
    is essential to assess and understand the risks before deploying generative AI
    solutions for the intended use cases. Consider mitigating measures, such as the
    human-in-the-loop method, for decision making and grounding of generative AI with
    curated data sources to reduce hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we provided a comprehensive overview of the generative AI project
    lifecycle, from identifying business use cases to model deployment. We explored
    major generative technologies like FMs and key techniques for customization including
    domain adaptation, instruction tuning, reinforcement learning with human feedback,
    and prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also covered specialized engineering considerations around large
    model hosting and mitigating risks like factual inaccuracies. While limitations
    exist, responsible development and governance can allow enterprises across industries
    to harness generative AI’s immense potential for creating business value. With
    an understanding of the end-to-end lifecycle, practitioners can thoughtfully architect
    and deliver innovative yet practical generative AI solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will talk about the key considerations for building
    a generative AI platform, **retrieval-augmented generation** (**RAG**) solutions,
    and practical generative AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code70205728346636561.png)'
  prefs: []
  type: TYPE_IMG
