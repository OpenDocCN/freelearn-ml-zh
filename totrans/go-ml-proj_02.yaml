- en: Linear Regression - House Price Prediction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归 - 房价预测
- en: Linear regression is one of the world's oldest machine learning concepts. Invented
    in the early nineteenth century, it is still one of the more vulnerable methods
    of understanding the relationship between input and output.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是世界上最早的机器学习概念之一。在19世纪初发明，它仍然是理解输入和输出之间关系的一种较为脆弱的方法。
- en: The ideas behind linear regression is familiar to us all. We feel that some
    things are correlated with one another. Sometimes they are causal in nature. There
    exists a very fine line between correlation and causation. For example, summer
    sees more sales in ice creams and cold beverages, while winter sees more sales
    in hot cocoa and coffee. We could say that the seasons themselves cause the amount
    of sales—they're causal in nature. But are they really?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归背后的思想对我们来说都很熟悉。我们觉得有些事物彼此相关。有时它们在本质上具有因果关系。相关性和因果关系之间存在着一条非常细微的界限。例如，夏天冰淇淋和冷饮的销售量增加，而冬天热巧克力饮料和咖啡的销售量增加。我们可以这样说，季节本身导致了销售量的变化——它们在本质上具有因果关系。但它们真的是这样吗？
- en: Without further analysis, the best thing we can say is that they are correlated
    with one another. The phenomenon of summer is connected to the phenomenon of greater-than
    the-rest-of-the-year sales of cold drinks and ice cream. The phenomenon of winter
    is connected, somehow, to the phenomenon of greater-than-the-rest-of-the-year
    sales of hot beverages.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 没有进一步的分析，我们所能说的最好的事情就是它们彼此相关。夏天的现象与一年中冷饮和冰淇淋销售量超过其他时间的现象相联系。冬天的现象，以某种方式，与一年中热饮料销售量超过其他时间的现象相联系。
- en: Understanding the relationship between things is what linear regression, at
    its core, is all about. There can be many lenses through which linear regression
    may be viewed, but we will be viewing it through a machine learning lens. That
    is to say, we wish to build a machine learning model that will accurately predict
    the results, given some input.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 理解事物之间的关系是线性回归的核心所在。线性回归可以从许多不同的角度来观察，但我们将从机器学习的角度来观察。也就是说，我们希望构建一个机器学习模型，能够根据一些输入准确预测结果。
- en: The desire to use correlation for predictive purposes was indeed the very reason
    why linear regression was invented in the first place. Francis Galton, who was
    coincidentally Charles Darwin's cousin, hailed from an upper-class family whose
    lineage included doctors. He had given up his medical studies after a nervous
    breakdown and began travelling the world as a geologist—this was back when being
    a geologist was the coolest job (much like being a data scientist today)—however,
    it was said that Galton hadn't the mettle of Darwin, and soon he gave up the idea
    of travelling around the world, soured by experiences in Africa. Having inherited
    his wealth after his father died, Galton dabbled in all things that tickled his
    fancy, including biology.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相关性进行预测的愿望确实是线性回归最初被发明的原因。弗朗西斯·高尔顿（Francis Galton）偶然是查尔斯·达尔文（Charles Darwin）的表亲，来自一个上层社会家庭，家族中有医生。他在经历了一次神经崩溃后放弃了医学研究，开始作为地质学家周游世界——那是在地质学家是最酷的工作的时候（就像今天的数据科学家一样）——然而，据说高尔顿没有达尔文的勇气，不久他就放弃了周游世界的想法，对非洲的经历感到失望。在父亲去世后，高尔顿继承了财富，开始涉足所有能引起他兴趣的事物，包括生物学。
- en: The publication of his cousin's magnum opus, *On the Origin of Species*, made
    Galton double down on his pursuits in biology and ultimately, eugenics. Galton
    experimented, rather coincidentally in the same manner as Mendel, on peas. He
    had wanted to predict the characteristics of the offspring plants, when only information
    about the parent plants' characteristics were available. He realized that the
    offspring was often somewhere in between the characteristics of the parent plants.
    When Galton realized that he could derive a mathematical equation that represented
    inheritance using elliptical curve fitting, he invented regression.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 他表亲的巨著《物种起源》（*On the Origin of Species*）的出版使高尔顿加倍致力于生物学研究，最终转向优生学。高尔顿像孟德尔一样，偶然地在豌豆上进行实验。他想要预测后代植物的特征，而当时只有关于亲本植物特征的信息。他意识到后代植物的特征通常位于亲本植物特征之间。当高尔顿意识到他可以通过椭圆曲线拟合推导出一个表示遗传的数学方程时，他发明了回归。
- en: 'The reasoning behind regression was simple: there was a driving force—a signal
    of sorts—that led the characteristics of the offspring plants to go towards the
    curve he had fitted. If that was the case, it meant that the driving force obeyed
    some mathematical law. And if it did obey the mathematical laws, then it could
    be used for prediction, Galton reasoned. To further refine his ideas, he sought
    the help of the mathematician Karl Pearson.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 回归背后的推理很简单：有一个驱动力——一种信号，它导致后代植物的特征趋向于他拟合的曲线。如果是这样，这意味着驱动力遵循某种数学定律。如果它确实遵循数学定律，那么它可以用于预测，高尔顿推理。为了进一步细化他的想法，他寻求数学家卡尔·皮尔逊的帮助。
- en: It took Galton and Pearson a few more attempts to refine the concept and quantify
    the trends. But ultimately they adopted a least-squares methodology for fitting
    the curves.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 高尔顿和皮尔逊需要尝试几次来细化概念并量化趋势。但最终，他们采用了最小二乘法来拟合曲线。
- en: Even to this day, when linear regression is mentioned, it can be safely assumed
    that a least- squares model will be used, which is precisely what we will be doing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 即使到现在，当提到线性回归时，可以安全地假设将使用最小二乘模型，这正是我们将要做的。
- en: We will be performing exploratory data analysis—this will allow us to understand
    the data better. Along the way, we will build and use the data structures necessary
    for a machine learning project. We will rely heavily on Gonum's plotting libraries
    for that. After that, we will run a linear regression, interpret the results,
    and identify the strengths and weaknesses of this technique of machine learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行探索性数据分析——这将使我们更好地理解数据。在这个过程中，我们将构建和使用机器学习项目所需的数据结构。我们将大量依赖Gonum的绘图库。之后，我们将运行线性回归，解释结果，并确定这种机器学习技术的优缺点。
- en: The project
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目
- en: What we want to do is to create a model of house prices. We will be using this
    open source dataset of house prices ([https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data))
    for our linear regression model. Specifically, the dataset is the data of price
    of houses that have been sold in the Ames area in Massachusetts, and their associated
    features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要做的是创建一个房价模型。我们将使用这个开源房价数据集（[https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)）来构建我们的线性回归模型。具体来说，数据集是马萨诸塞州阿默斯地区已售房屋的价格及其相关特征。
- en: 'As with any machine learning project, we start by asking the most basic of
    questions: what do we want to predict? In this case, I''ve already indicated that
    we''re going to be predicting house prices, therefore all the other data will
    be used as signals to predict house prices. In statistical parlance, we call house
    prices the dependent variable and the other fields the independent variables.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何机器学习项目一样，我们首先提出最基本的问题：我们想要预测什么？在这种情况下，我已经指出我们将预测房价，因此所有其他数据都将用作预测房价的信号。在统计学中，我们称房价为因变量，其他字段为自变量。
- en: In the following sections, we will build a graph of dependent logical conditions,
    then with that as a plan, write a program that finds a linear regression model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将构建一个依赖逻辑条件的图表，然后以此作为计划，编写一个寻找线性回归模型的程序。
- en: Exploratory data analysis
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Exploratory data analysis is part and parcel of any model-building process.
    Understanding the algorithm at play, too, is important. Given that this chapter
    revolves around linear regression, it might be worth it to explore the data through
    the lens of understanding linear regression.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析是任何建模过程的组成部分。理解正在运行的算法也同样重要。鉴于本章围绕线性回归展开，探索数据以理解线性回归的角度可能是有益的。
- en: But first, let's look at the data. One of the first things I recommend any budding
    data scientist keen on machine learning to do is to explore the data, or a subset
    of it, to get a feel for it. I usually do it in a spreadsheet application such
    as Excel or Google Sheets. I then try to understand, in human ways, the meaning
    of the data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们看看数据。我建议任何热衷于机器学习的初学者做的第一件事就是探索数据，或者数据的一个子集，以了解其感觉。我通常在电子表格应用程序，如Excel或Google
    Sheets中这样做。然后我尝试以人类的方式理解数据的含义。
- en: 'This dataset comes with a description of fields, which I can''t enumerate in
    full here. A snapshot, however, would be illuminating for the rest of the discussion
    in this chapter:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集附带字段描述，我无法在此全部列举。然而，本章后续讨论的一个快照将是有启发性的：
- en: '`SalePrice`: The property''s sale price in dollars. This is the dependent variable
    that we''re trying to predict.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SalePrice`：物业的售价（美元）。这是我们试图预测的依赖变量。'
- en: '`MSSubClass`: The building class.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MSSubClass`：建筑类别。'
- en: '`MSZoning`: The general zoning classification.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MSZoning`：一般的分区分类。'
- en: '`LotFrontage`: The linear feet of the street connected to the property.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LotFrontage`：与物业相连的街道的线性英尺数。'
- en: '`LotArea`: The lot size in square feet.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LotArea`：地块面积（平方英尺）。'
- en: There can be multiple ways of understanding linear regression. However, one
    of my favorite ways of understanding linear regression directly ties into exploratory
    data analysis. Specifically, we're interested in looking at linear regression
    through the lens of the **conditional** **expectation** **functions** (**CEFs**)
    of the independent variable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 理解线性回归的方式可能有多种。然而，我最喜欢的一种理解线性回归的方式与探索性数据分析直接相关。具体来说，我们感兴趣的是通过独立变量的**条件期望函数**（**CEFs**）来观察线性回归。
- en: 'The conditional expectation function of a variable is simply the expected value
    of the variable, dependent upon the value of another variable. This seems like
    a rather dense subject to get through, so I shall offer three different views
    of the same topic in an attempt to clarify:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的条件期望函数简单地说就是变量的期望值，取决于另一个变量的值。这似乎是一个相当复杂的话题，所以我将提供三种不同观点的同一主题，以试图澄清：
- en: '**Statistical point of view**: The conditional expectation function of a dependent
    variable ![](img/2cf1f34c-e741-4f73-8546-3259edb2f161.png)given a vector of covariates
    ![](img/6fae8631-f622-418d-afbd-1e84858f95dd.png)is simply the expected value
    of ![](img/3e4b7cfd-0811-4773-80a8-2a44003be7cd.png)(the average) when ![](img/f10daf27-0d56-46bf-89d0-354502d9dc2f.png)is
    fixed to ![](img/e26b71db-9cd5-4c99-b437-528a9b9ef5e6.png).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计观点**：给定协变量向量![](img/6fae8631-f622-418d-afbd-1e84858f95dd.png)的依赖变量的条件期望函数![](img/2cf1f34c-e741-4f73-8546-3259edb2f161.png)简单地是当![](img/f10daf27-0d56-46bf-89d0-354502d9dc2f.png)固定为![](img/e26b71db-9cd5-4c99-b437-528a9b9ef5e6.png)时的期望值（平均值）。'
- en: '**Programming point of view in pseudo-SQL**: `select avg(Y) from dataset where
    X = ''Xi''`. When conditioning upon multiple conditions, it''s simply this: `select
    avg(Y) from dataset where X1 = ''Xik'' and X2 = ''Xjl''`.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伪SQL编程观点**：`select avg(Y) from dataset where X = ''Xi''`。当基于多个条件进行条件化时，它只是这样：`select
    avg(Y) from dataset where X1 = ''Xik'' and X2 = ''Xjl''`。'
- en: '**Concrete example**: What are the expected house prices if one of the independent
    variables—say, MSZoning—is RL? The expected house price is the population average,
    which translates to: of all the houses in Boston, what is the average price of
    house sold whose zoning type is RL?'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具体例子**：如果其中一个独立变量——比如说，MSZoning是RL，那么预期的房价是多少？预期的房价是人口平均值，这可以转化为：在波士顿的所有房屋中，
    zoning类型为RL的房屋的平均售价是多少？'
- en: As it stands, this is a pretty bastardized version of what the CEF is—there
    are some subtleties involved in the definition of the CEF, but that is not within
    the scope of this book, so we shall leave that for later. For now, this rough
    understanding of CEF is enough to get us started with our exploratory data analysis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如此看来，这是对CEF的相当简化的版本——在CEF的定义中涉及一些细微之处，但这超出了本书的范围，所以我们将其留到以后。现在，对CEF的这种粗略理解足以让我们开始探索性数据分析。
- en: The programming point of view in pseudo-SQL is useful because it informs us
    about what we would need so that we can quickly calculate the aggregate of data.
    We would need to create indices. Because our dataset is small, we can be relatively
    blasé about the data structures used to index the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 伪SQL的编程观点是有用的，因为它告诉我们我们需要什么，以便我们可以快速计算数据的汇总。我们需要创建索引。由于我们的数据集很小，我们可以相对随意地选择用于索引数据的数据结构。
- en: Ingestion and indexing
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据摄取和索引
- en: Perhaps the best way to index the data is to do it at the time of ingestion.
    We will use the `encoding/csv` package found in the `Go standard` library to ingest
    the data and build the index.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最好的索引数据的方式是在摄取数据时进行。我们将使用`Go标准库`中找到的`encoding/csv`包来摄取数据并建立索引。
- en: Before we dive into the code, let's look at the notion of an index, and how
    one might be built. While indexes are extremely commonly used in databases, they
    are applicable in any production system as well. The purpose of the index is to
    allow us to access data quickly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入代码之前，让我们看看索引的概念以及它是如何构建的。虽然索引在数据库中非常常用，但它们也适用于任何生产系统。索引的目的是让我们能够快速访问数据。
- en: We want to build an index that will allow us to know at any time which row(s)
    has the value. In systems with much larger datasets, a more complicated index
    structure (such as a B-Tree) might be used. In the case of this dataset, however,
    a map-based index would be more than sufficient.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要构建一个索引，使我们能够随时知道哪些行具有某个值。在具有大量数据集的系统上，可能需要使用更复杂的索引结构（如 B-Tree）。然而，对于这个数据集，基于映射的索引就足够了。
- en: 'This is what our index looks like: `[]map[string][]int`—it''s a slice of maps.
    The first slice is indexed by the columns—meaning if we want column `0`, we simply
    get `index[0]`, and get `map[string][]int` in return. The map tells us what values
    are in the columns (the key of the map), and what rows contain those values (the
    value of the map).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的索引看起来像什么：`[]map[string][]int`——它是一个映射切片。第一个切片按列索引——这意味着如果我们想获取列 `0`，我们只需获取
    `index[0]`，然后返回 `map[string][]int`。映射告诉我们列中有什么值（映射的键），以及哪些行包含这些值（映射的值）。
- en: 'Now, the question turns to: how do you know which variables associate with
    which column? A more traditional answer would be to have something like `map[string]int`,
    where the key represents the variable name and the value represents the column
    number. While that is a valid strategy, I prefer to have `[]string` as the associative
    map between the index and column name. Searching is O(N), but for the most part,
    if you have named variables, N is small. In future chapters, we shall see much
    much larger Ns.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题转向：你如何知道哪些变量与哪些列相关联？一个更传统的答案可能是使用类似 `map[string]int` 的结构，其中键代表变量名，值代表列号。虽然这是一个有效的策略，但我更喜欢使用
    `[]string` 作为索引和列名之间的关联映射。搜索的时间复杂度是 O(N)，但大多数情况下，如果你有命名的变量，N 是很小的。在未来的章节中，我们将看到更大的
    N 值。
- en: 'So, we return the index of column names as `[]string` or, in the case of reading `CSVs`,
    it''s simply the first row, as shown in the following code snippet:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们返回列名的索引作为 `[]string`，或者在读取 `CSV` 的情况下，它简单地是第一行，如下面的代码片段所示：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Reading this code snippet, a good programmer would have alarm bells going off
    in their head. Why is everything a string? The answer to that is quite simple:
    we''ll convert the types later. All we need right now is some basic count-based
    statistics for exploratory data analysis.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这段代码片段，一个优秀的程序员会在脑海中响起警钟。为什么一切都是字符串类型？答案很简单：我们稍后会转换类型。我们现在需要的只是进行一些基于计数的基本统计数据，以进行数据探索性分析。
- en: 'The key is in the indexes that are returned by the function. What we have is
    a column count of unique values. This is how to count them:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于函数返回的索引中。我们有一个列的唯一值计数。这是如何计数的：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With this, we can then analyze the cardinality of each individual column—that
    is how many distinct values there are. If there are as many distinct values as
    there are rows in each column, then we can be quite sure that the column is not
    categorical. Or, if we know that the column is categorical, and there are as many
    distinct values as there are rows, then we know for sure that the column cannot
    be used in a linear regression.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们可以分析每个单独列的基数——即有多少个不同的值。如果一个列中的不同值与行数相同，那么我们可以相当确信该列不是分类数据。或者，如果我们知道该列是分类数据，并且不同值与行数相同，那么我们可以确定该列不能用于线性回归。
- en: 'Our main function now looks like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主函数现在看起来是这样的：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For completeness, this is the definition of `mHandleError`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，这是 `mHandleError` 的定义：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A quick `go run *.go` indicates this result (which has been truncated):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 快速运行 `go run *.go` 可以得到这个结果（已被截断）：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Alone, this tells us a lot of interesting facts, chief amongst which is that
    there is a lot more categorical data than there is continuous data. Additionally,
    for some columns that are indeed continuous in nature, there are only a few discrete
    values available. One particular example is the `LowQualSF` column—it's a continuous
    variable, but there are only 24 unique values.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 单独来看，这告诉我们很多有趣的事实，其中最显著的是，分类数据比连续数据要多得多。此外，对于一些本质上连续的列，可用的离散值只有少数。一个特定的例子是 `LowQualSF`
    列——它是一个连续变量，但只有 24 个唯一的值。
- en: We'd like to calculate the CEF of the discrete covariates for further analysis.
    But before that can happen, we would need to clean up the data. While we're at
    it, we might also want to create a logical grouping of data structures.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想计算离散协变量的CEF以进行进一步分析。但在那之前，我们需要清理数据。在这个过程中，我们可能还想创建数据结构的逻辑分组。
- en: Janitorial work
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洁工作
- en: A large part of doing data science work is focused on cleanup. In productionized
    systems, this data would typically be fetched directly from the database, already
    relatively clean (high -quality production data science work requires a database
    of clean data). However, we're not in production mode yet. We're still in the
    model-building phase. It would be helpful to imagine writing a program solely
    for cleaning data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学工作的很大一部分集中在清理上。在生产化系统中，这些数据通常会直接从数据库中获取，已经相对干净（高质量的生产数据科学工作需要一个干净的数据库）。然而，我们目前还没有进入生产模式。我们仍然处于模型构建阶段。想象编写一个专门用于清理数据的程序会有所帮助。
- en: 'Let''s look at our requirements: starting with our data, each column is a variable—most
    of them are independent variables, except for the last column, which is the dependent
    variable. Some variables are categorical, and some are continuous. Our task is
    to write a function that will convert the data, currently `[][]string` to `[][]float64`.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的需求：从我们的数据开始，每一列是一个变量——大多数是独立变量，除了最后一列，它是因变量。一些变量是分类的，一些是连续的。我们的任务是编写一个函数，将数据从当前的`[][]string`转换为`[][]float64`。
- en: 'To do that, we would require all the data to be converted into `float64`. For
    the continuous variables, it''s an easy task: simply parse the string into a float.
    There are oddities that need to be handled, which I hope you had spotted by the
    time you opened the file in a spreadsheet. But the main pain is in converting
    categorical data to `float64`.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要将所有数据转换为`float64`。对于连续变量，这是一个简单的任务：只需将字符串解析为浮点数。有一些异常需要处理，希望你在打开文件到电子表格中时已经注意到了。但主要的问题在于将分类数据转换为`float64`。
- en: Fortunately for us, people much smarter than have figured this out decades ago.
    There exists an encoding scheme that allows categorical data to play nicely with
    linear regression algorithms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，比我们聪明得多的人早在几十年前就解决了这个问题。存在一种编码方案，允许分类数据与线性回归算法良好地配合。
- en: Encoding categorical data
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码分类数据
- en: The trick to encode categorical data is to expand categorical data into multiple
    columns, each having a 1 or 0 representing whether it's true or false. This of
    course comes with some caveats and subtle issues that must be navigated with care.
    For the rest of this subsection, I shall use a real categorical variable to explain
    further.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 编码分类数据的技巧是将分类数据扩展为多个列，每列有一个1或0，表示它是真还是假。这当然伴随着一些需要注意的警告和微妙的问题。在接下来的这个子节中，我将使用一个真实的分类变量来进一步解释。
- en: 'Consider the `LandSlope` variable. There are three possible values for `LandSlope`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑`LandSlope`变量。`LandSlope`有三个可能的值：
- en: Gtl
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gtl
- en: Mod
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mod
- en: Sev
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sev
- en: 'This is one possible encoding scheme (this is commonly known as one-hot encoding):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种可能的编码方案（这通常被称为独热编码）：
- en: '| **Slope** | **Slope_Gtl** | **Slope_Mod** | **Slope_Sev** |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| **Slope** | **Slope_Gtl** | **Slope_Mod** | **Slope_Sev** |'
- en: '| Gtl | 1 | 0 | 0 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Gtl | 1 | 0 | 0 |'
- en: '| Mod | 0 | 1 | 0 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Mod | 0 | 1 | 0 |'
- en: '| Sev | 0 | 0 | 1 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Sev | 0 | 0 | 1 |'
- en: 'This would be a terrible encoding scheme. To understand why, we must first
    understand linear regression by means of ordinary least squares. Without going
    into too much detail, the meat of OLS-based linear regression is the following
    formula (which I am so in love with that I have had multiple T-shirts with the
    formula printed on):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个糟糕的编码方案。要理解为什么，我们首先必须通过普通最小二乘法来理解线性回归。不过不深入细节，基于OLS的线性回归的核心是以下公式（我非常喜欢这个公式，以至于我有多件印有这个公式的T恤）：
- en: '![](img/14494194-447a-4049-8a0d-ac848552b99e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14494194-447a-4049-8a0d-ac848552b99e.png)'
- en: Here,![](img/2c92b8b8-8893-4bf6-b17e-7e386e28d60e.png)is an(m x n) matrix and![](img/385ff6f9-3b14-4131-bd02-26b875c70791.png)is
    an (m x 1) vector. The multiplications, therefore, are not straightforward multiplications—they
    are matrix multiplications. When one-hot encoding is used for linear regression,
    the resulting input matrix![](img/e1cffed5-f7e4-41fe-be0e-53e4b12f04d4.png)will
    typically be singular—in other words, the determinant of the matrix is 0\. The
    problem with singular matrices is that they cannot be inverted.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/2c92b8b8-8893-4bf6-b17e-7e386e28d60e.png)是一个(m x n)矩阵，而![](img/385ff6f9-3b14-4131-bd02-26b875c70791.png)是一个(m
    x 1)向量。因此，这些乘法不是简单的乘法——它们是矩阵乘法。当使用独热编码进行线性回归时，得到的输入矩阵![](img/e1cffed5-f7e4-41fe-be0e-53e4b12f04d4.png)通常会是奇异的——换句话说，矩阵的行列式为0。奇异矩阵的问题在于它们不能被求逆。
- en: 'So, instead, we have this encoding scheme:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们采用以下编码方案：
- en: '| **Slope** | **Slope_Mod** | **Slope_Sev** |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **斜率** | **斜率_模** | **斜率_严重程度** |'
- en: '| Gtl | 0 | 0 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Gtl | 0 | 0 |'
- en: '| Mod | 1 | 0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 1 | 0 |'
- en: '| Sev | 0 | 1 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Sev | 0 | 1 |'
- en: Here, we see an application of the Go proverb make the zero value useful for
    being applied in a data science context. Indeed, clever encoding of categorical
    variables will yield slightly better results when dealing with previously unseen
    data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到Go谚语的应用，将零值变得有用，以便在数据科学环境中应用。确实，对分类变量进行巧妙的编码在处理先前未见过的数据时会产生略微更好的结果。
- en: The topic is far too wide to broach here, but if you have categorical data that
    can be partially ordered, then when exposed to unseen data, simply encode the
    unseen data to the closest ordered variable value, and the results will be slightly
    better than encoding to the zero value or using random encoding. We will cover
    more of this in the later parts of this chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个话题太广泛，无法在这里展开，但如果你的分类数据可以部分排序，那么当遇到未见过的数据时，只需将未见过的数据编码到最接近的有序变量值，结果将略好于编码到零值或使用随机编码。我们将在本章的后续部分更多地介绍这一点。
- en: Handling bad numbers
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理不良数据
- en: Another part of the janitorial work is handling bad numbers. A good example
    is in the `LotFrontage` variable. From the data description, we know that this
    is supposed to be a continuous variable. Therefore, all the numbers should be
    directly convertible to `float64`. Looking at the data, however, we see that it's
    not true—there is data that is NA.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一部分的清洁工作就是处理不良数据。一个很好的例子是在`LotFrontage`变量中。从数据描述中，我们知道这应该是一个连续变量。因此，所有数字都应该可以直接转换为`float64`。然而，当我们查看数据时，我们发现并非如此——存在NA数据。
- en: '`LotFrontage`, according to the description, is the linear feet of the street
    connected to property. NA could mean one of two things:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 根据描述，`LotFrontage`是连接到财产的街道的线性英尺。NA可能意味着两种情况之一：
- en: We have no information on whether there is a street connected to the property
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有关于是否有街道连接到该财产的信息
- en: There is no street connected to the property
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有街道连接到该财产
- en: In either case, it would be reasonable to replace NA with 0\. This is reasonable,
    because the second lowest value in `LotFrontage` is 21\. There are other ways
    of imputing the data, of course, and often the imputations will lead to better
    models. But for now, we'll impute it with 0.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，用0替换NA都是合理的。这是合理的，因为`LotFrontage`的第二低值是21。当然，还有其他方法可以插补数据，而且通常插补会导致更好的模型。但就目前而言，我们将用0进行插补。
- en: 'We can also do the same with any other continuous variables in this dataset
    simply because they make sense when you replace the NA with 0\. One tip is to
    use it in a sentence: this house has an Unknown `GarageArea`. If that is the case,
    then what should be the best guess? Well, it''d be helpful to assume that the
    house has no garage, so it''s OK to replace NA with 0.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用这个方法处理这个数据集中任何其他连续变量，因为当你用0替换NA时，它们是有意义的。一个技巧是在句子中使用它：这所房子有一个未知的`GarageArea`。如果是这种情况，那么最好的猜测是什么？嗯，假设这所房子没有车库，所以用0替换NA是合理的。
- en: Note that this may not be the case in other machine learning projects. Remember—human
    insight may be fallible, but its often the best solution for a lot of irregularities
    in the data. If you happen to be a realtor, and you have a lot more domain knowledge,
    you can infuse said domain knowledge into the imputation phase—you can use variables
    to calculate and estimate other variables for example.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在其他机器学习项目中可能并非如此。记住——人类的洞察力可能是有缺陷的，但通常它是解决数据中许多不规则性的最佳方案。如果你是一名房地产经纪人，并且拥有更多的领域知识，你可以在插补阶段注入这种领域知识——例如，你可以使用变量来计算和估计其他变量。
- en: 'As for the categorical variables, we can for the most part treat NA as the
    zero value of the variable, so no change there if there is an NA. There is some
    categorical data for which NA or None wouldn''t make sense. This is where the
    aforementioned clever encoding of category could come in handy. In the cases of
    these variables, we''ll use the most commonly found value as the zero value:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类变量，我们大部分可以将NA视为变量的零值，所以如果有NA，那里不会有变化。对于某些分类数据，NA或None可能没有意义。这就是上述巧妙编码分类数据可能派上用场的地方。对于这些变量的情况，我们将使用最常见的值作为零值：
- en: '`MSZoning`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MSZoning`'
- en: '`BsmtFullBath`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BsmtFullBath`'
- en: '`BsmtHalfBath`'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BsmtHalfBath`'
- en: '`Utilities`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Utilities`'
- en: '`Functional`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Functional`'
- en: '`Electrical`'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Electrical`'
- en: '`KitchenQual`'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KitchenQual`'
- en: '`SaleType`'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SaleType`'
- en: '`Exterior1st`'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exterior1st`'
- en: '`Exterior2nd`'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Exterior2nd`'
- en: Furthermore, there are some variables that are categorical, but the data is
    numerical. An example found in the dataset is the `MSSubclass` variable. It's
    essentially a categorical variable, but its data is numerical. When encoding these
    kinds of categorical data, it makes sense to have them sorted numerically, such
    that the 0 value is indeed the lowest value.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些变量是分类的，但数据是数值的。数据集中发现的一个例子是`MSSubclass`变量。它本质上是一个分类变量，但其数据是数值的。在编码这类分类数据时，按数值排序是有意义的，这样0值确实是最低值。
- en: Final requirement
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终要求
- en: Despite the fact that we're model building right now, we want to build with
    the future in mind. The future is a production-ready machine learning system that
    performs linear regression. So whatever functions and methods we write have to
    take into account other things that may occur in a production environment that
    may not occur in the model -building phase.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们现在正在构建模型，但我们希望从未来的角度来构建。未来是一个生产就绪的机器学习系统，它执行线性回归。因此，我们编写的任何函数和方法都必须考虑到在生产环境中可能发生而在模型构建阶段可能不会发生的事情。
- en: 'The following are things to consider:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要考虑的事项：
- en: '**Unseen values**: We have to write a function that is able to encode previously
    unseen values.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未见值**：我们必须编写一个函数，能够对之前未见过的值进行编码。'
- en: '**Unseen variables**: At some point in the future we might pass a different
    version of the data in that may contain variables that are unknown at model-building
    time. We would have to handle that.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未见变量**：在未来的某个时刻，我们可能会传递一个包含在模型构建时未知变量的数据的不同版本。我们必须处理这种情况。'
- en: '**Different imputation strategies**: Different variables will require different
    strategies for guessing missing data.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同的插补策略**：不同的变量将需要不同的策略来猜测缺失数据。'
- en: Writing the code
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写代码
- en: 'Up to this point, we have only done the cleanup in our heads. I personally
    find this to be a much more rewarding exercise: to mentally clean up the data
    before actually cleaning up. This is not because I''m highly confident that I
    will have handled all the irregularities in the data. Instead, I like this process
    because it clarifies what needs to be done. And that in turn guides the data structures
    required for the job.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只是在头脑中进行了清理。我个人认为这是一种更有回报的练习：在实际清理之前，先在心理上清理数据。这并不是因为我非常自信我会处理所有数据的不规则性。相反，我喜欢这个过程，因为它阐明了需要做什么。而这反过来又指导了完成这项工作所需的数据结构。
- en: But, once the thinking is done, it's time to validate our thinking with code.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，一旦思考完成，就是时候用代码来验证我们的思考了。
- en: 'We start with the clean function:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从清洁函数开始：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`clean` takes data (in the form of `[][]string`), and with the help of the
    indices built earlier, we want to build a matrix of `Xs` (which will be `float64`)
    and `Ys`. In Go, it''s a simple loop. We''ll read over the input data and try
    to convert that. A `hints` slice is also passed in to help us figure out if a
    variable should be considered a categorical or continuous variable.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`clean`函数接收数据（以`[][]string`的形式），借助之前构建的索引，我们想要构建一个`Xs`（这将是一个`float64`矩阵）和`Ys`的矩阵。在Go语言中，这是一个简单的循环。我们将读取输入数据并尝试转换它。同时还会传递一个`hints`切片，以帮助我们确定一个变量是否应该被视为分类变量或连续变量。'
- en: In particular, the treatment of any year variables is of contention. Some statisticians
    think it's fine to treat a year variable as a discrete, non-categorical variable,
    while some statisticians think otherwise. I'm personally of the opinion that it
    doesn't really matter. If treating a year variable as a categorical variable improves
    the model score, then by all means use it. It's unlikely, though.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，对任何年份变量的处理存在争议。一些统计学家认为将年份变量视为离散的、非分类变量是可以的，而另一些统计学家则持不同意见。我个人认为这并不重要。如果将年份变量作为分类变量可以提高模型得分，那么无论如何都可以使用它。不过，这种情况不太可能发生。
- en: The meat of the preceding code is the conversion of a string into `[]float64`,
    which is what the convert function does. We will look in that function in a bit,
    but it's important to note that the data has to be imputed before conversion.
    This is because Go's slices are well-typed. A `[]float64` can only contain `float64`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的核心是将字符串转换为`[]float64`，这正是`convert`函数所做的事情。我们稍后会查看这个函数，但重要的是要注意，在转换之前必须先填充数据。这是因为Go的切片类型严格。`[]float64`只能包含`float64`。
- en: 'While it''s true that we can also replace any unknown data with NaN, that would
    not be helpful, especially in the case of categorical data, where NA might actually
    have semantic meaning. So, we impute categorical data before converting them.
    This is what `imputeCategorical` looks like:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们也可以用NaN替换任何未知数据，但这并不有帮助，尤其是在分类数据的情况下，NA可能实际上具有语义意义。因此，我们在转换之前填充分类数据。这就是`imputeCategorical`的样子：
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: What this function says is, if the value is not `NA` and the value is not an
    empty string, then it's a valid value, hence we return early. Otherwise, we will
    have to consider whether to return `NA` as a valid category.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的意思是，如果值不是`NA`且值不是空字符串，那么它是一个有效值，因此我们可以提前返回。否则，我们得考虑是否将`NA`作为有效类别返回。
- en: For some specific categories, NAs are not valid categories, and they are replaced
    by the most-commonly occurring value. This is a logical thing to do—a shed in
    the middle of nowhere with no electricity, no gas, and no bath is a very rare
    occurrence. There are techniques to deal with that (such as LASSO regression),
    but we're not going to do that right now. Instead, we'll just replace them with
    the mode.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些特定的类别，NA不是有效的类别，它们被替换为最常出现的值。这是一件合乎逻辑的事情去做——一个位于荒野中、没有电力、没有天然气和没有浴室的棚屋是非常罕见的。有一些处理这种问题的技术（例如LASSO回归），但我们现在不会这么做。相反，我们将它们替换为众数。
- en: 'The mode was calculated in the clean function. This is a very simple definition
    for finding the modes; we simply find the value that has the greatest length and
    return the value:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 模式是在`clean`函数中计算的。这是一个非常简单的定义，用于寻找众数；我们只是找到具有最大长度的值并返回该值：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After we've imputed the categorical data, we'll convert all the data to `[]float`.
    For numerical data, that will result in a slice with a single value. But for categorical
    data, it will result in a slice of 0s and 1s.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在填充了分类数据之后，我们将所有数据转换为`[]float`。对于数值数据，这将导致包含单个值的切片。但对于分类数据，它将导致包含0和1的切片。
- en: For the purposes of this chapter, any NAs found in the numerical data will be
    converted to 0.0\. There are other valid strategies that will improve the results
    of the model very slightly, but these strategies are not brief.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章的目的，任何在数值数据中发现的NA将被转换为0.0。还有其他一些有效的策略可以略微提高模型的性能，但这些策略并不简短。
- en: 'And so, the conversion code looks simple:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，转换代码看起来很简单：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: I would like to draw your attention to the `convertCategorical` function. There
    is some verbosity involved in the code, but the verbosity wills away the magic.
    Because Go randomizes access to a map, it's important to get a list of keys, and
    then sort them. This way, all subsequent access will be deterministic.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我想引起您对`convertCategorical`函数的注意。代码中有些冗余，但冗余会消除魔法。因为Go随机访问映射，所以获取键的列表并对其进行排序很重要。这样，所有后续访问都将具有确定性。
- en: The function also allows room for optimization—making this function a `stateful`
    function would optimize it further, but for this project we shan't bother.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数还留有优化的空间——将这个函数做成一个`有状态的`函数可以进一步优化它，但在这个项目中我们不会去麻烦。
- en: 'This is our main function so far:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们到目前为止的主要函数：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And the output of the code is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出如下：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that while the original data had 81 variables, by the time we are done
    with the encoding there are 615 variables. This is what we want to pass into the
    regression. At this point, the seasoned data scientist may notice a few things
    that may not sit well with her. For example, the number of variables (615) is
    too close to the number of observations (1,460) for comfort, so we might run into
    some issues. We will address those issues later.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Another point to note is that we're converting the data to `*tensor.Dense`.
    You can think of the `*tensor.Dense` data structure as a matrix. It is an efficient
    data structure with a lot of niceness that we will use later.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Further exploratory work
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, it would be very tempting to just take these matrices and run
    the regression on them. While that could work, it wouldn't necessarily produce
    the best results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The conditional expectation functions
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead, let''s do what we originally set out to do: explore the `CEF`s of
    the variables. Fortunately, we already have the necessary data structures (in
    other words, the index), so writing the function to find the `CEF` is relatively
    easy.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code block:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This function finds the conditionally expected house price when a variable is
    held fixed. We can do an exploration of all the variables, but for the purpose
    of this chapter, I shall only share the exploration of one –the yearBuilt variable—as
    an example.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, YearBuilt is an interesting variable to dive deep into. It''s a categorical
    variable (1950.5 makes no sense), but it''s totally orderable as well (1,945 is
    smaller than 1,950). And there are many values of YearBuilt. So, instead of printing
    it out, we shall plot it out with the following function:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Our ever-growing main function now has this appended to it:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Running the program yields the following chart:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/feaaffe2-2a95-4164-9e69-34870dce328f.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: conditional expectation functions for Yearbuilt
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Upon inspecting the chart, I must confess that I was a little surprised. I'm
    not particularly familiar with real estate, but my initial instincts were that
    older houses would cost more—houses, in my mind, age like fine wine; the older
    the house, the more expensive it would be. Clearly this is not the case. Oh well,
    live and learn.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: The CEF exploration should be done for as many variables as possible. I am merely
    eliding for the sake of brevity in this book.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Skews
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s look at how the data for the house prices are distributed:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This section is added to the main function:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following diagram is:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de54cca7-582e-4cd4-8363-25d323c2ce77.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: Histogram of House prices
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be noted, the histogram of the prices is a little skewed. Fortunately,
    we can fix that by applying a function that performs the logging of the value
    and then adds 1\. The standard library provides a function for this: `math.Log1p`.
    So, we add the following to our main function:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following diagram is :'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51667581-630e-48e9-b1bc-66981011c36e.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Histogram of House Prices (Processed)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Ahh! This looks better. We did this for all the `Ys`. What about any of the
    `Xs`? To do that, we will have to iterate through each column of `Xs`, find out
    if they are skewed, and if they are, we need to apply the transformation function.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what we add to the main function:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`native.MatrixF64s` takes a `*tensor.Dense` and converts it into a native Go
    iterator. The underlying backing data doesn''t change, therefore if one were to
    write `it[0][0] = 1000`, the actual matrix itself would change too. This allows
    us to perform transformations without additional allocations. For this topic,
    it may not be as important; however, for larger projects, this will come to be
    very handy.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'This also allows us to write the functions to check and mutate the matrix:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Multicollinearity
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the opening paragraphs of this section, the number of variables
    is a little high for comfort. When there is a high number of variables the chances
    of multicollinearity increases. Multicollinearity is when two or more variables
    are correlated with each other somehow.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: From a cursory glance at the data, we can tell that is in fact true. A simple
    thing to note is GarageArea is correlated with GarageCars. In real life, this
    makes sense—a garage that can take two cars would be logically larger in area
    compared to a garage that can only store one car. Likewise, zoning is highly correlated
    with the neighborhood.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: A good way to think about the variables is in terms of information included
    in the variables. Sometimes, the variables have information that overlaps. For
    example, when GarageArea is 0, that overlaps with the GarageType of NA—after all,
    if you have no garage, the area of your garage is zero.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The difficult part is going through the list of variables, and deciding which
    to keep. It's something of an art that has help from algorithms. In fact, the
    first thing we're going to do is to find out how correlated a variable is with
    another variable. We do this by calculating the correlation matrix, then plotting
    out a heatmap.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the correlation matrix, we simply use the function in Gonum with
    this snippet:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s go through this line by line:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '`m64, err := tensor.ToMat64(Xs, tensor.UseUnsafe())` performs the conversion
    from `*tensor.Dense` to `mat.Mat64`. Because we don''t want to allocate an additional
    chunk of memory, and we''ve determined that it''s safe to actually reuse the data
    in the matrix, we pass in a `tensor.UseUnsafe()` function option that tells Gorgonia
    to reuse the underlying memory in the Gonum matrix.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '`stat.CorrelationMatrix(nil, m64, nil)` calculates the correlation matrix.
    The correlation matrix is a triangular matrix—a particularly useful data structure
    that the Gonum package provides. It is a clever little data structure for this
    use case because the matrix is mirrored along the diagonal.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we plot `heatmap` using the following snippet of code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `plotter.NewHeatMap` function expects an interface, which is why I wrapped `mat.Mat`
    in the heatmap data structure, which provides the interface for the plotter to
    draw a heatmap. This pattern will become more and more common in the coming chapters—wrapping
    a data structure just to provide an additional interface to other functions. They
    are cheap and readily available and should be used to the fullest extent.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'A large portion of this code involves a hack for the labels. The way Gonum
    plots work, is that when the canvas size is calculated, the label is considered
    to be inside the plot. To be able to draw the labels outside the plot, a lot of
    extra code would have to be written. So, instead, I shrunk the labels to fit into
    the gutter between the axis and the plot itself as to not overlay into important
    areas of the plot:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d89cadf-dfb0-44b2-afd3-3521ff14abb2.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Heatmap
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Of particular note in this heatmap are the white streaks. We expect a variable
    to correlate with itself completely. But if you notice, there are areas of white
    lines that are somewhat parallel to the diagonal white line. These are total correlations.
    We will need to remove them.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Heatmaps are nice to look at but are quite silly. The human eye isn't great
    at telling hues apart. So what we're going to do is also report back the numbers.
    The correlation between variables is between -1 and 1\. We're particularly interested
    in correlations that are close to either end.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'This snippet prints the results:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here I use an anonymous struct, instead of a named struct, because we're not
    going to reuse the data—it's solely for printing. An anonymous tuple would suffice.
    This is not the best practice in most cases.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: This correlation plot shows only the correlation of the independent variables.
    To truly understand multicollinearity, we would have to find the correlation of
    each variable to each other, and to the dependent variable. This will be left
    as an exercise for the reader.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: If you were to plot the correlation matrix, it'd look the same as the one we
    have right here, but with an additional row and column for the dependent variable.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, multicollinearity can only be detected after running a regression.
    The correlation plot is simply a shorthand way of guiding the inclusion and exclusion
    of variables. The actual process of removing multicollinearity is an iterative
    one, often with other statistics such as the variance inflation factor to lend
    a hand in deciding what to include and what not to include.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of this chapter, I've identified multiple variables to be included—and
    the majority of variables are excluded. This can be found in the `const.go` file.
    The commented out lines in the ignored list are what was included in the final
    model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the opening paragraph of this section, it's really a bit of
    an art, aided by algorithms.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Standardization
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a last bit of transformation, we would need to standardize our input data.
    This allow us to compare models to see if one model is better than another. To
    do so, I wrote two different scaling algorithms:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If you come from the Python world of data science, the first scale function
    is essentially what scikits-learn's `RobustScaler` does. The second function is
    essentially `StdScaler`, but with the variance adapted to work for sample data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: This function takes the values in a given column (`j`) and scales them in such
    a way that all the values are constrained to within a certain value. Also, note
    that the input to both scaling functions is `[][]float64`. This is where the benefits of
    the `tensor` package comes in handy. A `*tensor.Dense` can be converted to `[][]float64`
    without any extra allocations. An additional beneficial side effect is that you
    can mutate `a` and the tensor values will change as well. Essentially, `[][]float64`
    will act as an iterator to the underlying tensor data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Our transform function now looks like this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that we only want to scale the numerical variables. The categorical variables
    can be scaled, but there isn't really much difference.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that that's all done, let's do some linear regression! But first, let's
    clean up our code. We'll move our exploratory work so far into a function called
    `exploration()`. Then we will reread the file, split the dataset into training
    and testing dataset, and perform all the transformations before finally running
    the regression. For that, we will use `github.com/sajari/regression` and apply
    the regression.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part looks like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We first ingest and clean the data, then we create an iterator for the matrix
    of `Xs` for easier access. We then transform both the `Xs` and the `Ys`. Finally,
    we shuffle the `Xs`, and partition them into a training dataset and a testing
    dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Recall from the first chapter on knowing whether a model is good. A good model
    must be able to generalize to previously unseen combinations of values. To prevent
    overfitting, we must cross-validate our model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve that, we must only train on a limited subset of data, then
    use the model to predict on the test set of data. We can then get a score of how
    well it did when being run on the testing set.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, this should be done before the parsing of the data into the `Xs` and
    `Ys`. But we'd like to reuse the functions we wrote earlier, so we shan't do that.
    The separate functions of ingest and clean, however, allows you to do that. And
    if you visit the repository on GitHub, you will find that all the functions for
    such an act can easily be done.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: For now, we simply take out 20% of the dataset, and set it aside. A shuffle
    is used to resample the rows so that we don't train on the same 80% every time.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that now the `clean` function takes `ignored`, while in the exploratory
    mode, it took `nil`. This, along with the `shuffle`, are important for cross-validation
    later on.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: The regression
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And so, now we're ready to build the regression model. Bear in mind that this
    section is highly iterative in real life. I will describe the iterations, but
    will only share the model that I chose to settle on.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The `github.com/sajari/regression` package does an admirable job. But we want
    to extend the package a little to be able to compare models and the coefficients
    of the parameters. So I wrote this function:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`runRegression` will perform the regression analysis, and print the outputs
    of the standard errors of the coefficients. It is an estimate of the standard
    deviation of the coefficients—imagine this model being run many many times: each
    time the coefficients might be slightly different. The standard error simply reports
    amount of variation in the coefficients.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: The standard errors are calculated with the help of the `gorgonia.org/vecf64`
    package, which performs in-place operations for vectors. Optionally, you may choose
    to write them as loops.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: This function also introduces us to the API for the `github.com/sajari/regression`
    package—to predict, simply use `r.Predict(vars)`. This will be useful in cases
    where one would like to use this model for production.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let us focus on the other half of the main function:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, we run the regression, and then we print the results. We don't just want
    to output the regression coefficients. We also want to output the standard errors,
    the t-statistic, and the P-value. This would give us some confidence over the
    estimated coefficients.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`tdist := distuv.StudentsT{Mu: 0, Sigma: 1, Nu: float64(len(it) - len(newHdr)
    - 1), Src: rand.New(rand.NewSource(uint64(time.Now().UnixNano())))}` creates a
    Student''s t-distribution, which we will compare against our data. The t-statistic
    is very simply calculated by dividing the coefficient by the standard error.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And now we come to the final part—in order to compare models, we would like
    to cross-validate the model. We've already set aside a portion of the data. Now,
    we will have to test the model on the data that was set aside, and compute a score.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'The score we''ll be using is a Root Mean Square Error. It''s used because it''s
    simple and straightforward to understand:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: With this, now we're really ready to run the regression analysis.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Running the regression
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simply run the program. If the program is run with an empty ignored list, the
    result will show up as a bunch of NaNs. Do you recall that earlier we have done
    some correlation analysis on how some variables are correlated with one another?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by adding those into our ignored list, and then run the regression.
    Once we have a score that is no longer NaN, we can start comparing models.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'The final model I have prints the following output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The cross-validation results (a RMSE of 0.143) are decent—not the best, but
    not the worst either. This was done through careful elimination of variables.
    A seasoned econometrician may come into this, read the results, and decide that
    further feature engineering may be done.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, looking at these results, off the top of my head I could think of several
    other feature engineering that could be done—subtracting the year remodeled from
    the year sold (recency of remodeling/renovations). Another form of feature engineering
    is to run a PCA-whitening process on the dataset.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: For linear regression models, I tend to stay away from complicated feature engineering.
    This is because the key benefit of a linear regression is that it's explainable
    in natural language.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can say this: for every unit increase in lot area size, if
    everything else is held constant, we can expect a 0.07103 times increment in house
    price.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'A particularly counter intuitive result from this regression is the `PoolArea`
    variable. Interpreting the results, we would say: for every unit increase in pool
    area, we can expect a -0.00075 times increment in price, *ceteris paribus*. Granted,
    the p-value of the coefficient is 0.397, meaning that this coefficient could have
    been gotten by sheer random chance. Hence, we must be quite careful in saying
    this—having a pool decreases the value of your property in Ames, Massachusetts.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Discussion and further work
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This model is now ready to be used to predict things. Is this the best model?
    No, it's not. Finding the best model is a never ending quest. To be sure, there
    are indefinite ways of improving this model. One can use LASSO methods to determine
    the importance of variables before using them.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The model is not only the linear regression, but also the data cleaning functions
    and ingestion functions that come with it. This leads to a very high number of
    tweakable parameters. Maybe if you didn't like the way I imputed data, you can
    always write your own method!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore the code in this chapter can be cleaned up further. Instead of returning
    so many values in the clean function, a new tuple type can be created to hold
    the Xs and Ys—a data frame of sorts. In fact, that's what we're going to build
    in the upcoming chapters. Several functions can be made more efficient using a
    state-holder struct.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: If you will note, there are not very many statistical packages like Pandas for
    Go. This is not for the lack of trying. Go as a language is all about solving
    problems, not about building generic packages. There are definitely dataframe-like
    packages in Go, but in my experience, using them tends to blind one to the most
    obvious and efficient solutions. Often, it's better to build your own data structures
    that are specific to the problem at hand.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: For the most part in Go, the model building is an iterative process, while productionizing
    the model is a process that happens after the model has been built. This chapter
    shows that with a little awkwardness, it is possible to build a model using an
    iterative process that immediately translates to a production-ready system.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在Go语言中，模型构建通常是一个迭代过程，而将模型投入生产则是在模型构建之后的过程。本章表明，通过一点笨拙，我们可以使用迭代过程构建一个模型，该模型可以直接转化为一个生产就绪的系统。
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned how to explore data (with some awkwardness)
    using Go. We plotted some charts and used them as a guiding rod to select variables
    for the regression. Following that, we implemented a regression model that came
    with reporting of errors which enabled us to compare models. Lastly, to ensure
    we were not over fitting, we used a RMSE score to cross-validate our model and
    came out with a fairly decent score.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用Go语言（虽然有些笨拙）来探索数据。我们绘制了一些图表，并将它们用作选择回归变量的指南。随后，我们实现了一个回归模型，该模型包含错误报告，使我们能够比较模型。最后，为了确保我们没有过度拟合，我们使用RMSE分数来交叉验证我们的模型，并得到了一个相当不错的分数。
- en: This is just a taste of what is to come. The ideas in abstract are repeated
    over the next chapters—we will be cleaning data, then writing the machine learning
    model, which will be cross-validated. The only difference will generally be the
    data, and the models.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是未来内容的预览。在接下来的章节中，抽象中的观点将被重复——我们将清理数据，然后编写机器学习模型，该模型将进行交叉验证。唯一的一般性区别将是数据，以及模型。
- en: In the next chapter, we'll learn a simple way to determine if an email is spam
    or not.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习一种简单的方法来判断一封电子邮件是否为垃圾邮件。
