- en: Linear Regression - House Price Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression is one of the world's oldest machine learning concepts. Invented
    in the early nineteenth century, it is still one of the more vulnerable methods
    of understanding the relationship between input and output.
  prefs: []
  type: TYPE_NORMAL
- en: The ideas behind linear regression is familiar to us all. We feel that some
    things are correlated with one another. Sometimes they are causal in nature. There
    exists a very fine line between correlation and causation. For example, summer
    sees more sales in ice creams and cold beverages, while winter sees more sales
    in hot cocoa and coffee. We could say that the seasons themselves cause the amount
    of sales—they're causal in nature. But are they really?
  prefs: []
  type: TYPE_NORMAL
- en: Without further analysis, the best thing we can say is that they are correlated
    with one another. The phenomenon of summer is connected to the phenomenon of greater-than
    the-rest-of-the-year sales of cold drinks and ice cream. The phenomenon of winter
    is connected, somehow, to the phenomenon of greater-than-the-rest-of-the-year
    sales of hot beverages.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the relationship between things is what linear regression, at
    its core, is all about. There can be many lenses through which linear regression
    may be viewed, but we will be viewing it through a machine learning lens. That
    is to say, we wish to build a machine learning model that will accurately predict
    the results, given some input.
  prefs: []
  type: TYPE_NORMAL
- en: The desire to use correlation for predictive purposes was indeed the very reason
    why linear regression was invented in the first place. Francis Galton, who was
    coincidentally Charles Darwin's cousin, hailed from an upper-class family whose
    lineage included doctors. He had given up his medical studies after a nervous
    breakdown and began travelling the world as a geologist—this was back when being
    a geologist was the coolest job (much like being a data scientist today)—however,
    it was said that Galton hadn't the mettle of Darwin, and soon he gave up the idea
    of travelling around the world, soured by experiences in Africa. Having inherited
    his wealth after his father died, Galton dabbled in all things that tickled his
    fancy, including biology.
  prefs: []
  type: TYPE_NORMAL
- en: The publication of his cousin's magnum opus, *On the Origin of Species*, made
    Galton double down on his pursuits in biology and ultimately, eugenics. Galton
    experimented, rather coincidentally in the same manner as Mendel, on peas. He
    had wanted to predict the characteristics of the offspring plants, when only information
    about the parent plants' characteristics were available. He realized that the
    offspring was often somewhere in between the characteristics of the parent plants.
    When Galton realized that he could derive a mathematical equation that represented
    inheritance using elliptical curve fitting, he invented regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reasoning behind regression was simple: there was a driving force—a signal
    of sorts—that led the characteristics of the offspring plants to go towards the
    curve he had fitted. If that was the case, it meant that the driving force obeyed
    some mathematical law. And if it did obey the mathematical laws, then it could
    be used for prediction, Galton reasoned. To further refine his ideas, he sought
    the help of the mathematician Karl Pearson.'
  prefs: []
  type: TYPE_NORMAL
- en: It took Galton and Pearson a few more attempts to refine the concept and quantify
    the trends. But ultimately they adopted a least-squares methodology for fitting
    the curves.
  prefs: []
  type: TYPE_NORMAL
- en: Even to this day, when linear regression is mentioned, it can be safely assumed
    that a least- squares model will be used, which is precisely what we will be doing.
  prefs: []
  type: TYPE_NORMAL
- en: We will be performing exploratory data analysis—this will allow us to understand
    the data better. Along the way, we will build and use the data structures necessary
    for a machine learning project. We will rely heavily on Gonum's plotting libraries
    for that. After that, we will run a linear regression, interpret the results,
    and identify the strengths and weaknesses of this technique of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: The project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we want to do is to create a model of house prices. We will be using this
    open source dataset of house prices ([https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data))
    for our linear regression model. Specifically, the dataset is the data of price
    of houses that have been sold in the Ames area in Massachusetts, and their associated
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with any machine learning project, we start by asking the most basic of
    questions: what do we want to predict? In this case, I''ve already indicated that
    we''re going to be predicting house prices, therefore all the other data will
    be used as signals to predict house prices. In statistical parlance, we call house
    prices the dependent variable and the other fields the independent variables.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will build a graph of dependent logical conditions,
    then with that as a plan, write a program that finds a linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploratory data analysis is part and parcel of any model-building process.
    Understanding the algorithm at play, too, is important. Given that this chapter
    revolves around linear regression, it might be worth it to explore the data through
    the lens of understanding linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let's look at the data. One of the first things I recommend any budding
    data scientist keen on machine learning to do is to explore the data, or a subset
    of it, to get a feel for it. I usually do it in a spreadsheet application such
    as Excel or Google Sheets. I then try to understand, in human ways, the meaning
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset comes with a description of fields, which I can''t enumerate in
    full here. A snapshot, however, would be illuminating for the rest of the discussion
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SalePrice`: The property''s sale price in dollars. This is the dependent variable
    that we''re trying to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MSSubClass`: The building class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MSZoning`: The general zoning classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LotFrontage`: The linear feet of the street connected to the property.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LotArea`: The lot size in square feet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be multiple ways of understanding linear regression. However, one
    of my favorite ways of understanding linear regression directly ties into exploratory
    data analysis. Specifically, we're interested in looking at linear regression
    through the lens of the **conditional** **expectation** **functions** (**CEFs**)
    of the independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The conditional expectation function of a variable is simply the expected value
    of the variable, dependent upon the value of another variable. This seems like
    a rather dense subject to get through, so I shall offer three different views
    of the same topic in an attempt to clarify:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical point of view**: The conditional expectation function of a dependent
    variable ![](img/2cf1f34c-e741-4f73-8546-3259edb2f161.png)given a vector of covariates
    ![](img/6fae8631-f622-418d-afbd-1e84858f95dd.png)is simply the expected value
    of ![](img/3e4b7cfd-0811-4773-80a8-2a44003be7cd.png)(the average) when ![](img/f10daf27-0d56-46bf-89d0-354502d9dc2f.png)is
    fixed to ![](img/e26b71db-9cd5-4c99-b437-528a9b9ef5e6.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Programming point of view in pseudo-SQL**: `select avg(Y) from dataset where
    X = ''Xi''`. When conditioning upon multiple conditions, it''s simply this: `select
    avg(Y) from dataset where X1 = ''Xik'' and X2 = ''Xjl''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concrete example**: What are the expected house prices if one of the independent
    variables—say, MSZoning—is RL? The expected house price is the population average,
    which translates to: of all the houses in Boston, what is the average price of
    house sold whose zoning type is RL?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As it stands, this is a pretty bastardized version of what the CEF is—there
    are some subtleties involved in the definition of the CEF, but that is not within
    the scope of this book, so we shall leave that for later. For now, this rough
    understanding of CEF is enough to get us started with our exploratory data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The programming point of view in pseudo-SQL is useful because it informs us
    about what we would need so that we can quickly calculate the aggregate of data.
    We would need to create indices. Because our dataset is small, we can be relatively
    blasé about the data structures used to index the data.
  prefs: []
  type: TYPE_NORMAL
- en: Ingestion and indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the best way to index the data is to do it at the time of ingestion.
    We will use the `encoding/csv` package found in the `Go standard` library to ingest
    the data and build the index.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the code, let's look at the notion of an index, and how
    one might be built. While indexes are extremely commonly used in databases, they
    are applicable in any production system as well. The purpose of the index is to
    allow us to access data quickly.
  prefs: []
  type: TYPE_NORMAL
- en: We want to build an index that will allow us to know at any time which row(s)
    has the value. In systems with much larger datasets, a more complicated index
    structure (such as a B-Tree) might be used. In the case of this dataset, however,
    a map-based index would be more than sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what our index looks like: `[]map[string][]int`—it''s a slice of maps.
    The first slice is indexed by the columns—meaning if we want column `0`, we simply
    get `index[0]`, and get `map[string][]int` in return. The map tells us what values
    are in the columns (the key of the map), and what rows contain those values (the
    value of the map).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the question turns to: how do you know which variables associate with
    which column? A more traditional answer would be to have something like `map[string]int`,
    where the key represents the variable name and the value represents the column
    number. While that is a valid strategy, I prefer to have `[]string` as the associative
    map between the index and column name. Searching is O(N), but for the most part,
    if you have named variables, N is small. In future chapters, we shall see much
    much larger Ns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we return the index of column names as `[]string` or, in the case of reading `CSVs`,
    it''s simply the first row, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Reading this code snippet, a good programmer would have alarm bells going off
    in their head. Why is everything a string? The answer to that is quite simple:
    we''ll convert the types later. All we need right now is some basic count-based
    statistics for exploratory data analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key is in the indexes that are returned by the function. What we have is
    a column count of unique values. This is how to count them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With this, we can then analyze the cardinality of each individual column—that
    is how many distinct values there are. If there are as many distinct values as
    there are rows in each column, then we can be quite sure that the column is not
    categorical. Or, if we know that the column is categorical, and there are as many
    distinct values as there are rows, then we know for sure that the column cannot
    be used in a linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main function now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For completeness, this is the definition of `mHandleError`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick `go run *.go` indicates this result (which has been truncated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Alone, this tells us a lot of interesting facts, chief amongst which is that
    there is a lot more categorical data than there is continuous data. Additionally,
    for some columns that are indeed continuous in nature, there are only a few discrete
    values available. One particular example is the `LowQualSF` column—it's a continuous
    variable, but there are only 24 unique values.
  prefs: []
  type: TYPE_NORMAL
- en: We'd like to calculate the CEF of the discrete covariates for further analysis.
    But before that can happen, we would need to clean up the data. While we're at
    it, we might also want to create a logical grouping of data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Janitorial work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A large part of doing data science work is focused on cleanup. In productionized
    systems, this data would typically be fetched directly from the database, already
    relatively clean (high -quality production data science work requires a database
    of clean data). However, we're not in production mode yet. We're still in the
    model-building phase. It would be helpful to imagine writing a program solely
    for cleaning data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at our requirements: starting with our data, each column is a variable—most
    of them are independent variables, except for the last column, which is the dependent
    variable. Some variables are categorical, and some are continuous. Our task is
    to write a function that will convert the data, currently `[][]string` to `[][]float64`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we would require all the data to be converted into `float64`. For
    the continuous variables, it''s an easy task: simply parse the string into a float.
    There are oddities that need to be handled, which I hope you had spotted by the
    time you opened the file in a spreadsheet. But the main pain is in converting
    categorical data to `float64`.'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for us, people much smarter than have figured this out decades ago.
    There exists an encoding scheme that allows categorical data to play nicely with
    linear regression algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding categorical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The trick to encode categorical data is to expand categorical data into multiple
    columns, each having a 1 or 0 representing whether it's true or false. This of
    course comes with some caveats and subtle issues that must be navigated with care.
    For the rest of this subsection, I shall use a real categorical variable to explain
    further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the `LandSlope` variable. There are three possible values for `LandSlope`:'
  prefs: []
  type: TYPE_NORMAL
- en: Gtl
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sev
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is one possible encoding scheme (this is commonly known as one-hot encoding):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Slope** | **Slope_Gtl** | **Slope_Mod** | **Slope_Sev** |'
  prefs: []
  type: TYPE_TB
- en: '| Gtl | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mod | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Sev | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'This would be a terrible encoding scheme. To understand why, we must first
    understand linear regression by means of ordinary least squares. Without going
    into too much detail, the meat of OLS-based linear regression is the following
    formula (which I am so in love with that I have had multiple T-shirts with the
    formula printed on):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14494194-447a-4049-8a0d-ac848552b99e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here,![](img/2c92b8b8-8893-4bf6-b17e-7e386e28d60e.png)is an(m x n) matrix and![](img/385ff6f9-3b14-4131-bd02-26b875c70791.png)is
    an (m x 1) vector. The multiplications, therefore, are not straightforward multiplications—they
    are matrix multiplications. When one-hot encoding is used for linear regression,
    the resulting input matrix![](img/e1cffed5-f7e4-41fe-be0e-53e4b12f04d4.png)will
    typically be singular—in other words, the determinant of the matrix is 0\. The
    problem with singular matrices is that they cannot be inverted.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, instead, we have this encoding scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Slope** | **Slope_Mod** | **Slope_Sev** |'
  prefs: []
  type: TYPE_TB
- en: '| Gtl | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mod | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Sev | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Here, we see an application of the Go proverb make the zero value useful for
    being applied in a data science context. Indeed, clever encoding of categorical
    variables will yield slightly better results when dealing with previously unseen
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The topic is far too wide to broach here, but if you have categorical data that
    can be partially ordered, then when exposed to unseen data, simply encode the
    unseen data to the closest ordered variable value, and the results will be slightly
    better than encoding to the zero value or using random encoding. We will cover
    more of this in the later parts of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Handling bad numbers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another part of the janitorial work is handling bad numbers. A good example
    is in the `LotFrontage` variable. From the data description, we know that this
    is supposed to be a continuous variable. Therefore, all the numbers should be
    directly convertible to `float64`. Looking at the data, however, we see that it's
    not true—there is data that is NA.
  prefs: []
  type: TYPE_NORMAL
- en: '`LotFrontage`, according to the description, is the linear feet of the street
    connected to property. NA could mean one of two things:'
  prefs: []
  type: TYPE_NORMAL
- en: We have no information on whether there is a street connected to the property
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no street connected to the property
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In either case, it would be reasonable to replace NA with 0\. This is reasonable,
    because the second lowest value in `LotFrontage` is 21\. There are other ways
    of imputing the data, of course, and often the imputations will lead to better
    models. But for now, we'll impute it with 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also do the same with any other continuous variables in this dataset
    simply because they make sense when you replace the NA with 0\. One tip is to
    use it in a sentence: this house has an Unknown `GarageArea`. If that is the case,
    then what should be the best guess? Well, it''d be helpful to assume that the
    house has no garage, so it''s OK to replace NA with 0.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this may not be the case in other machine learning projects. Remember—human
    insight may be fallible, but its often the best solution for a lot of irregularities
    in the data. If you happen to be a realtor, and you have a lot more domain knowledge,
    you can infuse said domain knowledge into the imputation phase—you can use variables
    to calculate and estimate other variables for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the categorical variables, we can for the most part treat NA as the
    zero value of the variable, so no change there if there is an NA. There is some
    categorical data for which NA or None wouldn''t make sense. This is where the
    aforementioned clever encoding of category could come in handy. In the cases of
    these variables, we''ll use the most commonly found value as the zero value:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MSZoning`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtFullBath`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BsmtHalfBath`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Utilities`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Functional`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Electrical`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KitchenQual`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SaleType`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Exterior1st`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Exterior2nd`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, there are some variables that are categorical, but the data is
    numerical. An example found in the dataset is the `MSSubclass` variable. It's
    essentially a categorical variable, but its data is numerical. When encoding these
    kinds of categorical data, it makes sense to have them sorted numerically, such
    that the 0 value is indeed the lowest value.
  prefs: []
  type: TYPE_NORMAL
- en: Final requirement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite the fact that we're model building right now, we want to build with
    the future in mind. The future is a production-ready machine learning system that
    performs linear regression. So whatever functions and methods we write have to
    take into account other things that may occur in a production environment that
    may not occur in the model -building phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are things to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unseen values**: We have to write a function that is able to encode previously
    unseen values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unseen variables**: At some point in the future we might pass a different
    version of the data in that may contain variables that are unknown at model-building
    time. We would have to handle that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Different imputation strategies**: Different variables will require different
    strategies for guessing missing data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up to this point, we have only done the cleanup in our heads. I personally
    find this to be a much more rewarding exercise: to mentally clean up the data
    before actually cleaning up. This is not because I''m highly confident that I
    will have handled all the irregularities in the data. Instead, I like this process
    because it clarifies what needs to be done. And that in turn guides the data structures
    required for the job.'
  prefs: []
  type: TYPE_NORMAL
- en: But, once the thinking is done, it's time to validate our thinking with code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the clean function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`clean` takes data (in the form of `[][]string`), and with the help of the
    indices built earlier, we want to build a matrix of `Xs` (which will be `float64`)
    and `Ys`. In Go, it''s a simple loop. We''ll read over the input data and try
    to convert that. A `hints` slice is also passed in to help us figure out if a
    variable should be considered a categorical or continuous variable.'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the treatment of any year variables is of contention. Some statisticians
    think it's fine to treat a year variable as a discrete, non-categorical variable,
    while some statisticians think otherwise. I'm personally of the opinion that it
    doesn't really matter. If treating a year variable as a categorical variable improves
    the model score, then by all means use it. It's unlikely, though.
  prefs: []
  type: TYPE_NORMAL
- en: The meat of the preceding code is the conversion of a string into `[]float64`,
    which is what the convert function does. We will look in that function in a bit,
    but it's important to note that the data has to be imputed before conversion.
    This is because Go's slices are well-typed. A `[]float64` can only contain `float64`.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it''s true that we can also replace any unknown data with NaN, that would
    not be helpful, especially in the case of categorical data, where NA might actually
    have semantic meaning. So, we impute categorical data before converting them.
    This is what `imputeCategorical` looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: What this function says is, if the value is not `NA` and the value is not an
    empty string, then it's a valid value, hence we return early. Otherwise, we will
    have to consider whether to return `NA` as a valid category.
  prefs: []
  type: TYPE_NORMAL
- en: For some specific categories, NAs are not valid categories, and they are replaced
    by the most-commonly occurring value. This is a logical thing to do—a shed in
    the middle of nowhere with no electricity, no gas, and no bath is a very rare
    occurrence. There are techniques to deal with that (such as LASSO regression),
    but we're not going to do that right now. Instead, we'll just replace them with
    the mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mode was calculated in the clean function. This is a very simple definition
    for finding the modes; we simply find the value that has the greatest length and
    return the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: After we've imputed the categorical data, we'll convert all the data to `[]float`.
    For numerical data, that will result in a slice with a single value. But for categorical
    data, it will result in a slice of 0s and 1s.
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this chapter, any NAs found in the numerical data will be
    converted to 0.0\. There are other valid strategies that will improve the results
    of the model very slightly, but these strategies are not brief.
  prefs: []
  type: TYPE_NORMAL
- en: 'And so, the conversion code looks simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: I would like to draw your attention to the `convertCategorical` function. There
    is some verbosity involved in the code, but the verbosity wills away the magic.
    Because Go randomizes access to a map, it's important to get a list of keys, and
    then sort them. This way, all subsequent access will be deterministic.
  prefs: []
  type: TYPE_NORMAL
- en: The function also allows room for optimization—making this function a `stateful`
    function would optimize it further, but for this project we shan't bother.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is our main function so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And the output of the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note that while the original data had 81 variables, by the time we are done
    with the encoding there are 615 variables. This is what we want to pass into the
    regression. At this point, the seasoned data scientist may notice a few things
    that may not sit well with her. For example, the number of variables (615) is
    too close to the number of observations (1,460) for comfort, so we might run into
    some issues. We will address those issues later.
  prefs: []
  type: TYPE_NORMAL
- en: Another point to note is that we're converting the data to `*tensor.Dense`.
    You can think of the `*tensor.Dense` data structure as a matrix. It is an efficient
    data structure with a lot of niceness that we will use later.
  prefs: []
  type: TYPE_NORMAL
- en: Further exploratory work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, it would be very tempting to just take these matrices and run
    the regression on them. While that could work, it wouldn't necessarily produce
    the best results.
  prefs: []
  type: TYPE_NORMAL
- en: The conditional expectation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Instead, let''s do what we originally set out to do: explore the `CEF`s of
    the variables. Fortunately, we already have the necessary data structures (in
    other words, the index), so writing the function to find the `CEF` is relatively
    easy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This function finds the conditionally expected house price when a variable is
    held fixed. We can do an exploration of all the variables, but for the purpose
    of this chapter, I shall only share the exploration of one –the yearBuilt variable—as
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, YearBuilt is an interesting variable to dive deep into. It''s a categorical
    variable (1950.5 makes no sense), but it''s totally orderable as well (1,945 is
    smaller than 1,950). And there are many values of YearBuilt. So, instead of printing
    it out, we shall plot it out with the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Our ever-growing main function now has this appended to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the program yields the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/feaaffe2-2a95-4164-9e69-34870dce328f.png)'
  prefs: []
  type: TYPE_IMG
- en: conditional expectation functions for Yearbuilt
  prefs: []
  type: TYPE_NORMAL
- en: Upon inspecting the chart, I must confess that I was a little surprised. I'm
    not particularly familiar with real estate, but my initial instincts were that
    older houses would cost more—houses, in my mind, age like fine wine; the older
    the house, the more expensive it would be. Clearly this is not the case. Oh well,
    live and learn.
  prefs: []
  type: TYPE_NORMAL
- en: The CEF exploration should be done for as many variables as possible. I am merely
    eliding for the sake of brevity in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Skews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s look at how the data for the house prices are distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This section is added to the main function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de54cca7-582e-4cd4-8363-25d323c2ce77.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram of House prices
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be noted, the histogram of the prices is a little skewed. Fortunately,
    we can fix that by applying a function that performs the logging of the value
    and then adds 1\. The standard library provides a function for this: `math.Log1p`.
    So, we add the following to our main function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram is :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51667581-630e-48e9-b1bc-66981011c36e.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram of House Prices (Processed)
  prefs: []
  type: TYPE_NORMAL
- en: Ahh! This looks better. We did this for all the `Ys`. What about any of the
    `Xs`? To do that, we will have to iterate through each column of `Xs`, find out
    if they are skewed, and if they are, we need to apply the transformation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what we add to the main function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`native.MatrixF64s` takes a `*tensor.Dense` and converts it into a native Go
    iterator. The underlying backing data doesn''t change, therefore if one were to
    write `it[0][0] = 1000`, the actual matrix itself would change too. This allows
    us to perform transformations without additional allocations. For this topic,
    it may not be as important; however, for larger projects, this will come to be
    very handy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This also allows us to write the functions to check and mutate the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Multicollinearity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the opening paragraphs of this section, the number of variables
    is a little high for comfort. When there is a high number of variables the chances
    of multicollinearity increases. Multicollinearity is when two or more variables
    are correlated with each other somehow.
  prefs: []
  type: TYPE_NORMAL
- en: From a cursory glance at the data, we can tell that is in fact true. A simple
    thing to note is GarageArea is correlated with GarageCars. In real life, this
    makes sense—a garage that can take two cars would be logically larger in area
    compared to a garage that can only store one car. Likewise, zoning is highly correlated
    with the neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: A good way to think about the variables is in terms of information included
    in the variables. Sometimes, the variables have information that overlaps. For
    example, when GarageArea is 0, that overlaps with the GarageType of NA—after all,
    if you have no garage, the area of your garage is zero.
  prefs: []
  type: TYPE_NORMAL
- en: The difficult part is going through the list of variables, and deciding which
    to keep. It's something of an art that has help from algorithms. In fact, the
    first thing we're going to do is to find out how correlated a variable is with
    another variable. We do this by calculating the correlation matrix, then plotting
    out a heatmap.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the correlation matrix, we simply use the function in Gonum with
    this snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go through this line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: '`m64, err := tensor.ToMat64(Xs, tensor.UseUnsafe())` performs the conversion
    from `*tensor.Dense` to `mat.Mat64`. Because we don''t want to allocate an additional
    chunk of memory, and we''ve determined that it''s safe to actually reuse the data
    in the matrix, we pass in a `tensor.UseUnsafe()` function option that tells Gorgonia
    to reuse the underlying memory in the Gonum matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: '`stat.CorrelationMatrix(nil, m64, nil)` calculates the correlation matrix.
    The correlation matrix is a triangular matrix—a particularly useful data structure
    that the Gonum package provides. It is a clever little data structure for this
    use case because the matrix is mirrored along the diagonal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we plot `heatmap` using the following snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `plotter.NewHeatMap` function expects an interface, which is why I wrapped `mat.Mat`
    in the heatmap data structure, which provides the interface for the plotter to
    draw a heatmap. This pattern will become more and more common in the coming chapters—wrapping
    a data structure just to provide an additional interface to other functions. They
    are cheap and readily available and should be used to the fullest extent.
  prefs: []
  type: TYPE_NORMAL
- en: 'A large portion of this code involves a hack for the labels. The way Gonum
    plots work, is that when the canvas size is calculated, the label is considered
    to be inside the plot. To be able to draw the labels outside the plot, a lot of
    extra code would have to be written. So, instead, I shrunk the labels to fit into
    the gutter between the axis and the plot itself as to not overlay into important
    areas of the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d89cadf-dfb0-44b2-afd3-3521ff14abb2.png)'
  prefs: []
  type: TYPE_IMG
- en: Heatmap
  prefs: []
  type: TYPE_NORMAL
- en: Of particular note in this heatmap are the white streaks. We expect a variable
    to correlate with itself completely. But if you notice, there are areas of white
    lines that are somewhat parallel to the diagonal white line. These are total correlations.
    We will need to remove them.
  prefs: []
  type: TYPE_NORMAL
- en: Heatmaps are nice to look at but are quite silly. The human eye isn't great
    at telling hues apart. So what we're going to do is also report back the numbers.
    The correlation between variables is between -1 and 1\. We're particularly interested
    in correlations that are close to either end.
  prefs: []
  type: TYPE_NORMAL
- en: 'This snippet prints the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Here I use an anonymous struct, instead of a named struct, because we're not
    going to reuse the data—it's solely for printing. An anonymous tuple would suffice.
    This is not the best practice in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: This correlation plot shows only the correlation of the independent variables.
    To truly understand multicollinearity, we would have to find the correlation of
    each variable to each other, and to the dependent variable. This will be left
    as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: If you were to plot the correlation matrix, it'd look the same as the one we
    have right here, but with an additional row and column for the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, multicollinearity can only be detected after running a regression.
    The correlation plot is simply a shorthand way of guiding the inclusion and exclusion
    of variables. The actual process of removing multicollinearity is an iterative
    one, often with other statistics such as the variance inflation factor to lend
    a hand in deciding what to include and what not to include.
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of this chapter, I've identified multiple variables to be included—and
    the majority of variables are excluded. This can be found in the `const.go` file.
    The commented out lines in the ignored list are what was included in the final
    model.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the opening paragraph of this section, it's really a bit of
    an art, aided by algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a last bit of transformation, we would need to standardize our input data.
    This allow us to compare models to see if one model is better than another. To
    do so, I wrote two different scaling algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If you come from the Python world of data science, the first scale function
    is essentially what scikits-learn's `RobustScaler` does. The second function is
    essentially `StdScaler`, but with the variance adapted to work for sample data.
  prefs: []
  type: TYPE_NORMAL
- en: This function takes the values in a given column (`j`) and scales them in such
    a way that all the values are constrained to within a certain value. Also, note
    that the input to both scaling functions is `[][]float64`. This is where the benefits of
    the `tensor` package comes in handy. A `*tensor.Dense` can be converted to `[][]float64`
    without any extra allocations. An additional beneficial side effect is that you
    can mutate `a` and the tensor values will change as well. Essentially, `[][]float64`
    will act as an iterator to the underlying tensor data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our transform function now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that we only want to scale the numerical variables. The categorical variables
    can be scaled, but there isn't really much difference.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that that's all done, let's do some linear regression! But first, let's
    clean up our code. We'll move our exploratory work so far into a function called
    `exploration()`. Then we will reread the file, split the dataset into training
    and testing dataset, and perform all the transformations before finally running
    the regression. For that, we will use `github.com/sajari/regression` and apply
    the regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We first ingest and clean the data, then we create an iterator for the matrix
    of `Xs` for easier access. We then transform both the `Xs` and the `Ys`. Finally,
    we shuffle the `Xs`, and partition them into a training dataset and a testing
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Recall from the first chapter on knowing whether a model is good. A good model
    must be able to generalize to previously unseen combinations of values. To prevent
    overfitting, we must cross-validate our model.
  prefs: []
  type: TYPE_NORMAL
- en: In order to achieve that, we must only train on a limited subset of data, then
    use the model to predict on the test set of data. We can then get a score of how
    well it did when being run on the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, this should be done before the parsing of the data into the `Xs` and
    `Ys`. But we'd like to reuse the functions we wrote earlier, so we shan't do that.
    The separate functions of ingest and clean, however, allows you to do that. And
    if you visit the repository on GitHub, you will find that all the functions for
    such an act can easily be done.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we simply take out 20% of the dataset, and set it aside. A shuffle
    is used to resample the rows so that we don't train on the same 80% every time.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that now the `clean` function takes `ignored`, while in the exploratory
    mode, it took `nil`. This, along with the `shuffle`, are important for cross-validation
    later on.
  prefs: []
  type: TYPE_NORMAL
- en: The regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And so, now we're ready to build the regression model. Bear in mind that this
    section is highly iterative in real life. I will describe the iterations, but
    will only share the model that I chose to settle on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `github.com/sajari/regression` package does an admirable job. But we want
    to extend the package a little to be able to compare models and the coefficients
    of the parameters. So I wrote this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`runRegression` will perform the regression analysis, and print the outputs
    of the standard errors of the coefficients. It is an estimate of the standard
    deviation of the coefficients—imagine this model being run many many times: each
    time the coefficients might be slightly different. The standard error simply reports
    amount of variation in the coefficients.'
  prefs: []
  type: TYPE_NORMAL
- en: The standard errors are calculated with the help of the `gorgonia.org/vecf64`
    package, which performs in-place operations for vectors. Optionally, you may choose
    to write them as loops.
  prefs: []
  type: TYPE_NORMAL
- en: This function also introduces us to the API for the `github.com/sajari/regression`
    package—to predict, simply use `r.Predict(vars)`. This will be useful in cases
    where one would like to use this model for production.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let us focus on the other half of the main function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Here, we run the regression, and then we print the results. We don't just want
    to output the regression coefficients. We also want to output the standard errors,
    the t-statistic, and the P-value. This would give us some confidence over the
    estimated coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: '`tdist := distuv.StudentsT{Mu: 0, Sigma: 1, Nu: float64(len(it) - len(newHdr)
    - 1), Src: rand.New(rand.NewSource(uint64(time.Now().UnixNano())))}` creates a
    Student''s t-distribution, which we will compare against our data. The t-statistic
    is very simply calculated by dividing the coefficient by the standard error.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And now we come to the final part—in order to compare models, we would like
    to cross-validate the model. We've already set aside a portion of the data. Now,
    we will have to test the model on the data that was set aside, and compute a score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The score we''ll be using is a Root Mean Square Error. It''s used because it''s
    simple and straightforward to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: With this, now we're really ready to run the regression analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Running the regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simply run the program. If the program is run with an empty ignored list, the
    result will show up as a bunch of NaNs. Do you recall that earlier we have done
    some correlation analysis on how some variables are correlated with one another?
  prefs: []
  type: TYPE_NORMAL
- en: We'll start by adding those into our ignored list, and then run the regression.
    Once we have a score that is no longer NaN, we can start comparing models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final model I have prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The cross-validation results (a RMSE of 0.143) are decent—not the best, but
    not the worst either. This was done through careful elimination of variables.
    A seasoned econometrician may come into this, read the results, and decide that
    further feature engineering may be done.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, looking at these results, off the top of my head I could think of several
    other feature engineering that could be done—subtracting the year remodeled from
    the year sold (recency of remodeling/renovations). Another form of feature engineering
    is to run a PCA-whitening process on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For linear regression models, I tend to stay away from complicated feature engineering.
    This is because the key benefit of a linear regression is that it's explainable
    in natural language.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can say this: for every unit increase in lot area size, if
    everything else is held constant, we can expect a 0.07103 times increment in house
    price.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A particularly counter intuitive result from this regression is the `PoolArea`
    variable. Interpreting the results, we would say: for every unit increase in pool
    area, we can expect a -0.00075 times increment in price, *ceteris paribus*. Granted,
    the p-value of the coefficient is 0.397, meaning that this coefficient could have
    been gotten by sheer random chance. Hence, we must be quite careful in saying
    this—having a pool decreases the value of your property in Ames, Massachusetts.'
  prefs: []
  type: TYPE_NORMAL
- en: Discussion and further work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This model is now ready to be used to predict things. Is this the best model?
    No, it's not. Finding the best model is a never ending quest. To be sure, there
    are indefinite ways of improving this model. One can use LASSO methods to determine
    the importance of variables before using them.
  prefs: []
  type: TYPE_NORMAL
- en: The model is not only the linear regression, but also the data cleaning functions
    and ingestion functions that come with it. This leads to a very high number of
    tweakable parameters. Maybe if you didn't like the way I imputed data, you can
    always write your own method!
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore the code in this chapter can be cleaned up further. Instead of returning
    so many values in the clean function, a new tuple type can be created to hold
    the Xs and Ys—a data frame of sorts. In fact, that's what we're going to build
    in the upcoming chapters. Several functions can be made more efficient using a
    state-holder struct.
  prefs: []
  type: TYPE_NORMAL
- en: If you will note, there are not very many statistical packages like Pandas for
    Go. This is not for the lack of trying. Go as a language is all about solving
    problems, not about building generic packages. There are definitely dataframe-like
    packages in Go, but in my experience, using them tends to blind one to the most
    obvious and efficient solutions. Often, it's better to build your own data structures
    that are specific to the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part in Go, the model building is an iterative process, while productionizing
    the model is a process that happens after the model has been built. This chapter
    shows that with a little awkwardness, it is possible to build a model using an
    iterative process that immediately translates to a production-ready system.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to explore data (with some awkwardness)
    using Go. We plotted some charts and used them as a guiding rod to select variables
    for the regression. Following that, we implemented a regression model that came
    with reporting of errors which enabled us to compare models. Lastly, to ensure
    we were not over fitting, we used a RMSE score to cross-validate our model and
    came out with a fairly decent score.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a taste of what is to come. The ideas in abstract are repeated
    over the next chapters—we will be cleaning data, then writing the machine learning
    model, which will be cross-validated. The only difference will generally be the
    data, and the models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll learn a simple way to determine if an email is spam
    or not.
  prefs: []
  type: TYPE_NORMAL
