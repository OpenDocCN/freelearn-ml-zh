["```py\nfrom sklearn.datasets import make_blobs\n\nnb_samples = 1000\nnb_unlabeled = 750\n\nX, Y = make_blobs(n_samples=nb_samples, n_features=2, centers=2, cluster_std=2.5, random_state=100)\n\nunlabeled_idx = np.random.choice(np.arange(0, nb_samples, 1), replace=False, size=nb_unlabeled)\nY[unlabeled_idx] = -1\n```", "```py\nimport numpy as np\n\n# First Gaussian\nm1 = np.random.uniform(-7.5, 10.0, size=2)\nc1 = np.random.uniform(5.0, 15.0, size=(2, 2))\nc1 = np.dot(c1, c1.T)\nq1 = 0.5\n\n# Second Gaussian\nm2 = np.random.uniform(-7.5, 10.0, size=2)\nc2 = np.random.uniform(5.0, 15.0, size=(2, 2))\nc2 = np.dot(c2, c2.T)\nq2 = 0.5\n```", "```py\nimport numpy as np\n\n# First Gaussian\nm1 = np.array([-3.0, -4.5])\nc1 = np.array([[25.0, 5.0], \n               [5.0, 35.0]])\nq1 = 0.5\n\n# Second Gaussian\nm2 = np.array([5.0, 10.0])\nc2 = np.array([[25.0, -10.0], \n               [-10.0, 25.0]])\nq2 = 0.5\n```", "```py\nfrom scipy.stats import multivariate_normal\n\nnb_iterations = 5\n\nfor i in range(nb_iterations):\n    Pij = np.zeros((nb_samples, 2))\n\n    for i in range(nb_samples):\n        if Y[i] == -1:\n            p1 = multivariate_normal.pdf(X[i], m1, c1, allow_singular=True) * q1\n            p2 = multivariate_normal.pdf(X[i], m2, c2, allow_singular=True) * q2\n            Pij[i] = [p1, p2] / (p1 + p2)\n\n        else:\n            Pij[i, :] = [1.0, 0.0] if Y[i] == 0 else [0.0, 1.0]\n\n    n = np.sum(Pij, axis=0)\n    m = np.sum(np.dot(Pij.T, X), axis=0)\n\n    m1 = np.dot(Pij[:, 0], X) / n[0]\n    m2 = np.dot(Pij[:, 1], X) / n[1]\n\n    q1 = n[0] / float(nb_samples)\n    q2 = n[1] / float(nb_samples)\n\n    c1 = np.zeros((2, 2))\n    c2 = np.zeros((2, 2))\n\n    for t in range(nb_samples):\n        c1 += Pij[t, 0] * np.outer(X[t] - m1, X[t] - m1)\n        c2 += Pij[t, 1] * np.outer(X[t] - m2, X[t] - m2)\n\n    c1 /= n[0]\n    c2 /= n[1]\n```", "```py\nprint(np.round(X[Y==-1][0:10], 3))\n\n[[  1.67    7.204]\n [ -1.347  -5.672]\n [ -2.395  10.952]\n [ -0.261   6.526]\n [  1.053   8.961]\n [ -0.579  -7.431]\n [  0.956   9.739]\n [ -5.889   5.227]\n [ -2.761   8.615]\n [ -1.777   4.717]]\n```", "```py\nprint(np.round(Pij[Y==-1][0:10], 3))\n\n[[ 0.002  0.998]\n [ 1\\.     0\\.   ]\n [ 0\\.     1\\.   ]\n [ 0.003  0.997]\n [ 0\\.     1\\.   ]\n [ 1\\.     0\\.   ]\n [ 0\\.     1\\.   ]\n [ 0.007  0.993]\n [ 0\\.     1\\.   ]\n [ 0.02   0.98 ]]\n```", "```py\nfrom sklearn.datasets import load_digits\n\nimport numpy as np\n\nX_a, Y_a = load_digits(return_X_y=True)\n\nX = np.vstack((X_a[Y_a == 0], X_a[Y_a == 1]))\nY = np.vstack((np.expand_dims(Y_a, axis=1)[Y_a==0], np.expand_dims(Y_a, axis=1)[Y_a==1]))\n\nnb_samples = X.shape[0]\nnb_dimensions = X.shape[1]\nnb_unlabeled = 150\nY_true = np.zeros((nb_unlabeled,))\n\nunlabeled_idx = np.random.choice(np.arange(0, nb_samples, 1), replace=False, size=nb_unlabeled)\nY_true = Y[unlabeled_idx].copy()\nY[unlabeled_idx] = -1\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\n\nlr_test = LogisticRegression()\nlr_test.fit(X[Y.squeeze() != -1], Y[Y.squeeze() != -1].squeeze())\nunlabeled_score = lr_test.score(X[Y.squeeze() == -1], Y_true)\n\nprint(unlabeled_score)\n0.573333333333\n```", "```py\nfrom sklearn.model_selection import cross_val_score\n\ntotal_cv_scores = cross_val_score(LogisticRegression(), X, Y.squeeze(), cv=10)\n\nprint(total_cv_scores)\n[ 0.48648649  0.51351351  0.5         0.38888889  0.52777778  0.36111111\n  0.58333333  0.47222222  0.54285714  0.45714286]\n```", "```py\nlr = LogisticRegression()\nq0 = np.random.uniform(0, 1, size=nb_unlabeled)\n```", "```py\ntrh = np.vectorize(lambda x: 0.0 if x < 0.5 else 1.0)\n```", "```py\ndef weighted_log_loss(yt, p, w=None, eps=1e-15):\n    if w is None:\n        w_t = np.ones((yt.shape[0], 2))\n    else:\n        w_t = np.vstack((w, 1.0 - w)).T\n\n    Y_t = np.vstack((1.0 - yt.squeeze(), yt.squeeze())).T\n    L_t = np.sum(w_t * Y_t * np.log(np.clip(p, eps, 1.0 - eps)), axis=1)\n\n    return np.mean(L_t)\n```", "```py\ndef build_dataset(q):\n    Y_unlabeled = trh(q)\n\n    X_n = np.zeros((nb_samples, nb_dimensions))\n    X_n[0:nb_samples - nb_unlabeled] = X[Y.squeeze()!=-1]\n    X_n[nb_samples - nb_unlabeled:] = X[Y.squeeze()==-1]\n\n    Y_n = np.zeros((nb_samples, 1))\n    Y_n[0:nb_samples - nb_unlabeled] = Y[Y.squeeze()!=-1]\n    Y_n[nb_samples - nb_unlabeled:] = np.expand_dims(Y_unlabeled, axis=1)\n\n    return X_n, Y_n\n```", "```py\ndef log_likelihood(q):\n    X_n, Y_n = build_dataset(q)\n    Y_soft = trh(q)\n\n    lr.fit(X_n, Y_n.squeeze())\n\n    p_sup = lr.predict_proba(X[Y.squeeze() != -1])\n    p_semi = lr.predict_proba(X[Y.squeeze() == -1])\n\n    l_sup = weighted_log_loss(Y[Y.squeeze() != -1], p_sup)\n    l_semi = weighted_log_loss(Y_soft, p_semi, q)\n\n    return l_semi - l_sup\n```", "```py\nfrom scipy.optimize import fmin_bfgs\n\nq_end = fmin_bfgs(f=log_likelihood, x0=q0, maxiter=5000, disp=False)\n```", "```py\nX_n, Y_n = build_dataset(q_end)\n```", "```py\nfinal_semi_cv_scores = cross_val_score(LogisticRegression(), X_n, Y_n.squeeze(), cv=10)\n\nprint(final_semi_cv_scores)\n[ 1\\.          1\\.          0.89189189  0.77777778  0.97222222  0.88888889\n  0.61111111  0.88571429  0.94285714  0.48571429]\n```", "```py\nfrom sklearn.datasets import make_classification\n\nnb_samples = 500\nnb_unlabeled = 200\n\nX, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, random_state=1000)\nY[Y==0] = -1\nY[nb_samples - nb_unlabeled:nb_samples] = 0\n```", "```py\nimport numpy as np\n\nw = np.random.uniform(-0.1, 0.1, size=X.shape[1])\neta = np.random.uniform(0.0, 0.1, size=nb_samples - nb_unlabeled)\nxi = np.random.uniform(0.0, 0.1, size=nb_unlabeled)\nzi = np.random.uniform(0.0, 0.1, size=nb_unlabeled)\nb = np.random.uniform(-0.1, 0.1, size=1)\nC = 1.0\n\ntheta0 = np.hstack((w, eta, xi, zi, b))\n```", "```py\nvmin = np.vectorize(lambda x1, x2: x1 if x1 <= x2 else x2)\n```", "```py\ndef svm_target(theta, Xd, Yd):\n    wt = theta[0:2].reshape((Xd.shape[1], 1))\n\n    s_eta = np.sum(theta[2:2 + nb_samples - nb_unlabeled])\n    s_min_xi_zi = np.sum(vmin(theta[2 + nb_samples - nb_unlabeled:2 + nb_samples], \n                              theta[2 + nb_samples:2 + nb_samples + nb_unlabeled]))\n\n    return C * (s_eta + s_min_xi_zi) + 0.5 * np.dot(wt.T, wt)\n```", "```py\ndef labeled_constraint(theta, Xd, Yd, idx):\n    wt = theta[0:2].reshape((Xd.shape[1], 1))\n\n    c = Yd[idx] * (np.dot(Xd[idx], wt) + theta[-1]) + \\\n    theta[2:2 + nb_samples - nb_unlabeled][idx] - 1.0\n\n    return (c >= 0)[0]\n```", "```py\ndef unlabeled_constraint_1(theta, Xd, idx):\n    wt = theta[0:2].reshape((Xd.shape[1], 1))\n\n    c = np.dot(Xd[idx], wt) - theta[-1] + \\\n        theta[2 + nb_samples - nb_unlabeled:2 + nb_samples][idx - nb_samples + nb_unlabeled] - 1.0\n\n    return (c >= 0)[0]\n\ndef unlabeled_constraint_2(theta, Xd, idx):\n    wt = theta[0:2].reshape((Xd.shape[1], 1))\n\n    c = -(np.dot(Xd[idx], wt) - theta[-1]) + \\\n        theta[2 + nb_samples:2 + nb_samples + nb_unlabeled ][idx - nb_samples + nb_unlabeled] - 1.0\n\n    return (c >= 0)[0]\n```", "```py\ndef eta_constraint(theta, idx):\n    return theta[2:2 + nb_samples - nb_unlabeled][idx] >= 0\n\ndef xi_constraint(theta, idx):\n    return theta[2 + nb_samples - nb_unlabeled:2 + nb_samples][idx - nb_samples + nb_unlabeled] >= 0\n\ndef zi_constraint(theta, idx):\n    return theta[2 + nb_samples:2 + nb_samples+nb_unlabeled ][idx - nb_samples + nb_unlabeled] >= 0\n```", "```py\nsvm_constraints = []\n\nfor i in range(nb_samples - nb_unlabeled):\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': labeled_constraint,\n            'args': (X, Y, i)\n        })\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': eta_constraint,\n            'args': (i,)\n        })\n\nfor i in range(nb_samples - nb_unlabeled, nb_samples):\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': unlabeled_constraint_1,\n            'args': (X, i)\n        })\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': unlabeled_constraint_2,\n            'args': (X, i)\n        })\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': xi_constraint,\n            'args': (i,)\n        })\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': zi_constraint,\n            'args': (i,)\n        })\n```", "```py\nfrom scipy.optimize import minimize\n\nresult = minimize(fun=svm_target, \n                  x0=theta0, \n                  constraints=svm_constraints, \n                  args=(X, Y), \n                  method='SLSQP', \n                  tol=0.0001, \n                  options={'maxiter': 1000})\n```", "```py\ntheta_end = result['x']\nw = theta_end[0:2]\nb = theta_end[-1]\n\nXu= X[nb_samples - nb_unlabeled:nb_samples]\nyu = -np.sign(np.dot(Xu, w) + b)\n```", "```py\nfrom sklearn.datasets import make_classification\n\nnb_samples = 500\nnb_unlabeled = 400\n\nX, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, random_state=1000)\nY[Y==0] = -1\nY[nb_samples - nb_unlabeled:nb_samples] = 0\n```", "```py\nimport numpy as np\n\nw = np.random.uniform(-0.1, 0.1, size=X.shape[1])\neta_labeled = np.random.uniform(0.0, 0.1, size=nb_samples - nb_unlabeled)\neta_unlabeled = np.random.uniform(0.0, 0.1, size=nb_unlabeled)\ny_unlabeled = np.random.uniform(-1.0, 1.0, size=nb_unlabeled)\nb = np.random.uniform(-0.1, 0.1, size=1)\n\nC_labeled = 1.0\nC_unlabeled = 10.0\n\ntheta0 = np.hstack((w, eta_labeled, eta_unlabeled, y_unlabeled, b))\n```", "```py\ndef svm_target(theta, Xd, Yd):\n    wt = theta[0:2].reshape((Xd.shape[1], 1))\n\n    s_eta_labeled = np.sum(theta[2:2 + nb_samples - nb_unlabeled])\n    s_eta_unlabeled = np.sum(theta[2 + nb_samples - nb_unlabeled:2 + nb_samples])\n\n    return (C_labeled * s_eta_labeled) + (C_unlabeled * s_eta_unlabeled) + (0.5 * np.dot(wt.T, wt))\n```", "```py\ndef labeled_constraint(theta, Xd, Yd, idx):\n    wt = theta[0:2].reshape((Xd.shape[1], 1))\n\n    c = Yd[idx] * (np.dot(Xd[idx], wt) + theta[-1]) + \\\n    theta[2:2 + nb_samples - nb_unlabeled][idx] - 1.0\n\n    return (c >= 0)[0]\n\ndef unlabeled_constraint(theta, Xd, idx):\n    wt = theta[0:2].reshape((Xd.shape[1], 1))\n\n    c = theta[2 + nb_samples:2 + nb_samples + nb_unlabeled][idx - nb_samples + nb_unlabeled] * \\\n        (np.dot(Xd[idx], wt) + theta[-1]) + \\\n        theta[2 + nb_samples - nb_unlabeled:2 + nb_samples][idx - nb_samples + nb_unlabeled] - 1.0\n\n    return (c >= 0)[0]\n```", "```py\ndef eta_labeled_constraint(theta, idx):\n    return theta[2:2 + nb_samples - nb_unlabeled][idx] >= 0\n\ndef eta_unlabeled_constraint(theta, idx):\n    return theta[2 + nb_samples - nb_unlabeled:2 + nb_samples][idx - nb_samples + nb_unlabeled] >= 0\n\ndef y_constraint(theta, idx):\n    return np.power(theta[2 + nb_samples:2 + nb_samples + nb_unlabeled][idx], 2) == 1.0\n```", "```py\nsvm_constraints = []\n\nfor i in range(nb_samples - nb_unlabeled):\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': labeled_constraint,\n            'args': (X, Y, i)\n        })\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': eta_labeled_constraint,\n            'args': (i,)\n        })\n\nfor i in range(nb_samples - nb_unlabeled, nb_samples):\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': unlabeled_constraint,\n            'args': (X, i)\n        })\n    svm_constraints.append({\n            'type': 'ineq',\n            'fun': eta_unlabeled_constraint,\n            'args': (i,)\n        })\n\nfor i in range(nb_unlabeled):\n    svm_constraints.append({\n            'type': 'eq',\n            'fun': y_constraint,\n            'args': (i,)\n        })\n```", "```py\nfrom scipy.optimize import minimize\n\nresult = minimize(fun=svm_target, \n                  x0=theta0, \n                  constraints=svm_constraints, \n                  args=(X, Y), \n                  method='SLSQP', \n                  tol=0.0001, \n                  options={'maxiter': 1000})\n```", "```py\ntheta_end = result['x']\nw = theta_end[0:2]\nb = theta_end[-1]\n\nXu= X[nb_samples - nb_unlabeled:nb_samples]\nyu = -np.sign(np.dot(Xu, w) + b)\n```", "```py\nnb_samples = 100\nnb_unlabeled = 90\n\nX, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, random_state=100)\nY[Y==0] = -1\nY[nb_samples - nb_unlabeled:nb_samples] = 0\n```", "```py\nfrom sklearn.svm import SVC\n\nsvc = SVC(kernel='linear', C=1.0)\nsvc.fit(X[Y!=0], Y[Y!=0])\n\nXu_svc= X[nb_samples - nb_unlabeled:nb_samples]\nyu_svc = svc.predict(Xu_svc)\n```"]