- en: Chapter 9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quantum Support Vector Machines
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial Intelligence is the new electricity*'
  prefs: []
  type: TYPE_NORMAL
- en: — Andrew Ng
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned the basics of machine learning and we got
    a sneak peek into quantum machine learning. It is now time for us to work with
    our first family of quantum machine learning models: that of **Quantum** **Support
    Vector Machines** (often abbreviated as **QSVM**s). These are very popular models,
    and they are most naturally used in binary classification problems.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we shall learn what (classical) support vector machines are
    and how they are used, and we will use this knowledge as a foundation to understand
    quantum support vector machines. In addition, we will explore how to implement
    and train quantum support vector machines with Qiskit and PennyLane.
  prefs: []
  type: TYPE_NORMAL
- en: 'The contents of this chapter are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going quantum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantum support vector machines in PennyLane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantum support vector machines in Qiskit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.1 Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'QSVMs are actually particular cases of **Support Vector Machines** (abbreviated
    as **SVM**s). In this section, we will explore how these SVMs work and how they’re
    used in machine learning. We will do so by first motivating the SVM formalism
    with some simple examples, and then building up from there: all the way up into
    how SVMs can be used to tackle complex classification problems with the kernel
    trick.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.1 The simplest classifier you could think of
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us forget about data for a moment and begin by considering a very naive
    problem. Let’s say that we want to build a very simple classifier on the real
    line. In order to do this, all we have to do is split the real number line into
    two disjoint categories in such a way that any number belong to exactly one of
    these two categories. Thus, if we are given any input (a real number), our classifier
    will return the category to which it belongs.
  prefs: []
  type: TYPE_NORMAL
- en: 'What would be the easiest way in which you could do this? Odds are you would
    first pick a point ![a](img/file16.png "a") and divide the real number line into
    the set (category) of numbers smaller than ![a](img/file16.png "a") and the set
    of numbers larger than ![a](img/file16.png "a"). Then, of course, you would have
    to assign ![a](img/file16.png "a") to one of the two categories, so your categories
    would be either of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The set of real numbers ![x](img/file269.png "x") such that ![x \leq a](img/file1190.png
    "x \leq a") and the set of numbers ![x](img/file269.png "x") such that ![x > a](img/file1191.png
    "x > a")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The set of numbers ![x](img/file269.png "x") such that ![x < a](img/file1192.png
    "x < a") and the set of numbers ![x](img/file269.png "x") such that ![x \geq a](img/file1193.png
    "x \geq a")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either choice would be reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: Actually, the choice as to in which category to include ![a](img/file16.png
    "a") is, to some extent, meaningless. At the end of the day, if you choose a real
    number at random, the probability that it be exactly ![a](img/file16.png "a")
    is zero. This fun fact is sponsored by probability and measure theory!
  prefs: []
  type: TYPE_NORMAL
- en: That was easy. Let’s now say that we want to do the same with the real plane
    (the usual ![R^{2}](img/file1194.png "R^{2}")). In this case, a single point will
    not suffice to split it, but we could instead consider a good old line! This is
    exemplified in *Figure* [*9.1*](#Figure9.1). Any line can be used to perfectly
    split the real plane into two categories.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1: The line \left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right., which can be equivalently written as \left. y = (5\slash 4)x
    \right., can be used to divide the real plane into two disjoint categories, which
    are colored differently. The picture does not specify to which category the line
    belongs ](img/file1197.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.1**: The line ![\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right.](img/file1195.png "\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right."), which can be equivalently written as ![\left. y = (5\slash
    4)x \right.](img/file1196.png "\left. y = (5\slash 4)x \right."), can be used
    to divide the real plane into two disjoint categories, which are colored differently.
    The picture does not specify to which category the line belongs'
  prefs: []
  type: TYPE_NORMAL
- en: If you go back to your linear algebra notes, you may recall that any line in
    the plane can be characterized in terms of a vector ![\overset{\rightarrow}{w}
    \in R^{2}](img/file1198.png "\overset{\rightarrow}{w} \in R^{2}") and a scalar
    ![b \in R](img/file1199.png "b \in R") as the set of points ![\overset{\rightarrow}{x}
    = (x,y)](img/file1200.png "\overset{\rightarrow}{x} = (x,y)") such that ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0"). Of course, we are using ![\cdot](img/file1202.png
    "\cdot") to denote the scalar product (that is, ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{x} = w_{1}x + w_{2}y](img/file1203.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} = w_{1}x + w_{2}y"), provided that ![\overset{\rightarrow}{w}
    = (w_{1},w_{2})](img/file1204.png "\overset{\rightarrow}{w} = (w_{1},w_{2})")).
    The vector ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    defines the **normal**, or perpendicular, direction to the line, and the constant
    ![b](img/file17.png "b") determines the intersection of the line with the ![X](img/file9.png
    "X") and ![Y](img/file11.png "Y") axes.
  prefs: []
  type: TYPE_NORMAL
- en: When we worked on the one-dimensional case and used a point to split the real
    line, it was trivial to decide which category any input belonged to. In this case,
    it is slightly more complicated, but not too much. With some elementary geometry,
    you can check that any number ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    will be on one side or the other of the line defined by ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0") depending on the sign of the quantity
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b"). That is, if ![\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b](img/file1208.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b") and ![\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{2} + b](img/file1209.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{2} + b") have the same sign (both smaller than
    zero or both greater than zero), we will know that ![{\overset{\rightarrow}{x}}_{1}](img/file1210.png
    "{\overset{\rightarrow}{x}}_{1}") and ![{\overset{\rightarrow}{x}}_{2}](img/file1211.png
    "{\overset{\rightarrow}{x}}_{2}") will belong to the same category. Otherwise,
    we know they will not.
  prefs: []
  type: TYPE_NORMAL
- en: There is no reason for us to stop at two dimensions, so let’s kick this up a
    notch and consider an ![n](img/file244.png "n")-dimensional Euclidean space ![R^{n}](img/file1212.png
    "R^{n}"). Just as we split ![R^{2}](img/file1194.png "R^{2}") using a line, we
    could split ![R^{n}](img/file1212.png "R^{n}") using…an (![n - 1](img/file1213.png
    "n - 1"))-dimensional hyperplane! For instance, we could split ![R^{3}](img/file1214.png
    "R^{3}") using an ordinary plane.
  prefs: []
  type: TYPE_NORMAL
- en: These hyperplanes in ![R^{n}](img/file1212.png "R^{n}") are defined by their
    normal vectors ![\overset{\rightarrow}{w} \in R^{n}](img/file1215.png "\overset{\rightarrow}{w}
    \in R^{n}") and some constants ![b \in R](img/file1199.png "b \in R"). In analogy
    to what we saw in ![R^{2}](img/file1194.png "R^{2}"), their points are the ![\overset{\rightarrow}{x}
    \in R^{n}](img/file1216.png "\overset{\rightarrow}{x} \in R^{n}") that satisfy
    the equations of the form
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.](img/file1217.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.") |'
  prefs: []
  type: TYPE_TB
- en: Moreover, we can determine to which side of the hyperplane a certain ![\overset{\rightarrow}{x}
    \in R^{n}](img/file1216.png "\overset{\rightarrow}{x} \in R^{n}") is in terms
    of the sign of ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b").
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: In case you are confused with all these equations and you are curious as to
    where they come from, let us quickly explain them. An (affine) hyperplane can
    be defined by a normal vector ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    and by a point ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    in the plane. Thus, a point ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    will belong to the hyperplane if and only if ![\overset{\rightarrow}{x} = \overset{\rightarrow}{p}
    + \overset{\rightarrow}{v}](img/file1219.png "\overset{\rightarrow}{x} = \overset{\rightarrow}{p}
    + \overset{\rightarrow}{v}") for some vector ![\overset{\rightarrow}{v}](img/file1220.png
    "\overset{\rightarrow}{v}") that is orthogonal to ![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}"), that is, such that ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{v} = 0](img/file1221.png "\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{v} = 0"). By combining these two expressions, we know that
    ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}") will
    belong to the hyperplane if and only if ![\overset{\rightarrow}{w} \cdot (\overset{\rightarrow}{x}
    - \overset{\rightarrow}{p}) = 0](img/file1222.png "\overset{\rightarrow}{w} \cdot
    (\overset{\rightarrow}{x} - \overset{\rightarrow}{p}) = 0"), which can be rewritten
    as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + ( - \overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{p}) = \overset{\rightarrow}{x} \cdot \overset{\rightarrow}{w}
    + b = 0,](img/file1223.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + ( - \overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}) = \overset{\rightarrow}{x}
    \cdot \overset{\rightarrow}{w} + b = 0,") |'
  prefs: []
  type: TYPE_TB
- en: where we have implicitly defined ![b = - \overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}](img/file1224.png
    "b = - \overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}").
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we have just seen how ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b](img/file1207.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b") is the scalar product of ![\overset{\rightarrow}{x} - \overset{\rightarrow}{p}](img/file1225.png
    "\overset{\rightarrow}{x} - \overset{\rightarrow}{p}") with ![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}"), a fixed normal vector to the plane. This justifies
    why its sign determines on which side of the hyperplane ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}") lies. Remember that, geometrically, the dot product
    of two vectors ![\overset{\rightarrow}{u_{1}} \cdot {\overset{\rightarrow}{u}}_{2}](img/file1226.png
    "\overset{\rightarrow}{u_{1}} \cdot {\overset{\rightarrow}{u}}_{2}") is equal
    to ![\left\| u \right\|_{1} \cdot \left\| u \right\|_{2} \cdot \cos\theta](img/file1227.png
    "\left\| u \right\|_{1} \cdot \left\| u \right\|_{2} \cdot \cos\theta"), where
    ![\theta](img/file89.png "\theta") denotes the smallest angle between them.
  prefs: []
  type: TYPE_NORMAL
- en: With what we have done so far, we have the tools required to construct (admittedly
    simple) binary classifiers on any Euclidean space. All it takes for us to do so
    is fixing a hyperplane!
  prefs: []
  type: TYPE_NORMAL
- en: Why is this important to us? It turns out that support vector machines do exactly
    what we have discussed so far.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'A support vector machine takes inputs in an ![n](img/file244.png "n")-dimensional
    Euclidean space (![R^{n}](img/file1212.png "R^{n}")) and classifies them according
    to which side of a hyperplane they are on. This hyperplane fully defines the behavior
    of the SVM. Of course, the adjustable parameters of an SVM are the ones that define
    the hyperplane: following our notation, the components of the normal vector ![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}") and the constant ![b](img/file17.png "b").'
  prefs: []
  type: TYPE_NORMAL
- en: In order to get the label of any point ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"), all we have to do is look at the sign of ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b](img/file1207.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b").
  prefs: []
  type: TYPE_NORMAL
- en: 'As you may have suspected, vanilla SVMs, just on their own, are not the most
    powerful of binary classification models: they are intrinsically linear and they
    are not fit to capture sophisticated patterns. We will take care of this later
    in the chapter when we unleash the full potential of SVMs with ”the kernel trick”
    (stay tuned!). In any case, for now, let us rejoice in the simplicity of our model
    and let’s learn how to train it.'
  prefs: []
  type: TYPE_NORMAL
- en: '9.1.2 How to train support vector machines: the hard-margin case'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say that we have a binary classification problem, and we are given some
    training data consisting of datapoints in ![R^{n}](img/file1212.png "R^{n}") together
    with their corresponding labels. Naturally, when we train an SVM for this problem,
    we want to look for the hyperplane that best separates the two categories in the
    training dataset. Now we have to make this intuitive idea precise.
  prefs: []
  type: TYPE_NORMAL
- en: Let the datapoints in our training dataset be ![{\overset{\rightarrow}{x}}_{j}
    \in R^{n}](img/file1228.png "{\overset{\rightarrow}{x}}_{j} \in R^{n}") and their
    expected labels be ![y_{j} = 1, - 1](img/file1229.png "y_{j} = 1, - 1") (read
    as positive and negative, respectively). For now, we will assume that our data
    can be perfectly separated by a hyperplane. Later in the section, we will see
    what to do when this is not the case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that, under the assumption that there is at least one hyperplane separating
    our data, there will necessarily be an infinite number of such separating hyperplanes
    (see *Figure* [*9.2*](#Figure9.2)). Will any of them be suitable for our goal
    of building a classifier? If we only cared about the training data, then yes,
    any of them would do the trick. In fact, this is exactly what the perceptron model
    that we discussed in *Chapter* *[*8*](ch017.xhtml#x1-1390008), *What is Quantum
    Machine* *Learning?*, does: it just looks for a hyperplane separating the training
    data.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*However, as you surely remember, when we train a classifier, we are interested
    in getting a low generalization error. In our case, one way of trying to achieve
    this is by looking for a separating hyperplane that can maximize the distance
    from itself to the training datapoints. And that is the way in which SVMs are
    actually trained. The rationale behind this is clear: we expect the new, unseen
    datapoints to follow a similar distribution to the one that we have seen in the
    training data. So it is very likely that new examples of one class will be closer
    to training examples of that same class. Therefore, if our separating hyperplane
    is too close to one of the training datapoints, we risk another datapoint of the
    same class crossing to the other side of the hyperplane and being misclassified.
    For instance, in *Figure* [*9.2*](#Figure9.2), the dashed line does separate the
    training datapoints, but it is certainly a much more risky choice than, for example,
    the continuous line.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2: Both lines (hyperplanes) separate the two categories, but the
    continuous line is closer to the datapoints than the dashed line ](img/file1230.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.2**: Both lines (hyperplanes) separate the two categories, but the
    continuous line is closer to the datapoints than the dashed line'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea behind the training of an SVM is then clear: we seek to find not just
    any separating hyperplane, but one that is as far away from the training points
    as possible. This may seem difficult to achieve, but it can be posed as a rather
    straightforward optimization problem. Let’s explain how to do it in a little bit
    more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: In a first approach, we could just consider the distance from a separating hyperplane
    ![H](img/file10.png "H") to all the points in the training dataset, and then try
    to find a way to tweak ![H](img/file10.png "H") in order to maximize that distance
    while making sure that ![H](img/file10.png "H") still separates the data properly.
    This is, however, not the best way to present the problem. Instead, we may notice
    how we can associate to each data point a unique hyperplane that is parallel to
    ![H](img/file10.png "H") and contains that datapoint. And, what is more, the parallel
    hyperplane that goes through the point that is closest to ![H](img/file10.png
    "H") will itself be a separating hyperplane — and so will be its reflection over
    ![H](img/file10.png "H"). This is illustrated in *Figure* [*9.3*](#Figure9.3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3: The continuous black line represents a separating hyperplane
    H. One of the dashed lines is the parallel hyperplane that goes through the closest
    point to H, and its reflection over H is the other dashed line ](img/file1231.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.3**: The continuous black line represents a separating hyperplane
    ![H](img/file10.png "H"). One of the dashed lines is the parallel hyperplane that
    goes through the closest point to ![H](img/file10.png "H"), and its reflection
    over ![H](img/file10.png "H") is the other dashed line'
  prefs: []
  type: TYPE_NORMAL
- en: This pair of hyperplanes — the parallel plane that goes through the closest
    point and its reflection — will be the two equidistant parallel hyperplanes, which
    are the furthest apart from each other while still separating the data. They are
    unique to ![H](img/file10.png "H"). The distance between them is known as the
    **margin** and it is what we aim to maximize. This is illustrated in *Figure*
    [*9.4*](#Figure9.4).
  prefs: []
  type: TYPE_NORMAL
- en: We already know that any separating hyperplane ![H](img/file10.png "H") can
    be characterized by an equation of the form ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0"). Moreover, any hyperplane that is parallel to ![H](img/file10.png "H")
    — in particular those that define the margin! — can be characterized as ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = C](img/file1232.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = C") for some constant ![C](img/file234.png
    "C"). And not only that, but their reflection over ![H](img/file10.png "H") will
    be itself characterized by the equation ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - C](img/file1233.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - C"). Hence, we know that, for some constant ![C](img/file234.png "C"),
    the hyperplanes that define the margin of ![H](img/file10.png "H") can be represented
    by the equations ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b
    = \pm C](img/file1234.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = \pm C").
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, there is nothing preventing us here from dividing the whole expression
    by ![C](img/file234.png "C"). So, if we let ![\left. \overset{\sim}{w} = \overset{\rightarrow}{w}\slash
    C \right.](img/file1235.png "\left. \overset{\sim}{w} = \overset{\rightarrow}{w}\slash
    C \right.") and ![\left. \overset{\sim}{b} = b\slash C \right.](img/file1236.png
    "\left. \overset{\sim}{b} = b\slash C \right."), we know that the hyperplane ![H](img/file10.png
    "H") will still be represented by ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0](img/file1237.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0"), but the hyperplanes that define the margin will be
    characterized by
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b} =
    \pm 1,](img/file1238.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b}
    = \pm 1,") |'
  prefs: []
  type: TYPE_TB
- en: which looks much more neat!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s summarize what we have. We want to find a hyperplane that, while separating
    the data properly, maximizes the distance to the points in the training dataset.
    We have seen how we can see this as the problem of finding a hyperplane that maximizes
    the margin: the distance between the two equidistant parallel hyperplanes that
    are the furthest away from each other while still separating the data. And we
    have just proven that, for any separating hyperplane, we can always find some
    values of ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    and ![b](img/file17.png "b") such that those hyperplanes that define the margin
    can be represented as'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.](img/file1239.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.") |'
  prefs: []
  type: TYPE_TB
- en: It can be shown that the distance between these two hyperplanes is ![\left.
    2\slash\left\| w \right\| \right.](img/file1240.png "\left. 2\slash\left\| w \right\|
    \right."). Hence the problem of maximizing the margin can be equivalently stated
    as the problem of maximizing ![\left. 2\slash\left\| w \right\| \right.](img/file1240.png
    "\left. 2\slash\left\| w \right\| \right.") subject to the constraint that the
    planes ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1](img/file1241.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1") properly
    separate the data.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.1
  prefs: []
  type: TYPE_NORMAL
- en: Show that, as we claimed, the distance between the hyperplanes ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1](img/file1241.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1") is ![\left. 2\slash\left\| w \right\|
    \right.](img/file1240.png "\left. 2\slash\left\| w \right\| \right.").
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now consider an arbitrary element ![\overset{\rightarrow}{p} \in R^{N}](img/file1242.png
    "\overset{\rightarrow}{p} \in R^{N}") and a hyperplane ![H](img/file10.png "H")
    characterized by ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b
    = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0"). When the value of ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}
    + b](img/file1243.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}
    + b") is zero, we know that ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    is in the hyperplane and, as this value drifts away from zero, the point gets
    further and further away from the hyperplane. If it increases and it is between
    ![0](img/file12.png "0") and ![1](img/file13.png "1"), the point ![\overset{\rightarrow}{p}](img/file1218.png
    "\overset{\rightarrow}{p}") is between the hyperplane ![H](img/file10.png "H")
    and the hyperplane ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} +
    b = 1](img/file1244.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 1"). When this value reaches ![1](img/file13.png "1"), the point is in this
    latter hyperplane. And when the value becomes greater than ![1](img/file13.png
    "1"), it moves beyond both hyperplanes. Analogously, if this value decreases and
    it is between ![0](img/file12.png "0") and ![- 1](img/file312.png "- 1"), the
    point ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    is between the hyperplane ![H](img/file10.png "H") and ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - 1](img/file1245.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - 1"). When the value reaches ![- 1](img/file312.png
    "- 1"), the point is in this last hyperplane. And when it is smaller than ![-
    1](img/file312.png "- 1"), it has moved beyond both ![H](img/file10.png "H") and
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = - 1](img/file1245.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = - 1").
  prefs: []
  type: TYPE_NORMAL
- en: Since we are working under the assumption that there are no points inside the
    margin, the hyperplane ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0") will properly separate the data if, for all the positive entries, ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b \geq 1](img/file1246.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b \geq 1"), while all the negative ones will
    satisfy ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b \leq - 1](img/file1247.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b \leq - 1"). We can
    write this condition as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![y_{j}\left( {\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j}}
    \right) \geq 1,](img/file1248.png "y_{j}\left( {\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j}} \right) \geq 1,") |'
  prefs: []
  type: TYPE_TB
- en: because we are considering ![y_{j} = 1](img/file1249.png "y_{j} = 1") when the
    ![j](img/file258.png "j")-th example belongs to the positive class and ![y_{j}
    = - 1](img/file1250.png "y_{j} = - 1") when it belongs to the negative one.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4: The hyperplane that could have been returned by an SVM is represented
    by a black continuous line, and the lines in dashed lines are the equidistant
    parallel hyperplanes that are the furthest apart from each other while still separating
    the data. The margin is thus half of the thickness of the colored region ](img/file1251.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.4**: The hyperplane that could have been returned by an SVM is represented
    by a black continuous line, and the lines in dashed lines are the equidistant
    parallel hyperplanes that are the furthest apart from each other while still separating
    the data. The margin is thus half of the thickness of the colored region'
  prefs: []
  type: TYPE_NORMAL
- en: 'For all this, the problem of finding the hyperplane that best separates the
    data can be posed as the following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\text{Minimize}\quad} & {\left\| w \right\|\qquad} &
    & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j},
    + b) \geq 1,\qquad} & & \qquad \\ \end{array}](img/file1252.png "\begin{array}{rlrl}
    {\text{Minimize}\quad} & {\left\| w \right\|\qquad} & & \qquad \\ {\text{subject~to}\quad}
    & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j}, + b) \geq
    1,\qquad} & & \qquad \\ \end{array}")'
  prefs: []
  type: TYPE_IMG
- en: where, of course, each ![j](img/file258.png "j") defines an individual constraint.
    This formulation suffers from a small problem. The Euclidean norm is nice, visual,
    and geometric, but it has a square root. We personally have nothing against square
    roots — some of our best friends *are* square roots — but most optimization algorithms
    have some hard feelings against them. So just to make life easier for us, we may
    instead consider the following (equivalent) problem.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'If the data in the training dataset can be separated by a hyperplane, the problem
    of training an SVM can be posed as the following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1.\qquad} & & \qquad \\ \end{array}](img/file1253.png
    "\begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1.\qquad} & & \qquad \\ \end{array}")'
  prefs: []
  type: TYPE_IMG
- en: This is known as **hard-margin** training, because we are allowing no elements
    in the training dataset to be misclassified or even to be inside the margin.
  prefs: []
  type: TYPE_NORMAL
- en: That nice and innocent square will save us from so many troubles. Notice, by
    the way, that we’ve introduced a ![\left. 1\slash 2 \right.](img/file136.png "\left.
    1\slash 2 \right.") factor next to ![\left\| w \right\|^{2}](img/file1254.png
    "\left\| w \right\|^{2}"). That’s for reasons of technical convenience, but it
    isn’t really important.
  prefs: []
  type: TYPE_NORMAL
- en: With hard-margin training, we need our training data to be perfectly separable
    by a hyperplane because, otherwise, we will not find any feasible solutions to
    the optimization problem that we have just defined. This scenario is, in most
    situations, too restrictive. Thankfully, we can take an alternative approach known
    as **soft-margin training**.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.3 Soft-margin training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Soft-margin training is similar to hard-margin training. The only difference
    is that it also incorporates some adjustable **slack**, or ”tolerance,” parameters
    ![\xi_{j} \geq 0](img/file1255.png "\xi_{j} \geq 0") that will add flexibility
    to the constraints. In this way, instead of considering the constraint ![y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1](img/file1256.png "y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1"), we will use
  prefs: []
  type: TYPE_NORMAL
- en: '| ![y_{j}(w \cdot x_{j} + b) \geq 1 - \xi_{j}.](img/file1257.png "y_{j}(w \cdot
    x_{j} + b) \geq 1 - \xi_{j}.") |'
  prefs: []
  type: TYPE_TB
- en: Thus, when ![\xi_{j} > 0](img/file1258.png "\xi_{j} > 0"), we will allow ![{\overset{\rightarrow}{x}}_{j}](img/file1259.png
    "{\overset{\rightarrow}{x}}_{j}") to be close to the hyperplane or even on the
    wrong side of the space (as separated by the hyperplane). What is more, the bigger
    the value of ![\xi_{j}](img/file1260.png "\xi_{j}"), the further into the wrong
    side ![{\overset{\rightarrow}{x}}_{j}](img/file1259.png "{\overset{\rightarrow}{x}}_{j}")
    will be.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, we would like these ![\xi_{j}](img/file1260.png "\xi_{j}") to be
    as small as possible, so we need to include them in the cost function that we
    want to minimize. Taking all of this into account, the optimization problem that
    we shall consider in soft-margin training will be the following.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'A support vector machine that may not be *necessarily* able to properly separate
    the training data with a hyperplane can be trained by solving the following optimization
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}
    + C\sum\limits_{j}\xi_{j}\qquad} & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1 - \xi_{j},\qquad} & & \qquad
    \\ & {\xi_{j} \geq 0.\qquad} & & \qquad \\ \end{array}](img/file1261.png "\begin{array}{rlrl}
    {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2} + C\sum\limits_{j}\xi_{j}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1 - \xi_{j},\qquad} & & \qquad \\  &
    {\xi_{j} \geq 0.\qquad} & & \qquad \\ \end{array}")'
  prefs: []
  type: TYPE_IMG
- en: The value ![C > 0](img/file1262.png "C > 0") is a hyperparameter that can be
    chosen at will. The bigger ![C](img/file234.png "C") is, the less tolerant we
    will be to training examples falling inside the margin or on the wrong side of
    the hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: This formulation is known as **soft-margin training** of an SVM.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now try to digest this formulation. As expected, we also made the ![\xi_{j}](img/file1260.png
    "\xi_{j}") contribute to our cost function, in such a way that their taking large
    values will be penalized. In addition, we’ve incorporated this ![C](img/file234.png
    "C") constant and said that it can be tweaked at will. As we mentioned before,
    in broad terms, the bigger it is, the more unwilling we will be to accept misclassified
    elements in the training dataset. Actually, if there is a hyperplane that can
    perfectly separate the data, setting ![C](img/file234.png "C") to a huge value
    would be equivalent to doing hard-margin training. At first, it might seem tempting
    to make ![C](img/file234.png "C") huge, but this would make our model more prone
    to overfitting. Perfect fits are not that good! Balancing the value of ![C](img/file234.png
    "C") is one of the many keys behind successful SVM training.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: When we train an SVM, the actual loss function that we would like to minimize
    is
  prefs: []
  type: TYPE_NORMAL
- en: '| ![L(\overset{\rightarrow}{w},b;\overset{\rightarrow}{x},y) = \max\{ 0,1 -
    y(\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b)\},](img/file1263.png
    "L(\overset{\rightarrow}{w},b;\overset{\rightarrow}{x},y) = \max\{ 0,1 - y(\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b)\},") |'
  prefs: []
  type: TYPE_TB
- en: which is called the **hinge loss**. In fact, our ![\xi_{j}](img/file1260.png
    "\xi_{j}") variables are direct representatives of that loss. Minimizing the expected
    value of this loss function would be connected to minimizing the proportion of
    misclassified elements — which is what we want at the end of the day.
  prefs: []
  type: TYPE_NORMAL
- en: If, in our formulation, we didn’t have the ![\left. \left\| w \right\|^{2}\slash
    2 \right.](img/file1264.png "\left. \left\| w \right\|^{2}\slash 2 \right.") factor,
    that would be the training loss that we would be minimizing. We included this
    factor, however, because a small ![\left\| w \right\|^{2}](img/file1254.png "\left\|
    w \right\|^{2}") (that is, a large margin) makes SVM models more robust against
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: We will conclude this analysis of soft-margin training by presenting an equivalent
    formulation of its optimization problem. This formulation is known as the **Lagrangian
    dual** of the optimization problem that we presented previously. We will not discuss
    why these two formulations are equivalent, but you can take our word for it —
    or you can check the wonderful explanation by Abu-Mostafa, Magdon-Ismail, and
    Lin [[2](ch030.xhtml#Xabu2012blearning)].
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'The soft-margin training problem can be equivalently written in terms of some
    optimizable parameters ![\alpha_{j}](img/file1265.png "\alpha_{j}") as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\text{Maximize}\quad} & {\sum\limits_{j}\alpha_{j} -
    \frac{1}{2}\sum\limits_{j,k}y_{j}y_{k}\alpha_{j}\alpha_{k}\left( {{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}} \right),\qquad} & & \qquad \\ {\text{subject~to}\quad}
    & {0 \leq \alpha_{j} \leq C,\qquad} & & \qquad \\ & {\sum\limits_{j}\alpha_{j}y_{j}
    = 0.\qquad} & & \qquad \\ \end{array}](img/file1266.png "\begin{array}{rlrl} {\text{Maximize}\quad}
    & {\sum\limits_{j}\alpha_{j} - \frac{1}{2}\sum\limits_{j,k}y_{j}y_{k}\alpha_{j}\alpha_{k}\left(
    {{\overset{\rightarrow}{x}}_{j} \cdot {\overset{\rightarrow}{x}}_{k}} \right),\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {0 \leq \alpha_{j} \leq C,\qquad} & &
    \qquad \\  & {\sum\limits_{j}\alpha_{j}y_{j} = 0.\qquad} & & \qquad \\ \end{array}")'
  prefs: []
  type: TYPE_IMG
- en: This formulation of the SVM soft-margin training problem is, most of the time,
    easier to solve in practice, and it is the one that we will be working with. Once
    we obtain the ![\alpha_{j}](img/file1265.png "\alpha_{j}") values, it is also
    possible to go back to the original formulation. In fact, from the ![\alpha_{j}](img/file1265.png
    "\alpha_{j}") values, we can recover ![b](img/file17.png "b") and ![w](img/file1267.png
    "w"). For instance, it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '![\overset{\rightarrow}{w} = \sum\limits_{j}\alpha_{j}y_{j}\overset{\rightarrow}{x_{j}}.](img/file1268.png
    "\overset{\rightarrow}{w} = \sum\limits_{j}\alpha_{j}y_{j}\overset{\rightarrow}{x_{j}}.")'
  prefs: []
  type: TYPE_IMG
- en: Notice that ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    only depends on the training points ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}"), for which ![\alpha_{j} \neq 0](img/file1270.png
    "\alpha_{j} \neq 0"). These vectors are called **support vectors** and, as you
    can imagine, are the reason behind the name of the SVM model.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we can also recover ![b](img/file17.png "b") by finding some ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}") that lies at the boundary of the margin and solving
    a simple equation — see [[2](ch030.xhtml#Xabu2012blearning)] for all the details.
    Then, in order to classify a point ![x](img/file269.png "x"), we can just compute
  prefs: []
  type: TYPE_NORMAL
- en: '![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \sum\limits_{j}\alpha_{j}y_{j}\left(
    {\overset{\rightarrow}{x_{j}} \cdot \overset{\rightarrow}{x}} \right) + b,](img/file1271.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \sum\limits_{j}\alpha_{j}y_{j}\left(
    {\overset{\rightarrow}{x_{j}} \cdot \overset{\rightarrow}{x}} \right) + b,")'
  prefs: []
  type: TYPE_IMG
- en: and decide whether ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    goes into the positive or negative class depending on whether the result is bigger
    than 0 or not.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now covered all we need to know about how to train a support vector machine.
    But, with our tools, we can only train these models to obtain linear separations
    between data, which is, well, not the most exciting of prospects. In the next
    section, we will overcome this limitation with a simple yet powerful trick.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.4 The kernel trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vanilla SVMs can only be trained to find linear separations between data elements.
    For example, the data shown in *Figure* [*9.5a*](#Figure9.5a) cannot be separated
    effectively by any SVM, because there is no way to separate it linearly.
  prefs: []
  type: TYPE_NORMAL
- en: How do we overcome this? Using **the kernel trick**. This technique consists
    in mapping the data from its original space ![R^{n}](img/file1212.png "R^{n}")
    to a higher dimensional space ![R^{N}](img/file1272.png "R^{N}"), all in the hope
    that, in that space, there may be a way to separate the data with a hyperplane.
    This higher dimensional space is known as a **feature space**, and we will refer
    to the function ![\left. \varphi:R^{n}\rightarrow R^{N} \right.](img/file1273.png
    "\left. \varphi:R^{n}\rightarrow R^{N} \right.") — which takes the original data
    inputs into the feature space — as a **feature** **map**.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the data in *Figure* [*9.5a*](#Figure9.5a) is in the ![1](img/file13.png
    "1")-dimensional real line, but we can map it to the ![2](img/file302.png "2")-dimensional
    plane with the function
  prefs: []
  type: TYPE_NORMAL
- en: '| ![f(x) = (x,x^{2}).](img/file1274.png "f(x) = (x,x^{2}).") |'
  prefs: []
  type: TYPE_TB
- en: As we can see in *Figure* [*9.5b*](#Figure9.5b), upon doing this, there is a
    hyperplane that perfectly separates the two categories in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![(a) Original data in the real line](img/file1275.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(a)** Original data in the real line'
  prefs: []
  type: TYPE_NORMAL
- en: '![(b) Data in the feature space](img/file1276.png)'
  prefs: []
  type: TYPE_IMG
- en: '**(b)** Data in the feature space'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 9.5**: The original data cannot be separated by a hyperplane, but
    — upon taking it to a higher-dimensional space with a feature map — it can. The
    separating hyperplane is represented by a dashed line'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the dual form of the soft-margin SVM optimization problem, we can
    see how, in order to train an SVM — and to later classify new data — on a certain
    feature space with a feature map ![\varphi](img/file1277.png "\varphi"), all we
    need to ”know” about the feature space is how to compute scalar products in it
    of elements returned by the feature map. This is because, during the whole training
    process, the only operation that depends on the ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}") points is the inner product ![{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}](img/file1278.png "{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}") — or the inner product ![\overset{\rightarrow}{x_{j}}
    \cdot \overset{\rightarrow}{x}](img/file1279.png "\overset{\rightarrow}{x_{j}}
    \cdot \overset{\rightarrow}{x}") when classifying a new point ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"). If instead of ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}") we had ![\varphi(\overset{\rightarrow}{x_{j}})](img/file1280.png
    "\varphi(\overset{\rightarrow}{x_{j}})"), we would just need to know how to compute
    ![\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi({\overset{\rightarrow}{x}}_{k})](img/file1281.png
    "\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi({\overset{\rightarrow}{x}}_{k})")
    — or ![\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi(\overset{\rightarrow}{x})](img/file1282.png
    "\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi(\overset{\rightarrow}{x})")
    to classify new data ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}").
  prefs: []
  type: TYPE_NORMAL
- en: That is, it suffices to be able to compute the function
  prefs: []
  type: TYPE_NORMAL
- en: '| ![k(x,y) = \varphi(\overset{\rightarrow}{x}) \cdot \varphi(\overset{\rightarrow}{y}),](img/file1283.png
    "k(x,y) = \varphi(\overset{\rightarrow}{x}) \cdot \varphi(\overset{\rightarrow}{y}),")
    |'
  prefs: []
  type: TYPE_TB
- en: and that is the single and only computation that we need to perform in the feature
    space. This is a crucial fact. This function is a particular case of what are
    known as **kernel functions**. Broadly speaking, kernel functions are functions
    that *can be* represented as inner products in some space. Mercer’s theorem (see
    [[2](ch030.xhtml#Xabu2012blearning)]) gives a nice characterization of them in
    terms of certain properties such as being symmetric and some other conditions.
    In the cases that we will consider, these conditions are always going to be met,
    so we don’t need to worry too much about them.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have a general understanding of how support vector machines are
    used in general, and in classical setups in particular. We now have all the necessary
    background to take the step to quantum. Get ready to explore quantum support vector
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Going quantum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have already mentioned, quantum support vector machines are particular
    cases of SVMs. To be more precise, they are particular cases of SVMs that rely
    on the kernel trick.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen in the previous section how, with the kernel trick, we take our
    data to a feature space: a higher dimensional space in which, we hope, our data
    will be separable by a hyperplane with the right choice of feature map. This feature
    space is usually just the ordinary Euclidean space but, well, with a higher dimension.
    But we can consider other choices. How about…the space of quantum states?'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 The general idea behind quantum support vector machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A QSVM works just like an ordinary SVM that relies on the kernel trick — with
    the only difference that it uses as feature space a certain space of quantum states.
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed before, whenever we use the kernel trick, all we need from the
    feature space is a kernel function. That’s the only ingredient involving the feature
    space that is necessary in order to be able to train a kernel-based SVM and make
    predictions with it. This idea inspired some works, such as the famous paper by
    Havlíček et al. [[52](ch030.xhtml#Xhavlivcek2019supervised)], to try to use quantum
    circuits to compute kernels and, hopefully, obtain some advantage over classical
    computers by working in a sophisticated feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking this into account, in order to train and then use a quantum support
    vector machine for classification, we will be able to do business as usual — doing
    everything fully classically — except for the computation of the kernel function.
    This function will have to rely on a quantum computer in order to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Take as input two vectors in the original space of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map each of them to a quantum state through a **feature map**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the inner product of the quantum states and return it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will discuss how to implement these (quantum) feature maps in the next subsection,
    but, in essence, they are just circuits that are parametrized exclusively by the
    original (classical) data and thus prepare a quantum state that depends only on
    that data. For now, we will just take these feature maps as a given.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s say that we have a feature map ![\varphi](img/file1277.png "\varphi").
    This will be implemented by a circuit ![\Phi](img/file1284.png "\Phi") that will
    depend on some classical data in the original space: for each input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"), we will have a circuit ![\Phi(\overset{\rightarrow}{x})](img/file1285.png
    "\Phi(\overset{\rightarrow}{x})") such that the output of the feature map will
    be the quantum state ![\varphi(\overset{\rightarrow}{x}) = \Phi(\overset{\rightarrow}{x})\left|
    0 \right\rangle](img/file1286.png "\varphi(\overset{\rightarrow}{x}) = \Phi(\overset{\rightarrow}{x})\left|
    0 \right\rangle"). With a feature map ready, we can then take our kernel function
    to be'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![k(\overset{\rightarrow}{a},\overset{\rightarrow}{b}) = \left&#124; \left\langle
    \varphi(a) \middle&#124; \varphi(b) \right\rangle \right&#124;^{2} = \left&#124;
    {\left\langle 0 \right&#124;\Phi^{\dagger}(a)\Phi(b)\left&#124; 0 \right\rangle}
    \right&#124;^{2}.](img/file1287.png "k(\overset{\rightarrow}{a},\overset{\rightarrow}{b})
    = \left&#124; \left\langle \varphi(a) \middle&#124; \varphi(b) \right\rangle \right&#124;^{2}
    = \left&#124; {\left\langle 0 \right&#124;\Phi^{\dagger}(a)\Phi(b)\left&#124;
    0 \right\rangle} \right&#124;^{2}.") |'
  prefs: []
  type: TYPE_TB
- en: And that is something that we can trivially get from a quantum computer! As
    you can easily check yourself, it is nothing more than the probability of measuring
    all zeros after preparing the state ![\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle](img/file1288.png "\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle"). This follows from the fact that the computational basis is
    orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: In case you were wondering how to compute the circuit for ![\Phi^{\dagger}](img/file1289.png
    "\Phi^{\dagger}"), notice that this is just the inverse of ![\Phi](img/file1284.png
    "\Phi"), because quantum circuits are always represented by unitary operations.
    But ![\Phi](img/file1284.png "\Phi") will be given by a series of quantum gates.
    So all you need to do is apply the gates in the circuit from right to left and
    invert each of them.
  prefs: []
  type: TYPE_NORMAL
- en: And that is how you implement a quantum kernel function. You take a feature
    map that will return a circuit ![\Phi(\overset{\rightarrow}{x})](img/file1285.png
    "\Phi(\overset{\rightarrow}{x})") for any input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"), you prepare the state ![\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle](img/file1288.png "\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle") for the pair of vectors on which you want to compute the kernel,
    and you return the probability of measuring zero on all the qubits.
  prefs: []
  type: TYPE_NORMAL
- en: In case you were concerned, by the way, all quantum kernels, as we have defined
    them, satisfy the conditions needed to qualify as kernel functions [[85](ch030.xhtml#Xschuld2021supervised)].
    In fact, we’ll now ask you to check one of those conditions!
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.2
  prefs: []
  type: TYPE_NORMAL
- en: One of the conditions for a function ![k](img/file317.png "k") to be a kernel
    is that it be symmetric. Prove that, indeed, any quantum kernel is symmetric.
    (![k(\overset{\rightarrow}{a},\overset{\rightarrow}{b}) = k(\overset{\rightarrow}{b},\overset{\rightarrow}{a})](img/file1290.png
    "k(\overset{\rightarrow}{a},\overset{\rightarrow}{b}) = k(\overset{\rightarrow}{b},\overset{\rightarrow}{a})")
    for any inputs.)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now study how to actually construct those feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Feature maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feature map, as we have said, is often defined by a parametrized circuit ![\Phi(\overset{\rightarrow}{x})](img/file1285.png
    "\Phi(\overset{\rightarrow}{x})") that depends on the original data and thus can
    be used to prepare a state that depends on it. In this section, we will study
    a few interesting feature maps that we will use throughout the rest of the book.
    They will also serve as examples that will allow us to better illustrate what
    feature maps actually are.
  prefs: []
  type: TYPE_NORMAL
- en: '**Angle encoding** We shall begin with a simple yet powerful feature map known
    as **angle encoding**. When used on an ![n](img/file244.png "n")-qubit circuit,
    this feature map can take up to ![n](img/file244.png "n") numerical inputs ![x_{1},\ldots,x_{n}](img/file1291.png
    "x_{1},\ldots,x_{n}"). The action of its circuit consists in the application of
    a rotation gate on each qubit ![j](img/file258.png "j") parametrized by the value
    ![x_{j}](img/file407.png "x_{j}"). In this feature map, we are using the ![x_{j}](img/file407.png
    "x_{j}") values as angles in the rotations, hence the name of the encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: In angle encoding, we are free to use any rotation gate of our choice. However,
    if we use ![R_{Z}](img/file120.png "R_{Z}") gates and take ![\left| 0 \right\rangle](img/file6.png
    "\left| 0 \right\rangle") to be our initial state…the action of our feature map
    will have no effects whatsoever, as you can easily check from the definition of
    ![R_{Z}](img/file120.png "R_{Z}"). That is why, when ![R_{Z}](img/file120.png
    "R_{Z}") gates are used, it is customary to precede them by Hadamard gates acting
    on each qubit. All this is shown in *Figure* [*9.6*](#Figure9.6).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6: Angle encoding for an input (x_{1},\ldots,x_{n}) using different
    rotation gates](img/file1293.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.6**: Angle encoding for an input ![(x_{1},\ldots,x_{n})](img/file1292.png
    "(x_{1},\ldots,x_{n})") using different rotation gates'
  prefs: []
  type: TYPE_NORMAL
- en: The variables that are fed to the angle encoding feature map should be normalized
    within a certain interval. If they are normalized between ![0](img/file12.png
    "0") and ![4\pi](img/file1294.png "4\pi"), then the data will be mapped to a wider
    region of the feature space than if they were normalized between ![0](img/file12.png
    "0") and ![1](img/file13.png "1"), for example. However, this would come at the
    cost of having the two extrema of the dataset identified under the action of the
    feature map. That’s because 0 and ![2\pi](img/file1295.png "2\pi") are exactly
    the same angle and, in our definition of rotation gates, we divided the input
    angle by ![2](img/file302.png "2").
  prefs: []
  type: TYPE_NORMAL
- en: The choice of normalization will thus be a trade-off between separating the
    extrema in the feature space and using the widest possible region in it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Amplitude encoding** Angle encoding can take ![n](img/file244.png "n") inputs
    on ![n](img/file244.png "n") qubits. Does that seem good enough? Well, get ready
    for a big jump. The **amplitude encoding** feature map can take ![2^{n}](img/file256.png
    "2^{n}") inputs when implemented on an ![n](img/file244.png "n")-qubit circuit.
    That is a lot, and it will enable us to effectively train QSVMs on datasets with
    a large number of variables. So, how does it work then?'
  prefs: []
  type: TYPE_NORMAL
- en: If the amplitude encoding feature map is given an input ![x_{0},\ldots,x_{2^{n}
    - 1}](img/file1296.png "x_{0},\ldots,x_{2^{n} - 1}"), it simply prepares the state
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\left&#124; {\varphi(\overset{\rightarrow}{a})} \right\rangle = \frac{1}{\sqrt{\sum\limits_{k}x_{k}^{2}}}\sum\limits_{k
    = 0}^{2^{n} - 1}x_{k}\left&#124; k \right\rangle.](img/file1297.png "\left&#124;
    {\varphi(\overset{\rightarrow}{a})} \right\rangle = \frac{1}{\sqrt{\sum\limits_{k}x_{k}^{2}}}\sum\limits_{k
    = 0}^{2^{n} - 1}x_{k}\left&#124; k \right\rangle.") |'
  prefs: []
  type: TYPE_TB
- en: Notice how we’ve had to include a normalization factor to make sure that the
    output was, indeed, a quantum state. Remember from *Chapter* [*1*](ch008.xhtml#x1-180001),
    *Foundations of* *Quantum Computing*, that all quantum states need to be normalized
    vectors! It’s easy to see from the definition that amplitude encoding can work
    for any input except for the zero vector — for the zero vector, amplitude encoding
    is undefined. We can’t divide by zero!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing this feature map in terms of elementary quantum gates is by no
    means simple. If you want all the gory details, you can check the book by Schuld
    and Petruccione [[106](ch030.xhtml#Xschuld)]. Luckily, it is built into most quantum
    computing frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the way, when using amplitude encoding, there is an unavoidable loss of
    information if you decide to ”push the feature map to its limit.” In general,
    you won’t be using all the ![2^{n}](img/file256.png "2^{n}") parameters that it
    offers — you will only use some of them and fill the rest with zeros or any other
    value of your choice. But, if you use all the ![2^{n}](img/file256.png "2^{n}")
    inputs to encode variables, there’s a small issue: that the number of degrees
    of freedom of an ![n](img/file244.png "n")-qubit state is actually ![2^{n} - 1](img/file1298.png
    "2^{n} - 1"), not ![2^{n}](img/file256.png "2^{n}"). This is, in any case, not
    a big deal. This loss of information can be ignored for sufficiently big values
    of ![n](img/file244.png "n").'
  prefs: []
  type: TYPE_NORMAL
- en: '**ZZ feature map** Lastly, we will present a known feature map that may bring
    you memories from *Chapter* [*5*](ch013.xhtml#x1-940005), *QAOA: Quantum Approximate
    Optimization* *Algorithm*, where we implemented circuits for Hamiltonians with
    ![Z_{j}Z_{k}](img/file363.png "Z_{j}Z_{k}") terms. It’s called the **ZZ feature
    map**. It is implemented by Qiskit and it can take ![n](img/file244.png "n") inputs
    ![a_{1},\ldots,a_{n}](img/file1299.png "a_{1},\ldots,a_{n}") on ![n](img/file244.png
    "n") qubits, just like angle embedding. Its parametrized circuit is constructed
    following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply a Hadamard gate on each qubit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply, on each qubit ![j](img/file258.png "j"), a rotation ![R_{Z}(2x_{j})](img/file1300.png
    "R_{Z}(2x_{j})").
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each pair of elements ![\{ j,k\} \subseteq \{ 1,\ldots,n\}](img/file1301.png
    "\{ j,k\} \subseteq \{ 1,\ldots,n\}") with ![j < k](img/file1302.png "j < k"),
    do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a CNOT gate targeting qubit ![k](img/file317.png "k") and controlled by
    qubit ![j](img/file258.png "j").
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply, on qubit ![k](img/file317.png "k"), a rotation ![R_{Z}\left( {2(\pi -
    x_{j})(\pi - x_{k})} \right)](img/file1303.png "R_{Z}\left( {2(\pi - x_{j})(\pi
    - x_{k})} \right)").
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step [3a](#x1-171010x1).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Figure* [*9.7*](#Figure9.7) you can find a representation of the ZZ feature
    map on three qubits.
  prefs: []
  type: TYPE_NORMAL
- en: As with angle encoding, normalization plays a big role in the ZZ feature map.
    In order to guarantee a healthy balance between separating the extrema of the
    dataset and using as big a region a possible in the feature space, the variables
    could be normalized to ![\lbrack 0,1\rbrack](img/file1145.png "\lbrack 0,1\rbrack")
    or ![\lbrack 0,3\rbrack](img/file1304.png "\lbrack 0,3\rbrack"), for example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7: ZZ feature map on three qubits with inputs x_{1},x_{2},x_{3}](img/file1306.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.7**: ZZ feature map on three qubits with inputs ![x_{1},x_{2},x_{3}](img/file1305.png
    "x_{1},x_{2},x_{3}")'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, when designing a quantum feature map, your imagination is the only
    limit. The ones that we have presented here are some of the most popular ones
    — and the ones that you will find in frameworks such as PennyLane and Qiskit —
    but research on quantum feature maps and their properties is an active area. If
    you want to take a look at other possibilities, we can recommend the paper by
    Sim, Johnson, and Aspuru-Guzik [[88](ch030.xhtml#Xsim2019expressibility)].
  prefs: []
  type: TYPE_NORMAL
- en: But enough theory for now! Let’s put into practice all that we have learned
    by implementing some QSVMs with both PennyLane and Qiskit.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Quantum support vector machines in PennyLane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It has been a long journey but, finally, we are ready to see QSVMs in action.
    In this section, we are going to train and run a bunch of QSVM models using PennyLane.
    Just to get started, let’s import NumPy and set a seed so that our results are
    reproducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 9.3.1 Setting the scene for training a QSVM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, if we want to train QSVMs, we need some data to work with. In today’s ever-changing
    job market, you should always keep your options open and, as promising as quantum
    machine learning may be, you may want to have a backup career plan. Well, we’ve
    got you covered. Have you ever dreamed of becoming a world-class sommelier? Today
    is your lucky day! (We are just kidding, of course, but we will use this wine
    theme to give some flavor to our example!)
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already seen how the scikit-learn package offers lots of tools and resources
    for machine learning. It turns out that among them are a collection of pre-defined
    datasets on which to train ML models, and one of those datasets is a ”wine recognition
    dataset” [[32](ch030.xhtml#XDua:2019)]. This is a labeled dataset with information
    about wines. In total, it has ![13](img/file1307.png "13") numeric variables that
    describe the color intensity, alcohol concentration, and other fancy things whose
    meaning and significance we have no clue about. The labels correspond to the kind
    of wine. There are three possible labels, so, if we just ignore one, we are left
    with a beautiful dataset for a binary classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load the set with the `load_wine` function in `sklearn``.``datasets`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We have set `return_X_y` to true so that we also get the labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all the details about this dataset in its online documentation
    ([https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset)
    or [https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine),
    if you want to check the original source of the data). According to it, the ![59](img/file1308.png
    "59") first elements in the dataset must belong to the first category (label ![0](img/file12.png
    "0")) while the ![71](img/file1309.png "71") subsequent ones have to belong to
    the second one (label ![1](img/file13.png "1")). Thus, if we want to ignore the
    third category, we can just run the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And thus we have a labeled dataset with two categories. A perfect binary classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed, however, a few disclaimers are in order. This wine recognition
    problem that we are going to work with is — from a machine learning point of view
    — very simple. You don’t need very sophisticated models or a lot of computing
    power to tackle it. Thus, using a QSVM for this problem is overkill. It will work,
    yes, but that doesn’t diminish the fact that we will be overdoing it. Quantum
    support vector machines can tackle complex problems, but we thought it would be
    better to keep things simple. You may call us overprotective, but we thought that
    using examples that could take two hours to run — or even two days! — might not
    be exactly ideal from a pedagogical perspective. We will also see how some examples
    yield better results than others. Unless we state otherwise, that won’t be indicative
    of any general pattern. It will just mean that it so happens, some things work
    better than others for this particular problem. After all, from the few experiments
    that we will run, it wouldn’t be sensible to draw hard conclusions!
  prefs: []
  type: TYPE_NORMAL
- en: 'With those remarks out of the way, let’s attack our problem. We shall begin
    by splitting our dataset into a training dataset and a test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We won’t be making direct model comparisons, nor will we be using validation
    losses, so we will not use a validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed previously, most feature maps expect our data to be normalized,
    and, regardless of that, normalizing your data is in general a good practice in
    machine learning. So that’s what we shall do now! We will actually use the most
    simple of normalization techniques: scaling each of the variables linearly in
    such a way that the maximum absolute value taken by each variable be ![1](img/file13.png
    "1"). This can be achieved with a `MaxAbsScaler` object from `sklearn``.``preprocessing`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: And, with that, we know that — since all our variables were positive — all the
    values in our training dataset will be between ![0](img/file12.png "0") and ![1](img/file13.png
    "1"). If there were negative values, our scaled variables would take values in
    ![\lbrack - 1,1\rbrack](img/file1310.png "\lbrack - 1,1\rbrack") instead. Notice
    that we have only normalized our training dataset. Normalizing the whole dataset
    *simultaneously* would be, in a way, cheating, because we could be polluting the
    training dataset with information from the test dataset. For instance, if we had
    an outlier in the test dataset with a very high value in some variable — a value
    never reached in the training dataset — this would be reflected in the normalization,
    and, thus, the independence of our test dataset could be compromised.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the training dataset is normalized, we need to normalize the test
    dataset using the same proportions as the training dataset. In this way, the training
    dataset receives no information about the test dataset. This can be achieved with
    the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we have used the same `scaler` object as before, but we have called
    the `transform` method instead of `fit_transform`. In that way, the scaler uses
    the proportions that it saved before. In addition, we’ve run an instruction to
    ”cut” the values in the test dataset at ![0](img/file12.png "0") and ![1](img/file13.png
    "1") — just in case there were some outliers and in order to comply with the normalization
    requirements of some of the feature maps that we will use.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 PennyLane and scikit-learn go on their first date
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve said it countless times: QSVMs are like normal SVMs, but with a quantum
    kernel. So let’s implement that kernel with PennyLane.'
  prefs: []
  type: TYPE_NORMAL
- en: Our dataset has ![13](img/file1307.png "13") variables. Using angle encoding
    or the ZZ feature map on the ![13](img/file1307.png "13") variables would require
    us to use ![13](img/file1307.png "13") qubits, which might not be feasible if
    we want our kernel to be simulated on some not especially powerful computers.
    Thus, we can resort to amplitude encoding using ![4](img/file143.png "4") qubits.
    As we mentioned before, this feature map can accept up to ![16](img/file619.png
    "16") inputs; we will fill the remaining ones with zeros — PennyLane will make
    that easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how we can implement our quantum kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, there are a few things to digest here. We are first importing PennyLane,
    setting the number of qubits in a variable, and defining a device; nothing new
    there. And then comes the definition of the circuit of our kernel. In this definition,
    we are using `AmplitudeEmbedding`, which returns an operation equivalent to the
    amplitude encoding of its first argument. In our case, we use the arrays `a` and
    `b` for this first argument. They are the classical data that our kernel function
    takes as input. In addition to this, we also ask `AmplitudeEmbedding` to normalize
    each input vector for us, just as amplitude encoding needs us to do, and, since
    our arrays have ![13](img/file1307.png "13") elements instead of the required
    ![16](img/file619.png "16"), we set `pad_with` `=` `0` to fill the remaining values
    with zeros. Also notice that we are using `qml``.``adjoint` to compute the adjoint
    (or inverse) of the amplitude encoding of `b`.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we retrieve an array with the probabilities of measuring each possible
    state in the computational basis. The first element of this array (that is, the
    probability of getting a zero value in all the qubits) will be the output of our
    kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have our quantum kernel almost ready. If you’d like to check that the
    circuit works as expected, you can try it out on some elements from the training
    dataset. For instance, you could run `kernel_circ``(``x_tr``[0],` `x_tr``[1])`.
    If the two arguments are the same, keep in mind that you should always get ![1](img/file13.png
    "1") in the first entry of the returned array (which corresponds, as we have mentioned,
    to the output of the kernel).
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.3
  prefs: []
  type: TYPE_NORMAL
- en: Prove that, indeed, any quantum kernel evaluated on two identical entries always
    needs to return the output ![1](img/file13.png "1").
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next step will be using this quantum kernel in an SVM. Our good old scikit-learn
    has its own implementation, `SVC`, of support vector machines, and it works with
    custom kernels, so there we have it! In order to use a custom kernel, you are
    required to provide a `kernel` function accepting two arrays, `A` and `B`, and
    returning a matrix with entries ![(j,k)](img/file356.png "(j,k)") containing the
    kernel applied to `A``[``j``]` and `B``[``k``]`. Once the kernel is prepared,
    the SVM can be trained with the `fit` method. All of this is done in the following
    piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The training can take up to a few minutes depending on the performance of your
    computer. Once it is over, you can check the accuracy of your trained model with
    the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In our case, this gives an accuracy of ![0.92](img/file1311.png "0.92"), meaning
    that the SVM is capable of classifying most of the elements in the test dataset
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: This shows us how to train and run a quantum support vector in a fairly simple
    manner. But we can consider more sophisticated scenarios. Are you ready for that?
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3 Reducing the dimensionality of a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just seen how to use amplitude encoding to take full advantage of the
    ![13](img/file1307.png "13") variables of our dataset while only using ![4](img/file143.png
    "4") qubits. In most cases, that is a good approach. But there are also some problems
    in which it may prove better to reduce the number of variables in the dataset
    — while trying to minimize the loss of information, of course — and thus be able
    to use other feature maps that could perhaps yield better results.
  prefs: []
  type: TYPE_NORMAL
- en: In this subsection, we are going to illustrate this approach. We shall try to
    reduce the number of variables in our dataset to ![8](img/file506.png "8") and,
    then, we will train a QSVM on the new, reduced variables using angle encoding.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to reduce the dimensionality of a dataset while minimizing information
    loss, as we aim to do now, there are many options at your disposal. You may want
    to have a look at autoencoders, for instance. In any case, for the purposes of
    this section, we will consider a technique known as **principal** **component
    analysis**.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: Before actually using principal component analysis, you may reasonably be curious
    about how this fancy-sounding technique works.
  prefs: []
  type: TYPE_NORMAL
- en: When you have a dataset with ![n](img/file244.png "n") variables, you essentially
    have a set of points in ![R^{n}](img/file1212.png "R^{n}"). With this set, you
    may consider what are known as the **principal** **directions**. The first principal
    direction is the direction of the line that best fits the data as measured by
    the mean squared error. The second principal direction is the direction of the
    line that best fits the data while being orthogonal to the first principal direction.
    This goes on in such a way that the ![k](img/file317.png "k")-th principal direction
    is that of the line that best fits the data while being orthogonal to the first,
    second, and all the way up to the (![k - 1](img/file1312.png "k - 1"))-th principal
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: We thus may consider an orthonormal basis ![\{ v_{1},\ldots,v_{n}\}](img/file1313.png
    "\{ v_{1},\ldots,v_{n}\}") of ![R^{n}](img/file1212.png "R^{n}") in which ![v_{j}](img/file1314.png
    "v_{j}") points in the direction of the ![j](img/file258.png "j")-th principal
    component. The vectors in this orthonormal basis will be of the form ![v_{j} =
    (v_{j}^{1},\ldots,v_{j}^{n}) \in R^{n}](img/file1315.png "v_{j} = (v_{j}^{1},\ldots,v_{j}^{n})
    \in R^{n}"). Of course, the superscripts are not exponents! They are just superscripts.
  prefs: []
  type: TYPE_NORMAL
- en: When using principal component analysis, we simply compute the vectors of the
    aforementioned basis. And, then, we define the variables
  prefs: []
  type: TYPE_NORMAL
- en: '| ![{\overset{\sim}{x}}_{j} = v_{j}^{1}x_{1} + \cdots + v_{j}^{n}x_{n}.](img/file1316.png
    "{\overset{\sim}{x}}_{j} = v_{j}^{1}x_{1} + \cdots + v_{j}^{n}x_{n}.") |'
  prefs: []
  type: TYPE_TB
- en: And, lastly, in order to reduce the dimensionality of our dataset to ![m](img/file259.png
    "m") variables, we just keep the variables ![{\overset{\sim}{x}}_{1},\ldots,{\overset{\sim}{x}}_{m}](img/file1317.png
    "{\overset{\sim}{x}}_{1},\ldots,{\overset{\sim}{x}}_{m}"). This is all done under
    the assumption that the variables ![{\overset{\sim}{x}}_{j}](img/file1318.png
    "{\overset{\sim}{x}}_{j}") are, as we have defined them, sorted in decreasing
    order of relevance towards our problem.
  prefs: []
  type: TYPE_NORMAL
- en: So how do we use principal component analysis to reduce the number of variables
    in our dataset? Well, scikit-learn is here to save the day. It implements a `PCA`
    class that works in an analogous way to that of the `MaxAbsScaler` class that
    we used before.
  prefs: []
  type: TYPE_NORMAL
- en: 'This `PCA` class comes with a `fit` method that analyzes the data and figures
    out the best way to reduce its dimensionality using principal component analysis.
    Then, in addition, it comes with a `transform` method that can then transform
    any data in the way it learned to do when `fit` was invoked. Also, just like `MaxAbsScaler`,
    the `PCA` class has a `fit_transform` method that fits the data and transforms
    it simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: And, with this, we have effectively reduced the number of variables in our dataset
    to ![8](img/file506.png "8"). Notice, by the way, how we have used the `fit_transform`
    method on the training data and the `transform` method on the test data, all in
    order to preserve the independence of the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to implement and train a QSVM using angle encoding. For this,
    we may use the `AngleEmbedding` operator provided by PennyLane. The following
    piece of code defines the training; it is very similar to our previous kernel
    definition and, thus, pretty self-explanatory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have a kernel, we can train a QSVM as we did before, this time reusing
    the `qkernel` function, which will be using the new `kernel_circ` definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The returned accuracy on the test dataset is ![1](img/file13.png "1"). Just
    a perfect classification in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.4 Implementing and using custom feature maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PennyLane comes with a wide selection of built-in feature maps; you can find
    them all in the online documentation ([https://pennylane.readthedocs.io/en/stable/introduction/templates.html](https://pennylane.readthedocs.io/en/stable/introduction/templates.html)).
    Nevertheless, you may want to define your own. In this subsection, we will train
    a QSVM on the reduced dataset using our own implementation of the ZZ feature map.
    Let’s take feature maps into our own hands!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can begin by implementing the feature map as a function with the following
    piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this implementation, we have used the `combinations` function from the `itertools`
    module. It takes two arguments: an array `arr` and an integer `l`. And it returns
    an array with all the sorted tuples of length `l` with elements from the array
    `arr`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how we have written the `ZZFeatureMap` function as we would write any
    circuit, taking advantage of all the flexibility that PennyLane gives us. Having
    defined this function for the ZZ feature map, we may use it on a kernel function
    and then train a QSVM just as we have done before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the test accuracy is ![0.77](img/file1319.png "0.77").
  prefs: []
  type: TYPE_NORMAL
- en: There’s one detail to which you should pay attention here, and it is the fact
    that `qml``.``adjoint` is acting on the `ZZFeatureMap` function itself, not on
    its output! Remember that taking the adjoint of a circuit is the same as considering
    the inverse of that circuit.
  prefs: []
  type: TYPE_NORMAL
- en: That’s all we had in store about QSVMs on PennyLane. Now it’s time for us to
    see how things are done in Qiskit Land.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Quantum support vector machines in Qiskit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we mastered the use of QSVMs in PennyLane. You may
    want to review *subsection* [*9.3.1*](#x1-1730009.3.1) and the beginning of *subsection*
    [*9.3.3*](#x1-1750009.3.3). That is where we prepare the dataset that we will
    be using here too. In addition to running the code in those subsections, you will
    have to do the following import:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now it’s time for us to switch to Qiskit. In some ways, Qiskit can be easier
    to use than PennyLane — although this is probably a matter of taste. In addition,
    Qiskit will enable us to directly train and run our QSVM models using the real
    quantum computers available at IBM Quantum. Nevertheless, for now, let us begin
    with QSVMs on our beloved Qiskit Aer simulator.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.1 QSVMs on Qiskit Aer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, let us just import Qiskit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: When we defined a QSVM in PennyLane, we had to ”manually” implement a kernel
    function in order to pass it to scikit-learn. This process is simplified in Qiskit,
    for all it takes to define a quantum kernel is to instantiate a `QuantumKernel`
    object. In the initializer, we are asked to provide a `backend` argument, which
    will be, of course, the backend object on which the quantum kernel will run. By
    default, the feature map that the quantum kernel will use is the ZZ feature map
    with two qubits, but we can use a different feature map by passing a value to
    the `feature_map` object. This value should be a parametrized circuit representing
    the feature map.
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining parametrized circuits in Qiskit is actually fairly easy. If you want
    to use an individual parameter in a circuit, you can just import `Parameter` from
    `qiskit``.``circuit` and define a parameter object as `Parameter``(``"``label``"``)`
    with any label of your choice. This object can then be used in quantum circuits.
    For example, we may define a circuit with an ![x](img/file269.png "x")-rotation
    parametrized by a value ![x](img/file269.png "x") as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to use an array of parameters in a circuit, you may define a `ParameterVector`
    object instead. It can also be imported from `qiskit``.``circuit` and, in addition
    to the mandatory label, it accepts an optional `length` argument setting the length
    of the array. By default, this length is set to zero. We may use these parameter
    vector objects as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Exercise 9.4
  prefs: []
  type: TYPE_NORMAL
- en: Define an `AngleEncodingX``(``n` `)` function that return the feature map for
    angle encoding using ![R_{X}](img/file118.png "R_{X}") rotations on `n` qubits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using parametrized circuits, we may define any feature map of our choice for
    its use in a quantum kernel; for instance, we could just send any of the `qc`
    objects that we have created in the previous pieces of code as the `feature_map`
    parameter in the `QuantumKernel` constructor. Nevertheless, Qiskit already comes
    with some pre-defined feature maps out of the box. For our case, we may generate
    a circuit for the ZZ feature map on eight qubits using the following piece of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As a matter of fact, this feature map can be further customized by providing
    additional arguments. We shall use them in the following chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our feature map, we can trivially set up a quantum kernel reliant
    on the Aer simulator as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: And that’s all it takes! By the way, here we are using the Qiskit Machine Learning
    package. Please, refer to *Appendix* [*D*](ch027.xhtml#x1-240000D)*, Installing
    the Tools*, for installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: If we’d like to train a QSVM model using our freshly-created kernel, we can
    use Qiskit’s own extension of the SVC class provided by scikit-learn. It’s called
    `QSVC` and it can be imported from `quantum_machine_learning``.``algorithms`.
    It works just like the original `SVC` class, but it accepts a `quantum_kernel`
    argument to which we can pass `QuantumKernel` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, these are the instructions that we have to run in order to train a QSVM
    with our kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As with PennyLane, this will take a few minutes to run. Notice, by the way,
    that we have used the reduced dataset (`xs_tr`), because we are using the ZZ feature
    map on 8 qubits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the training is complete, we can get the accuracy on the test dataset
    as we have always done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the returned accuracy was ![1](img/file13.png "1").
  prefs: []
  type: TYPE_NORMAL
- en: That is all you need to know about how to run QSVMs on the Aer simulator. Now,
    let’s get real.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.2 QSVMs on IBM quantum computers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training and using QSVMs on real hardware with Qiskit couldn’t be easier. We
    will show how it can be done in this subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, as we did back in *Chapter* [*2*](ch009.xhtml#x1-400002), *The Tools
    of the Trade in Quantum* *Computing*, we will load our IBM Quantum account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Naturally, for this to work, you should have saved your access token beforehand.
    At the time of writing, free accounts don’t have access to any real quantum devices
    with eight qubits, but there are some with seven qubits. We can select the one
    that is the least busy with the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, we will have to further reduce our data to seven variables, but
    we can do that very easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: And, with this, we have all the ingredients ready to train a QSVM on real hardware!
    We will have to follow the same steps as before — only this time using our real
    device as `quantum_instance` in the instantiation of our quantum kernel!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: When you execute this code, all the circuit parameters are known in advance.
    For this reason, Qiskit will try to send as many circuits as possible at the same
    time. However, these jobs still have to wait in the queue. Depending on the number
    of points in your dataset and on your access privileges, this may take quite a
    long time to complete!
  prefs: []
  type: TYPE_NORMAL
- en: With this, we can bring our study of QSVMs in Qiskit to an end.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first learned what support vector machines are, and how
    they can be trained to solve binary classification problems. We began by considering
    vanilla vector machines, and then we introduced the kernel trick — which opened
    up a world of possibilities! In particular, we saw how QSVMs are nothing more
    than a support vector machine with a quantum kernel.
  prefs: []
  type: TYPE_NORMAL
- en: From there on, we learned how quantum kernels actually work and how to implement
    them. We explored the essential role of feature maps, and discussed a few of the
    most well-known ones.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned how to implement, train, and use quantum support vector
    machines with PennyLane and Qiskit. In addition, we were able to very easily run
    QSVMs on real hardware thanks to Qiskit’s interface to IBM Quantum.
  prefs: []
  type: TYPE_NORMAL
- en: And that pretty much covers how QSVMs can help you can identify wines — or solve
    any other classification task — like an expert, all while happily ignoring what
    the ”alkalinity of ash” of a wine is. Who knows? Maybe these SVM models could
    open the door for you to enjoy a bohemian life of wine-tasting! No need to thank
    us.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will consider another family of quantum machine learning
    models: that of quantum neural networks. Things are about to get deep!*'
  prefs: []
  type: TYPE_NORMAL
