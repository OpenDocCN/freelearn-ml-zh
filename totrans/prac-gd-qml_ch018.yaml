- en: Chapter 9
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章
- en: Quantum Support Vector Machines
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 量子支持向量机
- en: '*Artificial Intelligence is the new electricity*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能是新的电力*'
- en: — Andrew Ng
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: — 安德鲁·吴
- en: 'In the previous chapter, we learned the basics of machine learning and we got
    a sneak peek into quantum machine learning. It is now time for us to work with
    our first family of quantum machine learning models: that of **Quantum** **Support
    Vector Machines** (often abbreviated as **QSVM**s). These are very popular models,
    and they are most naturally used in binary classification problems.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了机器学习的基础，并提前了解了量子机器学习。现在是时候开始处理我们的第一个量子机器学习模型家族了：**量子****支持向量机**（通常缩写为**QSVM**）。这些是非常受欢迎的模型，它们最自然地用于二元分类问题。
- en: In this chapter, we shall learn what (classical) support vector machines are
    and how they are used, and we will use this knowledge as a foundation to understand
    quantum support vector machines. In addition, we will explore how to implement
    and train quantum support vector machines with Qiskit and PennyLane.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习什么是（经典）支持向量机以及它们是如何被使用的，并将这些知识作为理解量子支持向量机的基础。此外，我们将探讨如何使用Qiskit和PennyLane实现和训练量子支持向量机。
- en: 'The contents of this chapter are the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的内容如下：
- en: Support vector machines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Going quantum
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 走向量子
- en: Quantum support vector machines in PennyLane
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PennyLane中的量子支持向量机
- en: Quantum support vector machines in Qiskit
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiskit中的量子支持向量机
- en: 9.1 Support vector machines
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.1 支持向量机
- en: 'QSVMs are actually particular cases of **Support Vector Machines** (abbreviated
    as **SVM**s). In this section, we will explore how these SVMs work and how they’re
    used in machine learning. We will do so by first motivating the SVM formalism
    with some simple examples, and then building up from there: all the way up into
    how SVMs can be used to tackle complex classification problems with the kernel
    trick.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: QSVMs实际上是**支持向量机**（简称**SVM**）的特殊情况。在本节中，我们将探讨这些SVM是如何工作的，以及它们在机器学习中的应用。我们将首先通过一些简单的例子来激发SVM公式的动机，然后逐步深入：一直到如何使用核技巧来解决复杂分类问题。
- en: 9.1.1 The simplest classifier you could think of
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1.1 你能想到的最简单的分类器
- en: Let us forget about data for a moment and begin by considering a very naive
    problem. Let’s say that we want to build a very simple classifier on the real
    line. In order to do this, all we have to do is split the real number line into
    two disjoint categories in such a way that any number belong to exactly one of
    these two categories. Thus, if we are given any input (a real number), our classifier
    will return the category to which it belongs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时忘记数据，先考虑一个非常简单的问题。假设我们想在实数线上构建一个非常简单的分类器。为了做到这一点，我们只需要将实数线分成两个不相交的类别，使得任何数都属于这两个类别之一。因此，如果我们给出任何输入（一个实数），我们的分类器将返回它所属的类别。
- en: 'What would be the easiest way in which you could do this? Odds are you would
    first pick a point ![a](img/file16.png "a") and divide the real number line into
    the set (category) of numbers smaller than ![a](img/file16.png "a") and the set
    of numbers larger than ![a](img/file16.png "a"). Then, of course, you would have
    to assign ![a](img/file16.png "a") to one of the two categories, so your categories
    would be either of the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为最简单的方法是什么？很可能是你首先选择一个点![a](img/file16.png "a")，并将实数线分成小于![a](img/file16.png
    "a")的数（类别）和大于![a](img/file16.png "a")的数的集合。当然，你还得将![a](img/file16.png "a")分配给两个类别中的一个，所以你的类别可以是以下两种之一：
- en: The set of real numbers ![x](img/file269.png "x") such that ![x \leq a](img/file1190.png
    "x \leq a") and the set of numbers ![x](img/file269.png "x") such that ![x > a](img/file1191.png
    "x > a")
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实数集![x](img/file269.png "x")，满足![x \leq a](img/file1190.png "x \leq a")，以及实数集![x](img/file269.png
    "x")，满足![x > a](img/file1191.png "x > a")
- en: The set of numbers ![x](img/file269.png "x") such that ![x < a](img/file1192.png
    "x < a") and the set of numbers ![x](img/file269.png "x") such that ![x \geq a](img/file1193.png
    "x \geq a")
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实数集![x](img/file269.png "x")，满足![x < a](img/file1192.png "x < a")，以及实数集![x](img/file269.png
    "x")，满足![x \geq a](img/file1193.png "x \geq a")
- en: Either choice would be reasonable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 任何选择都是合理的。
- en: To learn more…
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多…
- en: Actually, the choice as to in which category to include ![a](img/file16.png
    "a") is, to some extent, meaningless. At the end of the day, if you choose a real
    number at random, the probability that it be exactly ![a](img/file16.png "a")
    is zero. This fun fact is sponsored by probability and measure theory!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，关于将 ![a](img/file16.png "a") 包括在哪个类别中的选择，在某种程度上是没有意义的。最终，如果你随机选择一个实数，它正好是
    ![a](img/file16.png "a") 的概率是零。这个有趣的事实是由概率和测度理论赞助的！
- en: That was easy. Let’s now say that we want to do the same with the real plane
    (the usual ![R^{2}](img/file1194.png "R^{2}")). In this case, a single point will
    not suffice to split it, but we could instead consider a good old line! This is
    exemplified in *Figure* [*9.1*](#Figure9.1). Any line can be used to perfectly
    split the real plane into two categories.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单。现在让我们说，我们想在实平面上（通常的 ![R^{2}](img/file1194.png "R^{2}")）做同样的事情。在这种情况下，一个点不足以将其分割，但我们可以考虑一条古老的直线！这可以在
    *图* [*9.1*](#Figure9.1) 中看到。任何一条直线都可以完美地将实平面分成两类。
- en: '![Figure 9.1: The line \left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right., which can be equivalently written as \left. y = (5\slash 4)x
    \right., can be used to divide the real plane into two disjoint categories, which
    are colored differently. The picture does not specify to which category the line
    belongs ](img/file1197.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1：直线 \left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x} + 0 = 0 \right.，可以等价地写成
    \left. y = (5\slash 4)x \right.，可以用来将实平面分成两个不相交的类别，这些类别被涂成不同的颜色。图片没有指定这条线属于哪个类别
    ](img/file1197.png)'
- en: '**Figure 9.1**: The line ![\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right.](img/file1195.png "\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right."), which can be equivalently written as ![\left. y = (5\slash
    4)x \right.](img/file1196.png "\left. y = (5\slash 4)x \right."), can be used
    to divide the real plane into two disjoint categories, which are colored differently.
    The picture does not specify to which category the line belongs'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.1**：直线 ![\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x} + 0 =
    0 \right.](img/file1195.png "\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right.")，可以等价地写成 ![\left. y = (5\slash 4)x \right.](img/file1196.png
    "\left. y = (5\slash 4)x \right.")，可以用来将实平面分成两个不相交的类别，这些类别被涂成不同的颜色。图片没有指定这条线属于哪个类别'
- en: If you go back to your linear algebra notes, you may recall that any line in
    the plane can be characterized in terms of a vector ![\overset{\rightarrow}{w}
    \in R^{2}](img/file1198.png "\overset{\rightarrow}{w} \in R^{2}") and a scalar
    ![b \in R](img/file1199.png "b \in R") as the set of points ![\overset{\rightarrow}{x}
    = (x,y)](img/file1200.png "\overset{\rightarrow}{x} = (x,y)") such that ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0"). Of course, we are using ![\cdot](img/file1202.png
    "\cdot") to denote the scalar product (that is, ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{x} = w_{1}x + w_{2}y](img/file1203.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} = w_{1}x + w_{2}y"), provided that ![\overset{\rightarrow}{w}
    = (w_{1},w_{2})](img/file1204.png "\overset{\rightarrow}{w} = (w_{1},w_{2})")).
    The vector ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    defines the **normal**, or perpendicular, direction to the line, and the constant
    ![b](img/file17.png "b") determines the intersection of the line with the ![X](img/file9.png
    "X") and ![Y](img/file11.png "Y") axes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回顾一下你的线性代数笔记，你可能还记得，在平面上任何一条直线都可以用向量 ![\overset{\rightarrow}{w} \in R^{2}](img/file1198.png
    "\overset{\rightarrow}{w} \in R^{2}") 和标量 ![b \in R](img/file1199.png "b \in R")
    来描述，即点集 ![\overset{\rightarrow}{x} = (x,y)](img/file1200.png "\overset{\rightarrow}{x}
    = (x,y)") 满足 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0")。当然，我们使用 ![\cdot](img/file1202.png
    "\cdot") 表示标量积（即 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} = w_{1}x
    + w_{2}y](img/file1203.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    = w_{1}x + w_{2}y")，前提是 ![overset{\rightarrow}{w} = (w_{1},w_{2})](img/file1204.png
    "\overset{\rightarrow}{w} = (w_{1},w_{2})")")). 向量 ![overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}") 定义了直线的**法线**或垂直方向，而常数 ![b](img/file17.png "b") 决定了直线与
    ![X](img/file9.png "X") 和 ![Y](img/file11.png "Y") 轴的交点。
- en: When we worked on the one-dimensional case and used a point to split the real
    line, it was trivial to decide which category any input belonged to. In this case,
    it is slightly more complicated, but not too much. With some elementary geometry,
    you can check that any number ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    will be on one side or the other of the line defined by ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0") depending on the sign of the quantity
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b"). That is, if ![\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b](img/file1208.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b") and ![\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{2} + b](img/file1209.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{2} + b") have the same sign (both smaller than
    zero or both greater than zero), we will know that ![{\overset{\rightarrow}{x}}_{1}](img/file1210.png
    "{\overset{\rightarrow}{x}}_{1}") and ![{\overset{\rightarrow}{x}}_{2}](img/file1211.png
    "{\overset{\rightarrow}{x}}_{2}") will belong to the same category. Otherwise,
    we know they will not.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理一维情况并使用一个点来分割实线时，决定任何输入属于哪个类别是显而易见的。在这种情况下，稍微复杂一些，但并不太多。通过一些基本的几何知识，你可以检查出任何数
    ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}") 将位于由
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0") 定义的线的两侧之一，这取决于
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b") 的符号。也就是说，如果 ![\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b](img/file1208.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b") 和 ![\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{2} + b](img/file1209.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{2} + b") 有相同的符号（两者都小于零或两者都大于零），我们将知道 ![{\overset{\rightarrow}{x}}_{1}](img/file1210.png
    "{\overset{\rightarrow}{x}}_{1}") 和 ![{\overset{\rightarrow}{x}}_{2}](img/file1211.png
    "{\overset{\rightarrow}{x}}_{2}") 将属于同一类别。否则，我们知道它们不会属于同一类别。
- en: There is no reason for us to stop at two dimensions, so let’s kick this up a
    notch and consider an ![n](img/file244.png "n")-dimensional Euclidean space ![R^{n}](img/file1212.png
    "R^{n}"). Just as we split ![R^{2}](img/file1194.png "R^{2}") using a line, we
    could split ![R^{n}](img/file1212.png "R^{n}") using…an (![n - 1](img/file1213.png
    "n - 1"))-dimensional hyperplane! For instance, we could split ![R^{3}](img/file1214.png
    "R^{3}") using an ordinary plane.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理由我们只停留在二维，让我们再提高一个层次，考虑一个 ![n](img/file244.png "n") 维欧几里得空间 ![R^{n}](img/file1212.png
    "R^{n}"). 就像我们用一条线分割 ![R^{2}](img/file1194.png "R^{2}") 一样，我们也可以用…一个 (![n - 1](img/file1213.png
    "n - 1")) 维超平面来分割 ![R^{n}](img/file1212.png "R^{n}"). 例如，我们可以用一个普通平面来分割 ![R^{3}](img/file1214.png
    "R^{3}").
- en: These hyperplanes in ![R^{n}](img/file1212.png "R^{n}") are defined by their
    normal vectors ![\overset{\rightarrow}{w} \in R^{n}](img/file1215.png "\overset{\rightarrow}{w}
    \in R^{n}") and some constants ![b \in R](img/file1199.png "b \in R"). In analogy
    to what we saw in ![R^{2}](img/file1194.png "R^{2}"), their points are the ![\overset{\rightarrow}{x}
    \in R^{n}](img/file1216.png "\overset{\rightarrow}{x} \in R^{n}") that satisfy
    the equations of the form
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![R^{n}](img/file1212.png "R^{n}") 中的这些超平面由它们的法向量 ![\overset{\rightarrow}{w}
    \in R^{n}](img/file1215.png "\overset{\rightarrow}{w} \in R^{n}") 和一些常数 ![b \in
    R](img/file1199.png "b \in R") 定义。类似于我们在 ![R^{2}](img/file1194.png "R^{2}") 中看到的情况，它们的点是满足以下形式的方程的
    ![\overset{\rightarrow}{x} \in R^{n}](img/file1216.png "\overset{\rightarrow}{x}
    \in R^{n}")。'
- en: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.](img/file1217.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.") |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.](img/file1217.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.") |'
- en: Moreover, we can determine to which side of the hyperplane a certain ![\overset{\rightarrow}{x}
    \in R^{n}](img/file1216.png "\overset{\rightarrow}{x} \in R^{n}") is in terms
    of the sign of ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b").
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以根据 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b") 的符号来确定某个 ![\overset{\rightarrow}{x}
    \in R^{n}](img/file1216.png "\overset{\rightarrow}{x} \in R^{n}") 属于超平面的哪一侧。
- en: To learn more…
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: In case you are confused with all these equations and you are curious as to
    where they come from, let us quickly explain them. An (affine) hyperplane can
    be defined by a normal vector ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    and by a point ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    in the plane. Thus, a point ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    will belong to the hyperplane if and only if ![\overset{\rightarrow}{x} = \overset{\rightarrow}{p}
    + \overset{\rightarrow}{v}](img/file1219.png "\overset{\rightarrow}{x} = \overset{\rightarrow}{p}
    + \overset{\rightarrow}{v}") for some vector ![\overset{\rightarrow}{v}](img/file1220.png
    "\overset{\rightarrow}{v}") that is orthogonal to ![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}"), that is, such that ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{v} = 0](img/file1221.png "\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{v} = 0"). By combining these two expressions, we know that
    ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}") will
    belong to the hyperplane if and only if ![\overset{\rightarrow}{w} \cdot (\overset{\rightarrow}{x}
    - \overset{\rightarrow}{p}) = 0](img/file1222.png "\overset{\rightarrow}{w} \cdot
    (\overset{\rightarrow}{x} - \overset{\rightarrow}{p}) = 0"), which can be rewritten
    as
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对所有这些方程感到困惑，并且好奇它们从何而来，让我们快速解释一下。一个（仿射）超平面可以通过一个法向量 ![向量w](img/file1205.png
    "向量w") 和平面上的一个点 ![向量p](img/file1218.png "向量p") 来定义。因此，一个点 ![向量x](img/file1206.png
    "向量x") 将属于超平面，当且仅当存在一个向量 ![向量v](img/file1220.png "向量v")，它是 ![向量w](img/file1205.png
    "向量w") 的正交向量，即 ![向量w与向量v的点积等于0](img/file1221.png "向量w与向量v的点积等于0")。通过结合这两个表达式，我们知道
    ![向量x](img/file1206.png "向量x") 将属于超平面，当且仅当 ![向量w与向量x减去向量p的点积等于0](img/file1222.png
    "向量w与向量x减去向量p的点积等于0")，这可以重写为
- en: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + ( - \overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{p}) = \overset{\rightarrow}{x} \cdot \overset{\rightarrow}{w}
    + b = 0,](img/file1223.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + ( - \overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}) = \overset{\rightarrow}{x}
    \cdot \overset{\rightarrow}{w} + b = 0,") |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ![向量w与向量x的点积加上向量w与向量p的点积的相反数等于向量x与向量w的点积加上常数b等于0](img/file1223.png "向量w与向量x的点积加上向量w与向量p的点积的相反数等于向量x与向量w的点积加上常数b等于0")
    |'
- en: where we have implicitly defined ![b = - \overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}](img/file1224.png
    "b = - \overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}").
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们隐式地定义了 ![b等于向量w与向量p的点积的相反数](img/file1224.png "b等于向量w与向量p的点积的相反数")。
- en: Moreover, we have just seen how ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b](img/file1207.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b") is the scalar product of ![\overset{\rightarrow}{x} - \overset{\rightarrow}{p}](img/file1225.png
    "\overset{\rightarrow}{x} - \overset{\rightarrow}{p}") with ![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}"), a fixed normal vector to the plane. This justifies
    why its sign determines on which side of the hyperplane ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}") lies. Remember that, geometrically, the dot product
    of two vectors ![\overset{\rightarrow}{u_{1}} \cdot {\overset{\rightarrow}{u}}_{2}](img/file1226.png
    "\overset{\rightarrow}{u_{1}} \cdot {\overset{\rightarrow}{u}}_{2}") is equal
    to ![\left\| u \right\|_{1} \cdot \left\| u \right\|_{2} \cdot \cos\theta](img/file1227.png
    "\left\| u \right\|_{1} \cdot \left\| u \right\|_{2} \cdot \cos\theta"), where
    ![\theta](img/file89.png "\theta") denotes the smallest angle between them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们刚刚看到![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b")是![\overset{\rightarrow}{x}
    - \overset{\rightarrow}{p}](img/file1225.png "\overset{\rightarrow}{x} - \overset{\rightarrow}{p}")与![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}")的标量积，其中![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}")是平面的一个固定法向量。这解释了为什么它的符号决定了![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}")位于超平面的哪一侧。记住，从几何上讲，两个向量![\overset{\rightarrow}{u_{1}}
    \cdot {\overset{\rightarrow}{u}}_{2}](img/file1226.png "\overset{\rightarrow}{u_{1}}
    \cdot {\overset{\rightarrow}{u}}_{2}")的点积等于![\left\| u \right\|_{1} \cdot \left\|
    u \right\|_{2} \cdot \cos\theta](img/file1227.png "\left\| u \right\|_{1} \cdot
    \left\| u \right\|_{2} \cdot \cos\theta")，其中![\theta](img/file89.png "\theta")表示它们之间的最小角度。
- en: With what we have done so far, we have the tools required to construct (admittedly
    simple) binary classifiers on any Euclidean space. All it takes for us to do so
    is fixing a hyperplane!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止我们所做的一切，我们已经拥有了在任意欧几里得空间上构建（诚然是简单的）二元分类器的工具。我们只需固定一个超平面即可做到这一点！
- en: Why is this important to us? It turns out that support vector machines do exactly
    what we have discussed so far.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们为什么很重要？结果是支持向量机正是我们之前讨论过的。
- en: Important note
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'A support vector machine takes inputs in an ![n](img/file244.png "n")-dimensional
    Euclidean space (![R^{n}](img/file1212.png "R^{n}")) and classifies them according
    to which side of a hyperplane they are on. This hyperplane fully defines the behavior
    of the SVM. Of course, the adjustable parameters of an SVM are the ones that define
    the hyperplane: following our notation, the components of the normal vector ![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}") and the constant ![b](img/file17.png "b").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机在![n](img/file244.png "n")维欧几里得空间([![R^{n}](img/file1212.png "R^{n}")))中接受输入，并根据它们位于超平面的哪一侧进行分类。这个超平面完全定义了SVM的行为。当然，SVM的可调整参数是定义超平面的那些参数：按照我们的符号，法向量![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}")的分量和常数![b](img/file17.png "b")。
- en: In order to get the label of any point ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"), all we have to do is look at the sign of ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b](img/file1207.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b").
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到任意点![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")的标签，我们只需查看![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b](img/file1207.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b")的符号。
- en: 'As you may have suspected, vanilla SVMs, just on their own, are not the most
    powerful of binary classification models: they are intrinsically linear and they
    are not fit to capture sophisticated patterns. We will take care of this later
    in the chapter when we unleash the full potential of SVMs with ”the kernel trick”
    (stay tuned!). In any case, for now, let us rejoice in the simplicity of our model
    and let’s learn how to train it.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经怀疑的那样，纯支持向量机（vanilla SVMs）本身并不是最强大的二元分类模型：它们本质上是线性的，并且不适合捕捉复杂的模式。我们将在本章后面通过“核技巧”释放SVMs的全部潜力时解决这个问题（敬请期待！）无论如何，现在让我们为我们的模型简单性感到高兴，并学习如何训练它。
- en: '9.1.2 How to train support vector machines: the hard-margin case'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1.2 如何训练支持向量机：硬间隔情况
- en: Let’s say that we have a binary classification problem, and we are given some
    training data consisting of datapoints in ![R^{n}](img/file1212.png "R^{n}") together
    with their corresponding labels. Naturally, when we train an SVM for this problem,
    we want to look for the hyperplane that best separates the two categories in the
    training dataset. Now we have to make this intuitive idea precise.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个二元分类问题，并且我们得到了一些训练数据，这些数据包括![R^{n}](img/file1212.png "R^{n}")中的数据点以及它们相应的标签。自然地，当我们为这个问题训练SVM时，我们希望寻找在训练数据集中最好地分离两个类别的超平面。现在我们必须使这个直观的想法变得精确。
- en: Let the datapoints in our training dataset be ![{\overset{\rightarrow}{x}}_{j}
    \in R^{n}](img/file1228.png "{\overset{\rightarrow}{x}}_{j} \in R^{n}") and their
    expected labels be ![y_{j} = 1, - 1](img/file1229.png "y_{j} = 1, - 1") (read
    as positive and negative, respectively). For now, we will assume that our data
    can be perfectly separated by a hyperplane. Later in the section, we will see
    what to do when this is not the case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练数据集中的数据点为 ![{\overset{\rightarrow}{x}}_{j} \in R^{n}](img/file1228.png
    "{\overset{\rightarrow}{x}}_{j} \in R^{n}")，它们的预期标签为 ![y_{j} = 1, - 1](img/file1229.png
    "y_{j} = 1, - 1")（分别读作正和负）。目前，我们假设我们的数据可以被一个超平面完美地分开。在后面的章节中，我们将看到当这种情况不成立时应该怎么做。
- en: 'Notice that, under the assumption that there is at least one hyperplane separating
    our data, there will necessarily be an infinite number of such separating hyperplanes
    (see *Figure* [*9.2*](#Figure9.2)). Will any of them be suitable for our goal
    of building a classifier? If we only cared about the training data, then yes,
    any of them would do the trick. In fact, this is exactly what the perceptron model
    that we discussed in *Chapter* *[*8*](ch017.xhtml#x1-1390008), *What is Quantum
    Machine* *Learning?*, does: it just looks for a hyperplane separating the training
    data.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在假设至少存在一个可以分离我们的数据的超平面的情况下，必然会有无限多个这样的分离超平面（参见*图* [*9.2*](#Figure9.2)）。它们中的任何一个都适合我们的目标，即构建一个分类器吗？如果我们只关心训练数据，那么是的，任何一个都可以做到这一点。事实上，这正是我们在*第*
    *8* *章* *“什么是量子机器学习”*中讨论的感知器模型所做的事情：它只是寻找一个可以分离训练数据的超平面。
- en: '*However, as you surely remember, when we train a classifier, we are interested
    in getting a low generalization error. In our case, one way of trying to achieve
    this is by looking for a separating hyperplane that can maximize the distance
    from itself to the training datapoints. And that is the way in which SVMs are
    actually trained. The rationale behind this is clear: we expect the new, unseen
    datapoints to follow a similar distribution to the one that we have seen in the
    training data. So it is very likely that new examples of one class will be closer
    to training examples of that same class. Therefore, if our separating hyperplane
    is too close to one of the training datapoints, we risk another datapoint of the
    same class crossing to the other side of the hyperplane and being misclassified.
    For instance, in *Figure* [*9.2*](#Figure9.2), the dashed line does separate the
    training datapoints, but it is certainly a much more risky choice than, for example,
    the continuous line.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而，正如你肯定记得的，当我们训练一个分类器时，我们感兴趣的是获得低泛化误差。在我们的情况下，尝试实现这一目标的一种方法是在寻找一个可以最大化其自身与训练数据点之间距离的分离超平面。这正是SVMs实际训练的方式。背后的原理是清晰的：我们期望新的、未见过的数据点遵循我们在训练数据中看到的类似分布。因此，新的一类例子很可能比同一类的训练例子更接近。因此，如果我们的分离超平面离某个训练数据点太近，我们就有风险让同一类的另一个数据点穿过超平面的另一边并被错误分类。例如，在*图*
    [*9.2*](#Figure9.2)中，虚线确实分离了训练数据点，但它肯定比例如连续线更冒险的选择。'
- en: '![Figure 9.2: Both lines (hyperplanes) separate the two categories, but the
    continuous line is closer to the datapoints than the dashed line ](img/file1230.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 9.2: 两条线（超平面）都分离了两个类别，但连续线比虚线更接近数据点](img/file1230.jpg)'
- en: '**Figure 9.2**: Both lines (hyperplanes) separate the two categories, but the
    continuous line is closer to the datapoints than the dashed line'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.2**：两条线（超平面）都分离了两个类别，但连续线比虚线更接近数据点'
- en: 'The idea behind the training of an SVM is then clear: we seek to find not just
    any separating hyperplane, but one that is as far away from the training points
    as possible. This may seem difficult to achieve, but it can be posed as a rather
    straightforward optimization problem. Let’s explain how to do it in a little bit
    more detail.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后SVM训练背后的思想就清晰了：我们寻求的不仅仅是一个分离超平面，而是一个尽可能远离训练点的超平面。这看起来可能很难实现，但它可以表述为一个相当直接的优化问题。让我们更详细地解释一下如何实现它。
- en: In a first approach, we could just consider the distance from a separating hyperplane
    ![H](img/file10.png "H") to all the points in the training dataset, and then try
    to find a way to tweak ![H](img/file10.png "H") in order to maximize that distance
    while making sure that ![H](img/file10.png "H") still separates the data properly.
    This is, however, not the best way to present the problem. Instead, we may notice
    how we can associate to each data point a unique hyperplane that is parallel to
    ![H](img/file10.png "H") and contains that datapoint. And, what is more, the parallel
    hyperplane that goes through the point that is closest to ![H](img/file10.png
    "H") will itself be a separating hyperplane — and so will be its reflection over
    ![H](img/file10.png "H"). This is illustrated in *Figure* [*9.3*](#Figure9.3).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种方法中，我们只需考虑分离超平面![H](img/file10.png "H")到训练数据集中所有点的距离，然后尝试找到一种方法来调整![H](img/file10.png
    "H")，以最大化这个距离，同时确保![H](img/file10.png "H")仍然正确地分离数据。然而，这并不是展示问题的最佳方式。相反，我们可能会注意到，我们可以将每个数据点与一个唯一的、与![H](img/file10.png
    "H")平行的超平面关联起来，该超平面包含该数据点。更重要的是，穿过与![H](img/file10.png "H")最近的点的平行超平面本身也将是一个分离超平面——其反射在![H](img/file10.png
    "H")上也将是。这如图*9.3*所示。
- en: '![Figure 9.3: The continuous black line represents a separating hyperplane
    H. One of the dashed lines is the parallel hyperplane that goes through the closest
    point to H, and its reflection over H is the other dashed line ](img/file1231.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3：连续的黑色线条代表分离超平面H。其中一条虚线是穿过H最近的点的平行超平面，其在H上的反射是另一条虚线](img/file1231.jpg)'
- en: '**Figure 9.3**: The continuous black line represents a separating hyperplane
    ![H](img/file10.png "H"). One of the dashed lines is the parallel hyperplane that
    goes through the closest point to ![H](img/file10.png "H"), and its reflection
    over ![H](img/file10.png "H") is the other dashed line'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.3**：连续的黑色线条代表分离超平面![H](img/file10.png "H")。其中一条虚线是穿过![H](img/file10.png
    "H")最近的点的平行超平面，其在![H](img/file10.png "H")上的反射是另一条虚线'
- en: This pair of hyperplanes — the parallel plane that goes through the closest
    point and its reflection — will be the two equidistant parallel hyperplanes, which
    are the furthest apart from each other while still separating the data. They are
    unique to ![H](img/file10.png "H"). The distance between them is known as the
    **margin** and it is what we aim to maximize. This is illustrated in *Figure*
    [*9.4*](#Figure9.4).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这对超平面——穿过最近点的平行平面及其反射——将是两个等距的平行超平面，它们彼此之间距离最远，同时仍然分离数据。它们是![H](img/file10.png
    "H")独有的。它们之间的距离被称为**间隔**，这是我们试图最大化的目标。这如图*9.4*所示。
- en: We already know that any separating hyperplane ![H](img/file10.png "H") can
    be characterized by an equation of the form ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0"). Moreover, any hyperplane that is parallel to ![H](img/file10.png "H")
    — in particular those that define the margin! — can be characterized as ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = C](img/file1232.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = C") for some constant ![C](img/file234.png
    "C"). And not only that, but their reflection over ![H](img/file10.png "H") will
    be itself characterized by the equation ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - C](img/file1233.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - C"). Hence, we know that, for some constant ![C](img/file234.png "C"),
    the hyperplanes that define the margin of ![H](img/file10.png "H") can be represented
    by the equations ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b
    = \pm C](img/file1234.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = \pm C").
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，任何分离超平面 ![H](img/file10.png "H") 都可以用形式为 ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{x} + b = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{x} + b = 0") 的方程来描述。此外，任何平行于 ![H](img/file10.png "H") 的超平面——特别是那些定义边界的超平面——都可以用
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = C](img/file1232.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = C") 来描述，其中 ![C](img/file234.png
    "C") 是某个常数。不仅如此，它们在 ![H](img/file10.png "H") 上的反射也将由方程 ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - C](img/file1233.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - C") 来描述。因此，我们知道，对于某个常数 ![C](img/file234.png
    "C")，定义 ![H](img/file10.png "H") 边界的超平面可以用方程 ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{x} + b = \pm C](img/file1234.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm C") 来表示。
- en: Nevertheless, there is nothing preventing us here from dividing the whole expression
    by ![C](img/file234.png "C"). So, if we let ![\left. \overset{\sim}{w} = \overset{\rightarrow}{w}\slash
    C \right.](img/file1235.png "\left. \overset{\sim}{w} = \overset{\rightarrow}{w}\slash
    C \right.") and ![\left. \overset{\sim}{b} = b\slash C \right.](img/file1236.png
    "\left. \overset{\sim}{b} = b\slash C \right."), we know that the hyperplane ![H](img/file10.png
    "H") will still be represented by ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0](img/file1237.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0"), but the hyperplanes that define the margin will be
    characterized by
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们在这里并没有阻止我们将整个表达式除以 ![C](img/file234.png "C")。因此，如果我们让 ![\left. \overset{\sim}{w}
    = \overset{\rightarrow}{w}\slash C \right.](img/file1235.png "\left. \overset{\sim}{w}
    = \overset{\rightarrow}{w}\slash C \right.") 和 ![\left. \overset{\sim}{b} = b\slash
    C \right.](img/file1236.png "\left. \overset{\sim}{b} = b\slash C \right.")，我们知道超平面
    ![H](img/file10.png "H") 仍然可以用 ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0](img/file1237.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0") 来表示，但定义边界的超平面将用以下方程来描述：
- en: '| ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b} =
    \pm 1,](img/file1238.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b}
    = \pm 1,") |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b} =
    \pm 1,](img/file1238.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b}
    = \pm 1,") |'
- en: which looks much more neat!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来要整洁得多！
- en: 'Let’s summarize what we have. We want to find a hyperplane that, while separating
    the data properly, maximizes the distance to the points in the training dataset.
    We have seen how we can see this as the problem of finding a hyperplane that maximizes
    the margin: the distance between the two equidistant parallel hyperplanes that
    are the furthest away from each other while still separating the data. And we
    have just proven that, for any separating hyperplane, we can always find some
    values of ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    and ![b](img/file17.png "b") such that those hyperplanes that define the margin
    can be represented as'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们已经学到的内容。我们希望找到一个超平面，它不仅能够正确地分离数据，而且还要最大化到训练数据集中点的距离。我们已经看到，我们可以将这个问题看作是寻找一个最大化边界的超平面：两个等距平行超平面之间的距离最大，同时仍然能够分离数据。我们刚刚已经证明，对于任何分离超平面，我们总能找到一些
    ![w](img/file1205.png "w") 和 ![b](img/file17.png "b") 的值，使得那些定义边界的超平面可以用以下形式表示：
- en: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.](img/file1239.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.") |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.](img/file1239.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.") |'
- en: It can be shown that the distance between these two hyperplanes is ![\left.
    2\slash\left\| w \right\| \right.](img/file1240.png "\left. 2\slash\left\| w \right\|
    \right."). Hence the problem of maximizing the margin can be equivalently stated
    as the problem of maximizing ![\left. 2\slash\left\| w \right\| \right.](img/file1240.png
    "\left. 2\slash\left\| w \right\| \right.") subject to the constraint that the
    planes ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1](img/file1241.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1") properly
    separate the data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明这两个超平面之间的距离是 ![\left. 2\slash\left\| w \right\| \right.](img/file1240.png
    "\left. 2\slash\left\| w \right\| \right.")。因此，最大化间隔的问题可以等价地表述为在约束条件 ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1](img/file1241.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1") 正确分离数据的情况下，最大化 ![\left. 2\slash\left\|
    w \right\| \right.](img/file1240.png "\left. 2\slash\left\| w \right\| \right.")
    的问题。
- en: Exercise 9.1
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 9.1
- en: Show that, as we claimed, the distance between the hyperplanes ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1](img/file1241.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1") is ![\left. 2\slash\left\| w \right\|
    \right.](img/file1240.png "\left. 2\slash\left\| w \right\| \right.").
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 证明，正如我们所声称的，超平面 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} +
    b = \pm 1](img/file1241.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = \pm 1") 之间的距离是 ![\left. 2\slash\left\| w \right\| \right.](img/file1240.png
    "\left. 2\slash\left\| w \right\| \right.")。
- en: Let’s now consider an arbitrary element ![\overset{\rightarrow}{p} \in R^{N}](img/file1242.png
    "\overset{\rightarrow}{p} \in R^{N}") and a hyperplane ![H](img/file10.png "H")
    characterized by ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b
    = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0"). When the value of ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}
    + b](img/file1243.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}
    + b") is zero, we know that ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    is in the hyperplane and, as this value drifts away from zero, the point gets
    further and further away from the hyperplane. If it increases and it is between
    ![0](img/file12.png "0") and ![1](img/file13.png "1"), the point ![\overset{\rightarrow}{p}](img/file1218.png
    "\overset{\rightarrow}{p}") is between the hyperplane ![H](img/file10.png "H")
    and the hyperplane ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} +
    b = 1](img/file1244.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 1"). When this value reaches ![1](img/file13.png "1"), the point is in this
    latter hyperplane. And when the value becomes greater than ![1](img/file13.png
    "1"), it moves beyond both hyperplanes. Analogously, if this value decreases and
    it is between ![0](img/file12.png "0") and ![- 1](img/file312.png "- 1"), the
    point ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    is between the hyperplane ![H](img/file10.png "H") and ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - 1](img/file1245.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - 1"). When the value reaches ![- 1](img/file312.png
    "- 1"), the point is in this last hyperplane. And when it is smaller than ![-
    1](img/file312.png "- 1"), it has moved beyond both ![H](img/file10.png "H") and
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = - 1](img/file1245.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = - 1").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个任意的元素 ![\overset{\rightarrow}{p} \in R^{N}](img/file1242.png "\overset{\rightarrow}{p}
    \in R^{N}") 和一个由 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b
    = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0") 定义的超平面 ![H](img/file10.png "H")。当 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}
    + b](img/file1243.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}
    + b") 的值为零时，我们知道 ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    在超平面内，并且随着这个值远离零，点会越来越远离超平面。如果它增加并且介于 ![0](img/file12.png "0") 和 ![1](img/file13.png
    "1") 之间，点 ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    就位于超平面 ![H](img/file10.png "H") 和超平面 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 1](img/file1244.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 1") 之间。当这个值达到 ![1](img/file13.png "1") 时，点就在这个后者的超平面内。而当它的值大于 ![1](img/file13.png
    "1") 时，它就超出了这两个超平面。类似地，如果这个值减少并且介于 ![0](img/file12.png "0") 和 ![- 1](img/file312.png
    "- 1") 之间，点 ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    就位于超平面 ![H](img/file10.png "H") 和 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - 1](img/file1245.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - 1") 之间。当这个值达到 ![- 1](img/file312.png "- 1") 时，点就在这个最后的超平面内。而当它小于 ![- 1](img/file312.png
    "- 1") 时，它已经超出了 ![H](img/file10.png "H") 和 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - 1](img/file1245.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - 1") 这两个超平面。
- en: Since we are working under the assumption that there are no points inside the
    margin, the hyperplane ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0") will properly separate the data if, for all the positive entries, ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b \geq 1](img/file1246.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b \geq 1"), while all the negative ones will
    satisfy ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b \leq - 1](img/file1247.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b \leq - 1"). We can
    write this condition as
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们假设没有点在边界内，当对于所有正项，![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b \geq 1](img/file1246.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b \geq 1")，而所有负项都满足 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b \leq - 1](img/file1247.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b \leq - 1") 时，超平面 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0") 将正确地分离数据。我们可以将这个条件写成
- en: '| ![y_{j}\left( {\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j}}
    \right) \geq 1,](img/file1248.png "y_{j}\left( {\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j}} \right) \geq 1,") |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ![y_{j}\left( {\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j}}
    \right) \geq 1,](img/file1248.png "y_{j}\left( {\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j}} \right) \geq 1,") |'
- en: because we are considering ![y_{j} = 1](img/file1249.png "y_{j} = 1") when the
    ![j](img/file258.png "j")-th example belongs to the positive class and ![y_{j}
    = - 1](img/file1250.png "y_{j} = - 1") when it belongs to the negative one.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因为当我们考虑![j](img/file258.png "j")-th个示例属于正类时，![y_{j} = 1](img/file1249.png "y_{j}
    = 1")，而当它属于负类时，![y_{j} = - 1](img/file1250.png "y_{j} = - 1")。
- en: '![Figure 9.4: The hyperplane that could have been returned by an SVM is represented
    by a black continuous line, and the lines in dashed lines are the equidistant
    parallel hyperplanes that are the furthest apart from each other while still separating
    the data. The margin is thus half of the thickness of the colored region ](img/file1251.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4：支持向量机可能返回的超平面用黑色连续线表示，而虚线表示的是相互之间距离最远但仍能分离数据的等距平行超平面。因此，边界是彩色区域厚度的一半](img/file1251.jpg)'
- en: '**Figure 9.4**: The hyperplane that could have been returned by an SVM is represented
    by a black continuous line, and the lines in dashed lines are the equidistant
    parallel hyperplanes that are the furthest apart from each other while still separating
    the data. The margin is thus half of the thickness of the colored region'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.4**：支持向量机可能返回的超平面用黑色连续线表示，而虚线表示的是相互之间距离最远但仍能分离数据的等距平行超平面。因此，边界是彩色区域厚度的一半。'
- en: 'For all this, the problem of finding the hyperplane that best separates the
    data can be posed as the following optimization problem:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有这些，寻找最佳分离数据的超平面的问题可以表述为以下优化问题：
- en: '![\begin{array}{rlrl} {\text{Minimize}\quad} & {\left\| w \right\|\qquad} &
    & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j},
    + b) \geq 1,\qquad} & & \qquad \\ \end{array}](img/file1252.png "\begin{array}{rlrl}
    {\text{Minimize}\quad} & {\left\| w \right\|\qquad} & & \qquad \\ {\text{subject~to}\quad}
    & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j}, + b) \geq
    1,\qquad} & & \qquad \\ \end{array}")'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![\begin{array}{rlrl} {\text{最小化}\quad} & {\left\| w \right\|\qquad} & & \qquad
    \\ {\text{约束条件}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j},
    + b) \geq 1,\qquad} & & \qquad \\ \end{array}](img/file1252.png "\begin{array}{rlrl}
    {\text{最小化}\quad} & {\left\| w \right\|\qquad} & & \qquad \\ {\text{约束条件}\quad}
    & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j}, + b) \geq
    1,\qquad} & & \qquad \\ \end{array}")'
- en: where, of course, each ![j](img/file258.png "j") defines an individual constraint.
    This formulation suffers from a small problem. The Euclidean norm is nice, visual,
    and geometric, but it has a square root. We personally have nothing against square
    roots — some of our best friends *are* square roots — but most optimization algorithms
    have some hard feelings against them. So just to make life easier for us, we may
    instead consider the following (equivalent) problem.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，当然，每个![j](img/file258.png "j")定义了一个单独的约束。这种表述存在一个小问题。欧几里得范数很棒，直观且几何性强，但它包含平方根。我们个人对平方根没有意见——我们最好的朋友中就有几个是平方根——但大多数优化算法对它们都有一些抵触。所以为了让我们更容易生活，我们可能考虑以下（等价）问题。
- en: Important note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'If the data in the training dataset can be separated by a hyperplane, the problem
    of training an SVM can be posed as the following optimization problem:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练数据集中的数据可以通过一个超平面分离，那么训练支持向量机的问题可以表述为以下优化问题：
- en: '![\begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1.\qquad} & & \qquad \\ \end{array}](img/file1253.png
    "\begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1.\qquad} & & \qquad \\ \end{array}")'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![\begin{array}{rlrl} {\text{最小化}\quad} & {\frac{1}{2}\left\| w \right\|^{2}\qquad}
    & & \qquad \\ {\text{约束条件}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j}
    + b) \geq 1.\qquad} & & \qquad \\ \end{array}](img/file1253.png "\begin{array}{rlrl}
    {\text{最小化}\quad} & {\frac{1}{2}\left\| w \right\|^{2}\qquad} & & \qquad \\ {\text{约束条件}\quad}
    & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq
    1.\qquad} & & \qquad \\ \end{array}")'
- en: This is known as **hard-margin** training, because we are allowing no elements
    in the training dataset to be misclassified or even to be inside the margin.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**硬边界**训练，因为我们允许训练数据集中没有任何元素被错误分类，甚至被包含在边界内。
- en: That nice and innocent square will save us from so many troubles. Notice, by
    the way, that we’ve introduced a ![\left. 1\slash 2 \right.](img/file136.png "\left.
    1\slash 2 \right.") factor next to ![\left\| w \right\|^{2}](img/file1254.png
    "\left\| w \right\|^{2}"). That’s for reasons of technical convenience, but it
    isn’t really important.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 那个可爱而单纯的正方形将帮助我们避免许多麻烦。顺便说一下，我们已经引入了一个![\left. 1\slash 2 \right.](img/file136.png
    "\left. 1\slash 2 \right.")因子在![\left\| w \right\|^{2}](img/file1254.png "\left\|
    w \right\|^{2}")旁边。这是出于技术方便的考虑，但并不是真正重要的。
- en: With hard-margin training, we need our training data to be perfectly separable
    by a hyperplane because, otherwise, we will not find any feasible solutions to
    the optimization problem that we have just defined. This scenario is, in most
    situations, too restrictive. Thankfully, we can take an alternative approach known
    as **soft-margin training**.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬间隔训练中，我们需要我们的训练数据能够被超平面完美地分离，因为否则，我们不会找到我们刚刚定义的优化问题的任何可行解。这种情况在大多数情况下过于严格。幸运的是，我们可以采取一种称为**软间隔训练**的替代方法。
- en: 9.1.3 Soft-margin training
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1.3 软间隔训练
- en: Soft-margin training is similar to hard-margin training. The only difference
    is that it also incorporates some adjustable **slack**, or ”tolerance,” parameters
    ![\xi_{j} \geq 0](img/file1255.png "\xi_{j} \geq 0") that will add flexibility
    to the constraints. In this way, instead of considering the constraint ![y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1](img/file1256.png "y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1"), we will use
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 软间隔训练类似于硬间隔训练。唯一的区别是它还包含一些可调整的**松弛**或“容忍”参数![ξ_{j} \geq 0](img/file1255.png
    "\xi_{j} \geq 0")，这将增加约束的灵活性。这样，我们不是考虑约束![y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1](img/file1256.png "y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1")，而是使用
- en: '| ![y_{j}(w \cdot x_{j} + b) \geq 1 - \xi_{j}.](img/file1257.png "y_{j}(w \cdot
    x_{j} + b) \geq 1 - \xi_{j}.") |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| ![y_{j}(w \cdot x_{j} + b) \geq 1 - \xi_{j}.](img/file1257.png "y_{j}(w \cdot
    x_{j} + b) \geq 1 - \xi_{j}.") |'
- en: Thus, when ![\xi_{j} > 0](img/file1258.png "\xi_{j} > 0"), we will allow ![{\overset{\rightarrow}{x}}_{j}](img/file1259.png
    "{\overset{\rightarrow}{x}}_{j}") to be close to the hyperplane or even on the
    wrong side of the space (as separated by the hyperplane). What is more, the bigger
    the value of ![\xi_{j}](img/file1260.png "\xi_{j}"), the further into the wrong
    side ![{\overset{\rightarrow}{x}}_{j}](img/file1259.png "{\overset{\rightarrow}{x}}_{j}")
    will be.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 ![ξ_{j} > 0](img/file1258.png "\xi_{j} > 0") 时，我们将允许 ![{\overset{\rightarrow}{x}}_{j}](img/file1259.png
    "{\overset{\rightarrow}{x}}_{j}") 接近超平面，甚至位于空间的错误一侧（由超平面分隔）。更重要的是，![ξ_{j}](img/file1260.png
    "\xi_{j}") 的值越大，![{\overset{\rightarrow}{x}}_{j}](img/file1259.png "{\overset{\rightarrow}{x}}_{j}")
    将越深入错误一侧。
- en: Naturally, we would like these ![\xi_{j}](img/file1260.png "\xi_{j}") to be
    as small as possible, so we need to include them in the cost function that we
    want to minimize. Taking all of this into account, the optimization problem that
    we shall consider in soft-margin training will be the following.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，我们希望这些![ξ_{j}](img/file1260.png "\xi_{j}") 尽可能小，因此我们需要将它们包含在我们想要最小化的成本函数中。考虑到所有这些因素，我们在软间隔训练中要考虑的优化问题将是以下内容。
- en: Important note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'A support vector machine that may not be *necessarily* able to properly separate
    the training data with a hyperplane can be trained by solving the following optimization
    problem:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能无法**必然**使用超平面正确分离训练数据的支持向量机可以通过解决以下优化问题进行训练：
- en: '![\begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}
    + C\sum\limits_{j}\xi_{j}\qquad} & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1 - \xi_{j},\qquad} & & \qquad
    \\ & {\xi_{j} \geq 0.\qquad} & & \qquad \\ \end{array}](img/file1261.png "\begin{array}{rlrl}
    {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2} + C\sum\limits_{j}\xi_{j}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1 - \xi_{j},\qquad} & & \qquad \\  &
    {\xi_{j} \geq 0.\qquad} & & \qquad \\ \end{array}")'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}
    + C\sum\limits_{j}\xi_{j}\qquad} & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1 - \xi_{j},\qquad} & & \qquad
    \\ & {\xi_{j} \geq 0.\qquad} & & \qquad \\ \end{array}](img/file1261.png "\begin{array}{rlrl}
    {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2} + C\sum\limits_{j}\xi_{j}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1 - \xi_{j},\qquad} & & \qquad \\  &
    {\xi_{j} \geq 0.\qquad} & & \qquad \\ \end{array}")'
- en: The value ![C > 0](img/file1262.png "C > 0") is a hyperparameter that can be
    chosen at will. The bigger ![C](img/file234.png "C") is, the less tolerant we
    will be to training examples falling inside the margin or on the wrong side of
    the hyperplane.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![C > 0](img/file1262.png "C > 0")的值是一个可以随意选择的超参数。![C](img/file234.png "C")越大，我们对训练示例落在间隔内或超平面的错误一侧的容忍度就越低。'
- en: This formulation is known as **soft-margin training** of an SVM.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式被称为SVM的**软间隔训练**。
- en: Let us now try to digest this formulation. As expected, we also made the ![\xi_{j}](img/file1260.png
    "\xi_{j}") contribute to our cost function, in such a way that their taking large
    values will be penalized. In addition, we’ve incorporated this ![C](img/file234.png
    "C") constant and said that it can be tweaked at will. As we mentioned before,
    in broad terms, the bigger it is, the more unwilling we will be to accept misclassified
    elements in the training dataset. Actually, if there is a hyperplane that can
    perfectly separate the data, setting ![C](img/file234.png "C") to a huge value
    would be equivalent to doing hard-margin training. At first, it might seem tempting
    to make ![C](img/file234.png "C") huge, but this would make our model more prone
    to overfitting. Perfect fits are not that good! Balancing the value of ![C](img/file234.png
    "C") is one of the many keys behind successful SVM training.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在尝试消化这个公式。正如预期的那样，我们也让![\xi_{j}](img/file1260.png "\xi_{j}")对我们的代价函数做出了贡献，这样它们取大值将会受到惩罚。此外，我们引入了![C](img/file234.png
    "C")这个常数，并表示它可以随意调整。正如我们之前提到的，从广义上讲，它越大，我们越不愿意接受训练数据集中被错误分类的元素。实际上，如果存在一个可以完美分离数据的超平面，将![C](img/file234.png
    "C")设置为一个很大的值将等同于进行硬间隔训练。起初，可能会觉得将![C](img/file234.png "C")设置得很大很有吸引力，但这样做会使我们的模型更容易过拟合。完美的拟合并不那么好！平衡![C](img/file234.png
    "C")的值是成功SVM训练背后的许多关键之一。
- en: To learn more…
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多…
- en: When we train an SVM, the actual loss function that we would like to minimize
    is
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练SVM时，我们实际上想要最小化的损失函数是
- en: '| ![L(\overset{\rightarrow}{w},b;\overset{\rightarrow}{x},y) = \max\{ 0,1 -
    y(\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b)\},](img/file1263.png
    "L(\overset{\rightarrow}{w},b;\overset{\rightarrow}{x},y) = \max\{ 0,1 - y(\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b)\},") |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ![L(\overset{\rightarrow}{w},b;\overset{\rightarrow}{x},y) = \max\{ 0,1 -
    y(\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b)\},](img/file1263.png
    "L(\overset{\rightarrow}{w},b;\overset{\rightarrow}{x},y) = \max\{ 0,1 - y(\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b)\},") |'
- en: which is called the **hinge loss**. In fact, our ![\xi_{j}](img/file1260.png
    "\xi_{j}") variables are direct representatives of that loss. Minimizing the expected
    value of this loss function would be connected to minimizing the proportion of
    misclassified elements — which is what we want at the end of the day.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**铰链损失**。实际上，我们的![\xi_{j}](img/file1260.png "\xi_{j}")变量是那个损失的直接代表。最小化这个损失函数的期望值将与最小化错误分类元素的比例相关——这正是我们最终想要的。
- en: If, in our formulation, we didn’t have the ![\left. \left\| w \right\|^{2}\slash
    2 \right.](img/file1264.png "\left. \left\| w \right\|^{2}\slash 2 \right.") factor,
    that would be the training loss that we would be minimizing. We included this
    factor, however, because a small ![\left\| w \right\|^{2}](img/file1254.png "\left\|
    w \right\|^{2}") (that is, a large margin) makes SVM models more robust against
    overfitting.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在我们的公式中没有![\left. \left\| w \right\|^{2}\slash 2 \right.](img/file1264.png
    "\left. \left\| w \right\|^{2}\slash 2 \right.")这个因子，那么这就是我们要最小化的训练损失。然而，我们包括了这一因子，因为一个小的![\left\|
    w \right\|^{2}](img/file1254.png "\left\| w \right\|^{2}")（即一个大的间隔）使SVM模型对过拟合更加鲁棒。
- en: We will conclude this analysis of soft-margin training by presenting an equivalent
    formulation of its optimization problem. This formulation is known as the **Lagrangian
    dual** of the optimization problem that we presented previously. We will not discuss
    why these two formulations are equivalent, but you can take our word for it —
    or you can check the wonderful explanation by Abu-Mostafa, Magdon-Ismail, and
    Lin [[2](ch030.xhtml#Xabu2012blearning)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过展示其优化问题的等价公式来结束对软间隔训练的分析。这个公式被称为我们之前提出的优化问题的**拉格朗日对偶**。我们不会讨论为什么这两个公式是等价的，但你可以相信我们的话——或者你可以查看Abu-Mostafa、Magdon-Ismail和Lin的精彩解释[[2](ch030.xhtml#Xabu2012blearning)]。
- en: Important note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'The soft-margin training problem can be equivalently written in terms of some
    optimizable parameters ![\alpha_{j}](img/file1265.png "\alpha_{j}") as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 软间隔训练问题可以用一些可优化的参数![\alpha_{j}](img/file1265.png "\alpha_{j}")等价地表示如下：
- en: '![\begin{array}{rlrl} {\text{Maximize}\quad} & {\sum\limits_{j}\alpha_{j} -
    \frac{1}{2}\sum\limits_{j,k}y_{j}y_{k}\alpha_{j}\alpha_{k}\left( {{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}} \right),\qquad} & & \qquad \\ {\text{subject~to}\quad}
    & {0 \leq \alpha_{j} \leq C,\qquad} & & \qquad \\ & {\sum\limits_{j}\alpha_{j}y_{j}
    = 0.\qquad} & & \qquad \\ \end{array}](img/file1266.png "\begin{array}{rlrl} {\text{Maximize}\quad}
    & {\sum\limits_{j}\alpha_{j} - \frac{1}{2}\sum\limits_{j,k}y_{j}y_{k}\alpha_{j}\alpha_{k}\left(
    {{\overset{\rightarrow}{x}}_{j} \cdot {\overset{\rightarrow}{x}}_{k}} \right),\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {0 \leq \alpha_{j} \leq C,\qquad} & &
    \qquad \\  & {\sum\limits_{j}\alpha_{j}y_{j} = 0.\qquad} & & \qquad \\ \end{array}")'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![\begin{array}{rlrl} {\text{Maximize}\quad} & {\sum\limits_{j}\alpha_{j} -
    \frac{1}{2}\sum\limits_{j,k}y_{j}y_{k}\alpha_{j}\alpha_{k}\left( {{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}} \right),\qquad} & & \qquad \\ {\text{subject~to}\quad}
    & {0 \leq \alpha_{j} \leq C,\qquad} & & \qquad \\ & {\sum\limits_{j}\alpha_{j}y_{j}
    = 0.\qquad} & & \qquad \\ \end{array}](img/file1266.png "\begin{array}{rlrl} {\text{Maximize}\quad}
    & {\sum\limits_{j}\alpha_{j} - \frac{1}{2}\sum\limits_{j,k}y_{j}y_{k}\alpha_{j}\alpha_{k}\left(
    {{\overset{\rightarrow}{x}}_{j} \cdot {\overset{\rightarrow}{x}}_{k}} \right),\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {0 \leq \alpha_{j} \leq C,\qquad} & &
    \qquad \\  & {\sum\limits_{j}\alpha_{j}y_{j} = 0.\qquad} & & \qquad \\ \end{array}")'
- en: This formulation of the SVM soft-margin training problem is, most of the time,
    easier to solve in practice, and it is the one that we will be working with. Once
    we obtain the ![\alpha_{j}](img/file1265.png "\alpha_{j}") values, it is also
    possible to go back to the original formulation. In fact, from the ![\alpha_{j}](img/file1265.png
    "\alpha_{j}") values, we can recover ![b](img/file17.png "b") and ![w](img/file1267.png
    "w"). For instance, it holds that
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: SVM软间隔训练问题的这种表述，在实践中大多数情况下更容易解决，这也是我们将要使用的方法。一旦我们获得了 ![\alpha_{j}](img/file1265.png
    "\alpha_{j}") 的值，我们也可以回到原始的表述。实际上，从 ![\alpha_{j}](img/file1265.png "\alpha_{j}")
    的值中，我们可以恢复 ![b](img/file17.png "b") 和 ![w](img/file1267.png "w")。例如，以下等式成立：
- en: '![\overset{\rightarrow}{w} = \sum\limits_{j}\alpha_{j}y_{j}\overset{\rightarrow}{x_{j}}.](img/file1268.png
    "\overset{\rightarrow}{w} = \sum\limits_{j}\alpha_{j}y_{j}\overset{\rightarrow}{x_{j}}.")'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![\overset{\rightarrow}{w} = \sum\limits_{j}\alpha_{j}y_{j}\overset{\rightarrow}{x_{j}}.](img/file1268.png
    "\overset{\rightarrow}{w} = \sum\limits_{j}\alpha_{j}y_{j}\overset{\rightarrow}{x_{j}}.")'
- en: Notice that ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    only depends on the training points ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}"), for which ![\alpha_{j} \neq 0](img/file1270.png
    "\alpha_{j} \neq 0"). These vectors are called **support vectors** and, as you
    can imagine, are the reason behind the name of the SVM model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    只依赖于训练点 ![\overset{\rightarrow}{x_{j}}](img/file1269.png "\overset{\rightarrow}{x_{j}}")，对于这些点
    ![\alpha_{j} \neq 0](img/file1270.png "\alpha_{j} \neq 0")。这些向量被称为**支持向量**，正如你所想象的那样，这也是SVM模型名称的由来。
- en: Furthermore, we can also recover ![b](img/file17.png "b") by finding some ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}") that lies at the boundary of the margin and solving
    a simple equation — see [[2](ch030.xhtml#Xabu2012blearning)] for all the details.
    Then, in order to classify a point ![x](img/file269.png "x"), we can just compute
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以通过找到位于边缘边界的一些 ![\overset{\rightarrow}{x_{j}}](img/file1269.png "\overset{\rightarrow}{x_{j}}")
    来恢复 ![b](img/file17.png "b")，并解一个简单的方程——有关所有细节，请参阅 [[2](ch030.xhtml#Xabu2012blearning)]。然后，为了对点
    ![x](img/file269.png "x") 进行分类，我们只需计算
- en: '![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \sum\limits_{j}\alpha_{j}y_{j}\left(
    {\overset{\rightarrow}{x_{j}} \cdot \overset{\rightarrow}{x}} \right) + b,](img/file1271.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \sum\limits_{j}\alpha_{j}y_{j}\left(
    {\overset{\rightarrow}{x_{j}} \cdot \overset{\rightarrow}{x}} \right) + b,")'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \sum\limits_{j}\alpha_{j}y_{j}\left(
    {\overset{\rightarrow}{x_{j}} \cdot \overset{\rightarrow}{x}} \right) + b,](img/file1271.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \sum\limits_{j}\alpha_{j}y_{j}\left(
    {\overset{\rightarrow}{x_{j}} \cdot \overset{\rightarrow}{x}} \right) + b,")'
- en: and decide whether ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    goes into the positive or negative class depending on whether the result is bigger
    than 0 or not.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 并根据结果是否大于0来决定 ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    是否属于正类或负类。
- en: We’ve now covered all we need to know about how to train a support vector machine.
    But, with our tools, we can only train these models to obtain linear separations
    between data, which is, well, not the most exciting of prospects. In the next
    section, we will overcome this limitation with a simple yet powerful trick.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了训练支持向量机所需的所有知识。但是，使用我们的工具，我们只能训练这些模型以获得数据之间的线性分离，这，嗯，并不是最令人兴奋的前景。在下一节中，我们将通过一个简单而强大的技巧来克服这一限制。
- en: 9.1.4 The kernel trick
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1.4 核技巧
- en: Vanilla SVMs can only be trained to find linear separations between data elements.
    For example, the data shown in *Figure* [*9.5a*](#Figure9.5a) cannot be separated
    effectively by any SVM, because there is no way to separate it linearly.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基础SVM只能训练以找到数据元素之间的线性分离。例如，*图* [*9.5a*](#Figure9.5a) 中所示的数据无法通过任何SVM有效地分离，因为没有方法可以线性地将其分离。
- en: How do we overcome this? Using **the kernel trick**. This technique consists
    in mapping the data from its original space ![R^{n}](img/file1212.png "R^{n}")
    to a higher dimensional space ![R^{N}](img/file1272.png "R^{N}"), all in the hope
    that, in that space, there may be a way to separate the data with a hyperplane.
    This higher dimensional space is known as a **feature space**, and we will refer
    to the function ![\left. \varphi:R^{n}\rightarrow R^{N} \right.](img/file1273.png
    "\left. \varphi:R^{n}\rightarrow R^{N} \right.") — which takes the original data
    inputs into the feature space — as a **feature** **map**.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何克服这一点？使用**核技巧**。这种技术包括将数据从其原始空间 ![R^{n}](img/file1212.png "R^{n}") 映射到一个更高维的空间
    ![R^{N}](img/file1272.png "R^{N}")，所有这些都是在希望在那个空间中可能有一种方法可以使用超平面来分离数据。这个更高维的空间被称为**特征空间**，我们将把将原始数据输入到特征空间的函数
    ![\left. \varphi:R^{n}\rightarrow R^{N} \right.](img/file1273.png "\left. \varphi:R^{n}\rightarrow
    R^{N} \right.") 称为**特征** **映射**。
- en: For instance, the data in *Figure* [*9.5a*](#Figure9.5a) is in the ![1](img/file13.png
    "1")-dimensional real line, but we can map it to the ![2](img/file302.png "2")-dimensional
    plane with the function
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*图* [*9.5a*](#Figure9.5a) 中的数据位于 ![1](img/file13.png "1") 维实数线上，但我们可以使用以下函数将其映射到
    ![2](img/file302.png "2") 维平面：
- en: '| ![f(x) = (x,x^{2}).](img/file1274.png "f(x) = (x,x^{2}).") |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ![f(x) = (x,x^{2}).](img/file1274.png "f(x) = (x,x^{2}).") |'
- en: As we can see in *Figure* [*9.5b*](#Figure9.5b), upon doing this, there is a
    hyperplane that perfectly separates the two categories in our dataset.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 *图* [*9.5b*](#Figure9.5b) 中所看到的，这样做之后，有一个超平面可以完美地分离我们的数据集中的两个类别。
- en: '![(a) Original data in the real line](img/file1275.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![(a) 实数线上的原始数据](img/file1275.png)'
- en: '**(a)** Original data in the real line'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**(a)** 实数线上的原始数据'
- en: '![(b) Data in the feature space](img/file1276.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![(b) 特征空间中的数据](img/file1276.png)'
- en: '**(b)** Data in the feature space'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**(b)** 特征空间中的数据'
- en: '**Figure 9.5**: The original data cannot be separated by a hyperplane, but
    — upon taking it to a higher-dimensional space with a feature map — it can. The
    separating hyperplane is represented by a dashed line'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.5**：原始数据无法通过超平面分离，但通过使用特征映射将其带到更高维的空间，它可以被分离。分离超平面用虚线表示'
- en: Looking at the dual form of the soft-margin SVM optimization problem, we can
    see how, in order to train an SVM — and to later classify new data — on a certain
    feature space with a feature map ![\varphi](img/file1277.png "\varphi"), all we
    need to ”know” about the feature space is how to compute scalar products in it
    of elements returned by the feature map. This is because, during the whole training
    process, the only operation that depends on the ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}") points is the inner product ![{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}](img/file1278.png "{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}") — or the inner product ![\overset{\rightarrow}{x_{j}}
    \cdot \overset{\rightarrow}{x}](img/file1279.png "\overset{\rightarrow}{x_{j}}
    \cdot \overset{\rightarrow}{x}") when classifying a new point ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"). If instead of ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}") we had ![\varphi(\overset{\rightarrow}{x_{j}})](img/file1280.png
    "\varphi(\overset{\rightarrow}{x_{j}})"), we would just need to know how to compute
    ![\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi({\overset{\rightarrow}{x}}_{k})](img/file1281.png
    "\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi({\overset{\rightarrow}{x}}_{k})")
    — or ![\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi(\overset{\rightarrow}{x})](img/file1282.png
    "\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi(\overset{\rightarrow}{x})")
    to classify new data ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}").
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 观察软间隔SVM优化问题的对偶形式，我们可以看到，为了在具有特征映射 ![\varphi](img/file1277.png "\varphi") 的某个特征空间上训练SVM——以及后来对新数据进行分类——我们只需要“知道”如何计算特征映射返回的元素在该特征空间中的标量积。这是因为，在整个训练过程中，唯一依赖于
    ![\overset{\rightarrow}{x_{j}}](img/file1269.png "\overset{\rightarrow}{x_{j}}")
    点的操作是内积 ![{\overset{\rightarrow}{x}}_{j} \cdot {\overset{\rightarrow}{x}}_{k}](img/file1278.png
    "{\overset{\rightarrow}{x}}_{j} \cdot {\overset{\rightarrow}{x}}_{k}")——或者当分类新点
    ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}") 时，内积
    ![{\overset{\rightarrow}{x}}_{j} \cdot {\overset{\rightarrow}{x}}](img/file1279.png
    "{\overset{\rightarrow}{x}}_{j} \cdot {\overset{\rightarrow}{x}}")。如果我们不是有 ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}")，而是有 ![\varphi(\overset{\rightarrow}{x_{j}})](img/file1280.png
    "\varphi(\overset{\rightarrow}{x_{j}})"),我们只需要知道如何计算 ![\varphi({\overset{\rightarrow}{x}}_{j})
    \cdot \varphi({\overset{\rightarrow}{x}}_{k})](img/file1281.png "\varphi({\overset{\rightarrow}{x}}_{j})
    \cdot \varphi({\overset{\rightarrow}{x}}_{k})")——或者 ![\varphi({\overset{\rightarrow}{x}}_{j})
    \cdot \varphi(\overset{\rightarrow}{x})](img/file1282.png "\varphi({\overset{\rightarrow}{x}}_{j})
    \cdot \varphi(\overset{\rightarrow}{x})")来对新数据 ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}")进行分类。
- en: That is, it suffices to be able to compute the function
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们只需要能够计算这个函数
- en: '| ![k(x,y) = \varphi(\overset{\rightarrow}{x}) \cdot \varphi(\overset{\rightarrow}{y}),](img/file1283.png
    "k(x,y) = \varphi(\overset{\rightarrow}{x}) \cdot \varphi(\overset{\rightarrow}{y}),")
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ![k(x,y) = \varphi(\overset{\rightarrow}{x}) \cdot \varphi(\overset{\rightarrow}{y}),](img/file1283.png
    "k(x,y) = \varphi(\overset{\rightarrow}{x}) \cdot \varphi(\overset{\rightarrow}{y}),")
    |'
- en: and that is the single and only computation that we need to perform in the feature
    space. This is a crucial fact. This function is a particular case of what are
    known as **kernel functions**. Broadly speaking, kernel functions are functions
    that *can be* represented as inner products in some space. Mercer’s theorem (see
    [[2](ch030.xhtml#Xabu2012blearning)]) gives a nice characterization of them in
    terms of certain properties such as being symmetric and some other conditions.
    In the cases that we will consider, these conditions are always going to be met,
    so we don’t need to worry too much about them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在特征空间中需要执行的单个且唯一的计算。这是一个关键事实。这个函数是所谓的**核函数**的特例。广义而言，核函数是可以表示为某些空间中内积的函数。Mercer定理（参见[[2](ch030.xhtml#Xabu2012blearning)])从某些性质（如对称性以及一些其他条件）的角度给出了它们的一个很好的描述。在我们将要考虑的案例中，这些条件总是会被满足，所以我们不需要过于担心它们。
- en: With this, we have a general understanding of how support vector machines are
    used in general, and in classical setups in particular. We now have all the necessary
    background to take the step to quantum. Get ready to explore quantum support vector
    machines.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一点，我们对支持向量机在一般情况下的应用，特别是在经典设置中的应用有了总体理解。我们现在已经具备了进入量子领域的所有必要背景知识。准备好去探索量子支持向量机。
- en: 9.2 Going quantum
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.2 进入量子领域
- en: As we have already mentioned, quantum support vector machines are particular
    cases of SVMs. To be more precise, they are particular cases of SVMs that rely
    on the kernel trick.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，量子支持向量机是支持向量机的特例。更准确地说，它们是依赖于核技巧的支持向量机的特例。
- en: 'We have seen in the previous section how, with the kernel trick, we take our
    data to a feature space: a higher dimensional space in which, we hope, our data
    will be separable by a hyperplane with the right choice of feature map. This feature
    space is usually just the ordinary Euclidean space but, well, with a higher dimension.
    But we can consider other choices. How about…the space of quantum states?'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们已经看到，通过核技巧，我们将数据转换到特征空间：一个更高维的空间，我们希望在这个空间中，通过选择合适的特征图，数据可以被一个超平面分开。这个特征空间通常是普通的欧几里得空间，但，嗯，维度更高。但是，我们可以考虑其他选择。比如…量子态的空间？
- en: 9.2.1 The general idea behind quantum support vector machines
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2.1 量子支持向量机背后的通用思想
- en: A QSVM works just like an ordinary SVM that relies on the kernel trick — with
    the only difference that it uses as feature space a certain space of quantum states.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: QSVM的工作方式与依赖于核技巧的普通SVM相同——唯一的区别是它使用一个特定的量子态空间作为特征空间。
- en: As we discussed before, whenever we use the kernel trick, all we need from the
    feature space is a kernel function. That’s the only ingredient involving the feature
    space that is necessary in order to be able to train a kernel-based SVM and make
    predictions with it. This idea inspired some works, such as the famous paper by
    Havlíček et al. [[52](ch030.xhtml#Xhavlivcek2019supervised)], to try to use quantum
    circuits to compute kernels and, hopefully, obtain some advantage over classical
    computers by working in a sophisticated feature space.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，每当使用核技巧时，我们只需要从特征空间中获得核函数。这是唯一涉及特征空间的成分，它是训练基于核的SVM并使用它进行预测所必需的。这个想法启发了某些工作，例如Havlíček等人著名的论文[[52](ch030.xhtml#Xhavlivcek2019supervised)]，尝试使用量子电路来计算核函数，并希望通过在复杂的特征空间中工作获得一些优势。
- en: 'Taking this into account, in order to train and then use a quantum support
    vector machine for classification, we will be able to do business as usual — doing
    everything fully classically — except for the computation of the kernel function.
    This function will have to rely on a quantum computer in order to do the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，为了训练并使用量子支持向量机进行分类，我们能够像往常一样进行操作——完全采用经典方式——除了核函数的计算。这个函数将必须依赖于量子计算机来完成以下工作：
- en: Take as input two vectors in the original space of data.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入原始数据空间中的两个向量。
- en: Map each of them to a quantum state through a **feature map**.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过**特征图**将它们映射到量子态。
- en: Compute the inner product of the quantum states and return it.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算量子态的内积并返回它。
- en: We will discuss how to implement these (quantum) feature maps in the next subsection,
    but, in essence, they are just circuits that are parametrized exclusively by the
    original (classical) data and thus prepare a quantum state that depends only on
    that data. For now, we will just take these feature maps as a given.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一小节讨论如何实现这些（量子）特征图，但本质上，它们只是由原始（经典）数据参数化的电路，从而准备一个只依赖于该数据的量子态。现在，我们只需将这些特征图视为已知。
- en: 'So, let’s say that we have a feature map ![\varphi](img/file1277.png "\varphi").
    This will be implemented by a circuit ![\Phi](img/file1284.png "\Phi") that will
    depend on some classical data in the original space: for each input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"), we will have a circuit ![\Phi(\overset{\rightarrow}{x})](img/file1285.png
    "\Phi(\overset{\rightarrow}{x})") such that the output of the feature map will
    be the quantum state ![\varphi(\overset{\rightarrow}{x}) = \Phi(\overset{\rightarrow}{x})\left|
    0 \right\rangle](img/file1286.png "\varphi(\overset{\rightarrow}{x}) = \Phi(\overset{\rightarrow}{x})\left|
    0 \right\rangle"). With a feature map ready, we can then take our kernel function
    to be'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，假设我们有一个特征图 ![φ](img/file1277.png "φ")。这将通过一个电路 ![Φ](img/file1284.png "Φ")
    来实现，它将依赖于原始空间中的某些经典数据：对于每个输入 ![x→](img/file1206.png "x→")，我们将有一个电路 ![Φ(x→)](img/file1285.png
    "Φ(x→)")，使得特征图的输出将是量子态 ![φ(x→) = Φ(x→)|0⟩](img/file1286.png "φ(x→) = Φ(x→)|0⟩")。有了准备好的特征图，我们就可以选择我们的核函数了
- en: '| ![k(\overset{\rightarrow}{a},\overset{\rightarrow}{b}) = \left&#124; \left\langle
    \varphi(a) \middle&#124; \varphi(b) \right\rangle \right&#124;^{2} = \left&#124;
    {\left\langle 0 \right&#124;\Phi^{\dagger}(a)\Phi(b)\left&#124; 0 \right\rangle}
    \right&#124;^{2}.](img/file1287.png "k(\overset{\rightarrow}{a},\overset{\rightarrow}{b})
    = \left&#124; \left\langle \varphi(a) \middle&#124; \varphi(b) \right\rangle \right&#124;^{2}
    = \left&#124; {\left\langle 0 \right&#124;\Phi^{\dagger}(a)\Phi(b)\left&#124;
    0 \right\rangle} \right&#124;^{2}.") |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ![k(→a,→b) = |<φ(a) |φ(b)>|^2 = |<0|Φ†(a)Φ(b)|0>|^2.](img/file1287.png "k(→a,→b)
    = |<φ(a) |φ(b)>|^2 = |<0|Φ†(a)Φ(b)|0>|^2.") |'
- en: And that is something that we can trivially get from a quantum computer! As
    you can easily check yourself, it is nothing more than the probability of measuring
    all zeros after preparing the state ![\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle](img/file1288.png "\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle"). This follows from the fact that the computational basis is
    orthonormal.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们可以从量子计算机中轻易得到的东西！正如你很容易自己检查的那样，这不过是准备状态 ![Φ†(→a)Φ(→b)|0⟩](img/file1288.png
    "Φ†(→a)Φ(→b)|0⟩") 后测量所有零的概率。这源于计算基是正交归一的事实。
- en: In case you were wondering how to compute the circuit for ![\Phi^{\dagger}](img/file1289.png
    "\Phi^{\dagger}"), notice that this is just the inverse of ![\Phi](img/file1284.png
    "\Phi"), because quantum circuits are always represented by unitary operations.
    But ![\Phi](img/file1284.png "\Phi") will be given by a series of quantum gates.
    So all you need to do is apply the gates in the circuit from right to left and
    invert each of them.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道如何计算 ![Φ†](img/file1289.png "Φ†") 的电路，请注意，这仅仅是 ![Φ](img/file1284.png "Φ")
    的逆，因为量子电路总是由幺正操作表示的。但是 ![Φ](img/file1284.png "Φ") 将由一系列量子门给出。所以你只需要从右到左应用电路中的门，并对每个门进行求逆。
- en: And that is how you implement a quantum kernel function. You take a feature
    map that will return a circuit ![\Phi(\overset{\rightarrow}{x})](img/file1285.png
    "\Phi(\overset{\rightarrow}{x})") for any input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"), you prepare the state ![\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle](img/file1288.png "\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle") for the pair of vectors on which you want to compute the kernel,
    and you return the probability of measuring zero on all the qubits.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是实现量子核函数的方法。你取一个特征映射，它将为任何输入 ![→x](img/file1206.png "→x") 返回一个 ![Φ(→x)](img/file1285.png
    "Φ(→x)") 电路，你为要计算核的向量对准备状态 ![Φ†(→a)Φ(→b)|0⟩](img/file1288.png "Φ†(→a)Φ(→b)|0⟩")，然后你返回测量所有量子位为零的概率。
- en: In case you were concerned, by the way, all quantum kernels, as we have defined
    them, satisfy the conditions needed to qualify as kernel functions [[85](ch030.xhtml#Xschuld2021supervised)].
    In fact, we’ll now ask you to check one of those conditions!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，如果你担心的话，按照我们定义的，所有量子核都满足作为核函数所需的条件 [[85](ch030.xhtml#Xschuld2021supervised)]。事实上，我们现在要求你检查这些条件中的一个！
- en: Exercise 9.2
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 9.2
- en: One of the conditions for a function ![k](img/file317.png "k") to be a kernel
    is that it be symmetric. Prove that, indeed, any quantum kernel is symmetric.
    (![k(\overset{\rightarrow}{a},\overset{\rightarrow}{b}) = k(\overset{\rightarrow}{b},\overset{\rightarrow}{a})](img/file1290.png
    "k(\overset{\rightarrow}{a},\overset{\rightarrow}{b}) = k(\overset{\rightarrow}{b},\overset{\rightarrow}{a})")
    for any inputs.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 ![k](img/file317.png "k") 成为核的一个条件是它必须是对称的。证明任何量子核确实是对称的。([k(→a,→b) = k(→b,→a)](img/file1290.png
    "k(→a,→b) = k(→b,→a)") 对于任何输入)。
- en: Let’s now study how to actually construct those feature maps.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在研究如何实际构建这些特征映射。
- en: 9.2.2 Feature maps
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2.2 特征映射
- en: A feature map, as we have said, is often defined by a parametrized circuit ![\Phi(\overset{\rightarrow}{x})](img/file1285.png
    "\Phi(\overset{\rightarrow}{x})") that depends on the original data and thus can
    be used to prepare a state that depends on it. In this section, we will study
    a few interesting feature maps that we will use throughout the rest of the book.
    They will also serve as examples that will allow us to better illustrate what
    feature maps actually are.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说的，特征图通常由一个参数化的电路 ![\Phi(\overset{\rightarrow}{x})](img/file1285.png "\Phi(\overset{\rightarrow}{x})")
    定义，它依赖于原始数据，因此可以用来制备一个依赖于它的状态。在本节中，我们将研究一些有趣的特征图，这些特征图我们将贯穿整本书。它们也将作为例子，使我们能够更好地说明特征图实际上是什么。
- en: '**Angle encoding** We shall begin with a simple yet powerful feature map known
    as **angle encoding**. When used on an ![n](img/file244.png "n")-qubit circuit,
    this feature map can take up to ![n](img/file244.png "n") numerical inputs ![x_{1},\ldots,x_{n}](img/file1291.png
    "x_{1},\ldots,x_{n}"). The action of its circuit consists in the application of
    a rotation gate on each qubit ![j](img/file258.png "j") parametrized by the value
    ![x_{j}](img/file407.png "x_{j}"). In this feature map, we are using the ![x_{j}](img/file407.png
    "x_{j}") values as angles in the rotations, hence the name of the encoding.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**角度编码** 我们将从一个简单但强大的特征图开始，这个特征图被称为 **角度编码**。当应用于 ![n](img/file244.png "n")
    量子比特电路时，这个特征图可以接受多达 ![n](img/file244.png "n") 个数值输入 ![x_{1},\ldots,x_{n}](img/file1291.png
    "x_{1},\ldots,x_{n}")。其电路的作用是在每个量子比特 ![j](img/file258.png "j") 上应用一个由 ![x_{j}](img/file407.png
    "x_{j}") 的值参数化的旋转门。在这个特征图中，我们使用 ![x_{j}](img/file407.png "x_{j}") 值作为旋转的角度，因此得名编码。'
- en: In angle encoding, we are free to use any rotation gate of our choice. However,
    if we use ![R_{Z}](img/file120.png "R_{Z}") gates and take ![\left| 0 \right\rangle](img/file6.png
    "\left| 0 \right\rangle") to be our initial state…the action of our feature map
    will have no effects whatsoever, as you can easily check from the definition of
    ![R_{Z}](img/file120.png "R_{Z}"). That is why, when ![R_{Z}](img/file120.png
    "R_{Z}") gates are used, it is customary to precede them by Hadamard gates acting
    on each qubit. All this is shown in *Figure* [*9.6*](#Figure9.6).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在角度编码中，我们可以自由地使用我们选择的任何旋转门。然而，如果我们使用 ![R_{Z}](img/file120.png "R_{Z}") 门，并将
    ![\left| 0 \right\rangle](img/file6.png "\left| 0 \right\rangle") 作为我们的初始状态……那么我们的特征图的作用将没有任何效果，这可以从
    ![R_{Z}](img/file120.png "R_{Z}") 的定义中轻松检查出来。这就是为什么，当使用 ![R_{Z}](img/file120.png
    "R_{Z}") 门时，通常会在它们之前使用作用于每个量子比特的哈达玛门。所有这些都在 *图* [*9.6*](#Figure9.6) 中展示。
- en: '![Figure 9.6: Angle encoding for an input (x_{1},\ldots,x_{n}) using different
    rotation gates](img/file1293.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6：使用不同的旋转门对输入 (x_{1},\ldots,x_{n}) 进行角度编码](img/file1293.jpg)'
- en: '**Figure 9.6**: Angle encoding for an input ![(x_{1},\ldots,x_{n})](img/file1292.png
    "(x_{1},\ldots,x_{n})") using different rotation gates'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.6**：使用不同的旋转门对输入 ![(x_{1},\ldots,x_{n})](img/file1292.png "(x_{1},\ldots,x_{n})")
    进行角度编码'
- en: The variables that are fed to the angle encoding feature map should be normalized
    within a certain interval. If they are normalized between ![0](img/file12.png
    "0") and ![4\pi](img/file1294.png "4\pi"), then the data will be mapped to a wider
    region of the feature space than if they were normalized between ![0](img/file12.png
    "0") and ![1](img/file13.png "1"), for example. However, this would come at the
    cost of having the two extrema of the dataset identified under the action of the
    feature map. That’s because 0 and ![2\pi](img/file1295.png "2\pi") are exactly
    the same angle and, in our definition of rotation gates, we divided the input
    angle by ![2](img/file302.png "2").
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到角度编码特征图中的变量应在一定区间内归一化。如果它们在 ![0](img/file12.png "0") 和 ![4\pi](img/file1294.png
    "4\pi") 之间归一化，那么数据将被映射到特征空间的一个更宽的区域，例如，如果它们在 ![0](img/file12.png "0") 和 ![1](img/file13.png
    "1") 之间归一化。然而，这将以在特征图的作用下识别数据集的两个极值为代价。这是因为 0 和 ![2\pi](img/file1295.png "2\pi")
    是完全相同的角，在我们的旋转门定义中，我们将输入角除以 ![2](img/file302.png "2")。
- en: The choice of normalization will thus be a trade-off between separating the
    extrema in the feature space and using the widest possible region in it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，归一化的选择将在特征空间中分离极值和在该空间中使用尽可能宽的区域之间进行权衡。
- en: '**Amplitude encoding** Angle encoding can take ![n](img/file244.png "n") inputs
    on ![n](img/file244.png "n") qubits. Does that seem good enough? Well, get ready
    for a big jump. The **amplitude encoding** feature map can take ![2^{n}](img/file256.png
    "2^{n}") inputs when implemented on an ![n](img/file244.png "n")-qubit circuit.
    That is a lot, and it will enable us to effectively train QSVMs on datasets with
    a large number of variables. So, how does it work then?'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**振幅编码** 角度编码可以在 ![n](img/file244.png "n") 个量子比特上接受 ![n](img/file244.png "n")
    个输入。这看起来足够好吗？好吧，准备好一个大跳跃。当在一个 ![n](img/file244.png "n")-比特电路中实现时，**振幅编码** 特征图可以接受
    ![2^{n}](img/file256.png "2^{n}") 个输入。这很多，并且将使我们能够有效地在具有大量变量的数据集上训练QSVM。那么，它是如何工作的呢？'
- en: If the amplitude encoding feature map is given an input ![x_{0},\ldots,x_{2^{n}
    - 1}](img/file1296.png "x_{0},\ldots,x_{2^{n} - 1}"), it simply prepares the state
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果振幅编码特征图被给定输入 ![x_{0},\ldots,x_{2^{n} - 1}](img/file1296.png "x_{0},\ldots,x_{2^{n}
    - 1}"), 它仅仅准备状态
- en: '| ![\left&#124; {\varphi(\overset{\rightarrow}{a})} \right\rangle = \frac{1}{\sqrt{\sum\limits_{k}x_{k}^{2}}}\sum\limits_{k
    = 0}^{2^{n} - 1}x_{k}\left&#124; k \right\rangle.](img/file1297.png "\left&#124;
    {\varphi(\overset{\rightarrow}{a})} \right\rangle = \frac{1}{\sqrt{\sum\limits_{k}x_{k}^{2}}}\sum\limits_{k
    = 0}^{2^{n} - 1}x_{k}\left&#124; k \right\rangle.") |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| ![\left&#124; {\varphi(\overset{\rightarrow}{a})} \right\rangle = \frac{1}{\sqrt{\sum\limits_{k}x_{k}^{2}}}\sum\limits_{k
    = 0}^{2^{n} - 1}x_{k}\left&#124; k \right\rangle.](img/file1297.png "\left&#124;
    {\varphi(\overset{\rightarrow}{a})} \right\rangle = \frac{1}{\sqrt{\sum\limits_{k}x_{k}^{2}}}\sum\limits_{k
    = 0}^{2^{n} - 1}x_{k}\left&#124; k \right\rangle.") |'
- en: Notice how we’ve had to include a normalization factor to make sure that the
    output was, indeed, a quantum state. Remember from *Chapter* [*1*](ch008.xhtml#x1-180001),
    *Foundations of* *Quantum Computing*, that all quantum states need to be normalized
    vectors! It’s easy to see from the definition that amplitude encoding can work
    for any input except for the zero vector — for the zero vector, amplitude encoding
    is undefined. We can’t divide by zero!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是如何包括一个归一化因子以确保输出确实是一个量子态的。记得从*第* [*1*](ch008.xhtml#x1-180001)，*量子计算基础*，所有量子态都需要是归一化向量！从定义中很容易看出，振幅编码可以适用于任何输入，除了零向量——对于零向量，振幅编码是未定义的。我们不能除以零！
- en: Implementing this feature map in terms of elementary quantum gates is by no
    means simple. If you want all the gory details, you can check the book by Schuld
    and Petruccione [[106](ch030.xhtml#Xschuld)]. Luckily, it is built into most quantum
    computing frameworks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个特征图用基本量子门实现绝非简单。如果你想了解所有细节，可以查阅Schuld和Petruccione的书籍[[106](ch030.xhtml#Xschuld)]。幸运的是，它已经内置在大多数量子计算框架中。
- en: 'By the way, when using amplitude encoding, there is an unavoidable loss of
    information if you decide to ”push the feature map to its limit.” In general,
    you won’t be using all the ![2^{n}](img/file256.png "2^{n}") parameters that it
    offers — you will only use some of them and fill the rest with zeros or any other
    value of your choice. But, if you use all the ![2^{n}](img/file256.png "2^{n}")
    inputs to encode variables, there’s a small issue: that the number of degrees
    of freedom of an ![n](img/file244.png "n")-qubit state is actually ![2^{n} - 1](img/file1298.png
    "2^{n} - 1"), not ![2^{n}](img/file256.png "2^{n}"). This is, in any case, not
    a big deal. This loss of information can be ignored for sufficiently big values
    of ![n](img/file244.png "n").'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，在使用振幅编码时，如果你决定“将特征图推到极限”，那么不可避免地会有信息损失。一般来说，你不会使用它提供的所有 ![2^{n}](img/file256.png
    "2^{n}") 个参数——你只会使用其中的一些，并将剩余的用零或任何其他你选择的价值填充。但是，如果你使用所有 ![2^{n}](img/file256.png
    "2^{n}") 个输入来编码变量，那么会有一个小问题：一个 ![n](img/file244.png "n")-比特态的自由度实际上是 ![2^{n} -
    1](img/file1298.png "2^{n} - 1")，而不是 ![2^{n}](img/file256.png "2^{n}").无论如何，这都不是什么大问题。对于足够大的
    ![n](img/file244.png "n") 值，这种信息损失可以忽略不计。
- en: '**ZZ feature map** Lastly, we will present a known feature map that may bring
    you memories from *Chapter* [*5*](ch013.xhtml#x1-940005), *QAOA: Quantum Approximate
    Optimization* *Algorithm*, where we implemented circuits for Hamiltonians with
    ![Z_{j}Z_{k}](img/file363.png "Z_{j}Z_{k}") terms. It’s called the **ZZ feature
    map**. It is implemented by Qiskit and it can take ![n](img/file244.png "n") inputs
    ![a_{1},\ldots,a_{n}](img/file1299.png "a_{1},\ldots,a_{n}") on ![n](img/file244.png
    "n") qubits, just like angle embedding. Its parametrized circuit is constructed
    following these steps:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Apply a Hadamard gate on each qubit.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply, on each qubit ![j](img/file258.png "j"), a rotation ![R_{Z}(2x_{j})](img/file1300.png
    "R_{Z}(2x_{j})").
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each pair of elements ![\{ j,k\} \subseteq \{ 1,\ldots,n\}](img/file1301.png
    "\{ j,k\} \subseteq \{ 1,\ldots,n\}") with ![j < k](img/file1302.png "j < k"),
    do the following:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a CNOT gate targeting qubit ![k](img/file317.png "k") and controlled by
    qubit ![j](img/file258.png "j").
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply, on qubit ![k](img/file317.png "k"), a rotation ![R_{Z}\left( {2(\pi -
    x_{j})(\pi - x_{k})} \right)](img/file1303.png "R_{Z}\left( {2(\pi - x_{j})(\pi
    - x_{k})} \right)").
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step [3a](#x1-171010x1).
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Figure* [*9.7*](#Figure9.7) you can find a representation of the ZZ feature
    map on three qubits.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: As with angle encoding, normalization plays a big role in the ZZ feature map.
    In order to guarantee a healthy balance between separating the extrema of the
    dataset and using as big a region a possible in the feature space, the variables
    could be normalized to ![\lbrack 0,1\rbrack](img/file1145.png "\lbrack 0,1\rbrack")
    or ![\lbrack 0,3\rbrack](img/file1304.png "\lbrack 0,3\rbrack"), for example.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7: ZZ feature map on three qubits with inputs x_{1},x_{2},x_{3}](img/file1306.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.7**: ZZ feature map on three qubits with inputs ![x_{1},x_{2},x_{3}](img/file1305.png
    "x_{1},x_{2},x_{3}")'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Of course, when designing a quantum feature map, your imagination is the only
    limit. The ones that we have presented here are some of the most popular ones
    — and the ones that you will find in frameworks such as PennyLane and Qiskit —
    but research on quantum feature maps and their properties is an active area. If
    you want to take a look at other possibilities, we can recommend the paper by
    Sim, Johnson, and Aspuru-Guzik [[88](ch030.xhtml#Xsim2019expressibility)].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: But enough theory for now! Let’s put into practice all that we have learned
    by implementing some QSVMs with both PennyLane and Qiskit.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Quantum support vector machines in PennyLane
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It has been a long journey but, finally, we are ready to see QSVMs in action.
    In this section, we are going to train and run a bunch of QSVM models using PennyLane.
    Just to get started, let’s import NumPy and set a seed so that our results are
    reproducible:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 9.3.1 Setting the scene for training a QSVM
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, if we want to train QSVMs, we need some data to work with. In today’s ever-changing
    job market, you should always keep your options open and, as promising as quantum
    machine learning may be, you may want to have a backup career plan. Well, we’ve
    got you covered. Have you ever dreamed of becoming a world-class sommelier? Today
    is your lucky day! (We are just kidding, of course, but we will use this wine
    theme to give some flavor to our example!)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想训练QSVMs，我们需要一些数据来工作。在当今不断变化的就业市场中，你应该始终保持开放的选择，尽管量子机器学习前景广阔，你可能还想有一个备选的职业计划。好吧，我们已经为你准备好了。你有没有想过成为一名世界级的品酒师？今天是你幸运的一天！（我们当然是在开玩笑，但我们将使用这个葡萄酒主题来为我们的示例增添一些风味！）
- en: We’ve already seen how the scikit-learn package offers lots of tools and resources
    for machine learning. It turns out that among them are a collection of pre-defined
    datasets on which to train ML models, and one of those datasets is a ”wine recognition
    dataset” [[32](ch030.xhtml#XDua:2019)]. This is a labeled dataset with information
    about wines. In total, it has ![13](img/file1307.png "13") numeric variables that
    describe the color intensity, alcohol concentration, and other fancy things whose
    meaning and significance we have no clue about. The labels correspond to the kind
    of wine. There are three possible labels, so, if we just ignore one, we are left
    with a beautiful dataset for a binary classification problem.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到scikit-learn包为机器学习提供了大量的工具和资源。结果证明，其中包含一组预定义的数据集，这些数据集可以用于训练机器学习模型，其中之一就是“葡萄酒识别数据集”[[32](ch030.xhtml#XDua:2019)]。这是一个带有葡萄酒信息的标记数据集。总共有![13](img/file1307.png
    "13")个数值变量描述了颜色强度、酒精浓度以及其他我们一无所知含义和重要性的复杂事物。标签对应着葡萄酒的种类。有三个可能的标签，所以如果我们忽略其中一个，我们就会剩下一个非常适合二元分类问题的美好数据集。
- en: 'We can load the set with the `load_wine` function in `sklearn``.``datasets`
    as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`sklearn.datasets`中的`load_wine`函数来加载这个集合，如下所示：
- en: '[PRE1]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have set `return_X_y` to true so that we also get the labels.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将`return_X_y`设置为true，这样我们也会得到标签。
- en: 'You can find all the details about this dataset in its online documentation
    ([https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset)
    or [https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine),
    if you want to check the original source of the data). According to it, the ![59](img/file1308.png
    "59") first elements in the dataset must belong to the first category (label ![0](img/file12.png
    "0")) while the ![71](img/file1309.png "71") subsequent ones have to belong to
    the second one (label ![1](img/file13.png "1")). Thus, if we want to ignore the
    third category, we can just run the following piece of code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在其在线文档中找到关于这个数据集的所有详细信息（[https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset)
    或 [https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)，如果你想检查数据的原始来源）。根据它，数据集中的前![59](img/file1308.png
    "59")个元素必须属于第一个类别（标签![0](img/file12.png "0"）），而后续的![71](img/file1309.png "71")个元素必须属于第二个类别（标签![1](img/file13.png
    "1"））。因此，如果我们想忽略第三个类别，我们只需运行以下代码即可：
- en: '[PRE2]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And thus we have a labeled dataset with two categories. A perfect binary classification
    problem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到了一个包含两个类别的标记数据集。一个完美的二元分类问题。
- en: Before we proceed, however, a few disclaimers are in order. This wine recognition
    problem that we are going to work with is — from a machine learning point of view
    — very simple. You don’t need very sophisticated models or a lot of computing
    power to tackle it. Thus, using a QSVM for this problem is overkill. It will work,
    yes, but that doesn’t diminish the fact that we will be overdoing it. Quantum
    support vector machines can tackle complex problems, but we thought it would be
    better to keep things simple. You may call us overprotective, but we thought that
    using examples that could take two hours to run — or even two days! — might not
    be exactly ideal from a pedagogical perspective. We will also see how some examples
    yield better results than others. Unless we state otherwise, that won’t be indicative
    of any general pattern. It will just mean that it so happens, some things work
    better than others for this particular problem. After all, from the few experiments
    that we will run, it wouldn’t be sensible to draw hard conclusions!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，然而，有一些免责声明是必要的。我们将要处理的这个葡萄酒识别问题——从机器学习的角度来看——是非常简单的。你不需要非常复杂的模型或大量的计算能力来处理它。因此，使用QSVM（量子支持向量机）来解决这个问题是过度杀鸡用牛刀。是的，它会工作，但这并不减少我们过度做这件事的事实。量子支持向量机可以处理复杂问题，但我们认为保持简单会更好。你可以称我们过于保护，但我们认为使用可能需要运行两小时——甚至两天！——的例子，从教学角度来看可能并不完全理想。我们还将看到一些例子比其他例子产生更好的结果。除非我们另有说明，否则这不会表明任何一般模式。这只意味着，对于这个特定问题，某些事情比其他事情工作得更好。毕竟，从我们将要运行的少数实验中，得出硬结论是不明智的！
- en: 'With those remarks out of the way, let’s attack our problem. We shall begin
    by splitting our dataset into a training dataset and a test dataset:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些评论之后，让我们着手解决问题。我们将首先将我们的数据集分成训练数据集和测试数据集：
- en: '[PRE3]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We won’t be making direct model comparisons, nor will we be using validation
    losses, so we will not use a validation dataset.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会进行直接模型比较，也不会使用验证损失，因此我们不会使用验证数据集。
- en: 'As we discussed previously, most feature maps expect our data to be normalized,
    and, regardless of that, normalizing your data is in general a good practice in
    machine learning. So that’s what we shall do now! We will actually use the most
    simple of normalization techniques: scaling each of the variables linearly in
    such a way that the maximum absolute value taken by each variable be ![1](img/file13.png
    "1"). This can be achieved with a `MaxAbsScaler` object from `sklearn``.``preprocessing`
    as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，大多数特征图都期望我们的数据是归一化的，而且，不管怎样，在机器学习中归一化你的数据通常是一个好的实践。所以这就是我们现在要做的！我们将实际上使用最简单的归一化技术：线性缩放每个变量，使得每个变量的最大绝对值取值为![1](img/file13.png
    "1")。这可以通过`sklearn``.``preprocessing`中的`MaxAbsScaler`对象来实现，如下所示：
- en: '[PRE4]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And, with that, we know that — since all our variables were positive — all the
    values in our training dataset will be between ![0](img/file12.png "0") and ![1](img/file13.png
    "1"). If there were negative values, our scaled variables would take values in
    ![\lbrack - 1,1\rbrack](img/file1310.png "\lbrack - 1,1\rbrack") instead. Notice
    that we have only normalized our training dataset. Normalizing the whole dataset
    *simultaneously* would be, in a way, cheating, because we could be polluting the
    training dataset with information from the test dataset. For instance, if we had
    an outlier in the test dataset with a very high value in some variable — a value
    never reached in the training dataset — this would be reflected in the normalization,
    and, thus, the independence of our test dataset could be compromised.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们知道——由于所有我们的变量都是正的——训练数据集中的所有值将在![0](img/file12.png "0")和![1](img/file13.png
    "1")之间。如果有负值，我们的缩放变量将取值在![\lbrack - 1,1\rbrack](img/file1310.png "\lbrack - 1,1\rbrack")之间。请注意，我们只归一化了训练数据集。同时归一化整个数据集在某种程度上是作弊，因为我们可能会用测试数据集的信息污染训练数据集。例如，如果测试数据集中有一个异常值，某个变量的值非常高——这是训练数据集中从未达到的值——这将在归一化中反映出来，从而可能损害测试数据集的独立性。
- en: 'Now that the training dataset is normalized, we need to normalize the test
    dataset using the same proportions as the training dataset. In this way, the training
    dataset receives no information about the test dataset. This can be achieved with
    the following piece of code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练数据集已经归一化，我们需要使用与训练数据集相同的比例来归一化测试数据集。这样，训练数据集就不会收到关于测试数据集的信息。这可以通过以下代码片段实现：
- en: '[PRE5]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice how we have used the same `scaler` object as before, but we have called
    the `transform` method instead of `fit_transform`. In that way, the scaler uses
    the proportions that it saved before. In addition, we’ve run an instruction to
    ”cut” the values in the test dataset at ![0](img/file12.png "0") and ![1](img/file13.png
    "1") — just in case there were some outliers and in order to comply with the normalization
    requirements of some of the feature maps that we will use.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们如何使用与之前相同的 `scaler` 对象，但我们调用的是 `transform` 方法而不是 `fit_transform`。这样，scaler
    使用它之前保存的比例。此外，我们还运行了一个指令来“切割”测试数据集中的值在 ![0](img/file12.png "0") 和 ![1](img/file13.png
    "1") 之间——以防有异常值，并且为了符合我们将使用的某些特征图的归一化要求。
- en: 9.3.2 PennyLane and scikit-learn go on their first date
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3.2 PennyLane 和 scikit-learn 的第一次约会
- en: 'We’ve said it countless times: QSVMs are like normal SVMs, but with a quantum
    kernel. So let’s implement that kernel with PennyLane.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经无数次地说过：QSVMs 就像正常的 SVMs，但有一个量子核。所以让我们用 PennyLane 实现这个核。
- en: Our dataset has ![13](img/file1307.png "13") variables. Using angle encoding
    or the ZZ feature map on the ![13](img/file1307.png "13") variables would require
    us to use ![13](img/file1307.png "13") qubits, which might not be feasible if
    we want our kernel to be simulated on some not especially powerful computers.
    Thus, we can resort to amplitude encoding using ![4](img/file143.png "4") qubits.
    As we mentioned before, this feature map can accept up to ![16](img/file619.png
    "16") inputs; we will fill the remaining ones with zeros — PennyLane will make
    that easy.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集有 ![13](img/file1307.png "13") 个变量。在 ![13](img/file1307.png "13") 个变量上使用角度编码或
    ZZ 特征图将需要我们使用 ![13](img/file1307.png "13") 个量子比特，如果我们想在一些不太强大的计算机上模拟我们的核，这可能不可行。因此，我们可以求助于使用
    ![4](img/file143.png "4") 个量子比特的振幅编码。正如我们之前提到的，这个特征图可以接受多达 ![16](img/file619.png
    "16") 个输入；我们将用零填充剩余的输入——PennyLane 会使这变得容易。
- en: 'This is how we can implement our quantum kernel:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们实现量子核的方法：
- en: '[PRE6]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, there are a few things to digest here. We are first importing PennyLane,
    setting the number of qubits in a variable, and defining a device; nothing new
    there. And then comes the definition of the circuit of our kernel. In this definition,
    we are using `AmplitudeEmbedding`, which returns an operation equivalent to the
    amplitude encoding of its first argument. In our case, we use the arrays `a` and
    `b` for this first argument. They are the classical data that our kernel function
    takes as input. In addition to this, we also ask `AmplitudeEmbedding` to normalize
    each input vector for us, just as amplitude encoding needs us to do, and, since
    our arrays have ![13](img/file1307.png "13") elements instead of the required
    ![16](img/file619.png "16"), we set `pad_with` `=` `0` to fill the remaining values
    with zeros. Also notice that we are using `qml``.``adjoint` to compute the adjoint
    (or inverse) of the amplitude encoding of `b`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这里有几个要点需要消化。我们首先导入 PennyLane，设置一个变量中的量子比特数量，并定义一个设备；没有什么新东西。然后是定义我们的核电路。在这个定义中，我们使用了
    `AmplitudeEmbedding`，它返回一个与其第一个参数等价的操作。在我们的情况下，我们使用数组 `a` 和 `b` 作为这个第一个参数。它们是我们核函数作为输入接受的经典数据。除此之外，我们还要求
    `AmplitudeEmbedding` 为我们归一化每个输入向量，就像振幅编码需要我们做的那样，并且，由于我们的数组有 ![13](img/file1307.png
    "13") 个元素而不是所需的 ![16](img/file619.png "16")，我们将 `pad_with` 设置为 `0` 以用零填充剩余的值。此外，请注意，我们正在使用
    `qml``.``adjoint` 来计算 `b` 的振幅编码的伴随（或逆）。
- en: Lastly, we retrieve an array with the probabilities of measuring each possible
    state in the computational basis. The first element of this array (that is, the
    probability of getting a zero value in all the qubits) will be the output of our
    kernel.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们检索一个包含测量计算基中每个可能状态的概率的数组。这个数组的第一元素（即所有量子比特得到零值的概率）将是我们的核的输出。
- en: Now we have our quantum kernel almost ready. If you’d like to check that the
    circuit works as expected, you can try it out on some elements from the training
    dataset. For instance, you could run `kernel_circ``(``x_tr``[0],` `x_tr``[1])`.
    If the two arguments are the same, keep in mind that you should always get ![1](img/file13.png
    "1") in the first entry of the returned array (which corresponds, as we have mentioned,
    to the output of the kernel).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们几乎准备好了量子核。如果你想要检查电路是否按预期工作，你可以在训练数据集的一些元素上尝试它。例如，你可以运行 `kernel_circ``(``x_tr``[0],
    ` `x_tr``[1])`。如果两个参数相同，请记住，你应该总是得到 ![1](img/file13.png "1") 在返回数组的第一条记录中（正如我们提到的，这对应于核的输出）。
- en: Exercise 9.3
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 9.3
- en: Prove that, indeed, any quantum kernel evaluated on two identical entries always
    needs to return the output ![1](img/file13.png "1").
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 证明，确实，任何在两个相同条目上评估的量子核总是需要返回![1](img/file13.png "1")的输出。
- en: 'Our next step will be using this quantum kernel in an SVM. Our good old scikit-learn
    has its own implementation, `SVC`, of support vector machines, and it works with
    custom kernels, so there we have it! In order to use a custom kernel, you are
    required to provide a `kernel` function accepting two arrays, `A` and `B`, and
    returning a matrix with entries ![(j,k)](img/file356.png "(j,k)") containing the
    kernel applied to `A``[``j``]` and `B``[``k``]`. Once the kernel is prepared,
    the SVM can be trained with the `fit` method. All of this is done in the following
    piece of code:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一步将使用这个量子核在SVM中。我们熟悉的scikit-learn有它自己的支持向量机实现，即`SVC`，并且它支持自定义核，所以这就解决了！为了使用自定义核，你需要提供一个接受两个数组`A`和`B`并返回一个矩阵的`kernel`函数，该矩阵的条目为![(j,k)](img/file356.png
    "(j,k)")，包含对`A[j]`和`B[k]`应用核的结果。一旦核准备好了，就可以使用`fit`方法训练SVM。所有这些都在以下代码片段中完成：
- en: '[PRE7]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The training can take up to a few minutes depending on the performance of your
    computer. Once it is over, you can check the accuracy of your trained model with
    the following instructions:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要几分钟，具体取决于你电脑的性能。一旦完成，你可以使用以下说明检查你训练的模型的准确率：
- en: '[PRE8]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In our case, this gives an accuracy of ![0.92](img/file1311.png "0.92"), meaning
    that the SVM is capable of classifying most of the elements in the test dataset
    correctly.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，这给出了![0.92](img/file1311.png "0.92")的准确率，这意味着支持向量机（SVM）能够正确地分类测试数据集中的大部分元素。
- en: This shows us how to train and run a quantum support vector in a fairly simple
    manner. But we can consider more sophisticated scenarios. Are you ready for that?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了如何以相当简单的方式训练和运行量子支持向量机。但我们可以考虑更复杂的情况。你准备好了吗？
- en: 9.3.3 Reducing the dimensionality of a dataset
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3.3 降低数据集的维度
- en: We have just seen how to use amplitude encoding to take full advantage of the
    ![13](img/file1307.png "13") variables of our dataset while only using ![4](img/file143.png
    "4") qubits. In most cases, that is a good approach. But there are also some problems
    in which it may prove better to reduce the number of variables in the dataset
    — while trying to minimize the loss of information, of course — and thus be able
    to use other feature maps that could perhaps yield better results.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了如何使用振幅编码充分利用数据集的![13](img/file1307.png "13")个变量，同时只使用![4](img/file143.png
    "4")个量子位。在大多数情况下，这是一个好的方法。但也有一些问题，在这些问题中，减少数据集中的变量数量可能证明更好——当然是在尽量减少信息损失的同时——从而能够使用可能产生更好结果的其它特征映射。
- en: In this subsection, we are going to illustrate this approach. We shall try to
    reduce the number of variables in our dataset to ![8](img/file506.png "8") and,
    then, we will train a QSVM on the new, reduced variables using angle encoding.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将展示这种方法。我们将尝试将数据集中的变量数量减少到![8](img/file506.png "8")，然后，我们将使用角度编码在这些新的、减少的变量上训练一个QSVM。
- en: If you want to reduce the dimensionality of a dataset while minimizing information
    loss, as we aim to do now, there are many options at your disposal. You may want
    to have a look at autoencoders, for instance. In any case, for the purposes of
    this section, we will consider a technique known as **principal** **component
    analysis**.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在最小化信息损失的同时降低数据集的维度，正如我们现在要做的，有许多选项可供选择。例如，你可能想看看自动编码器。无论如何，为了本节的目的，我们将考虑一种称为**主成分分析**的技术。
- en: To learn more…
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多…
- en: Before actually using principal component analysis, you may reasonably be curious
    about how this fancy-sounding technique works.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际使用主成分分析之前，你可能合理地好奇这个听起来很高级的技术是如何工作的。
- en: When you have a dataset with ![n](img/file244.png "n") variables, you essentially
    have a set of points in ![R^{n}](img/file1212.png "R^{n}"). With this set, you
    may consider what are known as the **principal** **directions**. The first principal
    direction is the direction of the line that best fits the data as measured by
    the mean squared error. The second principal direction is the direction of the
    line that best fits the data while being orthogonal to the first principal direction.
    This goes on in such a way that the ![k](img/file317.png "k")-th principal direction
    is that of the line that best fits the data while being orthogonal to the first,
    second, and all the way up to the (![k - 1](img/file1312.png "k - 1"))-th principal
    direction.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有一个包含![n](img/file244.png "n")个变量的数据集时，你本质上有一个![R^{n}](img/file1212.png "R^{n}")中的点集。有了这个集合，你可以考虑所谓的**主****方向**。第一个主方向是最佳拟合数据的线的方向，按均方误差来衡量。第二个主方向是最佳拟合数据的线的方向，同时与第一个主方向正交。这样一直进行下去，![k](img/file317.png
    "k")-th主方向是最佳拟合数据的线的方向，同时与第一个、第二个，一直到![k - 1](img/file1312.png "k - 1")-th主方向正交。
- en: We thus may consider an orthonormal basis ![\{ v_{1},\ldots,v_{n}\}](img/file1313.png
    "\{ v_{1},\ldots,v_{n}\}") of ![R^{n}](img/file1212.png "R^{n}") in which ![v_{j}](img/file1314.png
    "v_{j}") points in the direction of the ![j](img/file258.png "j")-th principal
    component. The vectors in this orthonormal basis will be of the form ![v_{j} =
    (v_{j}^{1},\ldots,v_{j}^{n}) \in R^{n}](img/file1315.png "v_{j} = (v_{j}^{1},\ldots,v_{j}^{n})
    \in R^{n}"). Of course, the superscripts are not exponents! They are just superscripts.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以考虑一个![R^{n}](img/file1212.png "R^{n}")的正交基![\{ v_{1},\ldots,v_{n}\}](img/file1313.png
    "\{ v_{1},\ldots,v_{n}\}")，其中![v_{j}](img/file1314.png "v_{j}")指向第![j](img/file258.png
    "j")个主成分的方向。这个正交基中的向量将具有形式![v_{j} = (v_{j}^{1},\ldots,v_{j}^{n}) \in R^{n}](img/file1315.png
    "v_{j} = (v_{j}^{1},\ldots,v_{j}^{n}) \in R^{n}")。当然，上标不是指数！它们只是上标。
- en: When using principal component analysis, we simply compute the vectors of the
    aforementioned basis. And, then, we define the variables
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用主成分分析时，我们只需计算上述基的向量。然后，我们定义变量
- en: '| ![{\overset{\sim}{x}}_{j} = v_{j}^{1}x_{1} + \cdots + v_{j}^{n}x_{n}.](img/file1316.png
    "{\overset{\sim}{x}}_{j} = v_{j}^{1}x_{1} + \cdots + v_{j}^{n}x_{n}.") |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| ![{\overset{\sim}{x}}_{j} = v_{j}^{1}x_{1} + \cdots + v_{j}^{n}x_{n}.](img/file1316.png
    "{\overset{\sim}{x}}_{j} = v_{j}^{1}x_{1} + \cdots + v_{j}^{n}x_{n}.") |'
- en: And, lastly, in order to reduce the dimensionality of our dataset to ![m](img/file259.png
    "m") variables, we just keep the variables ![{\overset{\sim}{x}}_{1},\ldots,{\overset{\sim}{x}}_{m}](img/file1317.png
    "{\overset{\sim}{x}}_{1},\ldots,{\overset{\sim}{x}}_{m}"). This is all done under
    the assumption that the variables ![{\overset{\sim}{x}}_{j}](img/file1318.png
    "{\overset{\sim}{x}}_{j}") are, as we have defined them, sorted in decreasing
    order of relevance towards our problem.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了将我们的数据集维度降低到![m](img/file259.png "m")个变量，我们只需保留变量![{\overset{\sim}{x}}_{1},\ldots,{\overset{\sim}{x}}_{m}](img/file1317.png
    "{\overset{\sim}{x}}_{1},\ldots,{\overset{\sim}{x}}_{m}")。这一切都是在假设变量![{\overset{\sim}{x}}_{j}](img/file1318.png
    "{\overset{\sim}{x}}_{j}")按照我们定义的顺序，按问题相关性的递减顺序排序的情况下进行的。
- en: So how do we use principal component analysis to reduce the number of variables
    in our dataset? Well, scikit-learn is here to save the day. It implements a `PCA`
    class that works in an analogous way to that of the `MaxAbsScaler` class that
    we used before.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何使用主成分分析来减少数据集中变量的数量呢？嗯，scikit-learn就在这里拯救了这一天。它实现了一个`PCA`类，其工作方式与我们之前使用的`MaxAbsScaler`类类似。
- en: 'This `PCA` class comes with a `fit` method that analyzes the data and figures
    out the best way to reduce its dimensionality using principal component analysis.
    Then, in addition, it comes with a `transform` method that can then transform
    any data in the way it learned to do when `fit` was invoked. Also, just like `MaxAbsScaler`,
    the `PCA` class has a `fit_transform` method that fits the data and transforms
    it simultaneously:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`PCA`类包含一个`fit`方法，它分析数据并找出使用主成分分析降低其维度的最佳方式。然后，此外，它还包含一个`transform`方法，可以在调用`fit`时以学习到的方式进行数据转换。同样，就像`MaxAbsScaler`一样，`PCA`类还有一个`fit_transform`方法，它可以同时拟合和转换数据：
- en: '[PRE9]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And, with this, we have effectively reduced the number of variables in our dataset
    to ![8](img/file506.png "8"). Notice, by the way, how we have used the `fit_transform`
    method on the training data and the `transform` method on the test data, all in
    order to preserve the independence of the test dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们实际上已经将数据集中的变量数量减少到了 ![8](img/file506.png "8")。顺便说一下，注意我们是如何在训练数据上使用
    `fit_transform` 方法，在测试数据上使用 `transform` 方法，所有这些都是为了保持测试数据集的独立性。
- en: 'We are now ready to implement and train a QSVM using angle encoding. For this,
    we may use the `AngleEmbedding` operator provided by PennyLane. The following
    piece of code defines the training; it is very similar to our previous kernel
    definition and, thus, pretty self-explanatory:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好使用角度编码来实现和训练一个 QSVM。为此，我们可以使用 PennyLane 提供的 `AngleEmbedding` 操作符。以下代码块定义了训练过程；它与我们的先前内核定义非常相似，因此相当直观：
- en: '[PRE10]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once we have a kernel, we can train a QSVM as we did before, this time reusing
    the `qkernel` function, which will be using the new `kernel_circ` definition:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有一个内核，我们就可以像之前一样训练一个 QSVM，这次重新使用 `qkernel` 函数，它将使用新的 `kernel_circ` 定义：
- en: '[PRE11]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The returned accuracy on the test dataset is ![1](img/file13.png "1"). Just
    a perfect classification in this case.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集上的返回准确率是 ![1](img/file13.png "1")。在这种情况下，分类完美无缺。
- en: 9.3.4 Implementing and using custom feature maps
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3.4 实现和使用自定义特征映射
- en: PennyLane comes with a wide selection of built-in feature maps; you can find
    them all in the online documentation ([https://pennylane.readthedocs.io/en/stable/introduction/templates.html](https://pennylane.readthedocs.io/en/stable/introduction/templates.html)).
    Nevertheless, you may want to define your own. In this subsection, we will train
    a QSVM on the reduced dataset using our own implementation of the ZZ feature map.
    Let’s take feature maps into our own hands!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: PennyLane 随带了许多内置的特征映射；你可以在在线文档中找到它们全部（[https://pennylane.readthedocs.io/en/stable/introduction/templates.html](https://pennylane.readthedocs.io/en/stable/introduction/templates.html)）。尽管如此，你可能想定义自己的。在本节中，我们将使用我们自己的
    ZZ 特征映射实现来在缩减数据集上训练 QSVM。让我们自己动手处理特征映射吧！
- en: 'We can begin by implementing the feature map as a function with the following
    piece of code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从实现以下代码块中的函数作为特征映射开始：
- en: '[PRE12]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this implementation, we have used the `combinations` function from the `itertools`
    module. It takes two arguments: an array `arr` and an integer `l`. And it returns
    an array with all the sorted tuples of length `l` with elements from the array
    `arr`.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，我们使用了 `itertools` 模块中的 `combinations` 函数。它接受两个参数：一个数组 `arr` 和一个整数 `l`。它返回一个数组，包含所有长度为
    `l` 的、由数组 `arr` 中的元素组成的排序元组。
- en: 'Notice how we have written the `ZZFeatureMap` function as we would write any
    circuit, taking advantage of all the flexibility that PennyLane gives us. Having
    defined this function for the ZZ feature map, we may use it on a kernel function
    and then train a QSVM just as we have done before:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是如何编写 `ZZFeatureMap` 函数的，就像编写任何电路一样，充分利用了 PennyLane 给我们的所有灵活性。定义了这个 ZZ 特征映射函数之后，我们可以在内核函数中使用它，然后像之前一样训练一个
    QSVM：
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this case, the test accuracy is ![0.77](img/file1319.png "0.77").
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，测试准确率是 ![0.77](img/file1319.png "0.77")。
- en: There’s one detail to which you should pay attention here, and it is the fact
    that `qml``.``adjoint` is acting on the `ZZFeatureMap` function itself, not on
    its output! Remember that taking the adjoint of a circuit is the same as considering
    the inverse of that circuit.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个细节需要注意，那就是 `qml``.``adjoint` 是作用于 `ZZFeatureMap` 函数本身，而不是其输出！记住，取电路的伴随矩阵等同于考虑该电路的逆。
- en: That’s all we had in store about QSVMs on PennyLane. Now it’s time for us to
    see how things are done in Qiskit Land.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 PennyLane 中的 QSVM，我们已经介绍完毕。现在是我们看看 Qiskit 领域是如何操作的时候了。
- en: 9.4 Quantum support vector machines in Qiskit
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.4 Qiskit 中的量子支持向量机
- en: 'In the previous section, we mastered the use of QSVMs in PennyLane. You may
    want to review *subsection* [*9.3.1*](#x1-1730009.3.1) and the beginning of *subsection*
    [*9.3.3*](#x1-1750009.3.3). That is where we prepare the dataset that we will
    be using here too. In addition to running the code in those subsections, you will
    have to do the following import:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们掌握了在 PennyLane 中使用 QSVM 的方法。你可能想回顾 *子节* [*9.3.1*](#x1-1730009.3.1) 和
    *子节* [*9.3.3*](#x1-1750009.3.3) 的开头。那里我们准备了我们将在这里使用的数据集。除了运行那些子节中的代码外，你还需要执行以下导入：
- en: '[PRE14]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now it’s time for us to switch to Qiskit. In some ways, Qiskit can be easier
    to use than PennyLane — although this is probably a matter of taste. In addition,
    Qiskit will enable us to directly train and run our QSVM models using the real
    quantum computers available at IBM Quantum. Nevertheless, for now, let us begin
    with QSVMs on our beloved Qiskit Aer simulator.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.1 QSVMs on Qiskit Aer
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, let us just import Qiskit:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When we defined a QSVM in PennyLane, we had to ”manually” implement a kernel
    function in order to pass it to scikit-learn. This process is simplified in Qiskit,
    for all it takes to define a quantum kernel is to instantiate a `QuantumKernel`
    object. In the initializer, we are asked to provide a `backend` argument, which
    will be, of course, the backend object on which the quantum kernel will run. By
    default, the feature map that the quantum kernel will use is the ZZ feature map
    with two qubits, but we can use a different feature map by passing a value to
    the `feature_map` object. This value should be a parametrized circuit representing
    the feature map.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining parametrized circuits in Qiskit is actually fairly easy. If you want
    to use an individual parameter in a circuit, you can just import `Parameter` from
    `qiskit``.``circuit` and define a parameter object as `Parameter``(``"``label``"``)`
    with any label of your choice. This object can then be used in quantum circuits.
    For example, we may define a circuit with an ![x](img/file269.png "x")-rotation
    parametrized by a value ![x](img/file269.png "x") as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If you want to use an array of parameters in a circuit, you may define a `ParameterVector`
    object instead. It can also be imported from `qiskit``.``circuit` and, in addition
    to the mandatory label, it accepts an optional `length` argument setting the length
    of the array. By default, this length is set to zero. We may use these parameter
    vector objects as in the following example:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Exercise 9.4
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Define an `AngleEncodingX``(``n` `)` function that return the feature map for
    angle encoding using ![R_{X}](img/file118.png "R_{X}") rotations on `n` qubits.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Using parametrized circuits, we may define any feature map of our choice for
    its use in a quantum kernel; for instance, we could just send any of the `qc`
    objects that we have created in the previous pieces of code as the `feature_map`
    parameter in the `QuantumKernel` constructor. Nevertheless, Qiskit already comes
    with some pre-defined feature maps out of the box. For our case, we may generate
    a circuit for the ZZ feature map on eight qubits using the following piece of
    code:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As a matter of fact, this feature map can be further customized by providing
    additional arguments. We shall use them in the following chapter.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our feature map, we can trivially set up a quantum kernel reliant
    on the Aer simulator as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: And that’s all it takes! By the way, here we are using the Qiskit Machine Learning
    package. Please, refer to *Appendix* [*D*](ch027.xhtml#x1-240000D)*, Installing
    the Tools*, for installation instructions.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部！顺便说一句，我们在这里使用的是Qiskit机器学习包。请参阅*附录*[*D*](ch027.xhtml#x1-240000D)*，安装工具*，以获取安装说明。
- en: If we’d like to train a QSVM model using our freshly-created kernel, we can
    use Qiskit’s own extension of the SVC class provided by scikit-learn. It’s called
    `QSVC` and it can be imported from `quantum_machine_learning``.``algorithms`.
    It works just like the original `SVC` class, but it accepts a `quantum_kernel`
    argument to which we can pass `QuantumKernel` objects.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用我们刚刚创建的核来训练QSVM模型，我们可以使用Qiskit对scikit-learn提供的SVC类的扩展。它被称为`QSVC`，可以从`quantum_machine_learning``.``algorithms`导入。它的工作方式与原始的`SVC`类相同，但它接受一个`quantum_kernel`参数，我们可以传递`QuantumKernel`对象。
- en: 'Thus, these are the instructions that we have to run in order to train a QSVM
    with our kernel:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些是我们必须运行以训练带有我们核的QSVM的指令：
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As with PennyLane, this will take a few minutes to run. Notice, by the way,
    that we have used the reduced dataset (`xs_tr`), because we are using the ZZ feature
    map on 8 qubits.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 与PennyLane一样，这需要几分钟的时间来运行。顺便说一下，请注意，我们使用了减少的数据集（`xs_tr`），因为我们正在使用8个量子比特的ZZ特征图。
- en: 'Once the training is complete, we can get the accuracy on the test dataset
    as we have always done:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们就可以像往常一样在测试数据集上获得准确率：
- en: '[PRE21]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this case, the returned accuracy was ![1](img/file13.png "1").
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，返回的准确率是![1](img/file13.png "1")。
- en: That is all you need to know about how to run QSVMs on the Aer simulator. Now,
    let’s get real.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您需要了解的所有关于如何在Aer模拟器上运行QSVM的信息。现在，让我们来点实际的。
- en: 9.4.2 QSVMs on IBM quantum computers
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4.2 IBM量子计算机上的QSVM
- en: Training and using QSVMs on real hardware with Qiskit couldn’t be easier. We
    will show how it can be done in this subsection.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Qiskit在真实硬件上训练和使用QSVM非常简单。我们将在本小节中展示如何做到这一点。
- en: 'Firstly, as we did back in *Chapter* [*2*](ch009.xhtml#x1-400002), *The Tools
    of the Trade in Quantum* *Computing*, we will load our IBM Quantum account:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，正如我们在*第2章*[*2*](ch009.xhtml#x1-400002)，*量子计算中的工具*中做的那样，我们将加载我们的IBM量子账户：
- en: '[PRE22]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Naturally, for this to work, you should have saved your access token beforehand.
    At the time of writing, free accounts don’t have access to any real quantum devices
    with eight qubits, but there are some with seven qubits. We can select the one
    that is the least busy with the following piece of code:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，为了使这可行，你应该事先保存了你的访问令牌。在撰写本文时，免费账户无法访问任何具有八个量子比特的真实量子设备，但有一些具有七个量子比特。我们可以使用以下代码选择最不繁忙的一个：
- en: '[PRE23]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Of course, we will have to further reduce our data to seven variables, but
    we can do that very easily:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不得不进一步将我们的数据减少到七个变量，但这很容易做到：
- en: '[PRE24]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: And, with this, we have all the ingredients ready to train a QSVM on real hardware!
    We will have to follow the same steps as before — only this time using our real
    device as `quantum_instance` in the instantiation of our quantum kernel!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，有了这些，我们就准备好了所有必要的成分来在真实硬件上训练QSVM！我们不得不遵循与之前相同的步骤——只是这次我们将在量子核的实例化中使用我们的真实设备作为`quantum_instance`！
- en: '[PRE25]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When you execute this code, all the circuit parameters are known in advance.
    For this reason, Qiskit will try to send as many circuits as possible at the same
    time. However, these jobs still have to wait in the queue. Depending on the number
    of points in your dataset and on your access privileges, this may take quite a
    long time to complete!
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行这段代码时，所有电路参数都是预先知道的。因此，Qiskit会尝试同时发送尽可能多的电路。然而，这些作业仍然需要排队等待。根据你的数据集中的点数和你的访问权限，这可能需要相当长的时间才能完成！
- en: With this, we can bring our study of QSVMs in Qiskit to an end.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就可以结束在Qiskit中对QSVM的研究。
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we first learned what support vector machines are, and how
    they can be trained to solve binary classification problems. We began by considering
    vanilla vector machines, and then we introduced the kernel trick — which opened
    up a world of possibilities! In particular, we saw how QSVMs are nothing more
    than a support vector machine with a quantum kernel.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先学习了什么是支持向量机，以及它们如何被训练来解决二元分类问题。我们首先考虑了普通向量机，然后介绍了核技巧——这开辟了一个全新的世界！特别是，我们看到了QSVM实际上只是一个带有量子核的支持向量机。
- en: From there on, we learned how quantum kernels actually work and how to implement
    them. We explored the essential role of feature maps, and discussed a few of the
    most well-known ones.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，我们学习了量子核实际上是如何工作的，以及如何实现它们。我们探讨了特征映射的关键作用，并讨论了几种最著名的映射。
- en: Finally, we learned how to implement, train, and use quantum support vector
    machines with PennyLane and Qiskit. In addition, we were able to very easily run
    QSVMs on real hardware thanks to Qiskit’s interface to IBM Quantum.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何使用PennyLane和Qiskit实现、训练和使用量子支持向量机。此外，得益于Qiskit与IBM Quantum的接口，我们能够非常容易地在真实硬件上运行QSVMs。
- en: And that pretty much covers how QSVMs can help you can identify wines — or solve
    any other classification task — like an expert, all while happily ignoring what
    the ”alkalinity of ash” of a wine is. Who knows? Maybe these SVM models could
    open the door for you to enjoy a bohemian life of wine-tasting! No need to thank
    us.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如此一来，QSVMs如何帮助你像专家一样识别葡萄酒——或者解决任何其他分类任务——就基本涵盖了，同时愉快地忽略葡萄酒的“灰分碱性”是什么。谁知道呢？也许这些SVM模型能为你开启享受波西米亚式葡萄酒品鉴生活的大门！无需感谢我们。
- en: 'In the next chapter, we will consider another family of quantum machine learning
    models: that of quantum neural networks. Things are about to get deep!*'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将考虑另一类量子机器学习模型：量子神经网络。事情即将变得复杂！*
