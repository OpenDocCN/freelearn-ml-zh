- en: Chapter 9
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章
- en: Quantum Support Vector Machines
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 量子支持向量机
- en: '*Artificial Intelligence is the new electricity*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能是新的电力*'
- en: — Andrew Ng
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: — 安德鲁·吴
- en: 'In the previous chapter, we learned the basics of machine learning and we got
    a sneak peek into quantum machine learning. It is now time for us to work with
    our first family of quantum machine learning models: that of **Quantum** **Support
    Vector Machines** (often abbreviated as **QSVM**s). These are very popular models,
    and they are most naturally used in binary classification problems.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了机器学习的基础，并提前了解了量子机器学习。现在是时候开始处理我们的第一个量子机器学习模型家族了：**量子****支持向量机**（通常缩写为**QSVM**）。这些是非常受欢迎的模型，它们最自然地用于二元分类问题。
- en: In this chapter, we shall learn what (classical) support vector machines are
    and how they are used, and we will use this knowledge as a foundation to understand
    quantum support vector machines. In addition, we will explore how to implement
    and train quantum support vector machines with Qiskit and PennyLane.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习什么是（经典）支持向量机以及它们是如何被使用的，并将这些知识作为理解量子支持向量机的基础。此外，我们将探讨如何使用Qiskit和PennyLane实现和训练量子支持向量机。
- en: 'The contents of this chapter are the following:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的内容如下：
- en: Support vector machines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Going quantum
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 走向量子
- en: Quantum support vector machines in PennyLane
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PennyLane中的量子支持向量机
- en: Quantum support vector machines in Qiskit
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiskit中的量子支持向量机
- en: 9.1 Support vector machines
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9.1 支持向量机
- en: 'QSVMs are actually particular cases of **Support Vector Machines** (abbreviated
    as **SVM**s). In this section, we will explore how these SVMs work and how they’re
    used in machine learning. We will do so by first motivating the SVM formalism
    with some simple examples, and then building up from there: all the way up into
    how SVMs can be used to tackle complex classification problems with the kernel
    trick.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: QSVMs实际上是**支持向量机**（简称**SVM**）的特殊情况。在本节中，我们将探讨这些SVM是如何工作的，以及它们在机器学习中的应用。我们将首先通过一些简单的例子来激发SVM公式的动机，然后逐步深入：一直到如何使用核技巧来解决复杂分类问题。
- en: 9.1.1 The simplest classifier you could think of
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1.1 你能想到的最简单的分类器
- en: Let us forget about data for a moment and begin by considering a very naive
    problem. Let’s say that we want to build a very simple classifier on the real
    line. In order to do this, all we have to do is split the real number line into
    two disjoint categories in such a way that any number belong to exactly one of
    these two categories. Thus, if we are given any input (a real number), our classifier
    will return the category to which it belongs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时忘记数据，先考虑一个非常简单的问题。假设我们想在实数线上构建一个非常简单的分类器。为了做到这一点，我们只需要将实数线分成两个不相交的类别，使得任何数都属于这两个类别之一。因此，如果我们给出任何输入（一个实数），我们的分类器将返回它所属的类别。
- en: 'What would be the easiest way in which you could do this? Odds are you would
    first pick a point ![a](img/file16.png "a") and divide the real number line into
    the set (category) of numbers smaller than ![a](img/file16.png "a") and the set
    of numbers larger than ![a](img/file16.png "a"). Then, of course, you would have
    to assign ![a](img/file16.png "a") to one of the two categories, so your categories
    would be either of the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为最简单的方法是什么？很可能是你首先选择一个点![a](img/file16.png "a")，并将实数线分成小于![a](img/file16.png
    "a")的数（类别）和大于![a](img/file16.png "a")的数的集合。当然，你还得将![a](img/file16.png "a")分配给两个类别中的一个，所以你的类别可以是以下两种之一：
- en: The set of real numbers ![x](img/file269.png "x") such that ![x \leq a](img/file1190.png
    "x \leq a") and the set of numbers ![x](img/file269.png "x") such that ![x > a](img/file1191.png
    "x > a")
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实数集![x](img/file269.png "x")，满足![x \leq a](img/file1190.png "x \leq a")，以及实数集![x](img/file269.png
    "x")，满足![x > a](img/file1191.png "x > a")
- en: The set of numbers ![x](img/file269.png "x") such that ![x < a](img/file1192.png
    "x < a") and the set of numbers ![x](img/file269.png "x") such that ![x \geq a](img/file1193.png
    "x \geq a")
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实数集![x](img/file269.png "x")，满足![x < a](img/file1192.png "x < a")，以及实数集![x](img/file269.png
    "x")，满足![x \geq a](img/file1193.png "x \geq a")
- en: Either choice would be reasonable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 任何选择都是合理的。
- en: To learn more…
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多…
- en: Actually, the choice as to in which category to include ![a](img/file16.png
    "a") is, to some extent, meaningless. At the end of the day, if you choose a real
    number at random, the probability that it be exactly ![a](img/file16.png "a")
    is zero. This fun fact is sponsored by probability and measure theory!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，关于将 ![a](img/file16.png "a") 包括在哪个类别中的选择，在某种程度上是没有意义的。最终，如果你随机选择一个实数，它正好是
    ![a](img/file16.png "a") 的概率是零。这个有趣的事实是由概率和测度理论赞助的！
- en: That was easy. Let’s now say that we want to do the same with the real plane
    (the usual ![R^{2}](img/file1194.png "R^{2}")). In this case, a single point will
    not suffice to split it, but we could instead consider a good old line! This is
    exemplified in *Figure* [*9.1*](#Figure9.1). Any line can be used to perfectly
    split the real plane into two categories.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单。现在让我们说，我们想在实平面上（通常的 ![R^{2}](img/file1194.png "R^{2}")）做同样的事情。在这种情况下，一个点不足以将其分割，但我们可以考虑一条古老的直线！这可以在
    *图* [*9.1*](#Figure9.1) 中看到。任何一条直线都可以完美地将实平面分成两类。
- en: '![Figure 9.1: The line \left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right., which can be equivalently written as \left. y = (5\slash 4)x
    \right., can be used to divide the real plane into two disjoint categories, which
    are colored differently. The picture does not specify to which category the line
    belongs ](img/file1197.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1：直线 \left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x} + 0 = 0 \right.，可以等价地写成
    \left. y = (5\slash 4)x \right.，可以用来将实平面分成两个不相交的类别，这些类别被涂成不同的颜色。图片没有指定这条线属于哪个类别
    ](img/file1197.png)'
- en: '**Figure 9.1**: The line ![\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right.](img/file1195.png "\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right."), which can be equivalently written as ![\left. y = (5\slash
    4)x \right.](img/file1196.png "\left. y = (5\slash 4)x \right."), can be used
    to divide the real plane into two disjoint categories, which are colored differently.
    The picture does not specify to which category the line belongs'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9.1**：直线 ![\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x} + 0 =
    0 \right.](img/file1195.png "\left. (5\slash 4, - 1) \cdot \overset{\rightarrow}{x}
    + 0 = 0 \right.")，可以等价地写成 ![\left. y = (5\slash 4)x \right.](img/file1196.png
    "\left. y = (5\slash 4)x \right.")，可以用来将实平面分成两个不相交的类别，这些类别被涂成不同的颜色。图片没有指定这条线属于哪个类别'
- en: If you go back to your linear algebra notes, you may recall that any line in
    the plane can be characterized in terms of a vector ![\overset{\rightarrow}{w}
    \in R^{2}](img/file1198.png "\overset{\rightarrow}{w} \in R^{2}") and a scalar
    ![b \in R](img/file1199.png "b \in R") as the set of points ![\overset{\rightarrow}{x}
    = (x,y)](img/file1200.png "\overset{\rightarrow}{x} = (x,y)") such that ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0"). Of course, we are using ![\cdot](img/file1202.png
    "\cdot") to denote the scalar product (that is, ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{x} = w_{1}x + w_{2}y](img/file1203.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} = w_{1}x + w_{2}y"), provided that ![\overset{\rightarrow}{w}
    = (w_{1},w_{2})](img/file1204.png "\overset{\rightarrow}{w} = (w_{1},w_{2})")).
    The vector ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    defines the **normal**, or perpendicular, direction to the line, and the constant
    ![b](img/file17.png "b") determines the intersection of the line with the ![X](img/file9.png
    "X") and ![Y](img/file11.png "Y") axes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回顾一下你的线性代数笔记，你可能还记得，在平面上任何一条直线都可以用向量 ![\overset{\rightarrow}{w} \in R^{2}](img/file1198.png
    "\overset{\rightarrow}{w} \in R^{2}") 和标量 ![b \in R](img/file1199.png "b \in R")
    来描述，即点集 ![\overset{\rightarrow}{x} = (x,y)](img/file1200.png "\overset{\rightarrow}{x}
    = (x,y)") 满足 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0")。当然，我们使用 ![\cdot](img/file1202.png
    "\cdot") 表示标量积（即 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} = w_{1}x
    + w_{2}y](img/file1203.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    = w_{1}x + w_{2}y")，前提是 ![overset{\rightarrow}{w} = (w_{1},w_{2})](img/file1204.png
    "\overset{\rightarrow}{w} = (w_{1},w_{2})")")). 向量 ![overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}") 定义了直线的**法线**或垂直方向，而常数 ![b](img/file17.png "b") 决定了直线与
    ![X](img/file9.png "X") 和 ![Y](img/file11.png "Y") 轴的交点。
- en: When we worked on the one-dimensional case and used a point to split the real
    line, it was trivial to decide which category any input belonged to. In this case,
    it is slightly more complicated, but not too much. With some elementary geometry,
    you can check that any number ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    will be on one side or the other of the line defined by ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = 0") depending on the sign of the quantity
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b"). That is, if ![\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b](img/file1208.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b") and ![\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{2} + b](img/file1209.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{2} + b") have the same sign (both smaller than
    zero or both greater than zero), we will know that ![{\overset{\rightarrow}{x}}_{1}](img/file1210.png
    "{\overset{\rightarrow}{x}}_{1}") and ![{\overset{\rightarrow}{x}}_{2}](img/file1211.png
    "{\overset{\rightarrow}{x}}_{2}") will belong to the same category. Otherwise,
    we know they will not.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理一维情况并使用一个点来分割实线时，决定任何输入属于哪个类别是显而易见的。在这种情况下，稍微复杂一些，但并不太多。通过一些基本的几何知识，你可以检查出任何数
    ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}") 将位于由
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0](img/file1201.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0") 定义的线的两侧之一，这取决于
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b") 的符号。也就是说，如果 ![\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b](img/file1208.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{1} + b") 和 ![\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{2} + b](img/file1209.png "\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{2} + b") 有相同的符号（两者都小于零或两者都大于零），我们将知道 ![{\overset{\rightarrow}{x}}_{1}](img/file1210.png
    "{\overset{\rightarrow}{x}}_{1}") 和 ![{\overset{\rightarrow}{x}}_{2}](img/file1211.png
    "{\overset{\rightarrow}{x}}_{2}") 将属于同一类别。否则，我们知道它们不会属于同一类别。
- en: There is no reason for us to stop at two dimensions, so let’s kick this up a
    notch and consider an ![n](img/file244.png "n")-dimensional Euclidean space ![R^{n}](img/file1212.png
    "R^{n}"). Just as we split ![R^{2}](img/file1194.png "R^{2}") using a line, we
    could split ![R^{n}](img/file1212.png "R^{n}") using…an (![n - 1](img/file1213.png
    "n - 1"))-dimensional hyperplane! For instance, we could split ![R^{3}](img/file1214.png
    "R^{3}") using an ordinary plane.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理由我们只停留在二维，让我们再提高一个层次，考虑一个 ![n](img/file244.png "n") 维欧几里得空间 ![R^{n}](img/file1212.png
    "R^{n}"). 就像我们用一条线分割 ![R^{2}](img/file1194.png "R^{2}") 一样，我们也可以用…一个 (![n - 1](img/file1213.png
    "n - 1")) 维超平面来分割 ![R^{n}](img/file1212.png "R^{n}"). 例如，我们可以用一个普通平面来分割 ![R^{3}](img/file1214.png
    "R^{3}").
- en: These hyperplanes in ![R^{n}](img/file1212.png "R^{n}") are defined by their
    normal vectors ![\overset{\rightarrow}{w} \in R^{n}](img/file1215.png "\overset{\rightarrow}{w}
    \in R^{n}") and some constants ![b \in R](img/file1199.png "b \in R"). In analogy
    to what we saw in ![R^{2}](img/file1194.png "R^{2}"), their points are the ![\overset{\rightarrow}{x}
    \in R^{n}](img/file1216.png "\overset{\rightarrow}{x} \in R^{n}") that satisfy
    the equations of the form
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![R^{n}](img/file1212.png "R^{n}") 中的这些超平面由它们的法向量 ![\overset{\rightarrow}{w}
    \in R^{n}](img/file1215.png "\overset{\rightarrow}{w} \in R^{n}") 和一些常数 ![b \in
    R](img/file1199.png "b \in R") 定义。类似于我们在 ![R^{2}](img/file1194.png "R^{2}") 中看到的情况，它们的点是满足以下形式的方程的
    ![\overset{\rightarrow}{x} \in R^{n}](img/file1216.png "\overset{\rightarrow}{x}
    \in R^{n}")。'
- en: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.](img/file1217.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.") |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.](img/file1217.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = 0.") |'
- en: Moreover, we can determine to which side of the hyperplane a certain ![\overset{\rightarrow}{x}
    \in R^{n}](img/file1216.png "\overset{\rightarrow}{x} \in R^{n}") is in terms
    of the sign of ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b").
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以根据 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b") 的符号来确定某个 ![\overset{\rightarrow}{x}
    \in R^{n}](img/file1216.png "\overset{\rightarrow}{x} \in R^{n}") 属于超平面的哪一侧。
- en: To learn more…
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多...
- en: In case you are confused with all these equations and you are curious as to
    where they come from, let us quickly explain them. An (affine) hyperplane can
    be defined by a normal vector ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    and by a point ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    in the plane. Thus, a point ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    will belong to the hyperplane if and only if ![\overset{\rightarrow}{x} = \overset{\rightarrow}{p}
    + \overset{\rightarrow}{v}](img/file1219.png "\overset{\rightarrow}{x} = \overset{\rightarrow}{p}
    + \overset{\rightarrow}{v}") for some vector ![\overset{\rightarrow}{v}](img/file1220.png
    "\overset{\rightarrow}{v}") that is orthogonal to ![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}"), that is, such that ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{v} = 0](img/file1221.png "\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{v} = 0"). By combining these two expressions, we know that
    ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}") will
    belong to the hyperplane if and only if ![\overset{\rightarrow}{w} \cdot (\overset{\rightarrow}{x}
    - \overset{\rightarrow}{p}) = 0](img/file1222.png "\overset{\rightarrow}{w} \cdot
    (\overset{\rightarrow}{x} - \overset{\rightarrow}{p}) = 0"), which can be rewritten
    as
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对所有这些方程感到困惑，并且好奇它们从何而来，让我们快速解释一下。一个（仿射）超平面可以通过一个法向量 ![向量w](img/file1205.png
    "向量w") 和平面上的一个点 ![向量p](img/file1218.png "向量p") 来定义。因此，一个点 ![向量x](img/file1206.png
    "向量x") 将属于超平面，当且仅当存在一个向量 ![向量v](img/file1220.png "向量v")，它是 ![向量w](img/file1205.png
    "向量w") 的正交向量，即 ![向量w与向量v的点积等于0](img/file1221.png "向量w与向量v的点积等于0")。通过结合这两个表达式，我们知道
    ![向量x](img/file1206.png "向量x") 将属于超平面，当且仅当 ![向量w与向量x减去向量p的点积等于0](img/file1222.png
    "向量w与向量x减去向量p的点积等于0")，这可以重写为
- en: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + ( - \overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{p}) = \overset{\rightarrow}{x} \cdot \overset{\rightarrow}{w}
    + b = 0,](img/file1223.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + ( - \overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}) = \overset{\rightarrow}{x}
    \cdot \overset{\rightarrow}{w} + b = 0,") |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ![向量w与向量x的点积加上向量w与向量p的点积的相反数等于向量x与向量w的点积加上常数b等于0](img/file1223.png "向量w与向量x的点积加上向量w与向量p的点积的相反数等于向量x与向量w的点积加上常数b等于0")
    |'
- en: where we have implicitly defined ![b = - \overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}](img/file1224.png
    "b = - \overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}").
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们隐式地定义了 ![b等于向量w与向量p的点积的相反数](img/file1224.png "b等于向量w与向量p的点积的相反数")。
- en: Moreover, we have just seen how ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b](img/file1207.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b") is the scalar product of ![\overset{\rightarrow}{x} - \overset{\rightarrow}{p}](img/file1225.png
    "\overset{\rightarrow}{x} - \overset{\rightarrow}{p}") with ![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}"), a fixed normal vector to the plane. This justifies
    why its sign determines on which side of the hyperplane ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}") lies. Remember that, geometrically, the dot product
    of two vectors ![\overset{\rightarrow}{u_{1}} \cdot {\overset{\rightarrow}{u}}_{2}](img/file1226.png
    "\overset{\rightarrow}{u_{1}} \cdot {\overset{\rightarrow}{u}}_{2}") is equal
    to ![\left\| u \right\|_{1} \cdot \left\| u \right\|_{2} \cdot \cos\theta](img/file1227.png
    "\left\| u \right\|_{1} \cdot \left\| u \right\|_{2} \cdot \cos\theta"), where
    ![\theta](img/file89.png "\theta") denotes the smallest angle between them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们刚刚看到![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b](img/file1207.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b")是![\overset{\rightarrow}{x}
    - \overset{\rightarrow}{p}](img/file1225.png "\overset{\rightarrow}{x} - \overset{\rightarrow}{p}")与![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}")的标量积，其中![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}")是平面的一个固定法向量。这解释了为什么它的符号决定了![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}")位于超平面的哪一侧。记住，从几何上讲，两个向量![\overset{\rightarrow}{u_{1}}
    \cdot {\overset{\rightarrow}{u}}_{2}](img/file1226.png "\overset{\rightarrow}{u_{1}}
    \cdot {\overset{\rightarrow}{u}}_{2}")的点积等于![\left\| u \right\|_{1} \cdot \left\|
    u \right\|_{2} \cdot \cos\theta](img/file1227.png "\left\| u \right\|_{1} \cdot
    \left\| u \right\|_{2} \cdot \cos\theta")，其中![\theta](img/file89.png "\theta")表示它们之间的最小角度。
- en: With what we have done so far, we have the tools required to construct (admittedly
    simple) binary classifiers on any Euclidean space. All it takes for us to do so
    is fixing a hyperplane!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止我们所做的一切，我们已经拥有了在任意欧几里得空间上构建（诚然是简单的）二元分类器的工具。我们只需固定一个超平面即可做到这一点！
- en: Why is this important to us? It turns out that support vector machines do exactly
    what we have discussed so far.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们为什么很重要？结果是支持向量机正是我们之前讨论过的。
- en: Important note
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'A support vector machine takes inputs in an ![n](img/file244.png "n")-dimensional
    Euclidean space (![R^{n}](img/file1212.png "R^{n}")) and classifies them according
    to which side of a hyperplane they are on. This hyperplane fully defines the behavior
    of the SVM. Of course, the adjustable parameters of an SVM are the ones that define
    the hyperplane: following our notation, the components of the normal vector ![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}") and the constant ![b](img/file17.png "b").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机在![n](img/file244.png "n")维欧几里得空间([![R^{n}](img/file1212.png "R^{n}")))中接受输入，并根据它们位于超平面的哪一侧进行分类。这个超平面完全定义了SVM的行为。当然，SVM的可调整参数是定义超平面的那些参数：按照我们的符号，法向量![\overset{\rightarrow}{w}](img/file1205.png
    "\overset{\rightarrow}{w}")的分量和常数![b](img/file17.png "b")。
- en: In order to get the label of any point ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"), all we have to do is look at the sign of ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b](img/file1207.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b").
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到任意点![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")的标签，我们只需查看![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b](img/file1207.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b")的符号。
- en: 'As you may have suspected, vanilla SVMs, just on their own, are not the most
    powerful of binary classification models: they are intrinsically linear and they
    are not fit to capture sophisticated patterns. We will take care of this later
    in the chapter when we unleash the full potential of SVMs with ”the kernel trick”
    (stay tuned!). In any case, for now, let us rejoice in the simplicity of our model
    and let’s learn how to train it.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经怀疑的那样，纯支持向量机（vanilla SVMs）本身并不是最强大的二元分类模型：它们本质上是线性的，并且不适合捕捉复杂的模式。我们将在本章后面通过“核技巧”释放SVMs的全部潜力时解决这个问题（敬请期待！）无论如何，现在让我们为我们的模型简单性感到高兴，并学习如何训练它。
- en: '9.1.2 How to train support vector machines: the hard-margin case'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1.2 如何训练支持向量机：硬间隔情况
- en: Let’s say that we have a binary classification problem, and we are given some
    training data consisting of datapoints in ![R^{n}](img/file1212.png "R^{n}") together
    with their corresponding labels. Naturally, when we train an SVM for this problem,
    we want to look for the hyperplane that best separates the two categories in the
    training dataset. Now we have to make this intuitive idea precise.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个二元分类问题，并且我们得到了一些训练数据，这些数据包括![R^{n}](img/file1212.png "R^{n}")中的数据点以及它们相应的标签。自然地，当我们为这个问题训练SVM时，我们希望寻找在训练数据集中最好地分离两个类别的超平面。现在我们必须使这个直观的想法变得精确。
- en: Let the datapoints in our training dataset be ![{\overset{\rightarrow}{x}}_{j}
    \in R^{n}](img/file1228.png "{\overset{\rightarrow}{x}}_{j} \in R^{n}") and their
    expected labels be ![y_{j} = 1, - 1](img/file1229.png "y_{j} = 1, - 1") (read
    as positive and negative, respectively). For now, we will assume that our data
    can be perfectly separated by a hyperplane. Later in the section, we will see
    what to do when this is not the case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练数据集中的数据点为 ![{\overset{\rightarrow}{x}}_{j} \in R^{n}](img/file1228.png
    "{\overset{\rightarrow}{x}}_{j} \in R^{n}")，它们的预期标签为 ![y_{j} = 1, - 1](img/file1229.png
    "y_{j} = 1, - 1")（分别读作正和负）。目前，我们假设我们的数据可以被一个超平面完美地分开。在后面的章节中，我们将看到当这种情况不成立时应该怎么做。
- en: 'Notice that, under the assumption that there is at least one hyperplane separating
    our data, there will necessarily be an infinite number of such separating hyperplanes
    (see *Figure* [*9.2*](#Figure9.2)). Will any of them be suitable for our goal
    of building a classifier? If we only cared about the training data, then yes,
    any of them would do the trick. In fact, this is exactly what the perceptron model
    that we discussed in *Chapter* *[*8*](ch017.xhtml#x1-1390008), *What is Quantum
    Machine* *Learning?*, does: it just looks for a hyperplane separating the training
    data.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在假设至少存在一个可以分离我们的数据的超平面的情况下，必然会有无限多个这样的分离超平面（参见*图* [*9.2*](#Figure9.2)）。它们中的任何一个都适合我们的目标，即构建一个分类器吗？如果我们只关心训练数据，那么是的，任何一个都可以做到这一点。事实上，这正是我们在*第*
    *8* *章* *“什么是量子机器学习”*中讨论的感知器模型所做的事情：它只是寻找一个可以分离训练数据的超平面。
- en: '*However, as you surely remember, when we train a classifier, we are interested
    in getting a low generalization error. In our case, one way of trying to achieve
    this is by looking for a separating hyperplane that can maximize the distance
    from itself to the training datapoints. And that is the way in which SVMs are
    actually trained. The rationale behind this is clear: we expect the new, unseen
    datapoints to follow a similar distribution to the one that we have seen in the
    training data. So it is very likely that new examples of one class will be closer
    to training examples of that same class. Therefore, if our separating hyperplane
    is too close to one of the training datapoints, we risk another datapoint of the
    same class crossing to the other side of the hyperplane and being misclassified.
    For instance, in *Figure* [*9.2*](#Figure9.2), the dashed line does separate the
    training datapoints, but it is certainly a much more risky choice than, for example,
    the continuous line.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而，正如你肯定记得的，当我们训练一个分类器时，我们感兴趣的是获得低泛化误差。在我们的情况下，尝试实现这一目标的一种方法是在寻找一个可以最大化其自身与训练数据点之间距离的分离超平面。这正是SVMs实际训练的方式。背后的原理是清晰的：我们期望新的、未见过的数据点遵循我们在训练数据中看到的类似分布。因此，新的一类例子很可能比同一类的训练例子更接近。因此，如果我们的分离超平面离某个训练数据点太近，我们就有风险让同一类的另一个数据点穿过超平面的另一边并被错误分类。例如，在*图*
    [*9.2*](#Figure9.2)中，虚线确实分离了训练数据点，但它肯定比例如连续线更冒险的选择。'
- en: '![Figure 9.2: Both lines (hyperplanes) separate the two categories, but the
    continuous line is closer to the datapoints than the dashed line ](img/file1230.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 9.2: 两条线（超平面）都分离了两个类别，但连续线比虚线更接近数据点](img/file1230.jpg)'
- en: '**Figure 9.2**: Both lines (hyperplanes) separate the two categories, but the
    continuous line is closer to the datapoints than the dashed line'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.2**：两条线（超平面）都分离了两个类别，但连续线比虚线更接近数据点'
- en: 'The idea behind the training of an SVM is then clear: we seek to find not just
    any separating hyperplane, but one that is as far away from the training points
    as possible. This may seem difficult to achieve, but it can be posed as a rather
    straightforward optimization problem. Let’s explain how to do it in a little bit
    more detail.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后SVM训练背后的思想就清晰了：我们寻求的不仅仅是一个分离超平面，而是一个尽可能远离训练点的超平面。这看起来可能很难实现，但它可以表述为一个相当直接的优化问题。让我们更详细地解释一下如何实现它。
- en: In a first approach, we could just consider the distance from a separating hyperplane
    ![H](img/file10.png "H") to all the points in the training dataset, and then try
    to find a way to tweak ![H](img/file10.png "H") in order to maximize that distance
    while making sure that ![H](img/file10.png "H") still separates the data properly.
    This is, however, not the best way to present the problem. Instead, we may notice
    how we can associate to each data point a unique hyperplane that is parallel to
    ![H](img/file10.png "H") and contains that datapoint. And, what is more, the parallel
    hyperplane that goes through the point that is closest to ![H](img/file10.png
    "H") will itself be a separating hyperplane — and so will be its reflection over
    ![H](img/file10.png "H"). This is illustrated in *Figure* [*9.3*](#Figure9.3).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种方法中，我们只需考虑分离超平面![H](img/file10.png "H")到训练数据集中所有点的距离，然后尝试找到一种方法来调整![H](img/file10.png
    "H")，以最大化这个距离，同时确保![H](img/file10.png "H")仍然正确地分离数据。然而，这并不是展示问题的最佳方式。相反，我们可能会注意到，我们可以将每个数据点与一个唯一的、与![H](img/file10.png
    "H")平行的超平面关联起来，该超平面包含该数据点。更重要的是，穿过与![H](img/file10.png "H")最近的点的平行超平面本身也将是一个分离超平面——其反射在![H](img/file10.png
    "H")上也将是。这如图*9.3*所示。
- en: '![Figure 9.3: The continuous black line represents a separating hyperplane
    H. One of the dashed lines is the parallel hyperplane that goes through the closest
    point to H, and its reflection over H is the other dashed line ](img/file1231.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3：连续的黑色线条代表分离超平面H。其中一条虚线是穿过H最近的点的平行超平面，其在H上的反射是另一条虚线](img/file1231.jpg)'
- en: '**Figure 9.3**: The continuous black line represents a separating hyperplane
    ![H](img/file10.png "H"). One of the dashed lines is the parallel hyperplane that
    goes through the closest point to ![H](img/file10.png "H"), and its reflection
    over ![H](img/file10.png "H") is the other dashed line'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9.3**：连续的黑色线条代表分离超平面![H](img/file10.png "H")。其中一条虚线是穿过![H](img/file10.png
    "H")最近的点的平行超平面，其在![H](img/file10.png "H")上的反射是另一条虚线'
- en: This pair of hyperplanes — the parallel plane that goes through the closest
    point and its reflection — will be the two equidistant parallel hyperplanes, which
    are the furthest apart from each other while still separating the data. They are
    unique to ![H](img/file10.png "H"). The distance between them is known as the
    **margin** and it is what we aim to maximize. This is illustrated in *Figure*
    [*9.4*](#Figure9.4).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这对超平面——穿过最近点的平行平面及其反射——将是两个等距的平行超平面，它们彼此之间距离最远，同时仍然分离数据。它们是![H](img/file10.png
    "H")独有的。它们之间的距离被称为**间隔**，这是我们试图最大化的目标。这如图*9.4*所示。
- en: We already know that any separating hyperplane ![H](img/file10.png "H") can
    be characterized by an equation of the form ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0"). Moreover, any hyperplane that is parallel to ![H](img/file10.png "H")
    — in particular those that define the margin! — can be characterized as ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = C](img/file1232.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = C") for some constant ![C](img/file234.png
    "C"). And not only that, but their reflection over ![H](img/file10.png "H") will
    be itself characterized by the equation ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - C](img/file1233.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = - C"). Hence, we know that, for some constant ![C](img/file234.png "C"),
    the hyperplanes that define the margin of ![H](img/file10.png "H") can be represented
    by the equations ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b
    = \pm C](img/file1234.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = \pm C").
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，任何分离超平面 ![H](img/file10.png "H") 都可以用形式为 ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{x} + b = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{x} + b = 0") 的方程来描述。此外，任何平行于 ![H](img/file10.png "H") 的超平面——特别是那些定义边界的超平面——都可以用
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = C](img/file1232.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = C") 来描述，其中 ![C](img/file234.png
    "C") 是某个常数。不仅如此，它们在 ![H](img/file10.png "H") 上的反射也将由方程 ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - C](img/file1233.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - C") 来描述。因此，我们知道，对于某个常数 ![C](img/file234.png
    "C")，定义 ![H](img/file10.png "H") 边界的超平面可以用方程 ![\overset{\rightarrow}{w} \cdot
    \overset{\rightarrow}{x} + b = \pm C](img/file1234.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm C") 来表示。
- en: Nevertheless, there is nothing preventing us here from dividing the whole expression
    by ![C](img/file234.png "C"). So, if we let ![\left. \overset{\sim}{w} = \overset{\rightarrow}{w}\slash
    C \right.](img/file1235.png "\left. \overset{\sim}{w} = \overset{\rightarrow}{w}\slash
    C \right.") and ![\left. \overset{\sim}{b} = b\slash C \right.](img/file1236.png
    "\left. \overset{\sim}{b} = b\slash C \right."), we know that the hyperplane ![H](img/file10.png
    "H") will still be represented by ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0](img/file1237.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0"), but the hyperplanes that define the margin will be
    characterized by
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们在这里并没有阻止我们将整个表达式除以 ![C](img/file234.png "C")。因此，如果我们让 ![\left. \overset{\sim}{w}
    = \overset{\rightarrow}{w}\slash C \right.](img/file1235.png "\left. \overset{\sim}{w}
    = \overset{\rightarrow}{w}\slash C \right.") 和 ![\left. \overset{\sim}{b} = b\slash
    C \right.](img/file1236.png "\left. \overset{\sim}{b} = b\slash C \right.")，我们知道超平面
    ![H](img/file10.png "H") 仍然可以用 ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0](img/file1237.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x}
    + \overset{\sim}{b} = 0") 来表示，但定义边界的超平面将用以下方程来描述：
- en: '| ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b} =
    \pm 1,](img/file1238.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b}
    = \pm 1,") |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ![\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b} =
    \pm 1,](img/file1238.png "\overset{\sim}{w} \cdot \overset{\rightarrow}{x} + \overset{\sim}{b}
    = \pm 1,") |'
- en: which looks much more neat!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来要整洁得多！
- en: 'Let’s summarize what we have. We want to find a hyperplane that, while separating
    the data properly, maximizes the distance to the points in the training dataset.
    We have seen how we can see this as the problem of finding a hyperplane that maximizes
    the margin: the distance between the two equidistant parallel hyperplanes that
    are the furthest away from each other while still separating the data. And we
    have just proven that, for any separating hyperplane, we can always find some
    values of ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    and ![b](img/file17.png "b") such that those hyperplanes that define the margin
    can be represented as'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们已经学到的内容。我们希望找到一个超平面，它不仅能够正确地分离数据，而且还要最大化到训练数据集中点的距离。我们已经看到，我们可以将这个问题看作是寻找一个最大化边界的超平面：两个等距平行超平面之间的距离最大，同时仍然能够分离数据。我们刚刚已经证明，对于任何分离超平面，我们总能找到一些
    ![w](img/file1205.png "w") 和 ![b](img/file17.png "b") 的值，使得那些定义边界的超平面可以用以下形式表示：
- en: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.](img/file1239.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.") |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.](img/file1239.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1.") |'
- en: It can be shown that the distance between these two hyperplanes is ![\left.
    2\slash\left\| w \right\| \right.](img/file1240.png "\left. 2\slash\left\| w \right\|
    \right."). Hence the problem of maximizing the margin can be equivalently stated
    as the problem of maximizing ![\left. 2\slash\left\| w \right\| \right.](img/file1240.png
    "\left. 2\slash\left\| w \right\| \right.") subject to the constraint that the
    planes ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1](img/file1241.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \pm 1") properly
    separate the data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明这两个超平面之间的距离是 ![\left. 2\slash\left\| w \right\| \right.](img/file1240.png
    "\left. 2\slash\left\| w \right\| \right.")。因此，最大化间隔的问题可以等价地表述为在约束条件 ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1](img/file1241.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1") 正确分离数据的情况下，最大化 ![\left. 2\slash\left\|
    w \right\| \right.](img/file1240.png "\left. 2\slash\left\| w \right\| \right.")
    的问题。
- en: Exercise 9.1
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 9.1
- en: Show that, as we claimed, the distance between the hyperplanes ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1](img/file1241.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = \pm 1") is ![\left. 2\slash\left\| w \right\|
    \right.](img/file1240.png "\left. 2\slash\left\| w \right\| \right.").
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 证明，正如我们所声称的，超平面 ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} +
    b = \pm 1](img/file1241.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = \pm 1") 之间的距离是 ![\left. 2\slash\left\| w \right\| \right.](img/file1240.png
    "\left. 2\slash\left\| w \right\| \right.")。
- en: Let’s now consider an arbitrary element ![\overset{\rightarrow}{p} \in R^{N}](img/file1242.png
    "\overset{\rightarrow}{p} \in R^{N}") and a hyperplane ![H](img/file10.png "H")
    characterized by ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b
    = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0"). When the value of ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}
    + b](img/file1243.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{p}
    + b") is zero, we know that ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    is in the hyperplane and, as this value drifts away from zero, the point gets
    further and further away from the hyperplane. If it increases and it is between
    ![0](img/file12.png "0") and ![1](img/file13.png "1"), the point ![\overset{\rightarrow}{p}](img/file1218.png
    "\overset{\rightarrow}{p}") is between the hyperplane ![H](img/file10.png "H")
    and the hyperplane ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} +
    b = 1](img/file1244.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 1"). When this value reaches ![1](img/file13.png "1"), the point is in this
    latter hyperplane. And when the value becomes greater than ![1](img/file13.png
    "1"), it moves beyond both hyperplanes. Analogously, if this value decreases and
    it is between ![0](img/file12.png "0") and ![- 1](img/file312.png "- 1"), the
    point ![\overset{\rightarrow}{p}](img/file1218.png "\overset{\rightarrow}{p}")
    is between the hyperplane ![H](img/file10.png "H") and ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - 1](img/file1245.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b = - 1"). When the value reaches ![- 1](img/file312.png
    "- 1"), the point is in this last hyperplane. And when it is smaller than ![-
    1](img/file312.png "- 1"), it has moved beyond both ![H](img/file10.png "H") and
    ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = - 1](img/file1245.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = - 1").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Since we are working under the assumption that there are no points inside the
    margin, the hyperplane ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0](img/file1201.png "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x}
    + b = 0") will properly separate the data if, for all the positive entries, ![\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b \geq 1](img/file1246.png "\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b \geq 1"), while all the negative ones will
    satisfy ![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b \leq - 1](img/file1247.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b \leq - 1"). We can
    write this condition as
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '| ![y_{j}\left( {\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j}}
    \right) \geq 1,](img/file1248.png "y_{j}\left( {\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j}} \right) \geq 1,") |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
- en: because we are considering ![y_{j} = 1](img/file1249.png "y_{j} = 1") when the
    ![j](img/file258.png "j")-th example belongs to the positive class and ![y_{j}
    = - 1](img/file1250.png "y_{j} = - 1") when it belongs to the negative one.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4: The hyperplane that could have been returned by an SVM is represented
    by a black continuous line, and the lines in dashed lines are the equidistant
    parallel hyperplanes that are the furthest apart from each other while still separating
    the data. The margin is thus half of the thickness of the colored region ](img/file1251.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.4**: The hyperplane that could have been returned by an SVM is represented
    by a black continuous line, and the lines in dashed lines are the equidistant
    parallel hyperplanes that are the furthest apart from each other while still separating
    the data. The margin is thus half of the thickness of the colored region'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'For all this, the problem of finding the hyperplane that best separates the
    data can be posed as the following optimization problem:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\text{Minimize}\quad} & {\left\| w \right\|\qquad} &
    & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j},
    + b) \geq 1,\qquad} & & \qquad \\ \end{array}](img/file1252.png "\begin{array}{rlrl}
    {\text{Minimize}\quad} & {\left\| w \right\|\qquad} & & \qquad \\ {\text{subject~to}\quad}
    & {y_{j}(\overset{\rightarrow}{w} \cdot {\overset{\rightarrow}{x}}_{j}, + b) \geq
    1,\qquad} & & \qquad \\ \end{array}")'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: where, of course, each ![j](img/file258.png "j") defines an individual constraint.
    This formulation suffers from a small problem. The Euclidean norm is nice, visual,
    and geometric, but it has a square root. We personally have nothing against square
    roots — some of our best friends *are* square roots — but most optimization algorithms
    have some hard feelings against them. So just to make life easier for us, we may
    instead consider the following (equivalent) problem.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'If the data in the training dataset can be separated by a hyperplane, the problem
    of training an SVM can be posed as the following optimization problem:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1.\qquad} & & \qquad \\ \end{array}](img/file1253.png
    "\begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1.\qquad} & & \qquad \\ \end{array}")'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: This is known as **hard-margin** training, because we are allowing no elements
    in the training dataset to be misclassified or even to be inside the margin.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: That nice and innocent square will save us from so many troubles. Notice, by
    the way, that we’ve introduced a ![\left. 1\slash 2 \right.](img/file136.png "\left.
    1\slash 2 \right.") factor next to ![\left\| w \right\|^{2}](img/file1254.png
    "\left\| w \right\|^{2}"). That’s for reasons of technical convenience, but it
    isn’t really important.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: With hard-margin training, we need our training data to be perfectly separable
    by a hyperplane because, otherwise, we will not find any feasible solutions to
    the optimization problem that we have just defined. This scenario is, in most
    situations, too restrictive. Thankfully, we can take an alternative approach known
    as **soft-margin training**.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.3 Soft-margin training
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Soft-margin training is similar to hard-margin training. The only difference
    is that it also incorporates some adjustable **slack**, or ”tolerance,” parameters
    ![\xi_{j} \geq 0](img/file1255.png "\xi_{j} \geq 0") that will add flexibility
    to the constraints. In this way, instead of considering the constraint ![y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1](img/file1256.png "y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1"), we will use
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '| ![y_{j}(w \cdot x_{j} + b) \geq 1 - \xi_{j}.](img/file1257.png "y_{j}(w \cdot
    x_{j} + b) \geq 1 - \xi_{j}.") |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: Thus, when ![\xi_{j} > 0](img/file1258.png "\xi_{j} > 0"), we will allow ![{\overset{\rightarrow}{x}}_{j}](img/file1259.png
    "{\overset{\rightarrow}{x}}_{j}") to be close to the hyperplane or even on the
    wrong side of the space (as separated by the hyperplane). What is more, the bigger
    the value of ![\xi_{j}](img/file1260.png "\xi_{j}"), the further into the wrong
    side ![{\overset{\rightarrow}{x}}_{j}](img/file1259.png "{\overset{\rightarrow}{x}}_{j}")
    will be.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, we would like these ![\xi_{j}](img/file1260.png "\xi_{j}") to be
    as small as possible, so we need to include them in the cost function that we
    want to minimize. Taking all of this into account, the optimization problem that
    we shall consider in soft-margin training will be the following.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'A support vector machine that may not be *necessarily* able to properly separate
    the training data with a hyperplane can be trained by solving the following optimization
    problem:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2}
    + C\sum\limits_{j}\xi_{j}\qquad} & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w}
    \cdot {\overset{\rightarrow}{x}}_{j} + b) \geq 1 - \xi_{j},\qquad} & & \qquad
    \\ & {\xi_{j} \geq 0.\qquad} & & \qquad \\ \end{array}](img/file1261.png "\begin{array}{rlrl}
    {\text{Minimize}\quad} & {\frac{1}{2}\left\| w \right\|^{2} + C\sum\limits_{j}\xi_{j}\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {y_{j}(\overset{\rightarrow}{w} \cdot
    {\overset{\rightarrow}{x}}_{j} + b) \geq 1 - \xi_{j},\qquad} & & \qquad \\  &
    {\xi_{j} \geq 0.\qquad} & & \qquad \\ \end{array}")'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: The value ![C > 0](img/file1262.png "C > 0") is a hyperparameter that can be
    chosen at will. The bigger ![C](img/file234.png "C") is, the less tolerant we
    will be to training examples falling inside the margin or on the wrong side of
    the hyperplane.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: This formulation is known as **soft-margin training** of an SVM.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Let us now try to digest this formulation. As expected, we also made the ![\xi_{j}](img/file1260.png
    "\xi_{j}") contribute to our cost function, in such a way that their taking large
    values will be penalized. In addition, we’ve incorporated this ![C](img/file234.png
    "C") constant and said that it can be tweaked at will. As we mentioned before,
    in broad terms, the bigger it is, the more unwilling we will be to accept misclassified
    elements in the training dataset. Actually, if there is a hyperplane that can
    perfectly separate the data, setting ![C](img/file234.png "C") to a huge value
    would be equivalent to doing hard-margin training. At first, it might seem tempting
    to make ![C](img/file234.png "C") huge, but this would make our model more prone
    to overfitting. Perfect fits are not that good! Balancing the value of ![C](img/file234.png
    "C") is one of the many keys behind successful SVM training.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: When we train an SVM, the actual loss function that we would like to minimize
    is
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '| ![L(\overset{\rightarrow}{w},b;\overset{\rightarrow}{x},y) = \max\{ 0,1 -
    y(\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b)\},](img/file1263.png
    "L(\overset{\rightarrow}{w},b;\overset{\rightarrow}{x},y) = \max\{ 0,1 - y(\overset{\rightarrow}{w}
    \cdot \overset{\rightarrow}{x} + b)\},") |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: which is called the **hinge loss**. In fact, our ![\xi_{j}](img/file1260.png
    "\xi_{j}") variables are direct representatives of that loss. Minimizing the expected
    value of this loss function would be connected to minimizing the proportion of
    misclassified elements — which is what we want at the end of the day.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: If, in our formulation, we didn’t have the ![\left. \left\| w \right\|^{2}\slash
    2 \right.](img/file1264.png "\left. \left\| w \right\|^{2}\slash 2 \right.") factor,
    that would be the training loss that we would be minimizing. We included this
    factor, however, because a small ![\left\| w \right\|^{2}](img/file1254.png "\left\|
    w \right\|^{2}") (that is, a large margin) makes SVM models more robust against
    overfitting.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: We will conclude this analysis of soft-margin training by presenting an equivalent
    formulation of its optimization problem. This formulation is known as the **Lagrangian
    dual** of the optimization problem that we presented previously. We will not discuss
    why these two formulations are equivalent, but you can take our word for it —
    or you can check the wonderful explanation by Abu-Mostafa, Magdon-Ismail, and
    Lin [[2](ch030.xhtml#Xabu2012blearning)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'The soft-margin training problem can be equivalently written in terms of some
    optimizable parameters ![\alpha_{j}](img/file1265.png "\alpha_{j}") as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![\begin{array}{rlrl} {\text{Maximize}\quad} & {\sum\limits_{j}\alpha_{j} -
    \frac{1}{2}\sum\limits_{j,k}y_{j}y_{k}\alpha_{j}\alpha_{k}\left( {{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}} \right),\qquad} & & \qquad \\ {\text{subject~to}\quad}
    & {0 \leq \alpha_{j} \leq C,\qquad} & & \qquad \\ & {\sum\limits_{j}\alpha_{j}y_{j}
    = 0.\qquad} & & \qquad \\ \end{array}](img/file1266.png "\begin{array}{rlrl} {\text{Maximize}\quad}
    & {\sum\limits_{j}\alpha_{j} - \frac{1}{2}\sum\limits_{j,k}y_{j}y_{k}\alpha_{j}\alpha_{k}\left(
    {{\overset{\rightarrow}{x}}_{j} \cdot {\overset{\rightarrow}{x}}_{k}} \right),\qquad}
    & & \qquad \\ {\text{subject~to}\quad} & {0 \leq \alpha_{j} \leq C,\qquad} & &
    \qquad \\  & {\sum\limits_{j}\alpha_{j}y_{j} = 0.\qquad} & & \qquad \\ \end{array}")'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: This formulation of the SVM soft-margin training problem is, most of the time,
    easier to solve in practice, and it is the one that we will be working with. Once
    we obtain the ![\alpha_{j}](img/file1265.png "\alpha_{j}") values, it is also
    possible to go back to the original formulation. In fact, from the ![\alpha_{j}](img/file1265.png
    "\alpha_{j}") values, we can recover ![b](img/file17.png "b") and ![w](img/file1267.png
    "w"). For instance, it holds that
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![\overset{\rightarrow}{w} = \sum\limits_{j}\alpha_{j}y_{j}\overset{\rightarrow}{x_{j}}.](img/file1268.png
    "\overset{\rightarrow}{w} = \sum\limits_{j}\alpha_{j}y_{j}\overset{\rightarrow}{x_{j}}.")'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: Notice that ![\overset{\rightarrow}{w}](img/file1205.png "\overset{\rightarrow}{w}")
    only depends on the training points ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}"), for which ![\alpha_{j} \neq 0](img/file1270.png
    "\alpha_{j} \neq 0"). These vectors are called **support vectors** and, as you
    can imagine, are the reason behind the name of the SVM model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we can also recover ![b](img/file17.png "b") by finding some ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}") that lies at the boundary of the margin and solving
    a simple equation — see [[2](ch030.xhtml#Xabu2012blearning)] for all the details.
    Then, in order to classify a point ![x](img/file269.png "x"), we can just compute
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \sum\limits_{j}\alpha_{j}y_{j}\left(
    {\overset{\rightarrow}{x_{j}} \cdot \overset{\rightarrow}{x}} \right) + b,](img/file1271.png
    "\overset{\rightarrow}{w} \cdot \overset{\rightarrow}{x} + b = \sum\limits_{j}\alpha_{j}y_{j}\left(
    {\overset{\rightarrow}{x_{j}} \cdot \overset{\rightarrow}{x}} \right) + b,")'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: and decide whether ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    goes into the positive or negative class depending on whether the result is bigger
    than 0 or not.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now covered all we need to know about how to train a support vector machine.
    But, with our tools, we can only train these models to obtain linear separations
    between data, which is, well, not the most exciting of prospects. In the next
    section, we will overcome this limitation with a simple yet powerful trick.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.4 The kernel trick
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vanilla SVMs can only be trained to find linear separations between data elements.
    For example, the data shown in *Figure* [*9.5a*](#Figure9.5a) cannot be separated
    effectively by any SVM, because there is no way to separate it linearly.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: How do we overcome this? Using **the kernel trick**. This technique consists
    in mapping the data from its original space ![R^{n}](img/file1212.png "R^{n}")
    to a higher dimensional space ![R^{N}](img/file1272.png "R^{N}"), all in the hope
    that, in that space, there may be a way to separate the data with a hyperplane.
    This higher dimensional space is known as a **feature space**, and we will refer
    to the function ![\left. \varphi:R^{n}\rightarrow R^{N} \right.](img/file1273.png
    "\left. \varphi:R^{n}\rightarrow R^{N} \right.") — which takes the original data
    inputs into the feature space — as a **feature** **map**.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the data in *Figure* [*9.5a*](#Figure9.5a) is in the ![1](img/file13.png
    "1")-dimensional real line, but we can map it to the ![2](img/file302.png "2")-dimensional
    plane with the function
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '| ![f(x) = (x,x^{2}).](img/file1274.png "f(x) = (x,x^{2}).") |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: As we can see in *Figure* [*9.5b*](#Figure9.5b), upon doing this, there is a
    hyperplane that perfectly separates the two categories in our dataset.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![(a) Original data in the real line](img/file1275.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: '**(a)** Original data in the real line'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![(b) Data in the feature space](img/file1276.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: '**(b)** Data in the feature space'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 9.5**: The original data cannot be separated by a hyperplane, but
    — upon taking it to a higher-dimensional space with a feature map — it can. The
    separating hyperplane is represented by a dashed line'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the dual form of the soft-margin SVM optimization problem, we can
    see how, in order to train an SVM — and to later classify new data — on a certain
    feature space with a feature map ![\varphi](img/file1277.png "\varphi"), all we
    need to ”know” about the feature space is how to compute scalar products in it
    of elements returned by the feature map. This is because, during the whole training
    process, the only operation that depends on the ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}") points is the inner product ![{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}](img/file1278.png "{\overset{\rightarrow}{x}}_{j}
    \cdot {\overset{\rightarrow}{x}}_{k}") — or the inner product ![\overset{\rightarrow}{x_{j}}
    \cdot \overset{\rightarrow}{x}](img/file1279.png "\overset{\rightarrow}{x_{j}}
    \cdot \overset{\rightarrow}{x}") when classifying a new point ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"). If instead of ![\overset{\rightarrow}{x_{j}}](img/file1269.png
    "\overset{\rightarrow}{x_{j}}") we had ![\varphi(\overset{\rightarrow}{x_{j}})](img/file1280.png
    "\varphi(\overset{\rightarrow}{x_{j}})"), we would just need to know how to compute
    ![\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi({\overset{\rightarrow}{x}}_{k})](img/file1281.png
    "\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi({\overset{\rightarrow}{x}}_{k})")
    — or ![\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi(\overset{\rightarrow}{x})](img/file1282.png
    "\varphi({\overset{\rightarrow}{x}}_{j}) \cdot \varphi(\overset{\rightarrow}{x})")
    to classify new data ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}").
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: That is, it suffices to be able to compute the function
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '| ![k(x,y) = \varphi(\overset{\rightarrow}{x}) \cdot \varphi(\overset{\rightarrow}{y}),](img/file1283.png
    "k(x,y) = \varphi(\overset{\rightarrow}{x}) \cdot \varphi(\overset{\rightarrow}{y}),")
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: and that is the single and only computation that we need to perform in the feature
    space. This is a crucial fact. This function is a particular case of what are
    known as **kernel functions**. Broadly speaking, kernel functions are functions
    that *can be* represented as inner products in some space. Mercer’s theorem (see
    [[2](ch030.xhtml#Xabu2012blearning)]) gives a nice characterization of them in
    terms of certain properties such as being symmetric and some other conditions.
    In the cases that we will consider, these conditions are always going to be met,
    so we don’t need to worry too much about them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have a general understanding of how support vector machines are
    used in general, and in classical setups in particular. We now have all the necessary
    background to take the step to quantum. Get ready to explore quantum support vector
    machines.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Going quantum
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have already mentioned, quantum support vector machines are particular
    cases of SVMs. To be more precise, they are particular cases of SVMs that rely
    on the kernel trick.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen in the previous section how, with the kernel trick, we take our
    data to a feature space: a higher dimensional space in which, we hope, our data
    will be separable by a hyperplane with the right choice of feature map. This feature
    space is usually just the ordinary Euclidean space but, well, with a higher dimension.
    But we can consider other choices. How about…the space of quantum states?'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 The general idea behind quantum support vector machines
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A QSVM works just like an ordinary SVM that relies on the kernel trick — with
    the only difference that it uses as feature space a certain space of quantum states.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed before, whenever we use the kernel trick, all we need from the
    feature space is a kernel function. That’s the only ingredient involving the feature
    space that is necessary in order to be able to train a kernel-based SVM and make
    predictions with it. This idea inspired some works, such as the famous paper by
    Havlíček et al. [[52](ch030.xhtml#Xhavlivcek2019supervised)], to try to use quantum
    circuits to compute kernels and, hopefully, obtain some advantage over classical
    computers by working in a sophisticated feature space.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking this into account, in order to train and then use a quantum support
    vector machine for classification, we will be able to do business as usual — doing
    everything fully classically — except for the computation of the kernel function.
    This function will have to rely on a quantum computer in order to do the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Take as input two vectors in the original space of data.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map each of them to a quantum state through a **feature map**.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the inner product of the quantum states and return it.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will discuss how to implement these (quantum) feature maps in the next subsection,
    but, in essence, they are just circuits that are parametrized exclusively by the
    original (classical) data and thus prepare a quantum state that depends only on
    that data. For now, we will just take these feature maps as a given.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s say that we have a feature map ![\varphi](img/file1277.png "\varphi").
    This will be implemented by a circuit ![\Phi](img/file1284.png "\Phi") that will
    depend on some classical data in the original space: for each input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"), we will have a circuit ![\Phi(\overset{\rightarrow}{x})](img/file1285.png
    "\Phi(\overset{\rightarrow}{x})") such that the output of the feature map will
    be the quantum state ![\varphi(\overset{\rightarrow}{x}) = \Phi(\overset{\rightarrow}{x})\left|
    0 \right\rangle](img/file1286.png "\varphi(\overset{\rightarrow}{x}) = \Phi(\overset{\rightarrow}{x})\left|
    0 \right\rangle"). With a feature map ready, we can then take our kernel function
    to be'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '| ![k(\overset{\rightarrow}{a},\overset{\rightarrow}{b}) = \left&#124; \left\langle
    \varphi(a) \middle&#124; \varphi(b) \right\rangle \right&#124;^{2} = \left&#124;
    {\left\langle 0 \right&#124;\Phi^{\dagger}(a)\Phi(b)\left&#124; 0 \right\rangle}
    \right&#124;^{2}.](img/file1287.png "k(\overset{\rightarrow}{a},\overset{\rightarrow}{b})
    = \left&#124; \left\langle \varphi(a) \middle&#124; \varphi(b) \right\rangle \right&#124;^{2}
    = \left&#124; {\left\langle 0 \right&#124;\Phi^{\dagger}(a)\Phi(b)\left&#124;
    0 \right\rangle} \right&#124;^{2}.") |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: And that is something that we can trivially get from a quantum computer! As
    you can easily check yourself, it is nothing more than the probability of measuring
    all zeros after preparing the state ![\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle](img/file1288.png "\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle"). This follows from the fact that the computational basis is
    orthonormal.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: In case you were wondering how to compute the circuit for ![\Phi^{\dagger}](img/file1289.png
    "\Phi^{\dagger}"), notice that this is just the inverse of ![\Phi](img/file1284.png
    "\Phi"), because quantum circuits are always represented by unitary operations.
    But ![\Phi](img/file1284.png "\Phi") will be given by a series of quantum gates.
    So all you need to do is apply the gates in the circuit from right to left and
    invert each of them.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: And that is how you implement a quantum kernel function. You take a feature
    map that will return a circuit ![\Phi(\overset{\rightarrow}{x})](img/file1285.png
    "\Phi(\overset{\rightarrow}{x})") for any input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}"), you prepare the state ![\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle](img/file1288.png "\Phi^{\dagger}(\overset{\rightarrow}{a})\Phi(\overset{\rightarrow}{b})\left|
    0 \right\rangle") for the pair of vectors on which you want to compute the kernel,
    and you return the probability of measuring zero on all the qubits.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: In case you were concerned, by the way, all quantum kernels, as we have defined
    them, satisfy the conditions needed to qualify as kernel functions [[85](ch030.xhtml#Xschuld2021supervised)].
    In fact, we’ll now ask you to check one of those conditions!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.2
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: One of the conditions for a function ![k](img/file317.png "k") to be a kernel
    is that it be symmetric. Prove that, indeed, any quantum kernel is symmetric.
    (![k(\overset{\rightarrow}{a},\overset{\rightarrow}{b}) = k(\overset{\rightarrow}{b},\overset{\rightarrow}{a})](img/file1290.png
    "k(\overset{\rightarrow}{a},\overset{\rightarrow}{b}) = k(\overset{\rightarrow}{b},\overset{\rightarrow}{a})")
    for any inputs.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now study how to actually construct those feature maps.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Feature maps
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feature map, as we have said, is often defined by a parametrized circuit ![\Phi(\overset{\rightarrow}{x})](img/file1285.png
    "\Phi(\overset{\rightarrow}{x})") that depends on the original data and thus can
    be used to prepare a state that depends on it. In this section, we will study
    a few interesting feature maps that we will use throughout the rest of the book.
    They will also serve as examples that will allow us to better illustrate what
    feature maps actually are.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Angle encoding** We shall begin with a simple yet powerful feature map known
    as **angle encoding**. When used on an ![n](img/file244.png "n")-qubit circuit,
    this feature map can take up to ![n](img/file244.png "n") numerical inputs ![x_{1},\ldots,x_{n}](img/file1291.png
    "x_{1},\ldots,x_{n}"). The action of its circuit consists in the application of
    a rotation gate on each qubit ![j](img/file258.png "j") parametrized by the value
    ![x_{j}](img/file407.png "x_{j}"). In this feature map, we are using the ![x_{j}](img/file407.png
    "x_{j}") values as angles in the rotations, hence the name of the encoding.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: In angle encoding, we are free to use any rotation gate of our choice. However,
    if we use ![R_{Z}](img/file120.png "R_{Z}") gates and take ![\left| 0 \right\rangle](img/file6.png
    "\left| 0 \right\rangle") to be our initial state…the action of our feature map
    will have no effects whatsoever, as you can easily check from the definition of
    ![R_{Z}](img/file120.png "R_{Z}"). That is why, when ![R_{Z}](img/file120.png
    "R_{Z}") gates are used, it is customary to precede them by Hadamard gates acting
    on each qubit. All this is shown in *Figure* [*9.6*](#Figure9.6).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6: Angle encoding for an input (x_{1},\ldots,x_{n}) using different
    rotation gates](img/file1293.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.6**: Angle encoding for an input ![(x_{1},\ldots,x_{n})](img/file1292.png
    "(x_{1},\ldots,x_{n})") using different rotation gates'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The variables that are fed to the angle encoding feature map should be normalized
    within a certain interval. If they are normalized between ![0](img/file12.png
    "0") and ![4\pi](img/file1294.png "4\pi"), then the data will be mapped to a wider
    region of the feature space than if they were normalized between ![0](img/file12.png
    "0") and ![1](img/file13.png "1"), for example. However, this would come at the
    cost of having the two extrema of the dataset identified under the action of the
    feature map. That’s because 0 and ![2\pi](img/file1295.png "2\pi") are exactly
    the same angle and, in our definition of rotation gates, we divided the input
    angle by ![2](img/file302.png "2").
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: The choice of normalization will thus be a trade-off between separating the
    extrema in the feature space and using the widest possible region in it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '**Amplitude encoding** Angle encoding can take ![n](img/file244.png "n") inputs
    on ![n](img/file244.png "n") qubits. Does that seem good enough? Well, get ready
    for a big jump. The **amplitude encoding** feature map can take ![2^{n}](img/file256.png
    "2^{n}") inputs when implemented on an ![n](img/file244.png "n")-qubit circuit.
    That is a lot, and it will enable us to effectively train QSVMs on datasets with
    a large number of variables. So, how does it work then?'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: If the amplitude encoding feature map is given an input ![x_{0},\ldots,x_{2^{n}
    - 1}](img/file1296.png "x_{0},\ldots,x_{2^{n} - 1}"), it simply prepares the state
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\left&#124; {\varphi(\overset{\rightarrow}{a})} \right\rangle = \frac{1}{\sqrt{\sum\limits_{k}x_{k}^{2}}}\sum\limits_{k
    = 0}^{2^{n} - 1}x_{k}\left&#124; k \right\rangle.](img/file1297.png "\left&#124;
    {\varphi(\overset{\rightarrow}{a})} \right\rangle = \frac{1}{\sqrt{\sum\limits_{k}x_{k}^{2}}}\sum\limits_{k
    = 0}^{2^{n} - 1}x_{k}\left&#124; k \right\rangle.") |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: Notice how we’ve had to include a normalization factor to make sure that the
    output was, indeed, a quantum state. Remember from *Chapter* [*1*](ch008.xhtml#x1-180001),
    *Foundations of* *Quantum Computing*, that all quantum states need to be normalized
    vectors! It’s easy to see from the definition that amplitude encoding can work
    for any input except for the zero vector — for the zero vector, amplitude encoding
    is undefined. We can’t divide by zero!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Implementing this feature map in terms of elementary quantum gates is by no
    means simple. If you want all the gory details, you can check the book by Schuld
    and Petruccione [[106](ch030.xhtml#Xschuld)]. Luckily, it is built into most quantum
    computing frameworks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'By the way, when using amplitude encoding, there is an unavoidable loss of
    information if you decide to ”push the feature map to its limit.” In general,
    you won’t be using all the ![2^{n}](img/file256.png "2^{n}") parameters that it
    offers — you will only use some of them and fill the rest with zeros or any other
    value of your choice. But, if you use all the ![2^{n}](img/file256.png "2^{n}")
    inputs to encode variables, there’s a small issue: that the number of degrees
    of freedom of an ![n](img/file244.png "n")-qubit state is actually ![2^{n} - 1](img/file1298.png
    "2^{n} - 1"), not ![2^{n}](img/file256.png "2^{n}"). This is, in any case, not
    a big deal. This loss of information can be ignored for sufficiently big values
    of ![n](img/file244.png "n").'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '**ZZ feature map** Lastly, we will present a known feature map that may bring
    you memories from *Chapter* [*5*](ch013.xhtml#x1-940005), *QAOA: Quantum Approximate
    Optimization* *Algorithm*, where we implemented circuits for Hamiltonians with
    ![Z_{j}Z_{k}](img/file363.png "Z_{j}Z_{k}") terms. It’s called the **ZZ feature
    map**. It is implemented by Qiskit and it can take ![n](img/file244.png "n") inputs
    ![a_{1},\ldots,a_{n}](img/file1299.png "a_{1},\ldots,a_{n}") on ![n](img/file244.png
    "n") qubits, just like angle embedding. Its parametrized circuit is constructed
    following these steps:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Apply a Hadamard gate on each qubit.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply, on each qubit ![j](img/file258.png "j"), a rotation ![R_{Z}(2x_{j})](img/file1300.png
    "R_{Z}(2x_{j})").
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each pair of elements ![\{ j,k\} \subseteq \{ 1,\ldots,n\}](img/file1301.png
    "\{ j,k\} \subseteq \{ 1,\ldots,n\}") with ![j < k](img/file1302.png "j < k"),
    do the following:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a CNOT gate targeting qubit ![k](img/file317.png "k") and controlled by
    qubit ![j](img/file258.png "j").
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply, on qubit ![k](img/file317.png "k"), a rotation ![R_{Z}\left( {2(\pi -
    x_{j})(\pi - x_{k})} \right)](img/file1303.png "R_{Z}\left( {2(\pi - x_{j})(\pi
    - x_{k})} \right)").
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step [3a](#x1-171010x1).
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In *Figure* [*9.7*](#Figure9.7) you can find a representation of the ZZ feature
    map on three qubits.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: As with angle encoding, normalization plays a big role in the ZZ feature map.
    In order to guarantee a healthy balance between separating the extrema of the
    dataset and using as big a region a possible in the feature space, the variables
    could be normalized to ![\lbrack 0,1\rbrack](img/file1145.png "\lbrack 0,1\rbrack")
    or ![\lbrack 0,3\rbrack](img/file1304.png "\lbrack 0,3\rbrack"), for example.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7: ZZ feature map on three qubits with inputs x_{1},x_{2},x_{3}](img/file1306.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: '**Figure 9.7**: ZZ feature map on three qubits with inputs ![x_{1},x_{2},x_{3}](img/file1305.png
    "x_{1},x_{2},x_{3}")'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Of course, when designing a quantum feature map, your imagination is the only
    limit. The ones that we have presented here are some of the most popular ones
    — and the ones that you will find in frameworks such as PennyLane and Qiskit —
    but research on quantum feature maps and their properties is an active area. If
    you want to take a look at other possibilities, we can recommend the paper by
    Sim, Johnson, and Aspuru-Guzik [[88](ch030.xhtml#Xsim2019expressibility)].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: But enough theory for now! Let’s put into practice all that we have learned
    by implementing some QSVMs with both PennyLane and Qiskit.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Quantum support vector machines in PennyLane
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It has been a long journey but, finally, we are ready to see QSVMs in action.
    In this section, we are going to train and run a bunch of QSVM models using PennyLane.
    Just to get started, let’s import NumPy and set a seed so that our results are
    reproducible:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 9.3.1 Setting the scene for training a QSVM
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, if we want to train QSVMs, we need some data to work with. In today’s ever-changing
    job market, you should always keep your options open and, as promising as quantum
    machine learning may be, you may want to have a backup career plan. Well, we’ve
    got you covered. Have you ever dreamed of becoming a world-class sommelier? Today
    is your lucky day! (We are just kidding, of course, but we will use this wine
    theme to give some flavor to our example!)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: We’ve already seen how the scikit-learn package offers lots of tools and resources
    for machine learning. It turns out that among them are a collection of pre-defined
    datasets on which to train ML models, and one of those datasets is a ”wine recognition
    dataset” [[32](ch030.xhtml#XDua:2019)]. This is a labeled dataset with information
    about wines. In total, it has ![13](img/file1307.png "13") numeric variables that
    describe the color intensity, alcohol concentration, and other fancy things whose
    meaning and significance we have no clue about. The labels correspond to the kind
    of wine. There are three possible labels, so, if we just ignore one, we are left
    with a beautiful dataset for a binary classification problem.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load the set with the `load_wine` function in `sklearn``.``datasets`
    as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have set `return_X_y` to true so that we also get the labels.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all the details about this dataset in its online documentation
    ([https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset)
    or [https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine),
    if you want to check the original source of the data). According to it, the ![59](img/file1308.png
    "59") first elements in the dataset must belong to the first category (label ![0](img/file12.png
    "0")) while the ![71](img/file1309.png "71") subsequent ones have to belong to
    the second one (label ![1](img/file13.png "1")). Thus, if we want to ignore the
    third category, we can just run the following piece of code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And thus we have a labeled dataset with two categories. A perfect binary classification
    problem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed, however, a few disclaimers are in order. This wine recognition
    problem that we are going to work with is — from a machine learning point of view
    — very simple. You don’t need very sophisticated models or a lot of computing
    power to tackle it. Thus, using a QSVM for this problem is overkill. It will work,
    yes, but that doesn’t diminish the fact that we will be overdoing it. Quantum
    support vector machines can tackle complex problems, but we thought it would be
    better to keep things simple. You may call us overprotective, but we thought that
    using examples that could take two hours to run — or even two days! — might not
    be exactly ideal from a pedagogical perspective. We will also see how some examples
    yield better results than others. Unless we state otherwise, that won’t be indicative
    of any general pattern. It will just mean that it so happens, some things work
    better than others for this particular problem. After all, from the few experiments
    that we will run, it wouldn’t be sensible to draw hard conclusions!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'With those remarks out of the way, let’s attack our problem. We shall begin
    by splitting our dataset into a training dataset and a test dataset:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We won’t be making direct model comparisons, nor will we be using validation
    losses, so we will not use a validation dataset.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed previously, most feature maps expect our data to be normalized,
    and, regardless of that, normalizing your data is in general a good practice in
    machine learning. So that’s what we shall do now! We will actually use the most
    simple of normalization techniques: scaling each of the variables linearly in
    such a way that the maximum absolute value taken by each variable be ![1](img/file13.png
    "1"). This can be achieved with a `MaxAbsScaler` object from `sklearn``.``preprocessing`
    as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And, with that, we know that — since all our variables were positive — all the
    values in our training dataset will be between ![0](img/file12.png "0") and ![1](img/file13.png
    "1"). If there were negative values, our scaled variables would take values in
    ![\lbrack - 1,1\rbrack](img/file1310.png "\lbrack - 1,1\rbrack") instead. Notice
    that we have only normalized our training dataset. Normalizing the whole dataset
    *simultaneously* would be, in a way, cheating, because we could be polluting the
    training dataset with information from the test dataset. For instance, if we had
    an outlier in the test dataset with a very high value in some variable — a value
    never reached in the training dataset — this would be reflected in the normalization,
    and, thus, the independence of our test dataset could be compromised.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the training dataset is normalized, we need to normalize the test
    dataset using the same proportions as the training dataset. In this way, the training
    dataset receives no information about the test dataset. This can be achieved with
    the following piece of code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice how we have used the same `scaler` object as before, but we have called
    the `transform` method instead of `fit_transform`. In that way, the scaler uses
    the proportions that it saved before. In addition, we’ve run an instruction to
    ”cut” the values in the test dataset at ![0](img/file12.png "0") and ![1](img/file13.png
    "1") — just in case there were some outliers and in order to comply with the normalization
    requirements of some of the feature maps that we will use.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 PennyLane and scikit-learn go on their first date
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve said it countless times: QSVMs are like normal SVMs, but with a quantum
    kernel. So let’s implement that kernel with PennyLane.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Our dataset has ![13](img/file1307.png "13") variables. Using angle encoding
    or the ZZ feature map on the ![13](img/file1307.png "13") variables would require
    us to use ![13](img/file1307.png "13") qubits, which might not be feasible if
    we want our kernel to be simulated on some not especially powerful computers.
    Thus, we can resort to amplitude encoding using ![4](img/file143.png "4") qubits.
    As we mentioned before, this feature map can accept up to ![16](img/file619.png
    "16") inputs; we will fill the remaining ones with zeros — PennyLane will make
    that easy.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how we can implement our quantum kernel:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, there are a few things to digest here. We are first importing PennyLane,
    setting the number of qubits in a variable, and defining a device; nothing new
    there. And then comes the definition of the circuit of our kernel. In this definition,
    we are using `AmplitudeEmbedding`, which returns an operation equivalent to the
    amplitude encoding of its first argument. In our case, we use the arrays `a` and
    `b` for this first argument. They are the classical data that our kernel function
    takes as input. In addition to this, we also ask `AmplitudeEmbedding` to normalize
    each input vector for us, just as amplitude encoding needs us to do, and, since
    our arrays have ![13](img/file1307.png "13") elements instead of the required
    ![16](img/file619.png "16"), we set `pad_with` `=` `0` to fill the remaining values
    with zeros. Also notice that we are using `qml``.``adjoint` to compute the adjoint
    (or inverse) of the amplitude encoding of `b`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we retrieve an array with the probabilities of measuring each possible
    state in the computational basis. The first element of this array (that is, the
    probability of getting a zero value in all the qubits) will be the output of our
    kernel.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Now we have our quantum kernel almost ready. If you’d like to check that the
    circuit works as expected, you can try it out on some elements from the training
    dataset. For instance, you could run `kernel_circ``(``x_tr``[0],` `x_tr``[1])`.
    If the two arguments are the same, keep in mind that you should always get ![1](img/file13.png
    "1") in the first entry of the returned array (which corresponds, as we have mentioned,
    to the output of the kernel).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 9.3
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Prove that, indeed, any quantum kernel evaluated on two identical entries always
    needs to return the output ![1](img/file13.png "1").
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next step will be using this quantum kernel in an SVM. Our good old scikit-learn
    has its own implementation, `SVC`, of support vector machines, and it works with
    custom kernels, so there we have it! In order to use a custom kernel, you are
    required to provide a `kernel` function accepting two arrays, `A` and `B`, and
    returning a matrix with entries ![(j,k)](img/file356.png "(j,k)") containing the
    kernel applied to `A``[``j``]` and `B``[``k``]`. Once the kernel is prepared,
    the SVM can be trained with the `fit` method. All of this is done in the following
    piece of code:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The training can take up to a few minutes depending on the performance of your
    computer. Once it is over, you can check the accuracy of your trained model with
    the following instructions:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In our case, this gives an accuracy of ![0.92](img/file1311.png "0.92"), meaning
    that the SVM is capable of classifying most of the elements in the test dataset
    correctly.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: This shows us how to train and run a quantum support vector in a fairly simple
    manner. But we can consider more sophisticated scenarios. Are you ready for that?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3 Reducing the dimensionality of a dataset
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just seen how to use amplitude encoding to take full advantage of the
    ![13](img/file1307.png "13") variables of our dataset while only using ![4](img/file143.png
    "4") qubits. In most cases, that is a good approach. But there are also some problems
    in which it may prove better to reduce the number of variables in the dataset
    — while trying to minimize the loss of information, of course — and thus be able
    to use other feature maps that could perhaps yield better results.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: In this subsection, we are going to illustrate this approach. We shall try to
    reduce the number of variables in our dataset to ![8](img/file506.png "8") and,
    then, we will train a QSVM on the new, reduced variables using angle encoding.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: If you want to reduce the dimensionality of a dataset while minimizing information
    loss, as we aim to do now, there are many options at your disposal. You may want
    to have a look at autoencoders, for instance. In any case, for the purposes of
    this section, we will consider a technique known as **principal** **component
    analysis**.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Before actually using principal component analysis, you may reasonably be curious
    about how this fancy-sounding technique works.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: When you have a dataset with ![n](img/file244.png "n") variables, you essentially
    have a set of points in ![R^{n}](img/file1212.png "R^{n}"). With this set, you
    may consider what are known as the **principal** **directions**. The first principal
    direction is the direction of the line that best fits the data as measured by
    the mean squared error. The second principal direction is the direction of the
    line that best fits the data while being orthogonal to the first principal direction.
    This goes on in such a way that the ![k](img/file317.png "k")-th principal direction
    is that of the line that best fits the data while being orthogonal to the first,
    second, and all the way up to the (![k - 1](img/file1312.png "k - 1"))-th principal
    direction.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: We thus may consider an orthonormal basis ![\{ v_{1},\ldots,v_{n}\}](img/file1313.png
    "\{ v_{1},\ldots,v_{n}\}") of ![R^{n}](img/file1212.png "R^{n}") in which ![v_{j}](img/file1314.png
    "v_{j}") points in the direction of the ![j](img/file258.png "j")-th principal
    component. The vectors in this orthonormal basis will be of the form ![v_{j} =
    (v_{j}^{1},\ldots,v_{j}^{n}) \in R^{n}](img/file1315.png "v_{j} = (v_{j}^{1},\ldots,v_{j}^{n})
    \in R^{n}"). Of course, the superscripts are not exponents! They are just superscripts.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: When using principal component analysis, we simply compute the vectors of the
    aforementioned basis. And, then, we define the variables
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '| ![{\overset{\sim}{x}}_{j} = v_{j}^{1}x_{1} + \cdots + v_{j}^{n}x_{n}.](img/file1316.png
    "{\overset{\sim}{x}}_{j} = v_{j}^{1}x_{1} + \cdots + v_{j}^{n}x_{n}.") |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: And, lastly, in order to reduce the dimensionality of our dataset to ![m](img/file259.png
    "m") variables, we just keep the variables ![{\overset{\sim}{x}}_{1},\ldots,{\overset{\sim}{x}}_{m}](img/file1317.png
    "{\overset{\sim}{x}}_{1},\ldots,{\overset{\sim}{x}}_{m}"). This is all done under
    the assumption that the variables ![{\overset{\sim}{x}}_{j}](img/file1318.png
    "{\overset{\sim}{x}}_{j}") are, as we have defined them, sorted in decreasing
    order of relevance towards our problem.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: So how do we use principal component analysis to reduce the number of variables
    in our dataset? Well, scikit-learn is here to save the day. It implements a `PCA`
    class that works in an analogous way to that of the `MaxAbsScaler` class that
    we used before.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'This `PCA` class comes with a `fit` method that analyzes the data and figures
    out the best way to reduce its dimensionality using principal component analysis.
    Then, in addition, it comes with a `transform` method that can then transform
    any data in the way it learned to do when `fit` was invoked. Also, just like `MaxAbsScaler`,
    the `PCA` class has a `fit_transform` method that fits the data and transforms
    it simultaneously:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And, with this, we have effectively reduced the number of variables in our dataset
    to ![8](img/file506.png "8"). Notice, by the way, how we have used the `fit_transform`
    method on the training data and the `transform` method on the test data, all in
    order to preserve the independence of the test dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to implement and train a QSVM using angle encoding. For this,
    we may use the `AngleEmbedding` operator provided by PennyLane. The following
    piece of code defines the training; it is very similar to our previous kernel
    definition and, thus, pretty self-explanatory:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once we have a kernel, we can train a QSVM as we did before, this time reusing
    the `qkernel` function, which will be using the new `kernel_circ` definition:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The returned accuracy on the test dataset is ![1](img/file13.png "1"). Just
    a perfect classification in this case.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.4 Implementing and using custom feature maps
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PennyLane comes with a wide selection of built-in feature maps; you can find
    them all in the online documentation ([https://pennylane.readthedocs.io/en/stable/introduction/templates.html](https://pennylane.readthedocs.io/en/stable/introduction/templates.html)).
    Nevertheless, you may want to define your own. In this subsection, we will train
    a QSVM on the reduced dataset using our own implementation of the ZZ feature map.
    Let’s take feature maps into our own hands!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'We can begin by implementing the feature map as a function with the following
    piece of code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this implementation, we have used the `combinations` function from the `itertools`
    module. It takes two arguments: an array `arr` and an integer `l`. And it returns
    an array with all the sorted tuples of length `l` with elements from the array
    `arr`.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how we have written the `ZZFeatureMap` function as we would write any
    circuit, taking advantage of all the flexibility that PennyLane gives us. Having
    defined this function for the ZZ feature map, we may use it on a kernel function
    and then train a QSVM just as we have done before:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this case, the test accuracy is ![0.77](img/file1319.png "0.77").
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: There’s one detail to which you should pay attention here, and it is the fact
    that `qml``.``adjoint` is acting on the `ZZFeatureMap` function itself, not on
    its output! Remember that taking the adjoint of a circuit is the same as considering
    the inverse of that circuit.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: That’s all we had in store about QSVMs on PennyLane. Now it’s time for us to
    see how things are done in Qiskit Land.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Quantum support vector machines in Qiskit
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we mastered the use of QSVMs in PennyLane. You may
    want to review *subsection* [*9.3.1*](#x1-1730009.3.1) and the beginning of *subsection*
    [*9.3.3*](#x1-1750009.3.3). That is where we prepare the dataset that we will
    be using here too. In addition to running the code in those subsections, you will
    have to do the following import:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now it’s time for us to switch to Qiskit. In some ways, Qiskit can be easier
    to use than PennyLane — although this is probably a matter of taste. In addition,
    Qiskit will enable us to directly train and run our QSVM models using the real
    quantum computers available at IBM Quantum. Nevertheless, for now, let us begin
    with QSVMs on our beloved Qiskit Aer simulator.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.1 QSVMs on Qiskit Aer
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, let us just import Qiskit:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: When we defined a QSVM in PennyLane, we had to ”manually” implement a kernel
    function in order to pass it to scikit-learn. This process is simplified in Qiskit,
    for all it takes to define a quantum kernel is to instantiate a `QuantumKernel`
    object. In the initializer, we are asked to provide a `backend` argument, which
    will be, of course, the backend object on which the quantum kernel will run. By
    default, the feature map that the quantum kernel will use is the ZZ feature map
    with two qubits, but we can use a different feature map by passing a value to
    the `feature_map` object. This value should be a parametrized circuit representing
    the feature map.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Defining parametrized circuits in Qiskit is actually fairly easy. If you want
    to use an individual parameter in a circuit, you can just import `Parameter` from
    `qiskit``.``circuit` and define a parameter object as `Parameter``(``"``label``"``)`
    with any label of your choice. This object can then be used in quantum circuits.
    For example, we may define a circuit with an ![x](img/file269.png "x")-rotation
    parametrized by a value ![x](img/file269.png "x") as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If you want to use an array of parameters in a circuit, you may define a `ParameterVector`
    object instead. It can also be imported from `qiskit``.``circuit` and, in addition
    to the mandatory label, it accepts an optional `length` argument setting the length
    of the array. By default, this length is set to zero. We may use these parameter
    vector objects as in the following example:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Exercise 9.4
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Define an `AngleEncodingX``(``n` `)` function that return the feature map for
    angle encoding using ![R_{X}](img/file118.png "R_{X}") rotations on `n` qubits.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Using parametrized circuits, we may define any feature map of our choice for
    its use in a quantum kernel; for instance, we could just send any of the `qc`
    objects that we have created in the previous pieces of code as the `feature_map`
    parameter in the `QuantumKernel` constructor. Nevertheless, Qiskit already comes
    with some pre-defined feature maps out of the box. For our case, we may generate
    a circuit for the ZZ feature map on eight qubits using the following piece of
    code:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As a matter of fact, this feature map can be further customized by providing
    additional arguments. We shall use them in the following chapter.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our feature map, we can trivially set up a quantum kernel reliant
    on the Aer simulator as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: And that’s all it takes! By the way, here we are using the Qiskit Machine Learning
    package. Please, refer to *Appendix* [*D*](ch027.xhtml#x1-240000D)*, Installing
    the Tools*, for installation instructions.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: If we’d like to train a QSVM model using our freshly-created kernel, we can
    use Qiskit’s own extension of the SVC class provided by scikit-learn. It’s called
    `QSVC` and it can be imported from `quantum_machine_learning``.``algorithms`.
    It works just like the original `SVC` class, but it accepts a `quantum_kernel`
    argument to which we can pass `QuantumKernel` objects.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, these are the instructions that we have to run in order to train a QSVM
    with our kernel:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As with PennyLane, this will take a few minutes to run. Notice, by the way,
    that we have used the reduced dataset (`xs_tr`), because we are using the ZZ feature
    map on 8 qubits.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the training is complete, we can get the accuracy on the test dataset
    as we have always done:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this case, the returned accuracy was ![1](img/file13.png "1").
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: That is all you need to know about how to run QSVMs on the Aer simulator. Now,
    let’s get real.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.2 QSVMs on IBM quantum computers
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training and using QSVMs on real hardware with Qiskit couldn’t be easier. We
    will show how it can be done in this subsection.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, as we did back in *Chapter* [*2*](ch009.xhtml#x1-400002), *The Tools
    of the Trade in Quantum* *Computing*, we will load our IBM Quantum account:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Naturally, for this to work, you should have saved your access token beforehand.
    At the time of writing, free accounts don’t have access to any real quantum devices
    with eight qubits, but there are some with seven qubits. We can select the one
    that is the least busy with the following piece of code:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Of course, we will have to further reduce our data to seven variables, but
    we can do that very easily:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: And, with this, we have all the ingredients ready to train a QSVM on real hardware!
    We will have to follow the same steps as before — only this time using our real
    device as `quantum_instance` in the instantiation of our quantum kernel!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When you execute this code, all the circuit parameters are known in advance.
    For this reason, Qiskit will try to send as many circuits as possible at the same
    time. However, these jobs still have to wait in the queue. Depending on the number
    of points in your dataset and on your access privileges, this may take quite a
    long time to complete!
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: With this, we can bring our study of QSVMs in Qiskit to an end.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first learned what support vector machines are, and how
    they can be trained to solve binary classification problems. We began by considering
    vanilla vector machines, and then we introduced the kernel trick — which opened
    up a world of possibilities! In particular, we saw how QSVMs are nothing more
    than a support vector machine with a quantum kernel.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: From there on, we learned how quantum kernels actually work and how to implement
    them. We explored the essential role of feature maps, and discussed a few of the
    most well-known ones.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned how to implement, train, and use quantum support vector
    machines with PennyLane and Qiskit. In addition, we were able to very easily run
    QSVMs on real hardware thanks to Qiskit’s interface to IBM Quantum.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: And that pretty much covers how QSVMs can help you can identify wines — or solve
    any other classification task — like an expert, all while happily ignoring what
    the ”alkalinity of ash” of a wine is. Who knows? Maybe these SVM models could
    open the door for you to enjoy a bohemian life of wine-tasting! No need to thank
    us.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will consider another family of quantum machine learning
    models: that of quantum neural networks. Things are about to get deep!*'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
