- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, now things are getting fun! Our models can now learn more complex functions,
    and we are now ready for a wonderful tour around the more contemporary and surprisingly
    effective models
  prefs: []
  type: TYPE_NORMAL
- en: After piling layers of neurons became the most popular solution to improving
    models, new ideas for richer nodes appeared, starting with models based on human
    vision. They started as little more than research themes, and after the image
    datasets and more processing power became available, they allowed researchers
    to reach almost human accuracy in classification challenges, and we are now ready
    to leverage that power in our projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will cover in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Origins of convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple implementation of discrete convolution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other operation types: pooling, dropout'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Origin of convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) have a remote origin. They developed
    while **multi-layer perceptrons** were perfected, and the first concrete example
    is the **neocognitron**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The neocognitron is a hierarchical, multilayered **Artificial Neural Network** (**ANN**),
    and was introduced in a 1980 paper by Prof. Fukushima and has the following principal
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-organizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tolerant to shifts and deformation in the input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c29e144d-13cd-46c1-a2fd-a92cc2b6aa99.png)'
  prefs: []
  type: TYPE_IMG
- en: This original idea appeared again in 1986 in the book version of the original
    backpropagation paper, and was also employed in 1988 for temporal signals in speech
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The design was improved in 1998, with a paper from Ian LeCun*,* *Gradient-Based
    Learning Aapplied to Document Recognition*, presenting the LeNet-5 network, an
    architecture used to classify handwritten digits. The model showed increased performance
    compared to other existing models, especially over several variations of SVM,
    one of the most performant operations in the year of publication.
  prefs: []
  type: TYPE_NORMAL
- en: Then a generalization of that paper came in 2003, with "*Hierarchical Neural
    Networks for Image Interpretation*". But in general, almost all kernels followed
    the original idea, until now.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to understand convolution, we will start by studying the origin of
    the convolution operator, and then we will explain how this concept is applied
    to the information.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution is basically an operation between two functions, continuous or discrete,
    and in practice, it has the effect of filtering one of them by another.
  prefs: []
  type: TYPE_NORMAL
- en: It has many uses across diverse fields, especially in digital signal processing,
    where it is the preferred tool for shaping and filtering audio, and images, and
    it is even used in probabilistic theory, where it represents the sum of two independent
    random variables.
  prefs: []
  type: TYPE_NORMAL
- en: And what do these filtering capabilities have to do with machine learning? The
    answer is that with filters, we will be able to build network nodes that can emphasize
    or hide certain characteristics of our inputs (by the definition of the filters)
    so we can build automatic custom detectors for all the features, which can be
    used to detect a determinate pattern. We will cover more of this in the following
    sections; now, let's review the formal definition of the operation, and a summary
    of how it is calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Convolution as an operation was first created during the 18 century by d''Alembert during
    the initial developments of differential calculus. The common definition of the
    operation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfa53da3-92fd-4150-919e-5949eecac3ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we try to describe the steps needed to apply the operation, and how it combines
    two functions, we can express the mathematical operation involved in the following
    detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Flip the signal: This is the (*-τ*) part of the variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shift it: This is given by the *t* summing factor for *g(τ)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiply it: The product of *f* and **g**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Integrate the resulting curve: This is the less intuitive part, because each
    instantaneous value is the result of an integral'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to understand all the steps involved, let''s visually represent all
    the steps involved for the calculation of the convolution between two functions,
    *f* and *g*, at a determinate point *t*[*0*]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1827d5c2-accf-4d2f-93bc-713e522cd3eb.png)'
  prefs: []
  type: TYPE_IMG
- en: This intuitive approximation to the rules of the convolution also applies to
    the discrete field of functions, and it is the real realm in which we will be
    working. So, let's start by defining it.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even when useful, the realm in which we work is eminently digital, so we need
    to translate this operation to the discrete domain. The convolution operation
    for two discrete functions *f* and *g* is the translation of the original integral
    to an equivalent summation in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2080076-17fb-4e9f-ab49-c12ada720b31.png)'
  prefs: []
  type: TYPE_IMG
- en: This original definition can be applied to functions of an arbitrary number
    of dimensions. In particular, we will work on 2D images, because this is the realm
    of a large number of applications, which we will describe further in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to learn the way in which we normally apply the convolution operator,
    which is through kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Kernels and convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When solving practical problems in the discrete domain, we normally have 2D
    functions of finite dimensions (which could be an image, for example) that we
    want to filter through another image. The discipline of filter development studies
    the effects of different kinds of filters when applied via convolution to a variety
    of classes. The most common types of function applied are of two to five elements
    per dimension, and of 0 value on the remaining elements. These little matrices,
    representing filtering functions, are called **kernels**.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation starts with the first element of an *n*-dimensional
    matrix (usually a 2D matrix representing an image) with all the elements of a
    kernel, applying the center element of the matrix to the specific value we are
    multiplying, and applying the remaining factors following the kernel's dimensions.
    The final result is, in the case of an image, an equivalent image in which certain
    elements of it (for example, lines and edges) have been highlighted, and others
    (for example, in the case of blurring) have been hidden.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, you will see how a particular 3 x 3 kernel is applied
    to a particular image element. This is repeated in a scan pattern to all elements
    of the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8732d877-f163-44ff-8548-701da33e19ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernels also have a couple of extra elements to consider when applying them,
    specifically stride and padding, which complete the specification for accommodating
    special application cases. Let's have a look at stride and padding.
  prefs: []
  type: TYPE_NORMAL
- en: Stride and padding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When applying the convolution operation, one of the variations that can be
    applied to the process is to change the displacement units for the kernels. This
    parameter, which can be specified per dimension, is called **stride**. In the
    following image, we show a couple of examples of how stride is applied. In the
    third case, we see an incompatible stride because the kernel can''t be applied
    to the last step. Depending on the library, this type of warning can be dismissed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9998091b-8d00-42e9-a034-11f3ebf7f49d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The other important fact when applying a kernel is that the bigger the kernel,
    the more units there are on the border of the image/matrix that won''t receive
    an answer because we need to cover the entire kernel. In order to cope with that,
    the **padding** parameters will add a border of the specified width to the image
    to allow the kernel to be able to apply evenly to the edge pixels/elements. Here,
    you have a graphical depiction of the padding parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8261782-72b9-4c56-8e97-c62b33b19db0.png)'
  prefs: []
  type: TYPE_IMG
- en: After describing the fundamental concepts of convolution, let's implement convolution
    in a practical example to see it applied to real image and get an intuitive idea
    of its effects.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the 2D discrete convolution operation in an example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand the mechanism of the discrete convolution operation,
    let''s do a simple intuitive implementation of this concept and apply it to a
    sample image with different types of kernel. Let''s import the required libraries.
    As we will implement the algorithms in the clearest possible way, we will just
    use the minimum necessary ones, such as NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `imread` method of the `imageio` package, let''s read the image (imported
    as three equal channels, as it is grayscale). We then slice the first channel,
    convert it to a floating point, and show it using matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e65c77d4-672a-4da5-b364-a6c8dd099a5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now it''s time to define the kernel convolution operation. As we did previously,
    we will simplify the operation on a 3 x 3 kernel in order to better understand
    the border conditions.  `apply3x3kernel` will apply the kernel over all the elements
    of the image, returning a new equivalent image. Note that we are restricting the
    kernels to 3 x 3 for simplicity, and so the 1 pixel border of the image won''t
    have a new value because we are not taking padding into consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw in the previous sections, the different kernel configurations highlight
    different elements and properties of the original image, building filters that
    in conjunction can specialize in very high-level features after many epochs of
    training, such as eyes, ears, and doors. Here, we will generate a dictionary of
    kernels with a name as the key, and the coefficients of the kernel arranged in
    a 3 x 3 array. The `Blur` filter is equivalent to calculating the average of the
    3 x 3 point neighborhood, `Identity` simply returns the pixel value as is, `Laplacian`
    is a classic derivative filter that highlights borders, and then the two `Sobel`
    filters will mark horizontal edges in the first case, and vertical ones in the
    second case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s generate a `ConvolutionalOperation` object and generate a comparative
    kernel graphical chart to see how they compare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the final image you can clearly see how our kernel has detected several
    high-detail features on the image—in the first one, you see the unchanged image
    because we used the unit kernel, then the Laplacian edge finder, the left border
    detector, the upper border detector, and then the blur operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fabe713d-8ade-4606-a804-df420bfbfae5.png)'
  prefs: []
  type: TYPE_IMG
- en: Having reviewed the main characteristics of the convolution operation for the
    continuous and discrete fields, we can conclude by saying that, basically, convolution
    kernels highlight or hide patterns. Depending on the trained or (in our example)
    manually set parameters, we can begin to discover many elements in the image,
    such as orientation and edges in different dimensions. We may also cover some
    unwanted details or outliers by blurring kernels, for example. Additionally, by
    piling layers of convolutions, we can even highlight higher-order composite elements,
    such as eyes or ears.
  prefs: []
  type: TYPE_NORMAL
- en: 'This characteristic of convolutional neural networks is their main advantage
    over previous data-processing techniques: we can determine with great flexibility
    the primary components of a certain dataset, and represent further samples as
    a combination of these basic building blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to look at another type of layer that is commonly used in combination
    with the former—the pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: Subsampling operation (pooling)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The subsampling operation consists of applying a kernel (of varying dimensions)
    and reducing the extension of the input dimensions by dividing the image into *mxn*
    blocks and taking one element representing that block, thus reducing the image
    resolution by some determinate factor. In the case of a 2 x 2 kernel, the image
    size will be reduced by half. The most well-known operations are maximum (max
    pool), average (avg pool), and minimum (min pool).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image gives you an idea of how to apply a 2 x 2 `maxpool` kernel,
    applied to a one-channel 16 x 16 matrix. It just maintains the maximum value of
    the internal zone it covers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/854251eb-1ea4-4f3a-aa9b-6e3aa8afaf0a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have seen this simple mechanism, let''s ask ourselves, what''s
    the main purpose of it? The main purpose of subsampling layers is related to the
    convolutional layers: to reduce the quantity and complexity of information while
    retaining the most important information elements. In other word, they build a
    ***compact representation*** of the underlying information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to write a simple pooling operator. It''s much easier and more
    direct to write than a convolutional operator, and in this case we will only be
    implementing max pooling, which chooses the brightest pixel in the 4 x 4 vicinity
    and projects it to the final image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s apply the newly created pooling operation, and as you can see, the final
    image resolution is much more blocky, and the details, in general, are brighter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here you can see the differences, even though they are subtle. The final image
    is of lower precision, and the chosen pixels, being the maximum of the environment,
    produce a brighter image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c37f3486-e074-4d1f-96d4-b1adbd85a921.png)'
  prefs: []
  type: TYPE_IMG
- en: Improving efficiency with the dropout operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have observed in the previous chapters, overfitting is a potential problem
    for every model. This is also the case for neural networks, where data can do
    very well on the training set but not on the test set, which renders it useless
    for generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, in 2012, a team led by Geoffrey Hinton published a paper in
    which the dropout operation was described. Its operation is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: A random number of nodes is chosen (the ratio of the chosen node from the total
    is a parameter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The values of the chosen weights are reviews to zero, invalidating their previously
    connected peers at the subsequent layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages of the dropout layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main advantage of this method is that it prevents all neurons in a layer
    from synchronously optimizing their weights. This adaptation, made in random groups,
    prevents all the neurons from converging to the same goal, thus decorrelating
    the weights.
  prefs: []
  type: TYPE_NORMAL
- en: A second property discovered for the application of dropout is that the activations
    of the hidden units become sparse, which is also a desirable characteristic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we have a representation of an original, fully-connected
    multi-layer neural network, and the associated network with dropout:'
  prefs: []
  type: TYPE_NORMAL
- en: '>![](img/000dfeda-4caa-4272-b4a2-28903b1d4044.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a rich number of layers, it's time to start a tour of how the
    neural architectures have evolved over time. Starting in 2012, a rapid succession
    of new and increasingly powerful combinations of layers began, and it has been
    unstoppable. This new set of architectures adopted the term **deep learning**,
    and we can approximately define them as complex neural architectures that involve
    at least three layers. They also tend to include more advanced layers than the
    **Single Layer Perceptrons**, like convolutional ones.
  prefs: []
  type: TYPE_NORMAL
- en: Deep convolutional network architectures through time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning architectures date from 20 years ago and have evolved, guided
    for the most part by the challenge of solving the human vision problem. Let's
    have a look at the main deep learning architectures and their principal building
    blocks, which we can then reuse for our own purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Lenet 5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the historical introduction of the convolutional neural networks,
    convolutional layers were discovered during the 1980s. But the available technology
    wasn't powerful enough to build complex combinations of layers until the end of
    the 1990s.
  prefs: []
  type: TYPE_NORMAL
- en: Around 1998, in Bell Labs, during research around the decodification of handwritten
    digits, Ian LeCun formed a new approach—a mix of convolutional, pooling, and fully
    connected layers—to solve the problem of recognizing handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: At this time, SVM and other much more mathematically defined problems were used
    more or less successfully, and the fundamental paper on CNNs shows that neural
    networks could perform comparatively well with the then state-of-the-art methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, there is a representation of all the layers of this
    architecture, which received a grayscale 28 x 28 image as input and returned a
    10-element vector, with the probability for each character as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/564b343b-c5aa-444e-b00d-38f719b0c32a.png)'
  prefs: []
  type: TYPE_IMG
- en: Alexnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After some more years of hiatus (even though Lecun was applying his networks
    to other tasks, such as face and object recognition), the exponential growth of
    both available structured data and raw processing power allowed the teams to grow
    and tune the models to an extent that could have been considered impossible just
    a few years before.
  prefs: []
  type: TYPE_NORMAL
- en: One of the elements that fostered innovation in the field was the availability
    of an image recognition benchmark called **Imagenet**, consisting of millions
    of images of objects organized into categories.
  prefs: []
  type: TYPE_NORMAL
- en: Starting in 2012, the **Large Scale Visual Recognition Challenge (LSVRC)** ran
    every year and helped researchers to innovate in network configurations, obtaining
    better and better results every year.
  prefs: []
  type: TYPE_NORMAL
- en: '**Alexnet**, developed by Alex Krizhevsky, was the first deep convolutional
    network that won this challenge, and set a precedent for years to come. It consisted
    of a model similar in structure to Lenet-5, but the convolutional layers of which
    had a depth of hundreds of units, and the total number of parameters was in the
    tens of millions.'
  prefs: []
  type: TYPE_NORMAL
- en: The following challenges saw the appearance of a powerful contender, the **Visual
    Geometry Group (VGG)** from Oxford University, with its VGG model.
  prefs: []
  type: TYPE_NORMAL
- en: The VGG model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main characteristic of the VGG network architecture is that it reduced the
    size of the convolutional filters to a simple 3 x 3 matrix and combined them in
    sequences, which was different to previous contenders, which had large kernel
    dimensions (up to 11 x 11).
  prefs: []
  type: TYPE_NORMAL
- en: Paradoxically, the series of small convolutional weights amounted to a really
    large number of parameters (in the order of many millions), and so it had to be
    limited by a number of measures.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7aee123c-c553-4558-a73d-a924b6c642a7.png)'
  prefs: []
  type: TYPE_IMG
- en: GoogLenet and the Inception model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GoogLenet** was the neural network architecture that won the LSVRC in 2014,
    and was the first really successful attempt by one of the big IT companies in
    the series, which has been won mostly by corporations with giant budgets since
    2014.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GoogLenet is basically a deep composition of nine chained Inception modules,
    with little or no modification. Each one of these Inception modules is represented
    in the following figure, and it''s a mix of tiny convolutional blocks, intermixed
    with a 3 x 3 max pooling node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c3db9cd-2c4e-47d0-a033-6fd7357c77be.png)'
  prefs: []
  type: TYPE_IMG
- en: Even with such complexity, GoogLenet managed to reduce the required parameter
    number (11 millon compared to 60 millon), and increased the accuracy (6.7% error
    compared to 16.4%) compared to Alexnet, which was released just two years previously.
    Additionally, the reuse of the Inception module allowed agile experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: But it wasn't the last version of this architecture; soon enough, a second version
    of the Inception module was created, with the following characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Batch-normalized inception V2 and V3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In December 2015, with the paper *Rethinking the Inception Architecture for
    Computer Vision*, Google Research released a new iteration of the Inception architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**The internal covariance shift problem**'
  prefs: []
  type: TYPE_NORMAL
- en: One of the main problems of the original GoogLenet was training instability.
    As we saw earlier, input normalization consisted basically of centering all the
    input values on zero and dividing its value by the standard deviation in order
    to get a good baseline for the gradients of the backpropagations.
  prefs: []
  type: TYPE_NORMAL
- en: What occurs during the training of really large datasets is that after a number
    of training examples, the different value osculations begin to amplify the mean
    parameter value, like in a resonance phenomenon. This phenomenon is called **covariance
    shift**.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate this, the solution was to apply normalization not only to the original
    input values, but also to the output values at each layer, avoiding the instabilities
    appearing between layers before they begin to drift from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from batch normalization, there were a number of additions proposed incrementally
    to V2:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the number of convolutions to maximum of 3 x 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase the general depth of the networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the width increase technique on each layer to improve feature combination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factorization of convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception V3 basically implements all the proposed innovations on the same architecture,
    and adds batch normalization to auxiliary classifiers of the networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we represent the new architecture. Note the reduced
    size of the convolutional units:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca0f4608-6d08-40a2-ab45-364692d956a2.png)'
  prefs: []
  type: TYPE_IMG
- en: At the end of 2015, the last fundamental improvement in this series of architectures
    came from another company, Microsoft, in the form of **ResNets**.
  prefs: []
  type: TYPE_NORMAL
- en: Residual Networks (ResNet)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This new architecture appeared in December 2015 (more or less the same time
    as Inception V3), and it had a simple but novel idea—not only should the output
    of each constitutional layer be used but the architecture should also combine
    the output of the layer with the original input.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we observe a simplified view of one of the ResNet
    modules. It clearly shows the sum operation at the end of the convolutional series,
    and a final ReLU operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b626c89d-0474-4e33-aab3-01fdf6cc9f19.png)'
  prefs: []
  type: TYPE_IMG
- en: The convolutional part of the module includes a feature reduction from 256 to
    64 values, a 3 x 3 filter layer maintaining the feature numbers, and a feature
    augmenting the 1 x 1 layer from 64 x 256 values. Originally, it spanned more than
    100 layers, but in recent developments, ResNet is also used in a depth of fewer
    than 30 layers, but with a wider distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen a general overview of the main developments of recent
    years, let's go directly to the main types of application that researchers have
    discovered for CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Types of problem solved by deep layers of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNNs have been employed to solve a wide variety of problems in the past. Here
    is a review of the main problem types, and a short reference to the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification models, as we have seen previously, take an image or other type
    of input as a parameter and return one array with as many elements as the number
    of the possible classes, with a corresponding probability for each one.
  prefs: []
  type: TYPE_NORMAL
- en: The normal architecture for this type of solution is a complex combination of
    convolutional and pooling layers with a logistic layer at the end, showing the
    probability any of the pretrained classes.
  prefs: []
  type: TYPE_NORMAL
- en: Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detection adds a level of complexity because it requires guessing the location
    of one or more elements pertaining to the image, and then trying to classify each
    of these elements of information.
  prefs: []
  type: TYPE_NORMAL
- en: For this task, a common strategy for the individual localization problem is
    to combine a classification and regression problem—one (classification) for the
    class of the object, and the remaining one (regression) for determining the coordinates
    of the detected object—and then combining the losses into a common one.
  prefs: []
  type: TYPE_NORMAL
- en: For multiple elements, the first step is to determine a number of regions of
    interest, searching for places in the image that are statistically showing blobs
    of information belonging to the same object and then only applying the classification
    algorithms to the detected regions, looking for positive cases with high probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Segmentation adds an additional layer of complexity to the mix because the
    model has to locate the elements in an image and mark the exact shapes of all
    the located objects, as in the following illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9650e822-2c45-41e6-b22f-cf1082db369a.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the most common approaches for this task is to implement sequential downsampling
    and upsampling operations, recovering a high-resolution image with only a certain
    number of possible outcomes per pixel that mark the class number for that element.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a deep neural network with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this exercise, we will generate an instance of the previously described
    Inception model, provided by the Keras application library. First of all, we will
    import all the required libraries, including the Keras model handling, the image
    preprocessing library, the gradient descent used to optimize the variables, and
    several Inception utilities. Additionally, we will use OpenCV libraries to adjust
    the new input images, and the common NumPy and matplotlib libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Keras makes it really simple to load a model. You just have to invoke a new
    instance of the `InceptionV3` class, and then we will assign an optimizer based
    on **stochastic gradient descent**, and the categorical cross-entropy for the
    loss, which is very suitable for image classification problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the model is loaded into memory, it''s time to load and adjust the
    photo using the `cv` library, and then we call the preprocess input of the Keras
    application, which will normalize the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the image looks like after it''s normalized—note how our structural
    understanding of the image has changed, but from the point of view of the model,
    this is the best way of allowing the model to converge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4adcfca4-f4b1-4e89-98ba-bee9896d17da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we will invoke the `predict` method of the model, which will show the results
    of the last layer of the neural network, an array of probabilities for each of
    the categories. The `decode_predictions` method reads a dictionary with all the
    category numbers as indexes, and the category name as the value, and so it provides
    the name of the detected item classification, instead of the number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, with this simple approach we have received a very approximate
    prediction from a list of similar birds. Additional tuning of the input images
    and the model itself could lead to more precise answers because the blue jay is
    a category included in the 1,000 possible classes.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a convolutional model with Quiver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this practical example, we will load one of the models we have previously
    studied (in this case, `Vgg19`) with the help of the Keras library and Quiver.
    Then we will observe the different stages of the architecture, and how the different
    layers work, with a certain input.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a convolutional network with Quiver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Quiver** ([https://github.com/keplr-io/quiver](https://github.com/keplr-io/quiver))
    is a recent and very convenient tool used to explore models with the help of Keras.
    It creates a server that can be accessed by a contemporary web browser and allows
    the visualization of a model''s structure and the evaluation of input images from
    the input layers until the final predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the following code snippet, we will create an instance of the `VGG16`
    model and then we will allow Quiver to read all the images sitting on the current
    directory and start a web application that will allow us to interact with our
    model and its parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The script will then download the `VGG16` model weights (you need a good connection
    because it weighs in the hundreds of megabytes). Then it loads the model in memory
    and creates a server listening on port 5000.
  prefs: []
  type: TYPE_NORMAL
- en: The model weights that the Keras library downloads have been previously trained
    thoroughly with Imagenet, so it is ready to get very good accuracy on the 1,000
    categories in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the following screenshot, we see the first screen we will see after loading
    the index page of the web application. On the left, an interactive graphical representation
    of the network architecture is shown. On the center right, you can select one
    of the pictures in your current directory, and the application will automatically
    feed it as input, printing the five most likely outcomes of the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The screenshot also shows the first network layer, which basically consists
    of three matrices representing the red, green, and blue components of the original
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f08fd1e-24e3-484d-b270-fcf57f39f584.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, as we advance into the model layers, we have the first convolutional
    layer. Here we can see that this stage highlights mainly high-level features,
    like the ones we set up with our 3 x 3 filters, such as different types of border,
    brightness, and contrast:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6515d78-8381-4126-884f-38a674246a5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s advance a bit more. We now can see an intermediate layer that isn''t
    focused on global features. Instead, we see that it has trained for intermediate
    features, such as different sets of textures, angles, or sets of features, such
    as eyes and noses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7883909-d0bb-40fa-83f0-55f4d1fb386d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When arriving at the last convolutional layers, really abstract concepts are
    appearing. This stage shows how incredibly powerful the models we are now training
    are, because now we are seeing highlighted elements without any useful (for us)
    meaning. These new abstract categories will lead, after some fully connected layers,
    to the final solution, which is a 1,000-element array with a float probability
    value, the probability value for each category in ImageNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ecddc0-353d-4f3b-b020-8a36010b61e2.png)'
  prefs: []
  type: TYPE_IMG
- en: We hope you can explore different examples and the layers' outputs, and try
    to discover how they highlight the different features for different categories
    of images.
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to work on a new type of machine learning, which consists of applying
    previously trained networks to work on new types of problem. This is called **transfer
    learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will implement one of the previously seen examples, replacing
    the last stages of a pretrained convolutional neural network and training the
    last stages for a new set of elements, applying it to classification. It has the
    following advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: It builds upon models with proved efficiency for image classification tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces the training time because we can reuse coefficients with an accuracy
    that could take weeks of computing power to reach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset classes will be two different flower types from the flower17 dataset.
    It is a 17-category flower dataset with 80 images for each class. The flowers
    chosen are some common flowers in the UK. The images have large scale, pose, and
    light variations, and there are also classes with large variations of images within
    the class and close similarity to other classes. In this case, we will gather
    the first two classes (daffodil and coltsfoot), and build a classifier on top
    of the pretrained VGG16 network.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will do image data augmentation, because the quantity of images may
    not be enough to abstract all the elements of each species. Let''s start by importing
    all the required libraries, including applications, preprocessing, the checkpoint
    model, and the associated object, to allow us to save the intermediate steps,
    and the `cv2` and `NumPy` libraries for image processing and numerical base operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this section, we will define all the variables affecting the input, data
    sources, and training parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will invoke the VGG16 pretrained model, not including the top flattening
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to compile the model and create the image data augmentation
    object for the training and testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will generate the new augmented data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s time to fit the new final layers for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s try this with a daffodil image. Let''s test the output of the classifier,
    which should output an array close to `[1.,0.]`, indicating that the probability
    for the first option is very high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: So, we have a very definitive answer for this kind of flower. You can play with
    new images and test the model with clipped or distorted images, even with related
    classes, to test the level of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fukushima, Kunihiko, and Sei Miyake, *Neocognitron: A Self-Organizing Neural
    Network Model for a Mechanism of Visual Pattern Recognition.* Competition and
    cooperation in neural nets. Springer, Berlin, Heidelberg, 1982\. 267-285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun, Yann, et al. *Gradient-based learning applied to document recognition.* Proceedings
    of the IEEE 86.11 (1998): 2278-2324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton, *ImageNet Classification
    with Deep Convolutional Neural Networks.* Advances in neural information processing
    systems. 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton, Geoffrey E., et al, *Improving Neural Networks by Preventing Co-Adaptation
    of Feature Detectors.* arXiv preprint arXiv:1207.0580 (2012).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan, Karen, and Andrew Zisserman, *Very Deep Convolutional Networks for
    Large-Scale Image Recognition*. arXiv preprint arXiv:1409.1556 (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava, Nitish, et al. *Dropout: A Simple Way to Prevent Neural Networks
    from Overfitting.* Journal of machine learning research15.1 (2014): 1929-1958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy, Christian, et al, *Rethinking the Inception Architecture for Computer
    Vision.* Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He, Kaiming, et al, *Deep Residual Learning for Image Recognition.* Proceedings
    of the IEEE conference on computer vision and pattern recognition. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet, François, *Xception: Deep Learning with Depthwise Separable Convolutions.* arXiv
    preprint arXiv:1610.02357 (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides important insights into one of the technologies responsible
    for the amazing new applications you see in the media every day. Also, with the
    practical example provided, you will even be able to create new customized solutions.
  prefs: []
  type: TYPE_NORMAL
- en: As our models won't be enough to solve very complex problems, in the following
    chapter, our scope will expand even more, adding the important dimension of time
    to the set of elements included in our generalization.
  prefs: []
  type: TYPE_NORMAL
