<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer045">
<h1 class="chapter-number" id="_idParaDest-96"><a id="_idTextAnchor099"/>6</h1>
<h1 id="_idParaDest-97"><a id="_idTextAnchor100"/>Leveraging Simulators and Rendering Engines to Generate Synthetic Data</h1>
<p>In this chapter, we will introduce a well-known method for synthetic data generation using simulators and rendering engines. We will explore the main pipeline for creating a simulator and generating automatically annotated synthetic data. Following this, we will highlight the challenges and briefly discuss two simulators for synthetic <span class="No-Break">data generation.</span></p>
<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Simulators and rendering engines: definitions, history, <span class="No-Break">and evolution</span></li>
<li>How to generate <span class="No-Break">synthetic data</span></li>
<li>Challenges <span class="No-Break">and limitations</span></li>
<li><span class="No-Break">Case studies</span></li>
</ul>
<h1 id="_idParaDest-98"><a id="_idTextAnchor101"/>Introduction to simulators and rendering engines</h1>
<p>In this section, we will dive into the world of simulators and rendering engines. We will look at the history and evolution of these powerful tools for synthetic <span class="No-Break">data generation.</span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor102"/>Simulators</h2>
<p>A <strong class="bold">simulator</strong> is <a id="_idIndexMarker223"/>software or a program written to imitate or simulate certain processes or phenomena of the real world. Simulators usually create a virtual world where scientists, engineers, and other users can test their algorithms, products, and hypotheses. At the same time, you can use this virtual environment to help you learn about and practice complex tasks. These tasks are usually dangerous and very expensive to perform in the real world. For example, driving simulators teach learners how to drive and how to react to unexpected scenarios such as a child suddenly crossing the street, which is extremely dangerous to do in the <span class="No-Break">real world.</span></p>
<p>Simulators are <a id="_idIndexMarker224"/>used in various fields, such as aviation, healthcare, engineering, driving, space, farming, and gaming. In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em>, you can find examples of <span class="No-Break">these simulators.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<img alt="Figure 6.1 – Examples of simulators utilized in driving, engineering, healthcare, and farming" height="460" src="image/Figure_06_01_B18494.jpg" width="906"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Examples of simulators utilized in driving, engineering, healthcare, and farming</p>
<p>Next, we will introduce rendering and <span class="No-Break">game engines.</span></p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor103"/>Rendering and game engines</h2>
<p>Rendering and<a id="_idIndexMarker225"/> game engines are software used mainly to generate images or videos. They<a id="_idIndexMarker226"/> are composed of various subsystems responsible for simulating, for example, physics, lighting, and sound. They are usually used in fields such as gaming, animation, virtual reality, augmented reality, and the metaverse. Unlike simulators, game engines can be used to create virtual worlds that may or may not be designed to mimic the real world. Game engines are mainly used to develop games. However, they can be utilized for training and simulation, films and television, and visualization. In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.2</em>, you can see some examples of modern rendering <a id="_idIndexMarker227"/>and <span class="No-Break">game engines.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<img alt="Figure 6.2 – Examples of modern rendering and game engines" height="525" src="image/Figure_06_02_B18494.jpg" width="539"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Examples of modern rendering and game engines</p>
<p>Next, we’ll learn more <a id="_idIndexMarker228"/>about the history of rendering and <span class="No-Break">game engines.</span></p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor104"/>History and evolution of simulators and game engines</h2>
<p>Game engines <a id="_idIndexMarker229"/>roughly started to appear in the 1970s. The computers <a id="_idIndexMarker230"/>of that era were limited in terms of processing capabilities and memory. At that time, most <a id="_idIndexMarker231"/>games were 2D games, such as <em class="italic">Pong</em> and <em class="italic">Spacewar!</em>. They<a id="_idIndexMarker232"/> were limited to simple graphics, basic lighting, elementary shading, and limited visual and <span class="No-Break">sound effects.</span></p>
<p>The great advancements in <a id="_idIndexMarker233"/>hardware led to more sophisticated game engines. These game engines, such as <em class="italic">Unreal </em>(<a href="https://www.unrealengine.com">https://www.unrealengine.com</a>) and <em class="italic">Unity </em>(<a href="https://unity.com">https://unity.com</a>) facilitated<a id="_idIndexMarker234"/> the creation of rich, photorealistic virtual worlds. As physics simulations became more sophisticated and advanced, the development of photorealistic graphics also progressed simultaneously. This allowed the simulation of complex physics and interactions between scene elements, such as fluid dynamics and cloth simulation. In more recent years, many photorealistic, complex games have been<a id="_idIndexMarker235"/> released, such<a id="_idIndexMarker236"/> as <em class="italic">Call of Duty</em> (<a href="https://www.callofduty.com">https://www.callofduty.com</a>) and <em class="italic">Grand Theft </em><span class="No-Break"><em class="italic">Auto </em></span><span class="No-Break">(</span><a href="https://www.rockstargames.com/games/vicecity"><span class="No-Break">https://www.rockstargames.com/games/vicecity</span></a><span class="No-Break">).</span></p>
<p>The <a id="_idIndexMarker237"/>availability <a id="_idIndexMarker238"/>of complex and easy-to-use game engines such as Unity has democratized game development and made it more accessible <a id="_idIndexMarker239"/>than ever before. Thus, game <a id="_idIndexMarker240"/>development is not just limited to big tech companies but is also available to independent companies <span class="No-Break">and artists.</span></p>
<p>At the same time, the huge sudden increase in the number of mobile phones recently made mobile games a more attractive destination for research <span class="No-Break">and industry.</span></p>
<p>Recently, synthetic data researchers started to experiment with using game engines and video games to generate rich synthetic data. Two of the pioneer works in this area are <em class="italic">Playing for Data: Ground Truth from Computer Games</em> (<a href="https://arxiv.org/abs/1608.02192">https://arxiv.org/abs/1608.02192</a>) and <em class="italic">Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real </em><span class="No-Break"><em class="italic">World</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/1703.06907.pdf"><span class="No-Break">https://arxiv.org/pdf/1703.06907.pdf</span></a><span class="No-Break">).</span></p>
<p>In the following section, we will explore exactly how to generate synthetic data using simulators and <span class="No-Break">game engines.</span></p>
<h1 id="_idParaDest-102"><a id="_idTextAnchor105"/>Generating synthetic data</h1>
<p>In this section, we <a id="_idIndexMarker241"/>will learn how to generate synthetic data using modern game engines such as Unity <span class="No-Break">and Unreal.</span></p>
<p>To generate synthetic data with its corresponding ground truth, it is recommended that we follow <span class="No-Break">these steps:</span></p>
<ol>
<li>Identify the task and ground truth <span class="No-Break">to generate.</span></li>
<li>Create the 3D virtual world in the <span class="No-Break">game engine.</span></li>
<li>Set the <span class="No-Break">virtual camera.</span></li>
<li>Add noise <span class="No-Break">and anomalies.</span></li>
<li>Set the <span class="No-Break">labeling pipeline.</span></li>
<li>Generate the training data with the <span class="No-Break">ground truth.</span></li>
</ol>
<p>Throughout this section, we will thoroughly discuss each facet of <span class="No-Break">this process.</span></p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor106"/>Identify the task and ground truth to generate</h2>
<p>The first step in<a id="_idIndexMarker242"/> the synthetic data generation process <a id="_idIndexMarker243"/>is defining the task, the type of the data, and the ground truth to generate. For example, the data could be images, videos, or audio. At the same time, you need to identify what ground truth to generate for your problem. For example, you can generate semantic segmentation, instance segmentation, depth maps, normal maps, human poses, and human body parts semantic segmentation, just to name <span class="No-Break">a few.</span></p>
<p>Next, we need to understand how to create the 3D virtual world, which we will explore in the <span class="No-Break">following section.</span></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor107"/>Create the 3D virtual world in the game engine</h2>
<p>To begin<a id="_idIndexMarker244"/> with, we must define the<a id="_idIndexMarker245"/> environment, its elements, and the interactions between these elements. You may need to decide on the level of photorealism, the degree of visual complexity, and the range of variations and diversity that you need to attain for your virtual scenes, and thus the synthetic data. In general, and for a typical plan to generate synthetic data, we can follow <span class="No-Break">these steps:</span></p>
<ol>
<li>Preparation <span class="No-Break">and conceptualization</span></li>
<li><span class="No-Break">Modeling</span></li>
<li>Materialization <span class="No-Break">and texturing</span></li>
<li>Integration into the <span class="No-Break">game engine</span></li>
<li>Polishing <span class="No-Break">and testing</span></li>
</ol>
<p>Next, we will delve into each of these aspects and provide <span class="No-Break">deeper insight.</span></p>
<h3>Preparation and conceptualization</h3>
<p>Before creating the 3D virtual world, we need to examine our ideas about the virtual world to be <a id="_idIndexMarker246"/>created. It is suggested to make simple drawings and sketches to visualize the elements of the world and how they will interact with each other. You may need to jot down the following: weather <a id="_idIndexMarker247"/>conditions to simulate, whether an indoor or outdoor environment, the time of day, and the scenes’ crowdedness, just to<a id="_idIndexMarker248"/> mention a few. Additionally, you need to decide which game engine<a id="_idIndexMarker249"/> to use, for <a id="_idIndexMarker250"/>instance, <strong class="bold">Unity</strong>, <strong class="bold">Unreal</strong>, or <strong class="bold">CryEngine</strong>. You also need to decide which rendering pipeline to utilize, which depends on the game engine <a id="_idIndexMarker251"/>itself. For example, the<a id="_idIndexMarker252"/> Unity game engine has different rendering pipelines, such <a id="_idIndexMarker253"/>as <strong class="bold">Built-in Render Pipeline (BRP)</strong>,<strong class="bold"> High-Definition Render Pipeline (HDRP)</strong>, <strong class="bold">Universal Render Pipeline (URP)</strong>, and <strong class="bold">Scriptable Render Pipeline (SRP)</strong>. The selection of the rendering <a id="_idIndexMarker254"/>pipeline also depends on the degree of photorealism that you want to achieve. Moreover, some game engines may support various programming languages, such as CryEngine, which supports C++ and C#. Thus, you may need to decide which language to use <span class="No-Break">as well.</span></p>
<p>After this, we need to determine the assets to use, such as objects, materials, visual effects, and sound effects. At the same time, you may need to consider the budget, timeframe, and the skills of <span class="No-Break">your team.</span></p>
<h3>Modeling</h3>
<p>The next step, after <a id="_idIndexMarker255"/>creating a solid idea about the 3D virtual world, is to start the modeling stage. <strong class="bold">3D modeling</strong> is the <a id="_idIndexMarker256"/>process of creating 3D objects using appropriate modeling software. 3D modeling is widely used in the game and entertainment industries, engineering fields, and architecture. To build the 3D virtual world, we need to create its elements, such as buildings, trees, pedestrians, and vehicles. Thus, we need to decide whether to import or model these <a id="_idIndexMarker257"/>elements. We can do 3D modeling using software such <a id="_idIndexMarker258"/>as <em class="italic">Blender </em>(<a href="https://www.blender.org">https://www.blender.org</a>), <em class="italic">ZBrush </em>(<a href="https://pixologic.com">https://pixologic.com</a>), and <em class="italic">3ds Max</em> (<a href="https://www.autodesk.co.uk/products/3ds-max">https://www.autodesk.co.uk/products/3ds-max</a>). As <a id="_idIndexMarker259"/>you may expect, a straightforward<a id="_idIndexMarker260"/> solution<a id="_idIndexMarker261"/> is importing these elements from websites such as <em class="italic">Adobe 3D Substance</em> (<a href="https://substance3d.adobe.com">https://substance3d.adobe.com</a>) and <em class="italic">Turbosquid </em>(<a href="https://www.turbosquid.com">https://www.turbosquid.com</a>). However, high-quality 3D models are usually expensive. Additionally, it should be noted that modeling complex 3D objects can be a challenging and time-consuming process that requires technical skills, effort, and time, but that depends on the object being modeled and technical constraints such as the <span class="No-Break">polygon count.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer042">
<img alt="Figure 6.3 – An example of a 3D car model (right) created from a car sketch (left)" height="385" src="image/Figure_06_03_B18494.jpg" width="1101"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – An example of a 3D car model (right) created from a car sketch (left)</p>
<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em> shows<a id="_idIndexMarker262"/> an example of the output that we get after the modeling stage, which is a car in <span class="No-Break">this instance.</span></p>
<h3>Materialization and texturing</h3>
<p>After creating a <a id="_idIndexMarker263"/>3D model or a mesh, we <a id="_idIndexMarker264"/>need to add the physical properties of this object, such as the color, transparency, and reflectivity. These properties simulate the matter and the surface of the objects. On the other hand, texturing is used to simulate surface details such as scratches and patterns and to give the object a non-uniform appearance similar to the object’s appearance in the real world. In most game engines, this information is encoded using a texture map. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em> shows a 3D object after the materialization and <span class="No-Break">texturing process.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<img alt="Figure 6.4 – A 3D object before (left) and after (right) materialization and texturing" height="386" src="image/Figure_06_04_B18494.jpg" width="925"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – A 3D object before (left) and after (right) materialization and texturing</p>
<h3>Integration into the game engine</h3>
<p>When the <a id="_idIndexMarker265"/>elements of the 3D virtual world are ready, we need to add them to our scene. We also need to configure and set up lighting and a <span class="No-Break">virtual camera:</span></p>
<ul>
<li><strong class="bold">Lighting</strong>: This is<a id="_idIndexMarker266"/> an essential step for creating photorealistic scenes. Lights are added to the virtual worlds to give a sense of depth and atmosphere. There are usually two options for lighting: <strong class="bold">pre-rendered lighting</strong> using <a id="_idIndexMarker267"/>lightmaps and <strong class="bold">real-time lighting</strong>. Lighting is <a id="_idIndexMarker268"/>fundamental but it is expensive computation-wise. Thus, you should pay attention to this step to achieve your target photorealism and <span class="No-Break">frame rate.</span></li>
<li><strong class="bold">Virtual camera</strong>: Once <a id="_idIndexMarker269"/>the virtual world is generated, a virtual camera is utilized to capture the required synthetic data. The behavior of the camera is usually controlled using a script. The camera parameters and behaviors can be configured to match the real-world scenario and to <a id="_idIndexMarker270"/>achieve the<a id="_idIndexMarker271"/> intended <a id="_idIndexMarker272"/>behavior. Camera parameters<a id="_idIndexMarker273"/> include <strong class="bold">Field of View (FoV)</strong>, <strong class="bold">Depth of Field (DoF)</strong>, <strong class="bold">Sensor Size</strong>, and <span class="No-Break"><strong class="bold">Lens Shift</strong></span><span class="No-Break">.</span></li>
</ul>
<h3>Polishing and testing</h3>
<p>The<a id="_idIndexMarker274"/> last<a id="_idIndexMarker275"/> step is to examine the generated synthetic data and iterate on the virtual world design. In this stage, you can fix bugs and optimize the performance. As expected, creating a 3D virtual world is not a simple<a id="_idIndexMarker276"/> process. It requires effort, time, and technical skills. However, once<a id="_idIndexMarker277"/> the virtual world is created, it can be leveraged to generate large-scale synthetic datasets for <span class="No-Break">enormous applications.</span></p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor108"/>Setting up the virtual camera</h2>
<p>In the <a id="_idIndexMarker278"/>virtual world, the camera plays a vital role in the synthetic data generation process. It represents the observer, and it is usually utilized to capture images, audio, and videos. The captured data may be used for training and testing <span class="No-Break">ML models.</span></p>
<p>As we have mentioned previously, camera properties and attributes can be customized and configured to achieve the target behavior. For example, the camera FoV controls how much of the world your observer agent can perceive, and therefore, how much visual information you can capture in a single generated image. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.5</em> shows two images generated with different <span class="No-Break">FoV values.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer044">
<img alt="Figure 6.5 – A scene captured using two different FoVs in the Unreal game engine" height="691" src="image/Figure_06_05_B18494.jpg" width="783"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – A scene captured using two different FoVs in the Unreal game engine</p>
<p>Please note that <a id="_idIndexMarker279"/>the camera position is fixed, and we only changed the FoV. Additionally, we can control the camera motion and transition to achieve the <span class="No-Break">required behavior.</span></p>
<p>The camera can take different setups in the 3D virtual world to imitate the relevant ones in the real world. These are some examples of <span class="No-Break">camera setups:</span></p>
<ul>
<li><strong class="bold">Fixed camera</strong>: The <a id="_idIndexMarker280"/>camera does not change its<a id="_idIndexMarker281"/> location or orientation while it captures the scene. This camera setup can be used to record the scene from a specific viewpoint. It is the simplest setup; it is easy to implement, and it does not require scripting. However, you need to pay attention to the position and the attributes of the camera. Otherwise, dynamic objects may accidentally block the view of the camera. In simulators and game engines, a fixed camera can be used, for<a id="_idIndexMarker282"/> instance, to imitate a traffic<a id="_idIndexMarker283"/> monitoring camera or a fixed camera used in <span class="No-Break">sports broadcasting.</span></li>
<li><strong class="bold">PTZ camera</strong>: This<a id="_idIndexMarker284"/> is a special type of camera setup in <a id="_idIndexMarker285"/>which the camera can pan, tilt, and zoom. In the real world, this type is usually controlled by an operator to capture an object of interest or a specific event. Thus, the camera can change its orientation and FoV to realize that. In a virtual world, the camera can be programmed to achieve that, or it can be controlled by a human operator during simulation or the synthetic data generation process. This setup gives you more freedom to capture the scene. However, it may require scripting to achieve the intended <span class="No-Break">camera behavior.</span></li>
<li><strong class="bold">First-person camera</strong>: First-person<a id="_idIndexMarker286"/> vision is a<a id="_idIndexMarker287"/> fundamental field in computer vision. This camera setup simulates an agent observing the world by wearing a camera. It has <a id="_idIndexMarker288"/>enormous applications in gaming and virtual reality, law enforcement, medicine, and education. For example, an ML model trained on first-person data can be used to assist surgeons and improve training, decision-making, performance, and accuracy. For a detailed discussion, refer to <em class="italic">Artificial Intelligence for Intraoperative Guidance: Using Semantic Segmentation to Identify Surgical Anatomy During Laparoscopic </em><span class="No-Break"><em class="italic">Cholecystectomy</em></span><span class="No-Break"> (</span><a href="https://pubmed.ncbi.nlm.nih.gov/33196488"><span class="No-Break">https://pubmed.ncbi.nlm.nih.gov/33196488</span></a><span class="No-Break">).</span></li>
<li><strong class="bold">Aerial or UAV camera</strong>: This <a id="_idIndexMarker289"/>camera setup<a id="_idIndexMarker290"/> is key for flight and drone simulators. It simulates a camera mounted on a drone or UAV. Usually, it is used to simulate a birds-eye view of the scene. It has a wide spectrum of applications and can be used to enhance the performance of ML models that require training images captured using drones. It supports various computer vision tasks, such as object detection, classification, tracking, <span class="No-Break">and recognition.</span></li>
<li><strong class="bold">Stereoscopic camera</strong>: A stereo <a id="_idIndexMarker291"/>camera is a <a id="_idIndexMarker292"/>special type of camera with two lenses separated by a short distance that can be leveraged to simulate how humans perceive depth. The distance between the two lenses is<a id="_idIndexMarker293"/> called <strong class="bold">intra-ocular distance</strong> and it is usually <a id="_idIndexMarker294"/>similar to the distance between a human’s eyes: approximately 6.35 cm. This distance is essential for creating a sense of depth in the vision system. This type of camera is important for VR and immersive <span class="No-Break">3D experiences.</span></li>
<li><strong class="bold">Tracking camera</strong>: This<a id="_idIndexMarker295"/> type is used to track an object of <a id="_idIndexMarker296"/>interest. In the virtual world, this camera can be programmed to follow the desired object, which facilitates creating large-scale synthetic data focused on a target object. For example, it is possible to track a human in the virtual world for an action recognition <a id="_idIndexMarker297"/>task. This will help you to generate large-scale training data focused on your subject (human). It is possible to use other camera setups, but you will end up having many videos with no actions or where your object of interest is not apparent. Additionally, you may use this camera setup for visual object tracking and other <span class="No-Break">similar tasks.</span></li>
</ul>
<p>The next step is <span class="No-Break">adding noise.</span></p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor109"/>Adding noise and anomalies</h2>
<p>The real world<a id="_idIndexMarker298"/> is not perfect and has anomalies. In the context of image generation, noise and anomalies refer to a deviation from the main pattern, process, and phenomenon. For example, when we observe street light poles at night, a small portion of them may have been accidentally turned off, the light may be flickering, the pole may be slightly rotated, painted a different color, or have different dimensions. Adding noise and anomalies to the attributes and behaviors of virtual world elements improves realism and boosts the usability of the generated synthetic data. Another example<a id="_idIndexMarker299"/> regarding anomalies in behavior can be seen when observing pedestrians crossing the road. The majority wait for the green light or walk signal, look both ways, and cross on the crosswalk. On the other hand, a small portion may cross the road when the red light is on or may cross without paying attention to oncoming cars. This behavior anomaly, for example, should be simulated in the virtual world to ensure that the generated training data is diverse. Thus, training an ML model on this data will ensure a robust <span class="No-Break">ML model.</span></p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor110"/>Setting up the labeling pipeline</h2>
<p>The <a id="_idIndexMarker300"/>labeling pipeline depends on your problem and how you plan to utilize the synthetic data. You may want to just generate the training data because it is too expensive in the real world, and you may prefer to ask human annotators to annotate your data. On the other hand, it is possible that you want to automate the annotation process in the simulator or the rendering engine. Simulators such as CARLA, NOVA, and Silver support generating the data with its corresponding ground truth for various computer <span class="No-Break">vision tasks.</span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor111"/>Generating the training data with the ground truth</h2>
<p>At this point, the synthetic data<a id="_idIndexMarker301"/> generation pipeline should be ready. The previous steps can be challenging, costly, and time-consuming. However, they are only required to set up the system. Following this, you can leverage the system to generate your task-specific, automatically annotated, large-scale datasets. Changing the annotating protocol is simple and is not expensive compared to real-world datasets. Please note that we have not provided hands-on examples on how to generate synthetic data using game engines or simulators because the focus of the book is not on the implementation and coding of synthetic data generation approaches. However, it is committed to the theoretical, conceptual, and design aspects of the process. For more details about the implementation and coding aspects, please<a id="_idIndexMarker302"/> refer to Unity Computer Vision (<a href="https://unity.com/products/computer-vision">https://unity.com/products/computer-vision</a>) and Synthetic for <a id="_idIndexMarker303"/>Computer <span class="No-Break">Vision (</span><a href="https://github.com/unrealcv/synthetic-computer-vision"><span class="No-Break">https://github.com/unrealcv/synthetic-computer-vision</span></a><span class="No-Break">).</span></p>
<p>In the next section, you will <a id="_idIndexMarker304"/>learn about the main limitations of deploying this synthetic data <span class="No-Break">generation approach.</span></p>
<h1 id="_idParaDest-109"><a id="_idTextAnchor112"/>Challenges and limitations</h1>
<p>In this section, we will highlight the main challenges in using this approach for synthetic data generation. We will look at realism, diversity, and complexity issues that present some difficulties in utilizing this approach for synthetic <span class="No-Break">data generation.</span></p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor113"/>Realism</h2>
<p>The <strong class="bold">domain gap</strong> between<a id="_idIndexMarker305"/> synthetic and <a id="_idIndexMarker306"/>real data is one of the main issues that limit the usability of synthetic data. For synthetic data to be useful, it should mimic the distribution and statistical characteristics of its real counterparts. Thus, for computer vision problems, we need to ensure a high degree of photorealism, otherwise, ML models trained on synthetic data may not generalize well to <span class="No-Break">real data.</span></p>
<p>Achieving a high degree of photorealism using game engines and simulators is not a simple task. Even with the help of contemporary game engines such as CryEngine, Unreal, and Unity, we need effort, skill, and time to create <span class="No-Break">photorealistic scenes.</span></p>
<p>The three essential elements for approaching realism and thus mitigating the domain gap problem for synthetic data generated by game engines and simulators are <span class="No-Break">as follows.</span></p>
<h3>Photorealism</h3>
<p>Generating photorealistic images, for <a id="_idIndexMarker307"/>computer vision problems, is vital for training and testing ML models. Building photorealistic scenes requires <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">High-quality assets</strong>: The 3D models, textures, and materials should be detailed <span class="No-Break">and realistic.</span></li>
<li><strong class="bold">Lighting</strong>: It is essential for rendering photorealistic scenes. You may need to use physically based rendering and physically based materials. Additionally, you need to use suitable light sources and carefully configure <span class="No-Break">their parameters.</span></li>
<li><strong class="bold">Postprocessing</strong>: Game engines such as Unreal and Unity support postprocessing effects to improve photorealism. For example, you can use these techniques to simulate motion blur, gloom, and depth <span class="No-Break">of field.</span></li>
</ul>
<h3>Realistic behavior</h3>
<p>To achieve<a id="_idIndexMarker308"/> this, we need to ensure realistic camera behavior. For example, the camera should not penetrate walls and its parameters should be close to real-world camera ones. Additionally, character animations should be realistic and emulate human body movement. Furthermore, scene element interactions should obey physics rules, for example, when <span class="No-Break">objects collide.</span></p>
<h3>Realistic distributions</h3>
<p>Objects and<a id="_idIndexMarker309"/> attributes are not randomly distributed in the real world. Thus, when building virtual worlds, we need to pay attention to matching these distributions as well. For example, people walking near shopping malls may have a higher probability of carrying objects. At the same time, under certain weather conditions, specific actions and objects may become more frequent or less. For example, in rainy weather conditions, pedestrians may <span class="No-Break">carry umbrellas.</span></p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor114"/>Diversity</h2>
<p>The world <a id="_idIndexMarker310"/>around us is remarkably varied and contains a multitude of elements that come in diverse colors, shapes, and behaviors, and possess different properties. Attaining a diverse virtual world requires time and effort. The usability of synthetic data comes from its primary advantage of generating large-scale datasets for training ML models. If the data is not diverse enough, this will cause ML models to overfit limited scenarios <span class="No-Break">and attributes.</span></p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor115"/>Complexity</h2>
<p>Nonlinearity, interdependence, uncertainty, and the dynamic nature of the real world make creating a <a id="_idIndexMarker311"/>realistic virtual world rather a complex task. Creating and simulating a realistic environment requires approximations, simplifications, and generalizations. This is necessary because of the trade-off between realism and computational complexity. Building a realistic virtual world that captures all real-world properties, phenomena, and processes is simply not feasible, even with state-of-the-art software and hardware. However, we can still approach an acceptable level of realism with a careful understanding of the ML problem: what is essential and what is auxiliary for this particular application? For example, if we<a id="_idIndexMarker312"/> would like to generate synthetic data for a face recognition task, we may need to pay extra attention to simulating photorealistic faces as compared to other elements in <span class="No-Break">the scene.</span></p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor116"/>Looking at two case studies</h1>
<p>In this section, we will briefly discuss two well-known simulators for synthetic data generation, and comment on the potential of using <span class="No-Break">these approaches.</span></p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor117"/>AirSim</h2>
<p>AirSim is an <a id="_idIndexMarker313"/>open source, cross-platform simulator developed by Microsoft using the Unreal game engine. It simulates drones and cars, opening the door for enormous applications in computer vision for DL and RL approaches for autonomous driving. Some of the key features of this simulator include <span class="No-Break">the following:</span></p>
<ul>
<li>Various weather effects <span class="No-Break">and conditions</span></li>
<li>LIDAR and <span class="No-Break">infrared sensors</span></li>
<li><span class="No-Break">Customizable environment</span></li>
<li>Realistic physics, environments, <span class="No-Break">and sensors</span></li>
</ul>
<p>As you can see, AirSim can be leveraged to generate rich, large-scale, and high-quality synthetic data from various sensors. Researchers in ML can train their models to fuse the different data modalities to develop more robust autonomous driving algorithms. Additionally, AirSim provides automatically labeled synthetic data for depth estimation, semantic segmentation, and surface normal estimation tasks. For more information about this simulator, please refer to <span class="No-Break"><em class="italic">AirSim</em></span><span class="No-Break"> (</span><a href="https://www.microsoft.com/en-us/AI/autonomous-systems-project-airsim"><span class="No-Break">https://www.microsoft.com/en-us/AI/autonomous-systems-project-airsim</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-115"><a id="_idTextAnchor118"/>CARLA</h2>
<p>CARLA is <a id="_idIndexMarker314"/>an open source simulator for autonomous driving. It was developed using the Unreal game engine <a id="_idIndexMarker315"/>by <strong class="bold">Computer Vision Centre</strong> (<strong class="bold">CVC</strong>), Intel, and Toyota. CARLA is a well-known simulator for synthetic data generation. It has a traffic manager system and users can configure several sensors, which include depth sensors, LIDARs, multiple cameras, and <strong class="bold">Global Positioning System</strong> (<strong class="bold">GPS</strong>). CARLA <a id="_idIndexMarker316"/>generates synthetic data for a number of computer vision tasks, such as semantic segmentation, depth estimation, object detection, and visual object tracking. In addition to generating automatically labeled and large-scale synthetic data, CARLA can be deployed to generate diverse traffic scenarios. Then, researchers can utilize the generated synthetic data to train more accurate and robust ML models on a myriad of driving scenarios. Please check the <a id="_idIndexMarker317"/>project’s <em class="italic">CARLA</em> web page (<a href="https://carla.org">https://carla.org</a>) and the <em class="italic">CARLA: An Open Urban Driving Simulator</em> paper (<a href="http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf">http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf</a>) for <span class="No-Break">more details.</span></p>
<p>There are many other simulators, such as Silver, AI Habitat, SynthCity, and IGibson. Creating a more realistic simulator, supporting more tasks, making the simulator easier to use, and the virtual environment more customizable are the main research directions in developing future synthetic data generators using game engines <span class="No-Break">and simulators.</span></p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor119"/>Summary</h1>
<p>In this chapter, we introduced a well-known method for synthetic data generation based on simulators and rendering engines. We learned how to generate synthetic data. We highlighted the main challenges and we discussed AirSim and CARLA simulators as examples of this data generation approach. We have seen that by using simulators and game engines, we can generate large-scale, rich, and automatically annotated synthetic data for many applications. It reduces the cost and effort and provides an ideal solution for training robust <span class="No-Break">ML models.</span></p>
<p>In the next chapter, we will learn about a new method for synthetic data generation using <strong class="bold">Generative Adversarial </strong><span class="No-Break"><strong class="bold">Networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GANs</strong></span><span class="No-Break">).</span></p>
</div>
</div></body></html>