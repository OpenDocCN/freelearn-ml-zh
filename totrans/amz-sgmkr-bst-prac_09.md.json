["```py\nEstimator(\n    …\n    debugger_hook_config=DebuggerHookConfig(\n        s3_output_path=bucket_path,  # Where the debug data is stored.\n        collection_configs=[ # Organize data to collect into collections.\n            CollectionConfig(\n                name=\"metrics\",\n                parameters={\n                    \"save_interval\": str(save_interval)\n                }\n            )\n        ],\n    ),\n    ….\n)\n```", "```py\n# Use Debugger CollectionConfig to create a custom collection\ncollection_configs=[\n        CollectionConfig(\n            name=\"custom_collection\",\n            parameters={\n                \"include_regex\": \".*relu |.*tanh | *weight \",\n        })\n]\n```", "```py\nEstimator(\n    …\n    rules=[\n        Rule.sagemaker(\n            rule_configs.loss_not_decreasing(),\n            rule_parameters={\n                \"collection_names\": \"metrics\",\n                \"num_steps\": str(save_interval * 2),\n            },\n        ),\n    ],\n)\n```", "```py\nEstimator(\n    …\n    profiler_config = ProfilerConfig(\n        ## Monitoring interval in milliseconds\n     system_monitor_interval_millis=500,       ## Start collecting metrics from step 2 and collect from the next 7 steps.\n      framework_profile_params=FrameworkProfile(\n    start_step=2, \n    num_steps=7\n)     )\n```", "```py\ntensors_path = estimator.latest_job_debugger_artifacts_path()\nprint('S3 location of tensors is: ', tensors_path)\ntrial.tensor_names()\ntrial.tensor(\"feature_importance/cover/f1\").values()\n```", "```py\nbuilt_rules=[\n        #Check for loss not decreasing during training and stop the training job.\n        Rule.sagemaker(\n            rule_configs.loss_not_decreasing(),\n            actions = (rule_configs.StopTraining())\n        )\n]\n```", "```py\n#Specify the rules you want to run\nbuilt_in_rules=[\n        #Check for loss not decreasing during training and stop the training job.\n        Rule.sagemaker(\n            rule_configs.loss_not_decreasing(),\n\n            actions = (rule_configs.StopTraining())\n        ),\n        #Check for overfit, overtraining and stalled training\n        Rule.sagemaker(rule_configs.overfit()),  \n   Rule.sagemaker(rule_configs.overtraining()),       \n   Rule.sagemaker(rule_configs.stalled_training_rule())     \n]\n#Create an estimator and pass in the built_in rules.\npt_estimator = PyTorch(\n    ...\n    rules = built_in_rules\n)\n```", "```py\npt_estimator.latest_training_job.rule_job_summary()\n```", "```py\nclass CustomGradientRule(Rule):\n    def __init__(self, base_trial, threshold=10.0):\n        super().__init__(base_trial)\n        self.threshold = float(threshold)\n    def invoke_at_step(self, step):\n        for tname in self.base_trial.tensor_names(collection=\"gradients\"):\n            t = self.base_trial.tensor(tname)\n            abs_mean = t.reduction_value(step, \"mean\", abs=True)\n            if abs_mean > self.threshold:\n                return True\n        return False\n```", "```py\ncustom_rule = Rule.custom(\n    name='CustomRule', # used to identify the rule\n    # rule evaluator container image\nimage_uri='759209512951.dkr.ecr.us-west-2.amazonaws.com/sagemaker-debugger-rule-evaluator:latest',    instance_type='ml.t3.medium',     source='rules/my_custom_rule.py', # path to the rule source file\n    rule_to_invoke='CustomGradientRule', # name of the class to invoke in the rule source file\n    volume_size_in_gb=30, # EBS volume size required to be attached to the rule evaluation instance\n    collections_to_save=[CollectionConfig(\"gradients\")],\n    # collections to be analyzed by the rule. since this is a first party collection we fetch it as above\n    rule_parameters={\n       #Threshold to compare the gradient value against\n      \"threshold\": \"20.0\"     }\n)\n```", "```py\npt_estimator_custom = PyTorch(\n    ….\n    ## New parameter\n    rules = [custom_rule]\n)\nestimator.fit(wait = False)\n```", "```py\npt_estimator.latest_training_job.rule_job_summary()\n```", "```py\n…\ntrain_instance_type = \"ml.p3.2xlarge\" \ninstance_count = 2\n```", "```py\nprofiler_config = ProfilerConfig(\n    system_monitor_interval_millis=500,\n    framework_profile_params=FrameworkProfile(start_step=2, num_steps=7)\n)\n```", "```py\npt_estimator = PyTorch(\n    entry_point=\"train_pytorch.py\",\n    source_dir=\"code\",\n    role=sagemaker.get_execution_role(),\n    instance_count=instance_count,\n    instance_type=train_instance_type,\n    framework_version=\"1.6\",\n    py_version=\"py3\",\n    volume_size=1024,\n    # Debugger-specific parameters\n    profiler_config=profiler_config,\n)\n```", "```py\nestimator.fit(inputs, wait= False)\n```", "```py\n#All collected metrics are persisted in S3.  Define path to the profiler artifacts\npath = estimator.latest_job_profiler_artifacts_path()\n#Create a reader for the system metrics\nsystem_metrics_reader = S3SystemMetricsReader(path)\n#Get the latest event\nlast_timestamp = system_metrics_reader.get_timestamp_of_latest_available_file()\nevents = system_metrics_reader.get_events(0, last_timestamp * 1000000)  # UTC time in microseconds\n#Show the first system metric event collected\nprint(\n    \"Event name:\",  events[0].name,\n    \"\\nTimestamp:\",  timestamp_to_utc(events[0].timestamp),\n    \"\\nValue:\", events[0].value,\n)\n```", "```py\nEvent name: gpu2 \nTimestamp: 2021-07-02 18:44:20 \nValue: 0.0\n```", "```py\n#Create a reader for the system metrics\nframework_metrics_reader = S3AlgorithmMetricsReader(path)\nframework_metrics_reader.refresh_event_file_list()\nlast_timestamp = framework_metrics_reader.get_timestamp_of_latest_available_file()\nevents = framework_metrics_reader.get_events(0, last_timestamp)\n#We can inspect one of the recorded events to get the following:\nprint(\"Event name:\", events[0].event_name, \n      \"\\nStart time:\", timestamp_to_utc(events[0].start_time/1000000000), \n      \"\\nEnd time:\", timestamp_to_utc(events[0].end_time/1000000000), \n      \"\\nDuration:\", events[0].duration, \"nanosecond\")\n```", "```py\nEvent name: embeddings.0 \nStart time: 1970-01-19 19:27:42 \nEnd time: 1970-01-19 19:27:42 \nDuration: 141298 nanosecond\n```", "```py\ntrain_instance_type='ml.p3.2xlarge'\ninstance_count = 2\nhyperparameters = {\"batch_size\": 1024}\n```", "```py\ntraining_job_name=\ntrain_instance_type='ml.p2.8xlarge'\ninstance_count = 2\nhyperparameters = {\"batch_size\": 1024}\n```"]