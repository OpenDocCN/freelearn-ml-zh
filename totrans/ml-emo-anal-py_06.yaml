- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naïve Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons and
    Vector-Space Models*, we investigated the use of simple lexicon-based classifiers,
    using both a hand-coded sentiment lexicon and extracting a lexicon from a corpus
    of marked-up texts. The results from this investigation were that such models
    can produce reasonable scores, with a variety of tweaks (using a stemmer or changing
    the way that weights are calculated, such as by using TF-IDF scores) that produce
    improvements in some cases but not in others. We will now turn to a range of machine
    learning algorithms to see whether they will lead to better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'For most of the algorithms that we will be looking at, we will use the Python
    scikit-learn (`sklearn`) implementations. A wide range of implementations for
    all these algorithms are available. The `sklearn` versions have two substantial
    advantages: they are freely available with a fairly consistent interface to the
    training and testing data and they can be easily installed and run on a standard
    computer. They also have a significant disadvantage, in that some of them are
    slower than versions that have been designed to run on computers with fast GPUs
    or other highly parallel processors. Fortunately, most of them run reasonably
    quickly on a standard machine, and even the slowest with our largest dataset can
    train a model in about half an hour. So, for the tasks we are investigating here,
    where we are interested in comparing the performance of the various algorithms
    on identical datasets, the advantages outweigh the fact that on very large datasets,
    some of them will take an infeasibly long time.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at the Naïve Bayes algorithm. We will look at
    the effects of the various preprocessing steps we used in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector Space Models* but we will not look at all the tweaks
    and parameter settings that are provided with this package. The various `sklearn`
    packages provide a range of options that can affect either the accuracy or the
    speed of the given algorithm, but we will not generally try out all the options
    – it is very easy to get distracted into playing with the parameters in the hope
    of gaining a few percentage points, but for our goal of looking at ways of carrying
    out sentiment mining, it is more useful to consider how changes to the data can
    affect performance. Once you have chosen your algorithm, then it may be worth
    investigating the effects of changing the parameters, but this book aims to see
    what the algorithms do with tweets that have been annotated with emotion labels,
    not to look at all the minutiae of the algorithms themselves.
  prefs: []
  type: TYPE_NORMAL
- en: We will start this chapter by looking at how to prepare our datasets to match
    the `sklearn` representation. We will then give a brief introduction to the Naïve
    Bayes approach to machine learning, and then
  prefs: []
  type: TYPE_NORMAL
- en: apply the Naïve Bayes implementation from `sklearn.naive_bayes.MultinomialNB`
    to our datasets and consider why the algorithm behaves as it does and what we
    can do to improve its performance on our data. By the end of this chapter, you’ll
    have a clear understanding of the theory behind Naive Bayes and the effectiveness
    of this as a way of assigning emotions to tweets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for `sklearn`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes as a machine learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naively applying Bayes' theorem as a classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-label and multi-class datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data for sklearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sklearn` packages expect training data consisting of a set of data points,
    where each data point is a real-valued vector, and a set of numerical labels representing
    the class to which each data point has been assigned. Our data consists of sets
    of tweets, where each tweet is represented by, among other things, a set of words
    and a set of values such as `[0, 0, 1, 1, 0, 0]`, where each element of the set
    corresponds to a single dimension. So, if the set of emotions in some training
    set were `['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']`, then the `[0,
    0, 1, 1, 0, 0]set` would indicate that the given tweet had been labeled as expressing
    joy and love. We will use the CARER dataset to illustrate how to convert our datasets
    into, as near as possible, the format required by the `sklearn` packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, we will represent a dataset as a DATASET, as defined in [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment* *Lexicons and* *Vector Space
    Models.* To convert a dataset into a form that is suitable for `sklearn`, we have
    to convert the one-hot encoding of the assignment of labels to a tweet into a
    single numerical label and the tokens that represent a tweet into a sparse matrix.
    The first of these is straightforward: we just enumerate the list of values until
    we hit a non-zero case, at which point the index of that case is the required
    value. If there is more than one non-zero column, this encoding will just record
    the first that is found – this will distort the data, but it is inevitable if
    we use a one-hot encoding for data with multiple labels. The only complication
    arises when it is possible for a tweet to have no label assigned to it because
    in that case, we will get to the end of the list without returning a value. If
    `allowZeros` is set to `True`, then we will return a column beyond the actual
    range of possible cases – that is, we will encode the absence of a value as a
    new explicit value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this to help construct the sparse matrix representation of the training
    set, as discussed in the *Vector spaces* section in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector Space Models.* To make a sparse matrix, you must
    collect parallel lists of rows, columns, and data for all cases where the data
    is non-zero. So, what we have to do is go through the tweets one by one (tweet
    number = row number), and then go through the tokens in the tweet; we must look
    the token up in the index (token index = column number), work out what value we
    want to use for that token (either 1 or its `idf`), and add those to `rows`, `columns`,
    and `data`. Once we have these three lists, we can just invoke the constructor
    for sparse matrices. There are several forms of sparse matrices: `csc_matrix`
    makes a representation that is suitable when each row contains only a few entries.
    We must exclude words that occur no more than `wthreshold` times because including
    very rare words makes the matrix less sparse, and hence slows things down, without
    improving the performance of the algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we can convert the representation of the labels assigned to a tweet into
    a one-hot format and we can convert a set of tweets with Gold Standard labels
    into a sparse matrix, we have all we need for making a classifier. Exactly what
    we do with these structures will depend on the type of classifier and the one-hot
    values for the data points to single class identifiers. All our `sklearn` classifiers
    will be subclasses of a generic class called `SKLEARNCLASSIFIER`: the definition
    of `SKLEARNCLASSIFIER` does not include a constructor. We will only ever make
    instances of subclasses of this class, so it is in some ways like an abstract
    class – it provides some methods that will be shared by several subclasses, such
    as for making Naïve Bayes classifiers, support vector machine classifiers, or
    deep neural network'
  prefs: []
  type: TYPE_NORMAL
- en: 'classifiers, but we will never actually make a `SKLEARNCLASSIFIER` class. The
    first thing we will need in `SKLEARNCLASSIFIER` is something for reading the training
    data and converting it into a sparse matrix. `readTrainingData` does this by using
    `makeDATASET` from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment
    Lexicons and Vector-Space Models*, and then converting the training data into
    a sparse matrix and the labels associated with the training data into one-hot
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need a function to apply a classifier to a tweet. The default value
    for this, defined as a method of `SKLEARNCLASSIFIER`, wraps up the `predict` method
    for the underlying `sklearn` class and returns the result in one of several formats,
    depending on what is wanted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: All our classifiers that make use of `sklearn` will be subclasses of this generic
    type. They will all make use of `readTrainingData` – that is, the machinery for
    converting sets of tweets into sparse matrices – and they will all require a version
    of `applyToTweet`. `SKLEARNCLASSIFIER` provides the default versions of these,
    though some of the classifiers may override them. The first classifier that we
    will develop using `SKLEARNCLASSIFIER` as a base class will involve using Bayes’
    theorem as a way of assigning probabilities to events. First, we will look at
    the theory behind Bayes’ theorem and its use for classification before turning
    to the details of how this may be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes as a machine learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key idea behind the Naïve Bayes algorithm is that you can estimate the likelihood
    of some outcome given a set of observations by using **conditional probabilities**
    and linking the individual observations to the outcome. Defining what conditional
    probability is turns out to be surprisingly slippery because the notion of probability
    itself is very slippery. Probabilities are often defined as something similar
    to proportions, but this view becomes difficult to maintain when you are looking
    at unique or unbounded sets, which is usually the case when you want to make use
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose, for instance, that I am trying to work out how likely it is that France
    will win the FIFA 2022 World Cup (this is being written 2 days before the final,
    between France and Argentina, is to be played). In some sense, it is reasonable
    to ask about this probability – if the bookmakers are offering 3 to 1 against
    France and the probability that they will win is 0.75, then I should place a bet
    on that outcome. But this probability *cannot* be defined as *#(times that France
    win the 2022 World Cup)/#(times that France play in the 2022 World Cup final)*.
    Right now, both those numbers are 0, so the probability appears to be 0/0, which
    is undefined. By the time you are reading this, the first of them will be either
    0 or 1 and the second will be 1, so the probability that France has won the World
    Cup will be either 0 or 1\. Bookmakers and gamblers will make estimates of this
    likelihood, but they cannot do so by actually counting the proportion of times
    the outcome of this yet-to-be-played match comes out in France’s favor.
  prefs: []
  type: TYPE_NORMAL
- en: So, we cannot define the likelihood of an outcome for a future one-off event
    in terms of the proportion of times that the event has the given outcome since
    we have not yet observed the outcome, and we cannot sensibly define the likelihood
    of an outcome for a past one-off event this way either, since it is bound to be
    either 0 or 1 once the event has occurred.
  prefs: []
  type: TYPE_NORMAL
- en: But we also cannot define the likelihood of an outcome, for instance, of a series
    of apparently similar events as a proportion. The fact that I have seen it become
    lighter in the morning every day of my life – that is, 25,488 out of 25,488 times
    – does not mean that the likelihood of it getting lighter tomorrow morning is
    1\. Tomorrow morning might be different. The sun may have turned into a black
    hole and have stopped emitting radiation. There may have been an enormous volcanic
    eruption and the sky might be completely blotted out. *Tomorrow may not be the
    same* *as today*.
  prefs: []
  type: TYPE_NORMAL
- en: 'And we also can’t define the likelihood that a member of an unbounded set satisfies
    some property in terms of the proportion of times that members of a finite subset
    of that property satisfy it. Consider the likelihood that a randomly chosen integer
    is prime. If we plot the number of occurrences of a prime number in the first
    few integers, we get a plot similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The number of primes in the first N integers](img/B18714_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – The number of primes in the first N integers
  prefs: []
  type: TYPE_NORMAL
- en: It looks as though the number of primes in the first 10,000 integers goes up
    linearly, with about 10% of numbers being prime. If we look at the first 100,000,000,
    then about 6% are prime. What is the true probability? It cannot be defined as
    the ratio of the number of primes to the number of integers because these two
    are both infinite and ∞/∞ is undefined. It looks as though the proportion declines
    as we look at more cases, so it probably tends to 0, but it isn’t 0\. It turns
    out to be very hard to either define or estimate probabilities involving unbounded
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: We can *estimate* the probability of the first two kinds of events. We can look
    at all football matches between teams that we believe to be similar to the current
    French and Argentinian teams and use the number of times that the team that is
    like the current French one beat the one that is like the current Argentinian
    one. I can look back at all the days of my life and say that if tomorrow is just
    like all the others *in all relevant respects*, then my estimate of the likelihood
    that it will get lighter in the morning is 1\. But these are just estimates and
    they depend on the next event being the same as the previous ones in all relevant
    respects.
  prefs: []
  type: TYPE_NORMAL
- en: This has been a thorny issue in probability theory and statistics since the
    19th century. Due to this, Thomas Bayes, among other people, defined probability
    as being, essentially, the odds that someone might reasonably assign for an outcome
    (Bayes, T, 1958). Counting previous experience might well be an important part
    of the information that such a person might use in coming up with their reasonable
    assignment, but since it is not possible to know that the next event will be similar
    to past ones in all relevant aspects, it cannot be used as the definition.
  prefs: []
  type: TYPE_NORMAL
- en: So, we cannot say what the probability of a given outcome is. What we can do,
    however, is define how such a probability should behave if we had it. If your
    reasonable estimate does not obey these constraints, then you should revise it!
  prefs: []
  type: TYPE_NORMAL
- en: 'What should a probability distribution be like? Assuming that we have a finite
    set, *{O1, ... On}*, of distinct possible outcomes, any probability distribution
    should satisfy the following constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p(Oi) >= 0* for all outcomes *Oi*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(O1) + ... +p(On) = 1*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(Oi or Oj) = p(Oi)+p(Oj)* for *i ≠ j*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two constraints taken together mean that *p(Oi) <= 1* for all *Oi*,
    and the second and third mean that *p(not(Oi)) = 1-p(Oi)* (since *not(Oi)* is
    *O1* or *O2* or ... or *Oi-1* or *Oi+1* or .. or *On*).
  prefs: []
  type: TYPE_NORMAL
- en: These constraints say nothing about the likelihood of *Oi* and *Oj* both occurring.
    Given the initial conditions, this is not possible since *O1*, ... On were specified
    as distinct possible outcomes. The most that we can say about multiple outcomes
    is that if we have two *completely distinct and unconnected* sets of events, each
    with a set of possible outcomes, *O1*, ... *On* and *Q1*, ..., *Qm*, then the
    probability of *Oi* and *Qj* occurring must be *p(Oi) X p(Qj)*. In the same way
    that we could not tell whether the event we are concerned with is like all the
    others in the set in all relevant ways, we cannot tell whether two sets of events
    are indeed unconnected, so, again, this is a constraint on how a probability measure
    should behave rather than a definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given all this, we can define the conditional probability of some event, *A*,
    given that we know that some other event, *B*, has occurred (or indeed that we
    know that *B* will occur):'
  prefs: []
  type: TYPE_NORMAL
- en: '*p(A | B) = p(A &* *B)/p(B)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How likely is *A* given that we know *B*? Well, it’s how likely that they occur
    together divided by how likely *B* is by itself (so, if they occur together 5%
    of the time and B occurs 95% of the time, then seeing B will not make us much
    more likely to expect *A*, since *A* only occurs 1 in 19 times that *B* does;
    however, if they occur together 5% of the time but *B* itself only occurs 6%,
    then seeing B will be a strong clue that *A* will happen since *A* occurs 5 in
    6 times that *B* does).
  prefs: []
  type: TYPE_NORMAL
- en: 'This definition leads very straightforwardly to **Bayes’ theorem**:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p(A | B) = p(A & B)/p(B)* definition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(B | A) = p(B & A)/p(A)* definition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(A & B)= p(B &A)* constraint on *A* and *B*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(B & A) = p(B | A)×p(A)* rearrange (*2*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(A | B) = p(B | A)×p(A)/p(B)* substitute (*4*) into (*1*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we have a set of events, *B1*, ... *Bn*, then we can use Bayes’ theorem to
    say that *p(A | B1 & ... Bn) = p(B1 & ...Bn | A)×p(A)/p(B1 & ...& Bn)*. And *if
    the B**i* *are completely unconnected*, we can say that *p(A | B1 & ... Bn) =
    p(B1 | A) ×p(Bn |* *A)×p(A)/(p(B1) ×p(Bn))*.
  prefs: []
  type: TYPE_NORMAL
- en: This can be very convenient. Suppose that *A* is “this tweet is labeled as angry”
    and *B1*, ..., *Bn* are “this tweet contains the word *furious*,” “this tweet
    contains the word *cross*,” ..., “this tweet contains the word *irritated*.” We
    may have never seen a tweet that contains these three words before, so we cannot
    estimate the likelihood of *A* by counting. However, we will have seen tweets
    that contain these words individually, and we can count how many tweets that have
    been labeled as **angry** contain *furious* (or *cross* or *irritated*), how many
    in total have been labeled as **angry**, ignoring what words they contain, and
    how many contain **furious** (or *cross* or *irritated*), ignoring how they are
    labeled. So, we can make sensible estimates of these, and we can then use Bayes’
    theorem to estimate *p(A | B1 & ..* *Bn)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This way of applying Bayes’ theorem assumes that the events, *B1*, ... *Bn*,
    are completely unconnected. This is rarely true: a tweet that contains the word
    *cross* is much more likely to also contain *irritated* than one that doesn’t.
    So, while we can indeed *naively* misuse Bayes’ theorem in this way to get usable
    estimates of some outcome given a set of observations, we should never lose sight
    of the fact that these estimates are intrinsically unreliable. In the next section,
    we’ll look at how to implement this kind of naïve application of Bayes’ theorem
    as a classifier and investigate how well it works with our various datasets. The
    key to the success of this approach is that while the estimates of the likelihood
    of some outcome are not reliable, the ranking of different outcomes is often sensible
    – if the estimates of the probability that some tweet is **angry** or **sad**
    are 0.6 and 0.3, respectively, then it is indeed more likely to be **angry** than
    **sad**, even if the actual numbers cannot be relied on.'
  prefs: []
  type: TYPE_NORMAL
- en: Naively applying Bayes’ theorem as a classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`sklearn.naive_bayes.MultinomialNB` does these sums for us (they are not very
    difficult sums, but it is handy to have a package that does them very fast). Given
    this, the class of `NBCLASSIFIER` is very simple to define:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s all we need to make a Naïve Bayes classifier: make `SKLEARNCLASSIFIER`
    using `sklearn.naive_bayes.MultinomialNB.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'How well does this work? We will try this out on our datasets, using stemming
    for the non-English datasets but not for the English ones (we will do this from
    here on since those seemed to be generally the right choices in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)
    *, Sentiment Lexicons and Vector* *Space Models*):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.873 | 0.873 | 0.873 | 0.873 | 0.775 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.625 | 0.262 | 0.369 | 0.373 | 0.227 |'
  prefs: []
  type: TYPE_TB
- en: '| WASSA-EN | 0.830 | 0.830 | 0.830 | 0.830 | 0.709 |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.874 | 0.874 | 0.874 | 0.874 | 0.776 |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.849 | 0.849 | 0.849 | 0.849 | 0.738 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-AR | 0.694 | 0.694 | 0.694 | 0.694 | 0.531 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.628 | 0.274 | 0.381 | 0.393 | 0.236 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.667 | 0.655 | 0.661 | 0.664 | 0.494 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-ES | 0.525 | 0.535 | 0.530 | 0.462 | 0.360 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.508 | 0.296 | 0.374 | 0.380 | 0.230 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.2 – Naïve Bayes, one emotion per tweet
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to note is that both making and applying a Naïve Bayes classifier
    is very quick – 10K tweets can be classified per second, and even training on
    a dataset containing 400K tweets takes just under 10 seconds. But, as before,
    what matters is whether the classifier is any good at the task we want it to carry
    out. The preceding table shows that for most of the English datasets, the scores
    are better than the scores in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector Space Models* with the improvement for the CARER
    dataset being particularly marked and the score for SEM11-EN being substantially
    worse than in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons
    and Vector* *Space Models*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the main differences between CARER and the others: CARER is much bigger
    than the others, and, in contrast to SEM11, every tweet has exactly one label
    associated with it. To see whether the issue is the size of the training set,
    we will plot the accuracy for this dataset against an increasing training size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18714_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Jaccard score against training size, Naïve Bayes, with the CARER
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jaccard score increases steadily from quite a low base, and while it is
    beginning to flatten out as we get to around 400K training tweets, it is clear
    that Naïve Bayes does require quite a lot of data. This is likely to be at least
    part of the reason why it is less effective for the other datasets: they simply
    do not contain enough data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth looking in some detail at the inner workings of this algorithm.
    Just like the lexicon-based classifiers, Naïve Bayes constructs a lexicon where
    each word has associated scores for the various emotions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| a | 0.0187 | 0.0194 | 0.0203 | 0.0201 | 0.0190 | 0.0172 |'
  prefs: []
  type: TYPE_TB
- en: '| and | 0.0291 | 0.0284 | 0.0311 | 0.0286 | 0.0308 | 0.0247 |'
  prefs: []
  type: TYPE_TB
- en: '| the | 0.0241 | 0.0238 | 0.0284 | 0.0275 | 0.0245 | 0.0230 |'
  prefs: []
  type: TYPE_TB
- en: '| angry | 0.0020 | 0.0003 | 0.0001 | 0.0001 | 0.0003 | 0.0001 |'
  prefs: []
  type: TYPE_TB
- en: '| happy | 0.0005 | 0.0003 | 0.0014 | 0.0004 | 0.0005 | 0.0004 |'
  prefs: []
  type: TYPE_TB
- en: '| hate | 0.0007 | 0.0005 | 0.0002 | 0.0003 | 0.0007 | 0.0002 |'
  prefs: []
  type: TYPE_TB
- en: '| irritated | 0.0013 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |'
  prefs: []
  type: TYPE_TB
- en: '| joy | 0.0001 | 0.0001 | 0.0002 | 0.0002 | 0.0001 | 0.0001 |'
  prefs: []
  type: TYPE_TB
- en: '| love | 0.0009 | 0.0009 | 0.0019 | 0.0030 | 0.0011 | 0.0011 |'
  prefs: []
  type: TYPE_TB
- en: '| sad | 0.0005 | 0.0003 | 0.0002 | 0.0002 | 0.0010 | 0.0003 |'
  prefs: []
  type: TYPE_TB
- en: '| scared | 0.0001 | 0.0019 | 0.0001 | 0.0001 | 0.0002 | 0.0001 |'
  prefs: []
  type: TYPE_TB
- en: '| terrified | 0.0000 | 0.0014 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.4 – Scores for individual words, Naïve Bayes, with the CARER dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the lexicon-based models, the scores for *a*, *and*, and *the* are
    quite high, reflecting the fact that these words occur in most tweets, and hence
    the conditional probability that they will occur in tweets that express the various
    emotions is also quite high. These words will get largely canceled out when we
    divide the contributions that they make by their overall frequencies. The others
    all have very small scores, but by and large, they do match the expected emotions
    – *angry* and *irritated* are most strongly linked to **anger**, *joy* is (just
    about) most strongly linked to **joy**, and so on. The differences in the levels
    of association to different emotions are much less marked than was the case for
    the simple lexicon-based algorithms, so the improved performance must be caused
    by the improvement in the way that Bayes’ theorem combines scores. It is clear
    that these words are not independently distributed: the proportion of tweets in
    the CARER dataset that contain *angry* and *irritated* and both *angry* and *irritated*
    are 0.008, 0.003, and 0.0001, respectively. If we take these as estimates of the
    respective probabilities, we will find that p(*angry* + *irritated*)/p(*angry*)
    X p(*irritated*) = 3.6, where it should be 1 if these words were distributed independently.
    This is hardly surprising – you are much more likely to use two words that express
    the same emotion in a single tweet than you are to use ones that express different
    emotions or that have nothing to do with each other. Nonetheless, Bayes’ theorem
    is robust enough to give us useful results even when the conditions for applying
    it soundly do not apply, so long as we have enough data.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-label datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The key difference between SEM11 and the other datasets is that tweets in the
    SEM11 sets can be assigned any number of emotions – they are multi-label datasets,
    as defined in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons
    and Vector-Space Models*. The actual distributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8**
    | **9** | **10** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 205 | 997 | 2827 | 2151 | 662 | 100 | 11 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 17 | 544 | 1005 | 769 | 210 | 33 | 3 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 179 | 1499 | 1605 | 479 | 52 | 1 | 1 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.5 – Number of tweets with 0, 1, 2, ... emotion labels for each SEM11
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: In each case, most tweets have two or more labels. This makes it all but impossible
    for any algorithm that assigns exactly one label to each tweet to score highly
    – there has to be a false positive for every tweet that has zero labels, and there
    have to be K-1 false negatives for every tweet that has K labels (since, at most,
    one of these, K, has been picked, and hence K-1 was not). Suppose we have *N*
    tweets, where *Z* has no labels, *O* has exactly one label, and *M* has more than
    one label. So, even if we assume that our classifier gets one of the labels right
    whenever a tweet has at least one label, the best Jaccard score that can be obtained
    is *(O+M)/(O+2*M+Z)* – there will be *O+M* true positives (all the cases that
    ought to be assigned one label, plus all the cases that ought to have more than
    one, by the assumption), at least *Z* false positives (one for each tweet that
    should have no labels), and at least *M* false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the best Jaccard score that can be obtained by an algorithm that assigns
    exactly one label per tweet for the SEM11-EN dataset is 0.41 (if every label that
    was assigned to any of the tweets that have one or more labels in their Gold Standards
    set was correct, then we would have 6,748 true positives, 205 false positives,
    and 9,570 false negatives). If that is the maximum possible Jaccard score for
    an algorithm, then the scores of around 0.2 that we obtained previously are not
    too bad.
  prefs: []
  type: TYPE_NORMAL
- en: But they are not as good as the scores we got for these datasets in [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and Vector Space Models*.
    We need to somehow make Naïve Bayes return multiple labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'This turns out to be fairly straightforward. We can use Bayes’ theorem to provide
    an estimate of the probability of each possible outcome. `sklearn.naive_bayes.MultinomialNB`
    usually picks the outcome with the highest probability, but it has a method, `predict_log_proba`,
    that returns the log of the probabilities for each possible outcome (it is often
    convenient to use the log of the probabilities since working with logs allows
    us to replace multiplications with additions, which are significantly faster).
    We can use this to pick, for instance, every outcome whose probability exceeds
    some threshold, or to pick the best two rather than just the best one. We will
    look at these two options in turn. For the first, we will use the same constructor
    as for `NBCLASSIFIER`, and we will just change `applyToTweet` so that it uses
    `predict_log_proba` rather than `predict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table is just a copy of the earlier table for Naïve Bayes that
    deals with the multi-label cases for ease of comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.625 | 0.262 | 0.369 | 0.373 | 0.227 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.628 | 0.274 | 0.381 | 0.393 | 0.236 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.667 | 0.655 | 0.661 | 0.664 | 0.494 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.508 | 0.296 | 0.374 | 0.380 | 0.230 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.6 – Naïve Bayes, one emotion per tweet, multi-class cases
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows what happens when we allow the classifier to assign
    more than one emotion to a tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.515 | 0.356 | 0.421 | 0.424 | 0.267 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.494 | 0.381 | 0.430 | 0.444 | 0.274 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.645 | 0.704 | 0.673 | 0.677 | 0.507 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.419 | 0.394 | 0.406 | 0.415 | 0.255 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.7 – Naïve Bayes, multiple outcomes with an optimal threshold, SEM11
    datasets
  prefs: []
  type: TYPE_NORMAL
- en: In each case, we have improved the recall considerably (because we are now allowing
    more than one label per tweet to be picked), at the cost of worsening precision.
    The Jaccard scores have increased slightly, but not to the point where they are
    better than the scores obtained in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*,
    Sentiment Lexicons and Vector* *Space Models*.
  prefs: []
  type: TYPE_NORMAL
- en: We can also simply demand to have two labels per tweet. Again, this will improve
    the recall since we have two labels for all the cases that should have two labels,
    two for all the cases that should have three, and two for all the cases that should
    have four – that is, we will potentially decrease the number of false
  prefs: []
  type: TYPE_NORMAL
- en: 'negatives. We will also inevitably increase the number of false positives since
    we will have two where we should have either none or one. This is an extremely
    simplistic algorithm since it pays no attention to when we should allow two labels
    – we just assume that this is the right thing to do in every case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This produces a further slight improvement for the SEM11 cases, but still not
    enough to improve over the [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment
    Lexicons and Vector Space Models* results, and is disastrous for KWT.M-AR, where
    there are a small number of cases with multiple assignments and a large number
    with no assignments at all – forcing the classifier to choose two assignments
    when there should be none will have a major effect on the precision!
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.477 | 0.404 | 0.437 | 0.429 | 0.280 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.474 | 0.413 | 0.441 | 0.440 | 0.283 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.461 | 0.906 | 0.611 | 0.612 | 0.440 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.370 | 0.431 | 0.398 | 0.395 | 0.249 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.8 – Naïve Bayes, best two outcomes, multi-label datasets
  prefs: []
  type: TYPE_NORMAL
- en: So, we have two very simple ways of turning Naïve Bayes into a classifier with
    multiple (or zero) outcomes. In both cases, the improvement over the standard
    version is minor but useful. And in both cases, it requires us to know something
    about the training set – the first requires us to choose a threshold to compare
    the individual scores with, and the second requires us to know the distribution
    of the number of outcomes per tweet. This means that, in both cases, we have to
    use the training data for two things – to find the conditional probabilities,
    as in the standard case, and then to pick the best possible threshold or to look
    at the distribution of the number of outcomes; for this, we have to split the
    training data into two parts, a training section to find the basic probabilities
    and then a **development** section to find the extra information. This is common
    in situations where you have to tune a basic model. No rule says that you *must*
    keep the training and development sections distinct like you must keep the training
    and test sets distinct, but pragmatically, it turns out that doing so usually
    produces better results than using the training set as the development set.
  prefs: []
  type: TYPE_NORMAL
- en: The scores for the multi-label datasets are still, however, worse than in [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and Vector Space Models*.
    We can try combinations of these two strategies, such as by demanding the best
    two outcomes so long as they both satisfy some
  prefs: []
  type: TYPE_NORMAL
- en: threshold, but no amount of fiddling around is going to transform Naïve Bayes
    into a good classifier for multi-label problems. We will return to this issue
    in [*Chapter* *10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to try to work out why Naïve Bayes produces a considerable improvement
    over the lexicon-based approaches for the SEM4, CARER, and IMDB datasets but a
    worse performance for WASSA. We have already seen that the performance of Naïve
    Bayes improves substantially for CARER as we increase the training data. The dataset
    sizes for these three datasets are SEM4-EN 6812, WASSA-EN 3564, and CARER-EN 411809\.
    What happens if we restrict the training data for all three cases to be the same
    as for WASSA? The following table is a copy of the relevant part of the original
    table, using the full dataset in each case:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.873 | 0.873 | 0.873 | 0.873 | 0.775 |'
  prefs: []
  type: TYPE_TB
- en: '| WASSA-EN | 0.830 | 0.830 | 0.830 | 0.830 | 0.709 |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.874 | 0.874 | 0.874 | 0.874 | 0.776 |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.849 | 0.849 | 0.849 | 0.849 | 0.738 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.9 – Naïve Bayes, English single-class datasets – full training sets
  prefs: []
  type: TYPE_NORMAL
- en: 'When we reduce the amount of data available to be the same as for WASSA, the
    results get worse, as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.837 | 0.837 | 0.837 | 0.837 | 0.719 |'
  prefs: []
  type: TYPE_TB
- en: '| WASSA-EN | 0.830 | 0.830 | 0.830 | 0.830 | 0.709 |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.732 | 0.732 | 0.732 | 0.732 | 0.577 |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.825 | 0.825 | 0.825 | 0.825 | 0.703 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.10 – Naïve Bayes, English single-class datasets – restricted training
    sets
  prefs: []
  type: TYPE_NORMAL
- en: 'The improvements that we’ve made over the results from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*,
    Sentiment Lexicons and Vector Space Models* for the SEM4-EN, CARER-EN, and IMDB-EN
    datasets are now less marked, particularly for CARER-EN: the loss of information
    when we restrict the size of the dataset is significant.'
  prefs: []
  type: TYPE_NORMAL
- en: Is there anything else that might explain the differences? Having more classes
    will make the problem more difficult. If you have, for instance, 10 classes, then
    making a random choice will get it right 10%
  prefs: []
  type: TYPE_NORMAL
- en: 'of the time whereas with 5 classes, a random choice will get it right 20% of
    the time. However, both SEM4-EN and WASSA-EN have the same set of labels, namely
    **anger**, **fear**, **joy**, and **sadness**, with CARER-EN having these four
    plus **love** and **surprise**, so if this were the key factor, we would expect
    the versions of SEM4-EN and WASSA to produce similar results and CARER to be a
    bit worse, which is not what we find. It is also likely that having a set where
    the distribution between classes is very uneven may make a difference. However,
    the distributions of the various emotions between SEM4-EN and WASSA-EN are fairly
    similar:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SEM4-EN: anger: 834, fear: 466, joy: 821, sadness: 1443'
  prefs: []
  type: TYPE_NORMAL
- en: 'WASSA-EN: anger: 857, fear: 1098, joy: 823, sadness: 786'
  prefs: []
  type: TYPE_NORMAL
- en: SEM4-EN has more tweets that express sadness and WASSA-EN has more that express
    fear, but the differences are not of a kind that would lead you to expect a difference
    in the performance of a classifier. The two also have almost identical vocabulary
    sizes (75723 versus 75795) and almost identical average numbers of tokens per
    tweet (both 21.2). Sometimes, it just seems that one classifier is well suited
    to one task, and a different classifier is better suited to another.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we saw that Naïve Bayes can work extremely well as a classifier
    for finding emotions in tweets. It works particularly well with large training
    sets (and takes very little time to train since it simply counts occurrences of
    words and the emotions associated with the tweets they appear in). It can be adapted
    fairly straightforwardly to work with datasets where a single tweet may have any
    number of labels (including zero) but is outperformed on the test sets with this
    property by the lexicon-based approaches from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*,
    Sentiment Lexicons and Vector Space Models*. *Figure 6**.11* shows the best classifiers
    so far for the various datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **CP** | **NB (single)** | **NB (multi)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.503 | 0.593 | 0.775 | *******0.778*** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.347 | *******0.353*** | 0.227 | 0.267 |'
  prefs: []
  type: TYPE_TB
- en: '| WASSA-EN | 0.445 | 0.505 | *******0.709*** | 0.707 |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.350 | 0.395 | *******0.776*** | 0.774 |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.722 | 0.722 | 0.738 | *******0.740*** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-AR | 0.506 | 0.513 | 0.531 | *******0.532*** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.378 | *******0.382*** | 0.236 | 0.274 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | *******0.687*** | 0.666 | 0.494 | 0.507 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-ES | *******0.425*** | 0.177 | 0.360 | 0.331 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.269 | *******0.278*** | 0.230 | 0.255 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.11 – Best classifiers so far
  prefs: []
  type: TYPE_NORMAL
- en: In general, Naïve Bayes is the best classifier for datasets where each tweet
    has only one label, with marginal differences in these datasets between the version
    of Naïve Bayes that assumes there is only one label per tweet and the version
    that allows for multiple labels. For the multi-label datasets, the version that
    allows for multiple labels always outperforms the one that doesn’t, but in all
    these cases, one of the lexicon-based classifiers from *wwwwwwwwwww, Sentiment
    Lexicons and Vector Space Models* is best. For now, the biggest lesson from this
    chapter is that when trying to solve a classification problem, you should try
    various approaches and take the one that works best. We will see what happens
    when we look at more sophisticated machine learning algorithms in the following
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Bayes, T. (1958). *An essay towards solving a problem in the doctrine of chances*.
    Biometrika, 45(3–4), 296–315\. [https://doi.org/10.1093/biomet/45.3-4.296](https://doi.org/10.1093/biomet/45.3-4.296).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
