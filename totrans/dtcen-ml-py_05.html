<html><head></head><body>
<div id="_idContainer058">
<h1 class="chapter-number" id="_idParaDest-71"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.2.1">Techniques for Data Cleaning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will cover six key dimensions of data quality and their corresponding techniques to improve data quality, commonly known as techniques for cleaning data in machine learning. </span><span class="koboSpan" id="kobo.3.2">Simply put, data cleaning is the process of implementing techniques to improve data quality by fixing errors in data or removing erroneous data. </span><span class="koboSpan" id="kobo.3.3">As covered in </span><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapters 1</span></em><span class="koboSpan" id="kobo.5.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.6.1">2</span></em><span class="koboSpan" id="kobo.7.1">, reducing errors in data is a highly efficient and effective way to improve model quality over using model-centric techniques such as adding more data and/or implementing </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">complex algorithms.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">At a high level, data cleaning techniques involve fixing or removing incorrect, incomplete, invalid, biased, inconsistent, stale, or corrupted data. </span><span class="koboSpan" id="kobo.9.2">As data is captured at multiple sources, due to different annotators following their judgment or due to poor system designs, combining these sources can often result in data being mislabeled, inconsistent, duplicated, or incomplete. </span><span class="koboSpan" id="kobo.9.3">As discovered in earlier chapters, incorrect data can make algorithms and outcomes unreliable. </span><span class="koboSpan" id="kobo.9.4">So, to achieve reliability in machine learning systems, it is important to help data scientists and data engineers question the data and systematically improve data quality using data </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">cleaning techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">The topics that will be covered in this chapter are </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">The six key dimensions of </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">data quality</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Measuring </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">data quality</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Data cleaning techniques required to improve data quality across the </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">six dimensions</span></span></li>
</ul>
<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.19.1">The six key dimensions of data quality</span></h1>
<p><span class="koboSpan" id="kobo.20.1">There are</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.21.1"> six key dimensions we can use to check the overall health of data. </span><span class="koboSpan" id="kobo.21.2">Ensuring good health across the data can ensure we can build reliable systems and make better decisions. </span><span class="koboSpan" id="kobo.21.3">For example, if 20% of survey data is duplicated, and the majority of the duplicates are filled by male candidates, we can imagine that the actions taken by decision-makers will favor the male candidates if data duplication is undetected. </span><span class="koboSpan" id="kobo.21.4">Hence, it’s important to understand the overall health of the data to make reliable and unbiased decisions. </span><span class="koboSpan" id="kobo.21.5">To measure data quality or look at the overall health of the data, we can break down data quality into the </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">following dimensions:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.23.1">Consistency</span></strong><span class="koboSpan" id="kobo.24.1">: This </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.25.1">refers to whether the same data is maintained across the rows for a given column or feature. </span><span class="koboSpan" id="kobo.25.2">An example of this could be whether the gender label for males is consistent or not. </span><span class="koboSpan" id="kobo.25.3">The label can take values of “1,” “Male,” “M”, or “male,” but if the data has multiple values to represent males, then an algorithm will treat each label individually, which can cause randomness and errors. </span><span class="koboSpan" id="kobo.25.4">Hence, the goal of ensuring consistency is to ensure labels are defined consistently across the </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">whole dataset.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.27.1">Uniqueness</span></strong><span class="koboSpan" id="kobo.28.1">: This</span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.29.1"> refers to whether each record can be uniquely identified. </span><span class="koboSpan" id="kobo.29.2">If duplicate values enter the system, the model will become biased due to those records. </span><span class="koboSpan" id="kobo.29.3">For example, if one region has a high loan approval rate and due to system failure, records from this region are duplicated, the algorithm will be biased toward approving more loans from that region. </span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.30.1">Completeness</span></strong><span class="koboSpan" id="kobo.31.1">: This</span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.32.1"> refers to whether data is complete across the rows for a given column or feature, and whether data is missing due to system errors or not captured, especially when that information will be used in the machine </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">learning system.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.34.1">Validity</span></strong><span class="koboSpan" id="kobo.35.1">: This </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.36.1">refers to whether the data labels conform to the rules. </span><span class="koboSpan" id="kobo.36.2">For example, if a label is present in the data but cannot be verified by any external source, this can be referred to as invalid data. </span><span class="koboSpan" id="kobo.36.3">In a housing loan dataset, the location of the property may have not conformed to the rules, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.37.1">semi_urban</span></strong><span class="koboSpan" id="kobo.38.1"> might be invalid if one or a couple of annotators believed some suburbs are neither urban nor rural, and they violated the rules of data and entered </span><strong class="source-inline"><span class="koboSpan" id="kobo.39.1">semi_urban</span></strong><span class="koboSpan" id="kobo.40.1">. </span><span class="koboSpan" id="kobo.40.2">This can introduce noise in the data, so it’s important to ensure data conforms to the </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">business rules.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.42.1">Accuracy</span></strong><span class="koboSpan" id="kobo.43.1">: This</span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.44.1"> refers to whether data was entered correctly in the first place and can be verified from an internal or external source. </span><span class="koboSpan" id="kobo.44.2">An example of this in a healthcare setting could be that if the date and time of admission are entered in a different time zone, then the analysis and insights would be inaccurate. </span><span class="koboSpan" id="kobo.44.3">If the time of admission is a predictor of quality of care, then misaligned time zones might lead to </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">wrong conclusions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.46.1">Freshness</span></strong><span class="koboSpan" id="kobo.47.1">: This </span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.48.1">refers to whether the data available is recent and up to date to meet data requirements. </span><span class="koboSpan" id="kobo.48.2">Some applications require data to be real-time – that is, updated every second – whereas other applications require data to be available once a month or every few months. </span><span class="koboSpan" id="kobo.48.3">What was true yesterday may change due to changes in factors such as regulation, weather conditions, trends, competition, business changes, </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">and more.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.50.1">Next, we will install various Python packages, and then dive into fixing and measuring data quality. </span><span class="koboSpan" id="kobo.50.2">In each section, we dive deeper into different data cleaning techniques and how to improve the quality </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">of</span><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.52.1"> data.</span></span></p>
<h1 id="_idParaDest-74"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.53.1">Installing the required packages</span></h1>
<p><span class="koboSpan" id="kobo.54.1">For this chapter, we </span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.55.1">will need the following Python packages </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">or libraries:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.57.1">pandas</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.58.1">version 1.5.3</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.59.1">numpy</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.60.1">version 1.22.4</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.61.1">scikit-learn</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.62.1">version 1.2.1</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.63.1">jupyter</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.64.1">version 1.0.0</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.65.1">alibi</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.66.1">version 0.9.0</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.67.1">alibi-detect</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.68.1">version 0.10.4</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.69.1">seaborn</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.70.1">version 0.12.2</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.71.1">matplotlib</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.72.1">version 3.6.3</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.73.1">missingno</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.74.1">version 0.5.1</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.75.1">feature-engine</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.76.1">version 1.5.2</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.77.1">Next, we will </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.78.1">provide a brief introduction to the dataset and start exploring </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">t</span><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.80.1">he data.</span></span></p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.81.1">Introducing the dataset</span></h1>
<p><span class="koboSpan" id="kobo.82.1">First, let’s</span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.83.1"> introduce our problem statement. </span><span class="koboSpan" id="kobo.83.2">For loan providers, it is important to ensure that people who get a loan can make payment and don’t default. </span><span class="koboSpan" id="kobo.83.3">However, it is equally important that people are not denied a loan due to a model trained on poor-quality data. </span><span class="koboSpan" id="kobo.83.4">This is where the data-centric approach helps make the world a better place – it provides a framework for data scientists and data engineers to question the quality </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">of data.</span></span></p>
<p><span class="koboSpan" id="kobo.85.1">For this chapter, we will use the loan prediction dataset from Analytics Vidhya. </span><span class="koboSpan" id="kobo.85.2">You can download the dataset from </span><a href="https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/#ProblemStatement"><span class="koboSpan" id="kobo.86.1">https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/#ProblemStatement</span></a><span class="koboSpan" id="kobo.87.1">. </span><span class="koboSpan" id="kobo.87.2">There are two files – one for training and one for testing. </span><span class="koboSpan" id="kobo.87.3">The test file doesn’t contain any labels. </span><span class="koboSpan" id="kobo.87.4">For this chapter, we will utilize the training file, which has been downloaded and saved </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">as </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.89.1">train_loan_prediction.csv</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.91.1">First, we will look at the dataset and check the first five rows. </span><span class="koboSpan" id="kobo.91.2">To do this, we must import the following </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">necessary packages:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.93.1">
import pandas as pd
import numpy as np
import missingno as msno
import matplotlib.pyplot as plt</span></pre> <p><span class="koboSpan" id="kobo.94.1">Next, we will read </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">the data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.96.1">
df = pd.read_csv('train_loan_prediction.csv')
df.head().T</span></pre> <p><span class="koboSpan" id="kobo.97.1">We will read</span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.98.1"> the data using pandas’ </span><strong class="source-inline"><span class="koboSpan" id="kobo.99.1">read_csv</span></strong><span class="koboSpan" id="kobo.100.1"> method. </span><span class="koboSpan" id="kobo.100.2">Then, we will visualize the first five rows of the dataset using </span><strong class="source-inline"><span class="koboSpan" id="kobo.101.1">.head()</span></strong><span class="koboSpan" id="kobo.102.1">. </span><span class="koboSpan" id="kobo.102.2">We can apply the </span><strong class="source-inline"><span class="koboSpan" id="kobo.103.1">.T</span></strong><span class="koboSpan" id="kobo.104.1"> method at the end of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.105.1">.head()</span></strong><span class="koboSpan" id="kobo.106.1"> method if we have a large feature set. </span><span class="koboSpan" id="kobo.106.2">This will represent columns as rows and rows as columns, where column names will not exceed 5 as we want to visualize the first </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">five rows.</span></span></p>
<p><span class="koboSpan" id="kobo.108.1">We get the </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<span class="koboSpan" id="kobo.110.1"><img alt="Figure 5.1 – The first five rows of our df dataset" src="image/B19297_05_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.111.1">Figure 5.1 – The first five rows of our df dataset</span></p>
<p><span class="koboSpan" id="kobo.112.1">As we can see, there are some inconsistencies across column names. </span><span class="koboSpan" id="kobo.112.2">All the columns follow the camel case convention, except </span><strong class="source-inline"><span class="koboSpan" id="kobo.113.1">Loan_ID</span></strong><span class="koboSpan" id="kobo.114.1">, and long column names are separated by </span><strong class="source-inline"><span class="koboSpan" id="kobo.115.1">_</span></strong><span class="koboSpan" id="kobo.116.1"> except for </span><strong class="source-inline"><span class="koboSpan" id="kobo.117.1">LoanAmount</span></strong><span class="koboSpan" id="kobo.118.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.119.1">CoapplicantIncome</span></strong><span class="koboSpan" id="kobo.120.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.121.1">ApplicantIncome</span></strong><span class="koboSpan" id="kobo.122.1">. </span><span class="koboSpan" id="kobo.122.2">This indicates that the column names have inconsistent naming conventions. </span><span class="koboSpan" id="kobo.122.3">Within the data, we can also see that some columns have data in camel case but </span><strong class="source-inline"><span class="koboSpan" id="kobo.123.1">Loan_ID</span></strong><span class="koboSpan" id="kobo.124.1"> has all its values in uppercase. </span><span class="koboSpan" id="kobo.124.2">Within the </span><strong class="source-inline"><span class="koboSpan" id="kobo.125.1">Education</span></strong><span class="koboSpan" id="kobo.126.1"> column, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.127.1">Not Graduate</span></strong><span class="koboSpan" id="kobo.128.1"> value is separated by a space. </span><span class="koboSpan" id="kobo.128.2">In machine learning, it’s important to ensure data is consistent; otherwise, models may produce inconsistent results. </span><span class="koboSpan" id="kobo.128.3">For </span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.129.1">instance, what happens if the </span><strong class="source-inline"><span class="koboSpan" id="kobo.130.1">Gender</span></strong><span class="koboSpan" id="kobo.131.1"> column has two distinct values for male customers – </span><strong class="source-inline"><span class="koboSpan" id="kobo.132.1">Male</span></strong><span class="koboSpan" id="kobo.133.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.134.1">male</span></strong><span class="koboSpan" id="kobo.135.1">? </span><span class="koboSpan" id="kobo.135.2">If we don’t treat this, then our machine learning model will consider </span><strong class="source-inline"><span class="koboSpan" id="kobo.136.1">male</span></strong><span class="koboSpan" id="kobo.137.1"> data points separate from </span><strong class="source-inline"><span class="koboSpan" id="kobo.138.1">Male</span></strong><span class="koboSpan" id="kobo.139.1">, and the model will not get an </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">accurate signal.</span></span></p>
<p><span class="koboSpan" id="kobo.141.1">Next, we will extract the list of column names from the data, make them all lowercase, and ensure words will be separated by a unique character, </span><strong class="source-inline"><span class="koboSpan" id="kobo.142.1">_</span></strong><span class="koboSpan" id="kobo.143.1">. </span><span class="koboSpan" id="kobo.143.2">We will also go through the data values of categorical columns and make them all lowercase before replacing all the special characters with </span><strong class="source-inline"><span class="koboSpan" id="kobo.144.1">_</span></strong><span class="koboSpan" id="kobo.145.1"> and making our </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">data c</span><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.147.1">onsistent.</span></span></p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.148.1">Ensuring the data is consistent</span></h1>
<p><span class="koboSpan" id="kobo.149.1">To </span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.150.1">ensure the data is consistent, we must check the names of the columns in </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">the DataFrame:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.152.1">
column_names = [cols for cols in df]
print(column_names)
['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']</span></pre> <p><span class="koboSpan" id="kobo.153.1">Next, we must get all the column names that don’t </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">contain underscores:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.155.1">
num_underscore_present_columns = [cols for cols in column_names if '_' not in cols]
num_underscore_present_columns
['Gender',
 'Married',
 'Dependents',
 'Education',
 'ApplicantIncome',
 'CoapplicantIncome',
 'LoanAmount']</span></pre> <p><span class="koboSpan" id="kobo.156.1">Since some </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.157.1">columns have two uppercase letters in their names, we must add the underscore before the start of the second uppercase letter. </span><span class="koboSpan" id="kobo.157.2">Next, we create a Boolean mapping against the index of each letter of the column, where the location of capital letters will be mapped as </span><strong class="source-inline"><span class="koboSpan" id="kobo.158.1">True</span></strong><span class="koboSpan" id="kobo.159.1"> so that we can locate the index of the second capital letter and prefix it with </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">an underscore:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.161.1">
cols_mappings = {}
for cols in num_underscore_present_columns:
    uppercase_in_cols = [val.isupper() for val in cols]
    num_uppercase_letters = sum(uppercase_in_cols)
    cols_mappings[cols] = {
        "is_uppercase_letter": uppercase_in_cols,
        "num_uppercase_letters": num_uppercase_letters,
        "needs_underscore": (num_uppercase_letters &gt; 1)
    }</span></pre> <p><span class="koboSpan" id="kobo.162.1">Then, we iterate over the mappings and print the column names that will require an underscore, and also print the location of the second </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">capital letter:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.164.1">
for key in cols_mappings.keys():
    if cols_mappings[key]['needs_underscore']:
        print()
        print(f'{key} need the underscore at location ', cols_mappings[key]['is_uppercase_letter'].index(True, 1))
ApplicantIncome need the underscore at location  9
CoapplicantIncome need the underscore at location  11
LoanAmount need the underscore at location  4</span></pre> <p><span class="koboSpan" id="kobo.165.1">Using this information, we build some logic for the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">ApplicantIncome</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.167.1"> column:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.168.1">
'ApplicantIncome'[:9] + '_' + 'ApplicantIncome'[9:]
'Applicant_Income'</span></pre> <p><span class="koboSpan" id="kobo.169.1">Next, we</span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.170.1"> combine the previous steps and iterate over columns that require an underscore and build a mapping. </span><span class="koboSpan" id="kobo.170.2">Then, we print the names of the columns that require underscores. </span><span class="koboSpan" id="kobo.170.3">Finally, we create a mapping of old column names and new </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">column names:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.172.1">
cols_mappings = {}
for cols in num_underscore_present_columns:
    uppercase_in_cols = [val.isupper() for val in cols]
    num_uppercase_letters = sum(uppercase_in_cols)
    if num_uppercase_letters &gt; 1:
        underscore_index = uppercase_in_cols.index(True, 1)
        updated_column_name = cols[:underscore_index] + "_" + cols[underscore_index:]
    else:
        updated_column_name = cols
    cols_mappings[cols] = {
        "is_uppercase_letter": uppercase_in_cols,
        "num_uppercase_letters": num_uppercase_letters,
        "needs_underscore": (num_uppercase_letters &gt; 1),
        "updated_column_name": updated_column_name
    }
    if cols_mappings[cols]['needs_underscore']:
        print(f"{cols} will be renamed to {cols_mappings[cols]['updated_column_name']}")
column_mappings = {key: cols_mappings[key]["updated_column_name"] for key in cols_mappings.keys()}
column_mappings
ApplicantIncome will be renamed to Applicant_Income
CoapplicantIncome will be renamed to Coapplicant_Income
LoanAmount will be renamed to Loan_Amount
{'Gender': 'Gender',
 'Married': 'Married',
 'Dependents': 'Dependents',
 'Education': 'Education',
 'ApplicantIncome': 'Applicant_Income',
 'CoapplicantIncome': 'Coapplicant_Income',
 'LoanAmount': 'Loan_Amount'}</span></pre> <p><span class="koboSpan" id="kobo.173.1">Finally, we </span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.174.1">apply the column mappings to update the column names and print the new </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">column names:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.176.1">
df = df.rename(columns=column_mappings)
column_names = [cols for cols in df]
print(column_names)
['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Applicant_Income', 'Coapplicant_Income', 'Loan_Amount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']</span></pre> <p><span class="koboSpan" id="kobo.177.1">Although the</span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.178.1"> preceding code could have simply been updated by manually choosing the columns, by doing this programmatically, we can ensure that data is following programmatic rules. </span><span class="koboSpan" id="kobo.178.2">To improve consistency, we make all column names lowercase. </span><span class="koboSpan" id="kobo.178.3">First, we create some simple one-line logic to convert column names </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">into lowercase:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.180.1">
print([cols.lower() for cols in df])
['loan_id', 'gender', 'married', 'dependents', 'education', 'self_employed', 'applicant_income', 'coapplicant_income', 'loan_amount', 'loan_amount_term', 'credit_history', 'property_area', 'loan_status']</span></pre> <p><span class="koboSpan" id="kobo.181.1">Then, we update the column names by passing the </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">preceding logic:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.183.1">
df.columns = [cols.lower() for cols in df]
print(df.columns)
Index(['loan_id', 'gender', 'married', 'dependents', 'education',
       'self_employed', 'applicant_income', 'coapplicant_income',
       'loan_amount', 'loan_amount_term', 'credit_history', 'property_area',
       'loan_status'],
      dtype='object')</span></pre> <p><span class="koboSpan" id="kobo.184.1">With that, we have ensured that the column names are consistent. </span><span class="koboSpan" id="kobo.184.2">But more importantly, we must ensure that categorical values are consistent so that we can future-proof machine learning systems from inconsistent data. </span><span class="koboSpan" id="kobo.184.3">First, we extract the </span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">id</span></strong><span class="koboSpan" id="kobo.186.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">target</span></strong><span class="koboSpan" id="kobo.188.1"> columns and then identify the categorical columns. </span><span class="koboSpan" id="kobo.188.2">These columns contain </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">non-numerical data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.190.1">
id_col = 'loan_id'
target = 'loan_status'
cat_cols = [cols for cols in df if df[cols].dtype == 'object' and cols not in [id_col, target]]
cat_cols
['gender',
 'married',
 'dependents',
 'education',
 'self_employed',
 'property_area']</span></pre> <p><span class="koboSpan" id="kobo.191.1">We</span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.192.1"> iterate over each column and check the unique values to ensure the values are distinct and not misspelled. </span><span class="koboSpan" id="kobo.192.2">We also check that the same value is not represented differently, such as it being in a different case or </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">being abbreviated:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.194.1">
for cols in cat_cols:
    print(cols)
    print(df[cols].unique())
    print()
gender
['Male' 'Female' nan]
married
['No' 'Yes' nan]
dependents
['0' '1' '2' '3+' nan]
education
['Graduate' 'Not Graduate']
self_employed
['No' 'Yes' nan]
property_area
['Urban' 'Rural' 'Semiurban']</span></pre> <p><span class="koboSpan" id="kobo.195.1">Looking at </span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.196.1">the data, it seems that values are distinct and not abbreviated or misspelled. </span><span class="koboSpan" id="kobo.196.2">But machine learning systems can be made future-proof. </span><span class="koboSpan" id="kobo.196.3">For instance, if we make all values lowercase, then in the future, if the same value comes with a different case, before entering the system, it will be made lowercase. </span><span class="koboSpan" id="kobo.196.4">We can also see that some strings, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">Not Graduate</span></strong><span class="koboSpan" id="kobo.198.1">, take up space. </span><span class="koboSpan" id="kobo.198.2">Just like how we ensured consistency for column names, we must replace all white spaces with underscores. </span><span class="koboSpan" id="kobo.198.3">First, we create a new DataFrame named </span><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">df_consistent</span></strong><span class="koboSpan" id="kobo.200.1">; then, we make all categorical values lowercase and replace all spaces </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">with underscores:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.202.1">
df_consistent = df.copy()
for col in cat_cols:
    df_consistent[col] = df_consistent[col].apply(lambda val: val.lower() if isinstance(val, str) else val)
    df_consistent[col] = df_consistent[col].apply(lambda val: val.replace(' ','_') if isinstance(val, str) else val)
for cols in cat_cols:
    print(cols)
    print(df_consistent[cols].unique())
    print()
gender
['male' 'female' nan]
married
['no' 'yes' nan]
dependents
['0' '1' '2' '3+' nan]
education
['graduate' 'not_graduate']
self_employed
['no' 'yes' nan]
property_area
['urban' 'rural' 'semiurban']</span></pre> <p><span class="koboSpan" id="kobo.203.1">With that, we</span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.204.1"> have ensured that data is consistent and all values are converted </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">into lowercase.</span></span></p>
<p><span class="koboSpan" id="kobo.206.1">As we can see, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.207.1">dependents</span></strong><span class="koboSpan" id="kobo.208.1"> column contains numerical information. </span><span class="koboSpan" id="kobo.208.2">However, due to the presence of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.209.1">3+</span></strong><span class="koboSpan" id="kobo.210.1"> value, the column values are encoded as strings. </span><span class="koboSpan" id="kobo.210.2">We must remove the special character and then encode this back to a numerical value since this column </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">is ordinal:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.212.1">
df_consistent.dependents = df_consistent.dependents.apply(lambda val: float(val.replace('+','')) if isinstance(val, str) else float(val))</span></pre> <p><span class="koboSpan" id="kobo.213.1">Next, we </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.214.1">look at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.215.1">married</span></strong><span class="koboSpan" id="kobo.216.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.217.1">self_employed</span></strong><span class="koboSpan" id="kobo.218.1"> columns since these are binary and must be encoded to </span><strong class="source-inline"><span class="koboSpan" id="kobo.219.1">1</span></strong><span class="koboSpan" id="kobo.220.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.221.1">0</span></strong><span class="koboSpan" id="kobo.222.1">. </span><span class="koboSpan" id="kobo.222.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.223.1">gender</span></strong><span class="koboSpan" id="kobo.224.1"> column has two values and can be binary encoded as well – for example, we can encode </span><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">male</span></strong><span class="koboSpan" id="kobo.226.1"> as </span><strong class="source-inline"><span class="koboSpan" id="kobo.227.1">1</span></strong><span class="koboSpan" id="kobo.228.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.229.1">female</span></strong><span class="koboSpan" id="kobo.230.1"> as </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">0</span></strong><span class="koboSpan" id="kobo.232.1">. </span><span class="koboSpan" id="kobo.232.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">education</span></strong><span class="koboSpan" id="kobo.234.1"> column also has two values, and we can encode </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">graduate</span></strong><span class="koboSpan" id="kobo.236.1"> as </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">1</span></strong><span class="koboSpan" id="kobo.238.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.239.1">not_graduate</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.240.1">as </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.241.1">0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.243.1">
for cols in ['married', 'self_employed']:
    df_consistent[cols] = df_consistent[cols].map({"yes": 1, "no": 0})
df_consistent.education = df_consistent.education.map({
    'graduate': 1,
    'not_graduate': 0
})
df_consistent.gender = df_consistent.gender.map({
    'male': 1,
    'female': 0
})
for cols in cat_cols:
    print(cols)
    print(df_consistent[cols].unique())
    print()
gender
[ 1.  0. </span><span class="koboSpan" id="kobo.243.2">nan]
married
[ 0.  1. </span><span class="koboSpan" id="kobo.243.3">nan]
dependents
[ 0.  1.  2.  3. </span><span class="koboSpan" id="kobo.243.4">nan]
education
[1 0]
self_employed
[ 0.  1. </span><span class="koboSpan" id="kobo.243.5">nan]
property_area
['urban' 'rural' 'semiurban']</span></pre> <p><span class="koboSpan" id="kobo.244.1">Now that the </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.245.1">data is consistent and has been encoded correctly, we must create a function for preprocessing data so that we can consistently process any future variations to categorical labels. </span><span class="koboSpan" id="kobo.245.2">Then, we apply the function to the DataFrame and print the values to ensure the function was </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">applied correctly:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.247.1">
def make_data_consistent(df, cat_cols) -&gt; pd.DataFrame:
    """Function to make data consistent and meaningful"""
    df = df.copy()
    for col in cat_cols:
        df[col] = df[col].apply(lambda val: val.lower() if isinstance(val, str) else val)
        df[col] = df[col].apply(lambda val: val.replace(' ','_') if isinstance(val, str) else val)
    df['dependents'] = df['dependents'].apply(lambda val: float(val.replace('+','')) if isinstance(val, str) else float(val))
    for cols in ['married', 'self_employed']:
        df[cols] = df[cols].map({"yes": 1, "no": 0})
    df['education'] = df['education'].map({
        'graduate': 1,
        'not_graduate': 0
    })
    df['gender'] = df['gender'].map({
        'male': 1,
        'female': 0
    })
    return df
df_consistent = df.copy()
df_consistent = make_data_consistent(df=df_consistent, cat_cols=cat_cols)
for cols in cat_cols:
    print(cols)
    print(df_consistent[cols].unique())
    print()
gender
[ 1.  0. </span><span class="koboSpan" id="kobo.247.2">nan]
married
[ 0.  1. </span><span class="koboSpan" id="kobo.247.3">nan]
dependents
[ 0.  1.  2.  3. </span><span class="koboSpan" id="kobo.247.4">nan]
education
[1 0]
self_employed
[ 0.  1. </span><span class="koboSpan" id="kobo.247.5">nan]
property_area
['urban' 'rural' 'semiurban']</span></pre> <p><span class="koboSpan" id="kobo.248.1">We have now ensured that the data is consistent so that if categorical values have spaces instead of </span><strong class="source-inline"><span class="koboSpan" id="kobo.249.1">_</span></strong><span class="koboSpan" id="kobo.250.1"> or are entered with a different case in the future, we can use the functionality we created </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.251.1">here to clean the data and make it consistent before it enters </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">our model.</span></span></p>
<p><span class="koboSpan" id="kobo.253.1">Next, we will explore data uniqueness to ensure no duplicate records have been provided to cre</span><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.254.1">ate bias in </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">the data.</span></span></p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.256.1">Checking that the data is unique</span></h1>
<p><span class="koboSpan" id="kobo.257.1">Now </span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.258.1">that we have ensured the data is consistent, we must also ensure it's unique, before it enters the machine </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">learning system.</span></span></p>
<p><span class="koboSpan" id="kobo.260.1">In this section, we will investigate the data and check whether the values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">loan_id</span></strong><span class="koboSpan" id="kobo.262.1"> column are unique, as well as whether a combination of certain columns can ensure data </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">is unique.</span></span></p>
<p><span class="koboSpan" id="kobo.264.1">In pandas, we can utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.265.1">.nunique()</span></strong><span class="koboSpan" id="kobo.266.1"> method to check the number of unique records for the column and compare it with the number of rows. </span><span class="koboSpan" id="kobo.266.2">First, we will check that </span><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">loan_id</span></strong><span class="koboSpan" id="kobo.268.1"> is unique and that no duplicate applications have </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">been entered:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.270.1">
df.loan_id.nunique(), df.shape[0]
(614, 614)</span></pre> <p><span class="koboSpan" id="kobo.271.1">With this, we have ensured that loan IDs are unique. </span><span class="koboSpan" id="kobo.271.2">However, we can go one step further to ensure that incorrect data is not added to another loan application. </span><span class="koboSpan" id="kobo.271.3">We believe it’s quite unlikely that a loan application will require more than one combination of income and loan amount. </span><span class="koboSpan" id="kobo.271.4">We must check that we can use a combination of column values to ensure uniqueness across </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">those columns:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.273.1">
df[['applicant_income', 'coapplicant_income', 'loan_amount']].value_counts().reset_index(name='count')
     applicant_income  coapplicant_income  loan_amount  count
0                4333              2451.0        110.0      2
1                 150              1800.0        135.0      1
2                4887                 0.0        133.0      1
3                4758                 0.0        158.0      1
4                4817               923.0        120.0      1
..                ...                 ...          ...    ...
</span><span class="koboSpan" id="kobo.273.2">586              3166              2985.0        132.0      1
587              3167                 0.0         74.0      1
588              3167              2283.0        154.0      1
589              3167              4000.0        180.0      1
590             81000                 0.0        360.0      1
[591 rows x 4 columns]</span></pre> <p><span class="koboSpan" id="kobo.274.1">As we can</span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.275.1"> see, in the first row, there are two applications with the same income variables and loan amount. </span><span class="koboSpan" id="kobo.275.2">Let’s filter the dataset to find these believed-to-be duplicate records by using values from the </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">first row:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.277.1">
df[(df.applicant_income == 4333) &amp; (df.coapplicant_income == 2451) &amp; (df.loan_amount == 110)]
      loan_id  gender married dependents education self_employed  \
328  LP002086  Female     Yes          0  Graduate            No
469  LP002505    Male     Yes          0  Graduate            No
     applicant_income  coapplicant_income  loan_amount  loan_amount_term  \
328           4333              2451.0        110.0             360.0
469          4333              2451.0        110.0             360.0
     credit_history property_area loan_status
328             1.0         Urban           N
469             1.0         Urban           N</span></pre> <p><span class="koboSpan" id="kobo.278.1">Looking at </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.279.1">this subset, it is quite obvious that the data contains duplicates or two different applications were made – one by the husband and another one by the wife. </span><span class="koboSpan" id="kobo.279.2">This data is not providing any more information, other than that one application has been made by a male candidate and another has been made by a female candidate. </span><span class="koboSpan" id="kobo.279.3">We could drop one of the data points, but there is a big imbalance in the ratio of male to female applications. </span><span class="koboSpan" id="kobo.279.4">Also, if the second one was a genuine application, then we should keep the </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">data point:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.281.1">
df.gender.value_counts(normalize=True)
Male      0.813644
Female    0.186356
Name: gender, dtype: float64</span></pre> <p><span class="koboSpan" id="kobo.282.1">Based on this, we have understood what makes a data point unique – that is, a combination of </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">gender</span></strong><span class="koboSpan" id="kobo.284.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">applicant_income</span></strong><span class="koboSpan" id="kobo.286.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">coapplicant_income</span></strong><span class="koboSpan" id="kobo.288.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">loan_amount</span></strong><span class="koboSpan" id="kobo.290.1">. </span><span class="koboSpan" id="kobo.290.2">It’s our goal, as data scientists and data engineers, to ensure that once uniqueness rules have been defined, data coming into the machine learning system conforms to those </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">uniqueness checks.</span></span></p>
<p><span class="koboSpan" id="kobo.292.1">In the next section, we will discuss data completeness or issues with incomplete data, and how to</span><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.293.1"> handle </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">incomplete data.</span></span></p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.295.1">Ensuring that the data is complete and not missing</span></h1>
<p><span class="koboSpan" id="kobo.296.1">Now that we </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.297.1">have achieved data consistency and uniqueness, it’s time to identify and address other quality issues. </span><span class="koboSpan" id="kobo.297.2">One</span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.298.1"> such issue is missing information in the data or incomplete data. </span><span class="koboSpan" id="kobo.298.2">Missing data is a common problem with real datasets. </span><span class="koboSpan" id="kobo.298.3">As a dataset’s size increases, the chance of data points going missing in the data increases. </span><span class="koboSpan" id="kobo.298.4">Missing records can occur in several ways, some of </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">which include:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.300.1">Merging of source datasets</span></strong><span class="koboSpan" id="kobo.301.1">: For example, when we try to match records against </span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.302.1">date of birth or a postcode to enrich data, and either of these is missing in one dataset or is inaccurate, such occurrences will take </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">NA values.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.304.1">Random events</span></strong><span class="koboSpan" id="kobo.305.1">: This</span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.306.1"> is quite common in surveys, where the person may not be aware of whether the information required is compulsory or they may not know </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">the answer.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.308.1">Failures of measurement</span></strong><span class="koboSpan" id="kobo.309.1">: For </span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.310.1">example, some traits, such as blood pressure, are known to have a very substantial component of random error when measured in the conventional way (that is, with a blood pressure cuff). </span><span class="koboSpan" id="kobo.310.2">If two people measure a subject’s blood pressure at almost the same time, or if one person measures a subject’s blood pressure twice in rapid succession, the measured values can easily differ by 10 mm/Hg (https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/measurement/). </span><span class="koboSpan" id="kobo.310.3">If a person is aware of these errors, they may decide to omit this information; for some patients, this data will take NA values. </span><span class="koboSpan" id="kobo.310.4">In finance, an important measurement ratio to determine the credit worthiness of someone or a firm is the debt-to-income ratio. </span><span class="koboSpan" id="kobo.310.5">There are scenarios when income is not declared, and in those circumstances, dividing debt by 0 or missing data would result in missing information for </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">the ratio.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.312.1">Poor process design around collecting data</span></strong><span class="koboSpan" id="kobo.313.1">: For example, in health surveys, people are often asked about their BMI, and not everyone knows their BMI or </span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.314.1">understands the measurement. </span><span class="koboSpan" id="kobo.314.2">It would be simpler and easier if we were to ask for someone’s height and weight as they are more likely to know this. </span><span class="koboSpan" id="kobo.314.3">Another problem arises when someone is asked about their weight </span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.315.1">measurement, where some people might be likely to omit or lie about this information. </span><span class="koboSpan" id="kobo.315.2">If BMI cannot be understood or measured at the time of collecting data, the data will take </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">NA values.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.317.1">When training datasets contain missing values, machine learning models can produce inaccurate predictions or fail to train properly due to the lack of complete information. </span><span class="koboSpan" id="kobo.317.2">In this section, we will discuss the following techniques for handling </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">missing data:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.319.1">Deleting data</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.320.1">Encoding missingness</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.321.1">Imputation methods</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.322.1">One way to get rid of missing data is by deleting the missing records. </span><span class="koboSpan" id="kobo.322.2">This is also known as </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.323.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.324.1">complete case analysis</span></strong><span class="koboSpan" id="kobo.325.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.326.1">CCA</span></strong><span class="koboSpan" id="kobo.327.1">) method. </span><span class="koboSpan" id="kobo.327.2">This is fine if less than 5% of rows contain missing values, but deleting more records could reduce the power of the model since the sample size will become smaller. </span><span class="koboSpan" id="kobo.327.3">There might also be a systematic bias in the data since this technique assumes that the data is missing completely and random, but it violates other assumptions such as when data</span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.328.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.329.1">missing at random</span></strong><span class="koboSpan" id="kobo.330.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.331.1">MAR</span></strong><span class="koboSpan" id="kobo.332.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.333.1">missing not at random</span></strong><span class="koboSpan" id="kobo.334.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.335.1">MNAR</span></strong><span class="koboSpan" id="kobo.336.1">). </span><span class="koboSpan" id="kobo.336.2">Hence, blindly removing the data could make</span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.337.1"> the model more biased. </span><span class="koboSpan" id="kobo.337.2">For instance, if a minority population has not declared their income in the past or has not held credit in the past, they may not have a credit score. </span><span class="koboSpan" id="kobo.337.3">If we remove this data blindly without understanding the reason for missingness, the algorithm could be more biased toward giving loans to majority groups that have credit information, and minority groups will be denied the opportunity, despite some members having solid income </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">and creditworthiness.</span></span></p>
<p><span class="koboSpan" id="kobo.339.1">Let’s explore the dataset using the CCA technique, remove all rows where information is missing, and figure out what volume of data </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">is lost:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.341.1">
remaining_rows = df_consistent.dropna(axis=0).shape[0]
total_records = df_consistent.shape[0]
perc_dropped = ((total_records - remaining_rows)/total_records)*100
print("By dropping all missing data, only {:,} records will be left out of {:,}, a reduction by {:,.3f}%".format(remaining_rows, total_records, perc_dropped))
By dropping all missing data, only 480 records will be left out of 614, a reduction by 21.824%</span></pre> <p><span class="koboSpan" id="kobo.342.1">Since 21% is </span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.343.1">almost one-fourth of the dataset, this is not a feasible </span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.344.1">method. </span><span class="koboSpan" id="kobo.344.2">Hence, in this section, we will explore how to identify missing data, uncover patterns or reasons for data being missing, and discover techniques for handling missing data so that the dataset can be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">machine learning.</span></span></p>
<p><span class="koboSpan" id="kobo.346.1">First, we will extract categorical features, binary features, and numerical features. </span><span class="koboSpan" id="kobo.346.2">To do this, we must separate the identifier and the </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">target label:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.348.1">
id_col = 'loan_id'
target = 'loan_status'
feature_cols = [cols for cols in df_consistent if cols not in [id_col, target]]
binary_cols = [cols for cols in feature_cols if df_consistent[cols].nunique() == 2]
cat_cols = [cols for cols in feature_cols if (df_consistent[cols].dtype == 'object' or df_consistent[cols].nunique() &lt;= 15)]
num_cols = [cols for cols in feature_cols if cols not in cat_cols]
cat_cols
['gender',
 'married',
 'dependents',
 'education',
 'self_employed',
 'loan_amount_term',
 'credit_history',
 'property_area']
binary_cols
['gender', 'married', 'education', 'self_employed', 'credit_history']
num_cols
['applicant_income', 'coapplicant_income', 'loan_amount']</span></pre> <p><span class="koboSpan" id="kobo.349.1">To check if data is </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.350.1">missing in the dataset, pandas provides a </span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.351.1">convenience method called </span><strong class="source-inline"><span class="koboSpan" id="kobo.352.1">.info()</span></strong><span class="koboSpan" id="kobo.353.1">. </span><span class="koboSpan" id="kobo.353.2">This method shows us how many rows are complete among the total records. </span><span class="koboSpan" id="kobo.353.3">The method also displays the data type of </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">each column:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.355.1">
df_consistent.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 614 entries, 0 to 613
Data columns (total 13 columns):
 #   Column              Non-Null Count  Dtype
---  ------              --------------  -----
 0   loan_id             614 non-null    object
 1   gender              601 non-null    float64
 2   married             611 non-null    float64
 3   dependents          599 non-null    float64
 4   education           614 non-null    int64
 5   self_employed       582 non-null    float64
 6   applicant_income    614 non-null    int64
 7   coapplicant_income  614 non-null    float64
 8   loan_amount         592 non-null    float64
 9   loan_amount_term    600 non-null    float64
 10  credit_history      564 non-null    float64
 11  property_area       614 non-null    object
 12  loan_status         614 non-null    object
dtypes: float64(8), int64(2), object(3)
memory usage: 62.5+ KB</span></pre> <p><span class="koboSpan" id="kobo.356.1">The </span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.357.1">pandas</span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.358.1"> library has another convenience method called </span><strong class="source-inline"><span class="koboSpan" id="kobo.359.1">.isnull()</span></strong><span class="koboSpan" id="kobo.360.1"> to check which row is missing data for a column and which row is complete. </span><span class="koboSpan" id="kobo.360.2">By combining </span><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">.sum()</span></strong><span class="koboSpan" id="kobo.362.1"> with </span><strong class="source-inline"><span class="koboSpan" id="kobo.363.1">.isnull()</span></strong><span class="koboSpan" id="kobo.364.1">, we can get the total number of missing records for </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">each column:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.366.1">
df_consistent.isnull().sum()
loan_id                0
gender                13
married                3
dependents            15
education              0
self_employed         32
applicant_income       0
coapplicant_income     0
loan_amount           22
loan_amount_term      14
credit_history        50
property_area          0
loan_status            0
dtype: int64</span></pre> <p><span class="koboSpan" id="kobo.367.1">As we can see, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">credit_history</span></strong><span class="koboSpan" id="kobo.369.1"> , </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">self_employed</span></strong><span class="koboSpan" id="kobo.371.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">loan_amount</span></strong><span class="koboSpan" id="kobo.373.1"> columns have the most missing data. </span><span class="koboSpan" id="kobo.373.2">Raw values can sometimes be difficult to comprehend and</span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.374.1"> it’s more useful to know the percentage of data that’s missing from each column. </span><span class="koboSpan" id="kobo.374.2">In </span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.375.1">the next step, we will create a function that will take the DataFrame and print out the missing percentage of data against each column. </span><span class="koboSpan" id="kobo.375.2">Then, we sort the data in descending order </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">of missingness:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.377.1">
def missing_data_percentage(df: pd.DataFrame):
    """Function to print percentage of missing values"""
    df = df.copy()
    missing_data = df.isnull().sum()
    total_records = df.shape[0]
    perc_missing = round((missing_data/total_records)*100, 3)
    missing_df = pd.DataFrame(data={'columm_name':perc_missing.index, 'perc_missing':perc_missing.values})
    return missing_df
missing_data_percentage(df_consistent[feature_cols]).sort_values(by='perc_missing', ascending=False)
           columm_name  perc_missing
9       credit_history         8.143
4        self_employed         5.212
7          loan_amount         3.583
2           dependents         2.443
8     loan_amount_term         2.280
0               gender         2.117
1              married         0.489
3            education         0.000
5     applicant_income         0.000
6   coapplicant_income         0.000
10       property_area         0.000</span></pre> <p><span class="koboSpan" id="kobo.378.1">Now, we </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.379.1">can extract the magnitude of missing data. </span><span class="koboSpan" id="kobo.379.2">However, before we dive into handling missing data, it is important to understand</span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.380.1"> the patterns for missing data. </span><span class="koboSpan" id="kobo.380.2">By understanding these relationships, we will be able to take appropriate steps. </span><span class="koboSpan" id="kobo.380.3">This is because imputing missing data can alter the distribution of the data, which may further affect </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">variable interaction.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">We will utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.383.1">missingno</span></strong><span class="koboSpan" id="kobo.384.1"> library and other visualizations to understand where data is missing, and in the absence of subject matter experts, we will make some assumptions on reasons for </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">missing data.</span></span></p>
<p><span class="koboSpan" id="kobo.386.1">To see where values are missing and where there are gaps in the data, we will utilize a </span><strong class="source-inline"><span class="koboSpan" id="kobo.387.1">matrix</span></strong><span class="koboSpan" id="kobo.388.1"> plot. </span><span class="koboSpan" id="kobo.388.2">A </span><strong class="source-inline"><span class="koboSpan" id="kobo.389.1">matrix</span></strong><span class="koboSpan" id="kobo.390.1"> plot can be quite useful when the dataset has depth or when the data contains time-related information. </span><span class="koboSpan" id="kobo.390.2">The presence of data is represented in gray, while absent data is displayed </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">in white:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.392.1">
msno.matrix(df_consistent[feature_cols], figsize=(35, 15))
&lt;AxesSubplot: &gt;</span></pre> <p><span class="koboSpan" id="kobo.393.1">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<span class="koboSpan" id="kobo.395.1"><img alt="Figure 5.2 – Matrix plot" src="image/B19297_05_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.396.1">Figure 5.2 – Matrix plot</span></p>
<p><span class="koboSpan" id="kobo.397.1">Looking </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.398.1">closer, we can see that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.399.1">credit_history</span></strong><span class="koboSpan" id="kobo.400.1"> column has a lot of missing points, and the occurrence of missingness is spread throughout the data and not at a given point </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">in time.</span></span></p>
<p><span class="koboSpan" id="kobo.402.1">As we </span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.403.1">touched upon earlier, understanding the reasons behind the missingness of data can help us choose the right technique to handle missing data. </span><span class="koboSpan" id="kobo.403.2">At a high level, we can call these mechanisms of missing data and classify them into </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">three categories:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.405.1">Missing completely at </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.406.1">random</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.407.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.408.1">MCAR</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.410.1">Missing not at </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.411.1">random</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.412.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.413.1">MNAR</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.415.1">Missing at </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.416.1">random</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.417.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.418.1">MAR</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.420.1">Data is MCAR when the likelihood of missing data is the same for all the observations, and there is no relationship between the data being missing and any other features in the dataset. </span><span class="koboSpan" id="kobo.420.2">For example, a mail questionnaire might get lost in the post or a person may have forgotten to answer a question if they were in a hurry. </span><span class="koboSpan" id="kobo.420.3">In such cases, data being missing has nothing to do with the type of question, age group, or gender (relationship with other variables), and we can classify such features or data points as MCAR. </span><span class="koboSpan" id="kobo.420.4">Removing these data points or changing the value to 0 for such instances will not bias </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">the prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.422.1">On the other hand, data is MAR when the likelihood of a data point being missing depends on other existing data points. </span><span class="koboSpan" id="kobo.422.2">For instance, if men don’t disclose their weight 5% of the time on average, whereas women don’t disclose their weight 15% of the time, we can assume that missingness in data is caused by the presence of gender bias. </span><span class="koboSpan" id="kobo.422.3">This will lead to a higher percentage of data being missing for women than for men. </span><span class="koboSpan" id="kobo.422.4">For this mechanism, we can impute data using statistical techniques or machine learning to predict the missing value by utilizing other features in </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">the dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.424.1">The third</span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.425.1"> mechanism, MNAR, can often be confused with</span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.426.1"> MCAR but is slightly different. </span><span class="koboSpan" id="kobo.426.2">In this scenario, a clear assumption can be made as to why data is not missing at random. </span><span class="koboSpan" id="kobo.426.3">For instance, if we are trying to understand what factors lead to depression (outcome), depressed people could be more likely to not answer questions or less likely to be contacted. </span><span class="koboSpan" id="kobo.426.4">Since missingness is related to the outcome, these missing records can be flagged as “missing” and for numerical features, we can use a combination of machine learning to impute missing data from other features and flag data points, where data is missing, by creating </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">another variable.</span></span></p>
<p><span class="koboSpan" id="kobo.428.1">Now that we understand the different types of missing data, we will utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.429.1">heatmap</span></strong><span class="koboSpan" id="kobo.430.1"> function from </span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">missingno</span></strong><span class="koboSpan" id="kobo.432.1">, which will create a correlation heatmap. </span><span class="koboSpan" id="kobo.432.2">The visualization shows a nullity correlation between columns of the dataset. </span><span class="koboSpan" id="kobo.432.3">It shows how strongly the presence or absence of one feature affects </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">the other.</span></span></p>
<p><span class="koboSpan" id="kobo.434.1">Nullity correlation ranges from -1 </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">to 1:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.436.1">-1 means if one column (attribute) is present, the other is almost </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">certainly absent</span></span></li>
<li><span class="koboSpan" id="kobo.438.1">0 means there is no dependence between the </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">columns (attributes)</span></span></li>
<li><span class="koboSpan" id="kobo.440.1">1 means that if one column (attribute) is present, the other is also </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">certainly present</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.442.1">Unlike a standard correlation heatmap, the following visualization is about the relationship between missing data features since few of them have missing data. </span><span class="koboSpan" id="kobo.442.2">Those columns that are always full or always empty have no meaningful correlation and are removed from </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">the visualization.</span></span></p>
<p><span class="koboSpan" id="kobo.444.1">This heatmap helps identify data completeness correlations between attribute pairs, but it has limited explanatory ability for </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">broader relationships:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.446.1">
msno.heatmap(df_consistent[feature_cols], labels=True)</span></pre> <p><span class="koboSpan" id="kobo.447.1">This results in the </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">following heatmap:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<span class="koboSpan" id="kobo.449.1"><img alt="Figure 5.3 – Heatmap plot" src="image/B19297_05_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.450.1">Figure 5.3 – Heatmap plot</span></p>
<p><span class="koboSpan" id="kobo.451.1">From</span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.452.1"> this plot, we </span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.453.1">can interpret relationships of missingness across a few variables. </span><span class="koboSpan" id="kobo.453.2">There is a correlation of 0.4 between </span><strong class="source-inline"><span class="koboSpan" id="kobo.454.1">dependents</span></strong><span class="koboSpan" id="kobo.455.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.456.1">married</span></strong><span class="koboSpan" id="kobo.457.1">, which makes sense as the majority of the time, someone gets married first before </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">having dependents.</span></span></p>
<p><span class="koboSpan" id="kobo.459.1">Next, we will extract columns that contain missing data and use these for the next visualization. </span><span class="koboSpan" id="kobo.459.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.460.1">dendrogram</span></strong><span class="koboSpan" id="kobo.461.1"> method uses hierarchical clustering and groups attributes together where missingness is associated with the missingness of another variable or completeness is associated with the completeness of </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">another variable:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.463.1">
missing_cols = [cols for cols in feature_cols if df_consistent[cols].isnull().sum() &gt; 0]
msno.dendrogram(df_consistent[missing_cols])</span></pre> <p><span class="koboSpan" id="kobo.464.1">The output is </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<span class="koboSpan" id="kobo.466.1"><img alt="Figure 5.4 – Dendrogram plot" src="image/B19297_05_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.467.1">Figure 5.4 – Dendrogram plot</span></p>
<p><span class="koboSpan" id="kobo.468.1">We</span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.469.1"> interpret the dendrogram based on a top-down approach – that is, we focus on the height at which any two columns are connected</span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.470.1"> with matters of nullity. </span><span class="koboSpan" id="kobo.470.2">The bigger the height, the smaller the relation, and vice versa. </span><span class="koboSpan" id="kobo.470.3">For example, the missingness or presence of data in </span><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">credit_history</span></strong><span class="koboSpan" id="kobo.472.1"> has no relationship with the missingness or completeness of any </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">other variable.</span></span></p>
<p><span class="koboSpan" id="kobo.474.1">With that, we have understood patterns of missing data and if there are relationships between missing data columns. </span><span class="koboSpan" id="kobo.474.2">Next, we will explore the relationship between missing data and the outcome. </span><span class="koboSpan" id="kobo.474.3">Before we decide to remove missing data or impute it, we should also look at whether the missingness of a variable is associated with the outcome – that is, is there a chance that data may </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">be MNAR?</span></span></p>
<p><span class="koboSpan" id="kobo.476.1">First, we will visualize this relationship in missing </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">categorical data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.478.1">
cat_missing = [cols for cols in cat_cols if df_consistent[cols].isnull().sum() &gt; 0]
def cat_missing_association_with_outcome(data, missing_data_column, outcome):
    """Function to plot missing association of categorical varibles with outcome"""
    df = data.copy()
    df[f"{missing_data_column}_is_missing"] = df[missing_data_column].isnull().astype(int)
    df.groupby([outcome]).agg({f"{missing_data_column}_is_missing": 'mean'}).plot.bar()
for cols in cat_missing:
    cat_missing_association_with_outcome(df_consistent, cols, target)</span></pre> <p><span class="koboSpan" id="kobo.479.1">This will </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.480.1">create some plots and show how categorical </span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.481.1">features are associated with the </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">target variable:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.483.1"><img alt="" role="presentation" src="image/B19297_05_5.jpg"/></span>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer040">
<span class="koboSpan" id="kobo.484.1"><img alt="" role="presentation" src="image/B19297_05_6.jpg"/></span>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.485.1"><img alt="" role="presentation" src="image/B19297_05_7.jpg"/></span>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer042">
<span class="koboSpan" id="kobo.486.1"><img alt="" role="presentation" src="image/B19297_05_8.jpg"/></span>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.487.1"><img alt="" role="presentation" src="image/B19297_05_9.jpg"/></span>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer044">
<span class="koboSpan" id="kobo.488.1"><img alt="" role="presentation" src="image/B19297_05_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.489.1">Figure 5.5 – The output plots displaying the association of categorical features with the target variable</span></p>
<p><span class="koboSpan" id="kobo.490.1">At a high level, we can assume that for variables such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">married</span></strong><span class="koboSpan" id="kobo.492.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">dependents</span></strong><span class="koboSpan" id="kobo.494.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">loan_amount_term</span></strong><span class="koboSpan" id="kobo.496.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.497.1">gender</span></strong><span class="koboSpan" id="kobo.498.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.499.1">credit_history</span></strong><span class="koboSpan" id="kobo.500.1">, the missingness of data is associated with the loan-approved status. </span><span class="koboSpan" id="kobo.500.2">Hence, we can say that the data for these variables is MNAR. </span><span class="koboSpan" id="kobo.500.3">For these three variables, we can encode missing data with the word “missing” as this signal will help predict the outcome. </span><span class="koboSpan" id="kobo.500.4">The missingness or completeness of </span><strong class="source-inline"><span class="koboSpan" id="kobo.501.1">credit_history</span></strong><span class="koboSpan" id="kobo.502.1"> is slightly associated with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">self_employed</span></strong><span class="koboSpan" id="kobo.504.1"> status, as indicated in the heatmap plot, which shows that the data might be</span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.505.1"> missing at random. </span><span class="koboSpan" id="kobo.505.2">Similarly, the missingness of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.506.1">married</span></strong><span class="koboSpan" id="kobo.507.1"> status is associated with the missingness of </span><strong class="source-inline"><span class="koboSpan" id="kobo.508.1">dependents</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.509.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">loan_amount</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.512.1">For all </span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.513.1">binary variables where data is missing, we can assume that data is not MCAR and rather assume that data is MNAR as there was some relationship of missingness of information with the outcome, or MAR, since missingness is associated with the presence or absence of other variables, as shown in </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">the dendrogram.</span></span></p>
<p><span class="koboSpan" id="kobo.515.1">One way to encode missing values is to encode these with the most frequent values or get rid of missing values or/and create an additional column that indicates missingness with 1 or 0. </span><span class="koboSpan" id="kobo.515.2">However, for MAR scenarios, this is not the best technique. </span><span class="koboSpan" id="kobo.515.3">As mentioned earlier, the goal of a data-centric approach is to improve data quality and reduce bias. </span><span class="koboSpan" id="kobo.515.4">Hence, instead of using frequency imputation methods or just deleting records, we should consider asking annotators to provide information where data is missing or make system fixes to recover from missing information. </span><span class="koboSpan" id="kobo.515.5">If that is not possible, we should consider using machine learning techniques or probabilistic techniques to determine possible values over simple imputation methods of </span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.516.1">mode, mean, and median. </span><span class="koboSpan" id="kobo.516.2">However, when missingness</span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.517.1"> exceeds certain thresholds, even advanced techniques are not reliable and it’s better to drop the feature. </span><span class="koboSpan" id="kobo.517.2">For the remaining variables, we will use a machine learning technique to determine the missing values since we cannot get annotators to help us provide </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">complete information.</span></span></p>
<p><span class="koboSpan" id="kobo.519.1">Now that we have identified how the missingness of categorical values is associated with the outcome, next, we will study the relationship between missing numerical data and </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">the outcome:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.521.1">
num_missing = [cols for cols in num_cols if df_consistent[cols].isnull().sum() &gt; 0]
def num_missing_association_with_outcome(data, missing_data_column, outcome):
    """Function to plot missing association of categorical varibles with outcome"""
    df = data.copy()
    df[f"{missing_data_column}_is_missing"] = df[missing_data_column].isnull().astype(int)
    df.groupby([outcome]).agg({f"{missing_data_column}_is_missing": 'mean'}).plot.bar()
for cols in num_missing:
    num_missing_association_with_outcome(df, cols, target)</span></pre> <p><span class="koboSpan" id="kobo.522.1">This will display the </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">following plot:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer045">
<span class="koboSpan" id="kobo.524.1"><img alt="Figure 5.6 – Loan amount missing association with target" src="image/B19297_05_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.525.1">Figure 5.6 – Loan amount missing association with target</span></p>
<p><span class="koboSpan" id="kobo.526.1">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.527.1">loan_amount</span></strong><span class="koboSpan" id="kobo.528.1">, it can be assumed that data is MNAR as well as MAR since the missingness or completion of data in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.529.1">married</span></strong><span class="koboSpan" id="kobo.530.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.531.1">dependents</span></strong><span class="koboSpan" id="kobo.532.1"> variables is slightly </span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.533.1">associated with the missingness and completeness of </span><strong class="source-inline"><span class="koboSpan" id="kobo.534.1">loan_amount</span></strong><span class="koboSpan" id="kobo.535.1">, as observed in the heatmap. </span><span class="koboSpan" id="kobo.535.2">Hence, we choose to impute missing values using machine learning and create an additional column to indicate missingness, which will provide a better signal to </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">our model.</span></span></p>
<p><span class="koboSpan" id="kobo.537.1">Next, we will dive into various methods of imputing data and compare these, as well as talking about the shortcomings of each approach. </span><span class="koboSpan" id="kobo.537.2">We will also discuss the implications of machine learning on imputing missing data in data-centric </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">machine learning.</span></span></p>
<p><span class="koboSpan" id="kobo.539.1">Following a model-centric approach, the standard rule of thumb for imputing numerical variables is that when 5% of the data is missing, impute it with the mean, median, or mode. </span><span class="koboSpan" id="kobo.539.2">This approach assumes that data is missing completely at random. </span><span class="koboSpan" id="kobo.539.3">If this assumption is not true, these simple imputation methods may obscure distributions and relationships within </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.541.1">First, we will explore the distribution of </span><strong class="source-inline"><span class="koboSpan" id="kobo.542.1">loan_amount</span></strong><span class="koboSpan" id="kobo.543.1"> without imputation and when </span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.544.1">imputed with the median. </span><span class="koboSpan" id="kobo.544.2">The distribution</span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.545.1"> changes when we impute 6% of the values with </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">the median:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.547.1">
df_consistent.loan_amount.plot.kde(color='orange', label='loan_amount', legend=True)
df_consistent.loan_amount.fillna(value=df.loan_amount.median()).plot.kde(color='b', label='loan_amount_imputed', alpha=0.5, figsize=(9,7), legend=True)</span></pre> <p><span class="koboSpan" id="kobo.548.1">The following plot is displayed as </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<span class="koboSpan" id="kobo.550.1"><img alt="Figure 5.7 – Simple density plot imputation with the median" src="image/B19297_05_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.551.1">Figure 5.7 – Simple density plot imputation with the median</span></p>
<p><span class="koboSpan" id="kobo.552.1">Next, we compare the standard deviation of the loan amount before and </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">after imputation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.554.1">
round(df_consistent.loan_amount.std(),2), round(df_consistent.loan_amount.fillna(value=df_consistent.loan_amount.median()).std(),2)
(85.59, 84.11)</span></pre> <p><span class="koboSpan" id="kobo.555.1">The </span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.556.1">preceding code shows how a simple imputation</span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.557.1"> method can obscure the distribution of data. </span><span class="koboSpan" id="kobo.557.2">To counter these effects and preserve the distribution, we will use the random sample </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">imputation method.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">First, we extract all the rows where </span><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">loan_amount</span></strong><span class="koboSpan" id="kobo.561.1"> is missing. </span><span class="koboSpan" id="kobo.561.2">Then, we compute the variables that are correlated with </span><strong class="source-inline"><span class="koboSpan" id="kobo.562.1">loan_amount</span></strong><span class="koboSpan" id="kobo.563.1"> and use those values to set a seed. </span><span class="koboSpan" id="kobo.563.2">This is because, if we use the same seed for all values, then the same random number will be generated and the method will behave similarly to arbitrary value imputation, which will be as ineffective as the simple imputation methods of mean </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">and median.</span></span></p>
<p><span class="koboSpan" id="kobo.565.1">The downside to random sample distribution is that covariance will be affected and we need a method that preserves the covariance </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">as well.</span></span></p>
<p><span class="koboSpan" id="kobo.567.1">First, we check which feature is highly correlated </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.569.1">loan_amount</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.571.1">
df_consistent[num_cols].corr()
                    applicant_income  coapplicant_income  loan_amount
applicant_income            1.000000           -0.116605     0.570909
coapplicant_income         -0.116605            1.000000     0.188619
loan_amount                 0.570909            0.188619     1.000000</span></pre> <p><span class="koboSpan" id="kobo.572.1">Here, we can see that </span><strong class="source-inline"><span class="koboSpan" id="kobo.573.1">loan_amount</span></strong><span class="koboSpan" id="kobo.574.1"> is highly correlated with </span><strong class="source-inline"><span class="koboSpan" id="kobo.575.1">applicant_income</span></strong><span class="koboSpan" id="kobo.576.1">, so for this example, we use this variable to set the seed. </span><span class="koboSpan" id="kobo.576.2">First, we extract the indexes where </span><strong class="source-inline"><span class="koboSpan" id="kobo.577.1">loan_amount</span></strong><span class="koboSpan" id="kobo.578.1"> is missing. </span><span class="koboSpan" id="kobo.578.2">Then, we use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.579.1">applicant_income</span></strong><span class="koboSpan" id="kobo.580.1"> value at the missing location and use this value to set the seed. </span><span class="koboSpan" id="kobo.580.2">Next, we use this seed to generate a random value from </span><strong class="source-inline"><span class="koboSpan" id="kobo.581.1">loan_amount</span></strong><span class="koboSpan" id="kobo.582.1"> to impute the missing row. </span><span class="koboSpan" id="kobo.582.2">We use this approach to impute all the missing data </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">for </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">loan_amount</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.586.1">
observation = df_consistent[df_consistent.loan_amount.isnull()]
imputed_values = []
for idx in observation.index:
    seed = int(observation.loc[idx,['applicant_income']])
    imputed_value = df_consistent['loan_amount'].dropna().sample(1, random_state=seed)
    imputed_values.append(imputed_value)
df_consistent.loc[df_consistent['loan_amount'].isnull(),'loan_amount_random_imputed']=imputed_values
df_consistent.loc[df['loan_amount'].isnull()==False,'loan_amount_random_imputed']=df_consistent[df_consistent['loan_amount'].isnull()==False]['loan_amount'].values</span></pre> <p><span class="koboSpan" id="kobo.587.1">Next, we </span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.588.1">compare the distribution of </span><strong class="source-inline"><span class="koboSpan" id="kobo.589.1">loan_amount</span></strong><span class="koboSpan" id="kobo.590.1"> with </span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.591.1">the random sample imputed </span><strong class="source-inline"><span class="koboSpan" id="kobo.592.1">loan_amount</span></strong><span class="koboSpan" id="kobo.593.1"> and median </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">imputed </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">loan_amount</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.597.1">
df_consistent.loan_amount.plot.kde(color='orange', label='loan_amount', legend=True, linewidth=2)
df_consistent.loan_amount_random_imputed.plot.kde(color='g', label='loan_amount_random_imputed', legend=True, linewidth=2)
df_consistent.loan_amount.fillna(value=df_consistent.loan_amount.median()).plot.kde(color='b', label='loan_amount_median_imputed', linewidth=1, alpha=0.5, figsize=(9,7), legend=True)
&lt;AxesSubplot: ylabel='Density'&gt;</span></pre> <p><span class="koboSpan" id="kobo.598.1">This will output the </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">following plot:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<span class="koboSpan" id="kobo.600.1"><img alt="Figure 5.8 – Density plot showing random and median imputations" src="image/B19297_05_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.601.1">Figure 5.8 – Density plot showing random and median imputations</span></p>
<p><span class="koboSpan" id="kobo.602.1">Now, we </span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.603.1">compare the standard deviation of the pre-imputed </span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.604.1">loan with the random sample imputed method and median </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">imputed method:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.606.1">
round(df_consistent.loan_amount.std(),2), round(df_consistent.loan_amount_random_imputed.std(),2), round(df_consistent.loan_amount.fillna(value=df_consistent.loan_amount.median()).std(),2)
(85.59, 85.57, 84.11)</span></pre> <p><span class="koboSpan" id="kobo.607.1">The random sample imputation method is much closer in distribution and standard deviation to the pre-imputed </span><strong class="source-inline"><span class="koboSpan" id="kobo.608.1">loan_amount</span></strong><span class="koboSpan" id="kobo.609.1"> method than the median imputed </span><strong class="source-inline"><span class="koboSpan" id="kobo.610.1">loan_amount</span></strong><span class="koboSpan" id="kobo.611.1"> method. </span><span class="koboSpan" id="kobo.611.2">Next, we check whether the random sample imputation method preserves the correlation with other variables compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">other methods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.613.1">
df_consistent['loan_amount_median_imputed'] = df_consistent['loan_amount'].fillna(value=df_consistent['loan_amount'].median())
df_consistent[['loan_amount', 'loan_amount_median_imputed','loan_amount_random_imputed', 'applicant_income']].corr()</span></pre> <p><span class="koboSpan" id="kobo.614.1">The resulting DataFrame is </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<span class="koboSpan" id="kobo.616.1"><img alt="Figure 5.9 – Correlation DataFrame" src="image/B19297_05_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.617.1">Figure 5.9 – Correlation DataFrame</span></p>
<p><span class="koboSpan" id="kobo.618.1">From this, it’s </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.619.1">evident that the random imputation method </span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.620.1">can preserve the distribution but may obscure the inter-relationships with other variables. </span><span class="koboSpan" id="kobo.620.2">We need a method that can preserve the distribution and also maintain the inter-relationships with other variables. </span><span class="koboSpan" id="kobo.620.3">We will use machine learning to help us achieve this. </span><span class="koboSpan" id="kobo.620.4">Before we move on to machine learning, we will first discuss the impact of simple imputation on categorical/binary variables. </span><span class="koboSpan" id="kobo.620.5">We impute the </span><strong class="source-inline"><span class="koboSpan" id="kobo.621.1">credit_history</span></strong><span class="koboSpan" id="kobo.622.1"> binary column with the most frequent value and compare the distribution before and </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">after imputation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.624.1">
df_consistent.credit_history.value_counts(normalize=True)
1.0    0.842199
0.0    0.157801
Name: credit_history, dtype: float64
df_consistent.credit_history.fillna(value=df_consistent.credit_history.mode()[0]).value_counts(normalize=True)
1.0    0.855049
0.0    0.144951
Name: credit_history, dtype: float64</span></pre> <p><span class="koboSpan" id="kobo.625.1">By imputing </span><strong class="source-inline"><span class="koboSpan" id="kobo.626.1">credit_history</span></strong><span class="koboSpan" id="kobo.627.1"> with the most frequent values, we have biased the data toward the </span><strong class="source-inline"><span class="koboSpan" id="kobo.628.1">credit_history</span></strong><span class="koboSpan" id="kobo.629.1"> status. </span><span class="koboSpan" id="kobo.629.2">As we discovered previously, the missingness of </span><strong class="source-inline"><span class="koboSpan" id="kobo.630.1">credit_history</span></strong><span class="koboSpan" id="kobo.631.1"> is not associated with any other variables, but it might be associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">the outcome.</span></span></p>
<p><span class="koboSpan" id="kobo.633.1">The </span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.634.1">preceding examples show that if we utilize simple</span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.635.1"> imputation methods then we may bias the data, and the distribution will be altered as well, whereas if we utilize random methods, the distribution will be preserved but the data relationships may change and data variance may increase. </span><span class="koboSpan" id="kobo.635.2">Hence, when data is MAR or MNAR, to achieve a balance between data bias and data variance, we can use a machine </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">learning model.</span></span></p>
<p><span class="koboSpan" id="kobo.637.1">To utilize machine learning for numerical imputation, we will leverage the nearest neighbors imputation method available in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.638.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.639.1"> library – </span><strong class="source-inline"><span class="koboSpan" id="kobo.640.1">KNNImputer</span></strong><span class="koboSpan" id="kobo.641.1">. </span><span class="koboSpan" id="kobo.641.2">One issue with the imputer is that we can only pass a DataFrame to it, and not pass a list of columns. </span><span class="koboSpan" id="kobo.641.3">Hence, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.642.1">SklearnTransformerWrapper</span></strong><span class="koboSpan" id="kobo.643.1"> module, which is available as part of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">feature-engine</span></strong><span class="koboSpan" id="kobo.645.1"> library, to pass a list of columns. </span><span class="koboSpan" id="kobo.645.2">Since KNN is a distance-based algorithm, to ensure that the model converges and one variable doesn’t overpower the other variable, we must scale the data before using </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">this algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.647.1">Another technique to impute data is referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.648.1">Multiple Imputation by Chained Equations</span></strong><span class="koboSpan" id="kobo.649.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.650.1">MICE</span></strong><span class="koboSpan" id="kobo.651.1">). </span><span class="koboSpan" id="kobo.651.2">MICE </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.652.1">works by imputing all the data with the mean, median, or mode. </span><span class="koboSpan" id="kobo.652.2">Then, regarding the variable that will be imputed, the initial imputed values are converted back into missing values. </span><span class="koboSpan" id="kobo.652.3">Then, using other variables as predictors, a machine learning model is used to predict missing values. </span><span class="koboSpan" id="kobo.652.4">After this, the next variable is imputed in a similar manner, where initially imputed values are converted back into missing values, and other variables, including the recently imputed variable, are used as predictors to impute the missing values. </span><span class="koboSpan" id="kobo.652.5">Once all the variables with missing values have been modeled and values have been imputed with predictions, the first round of imputation is completed. </span><span class="koboSpan" id="kobo.652.6">This procedure is repeated </span><em class="italic"><span class="koboSpan" id="kobo.653.1">n</span></em><span class="koboSpan" id="kobo.654.1"> number of times (ideally 10), and from round two, round one predictions are used to predict records that were </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">initially missing.</span></span></p>
<p><span class="koboSpan" id="kobo.656.1">The reason for using several rounds is that, initially, we are modeling the missing data using other variables that also have NA values, and the initial strategy of imputation uses suboptimal methods such as the mean, median, or mode, which may bias the predictions. </span><span class="koboSpan" id="kobo.656.2">As we continue to regress over multiple rounds, predictions will stabilize and become </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">less biased.</span></span></p>
<p><span class="koboSpan" id="kobo.658.1">One issue with MICE is that we have to choose which machine learning model to use for the task. </span><span class="koboSpan" id="kobo.658.2">We will implement MICE with the random forest algorithm, which in R language is referred to </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">as </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.660.1">[missForest]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.662.1">In our implementation of MICE, we will refer to it as </span><strong class="source-inline"><span class="koboSpan" id="kobo.663.1">missForest</span></strong><span class="koboSpan" id="kobo.664.1"> since it will be a replica of how it is implemented in R (https://rpubs.com/lmorgan95/MissForest#:~:text=MissForest%20is%20a%20random%20forest,then%20predicts%20the%20missing%20part). </span><span class="koboSpan" id="kobo.664.2">To counter the effects of choosing an algorithm, we encourage practitioners to leverage automated machine </span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.665.1">learning, where for each imputation and iteration, a </span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.666.1">new algorithm will be chosen. </span><span class="koboSpan" id="kobo.666.2">The one disadvantage of this approach is that it’s computationally intensive and time-intensive when utilized for </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">big datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.668.1">First, we import the </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">necessary packages:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.670.1">
from sklearn.impute import KNNImputer
from feature_engine.wrappers import SklearnTransformerWrapper
from sklearn.preprocessing import StandardScaler</span></pre> <p><span class="koboSpan" id="kobo.671.1">Next, we extract the numerical columns by filtering any columns that may have more than 15 categories while filtering the </span><strong class="source-inline"><span class="koboSpan" id="kobo.672.1">id</span></strong><span class="koboSpan" id="kobo.673.1"> column and outcome, as well as filtering newly created variables using </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">imputation methods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.675.1">
num_cols = [cols for cols in df_consistent if df_consistent[cols].nunique() &gt; 15 and cols not in [id_col, target] and not cols.endswith('imputed')]</span></pre> <p><span class="koboSpan" id="kobo.676.1">Next, we create the DataFrame with numerical variables and </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">visualize it:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.678.1">
df_num = df_consistent[num_cols].copy()
df_num.head()
   applicant_income  coapplicant_income  loan_amount
0              5849                 0.0          NaN
1              4583              1508.0        128.0
2              3000                 0.0         66.0
3              2583              2358.0        120.0
4              6000                 0.0        141.0</span></pre> <p><span class="koboSpan" id="kobo.679.1">Next, we build a function that takes the scaler (standard scaler or any other scaler) and DataFrame and returns scaled data and the processed scaler. </span><span class="koboSpan" id="kobo.679.2">We must scale the dataset before applying the KNN imputer since a distance-based method requires data to be on the same scale. </span><span class="koboSpan" id="kobo.679.3">Once we have scaled the data, we apply the KNN imputer to impute the data, and then unscale the data using the processed</span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.680.1"> scaler returned by the function. </span><span class="koboSpan" id="kobo.680.2">Once we’ve done this, we </span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.681.1">can compare the machine learning imputed data with the median and random </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">imputation methods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.683.1">
def scale_data(df, scaler, columns):
    """Function to scale the data"""
    df_scaled = df.copy()
    if columns:
        df_scaled[columns] = scaler.fit_transform(df_scaled[columns])
    else:
        columns = [cols for cols in df_scaled]
        df_scaled[columns] = scaler.fit_transform(df_scaled[columns])
    return df_scaled, scaler</span></pre> <p><span class="koboSpan" id="kobo.684.1">Next, we define the scaler and call the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.685.1">scale_data</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.686.1"> function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.687.1">
scaler = StandardScaler()
df_scaled, scaler = scale_data(df_num, scaler=scaler, columns=num_cols)</span></pre> <p><span class="koboSpan" id="kobo.688.1">Then, we apply the KNN imputer with a parameter of 10 neighbors to impute the data. </span><span class="koboSpan" id="kobo.688.2">We utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.689.1">weights='distance'</span></strong><span class="koboSpan" id="kobo.690.1"> parameter so that more weightage will be given to the votes of the nearer neighbors compared to the ones that are further away when predicting </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">the outcome.</span></span></p>
<p><span class="koboSpan" id="kobo.692.1">First, we initialize </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">the imputer:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.694.1">
knn_imputer = SklearnTransformerWrapper(
    transformer = KNNImputer(n_neighbors=10, weights='distance'),
    variables = num_cols
)</span></pre> <p><span class="koboSpan" id="kobo.695.1">Then, we apply </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">the imputation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.697.1">
df_imputed = knn_imputer.fit_transform(df_scaled)</span></pre> <p><span class="koboSpan" id="kobo.698.1">Next, we</span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.699.1"> unscale the data by calling the </span><strong class="source-inline"><span class="koboSpan" id="kobo.700.1">inverse_transform</span></strong><span class="koboSpan" id="kobo.701.1"> method from the scaler object and overwrite the </span><strong class="source-inline"><span class="koboSpan" id="kobo.702.1">df_imputed</span></strong><span class="koboSpan" id="kobo.703.1"> DataFrame with </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">unscaled values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.705.1">
df_imputed = pd.DataFrame(columns=num_cols, data=scaler.inverse_transform(df_imputed))
df_imputed.head()
   applicant_income  coapplicant_income  loan_amount
0            5849.0                 0.0   149.666345
1            4583.0              1508.0   128.000000
2            3000.0                 0.0    66.000000
3            2583.0              2358.0   120.000000
4            6000.0                 0.0   141.000000</span></pre> <p><span class="koboSpan" id="kobo.706.1">Next, we </span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.707.1">compare the distribution of the pre-imputed </span><strong class="source-inline"><span class="koboSpan" id="kobo.708.1">loan_amount</span></strong><span class="koboSpan" id="kobo.709.1"> and compare it with the machine learning imputed method. </span><span class="koboSpan" id="kobo.709.2">Then, we check the correlation of the machine learning imputed method to the applicant’s income and compare it with other </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">imputed methods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.711.1">
df_imputed['loan_amount'].plot.kde(color='orange', label='loan_amount_knn_imputed',linewidth=2, legend=True)
df_consistent['loan_amount'].plot.kde(color='b', label='loan_amount', legend=True, linewidth=2, figsize=(9,7), alpha=0.5)</span></pre> <p><span class="koboSpan" id="kobo.712.1">The resulting plot is </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.714.1"><img alt="Figure 5.10 – Loan amount KNN imputation" src="image/B19297_05_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.715.1">Figure 5.10 – Loan amount KNN imputation</span></p>
<p><span class="koboSpan" id="kobo.716.1">Next, we</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.717.1"> compare the standard deviation of the pre-imputed loan amount with all the </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">imputation methods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.719.1">
round(df_consistent.loan_amount.std(),2), round(df_consistent.loan_amount_random_imputed.std(),2), round(df_consistent.loan_amount_median_imputed.std(),2), round(df_imputed.loan_amount.std(),2)
(85.59, 85.57, 84.11, 85.59)</span></pre> <p><span class="koboSpan" id="kobo.720.1">Then, we will </span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.721.1">check if the correlation is maintained when </span><strong class="source-inline"><span class="koboSpan" id="kobo.722.1">loan_amount</span></strong><span class="koboSpan" id="kobo.723.1"> is imputed using </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">machine learning:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.725.1">
df_consistent['loan_amount_knn_imputed'] = df_imputed.loan_amount
df_consistent[['loan_amount', 'loan_amount_median_imputed','loan_amount_random_imputed', 'loan_amount_knn_imputed', 'applicant_income']].corr()</span></pre> <div>
<div class="IMG---Figure" id="_idContainer050">
<span class="koboSpan" id="kobo.726.1"><img alt="Figure 5.11 – Correlation after loan_amount is imputed" src="image/B19297_05_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.727.1">Figure 5.11 – Correlation after loan_amount is imputed</span></p>
<p><span class="koboSpan" id="kobo.728.1">The machine learning-imputed method has almost the same distribution as the original. </span><span class="koboSpan" id="kobo.728.2">However, the correlation is a bit higher with </span><strong class="source-inline"><span class="koboSpan" id="kobo.729.1">applicant_income</span></strong><span class="koboSpan" id="kobo.730.1"> compared to the pre-imputed </span><strong class="source-inline"><span class="koboSpan" id="kobo.731.1">loan_amount</span></strong><span class="koboSpan" id="kobo.732.1">. </span><span class="koboSpan" id="kobo.732.2">We have now seen how to use out-of-the-box techniques to impute missing data. </span><span class="koboSpan" id="kobo.732.3">One advantage of this method is that it’s simple to implement. </span><span class="koboSpan" id="kobo.732.4">However, the disadvantage is that we cannot choose </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">another algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.734.1">Hence, in the </span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.735.1">next step, we go one step further and build </span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.736.1">a MICE implementation with random forest. </span><span class="koboSpan" id="kobo.736.2">First, we convert categorical data into numerical data using one-hot encoding. </span><span class="koboSpan" id="kobo.736.3">Then, we impute missing categorical data with the MICE implementation </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.738.1">RandomForestClassifier</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.740.1">Once the categorical data has been imputed, we use categorical and numerical data to impute numerical missing values by utilizing MICE </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.742.1">RandomForestRegressor</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.744.1">To build the MICE implementation, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.745.1">IterativeImputer</span></strong><span class="koboSpan" id="kobo.746.1">, which is available in scikit-learn, to help with 10 rounds of MICE. </span><span class="koboSpan" id="kobo.746.2">To leverage </span><strong class="source-inline"><span class="koboSpan" id="kobo.747.1">IterativeImputer</span></strong><span class="koboSpan" id="kobo.748.1">, we must import </span><strong class="source-inline"><span class="koboSpan" id="kobo.749.1">enable_iterative_imputer</span></strong><span class="koboSpan" id="kobo.750.1"> from scikit-learn’s experimental packages, as per the </span><span class="No-Break"><span class="koboSpan" id="kobo.751.1">docs: </span></span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html"><span class="No-Break"><span class="koboSpan" id="kobo.752.1">https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.753.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.754.1">First, we import the </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">necessary packages:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.756.1">
from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from feature_engine.encoding import OneHotEncoder</span></pre> <p><span class="koboSpan" id="kobo.757.1">Next, we extract the categorical columns that are string-encoded so that we can one-hot </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">encode these:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.759.1">
ohe_cols = [cols for cols in cat_cols if df_consistent[cols].dtype == 'object']
ohe_cols
['property_area']</span></pre> <p><span class="koboSpan" id="kobo.760.1">Then, we</span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.761.1"> one-hot encode the </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">categorical columns:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.763.1">
df_ohe_encoded = df_consistent.copy()
ohe = OneHotEncoder(variables=ohe_cols)
df_ohe_encoded = ohe.fit_transform(df_ohe_encoded)</span></pre> <p><span class="koboSpan" id="kobo.764.1">After that, we</span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.765.1"> visualize the first five results of the one-hot </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">encoded data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.767.1">
df_ohe_encoded[[cols for cols in df_ohe_encoded if 'property_area' in cols]].head()
   property_area_urban  property_area_rural  property_area_semiurban
0                    1                    0                        0
1                    0                    1                        0
2                    1                    0                        0
3                    1                    0                        0
4                    1                    0                        0</span></pre> <p><span class="koboSpan" id="kobo.768.1">Next, we extract the categorical variables that are binary encoded, including the data that has been </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">one-hot encoded:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.770.1">
cat_cols = [cols for cols in df_ohe_encoded if df_ohe_encoded[cols].nunique() &lt;= 15 and cols not in [id_col, target]]
cat_cols
['gender',
 'married',
 'dependents',
 'education',
 'self_employed',
 'loan_amount_term',
 'credit_history',
 'property_area_urban',
 'property_area_rural',
 'property_area_semiurban']</span></pre> <p><span class="koboSpan" id="kobo.771.1">Then, we</span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.772.1"> build the MICE implementation with random </span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.773.1">forest to impute </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">categorical data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.775.1">
miss_forest_classifier = IterativeImputer(
    estimator=ExtraTreesClassifier(n_estimators=100,
                                   random_state=1,
                                   bootstrap=True,
                                   n_jobs=-1),
    max_iter=10,
    random_state=1,
    add_indicator=True,
    initial_strategy='median')
df_cat_imputed = miss_forest_classifier.fit_transform(df_ohe_encoded[cat_cols])</span></pre> <p><span class="koboSpan" id="kobo.776.1">Next, we </span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.777.1">extract the features from the imputation by converting the NumPy array into a DataFrame </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">called </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.779.1">df_cat_imputed</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.781.1">
df_cat_imputed = pd.DataFrame(
    columns=miss_forest_classifier.get_feature_names_out(),
    data=df_cat_imputed,
    index=df_ohe_encoded.index)</span></pre> <p><span class="koboSpan" id="kobo.782.1">Let’s ensure </span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.783.1">we don’t have any new unexpected values being created by the classifier. </span><span class="koboSpan" id="kobo.783.2">To check this, we iterate over all the columns and print the unique values for </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">each column:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.785.1">
for cols in cat_cols:
    print(cols)
    print(df_cat_imputed[cols].unique())
    print()
gender
[1. </span><span class="koboSpan" id="kobo.785.2">0.]
married
[0. </span><span class="koboSpan" id="kobo.785.3">1.]
dependents
[0. </span><span class="koboSpan" id="kobo.785.4">1. </span><span class="koboSpan" id="kobo.785.5">2. </span><span class="koboSpan" id="kobo.785.6">3.]
education
[1. </span><span class="koboSpan" id="kobo.785.7">0.]
self_employed
[0. </span><span class="koboSpan" id="kobo.785.8">1.]
loan_amount_term
[360. </span><span class="koboSpan" id="kobo.785.9">120. </span><span class="koboSpan" id="kobo.785.10">240. </span><span class="koboSpan" id="kobo.785.11">180.  60. </span><span class="koboSpan" id="kobo.785.12">300. </span><span class="koboSpan" id="kobo.785.13">480.  36.  84.  12.]
credit_history
[1. </span><span class="koboSpan" id="kobo.785.14">0.]
property_area_urban
[1. </span><span class="koboSpan" id="kobo.785.15">0.]
property_area_rural
[0. </span><span class="koboSpan" id="kobo.785.16">1.]
property_area_semiurban
[0. </span><span class="koboSpan" id="kobo.785.17">1.]</span></pre> <p><span class="koboSpan" id="kobo.786.1">Now, we </span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.787.1">combine the categorical imputed data with </span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.788.1">numerical data. </span><span class="koboSpan" id="kobo.788.2">Then, we use all the data to impute </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">numerical data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.790.1">
num_cols = [cols for cols in df_consistent if cols not in df_cat_imputed and cols not in [id_col, target] + ohe_cols
            and not cols.endswith("imputed")]
df_combined = pd.concat([df_consistent[num_cols], df_cat_imputed], axis=1)
feature_cols = [cols for cols in df_combined]
feature_cols
['applicant_income',
 'coapplicant_income',
 'loan_amount',
 'gender',
 'married',
 'dependents',
 'education',
 'self_employed',
 'loan_amount_term',
 'credit_history',
 'property_area_urban',
 'property_area_rural',
 'property_area_semiurban',
 'missingindicator_gender',
 'missingindicator_married',
 'missingindicator_dependents',
 'missingindicator_self_employed',
 'missingindicator_loan_amount_term',
 'missingindicator_credit_history']</span></pre> <p><span class="koboSpan" id="kobo.791.1">Next, we</span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.792.1"> implement </span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.793.1">MICE imputation with random forest to impute </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">numerical data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.795.1">
miss_forest_regressor = IterativeImputer(
    estimator=ExtraTreesRegressor(n_estimators=100,
                                  random_state=1,
                                  bootstrap=True,
                                  n_jobs=-1),
    max_iter=10,
    random_state=1,
    add_indicator=True,
    initial_strategy='median')
df_imputed = miss_forest_regressor.fit_transform(df_combined[feature_cols])</span></pre> <p><span class="koboSpan" id="kobo.796.1">Now, we extract the features from the imputation by converting the NumPy array into </span><span class="No-Break"><span class="koboSpan" id="kobo.797.1">a DataFrame:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.798.1">
df_imputed
df_imputed = pd.DataFrame(data=df_imputed,
                           columns=miss_forest_regressor.get_feature_names_out(),
                           index=df_combined.index)</span></pre> <p><span class="koboSpan" id="kobo.799.1">Then, we check whether all the columns have been imputed and there are no </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">missing values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.801.1">
df_imputed.isnull().sum()
applicant_income                     0
coapplicant_income                   0
loan_amount                          0
gender                               0
married                              0
dependents                           0
education                            0
self_employed                        0
loan_amount_term                     0
credit_history                       0
property_area_urban                  0
property_area_rural                  0
property_area_semiurban              0
missingindicator_gender              0
missingindicator_married             0
missingindicator_dependents          0
missingindicator_self_employed       0
missingindicator_loan_amount_term    0
missingindicator_credit_history      0
missingindicator_loan_amount         0
dtype: int64</span></pre> <p><span class="koboSpan" id="kobo.802.1">Next, we </span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.803.1">compare the distribution of the pre-imputed </span><strong class="source-inline"><span class="koboSpan" id="kobo.804.1">loan_amount</span></strong><span class="koboSpan" id="kobo.805.1"> and compare it with the MICE imputed method. </span><span class="koboSpan" id="kobo.805.2">We then </span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.806.1">check the correlation of the MICE imputed method with the applicant’s income and compare it with other </span><span class="No-Break"><span class="koboSpan" id="kobo.807.1">imputed methods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.808.1">
df_imputed['loan_amount'].plot.kde(color='orange', label='loan_amount_miss_forest_imputed',linewidth=2, legend=True)
df_consistent['loan_amount'].plot.kde(color='b', label='loan_amount', legend=True, linewidth=2, figsize=(9,7), alpha=0.5)
&lt;AxesSubplot: ylabel='Density'&gt;</span></pre> <p><span class="koboSpan" id="kobo.809.1">The output is </span><span class="No-Break"><span class="koboSpan" id="kobo.810.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.811.1"><img alt="Figure 5.12 – loan_amount_miss_forest_imputed" src="image/B19297_05_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.812.1">Figure 5.12 – loan_amount_miss_forest_imputed</span></p>
<p><span class="koboSpan" id="kobo.813.1">Next, we </span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.814.1">compare the standard deviation of the pre-imputed </span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.815.1">loan amount with all the imputation methods, including the MICE </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">imputation method:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.817.1">
round(df_consistent.loan_amount.std(),2), round(df_consistent.loan_amount_random_imputed.std(),2), round(df_consistent.loan_amount_median_imputed.std(),2), round(df_imputed.loan_amount.std(),2)
(85.59, 85.57, 84.11, 85.41)</span></pre> <p><span class="koboSpan" id="kobo.818.1">Then, we check if the correlation is maintained when </span><strong class="source-inline"><span class="koboSpan" id="kobo.819.1">loan_amount</span></strong><span class="koboSpan" id="kobo.820.1"> is imputed using the MICE imputation method compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">other methods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.822.1">
df_consistent['loan_amount_miss_forest_imputed'] = df_imputed.loan_amount
df_consistent[['loan_amount', 'loan_amount_median_imputed','loan_amount_random_imputed', 'loan_amount_miss_forest_imputed', 'applicant_income']].corr()</span></pre> <p><span class="koboSpan" id="kobo.823.1">The output DataFrame is </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.825.1"><img alt="Figure 5.13 – Correlation after imputing loan_amou﻿nt using the MICE imputation method" src="image/B19297_05_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.826.1">Figure 5.13 – Correlation after imputing loan_amount using the MICE imputation method</span></p>
<p><span class="koboSpan" id="kobo.827.1">The</span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.828.1"> standard </span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.829.1">deviation is slightly below the random imputation but higher than the median imputed method. </span><span class="koboSpan" id="kobo.829.2">As we can see, the correlation with </span><strong class="source-inline"><span class="koboSpan" id="kobo.830.1">applicant_income</span></strong><span class="koboSpan" id="kobo.831.1"> hasn’t improved compared to the random imputation method or median imputation method. </span><span class="koboSpan" id="kobo.831.2">Hence, to test whether MICE with random forest is a better implementation for this use case, we can compare the evaluation metric of the machine learning model when MICE is utilized versus when median imputation </span><span class="No-Break"><span class="koboSpan" id="kobo.832.1">is utilized.</span></span></p>
<p><span class="koboSpan" id="kobo.833.1">But before we do that, we want machine learning practitioners to explore </span><strong class="bold"><span class="koboSpan" id="kobo.834.1">automated machine learning</span></strong><span class="koboSpan" id="kobo.835.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.836.1">AutoML</span></strong><span class="koboSpan" id="kobo.837.1">) with</span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.838.1"> the MICE imputation framework. </span><span class="koboSpan" id="kobo.838.2">Machine learning can be a tedious process that consists of trial and error, hence why AutoML frameworks are getting quite popular when it comes to reducing human time. </span><span class="koboSpan" id="kobo.838.3">These frameworks automate feature engineering, cross-validation, model selection, and model tuning. </span><span class="koboSpan" id="kobo.838.4">One issue with the current implementation of MICE is that we have to choose which machine learning model to use for the task. </span><span class="koboSpan" id="kobo.838.5">What if we wanted to trial multiple algorithms to see which provided the best prediction for the imputation task, and while doing that wanted to ensure the prediction was generalizable and the model was not overfitted or underfitted? </span><span class="koboSpan" id="kobo.838.6">We can imagine the complexity. </span><span class="koboSpan" id="kobo.838.7">To counter this, we’ll combine AutoML </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">with MICE.</span></span></p>
<p><span class="koboSpan" id="kobo.840.1">One advantage of this approach is that at each iteration, a new model will be picked by AutoML, thus freeing the machine learning practitioner from tedious tasks. </span><span class="koboSpan" id="kobo.840.2">However, the disadvantage of this approach is that when the data increases in size, a lot more resources will be needed, which may not be viable. </span><span class="koboSpan" id="kobo.840.3">Another disadvantage with some open source AutoML frameworks is that on some operating systems, full functionality is error-prone. </span><span class="koboSpan" id="kobo.840.4">For instance, on Mac computers, both TPOT and AutoSklearn frameworks give errors when parallel processing is used. </span><span class="koboSpan" id="kobo.840.5">Hence, we will let you explore your own flavor of AutoML </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">with MICE.</span></span></p>
<p><span class="koboSpan" id="kobo.842.1">Next, we </span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.843.1">will </span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.844.1">implement a scikit-learn pipeline that will include the MICE implementation with random forest. </span><span class="koboSpan" id="kobo.844.2">Then, we train the decision tree model with cross-validation and evaluate the model using accuracy and ROC. </span><span class="koboSpan" id="kobo.844.3">Once we’ve done this, we create another pipeline, which will use simple imputation methods, and compare the evaluation results. </span><span class="koboSpan" id="kobo.844.4">Finally, we explore techniques for further improving the data to enhance </span><span class="No-Break"><span class="koboSpan" id="kobo.845.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.846.1">We’ll be converting these steps into a scikit-learn pipeline since by using a pipeline, we can define the sequence of steps and also save these steps as a pickle object. </span><span class="koboSpan" id="kobo.846.2">By utilizing this practice, we maintain machine learning system best practices and can ensure reliability and reproducibility without replicating the code in the </span><span class="No-Break"><span class="koboSpan" id="kobo.847.1">inference environment.</span></span></p>
<p><span class="koboSpan" id="kobo.848.1">First, let’s drop all the newly created columns in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.849.1">df_consistent</span></strong><span class="koboSpan" id="kobo.850.1"> DataFrame that end </span><span class="No-Break"><span class="koboSpan" id="kobo.851.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.852.1">_imputed</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.853.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.854.1">
df_consistent.drop([cols for cols in df_consistent if cols.endswith('imputed')], axis=1, inplace=True)</span></pre> <p><span class="koboSpan" id="kobo.855.1">Next, we import all the necessary packages and modules to help split the data into train and test sets, evaluate the performance of the model, and create a machine </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">learning pipeline:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.857.1">
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from typing import List
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer</span></pre> <p><span class="koboSpan" id="kobo.858.1">Now, we </span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.859.1">extract the features for the model and split the</span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.860.1"> data into train and test sets, where 10% of the data is reserved </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">for testing:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.862.1">
feature_cols = [cols for cols in df_consistent if cols not in [target, id_col]]
X_train, X_test, y_train, y_test = train_test_split(df_consistent[feature_cols],
                                                    df_consistent[target].map({'Y':1, 'N':0}),
                                                    test_size=0.1,
                                                    random_state=1,
                                                    stratify=df_consistent[target].map({'Y':1, 'N':0}))
feature_cols
['gender',
 'married',
 'dependents',
 'education',
 'self_employed',
 'applicant_income',
 'coapplicant_income',
 'loan_amount',
 'loan_amount_term',
 'credit_history',
 'property_area']</span></pre> <p><span class="koboSpan" id="kobo.863.1">Next, we extract the categorical data and numerical data into separate lists so that we can use these to set the pipeline for each type </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">of data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.865.1">
cat_cols = [cols for cols in X_train if X_train[cols].nunique() &lt;= 15]
num_cols = [cols for cols in X_train if cols not in cat_cols]</span></pre> <p><span class="koboSpan" id="kobo.866.1">Now, we </span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.867.1">create a function that will return the pipeline for categorical data. </span><span class="koboSpan" id="kobo.867.2">First, the pipeline one-hot encodes the list of columns in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.868.1">ohe_cols</span></strong><span class="koboSpan" id="kobo.869.1"> variable, which includes </span><strong class="source-inline"><span class="koboSpan" id="kobo.870.1">property_area</span></strong><span class="koboSpan" id="kobo.871.1">. </span><span class="koboSpan" id="kobo.871.2">The pipeline then imputes </span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.872.1">the columns with missing data using the MICE implementation with random forest. </span><span class="koboSpan" id="kobo.872.2">The function will return the transformer so that when we pass the categorical data, while the transformer one-hot encodes the data and then imputes the missing data. </span><span class="koboSpan" id="kobo.872.3">The transformer will be run against the training data first so that it learns about the data and saves all the metadata for running the same steps with new data. </span><span class="koboSpan" id="kobo.872.4">The transformer can then be used to transform the </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">test data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.874.1">
def miss_forest_categorical_transformer():
    """Function to define categorical pipeline"""
    cat_transformer = Pipeline(
        steps=[
            ("one_hot_encoding",
             OneHotEncoder(variables=ohe_cols)
            ),
            ("miss_forest_classifier",
             IterativeImputer(
                 estimator=ExtraTreesClassifier(
                     n_estimators=100,
                     random_state=1,
                     bootstrap=True,
                     n_jobs=-1),
                max_iter=10,
                random_state=1,
                initial_strategy='median',
                add_indicator=True)
            )
        ]
    )
    return cat_transformer</span></pre> <p><span class="koboSpan" id="kobo.875.1">Next, we</span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.876.1"> create a function that returns the pipeline </span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.877.1">transformer to impute numerical missing data with the MICE implementation. </span><span class="koboSpan" id="kobo.877.2">Similarly to the categorical transformer, the numerical transformer will be trained against the training data and then applied to the test data to impute missing values in the train and </span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">test data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.879.1">
def miss_forest_numerical_transformer():
    """Function to define numerical pipeline"""
    num_transformer = Pipeline(
        steps=[
            ("miss_forest",
             IterativeImputer(
                estimator=ExtraTreesRegressor(n_estimators=100,
                                              random_state=1,
                                              bootstrap=True,
                                              n_jobs=-1),
                max_iter=10,
                random_state=1,
                initial_strategy='median',
                add_indicator=True)
            )
        ]
    )
    return num_transformer</span></pre> <p><span class="koboSpan" id="kobo.880.1">Then, we</span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.881.1"> initialize the categorical and numerical </span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.882.1">transformers, and then transform training and test data. </span><span class="koboSpan" id="kobo.882.2">The transformed categorical data is combined with numerical data before the numerical data is transformed. </span><span class="koboSpan" id="kobo.882.3">The output of this is imputed train and </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">test DataFrames:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.884.1">
cat_transformer = miss_forest_categorical_transformer()
num_transformer = miss_forest_numerical_transformer()
X_train_cat_imputed = cat_transformer.fit_transform(X_train[cat_cols])
X_test_cat_imputed = cat_transformer.transform(X_test[cat_cols])
X_train_cat_imputed_df = pd.DataFrame(data=X_train_cat_imputed,
                                      columns=cat_transformer.get_feature_names_out(),
                                      index=X_train.index)
X_test_cat_imputed_df = pd.DataFrame(data=X_test_cat_imputed,
                                     columns=cat_transformer.get_feature_names_out(),
                                     index=X_test.index)
X_train_cat_imputed_df = pd.concat([X_train_cat_imputed_df, X_train[num_cols]], axis=1)
X_test_cat_imputed_df = pd.concat([X_test_cat_imputed_df, X_test[num_cols]], axis=1)
X_train_imputed = num_transformer.fit_transform(X_train_cat_imputed_df)
X_test_imputed = num_transformer.transform(X_test_cat_imputed_df)
X_train_transformed = pd.DataFrame(data=X_train_imputed,
                                   columns=num_transformer.get_feature_names_out(),
                                   index=X_train.index)
X_test_transformed = pd.DataFrame(data=X_test_imputed,
                                  columns=num_transformer.get_feature_names_out(),
                                  index=X_test.index)</span></pre> <p><span class="koboSpan" id="kobo.885.1">Before passing the complete datasets to the machine learning model, we check if both the train and test labels have similar loan </span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">approval rates:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.887.1">
y_train.mean(), y_test.mean()
(0.6865942028985508, 0.6935483870967742)</span></pre> <p><span class="koboSpan" id="kobo.888.1">Because the classes are slightly imbalanced, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.889.1">class_weight='balanced'</span></strong><span class="koboSpan" id="kobo.890.1"> option since this option uses the values of </span><strong class="source-inline"><span class="koboSpan" id="kobo.891.1">y</span></strong><span class="koboSpan" id="kobo.892.1"> to automatically adjust weights inversely proportional to class frequencies in the input data when</span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.893.1"> training the algorithm. </span><span class="koboSpan" id="kobo.893.2">The objective of the problem</span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.894.1"> is to identify better than a human being who is likely to get a loan. </span><span class="koboSpan" id="kobo.894.2">Since the majority class is trained on people who received a loan, the model will be biased toward giving someone a loan. </span><span class="koboSpan" id="kobo.894.3">By using </span><strong class="source-inline"><span class="koboSpan" id="kobo.895.1">class_weight='balanced'</span></strong><span class="koboSpan" id="kobo.896.1">, the algorithm will put more emphasis on class label 0 since it’s a </span><span class="No-Break"><span class="koboSpan" id="kobo.897.1">minority class.</span></span></p>
<p><span class="koboSpan" id="kobo.898.1">We define grid search for the decision tree classifier to perform cross-validation, to ensure the model </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">is generalizable:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.900.1">
d_param_grid = {
    'max_features': [None, 'sqrt', 'log2'],
    'max_depth' : [4,5,6,7,8,10,20],
    'min_samples_leaf' : [1,3,5,8,10,12,15],
    'min_samples_split': [2,6,10,16,20,24,30],
    'criterion' : ['gini', 'entropy'],
    'random_state' : [1],
    'class_weight' : ['balanced']
}
d_clf = DecisionTreeClassifier()</span></pre> <p><span class="koboSpan" id="kobo.901.1">Next, we create a custom function that will take in training data, testing data, the classifier, and grid search parameters. </span><span class="koboSpan" id="kobo.901.2">The function performs 10K cross-validation to find</span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.902.1"> the best hyperparameters and trains the model</span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.903.1"> on the best parameters. </span><span class="koboSpan" id="kobo.903.2">The function then returns the model, predictions, training and test accuracies, and </span><span class="No-Break"><span class="koboSpan" id="kobo.904.1">ROC-AUC score:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.905.1">
def train_custom_classifier(X_train, y_train, X_test, y_test, clf, params):
    """Function to train the decision tree classifier and return some metrics"""
    d_clf_cv = GridSearchCV(estimator=d_clf, param_grid=d_param_grid, cv=10, scoring='roc_auc')
    d_clf_cv.fit(X_train_transformed, y_train)
    print("Decision tree optimised")
    d_best_params = d_clf_cv.best_params_
    print(f"Getting the best params which are {d_best_params}")
    model = DecisionTreeClassifier(**d_best_params)
    model.fit(X_train_transformed, y_train)
    training_predictions_prob = model.predict_proba(X_train_transformed)
    testing_predictions_prob = model.predict_proba(X_test_transformed)
    training_predictions = model.predict(X_train_transformed)
    testing_predictions = model.predict(X_test_transformed)
    training_roc_auc = roc_auc_score(y_train, training_predictions_prob[:,1])
    testing_roc_auc = roc_auc_score(y_test, testing_predictions_prob[:,1])
    training_acc = accuracy_score(y_train, training_predictions)
    testing_acc = accuracy_score(y_test, testing_predictions)
    print(f"Training roc is {training_roc_auc}, and testing roc is {testing_roc_auc} \n \
            training accuracy is {training_acc}, testing_acc as {testing_acc}")
    return model, testing_predictions, training_roc_auc, testing_roc_auc, training_acc, testing_acc</span></pre> <p><span class="koboSpan" id="kobo.906.1">Next, we run the custom classifier and calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.907.1">model performance:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.908.1">
model, test_predictions, train_roc, test_roc, train_acc, test_acc  = train_custom_classifier(
    X_train=X_train_transformed,
    y_train=y_train,
    X_test=X_test_transformed,
    y_test=y_test,
    clf=d_clf,
    params=d_param_grid
)
Decision tree optimised
Getting the best params which are {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 8, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 30, 'random_state': 1}
Training roc is 0.8763326063416048, and testing roc is 0.7858017135862914
             training accuracy is 0.8152173913043478, testing_acc as 0.7903225806451613</span></pre> <p><span class="koboSpan" id="kobo.909.1">The test</span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.910.1"> accuracy is just under 80%. </span><span class="koboSpan" id="kobo.910.2">Let’s see where the model</span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.911.1"> is performing poorly by observing the </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">confusion matrix:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.913.1">
cm = confusion_matrix(y_test, test_predictions, labels=model.classes_, normalize='true')
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot()</span></pre> <p><span class="koboSpan" id="kobo.914.1">This outputs the following </span><span class="No-Break"><span class="koboSpan" id="kobo.915.1">confusion matrix:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.916.1"><img alt="Figure 5.14 – Confusion matrix" src="image/B19297_05_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.917.1">Figure 5.14 – Confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.918.1">We have </span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.919.1">now applied machine learning pipelines</span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.920.1"> with MICE imputation to create a machine learning model. </span><span class="koboSpan" id="kobo.920.2">To demonstrate that MICE imputation is a better technique than the simple imputation technique, we will recreate the machine learning pipelines with simple imputation methods and evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.922.1">Once we have created the pipeline steps, we transform the train and test data before passing it to the decision tree classifier and custom classifier function to measure </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">model performance:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.924.1">
cat_transformer = Pipeline(
    steps=[
        ("one_hot_encoding",
         OneHotEncoder(variables=ohe_cols)
        )
    ]
)
impute_transformer = Pipeline(
    steps=[
        ("simple_imputer",
         SimpleImputer(strategy='median',
                       add_indicator=True)
        )
    ]
)
X_train_ohe = cat_transformer.fit_transform(X_train)
X_test_ohe = cat_transformer.transform(X_test)
X_train_imputed = impute_transformer.fit_transform(X_train_ohe)
X_test_imputed = impute_transformer.transform(X_test_ohe)
X_train_transformed = pd.DataFrame(data=X_train_imputed,
                                   columns=impute_transformer.get_feature_names_out(),
                                   index=X_train.index)
X_test_transformed = pd.DataFrame(data=X_test_imputed,
                                  columns=impute_transformer.get_feature_names_out(),
                                  index=X_test.index)</span></pre> <p><span class="koboSpan" id="kobo.925.1">Next, we </span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.926.1">run the custom classifier and extract </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">model performance:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.928.1">
model, test_predictions, train_roc, test_roc, train_acc, test_acc = train_custom_classifier(
    X_train=X_train_transformed,
    y_train=y_train,
    X_test=X_test_transformed,
    y_test=y_test,
    clf=d_clf,
    params=d_param_grid
)</span></pre> <p><span class="koboSpan" id="kobo.929.1">The test</span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.930.1"> accuracy has dropped under 67%, which is a 12% reduction, and the ROC-AUC has dropped by 6%. </span><span class="koboSpan" id="kobo.930.2">Next, we review the </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">confusion matrix:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.932.1">
cm = confusion_matrix(y_test, test_predictions, labels=model.classes_, normalize='true')
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot()</span></pre> <p><span class="koboSpan" id="kobo.933.1">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.935.1"><img alt="Figure 5.15 – Confusion matrix" src="image/B19297_05_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.936.1">Figure 5.15 – Confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.937.1">The accuracy </span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.938.1">of the true positive class has dropped from 88% to 67%, whereas the accuracy of the negative class has increased from 58% to 63%. </span><span class="koboSpan" id="kobo.938.2">By using basic imputation techniques, we can conclude that the model is </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.939.1">more likely to be biased and the model performance may be </span><span class="No-Break"><span class="koboSpan" id="kobo.940.1">less accurate.</span></span></p>
<p><span class="koboSpan" id="kobo.941.1">In data-centric machine learning, the goal is to improve on the data and tune it, rather than improving on the algorithm and tuning the model. </span><span class="koboSpan" id="kobo.941.2">But how can we identify whether a dataset contains poorly labeled data, missing features, or another data-related issue? </span><span class="koboSpan" id="kobo.941.3">We will cover how to identify if data is poorly labeled and apply techniques to improve on mislabeled data in </span><a href="B19297_06.xhtml#_idTextAnchor089"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.942.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.943.1">, </span><em class="italic"><span class="koboSpan" id="kobo.944.1">Techniques for Programmatic Labeling in </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.945.1">Machine Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.947.1">To find out if more features are needed or more data is needed, we utilize a technique called error analysis. </span><span class="koboSpan" id="kobo.947.2">In machine learning, error analysis is utilized to identify and diagnose erroneous predictions by focusing on the pockets of data where the model performed well and poorly. </span><span class="koboSpan" id="kobo.947.3">Although the overall performance of the model might be 79%, this performance may not be uniform across all pockets of the data, and these highs and lows could be due to inputs present in some pockets and absent in other pockets </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">of data.</span></span></p>
<p><span class="koboSpan" id="kobo.949.1">To identify data issues, we will start training the model with 10% of the data, and with each iteration add 10%. </span><span class="koboSpan" id="kobo.949.2">Then, we plot the training ROC and test the ROC concerning the increase in the size of the data. </span><span class="koboSpan" id="kobo.949.3">If the plot seems to converge and indicate an increase in the size of the data, this will lead to an improvement in the test ROC, at which point we will generate synthetic data to increase the data’s size. </span><span class="koboSpan" id="kobo.949.4">This technique will be covered in </span><a href="B19297_07.xhtml#_idTextAnchor111"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.950.1">Chapter 7</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.951.1">, Using Synthetic Data in Data-Centric </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.952.1">Machine Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.953.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.954.1">If the plot </span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.955.1">doesn’t seem to converge and indicates an</span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.956.1"> increase in data, it will have a minimal impact on improving test ROC. </span><span class="koboSpan" id="kobo.956.2">In this case, we can observe which data points the model performed poorly on, and may utilize feature engineering to generate new columns. </span><span class="koboSpan" id="kobo.956.3">Although feature engineering can be an iterative approach, for the scope of this chapter, we cover adding a feature </span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">or two.</span></span></p>
<p><span class="koboSpan" id="kobo.958.1">To run error analysis, first, we create data cutoff points from 0.1 to 1.0, where 0.1 means 10% of the training data and 1.0 means 100% of the </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">training data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.960.1">
data_cutoff_points = np.linspace(start=0.1, stop=1, num=10)
data_cutoff_points
array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. </span><span class="koboSpan" id="kobo.960.2">])</span></pre> <p><span class="koboSpan" id="kobo.961.1">Next, we create an empty list called </span><strong class="source-inline"><span class="koboSpan" id="kobo.962.1">scores</span></strong><span class="koboSpan" id="kobo.963.1"> and run data preprocessing, model training, and evaluation with each cutoff of data. </span><span class="koboSpan" id="kobo.963.2">If the cutoff is &lt; 1.0, we subset the training data; otherwise, we pass all the data for training. </span><span class="koboSpan" id="kobo.963.3">At the end of each iteration, we save the cutoff, train, and test evaluation metrics in </span><strong class="source-inline"><span class="koboSpan" id="kobo.964.1">scores</span></strong><span class="koboSpan" id="kobo.965.1"> by appending the metrics to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.966.1">scores</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.967.1"> list:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.968.1">
scores = []
for cutoff in data_cutoff_points:
    if cutoff &lt; 1.0:
        X_train_subset, X_train_rem, y_train_subset, y_train_rem = train_test_split(X_train,
                     y_train,
                          random_state=1,
                             train_size=cutoff,
                        stratify=y_train)
    else:
        X_train_subset = X_train.copy()
        y_train_subset = y_train.copy()
    print(f"Model will be trained on {X_train_subset.shape[0]} rows out of {X_train.shape[0]}")
    cat_transformer = miss_forest_categorical_transformer()
    num_transformer = miss_forest_numerical_transformer()
    X_train_cat_imputed = cat_transformer.fit_transform(X_train_subset[cat_cols])
    X_test_cat_imputed = cat_transformer.transform(X_test[cat_cols])
    X_train_cat_imputed_df = pd.DataFrame(data=X_train_cat_imputed,
                                          columns=cat_transformer.get_feature_names_out(),
                                          index=X_train_subset.index)
    X_test_cat_imputed_df = pd.DataFrame(data=X_test_cat_imputed,
                                         columns=cat_transformer.get_feature_names_out(),
                                         index=X_test.index)
    X_train_cat_imputed_df = pd.concat([X_train_cat_imputed_df, X_train_subset[num_cols]], axis=1)
    X_test_cat_imputed_df = pd.concat([X_test_cat_imputed_df, X_test[num_cols]], axis=1)
    X_train_imputed = num_transformer.fit_transform(X_train_cat_imputed_df)
    X_test_imputed = num_transformer.transform(X_test_cat_imputed_df)
    X_train_transformed = pd.DataFrame(data=X_train_imputed,
                                       columns=num_transformer.get_feature_names_out(),
                                       index=X_train_subset.index)
    X_test_transformed = pd.DataFrame(data=X_test_imputed,
                                      columns=num_transformer.get_feature_names_out(),
                                      index=X_test.index)
    model, test_predictions, train_roc, test_roc, train_acc, test_acc = train_custom_classifier(
        X_train=X_train_transformed,
        y_train=y_train_subset,
        X_test=X_test_transformed,
        y_test=y_test,
        clf=d_clf,
        params=d_param_grid)
    scores.append((cutoff, train_roc, test_roc, train_acc, test_acc))
Model will be trained on 55 rows out of 552
Training roc is 0.9094427244582044, and testing roc is 0.5917992656058751
             training accuracy is 0.7454545454545455, testing_acc as 0.5806451612903226
Model will be trained on 110 rows out of 552
Training roc is 0.901702786377709, and testing roc is 0.7552019583843328
             training accuracy is 0.7272727272727273, testing_acc as 0.6290322580645161
Model will be trained on 165 rows out of 552
Training roc is 0.8986555479918311, and testing roc is 0.7099143206854346
             training accuracy is 0.7696969696969697, testing_acc as 0.5967741935483871
Model will be trained on 220 rows out of 552
Training roc is 0.8207601497264613, and testing roc is 0.8084455324357405
             training accuracy is 0.8318181818181818, testing_acc as 0.8064516129032258
Model will be trained on 276 rows out of 552
Training roc is 0.8728942407103326, and testing roc is 0.7906976744186047
             training accuracy is 0.822463768115942, testing_acc as 0.7419354838709677
Model will be trained on 331 rows out of 552
Training roc is 0.9344501863774991, and testing roc is 0.7753977968176254
             training accuracy is 0.8368580060422961, testing_acc as 0.7419354838709677
Model will be trained on 386 rows out of 552
Training roc is 0.8977545610478715, and testing roc is 0.7184822521419829
             training accuracy is 0.7849740932642487, testing_acc as 0.6612903225806451
Model will be trained on 441 rows out of 552
Training roc is 0.8954656335198737, and testing roc is 0.7429620563035496
             training accuracy is 0.81859410430839, testing_acc as 0.7258064516129032
Model will be trained on 496 rows out of 552
Training roc is 0.9102355500898685, and testing roc is 0.7441860465116278
             training accuracy is 0.8266129032258065, testing_acc as 0.7258064516129032
Model will be trained on 552 rows out of 552
Training roc is 0.8763326063416048, and testing roc is 0.7858017135862914
             training accuracy is 0.8152173913043478, testing_acc as 0.7903225806451613</span></pre> <p><span class="koboSpan" id="kobo.969.1">Next, we </span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.970.1">create a DataFrame from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.971.1">scores</span></strong><span class="koboSpan" id="kobo.972.1"> list and pass the relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">column names:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.974.1">
df = pd.DataFrame(data=scores, columns=['data_size', 'training_roc', 'testing_roc', "training_acc", "testing_acc"])</span></pre> <p><span class="koboSpan" id="kobo.975.1">Then, we </span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.976.1">plot the train and test ROC against </span><span class="No-Break"><span class="koboSpan" id="kobo.977.1">each cutoff:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.978.1">
plt.plot(df.data_size, df.training_roc, label='training_roc')
plt.plot(df.data_size, df.testing_roc, label='testing_roc')
plt.xlabel("Data Size")
plt.ylabel("ROC")
plt.title("Error Analysis")
plt.legend()</span></pre> <p><span class="koboSpan" id="kobo.979.1">This will output the </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">following plot:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.981.1"><img alt="Figure 5.15 – Error analysis train and test ROC" src="image/B19297_05_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.982.1">Figure 5.15 – Error analysis train and test ROC</span></p>
<p><span class="koboSpan" id="kobo.983.1">Next, </span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.984.1">plot the train and test accuracy against </span><a id="_idIndexMarker287"/><span class="No-Break"><span class="koboSpan" id="kobo.985.1">each cutoff:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.986.1">
plt.plot(df.data_size, df.training_acc, label='training_acc')
plt.plot(df.data_size, df.testing_acc, label='testing_acc')
plt.xlabel("Data Size")
plt.ylabel("Accuracy")
plt.title("Error Analysis")
plt.legend()</span></pre> <p><span class="koboSpan" id="kobo.987.1">This will output the </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">following plot:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.989.1"><img alt="Figure 5.17 – Error analysis train and test accuracy" src="image/B19297_05_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.990.1">Figure 5.17 – Error analysis train and test accuracy</span></p>
<p><span class="koboSpan" id="kobo.991.1">Both the</span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.992.1"> test ROC and test accuracy seem to show signs </span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.993.1">of convergence with the train ROC and train accuracy, which indicates that model performance may be boosted if more data points were made available. </span><span class="koboSpan" id="kobo.993.2">This is why we will generate synthetic data (data that mimics the real data) in the next chapter and retrain the model with added data to get better </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.995.1">As we learned in the previous chapters, one of the principles of data-centric machine learning is keeping humans in the loop. </span><span class="koboSpan" id="kobo.995.2">Let’s imagine we spoke to the domain experts and they mentioned that one of the key determinants for someone getting a loan is the income-to-debt ratio – that is, the total income divided by the loan amount. </span><span class="koboSpan" id="kobo.995.3">This determines if someone will be able to pay back the loan or not. </span><span class="koboSpan" id="kobo.995.4">An application with a lower income-to-loan ratio is more likely to be rejected. </span><span class="koboSpan" id="kobo.995.5">In the dataset, there are two income variables – applicant income and co-applicant income. </span><span class="koboSpan" id="kobo.995.6">Also, the loan amount is represented in thousand figures – that is, a loan amount in the data loan amount of 66 represents 66,000. </span><span class="koboSpan" id="kobo.995.7">To create this ratio, we will multiply the loan amount by 1,000 and then combine the income of the applicant and co-applicant. </span><span class="koboSpan" id="kobo.995.8">Once we’ve done this, we will divide the combined income by the loan amount to get the income-to-loan ratio. </span><span class="koboSpan" id="kobo.995.9">The domain experts also mentioned that </span><strong class="bold"><span class="koboSpan" id="kobo.996.1">equated monthly installments</span></strong><span class="koboSpan" id="kobo.997.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.998.1">EMIs</span></strong><span class="koboSpan" id="kobo.999.1">) can </span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.1000.1">also determine a candidate’s capability to pay the loan. </span><span class="koboSpan" id="kobo.1000.2">The lower the EMI, the more likely a loan will be accepted, whereas the higher the EMI, the more likely a loan will be rejected. </span><span class="koboSpan" id="kobo.1000.3">To calculate this without the interest rate, we can use the loan term and loan amount to get an approximate EMI amount for </span><span class="No-Break"><span class="koboSpan" id="kobo.1001.1">each month.</span></span></p>
<p><span class="koboSpan" id="kobo.1002.1">For the </span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.1003.1">income-to-loan ratio, we will create a custom transformer for multiplying the loan amount by 1,000 so that we can use it in </span><span class="No-Break"><span class="koboSpan" id="kobo.1004.1">the pipeline.</span></span></p>
<p><span class="koboSpan" id="kobo.1005.1">This </span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.1006.1">transformer is a Python class that we can use to overload the fit and transform functions required by the pipeline. </span><span class="koboSpan" id="kobo.1006.2">This class will inherit from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1007.1">BaseEstimator</span></strong><span class="koboSpan" id="kobo.1008.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1009.1">TransformerMixin</span></strong><span class="koboSpan" id="kobo.1010.1"> classes, both of which can be found in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1011.1">sklearn.base</span></strong><span class="koboSpan" id="kobo.1012.1"> module. </span><span class="koboSpan" id="kobo.1012.2">The class will be used to implement the fit and transform methods. </span><span class="koboSpan" id="kobo.1012.3">These methods should contain </span><strong class="source-inline"><span class="koboSpan" id="kobo.1013.1">X</span></strong><span class="koboSpan" id="kobo.1014.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1015.1">y</span></strong><span class="koboSpan" id="kobo.1016.1"> parameters, and the transform method should return a pandas DataFrame to ensure compatibility with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">scikit-learn pipeline.</span></span></p>
<p><span class="koboSpan" id="kobo.1018.1">To create a full income column, we leverage the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1019.1">feature_engine</span></strong><span class="koboSpan" id="kobo.1020.1"> library since it is already compatible with the scikit-learn pipeline and has methods to apply mathematical operations relative to other variables. </span><span class="koboSpan" id="kobo.1020.2">First, we sum the income variables. </span><span class="koboSpan" id="kobo.1020.3">The output of that transformation will be divided by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1021.1">loan_amount</span></strong><span class="koboSpan" id="kobo.1022.1"> variable to create the </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">income-to-loan ratio.</span></span></p>
<p><span class="koboSpan" id="kobo.1024.1">To create the EMI, we leverage the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1025.1">feature_engine</span></strong><span class="koboSpan" id="kobo.1026.1"> library and divide </span><strong class="source-inline"><span class="koboSpan" id="kobo.1027.1">loan_amount</span></strong><span class="koboSpan" id="kobo.1028.1"> with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1029.1">loan_amount_term</span></strong><span class="koboSpan" id="kobo.1030.1">. </span><span class="koboSpan" id="kobo.1030.2">Once we have created these features, we remove the two income variables since we already created a combination of the two. </span><span class="koboSpan" id="kobo.1030.3">For this step, we use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1031.1">DropFeatures</span></strong><span class="koboSpan" id="kobo.1032.1"> class from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1033.1">feature_engine</span></strong><span class="koboSpan" id="kobo.1034.1"> library. </span><span class="koboSpan" id="kobo.1034.2">All these feature engineering steps will be combined in a new pipeline called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1035.1">feature-transformer</span></strong><span class="koboSpan" id="kobo.1036.1"> and will be applied </span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">post-data imputation.</span></span></p>
<p><span class="koboSpan" id="kobo.1038.1">We believe that by adding these extra features, the model performance of the decision tree algorithm will improve. </span><span class="koboSpan" id="kobo.1038.2">Let’s run the algorithm post-feature engineering and evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">the results.</span></span></p>
<p><span class="koboSpan" id="kobo.1040.1">First, we create custom variables that will take in a list of variables for the feature </span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">engineering steps:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1042.1">
income_variables = ['applicant_income', 'coapplicant_income']
loan_variable = ['loan_amount']
loan_term_variable = ['loan_amount_term']</span></pre> <p><span class="koboSpan" id="kobo.1043.1">Next, we import relevant packages from </span><strong class="source-inline"><span class="koboSpan" id="kobo.1044.1">feature_engine</span></strong><span class="koboSpan" id="kobo.1045.1"> to perform the feature engineering steps and import the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1046.1">BaseEstimator</span></strong><span class="koboSpan" id="kobo.1047.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1048.1">TransformerMixin</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1049.1"> classes:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1050.1">
from feature_engine.creation.math_features import MathFeatures
from feature_engine.creation.relative_features import RelativeFeatures
from sklearn.base import BaseEstimator, TransformerMixin
from feature_engine.selection import DropFeatures</span></pre> <p><span class="koboSpan" id="kobo.1051.1">Then, we </span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.1052.1">create a custom transformer that will take in </span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.1053.1">variable names and a value that will be multiplied by each variable. </span><span class="koboSpan" id="kobo.1053.2">By default, each variable will be multiplied </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">by 1:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1055.1">
class MultiplyColumns(BaseEstimator, TransformerMixin):
    """Custom pipeline class to multiply columns passed in a DataFrame with a value"""
    def __init__(self, multiply_by=1, variables=None):
        self.multiply_by = multiply_by
        self.variables = variables
    def fit(self, X, y=None):
        return self
    def transform(self, X, y=None):
        if self.variables:
            X[self.variables] = X[self.variables] * self.multiply_by
        return X</span></pre> <p><span class="koboSpan" id="kobo.1056.1">Next, we </span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.1057.1">call the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1058.1">missForest</span></strong><span class="koboSpan" id="kobo.1059.1"> categorical and numerical transformers we created previously. </span><span class="koboSpan" id="kobo.1059.2">Once we’ve done this, we create a </span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.1060.1">feature transformer pipeline that multiplies </span><strong class="source-inline"><span class="koboSpan" id="kobo.1061.1">loan_amount</span></strong><span class="koboSpan" id="kobo.1062.1"> by 1,000 by leveraging the custom transformer we created previously. </span><span class="koboSpan" id="kobo.1062.2">The new pipeline then adds income variables to create one income variable, the income-to-loan ratio, and the EMI features. </span><span class="koboSpan" id="kobo.1062.3">Finally, the pipeline drops the two income variables since the new income variable will be created. </span><span class="koboSpan" id="kobo.1062.4">By using the transformer pipelines, the train and test data will be transformed, and new features will be created. </span><span class="koboSpan" id="kobo.1062.5">The output of this step will be fully transformed into train and test data so that it can be passed to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1063.1">custom classifier:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1064.1">
cat_transformer = miss_forest_categorical_transformer()
num_transformer = miss_forest_numerical_transformer()
feature_transformer = Pipeline(
    steps=[
        ("multiply_by_thousand",
         MultiplyColumns(
             multiply_by=1000,
             variables=loan_variable
         )
        ),
        ("add_columns",
         MathFeatures(
             variables=income_variables,
             func='sum'
         )
        ),
        ("income_to_loan_ratio",
         RelativeFeatures(variables=[f"sum_{income_variables[0]}_{income_variables[1]}"],
                          reference=loan_variable,
                          func=["div"]
                         )
        ),
        ("emi",
         RelativeFeatures(variables=loan_variable,
                          reference=loan_term_variable,
                          func=["div"])
        ),
        ("drop_features",
         DropFeatures(features_to_drop=income_variables
          ))
    ]
)</span></pre> <p><span class="koboSpan" id="kobo.1065.1">Next, we</span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.1066.1"> create the categorical transformers </span><span class="No-Break"><span class="koboSpan" id="kobo.1067.1">for imputation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1068.1">
X_train_cat_imputed = cat_transformer.fit_transform(X_train[cat_cols])
X_test_cat_imputed = cat_transformer.transform(X_test[cat_cols])
X_train_cat_imputed_df = pd.DataFrame(data=X_train_cat_imputed,
                                      columns=cat_transformer.get_feature_names_out(),
                                      index=X_train.index)
X_test_cat_imputed_df = pd.DataFrame(data=X_test_cat_imputed,
                                     columns=cat_transformer.get_feature_names_out(),
                                     index=X_test.index)
X_train_cat_imputed_df = pd.concat([X_train_cat_imputed_df, X_train[num_cols]], axis=1)
X_test_cat_imputed_df = pd.concat([X_test_cat_imputed_df, X_test[num_cols]], axis=1)</span></pre> <p><span class="koboSpan" id="kobo.1069.1">Then, we </span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.1070.1">add the numerical imputation and complete </span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.1071.1">the </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">imputation steps:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1073.1">
X_train_imputed = num_transformer.fit_transform(X_train_cat_imputed_df)
X_test_imputed = num_transformer.transform(X_test_cat_imputed_df)
X_train_imputed_df = pd.DataFrame(data=X_train_imputed,
                                   columns=num_transformer.get_feature_names_out(),
                                   index=X_train.index)
X_test_imputed_df = pd.DataFrame(data=X_test_imputed,
                                  columns=num_transformer.get_feature_names_out(),
                                  index=X_test.index)</span></pre> <p><span class="koboSpan" id="kobo.1074.1">Next, we</span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.1075.1"> transform the imputed data using the feature imputation pipeline we </span><span class="No-Break"><span class="koboSpan" id="kobo.1076.1">created previously:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1077.1">
X_train_transformed = feature_transformer.fit_transform(X_train_imputed_df)
X_test_transformed = feature_transformer.transform(X_test_imputed_df)</span></pre> <p><span class="koboSpan" id="kobo.1078.1">At this point, we </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.1079.1">call the custom classifier function to evaluate model performance with added feature </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">engineering steps:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1081.1">
model, test_predictions, train_roc, test_roc, train_acc, test_acc = train_custom_classifier(
    X_train=X_train_transformed,
    y_train=y_train,
    X_test=X_test_transformed,
    y_test=y_test,
    clf=d_clf,
    params=d_param_grid)
Training roc is 0.8465996614150411, and testing roc is 0.8188494492044063
             training accuracy is 0.8206521739130435, testing_acc as 0.8225806451612904</span></pre> <p><span class="koboSpan" id="kobo.1082.1">Next, we </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.1083.1">call the </span><span class="No-Break"><span class="koboSpan" id="kobo.1084.1">confusion matrix:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1085.1">
cm = confusion_matrix(y_test, test_predictions, labels=model.classes_, normalize='true')
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)
disp.plot()</span></pre> <p><span class="koboSpan" id="kobo.1086.1">The </span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.1087.1">resulting confusion matrix is </span><span class="No-Break"><span class="koboSpan" id="kobo.1088.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.1089.1"><img alt="Figure 5.18 – Confusion matrix when using custom feature engineering" src="image/B19297_05_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1090.1">Figure 5.18 – Confusion matrix when using custom feature engineering</span></p>
<p><span class="koboSpan" id="kobo.1091.1">Our test accuracy has increased from 79% to 82% and the ROC has been boosted from 78.5% to 81.8%. </span><span class="koboSpan" id="kobo.1091.2">The preceding confusion matrix shows that the accuracy of the positive class has been boosted from 88% to 91%, while the accuracy of the negative class has been boosted from 58% </span><span class="No-Break"><span class="koboSpan" id="kobo.1092.1">to 63%.</span></span></p>
<p><span class="koboSpan" id="kobo.1093.1">With this, we have demonstrated that by using a data-centric approach, we can iterate over the data rather than iterate over multiple algorithms, and manage to improve mod</span><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.1094.1">el performance. </span><span class="koboSpan" id="kobo.1094.2">We will explore how to add some synthetic data and boost model performance even further in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1095.1">next chapter.</span></span></p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.1096.1">Ensuring that the data is valid</span></h1>
<p><span class="koboSpan" id="kobo.1097.1">So far, we </span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.1098.1">have ensured that our data is consistent, unique, and complete. </span><span class="koboSpan" id="kobo.1098.2">But do we know if the data we have is valid? </span><span class="koboSpan" id="kobo.1098.3">Do the data labels conform to the rules? </span><span class="koboSpan" id="kobo.1098.4">For example, what if the property area in the dataset didn’t conform to the rules and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1099.1">semi_urban</span></strong><span class="koboSpan" id="kobo.1100.1"> is invalid? </span><span class="koboSpan" id="kobo.1100.2">What if one or a couple of annotators believed some suburbs are neither urban nor rural, and they violated the rules and entered </span><strong class="source-inline"><span class="koboSpan" id="kobo.1101.1">semi_urban</span></strong><span class="koboSpan" id="kobo.1102.1">? </span><span class="koboSpan" id="kobo.1102.2">To measure validity, we may need to look at business rules and check the percentage of data that conforms to these business rules. </span><span class="koboSpan" id="kobo.1102.3">Let’s assume that </span><strong class="source-inline"><span class="koboSpan" id="kobo.1103.1">semi_urban</span></strong><span class="koboSpan" id="kobo.1104.1"> is an invalid value. </span><span class="koboSpan" id="kobo.1104.2">In Python, we could check the percentage of invalid labels and then reach out to annotators to correct the data. </span><span class="koboSpan" id="kobo.1104.3">We could also achieve this by using the data that was used to generate the label. </span><span class="koboSpan" id="kobo.1104.4">If we had the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1105.1">suburb_name</span></strong><span class="koboSpan" id="kobo.1106.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1107.1">property_area</span></strong><span class="koboSpan" id="kobo.1108.1"> data mapping, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1109.1">suburb_name</span></strong><span class="koboSpan" id="kobo.1110.1"> was available in the dataset, then we could leverage the mapping and catch invalid values, as well as encoding labels programmatically. </span><span class="koboSpan" id="kobo.1110.2">Building business rules in the system so that upcoming data is automatically encoded is referred to as programmatic labeling. </span><span class="koboSpan" id="kobo.1110.3">We will dive into programmatic labeling in the upcoming chapters, where we explore techniques to make data consistent and valid at the time of data capture, so that when the data comes in, it’s already pristine and some of the data cleaning process will </span><span class="No-Break"><span class="koboSpan" id="kobo.1111.1">be redundant.</span></span></p>
<p><span class="koboSpan" id="kobo.1112.1">First, we make a fake dataset that consists of 10 rows and write a business rule. </span><span class="koboSpan" id="kobo.1112.2">It will contain an </span><strong class="source-inline"><span class="koboSpan" id="kobo.1113.1">id</span></strong><span class="koboSpan" id="kobo.1114.1"> column with values from 1 to 10, a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1115.1">population</span></strong><span class="koboSpan" id="kobo.1116.1"> column with 10 random values between 1,000 and 100,000, and a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1117.1">property_area</span></strong><span class="koboSpan" id="kobo.1118.1"> column with four values set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1119.1">urban</span></strong><span class="koboSpan" id="kobo.1120.1">, five values set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1121.1">semi_urban</span></strong><span class="koboSpan" id="kobo.1122.1">, and one value set </span><span class="No-Break"><span class="koboSpan" id="kobo.1123.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1124.1">rural</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1125.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1126.1">
np.random.seed(1)
data = {
    "id": np.linspace(start=1, stop=10, num=10, dtype=int),
    "population" : np.random.randint(low=1000, high=100000, size=10),
    "property_area": ["urban"]*4 + ["semi_urban"]*5 + ["rural"]*1
}
df = pd.DataFrame(data=data)</span></pre> <p><span class="koboSpan" id="kobo.1127.1">Next, we print the first </span><span class="No-Break"><span class="koboSpan" id="kobo.1128.1">five rows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1129.1">
df.head()
   id  population property_area
0   1       99539         urban
1   2       78708         urban
2   3        6192         urban
3   4       99047         urban
4   5       51057    semi_urban</span></pre> <p><span class="koboSpan" id="kobo.1130.1">Imagine </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.1131.1">that the business rule says a suburb or </span><strong class="source-inline"><span class="koboSpan" id="kobo.1132.1">property_area</span></strong><span class="koboSpan" id="kobo.1133.1"> is classified as urban when the population is more than 20,000; otherwise, it’s classified as rural. </span><span class="koboSpan" id="kobo.1133.2">In this case, the validation rule should check that </span><strong class="source-inline"><span class="koboSpan" id="kobo.1134.1">property_area</span></strong><span class="koboSpan" id="kobo.1135.1"> only contains </span><strong class="source-inline"><span class="koboSpan" id="kobo.1136.1">urban</span></strong><span class="koboSpan" id="kobo.1137.1"> or </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1138.1">rural</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1139.1"> values.</span></span></p>
<p><span class="koboSpan" id="kobo.1140.1">A simple way to check this in Python is to use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1141.1">value_counts()</span></strong><span class="koboSpan" id="kobo.1142.1"> method alongside the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1143.1">normalize=True</span></strong><span class="koboSpan" id="kobo.1144.1"> parameter. </span><span class="koboSpan" id="kobo.1144.2">The output of this will show that 50% of the data </span><span class="No-Break"><span class="koboSpan" id="kobo.1145.1">is invalid:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1146.1">
df.property_area.value_counts(normalize=True)
semi_urban    0.5
urban         0.4
rural         0.1
Name: property_area, dtype: float64</span></pre> <p><span class="koboSpan" id="kobo.1147.1">Next, we can run the check against each row and flag when the value is in the expected</span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.1148.1"> list of values, as well as when the value is not in the expected set </span><span class="No-Break"><span class="koboSpan" id="kobo.1149.1">of values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1150.1">
df.property_area.isin(['rural', 'urban']) == False
0    False
1    False
2    False
3    False
4     True
5     True
6     True
7     True
8     True
9    False
Name: property_area, dtype: bool</span></pre> <p><span class="koboSpan" id="kobo.1151.1">Now, we sum the rows where the data validation rule was breached and divide the number of invalid rows by the total rows to provide a metric – that is, the percentage of </span><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">invalid labels:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1153.1">
sum(df.property_area.isin(['rural', 'urban']) == False) / df.shape[0]
0.5</span></pre> <p><span class="koboSpan" id="kobo.1154.1">Invalid data must be communicated to the source data providers and must be cleaned; otherwise, this data can creep in and the machine learning model will learn about these invalid labels. </span><span class="koboSpan" id="kobo.1154.2">The model's learning capability significantly improves when trained with valid data, which provides stronger label signals, as opposed to invalid data that weakens these signals due to reduced exposure to valid </span><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">label points.</span></span></p>
<h1 id="_idParaDest-80"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.1156.1">Ensuring that the data is accurate</span></h1>
<p><span class="koboSpan" id="kobo.1157.1">Even</span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.1158.1"> though the data is valid, it may not be accurate. </span><span class="koboSpan" id="kobo.1158.2">Data accuracy measures the percentage of data that matches real-world data or verifiable sources. </span><span class="koboSpan" id="kobo.1158.3">Considering the preceding example of the property area, to measure data accuracy, we may have to look up a reliable published dataset and check the population of the area and the type of the area. </span><span class="koboSpan" id="kobo.1158.4">Let’s assume that the population matches the verifiable data source, but the area type source is unavailable. </span><span class="koboSpan" id="kobo.1158.5">Using the rule of what defines a rural area and what defines an urban area, we can measure </span><span class="No-Break"><span class="koboSpan" id="kobo.1159.1">data accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.1160.1">Using this business rule, we will create a new label called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1161.1">true_property_area</span></strong><span class="koboSpan" id="kobo.1162.1"> that takes </span><strong class="source-inline"><span class="koboSpan" id="kobo.1163.1">rural</span></strong><span class="koboSpan" id="kobo.1164.1"> as a value when the population is 20,000 or below; otherwise, takes </span><strong class="source-inline"><span class="koboSpan" id="kobo.1165.1">urban</span></strong><span class="koboSpan" id="kobo.1166.1"> as </span><span class="No-Break"><span class="koboSpan" id="kobo.1167.1">a value:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1168.1">
df['true_property_area'] = df.population.apply(lambda value: 'rural' if value &lt;= 20000 else 'urban')</span></pre> <p><span class="koboSpan" id="kobo.1169.1">Next, we print the rows of the dataset to see if there are any mismatches between </span><strong class="source-inline"><span class="koboSpan" id="kobo.1170.1">property_area</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1171.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1172.1">true_property_area</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1173.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1174.1">
df[['true_property_area', 'property_area', 'population']]
  true_property_area property_area  population
0              urban         urban       99539
1              urban         urban       78708
2              rural         urban        6192
3              urban         urban       99047
4              urban    semi_urban       51057
5              urban    semi_urban       74349
6              urban    semi_urban       22440
7              urban    semi_urban       99448
8              urban    semi_urban       21609
9              urban         rural       50100</span></pre> <p><span class="koboSpan" id="kobo.1175.1">Then, we sum the rows where </span><strong class="source-inline"><span class="koboSpan" id="kobo.1176.1">property_area</span></strong><span class="koboSpan" id="kobo.1177.1"> values match with the true values and divide this by the total number of rows to calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1">data accuracy:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1179.1">
sum(df.property_area == df.true_property_area) / df.shape[0]
0.3</span></pre> <p><span class="koboSpan" id="kobo.1180.1">Instead of creating a function to calculate the accuracy, we can leverage </span><strong class="source-inline"><span class="koboSpan" id="kobo.1181.1">accuracy_score</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1182.1">from scikit-learn:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1183.1">
accuracy_score(y_pred=df.property_area, y_true=df.true_property_area)
0.3</span></pre> <p><span class="koboSpan" id="kobo.1184.1">As we</span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.1185.1"> can see, both methods return the same score. </span><span class="koboSpan" id="kobo.1185.2">If inaccurate data enters the system, the model may learn inaccurately about semi-urban and rural areas, and at the time of inference, produce </span><span class="No-Break"><span class="koboSpan" id="kobo.1186.1">undesirable outcomes.</span></span></p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.1187.1">Ensuring that the data is fresh</span></h1>
<p><span class="koboSpan" id="kobo.1188.1">Data freshness </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.1189.1">is another important aspect of measuring data quality that has an impact on the quality and robustness of machine learning applications. </span><span class="koboSpan" id="kobo.1189.2">Let’s imagine that we have a machine learning application that’s been trained on 2019 and 2020 customer behavior and utilized to predict hotel room bookings up to April 2021. </span><span class="koboSpan" id="kobo.1189.3">Maybe January and February numbers were quite accurate, but when March and April hit, accuracy dropped. </span><span class="koboSpan" id="kobo.1189.4">This might have been due to COVID-19, something that was unseen by the data, and its effects were not captured. </span><span class="koboSpan" id="kobo.1189.5">In machine learning, this is called data drift. </span><span class="koboSpan" id="kobo.1189.6">This is happening here; the data distribution in March and April was quite different from the data distribution in 2019 and 2020. </span><span class="koboSpan" id="kobo.1189.7">By ensuring that the data is fresh and up to date, we can train the model more regularly or as soon as data drift </span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">is detected.</span></span></p>
<p><span class="koboSpan" id="kobo.1191.1">To measure data drift, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1192.1">alibi</span></strong><span class="koboSpan" id="kobo.1193.1"> Python package. </span><span class="koboSpan" id="kobo.1193.2">However, there are more extensive Python packages that can help with this job. </span><span class="koboSpan" id="kobo.1193.3">We recommend Evidently AI (</span><a href="https://www.evidentlyai.com/"><span class="koboSpan" id="kobo.1194.1">https://www.evidentlyai.com/</span></a><span class="koboSpan" id="kobo.1195.1">), a data and machine learning model monitoring toolkit, or WhyLogs (</span><a href="https://whylabs.ai/whylogs"><span class="koboSpan" id="kobo.1196.1">https://whylabs.ai/whylogs</span></a><span class="koboSpan" id="kobo.1197.1">), an open source initiative by WhyLabs, to monitor model degradation and </span><span class="No-Break"><span class="koboSpan" id="kobo.1198.1">data drift.</span></span></p>
<p><span class="koboSpan" id="kobo.1199.1">Let’s imagine that, on average, model accuracy starts degrading when the model is trained on data that is more than 5 days old and the model does poorly and starts costing the business when data is more than 10 days old. </span><span class="koboSpan" id="kobo.1199.2">We want to be able to alert and capture when this happens. </span><span class="koboSpan" id="kobo.1199.3">To demonstrate this scenario, we will create a sample dataset with a date column and define error and warning thresholds – that is, we print a warning if the data is 5 days old and block the application if the data is more than 10 days old. </span><span class="koboSpan" id="kobo.1199.4">In practice, it is recommended to train the model on the most</span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.1200.1"> recent data available. </span><span class="koboSpan" id="kobo.1200.2">Following a data-centric approach, we must encourage practitioners to have thresholds and </span><strong class="bold"><span class="koboSpan" id="kobo.1201.1">service-level agreements</span></strong><span class="koboSpan" id="kobo.1202.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1203.1">SLAs</span></strong><span class="koboSpan" id="kobo.1204.1">) defined </span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.1205.1">with the providers of data so that they have mechanisms in place to request up-to-date data, penalize when SLAs are breached, and maybe reward when SLAs are met (encourage the importance of maintaining </span><span class="No-Break"><span class="koboSpan" id="kobo.1206.1">good-quality data).</span></span></p>
<p><span class="koboSpan" id="kobo.1207.1">Now we will generate 100 sample data points and demonstrate how to identify if data is stale using a </span><span class="No-Break"><span class="koboSpan" id="kobo.1208.1">date variable.</span></span></p>
<p><span class="koboSpan" id="kobo.1209.1">We utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1210.1">alibi</span></strong><span class="koboSpan" id="kobo.1211.1"> package to detect drift in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1212.1">loan_prediction</span></strong><span class="koboSpan" id="kobo.1213.1"> dataset. </span><span class="koboSpan" id="kobo.1213.2">We will demonstrate the pitfalls of not detecting and acting on data drift by comparing accuracy before and </span><span class="No-Break"><span class="koboSpan" id="kobo.1214.1">after drift.</span></span></p>
<p><span class="koboSpan" id="kobo.1215.1">First, we import the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1216.1">datetime</span></strong><span class="koboSpan" id="kobo.1217.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1218.1">warning</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1219.1"> packages:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1220.1">
from datetime import datetime, timedelta
import warnings</span></pre> <p><span class="koboSpan" id="kobo.1221.1">Next, we generate a base date – let’s say the date when we run the code – and from the base date, generate 100 more dates in the past by subtracting one day at </span><span class="No-Break"><span class="koboSpan" id="kobo.1222.1">a time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1223.1">
numdays = 100
base = datetime.today()
date_list = [base - timedelta(days=day) for day in range(numdays)] # Subracting values from 1 to 100 from todays date</span></pre> <p><span class="koboSpan" id="kobo.1224.1">Then, we print the first 10 dates in the order these dates were generated, with the most recent being the </span><span class="No-Break"><span class="koboSpan" id="kobo.1225.1">first date:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1226.1">
[date.date().strftime('%Y-%m-%d') for date in date_list[0:10]]
['2023-02-04',
 '2023-02-03',
 '2023-02-02',
 '2023-02-01',
 '2023-01-31',
 '2023-01-30',
 '2023-01-29',
 '2023-01-28',
 '2023-01-27',
 '2023-01-26']</span></pre> <p><span class="koboSpan" id="kobo.1227.1">Next, we</span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.1228.1"> create a DataFrame with 100 rows by creating four columns. </span><span class="koboSpan" id="kobo.1228.2">It will contain an </span><strong class="source-inline"><span class="koboSpan" id="kobo.1229.1">id</span></strong><span class="koboSpan" id="kobo.1230.1"> column with values from 1 to 100, a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1231.1">date_loaded</span></strong><span class="koboSpan" id="kobo.1232.1"> column that contains the 100 dates we created previously, a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1233.1">population</span></strong><span class="koboSpan" id="kobo.1234.1"> column with 100 random values between 1,000 and 100,000, and a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1235.1">property_area</span></strong><span class="koboSpan" id="kobo.1236.1"> column with 40 values set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1237.1">urban</span></strong><span class="koboSpan" id="kobo.1238.1">, 50 values set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1239.1">semi_urban,</span></strong><span class="koboSpan" id="kobo.1240.1"> and 10 values set </span><span class="No-Break"><span class="koboSpan" id="kobo.1241.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1242.1">rural</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1243.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1244.1">
np.random.seed(1)
data = {
    "id": np.linspace(start=1, stop=100, num=100, dtype=int),
    "population" : np.random.randint(low=1000, high=100000, size=100),
    "property_area": ["urban"]*40 + ["semi_urban"]*50 + ["rural"]*10,
    "date_loaded": date_list
}
df = pd.DataFrame(data=data)</span></pre> <p><span class="koboSpan" id="kobo.1245.1">Now, we visualize the first five </span><span class="No-Break"><span class="koboSpan" id="kobo.1246.1">data points:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1247.1">
df.head()
   id  population property_area                date_loaded
0   1       99539         urban 2023-02-04 11:18:46.771142
1   2       78708         urban 2023-02-03 11:18:46.771142
2   3        6192         urban 2023-02-02 11:18:46.771142
3   4       99047         urban 2023-02-01 11:18:46.771142
4   5       51057         urban 2023-01-31 11:18:46.771142</span></pre> <p><span class="koboSpan" id="kobo.1248.1">Next, we create a one-line piece of code to demonstrate a way to subtract any date from today’s date and extract the number of days between </span><span class="No-Break"><span class="koboSpan" id="kobo.1249.1">the dates:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1250.1">
(datetime.now() - df.date_loaded.max()).days
0</span></pre> <p><span class="koboSpan" id="kobo.1251.1">Then, we </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.1252.1">create a function that will accept a DataFrame and the date column of the DataFrame, and by default will issue a warning if data is more than 5 days old and block the application when data is more than 10 </span><span class="No-Break"><span class="koboSpan" id="kobo.1253.1">days old:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1254.1">
def check_data_recency_days(df: pd.DataFrame, loaded_at_column: str, warning_at: int=5, error_at: int=10):
    """Function to detect data freshness"""
    df = df.copy()
    days_since_data_refreshed = (datetime.now() - df[loaded_at_column].max()).days
    if days_since_data_refreshed &lt; warning_at:
        print(f"Data is fresh and is {days_since_data_refreshed} days old")
    elif error_at &gt; days_since_data_refreshed &gt;= warning_at:
        warnings.warn(f"Warning: Data is not fresh, and is {days_since_data_refreshed} days old")
    else:
        raise ValueError(f"Date provided is too old and stale, please contact source provider: {days_since_data_refreshed} days old")</span></pre> <p><span class="koboSpan" id="kobo.1255.1">Next, we </span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.1256.1">run the function with the sample DataFrame we created previously. </span><span class="koboSpan" id="kobo.1256.2">The function will state that the data is fresh and just 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.1257.1">days old:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1258.1">
check_data_recency_days(df, "date_loaded")
Data is fresh and is 0 days old</span></pre> <p><span class="koboSpan" id="kobo.1259.1">To demonstrate the function’s ability to issue a warning or error out when data is stale, we subset the data by removing 6 recent days and 12 recent days. </span><span class="koboSpan" id="kobo.1259.2">We create two DataFrames – one that has 6 recent days removed and another that has 12 recent days removed. </span><span class="koboSpan" id="kobo.1259.3">Then, we run the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1260.1">check_data_recency_day</span></strong><span class="koboSpan" id="kobo.1261.1"> function over these DataFrames. </span><span class="koboSpan" id="kobo.1261.2">We see that when we run the function with 6-day-old data, the function will issue the warning, but when we run the function with 12-day-old data, the function will issue a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1262.1">Value</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1263.1"> error.</span></span></p>
<p><span class="koboSpan" id="kobo.1264.1">Let’s create the </span><span class="No-Break"><span class="koboSpan" id="kobo.1265.1">two DataFrames:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1266.1">
df_filter_6_days = df[df.date_loaded &lt;= (datetime.today() -  timedelta(days=6))]
df_filter_12_days = df[df.date_loaded &lt;= (datetime.today() -  timedelta(days=12))]</span></pre> <p><span class="koboSpan" id="kobo.1267.1">Next, we run the function against the data that is 6 </span><span class="No-Break"><span class="koboSpan" id="kobo.1268.1">days old:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1269.1">
check_data_recency_days(df_filter_6_days, "date_loaded")
/var/folders/6f/p7312_7n4nq5hp35rfymms1h0000gn/T/ipykernel_5374/1750573000.py:11: UserWarning: Warning: Data is not fresh, and is 6 days old
  warnings.warn(f"Warning: Data is not fresh, and is {days_since_data_refreshed} days old")</span></pre> <p><span class="koboSpan" id="kobo.1270.1">You can also run the function against the data that is 12 days old; it will generate </span><span class="No-Break"><span class="koboSpan" id="kobo.1271.1">similar output.</span></span></p>
<p><span class="koboSpan" id="kobo.1272.1">With that, we have demonstrated how to measure data staleness, catch warnings, and block the application when data is extremely stale. </span><span class="koboSpan" id="kobo.1272.2">Next, we will demonstrate the impact of data staleness on a </span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">real-life dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.1274.1">In real life, we</span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.1275.1"> wouldn’t expect a company not to change its products or for consumer behavior not to change as newer products become available on the market. </span><span class="koboSpan" id="kobo.1275.2">Companies have to constantly study changes in consumer behavior; otherwise, their performance degrades. </span><span class="koboSpan" id="kobo.1275.3">Machine learning systems face the same issues as market forces change, data changes, and data distributions change. </span><span class="koboSpan" id="kobo.1275.4">This has an impact on a model’s performance if new data is quite different from the data it was </span><span class="No-Break"><span class="koboSpan" id="kobo.1276.1">trained on.</span></span></p>
<p><span class="koboSpan" id="kobo.1277.1">This is referred to as drift in machine learning and if it goes undetected and untreated, it can cause models </span><span class="No-Break"><span class="koboSpan" id="kobo.1278.1">to degrade.</span></span></p>
<p><span class="koboSpan" id="kobo.1279.1">Let’s explore how to </span><span class="No-Break"><span class="koboSpan" id="kobo.1280.1">detect drift.</span></span></p>
<p><span class="koboSpan" id="kobo.1281.1">First, we import </span><strong class="source-inline"><span class="koboSpan" id="kobo.1282.1">TabularDrift</span></strong><span class="koboSpan" id="kobo.1283.1"> from the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1284.1">alibi-detect</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1285.1"> package:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1286.1">
import alibi
from alibi_detect.cd import TabularDrift</span></pre> <p><span class="koboSpan" id="kobo.1287.1">Next, we show the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1288.1">TabularDrift</span></strong><span class="koboSpan" id="kobo.1289.1"> reference data, which is the data the machine learning system was trained on – in our case, the loan prediction transformed data before we passed it to the decision tree classifier. </span><span class="koboSpan" id="kobo.1289.2">We also pass a value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1290.1">0.05</span></strong><span class="koboSpan" id="kobo.1291.1"> for the p-value test. </span><span class="koboSpan" id="kobo.1291.2">If this value is breached by the test data distribution, the package will inform us that the test data has drifted from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1292.1">training data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1293.1">
cd = TabularDrift(x_ref=X_train_transformed.to_numpy(), p_val=.05 )</span></pre> <p><span class="koboSpan" id="kobo.1294.1">Now, we run the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1295.1">predict</span></strong><span class="koboSpan" id="kobo.1296.1"> method to check if the test data has drifted. </span><span class="koboSpan" id="kobo.1296.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1297.1">alibi</span></strong><span class="koboSpan" id="kobo.1298.1"> package utilizes the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1299.1">Kolmogorov-Smirnov</span></strong><span class="koboSpan" id="kobo.1300.1"> test to determine if the two distributions differ. </span><span class="koboSpan" id="kobo.1300.2">If the p-value exceeds 0.05, then the null hypothesis is rejected and it can be inferred that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1301.1">test</span></strong><span class="koboSpan" id="kobo.1302.1"> data distribution differs from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1303.1">train</span></strong><span class="koboSpan" id="kobo.1304.1"> data distribution. </span><span class="koboSpan" id="kobo.1304.2">The output for this step will </span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">be </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1306.1">No</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1307.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1308.1">
preds = cd.predict(X_test_transformed.to_numpy())
labels = ['No', 'Yes']
print('Drift: {}'.format(labels[preds['data']['is_drift']]))
Drift: No</span></pre> <p><span class="koboSpan" id="kobo.1309.1">Now, let’s </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.1310.1">imagine that house prices started booming while incomes did not increase at the same rate. </span><span class="koboSpan" id="kobo.1310.2">To simulate this scenario, we will increase the loan amount requested by 1.5 times the original test set, but increase the total income by 1.2 times the test set. </span><span class="koboSpan" id="kobo.1310.3">Then, we update the new feature values that relied on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1311.1">loan_amount</span></strong><span class="koboSpan" id="kobo.1312.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1313.1">income</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1"> variables:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1315.1">
 X_test_transformed['loan_amount'] = X_test_transformed['loan_amount']*1.5
X_test_transformed['sum_applicant_income_coapplicant_income'] = X_test_transformed['sum_applicant_income_coapplicant_income']*1.2
X_test_transformed.sum_applicant_income_coapplicant_income_div_loan_amount = X_test_transformed.sum_applicant_income_coapplicant_income/X_test_transformed.loan_amount
X_test_transformed.loan_amount_div_loan_amount_term = X_test_transformed.loan_amount/X_test_transformed.loan_amount_term</span></pre> <p><span class="koboSpan" id="kobo.1316.1">Next, we rerun TabularDrift’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.1317.1">predict</span></strong><span class="koboSpan" id="kobo.1318.1"> method to check whether drift was detected. </span><span class="koboSpan" id="kobo.1318.2">The output of this step </span><span class="No-Break"><span class="koboSpan" id="kobo.1319.1">is </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1320.1">Yes</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1321.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1322.1">
preds = cd.predict(X_test_transformed.to_numpy())
labels = ['No', 'Yes']
print('Drift: {}'.format(labels[preds['data']['is_drift']]))
Drift: Yes</span></pre> <p><span class="koboSpan" id="kobo.1323.1">Then, we </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.1324.1">rerun the prediction on this drift-induced test data and check whether accuracy and ROC </span><span class="No-Break"><span class="koboSpan" id="kobo.1325.1">are affected:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1326.1">
testing_predictions_prob = model.predict_proba(X_test_transformed)
testing_predictions = model.predict(X_test_transformed)
testing_roc_auc = roc_auc_score(y_test, testing_predictions_prob[:,1])
testing_acc = accuracy_score(y_test, testing_predictions)
print(f"Testing roc is {testing_roc_auc} and testing_acc as {testing_acc}")
Testing roc is 0.747858017135863 and testing_acc as 0.6935483870967742</span></pre> <p><span class="koboSpan" id="kobo.1327.1">As we can see, the distribution that was seen by the model when it was trained is different from the real data, and the impact is that model performance has degraded significantly. </span><span class="koboSpan" id="kobo.1327.2">The ROC has dropped from 0.82 to 0.74 and accuracy has dropped from 82% to 70%. </span><span class="koboSpan" id="kobo.1327.3">Hence, it’s important to ensure that d</span><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.1328.1">ata is fresh and that as soon as drift is detected, the model is retrained with new data to ensure model performance does </span><span class="No-Break"><span class="koboSpan" id="kobo.1329.1">not deteriorate.</span></span></p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.1330.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1331.1">In this chapter, we gained a good understanding of the six key dimensions of data quality and why it’s important to improve data quality for superior model performance. </span><span class="koboSpan" id="kobo.1331.2">We further dived into the data-centric approach of improving model performance by iterating over the data, rather than iterating over various algorithms (model-centric approach), by improving the overall health of </span><span class="No-Break"><span class="koboSpan" id="kobo.1332.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.1333.1">Next, we learned how to ensure data is consistent, unique, accurate, valid, fresh, and complete. </span><span class="koboSpan" id="kobo.1333.2">We dived into various techniques of imputing missing values and when to apply which approach. </span><span class="koboSpan" id="kobo.1333.3">We concluded that imputing missing values with machine learning can be better than using simple imputation methods, especially when data is MAR or MNAR. </span><span class="koboSpan" id="kobo.1333.4">We also showed how to conduct error analysis and how to use the results to further improve model performance by either performing feature engineering, which involves building new features, or increasing the data size by creating synthetic data, which we will cover in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1334.1">next chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.1335.1">We also discussed why is it important to ensure data is fresh and not drifted from the original training set, and concluded that drifted data can hamper </span><span class="No-Break"><span class="koboSpan" id="kobo.1336.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.1337.1">Now that we have understood the importance of ensuring good-quality data across the six key dimensions of data quality, in the next chapter, we will dive into using synthetic data to further improve model performance, especially over edge cases. </span><span class="koboSpan" id="kobo.1337.2">We will also dive into data augmentation, a technique that’s used to create synthetic data for images so that algorithms can learn from more and better, especially when these new examples can come in </span><span class="No-Break"><span class="koboSpan" id="kobo.1338.1">various forms.</span></span></p>
</div>
</body></html>