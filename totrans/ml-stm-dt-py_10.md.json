["```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nstarting = 0\nvalues = [starting]\nfor i in range(30000):\n    values.append(values[-1] + np.random.normal())\nvalues = (values - np.mean(values)) / np.std(values)\nplt.plot(values)\n```", "```py\nrounded_values = []\nfor value in values:\n    rounded_values.append(round(value, 1))\nplt.plot(rounded_values)\n```", "```py\nstates = set(rounded_values)\nimport pandas as pd\npolicy = pd.DataFrame(0, index=states, columns=['buy', 'sell'])\n```", "```py\ndef find_action(policy, current_value):\n\n    if policy.loc[current_value,:].sum() == 0:\n        return random.choice([ 'buy', 'sell'])\n    return policy.columns[policy.loc[current_value,:].argmax()]\n```", "```py\ndef update_policy(reward, current_state_value, action):\n\n    LEARNING_RATE = 0.1\n    MAX_REWARD = 10\n    DISCOUNT_FACTOR = 0.05\n\n    return LEARNING_RATE * (reward + DISCOUNT_FACTOR * MAX_REWARD - policy.loc[current_state_value,action])\n```", "```py\npast_state_value = 0\npast_action = 'buy'\ntotal_reward = 0.\nrewards = []\nfor i, current_state_value in enumerate(rounded_values):\n\n    # do the action\n    action = find_action(policy, current_state_value)\n\n    # also compute reward from previous action and update state\n    if past_action == 'buy':\n        reward = current_state_value - past_state_value\n\n    if past_action == 'sell':\n        reward = past_state_value - current_state_value\n\n    total_reward = total_reward + float(reward)\n\n    policy.loc[current_state_value, action] = policy.loc[current_state_value, action] + update_policy(reward, current_state_value,action)\n\n    #print(policy)\n    rewards.append(total_reward)\n\n    past_action = action\n    past_state_value = current_state_value\n```", "```py\nplt.plot(rewards)\n```", "```py\nimport seaborn as sns\nsns.heatmap(policy.sort_index())\n```"]