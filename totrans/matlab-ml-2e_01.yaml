- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring MATLAB for Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) is a branch of artificial intelligence that is
    based on the development of algorithms and mathematical models capable of learning
    from data and autonomously adapting to improve their performance according to
    a set of objectives. Thanks to this learning ability, ML is used in a wide range
    of applications, such as data analysis, computer vision, language modeling, speech
    recognition, medical diagnosis, and financial risk prediction. ML is an ever-evolving
    area of research and is revolutionizing many fields of science and industry. The
    aim of this chapter is to provide you with an introduction, background information,
    and a basic knowledge of ML, as well as an understanding of how to apply these
    concepts using MATLAB tools.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering the different types of learning processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ML techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring MATLAB toolboxes for ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML applications in real life
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic concepts relating to ML. To understand
    these topics, a basic knowledge of algebra and mathematical modeling is needed.
    A working knowledge of the MATLAB environment is also required.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML is based on the idea of providing computers with a large amount of input
    data, together with the corresponding correct answers or labels, and allowing
    them to learn from this data, identifying patterns, relationships, and regularities
    within them. Unlike traditional programming approaches, in which computers follow
    precise instructions to perform specific tasks, ML allows machines to independently
    learn from data and make decisions based on statistical models and predictions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key concepts of ML is the ability to generalize. This means that
    a model trained on information in the training dataset should be able to make
    accurate predictions about new data that it has never seen before. This allows
    ML to be applied across a wide range of domains.
  prefs: []
  type: TYPE_NORMAL
- en: How to define ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To better understand the basic concepts of ML, we can start from the definitions
    formulated by the pioneers in this field. According to Arthur L. Samuel (1959)
    – “ML is a field of study that gives computers the ability to learn without being
    explicitly programmed.”
  prefs: []
  type: TYPE_NORMAL
- en: The definition mentioned refers to the ability to learn from experience, which
    humans do in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: ML is an interdisciplinary domain forged at the crossroads and harmonious blending
    of computer science, statistics, neurobiology, and control theory. Its practical
    applications have successfully surmounted various challenges across diverse fields,
    fundamentally altering the paradigm of software development. In essence, ML constitutes
    a foundational approach that empowers computers with a degree of autonomy. It
    is evident that ML draws its inspiration from the study of human learning; just
    as the human brain and its neurons underlie intuition, artificial neural networks
    serve as the bedrock for computer decision-making. ML facilitates the creation
    of models that can accurately depict data patterns through a meticulous analysis
    of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: As an illustration, we can establish a connection between input variables and
    output variables within a given system. One approach to achieving this is by assuming
    the existence of a mechanism for generating parametric data, albeit without precise
    knowledge of the parameter values. This procedure is commonly referred to as employing
    statistical methods.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of logical reasoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logical reasoning is based on the concepts of induction, deduction, and inference.
    Here are the differences between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Induction** is a reasoning process that starts from specific observations
    or data to arrive at general conclusions. In other words, it is about extracting
    a general rule or principle based on specific examples. Induction can be useful
    for making predictions and generalizations, but the conclusions obtained are not
    necessarily certain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deduction** is a reasoning process that starts from general premises or rules
    and arrives at specific conclusions. In other words, it is about applying general
    principles to obtain specific information. The deduction is based on formal logic
    and the use of valid inference rules. The deduction produces logically correct
    conclusions from the given premises.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, **inference** is a reasoning process that leads us to draw conclusions,
    deductions, or judgments based on available evidence or information. Inference
    can involve both induction and deduction as well as other forms of reasoning,
    such as abduction. Inference can be viewed as the application of reasoning rules
    or strategies to obtain new information or arrive at conclusions based on existing
    ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, induction is based on extracting general principles from specific
    examples, deduction is based on applying general principles to obtain specific
    conclusions, while inference is a broader term that encompasses both induction
    and deduction, as well as other reasoning processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the realm of ML, the teacher and the learner are two pivotal entities that
    play significant roles. The teacher possesses the necessary knowledge to execute
    a given task, while the learner’s objective is to acquire that knowledge to perform
    the task. The strategies employed for learning can be distinguished based on the
    level of inference carried out by the learner, considering two extremes: **no
    inference** and **substantial inference**.'
  prefs: []
  type: TYPE_NORMAL
- en: When a computer system (the learner) is directly programmed, it acquires knowledge
    without engaging in any inference, as all cognitive processes are handled by the
    programmer (the teacher). Quite the opposite, when a system finds new solutions
    autonomously, it necessitates a substantial amount of inference. In this scenario,
    organized knowledge is derived through experiments and observations. In the following
    figure, the difference between induction and deduction for inference tasks is
    shown.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Logical reasoning keys](img/B21156_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Logical reasoning keys
  prefs: []
  type: TYPE_NORMAL
- en: In between induction and deduction is a midpoint called inference – for example,
    say a student seeks to work out a math challenge by drawing analogies to solutions
    provided in a workbook. This activity demands inference, albeit to a lesser extent
    than the discovery of a new mathematical theorem.
  prefs: []
  type: TYPE_NORMAL
- en: By increasing the learner’s capacity for inference, the drain on the teacher
    diminishes. The following taxonomy of ML endeavors to portray the concept of trade-offs
    in terms of work demanded by both the learner and the teacher.
  prefs: []
  type: TYPE_NORMAL
- en: Learning strategy typologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the inference performed, we can identify different mechanisms that
    a learning system can adopt. Let’s see some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rote learning**: Let’s start with a basic one, rote learning, which involves
    the use of content without any transformation being carried out on it. No inference
    process is activated, and the knowledge is not processed in any way; it is essentially
    a memorization process programmed with considerable effort on the part of the
    learner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning from instruction**: When acquiring knowledge from a teacher, the
    learner needs to convert the information presented into a format that can be internally
    processed. Additionally, it is crucial for the learner to integrate the new knowledge
    with their existing understanding to effectively utilize it. This process involves
    some level of inference on the learner’s part, but a significant portion of the
    responsibility lies with the teacher. The teacher must present and organize the
    knowledge in a manner that progressively enhances the student’s existing knowledge.
    This aligns with the approach used in most formal education methods. Consequently,
    the task of ML involves developing a system that can receive instruction or advice,
    store it, and apply the acquired knowledge effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Learning strategy typologies based on the inference performed](img/B21156_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Learning strategy typologies based on the inference performed
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning by analogy**: When acquiring new statements or skills, the process
    involves converting and enhancing existing knowledge that shares a strong resemblance
    to the new setting. This allows the transformed knowledge to be effectively applied
    in the new context. For example, a learning-by-analogy system could be utilized
    to transform code into something that implements a closely related function, even
    if it wasn’t originally designed for that purpose. Learning by analogy needs more
    inference on the part of the learner compared to the previous learning mechanism.
    The learner must retrieve a relevant fact or skill from their memory that is analogous
    in terms of relevant parameters. Then, the retrieved knowledge needs to be converted,
    related to the new context, and archived for future use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning from examples**: When confronted with a set of instances, including
    both examples that support an idea and counterexamples that contradict it, the
    learner infers a generalized description of the idea. This description encompasses
    all the positive examples while systematically removing the negative ones. Learning
    from examples has been extensively studied in the field of artificial intelligence.
    In this method, the learner engages in a higher degree of inference compared to
    learning from instruction, as there are no general ideas provided by a teacher.
    It also involves a slightly greater level of inference than learning by analogy
    since there are no related ideas provided as starting points for developing the
    new concept.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning from observation**: This form of inductive learning is highly versatile
    and encompasses various tasks, such as discovery systems, theory formation, and
    establishing grouping principles for creating taxonomic hierarchies. Unlike other
    approaches discussed, this form of learning does not rely on an external teacher.
    Instead, the learner is required to engage in extensive inference. They are not
    presented with a specific set of examples for a specific idea, nor do they have
    access to a prediction that can group instances produced in an automated way as
    positive or negative examples of any given concept. Furthermore, instead of focusing
    on a single idea, the learner must handle multiple concepts simultaneously, which
    introduces a significant challenge in terms of attention allocation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning by chunking**: This is a cognitive strategy that involves breaking
    down information into smaller, manageable units or “chunks” to enhance memory
    and comprehension. This technique capitalizes on the brain’s natural tendency
    to organize and process information in meaningful clusters. By grouping related
    concepts together, learners can more easily absorb and retain complex material.
    Chunking is particularly effective in various educational settings, from memorizing
    lists and sequences to mastering intricate subjects. When information is organized
    into cohesive chunks, it becomes easier to grasp the relationships between different
    components, facilitating a deeper understanding of the overall content. This approach
    is especially beneficial in fields such as language acquisition, where breaking
    down phrases or words into manageable chunks aids in faster and more efficient
    learning. Moreover, chunking promotes efficient recall and problem-solving. Instead
    of struggling to remember individual pieces of information, learners can access
    broader chunks of knowledge, leading to quicker and more accurate retrieval. This
    cognitive strategy aligns with the brain’s capacity to process information in
    parallel, optimizing the learning process and enhancing overall cognitive performance.
    In essence, learning by chunking empowers individuals to navigate the complexities
    of information with greater ease and effectiveness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the learning mechanisms that a system can adopt will help us better
    understand the different types of learning processes that can be adopted in an
    ML-based system.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the different types of learning processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning is based on the idea that perceptions should not only guide actions
    but also enhance the agent’s ability to automatically learn from interactions
    with the world and the decision-making processes themselves. A system is considered
    capable of learning when it has an executive component for making decisions and
    a learning component for modifying the executive component to improve decisions.
    Learning is influenced by the components learned from the system, by the feedback
    received after the actions are performed, and by the type of representation used.
  prefs: []
  type: TYPE_NORMAL
- en: ML offers several ways of allowing algorithms to learn from data, which are
    classified into categories based on the type of feedback on which the learning
    system is based. Choosing which learning category to use for a specific problem
    must be done in advance to find the best solution. It is useful to evaluate the
    robustness of the algorithm, such as its ability to make correct predictions even
    with missing data, the scalability, the efficiency of the algorithm with small
    or large datasets, and the interpretability, which refers to the possibility of
    attributing a more subjective than objective outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Different types of learning processes](img/B21156_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Different types of learning processes
  prefs: []
  type: TYPE_NORMAL
- en: 'ML algorithms can be categorized according to the type of experience they are
    subjected to during the learning process. It is common to categorize the paradigms
    of ML as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: The algorithm generates a function that links input
    values to a desired output, through the observation of a set of examples in which
    each data input has its relative output data that is used to construct predictive
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: The algorithm tries to derive knowledge from a general
    input, without the help of a set of pre-classified examples, which is used to
    build descriptive models. A typical example of the application of these algorithms
    is search engines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: The algorithm can learn depending on the changes
    that occur in the environment in which it is performed. The agent receives feedback
    in the form of rewards or punishments based on the actions it takes, allowing
    it to learn optimal strategies over time. In fact, since every action has some
    effect in the environment concerned, the algorithm is driven by the same feedback
    environment. Some of these algorithms are used in self-driving cars and autonomous
    AlphaGo gaming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see these categories in detail, analyzing their characteristics and trying
    to understand how to choose the most suitable paradigm for our problem.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised learning is an ML technique designed to enable a computer system
    to automatically solve tasks. The process involves providing input data, typically
    in the form of vectors, which form a set, *I*. The output data is defined as a
    set, *O*, and a function, *f*, is established to associate each input with the
    correct answer. This set of data used for training is referred to as the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: '*O**=**f**(**I**)*'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying principle of all supervised learning algorithms is that with
    enough examples, an algorithm can create a derived function, *B*, that approximates
    the desired function, *A*. If the approximation is accurate enough, the derived
    function should provide output answers like those of the desired function, making
    them acceptable. These algorithms rely on the assumption that similar inputs correspond
    to similar outputs. This assumption is often not perfectly met. However, there
    are situations where this approximation is acceptable.
  prefs: []
  type: TYPE_NORMAL
- en: With this type of learning, it is possible, for example, to use correctly labeled
    images of animals as input, to make the algorithm learn the correlation between
    the characteristics of a specific animal and the image containing it. This is
    done in such a way that, subsequently, the algorithm can recognize it, given the
    input of an image containing that type of animal. The algorithm learns to recognize
    a certain pattern to understand the correlation between the image and the assigned
    label, to then deduce a general rule with which to recognize whether the same
    correlation exists in the subsequent data.
  prefs: []
  type: TYPE_NORMAL
- en: To use supervised learning, it is therefore necessary to have data with known
    outputs. In the case of fully observable environments, the system can see what
    effects its actions have and can use the supervised learning method to learn to
    predict them. If the environment were to be not completely observable, the immediate
    effects could be invisible. The performance of these algorithms is highly dependent
    on the input data. If only a few training inputs are provided, the algorithm may
    lack sufficient experience to produce correct outputs. On the other hand, an excessive
    number of inputs can make the algorithm excessively slow, as the derived function
    becomes more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, supervised learning algorithms are sensitive to noise. Even a small
    number of incorrect data points can render the entire system unreliable, leading
    to erroneous decisions.
  prefs: []
  type: TYPE_NORMAL
- en: When the output value is categorical, such as membership/non-membership in a
    certain class, it is considered a classification problem. On the other hand, if
    the output is a continuous real value within a certain range, it is classified
    as a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of unsupervised learning is to automatically extract information
    from databases without prior knowledge of the content to be analyzed. Unlike supervised
    learning, there is no information available regarding membership classes or the
    output corresponding to a given input. The goal is to obtain a model capable of
    discovering interesting properties, such as groups (clusters) of examples with
    similar characteristics (clustering). An example of the application of these algorithms
    is seen in search engines. By providing one or more keywords, they can generate
    a list of links related to our search. The differences between supervised learning
    and unsupervised learning are highlighted in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Supervised learning versus unsupervised learning](img/B21156_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Supervised learning versus unsupervised learning
  prefs: []
  type: TYPE_NORMAL
- en: The objective of unsupervised learning is to autonomously learn the underlying
    structure of the input data. The outcomes are influenced by decisions regarding
    which data to input and the order in which they are presented. This approach reorganizes
    the data in a different manner, creating more meaningful data clusters for subsequent
    analyses and enabling the discovery of previously unnoticed patterns in the data.
    A system capable of pure unsupervised learning lacks the ability to learn what
    actions to take, as it lacks information about what constitutes a correct action
    or a desirable state to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is commonly employed in clustering problems, where it
    identifies groups of data based on shared characteristics, resulting in the creation
    of data clusters. Additionally, this approach is utilized in association learning,
    which involves identifying associative rules among the input data and in recommendation
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of these algorithms hinges on the value of the information
    they can derive from databases. These algorithms function by scrutinizing data
    and actively seeking out resemblances or disparities among them. The data at hand
    pertains exclusively to the array of attributes that characterize each individual
    example.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning demonstrates high efficiency when working with numeric
    elements, but their accuracy decreases when dealing with non-numeric data. Typically,
    these algorithms perform well when the data exhibits a clear order or distinct
    grouping that can be easily identified.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning aims to develop algorithms that can learn and adapt to
    changes in their environment. This programming technique is based on the concept
    of receiving external stimuli based on the choices made by the algorithm. Making
    a correct choice results in a reward, while an incorrect choice leads to a penalty.
    The ultimate objective of the system is to achieve the best possible outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike supervised learning, where a teacher provides the system with correct
    outputs (learning with guidance), this is not always feasible. Often, only qualitative
    information is available, sometimes in a binary form (such as right/wrong or success/failure).
    These available pieces of information are referred to as reinforcement signals.
    However, the system does not provide any guidance on how to update the agent as
    an optimization algorithm and the environment as being characterized by the family
    of objective functions that we’d like to learn an optimizer for. The goal of the
    system is to create “intelligent” agents that possess the ability to learn from
    their experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning algorithms operate based on rewards that are assigned
    according to achieved objectives. These rewards are essential for understanding
    what actions are desirable and should be pursued, as well as what actions should
    be avoided. Without rewards, the algorithm would struggle to make decisions. Reinforcement
    learning is employed when algorithms need to make decisions that have consequences.
    It goes beyond being purely descriptive and becomes prescriptive, providing guidance
    on what actions to take. This type of learning is highly innovative in the field
    of ML and is well suited for understanding how environments work. The flow of
    the reinforcement learning algorithm is shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Reinforcement learning mechanism](img/B21156_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Reinforcement learning mechanism
  prefs: []
  type: TYPE_NORMAL
- en: A crucial aspect is the algorithm’s ability to interpret feedback as reinforcement
    rather than new input. The goal of reinforcement learning is to utilize received
    feedback to construct an optimal policy, maximizing the total expected reward.
    A particular policy represents a particular update formula. Hence, learning the
    policy is equivalent to learning the update formula, and hence the optimization
    algorithm. This approach allows training an algorithm without explicitly defining
    rules deduced from feedback. It is particularly useful in complex domains where
    defining rules can be challenging, and performance can be greatly improved.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning encompasses both passive learning and active learning.
    In passive learning, the agent follows a fixed policy and aims to learn the utilities
    of state-action pairs. In some cases, the agent may also need to learn the model
    of the environment. In active learning, the agent must learn what actions to take
    and, through exploration, gain experience on how to behave within the environment.
    One way to implement this strategy is by having the algorithm play a game without
    providing it with the rules. Positive feedback is given when the agent takes allowed
    or beneficial actions, while negative feedback is provided for undesirable actions.
    This allows the algorithm to learn optimal moves during the game without explicit
    knowledge of the game rules. The agent learns to behave successfully in its environment
    solely based on the received feedback and without prior knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside the three pillars of the learning paradigms just seen (supervised
    learning, unsupervised learning, and reinforcement learning) are some new typologies
    derived from these approaches. Let’s see some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semi-supervised learning combines the strengths of both supervised and unsupervised
    learning. Initially, the supervised approach is employed by providing inputs along
    with their corresponding outputs. Then, additional similar inputs are introduced
    without their associated output references. These inserted inputs and outputs
    contribute to the creation of a general model, which can be utilized to extrapolate
    outputs for the remaining inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The process begins with the supervised phase, where the labeled data is used
    to train a model. This model learns to map the inputs to their corresponding outputs
    based on the provided labels. Once the initial model is trained, it can be used
    to make predictions on the unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: During the unsupervised phase, the model uses the unlabeled data to extract
    patterns, structures, or relationships within the data. This can be done through
    techniques such as clustering, dimensionality reduction, or generative modeling.
    By leveraging the unlabeled data, the model can gain a better understanding of
    the underlying distribution of the data and potentially improve its performance.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of semi-supervised learning is to use the knowledge gained from the
    labeled and unlabeled data to create a more robust and accurate model. By combining
    the labeled data’s explicit guidance with the unsupervised learning’s ability
    to capture hidden patterns, semi-supervised learning can be a powerful approach,
    especially in scenarios where obtaining labeled data is expensive or time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfer learning offers the ability to transfer the acquired knowledge from
    addressing one problem to effectively tackling a similar problem. The significant
    advantage of reusing knowledge is evident, although it may not always be feasible
    due to the need to adapt many ML algorithms to the specific case at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning involves leveraging knowledge gained from solving one problem
    and applying it to a different but related problem. In transfer learning, a pre-trained
    model that has been trained on a large dataset is used as a starting point for
    solving a new task or problem.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind transfer learning is that the features learned by a model
    on one task can be useful for another task. Instead of starting the learning process
    from scratch, transfer learning allows us to transfer the knowledge and representations
    acquired by the pre-trained model to the new task. Using such an approach can
    lead to a substantial reduction in the required amount of training data and computational
    resources for the new task.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are typically two main approaches to transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning**: In this approach, the pre-trained model is taken and further
    trained on the new task with a smaller dataset specific to the new task. The idea
    is to adjust the parameters of the pre-trained model to make it more relevant
    to the new problem while retaining the knowledge it has already learned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: With this approach, the pre-trained model is used as
    a fixed feature extractor available for deep learning models. The earlier layers
    of the pre-trained model are frozen, and only the later layers are replaced or
    modified to adapt to the new task. The new data is passed through the pre-trained
    model, and the output features from the last few layers are extracted and used
    as input for a new classifier or model specific to the new task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning can be particularly effective when the pre-trained model has
    been trained on a large and diverse dataset, as it tends to learn generic features
    that are useful for a wide range of tasks. By leveraging pre-existing knowledge,
    transfer learning enables faster training, better performance, and improved generalization
    on new tasks, especially when labeled data for the new task is limited or unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: After introducing the different types of learning paradigms, we can move on
    to analyzing how to approach a problem using techniques based on ML.
  prefs: []
  type: TYPE_NORMAL
- en: Using ML techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we explored the various types of ML paradigms in detail.
    So, we have understood the basic principles that underlie the different approaches.
    At this point, it is necessary to understand what the elements that allow us to
    discriminate between the different approaches are; in other words, in this section,
    we will understand how to adequately choose the learning approach necessary to
    obtain our results.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the ML paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Selecting the appropriate ML algorithm can feel overwhelming given the numerous
    options available, including both supervised and unsupervised approaches, each
    employing different learning strategies.
  prefs: []
  type: TYPE_NORMAL
- en: There is no universally superior method, nor one that fits all situations. In
    large part, the search for the right algorithm involves trial and error; even
    seasoned data scientists cannot determine whether an algorithm will work without
    testing it. Nonetheless, the algorithm choice is also influenced by factors such
    as the data format and type, the desired information to be extracted, and how
    that information will be utilized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some guidelines to assist in selecting the most suitable approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Opt for supervised learning when the objective is to train a model for making
    predictions, such as determining future values of a continuous variable such as
    barometric pressure or stock prices, or performing classification tasks such as
    identifying vehicle types from webcam footage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose unsupervised learning when the aim is to analyze data and develop a model
    that discovers meaningful internal representations, such as clustering the data
    into distinct groups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select reinforcement learning when the objective is to achieve a goal within
    an uncertain environment where all variables cannot be predicted. This approach
    is valuable when there are multiple ways to accomplish a task, but certain rules
    need to be followed. An example is autonomous driving, where adherence to traffic
    regulations is essential.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By considering these factors and aligning them with the specific goals and characteristics
    of the data, one can make a more informed choice when selecting an appropriate
    ML paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: The selection of the learning paradigm and the specific algorithm is clearly
    dependent on the characteristics of the data we are working with. This includes
    factors such as data size, quality, and nature, as well as the desired outcome
    and the implementation details of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There is no universally superior method or one-size-fits-all solution. The only
    way to ascertain the suitability of an algorithm is to experiment and evaluate
    its performance using appropriate metrics.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can conduct a preliminary analysis to better understand the approach
    that aligns with our requirements. We start by considering what we have (the data),
    the available tools (algorithms), and the goals we aim to achieve (results). Through
    this analysis, we can gather valuable information to guide our decision-making
    process. The following figure shows the different learning paradigms and the activities
    that can be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – ML algorithm classification](img/B21156_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – ML algorithm classification
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin with the data and classify its characteristics. This classification
    helps us determine the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification based on input**: If we have labeled input data, it indicates
    a supervised learning problem. If labeling is not available but we seek to uncover
    the system’s structure, it indicates an unsupervised learning problem. Finally,
    if our goal is to optimize an objective function through interactions with the
    environment, it indicates a reinforcement learning problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification based on output**: If the model’s output is a numerical value,
    it suggests a regression problem. If the output is categorical, it indicates a
    classification problem. If the output involves grouping the input data, it indicates
    a clustering problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have classified the problem, we can explore the available tools to solve
    it. This involves identifying applicable algorithms and focusing our study on
    the methods required to implement these tools for our specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: After identifying the tools, we need to evaluate their performance. This can
    be accomplished by applying the selected algorithms to our datasets. By carefully
    selecting evaluation metrics, we can compare the performance of each algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: By following this process of data analysis, problem classification, tool identification,
    and performance evaluation, we can make informed decisions and choose the most
    suitable algorithm for our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-step guide on how to build ML models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the algorithm being applied to our data has been chosen, it is essential
    to establish a well-defined workflow before diving into the task at hand. Before
    embarking on the actual implementation, it is crucial to allocate some time to
    set up the workflow. When developing an ML-based application, this procedure typically
    involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the problem**: Clearly articulate the problem you want to solve with
    ML. Determine the specific task, such as classification, regression, or clustering,
    and understand the objectives and requirements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Collect and preprocess data**: Collect relevant data for your problem. Ensure
    the data is of high quality, representative, and covers a wide range of scenarios.
    Preprocess the data by handling missing values and outliers and performing data
    normalization or feature scaling and data cleaning. The foundation of any data-driven
    process lies in the data itself, and it is natural to question the origin of such
    vast amounts of data. Data collection involves a variety of methods, often through
    extensive procedures such as measurement campaigns or face-to-face interviews.
    Regardless of the specific method employed, the collected data is typically stored
    in a database, ready to be analyzed and transformed into valuable insights and
    knowledge.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Split the data**: Data splitting is a crucial step in ML to effectively evaluate
    and validate the performance of models. It involves dividing the available data
    into distinct subsets for training, validation, and testing purposes. A largely
    used data splitting technique, **train-validation-test split**, allows the division
    of data into training, validation, and testing sets as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training set**: The largest portion of the data used for model training.
    It is utilized to optimize the model’s parameters and learn patterns from the
    data.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Validation set**: A smaller subset of the data used to fine-tune the model’s
    hyperparameters and assess its performance during development. It helps in avoiding
    overfitting and selecting the best-performing model.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test set**: A separate portion of the data used as a final evaluation measure
    to assess the model’s generalization and performance on unseen examples. It provides
    an unbiased estimate of the model’s capabilities in real-world scenarios. The
    following flow chart shows the entire workflow of an ML model.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.7 – ML model implementation workflow](img/B21156_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – ML model implementation workflow
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature engineering**: Extract or create meaningful features from the available
    data that can help the model learn patterns and make accurate predictions. This
    may involve transforming or combining existing features, encoding categorical
    variables, or generating new features based on domain knowledge. Essentially,
    there are two main approaches: **feature extraction** and **selection**. Feature
    extraction involves transforming raw data into a set of meaningful features that
    can effectively represent the underlying patterns and relationships in the data.
    It aims to capture the most relevant information and discard irrelevant or redundant
    data. This process can be particularly useful when dealing with high-dimensional
    or unstructured data. Feature selection involves selecting a subset of the most
    informative and relevant features from the available set of features. It helps
    in reducing dimensionality, improving model interpretability, and avoiding overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Select a model**: Choose an appropriate ML model based on the type of problem,
    data characteristics, and available resources. We have seen which considerations
    it is necessary to make for the choice of the learning paradigm: *An adequate
    analysis is also necessary for the choice of the type of algorithm to adopt*.
    In fact, there is not just one way to go; there are many, and each solution has
    strengths and weaknesses. Consider algorithms such as decision trees, helper vector
    machines, neural networks, or ensemble methods. Research and compare different
    models to identify the most suitable one, that is, the one that best fits the
    available data and that best allows us to make an adequate inference and extract
    adequate knowledge.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train the model**: Now, the process becomes more substantial. At this stage,
    ML takes center stage as we define the model and proceed with training. The selected
    model begins the task of extracting valuable insights from the extensive volume
    of available data, uncovering new knowledge that was previously unknown to us.
    This marks the beginning of the learning journey, where the model dives deep into
    the data to uncover patterns, relationships, and hidden information that has yet
    to be revealed. Train the selected model using the training data. This involves
    feeding the input features and their corresponding target values to the model
    and adjusting its internal parameters through an optimization algorithm. The model
    learns from the training data to make accurate predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Validate and tune the model**: In this phase, we use the knowledge gained
    from the previous step of the test to determine the effectiveness of the model.
    Evaluating an algorithm involves assessing how closely the model approximates
    the real-world system. In supervised learning, we have known values that allow
    us to evaluate the algorithm’s performance. In unsupervised learning, alternative
    metrics are employed to gauge success. If the results are unsatisfactory, we can
    revisit the preceding steps, make necessary adjustments, and retest the model
    until we achieve the desired outcome. This iterative process allows for refinement
    and improvement as we strive to develop a more accurate and reliable model. Evaluate
    the model’s performance on the validation set. Use suitable evaluation metrics
    for your specific problem, such as accuracy, precision, recall, or mean squared
    error. Fine-tune the model by adjusting hyperparameters, such as learning rate,
    regularization, or number of layers, to optimize its performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate the model**: Now, we have reached the critical juncture where we
    can put our work into action. It’s time to apply the model we have built to real-world
    data and evaluate its ability to approximate the desired outcomes. The model,
    having undergone thorough training and testing, is now assessed and valued in
    this phase. By deploying the model on real data, we can observe its performance,
    analyze its predictions, and assess how well it aligns with our expectations.
    This stage allows us to gauge the practicality and effectiveness of the model
    in real-world scenarios, providing valuable insights for further improvements
    and optimizations. Once the model is tuned, assess its performance on the testing
    set, as this offers an impartial estimation of the model’s performance on fresh,
    unseen data. Evaluate the model’s metrics and analyze its strengths and weaknesses.
    Consider additional techniques such as cross-validation for a more robust evaluation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deploy the model**: If the model meets the desired performance criteria,
    deploy it for real-world use. This involves integrating the model into your application
    or system and ensuring its compatibility with the production environment. Implement
    mechanisms to monitor and maintain the model’s performance over time. The ultimate
    objective of building an ML application is to solve a problem, and this can only
    be achieved when the ML model is actively utilized in a production setting. Consequently,
    ML model deployment holds equal importance to the development phase. Deployment
    entails transitioning a trained ML model from an offline environment to integration
    within an existing production system, such as a live application. It represents
    a critical stage that must be completed to enable the model to fulfill its intended
    purpose and address the challenges it was designed for. Deployments establish
    an online learning framework where the model is continuously updated with new
    data, enabling ongoing improvement. The precise process of deploying an ML model
    varies depending on factors such as the system environment, model type, and organizational
    DevOps practices. Nonetheless, the general deployment process, particularly in
    a containerized environment, can be summarized into four key steps, which will
    be elaborated upon later.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterate and improve**: At last, we can shift our attention to the final tasks.
    After confirming the functionality of the model and assessing its performance,
    we are now prepared to thoroughly analyze the results to identify any areas that
    can be further enhanced or optimized. This stage involves a comprehensive examination
    of the model’s performance metrics, exploring potential shortcomings, and seeking
    opportunities for improvement. By delving into the analysis, we can gain valuable
    insights that will guide us in refining the model, addressing any limitations,
    and maximizing its efficacy. This meticulous evaluation process is vital for achieving
    a high-performing and robust ML solution. ML is an iterative process. Continuously
    gather feedback, monitor the model’s performance, and collect new data to retrain
    and refine the model. As new insights are gained or requirements change, update
    the model accordingly to ensure it remains effective and accurate.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By following these steps, you can systematically build ML models and iteratively
    improve their performance to solve complex problems and make accurate predictions
    or decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, our basic understanding of ML and how to choose the learning paradigm and
    algorithm that best fits our data is adequate. The next step involves an initial
    exploration of the resources that the MATLAB environment makes available to us
    to build our ML-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring MATLAB toolboxes for ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have acquired knowledge about the functions and capabilities
    of ML algorithms. We have also gained an understanding of how to identify various
    types of algorithms, select the appropriate solution for our requirements, and
    establish an effective workflow. Now, it is time to delve into the process of
    executing these tasks within the MATLAB environment.
  prefs: []
  type: TYPE_NORMAL
- en: With MATLAB, the process of solving ML problems becomes remarkably straightforward.
    The comprehensive set of tools and functionalities provided by MATLAB empowers
    users to leverage various algorithms and techniques effortlessly. Whether you
    are a beginner or an experienced practitioner, MATLAB equips you with the necessary
    resources to dive into the world of ML with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: MATLAB is a software platform specifically designed to address scientific problems
    and facilitate design processes. It offers an integrated environment where calculations,
    visualizations, and programming seamlessly come together. This user-friendly environment
    allows problems and their solutions to be expressed using familiar mathematical
    notation.
  prefs: []
  type: TYPE_NORMAL
- en: The name MATLAB is an acronym for **Matrix Laboratory**, highlighting its initial
    focus on providing convenient access to matrix-related software. Over the years,
    MATLAB has evolved through valuable user feedback and input. The MATLAB programming
    language is centered around matrices, which naturally represent computational
    mathematics. This matrix-based approach allows for an intuitive and efficient
    representation of mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: The desktop environment of MATLAB encourages experimentation, exploration, and
    discovery. Its integrated graphics capabilities facilitate clear visualization
    and enable users to gain a comprehensive understanding of the underlying data.
    With MATLAB, users are empowered to engage in a wide range of scientific and computational
    tasks with ease and confidence.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, MATLAB is known for its extensive collection of specialized solutions
    known as toolboxes. These toolboxes offer specific solutions to application problems
    and serve as invaluable resources for a wide range of users. They provide the
    necessary foundations and tools to apply MATLAB’s capabilities to various specialized
    domains and technologies. Each toolbox consists of a curated set of MATLAB functions,
    commonly referred to as M-files, which extend the functionality of the MATLAB
    environment. These toolboxes cater to specific problem domains, enabling users
    to solve complex and specialized challenges with ease, making MATLAB a versatile
    and comprehensive platform for a diverse range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s see some toolboxes that offer us important resources to be able to
    adequately build our applications based on ML.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics and Machine Learning Toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The toolbox encompasses a wide range of functions and applications tailored
    for data description, analysis, and modeling. It facilitates exploratory data
    analysis through descriptive statistics, visualizations, and clustering techniques.
    Probability distributions can be fitted to data, random numbers can be generated
    for Monte Carlo simulations, and hypothesis tests can be performed. The toolbox
    includes classification and regression algorithms, allowing for data inference
    and the construction of predictive models. Both interactive methods, using the
    Regression Learner and Classification Learner applications, and programmatically
    through AutoML can be employed.
  prefs: []
  type: TYPE_NORMAL
- en: For feature extraction and analysis of multidimensional data, the toolbox provides
    techniques such as **principal component analysis** (**PCA**), feature selection
    techniques, dimensionality reduction methods, and regularization. These methods
    assist in identifying variables with the most significant predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: The toolbox encompasses a diverse set of unsupervised, supervised, and semi-supervised
    ML algorithms, including **support vector machines** (**SVMs**), boosted decision
    trees, shallow neural networks, and k-means clustering. Interpretability techniques,
    such as partial dependency graphs, Shapley, and LIME values, are available, and
    C/C++ code can be automatically generated for embedded deployment. Native Simulink
    building blocks enable the integration of predictive models with model-based design
    and simulations. Moreover, many algorithms within the toolbox are designed to
    handle datasets that are too large to be stored entirely in memory, allowing for
    efficient processing of large-scale data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This tool offers a comprehensive set of features that cover key areas in the
    field of ML. Here are the main topics and features it encompasses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression techniques**: This includes a wide range of regression models
    such as linear, generalized linear, nonlinear, robust, regularized, ANOVA, repeated
    measures, and mixed-effects models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Big data algorithms**: The tool provides algorithms specifically designed
    for big data analysis, including dimension reduction, descriptive statistics,
    k-means clustering, linear regression, logistic regression, and discriminant analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability distributions**: Both univariate and multivariate probability
    distributions are available, along with random and quasi-random number generators
    and Markov chain samplers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hypothesis tests and design of experiments** (**DOE**): The tool offers hypothesis
    tests for distributions, dispersion, and location. It also provides techniques
    for designing optimal, factorial, and response surface experiments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification Learner app and algorithms**: For supervised ML, the tool
    provides the Classification Learner app, which allows for interactive exploration
    and modeling. It also offers algorithms such as SVMs, boosted and bagged decision
    trees, k-nearest neighbor, Naïve Bayes, discriminant analysis, and Gaussian process
    regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised ML algorithms**: The tool includes unsupervised ML algorithms,
    such as k-means clustering, k-medoids, hierarchical clustering, Gaussian mixtures,
    and hidden Markov models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian optimization**: It provides Bayesian optimization techniques to
    search for optimal hyperparameters and fine-tune ML algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These features collectively make the tool a comprehensive solution for various
    ML tasks and enable users to effectively explore, analyze, and model data in diverse
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning Toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Deep Learning Toolbox is a powerful resource for designing and implementing
    deep neural networks. It provides a range of algorithms, pre-trained models, and
    applications to facilitate the development of deep learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key features of the Deep Learning Toolbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**ConvNet, CNN**) and **long short-term
    memory** (**LSTM**) **networks**: These network architectures enable classification
    and regression tasks on images, time series, and textual data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network architectures**: The toolbox supports various network architectures,
    including **generative adversarial networks** (**GANs**) and Siamese networks.
    These architectures can be built using automatic differentiation, custom training
    cycles, and shared weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Network Designer app**: With the Deep Network Designer app, users can
    design, analyze, and train networks using a graphical interface. This allows intuitive
    network exploration and customization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experiment Manager app**: The Experiment Manager app simplifies the oversight
    of numerous deep learning experiments. It assists in monitoring training metrics,
    evaluating outcomes, and contrasting code across various experiments, delivering
    an efficient workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other frameworks**: The Deep Learning Toolbox offers seamless
    integration with popular deep learning frameworks such as TensorFlow 2, TensorFlow-Keras,
    PyTorch, **Open Neural Network Exchange** (**ONNX**), and Caffe. This allows for
    the easy import and export of networks, layer plots, and models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transfer learning and pre-trained models**: The toolbox supports transfer
    learning by providing access to pre-trained models such as DarkNet-53, ResNet-50,
    NASNet, and SqueezeNet. These models can be leveraged to expedite the development
    process and achieve excellent performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing a pre-trained image classification neural network allows you to leverage
    its already acquired knowledge of extracting meaningful features from natural
    images. This serves as a valuable starting point for learning a new task. ImageNet
    databases are usually used to train pre-trained neural networks; this is data
    largely employed in the **ImageNet Large-Scale Visual Recognition Challenge**
    (**ILSVRC**). These algorithms have been trained on a dataset comprising more
    than a million images and are capable of categorizing images into 1,000 distinct
    object categories. Adopting a pre-trained neural network and applying transfer
    learning is generally more efficient and less time-consuming compared to training
    a neural network from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: The Deep Learning Toolbox empowers users to work efficiently in deep learning
    projects, offering advanced capabilities, visualization tools, and compatibility
    with various frameworks and pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement Learning Toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Reinforcement Learning Toolbox offers an application, functions, and a Simulink
    block designed for training policies using reinforcement learning algorithms.
    These algorithms can be utilized to develop controllers and decision-making algorithms
    for complex applications such as resource allocation, robotics, and autonomous
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: With this toolbox, you can represent policies and value functions using deep
    neural networks or look-up tables and train them by interacting with environments
    modeled in MATLAB or Simulink. The reinforcement learning algorithms provided
    in the toolbox can be evaluated with one or more agents included, or you can create
    your own custom agents. You have the flexibility to experiment with hyperparameter
    settings, monitor training progress, and simulate trained agents interactively
    using the application or programmatically. To enhance training performance, simulations
    can be executed in parallel across multiple CPUs, GPUs, computer clusters, and
    the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the ONNX template format allows you to import existing policies
    from popular deep learning frameworks such as TensorFlow Keras and PyTorch (with
    the Deep Learning Toolbox). You can also generate optimized C, C++, and CUDA code
    to deploy trained policies on microcontrollers and GPUs. The toolbox includes
    reference examples to assist you in getting started on your reinforcement learning
    journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of reinforcement learning, the environment plays a crucial role
    as it models the dynamics in which the agent operates. The environment follows
    these key principles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It receives actions from the agent**: The agent selects and sends actions
    to the environment based on its policy or decision-making strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It generates observations**: The environment produces observations that reflect
    the state of the system or environment after the agent’s action. These observations
    capture the dynamic behavior and provide feedback to the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It provides rewards**: The environment assigns rewards to the agent based
    on the outcomes of its actions. Rewards measure how well the agent’s action contributes
    to achieving the desired task or goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In MATLAB, you have the flexibility to create both predefined and custom environments
    for your reinforcement learning scenarios. This allows you to model the specific
    dynamics and interactions that your agent will encounter in the learning process.
    With MATLAB, you can define the rules, constraints, and characteristics of the
    environment, enabling you to tailor it to your specific problem domain.
  prefs: []
  type: TYPE_NORMAL
- en: Computer Vision Toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Computer Vision Toolbox in MATLAB offers a wide range of capabilities for
    designing and testing computer vision, 3D vision, and video processing systems.
    Here are the key features and functions it provides:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Object detection and tracking**: You can use algorithms and functions to
    detect and track objects in images or videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature detection, extraction, and matching**: The toolbox includes functions
    for detecting and extracting features from images, as well as matching features
    across multiple images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Camera calibration**: Automation workflows are available for calibrating
    single, stereo, and fisheye cameras, ensuring accurate measurements and geometric
    transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3D vision**: The toolbox provides assistance for various tasks, including
    stereo vision, point cloud processing, structure from motion, and visual **simultaneous
    localization and** **mapping** (**SLAM**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Computer vision apps**: The toolbox offers user-friendly apps for automating
    ground truth labeling and camera calibration workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning-based object detection and segmentation**: This tool allows
    you to detect objects using deep learning algorithms such as SSD, YOLO, and ACF.
    Furthermore, it enables semantic and instance segmentation using deep learning
    algorithms such as Mask R-CNN and U-Net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large image support**: The toolbox includes algorithms that can handle images
    too large to fit into memory, enabling analysis of high-resolution or gigapixel
    images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance acceleration**: You can leverage the power of multicore processors
    and GPUs to accelerate your computer vision algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: MATLAB’s code generation capabilities allow you to generate
    C/C++ code for integrating with existing code, prototyping on desktop systems,
    and deploying algorithms on embedded vision systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the Computer Vision Toolbox you have a comprehensive set of tools and functions
    to tackle various computer vision tasks, from basic image processing to advanced
    deep learning-based object detection and segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Text Analytics Toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Text Analytics Toolbox in MATLAB provides a range of algorithms and visualizations
    for preprocessing, analyzing, and shaping text data. It is designed to support
    various applications, including sentiment analysis, language modeling, machine
    translation, and topic modeling. Here are the key features of the Text Analytics
    Toolbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text preprocessing**: You can use the toolbox to process raw text data extracted
    from diverse sources such as equipment logs, news feeds, surveys, operator reports,
    and social media. The toolbox provides tools for extracting text from popular
    file formats and performing preprocessing tasks such as cleaning, tokenization,
    and stopword removal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text extraction and transformation**: With the toolbox, you can extract single
    words or n-grams from text data and convert them into numeric representations
    suitable for analysis. This enables you to transform raw text into structured
    and meaningful features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical modeling**: The Text Analytics Toolbox offers ML techniques such
    as **latent semantic analysis** (**LSA**), **latent Dirichlet allocation** (**LDA**),
    and **word embedding** to uncover patterns, clusters, and topics within large
    text datasets. These models help in understanding the underlying structure and
    content of the text data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other data sources**: Features generated using the Text
    Analytics Toolbox have the capability to be merged with features obtained from
    different data sources, including numerical or categorical data. This integration
    allows you to develop comprehensive ML models that leverage multiple data types
    for improved performance and insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Text Analytics Toolbox provides a comprehensive set of tools for processing
    and analyzing text data. By leveraging ML techniques and combining text features
    with other data sources, you can gain valuable insights and develop powerful predictive
    models for various applications.
  prefs: []
  type: TYPE_NORMAL
- en: After having explored the tools available in MATLAB to build ML-based applications,
    we can now analyze examples of applications already available in real life that
    make use of this technology. In this way, we will be able to have some interesting
    ideas and immediately start thinking about the possible reuse of the ML to adapt
    it to our work environment.
  prefs: []
  type: TYPE_NORMAL
- en: ML applications in real life
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML, as a modern innovation, has revolutionized numerous industrial and professional
    processes, enhancing various aspects of our daily lives. Intelligent systems powered
    by ML algorithms possess the capability to learn from historical data or past
    experiences. By leveraging this knowledge, ML applications can generate outcomes
    and insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fields of study in which ML is used cover many types of problems. The main
    ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The representation of knowledge and reasoning that aims to reproduce the way
    of reasoning of the human brain through the definition of symbolism and languages
    to create machines capable of performing automatic reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning and coordination dealing with the development of systems that, given
    an application domain, have the objective of predicting future results and making
    decisions to achieve these objectives and maximize their benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robotics, for studies related to the movement of mechanical parts for the creation
    of a sequence of actions that perform a specific task or that can react to a specific
    phenomenon, the manipulation or movement of objects, or the localization or construction
    of maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language processing, which concerns the automatic interpretation of
    natural language, both in written and spoken form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Artificial vision: the set of processes for processing and analyzing images,
    pattern recognition, and automatic analysis of the scene, using 2D and 3D technologies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ML has a wide range of real-life applications; here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Facial recognition**: Facial recognition is used in many applications, such
    as biometric authentication on smartphones, security monitoring in public places,
    and identifying individuals for investigative purposes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized recommendations**: Platforms such as Netflix, Amazon, and Spotify
    use ML algorithms to analyze user data and suggest personalized content or products
    based on their tastes and preferences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virtual assistance**: Virtual assistants such as Apple’s Siri, Amazon’s Alexa,
    and Google Assistant rely on ML algorithms to understand and answer users’ questions,
    as well as to provide information and support in daily activities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autonomous vehicles**: ML plays a key role in the development of autonomous
    vehicles. Autonomous driving systems use ML algorithms to analyze sensor data
    and make real-time decisions for navigation and road safety'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fraud detection**: Financial institutions use ML models to analyze spending
    patterns and customer transactions to identify suspicious behavior or fraudulent
    transactions and prevent fraud'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnostics**: ML is used to analyze large amounts of medical data,
    such as diagnostic images, patient records, and genetic data, to help doctors
    diagnose and treat diseases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizing supply chains**: Companies use ML algorithms to optimize inventory
    management operations, forecast customer demand, and optimize delivery times,
    improving efficiency and reducing costs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Emotion sensing**: Emotion analysis using ML algorithms is used in several
    applications, such as analyzing sentiments on social media, evaluating user responses
    to a product or service, and personalizing user experiences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of how ML is applied in real life. Its applications
    are continuously growing, and its impact is increasingly evident in different
    sectors.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to the summary next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on an exciting journey into the world of ML, exploring
    a range of popular algorithms to find the best fit for our specific needs. We
    learned the importance of conducting a preliminary analysis to determine the most
    suitable algorithm and gained insights into the step-by-step process of building
    ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we delved into the powerful capabilities of MATLAB for ML, including
    its support for classification, regression, clustering, and deep learning tasks.
    We discovered the convenience of using MATLAB apps for automated model training
    and code generation, streamlining our workflow.
  prefs: []
  type: TYPE_NORMAL
- en: We also introduced the Statistics and Machine Learning Toolbox and the Deep
    Learning Toolbox, which provided us with additional tools and functionalities
    to solve our specific problems. We recognized the significance of statistics and
    algebra in the field of ML and understood how MATLAB could assist us in leveraging
    these concepts effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead to the next chapter, we will focus on seamlessly interacting with
    the MATLAB workspace. We will explore techniques for importing and organizing
    our data within MATLAB, as well as methods for exporting data from the workspace.
    Moreover, we will learn how to properly format and organize our data for the subsequent
    stages of data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: By mastering these essential skills, we will be well equipped to tackle data
    analysis challenges and make the most of MATLAB’s capabilities in the upcoming
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the next chapter, we will learn how to import and export data into MATLAB.
    We will understand how to work with different types of data and how to perform
    data wrangling. Additionally, we will understand exploratory statistics and exploratory
    visualization, data normalization, feature scaling, and data augmentation.
  prefs: []
  type: TYPE_NORMAL
