["```py\n# determining random state for data split and model initializationrandom_state = 42\n# loading and splitting digit data to train and test sets\ndigits = datasets.load_digits()\nx = digits.data\ny = digits.target\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, random_state= random_state, test_size=0.2)\n# list of hyperparameters to use for tuning\nparameter_grid = {\"max_depth\": [2, 5, 10, 15, 20],\n    \"min_samples_split\": [2, 5, 7]}\n# validating using stratified k-fold (k=5) cross-validation\nstratified_kfold_cv = StratifiedKFold(\n    n_splits = 5, shuffle=True, random_state=random_state)\n# generating the grid search\nstart_time = time.time()\nsklearn_gridsearch = GridSearchCV(\n    estimator = RFC(n_estimators = 10,\n        random_state = random_state),\n    param_grid = parameter_grid, cv = stratified_kfold_cv,\n    n_jobs=-1)\n# fitting the grid search cross-validation\nsklearn_gridsearch.fit(x_train, y_train)\n```", "```py\n# generating the grid searchstart_time = time.time()\nsklearn_randomsearch = RandomizedSearchCV(\n    estimator = RFC(n_estimators = 10,\n        random_state = random_state),\n    param_distributions = parameter_grid,\n    cv = stratified_kfold_cv, random_state = random_state,\n    n_iter = 5, n_jobs=-1)\n# fitting the grid search cross-validation\nsklearn_randomsearch.fit(x_train, y_train)\n```", "```py\nstart_time = time.time()tune_bayessearch = TuneSearchCV(\n    RFC(n_estimators = 10, random_state = random_state),\n    parameter_grid,\n    search_optimization=\"bayesian\",\n    cv = stratified_kfold_cv,\n    n_trials=3, # number of sampled parameter settings\n    early_stopping=True,\n    max_iters=10,\n    random_state = random_state)\ntune_bayessearch.fit(x_train, y_train)\n```", "```py\ndef plot_fun(x_plot: list, y_plot: list, title: str):    \"\"\"\n    Plotting a binary classification dataset\n    :param x_plot: list of x coordinates (i.e. dimension 1)\n    :param y_plot: list of y coordinates (i.e. dimension 2)\n    :param title: title of plot\n    \"\"\"\n    cmap, norm = mcolors.from_levels_and_colors([0, 1, 2],\n        ['black', 'red'])\n    plt.scatter([x_plot[iter][0] for iter in range(\n        0, len(x_plot))],\n        [x_plot[iter][1] for iter in range(\n            0, len(x_plot))],\n        c=y_plot, cmap=cmap, norm=norm)\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.xlabel('1st dimension', fontsize = 12)\n    plt.ylabel('2nd dimension', fontsize = 12)\n    plt.title(title)\n    plt.show()\n```", "```py\nnp.random.seed(12)minority_sample_size = 10\nmajority_sample_size = 100\n# generating random set of x coordinates\ngroup_1_X1 = np.repeat(2,majority_sample_size)+\\\nnp.random.normal(loc=0, scale=1,size=majority_sample_size)\ngroup_1_X2 = np.repeat(2,majority_sample_size)+\\\nnp.random.normal(loc=0, scale=1,size=majority_sample_size)\n# generating random set of x coordinates\ngroup_2_X1 = np.repeat(4,minority_sample_size)+\\\nnp.random.normal(loc=0, scale=1,size=minority_sample_size)\ngroup_2_X2 = np.repeat(4,minority_sample_size)+\\\nnp.random.normal(loc=0, scale=1,size=minority_sample_size)\nX_all = [[group_1_X1[iter], group_1_X2[iter]] for\\\n            iter in range(0, len(group_1_X1))]+\\\n            [[group_2_X1[iter], group_2_X2[iter]]\\\n              for iter in range(0, len(group_2_X1))]\ny_all = [0]*majority_sample_size+[1]*minority_sample_size\n# plotting the randomly generated data\nplot_fun(x_plot = X_all, y_plot = y_all,\n    title = 'Original')\n```", "```py\nk_neighbors = 5# initializing smote\n# using 'auto', equivalent to 'not majority',\n# sampling_strategy that enforces resampling all classes but the majority class\nsmote = SMOTE(sampling_strategy='auto',\n                    k_neighbors=k_neighbors)\n# fitting smote to oversample the minority class\nx_smote, y_smote = smote.fit_resample(X_all, y_all)\n# plotting the resulted oversampled data\nplot_fun(x_plot = x_smote, y_plot = y_smote,\n    title = 'SMOTE')\n```", "```py\nk_neighbors = 5# using 5 neighbors to determine if a minority sample is in \"danger\"\nm_neighbors = 10\n# initializing borderline smote\n# using 'auto', equivalent to 'not majority', sampling_strategy that enforces resampling all classes but the majority class\nborderline_smote = BorderlineSMOTE(\n    sampling_strategy='auto',\n    k_neighbors=k_neighbors,\n    m_neighbors=m_neighbors)\n# fitting borderline smote to oversample the minority class\nx_bordersmote,y_bordersmote =borderline_smote.fit_resample(\n    X_all, y_all)\n# plotting the resulted oversampled data\nplot_fun(x_plot = x_bordersmote, y_plot = y_bordersmote,\n    title = 'Borderline-SMOTE')\n```", "```py\n# using 5 neighbors for each datapoint in the oversampling process by SMOTEn_neighbors = 5\n# initializing ADASYN\n# using 'auto', equivalent to 'not majority', sampling_strategy that enforces resampling all classes but the majority class\nadasyn_smote = ADASYN(sampling_strategy = 'auto',n_neighbors                                         = n_neighbors)\n# fitting ADASYN to oversample the minority class\nx_adasyn_smote, y_adasyn_smote = adasyn_smote.fit_resample(X_all, y_all)\n# plotting the resulted oversampled data\nplot_fun(x_plot = x_adasyn_smote, y_plot = y_adasyn_smote,\n    title = \"ADASYN\")\n```", "```py\nn_samples, n_outliers = 100, 20rng = np.random.RandomState(12)\n# Generating two synthetic clusters of datapoints sampled from a univariate \"normal\" (Gaussian) distribution of mean 0 and variance 1\ncluster_1 = 0.2 * rng.randn(n_samples, 2) + np.array(\n    [1, 1])\ncluster_2 = 0.3 * rng.randn(n_samples, 2) + np.array(\n    [5, 5])\n# Generating synthetic outliers\noutliers = rng.uniform(low=2, high=4, size=(n_outliers, 2))\nX = np.concatenate([cluster_1, cluster_2, outliers])\ny = np.concatenate(\n    [np.ones((2 * n_samples), dtype=int),\n        -np.ones((n_outliers), dtype=int)])\n```", "```py\n# initializing iForestclf = IsolationForest(n_estimators = 10, random_state=10)\n# fitting iForest using training data\nclf.fit(X)\n# plotting the results\nscatter = plt.scatter(X[:, 0], X[:, 1])\nhandles, labels = scatter.legend_elements()\ndisp = DecisionBoundaryDisplay.from_estimator(\n    clf,\n    X,\n    plot_method = \"contour\",\n    response_method=\"predict\",\n    alpha=1\n)\ndisp.ax_.scatter(X[:, 0], X[:, 1], s = 10)\ndisp.ax_.set_title(\"Binary decision boundary of iForest (\n    n_estimators = 10)\")\nplt.xlabel('Dimension 1', fontsize = 12)\nplt.ylabel('Dimension 2', fontsize = 12)\nplt.show()\n```", "```py\nrandom_state = 42# loading and splitting digit data to train and test sets\ndigits = datasets.load_digits()\nx = digits.data\ny = digits.target\n# using stratified k-fold (k=5) cross-validation\nstratified_kfold_cv = StratifiedKFold(n_splits = 5,\n    shuffle=True, random_state=random_state)\n# function for plotting the CV score across different hyperparameter values\ndef reg_search_plot(search_fit, parameter: str):\n    \"\"\"\n    :param search_fit: hyperparameter search object after model fitting\n    :param parameter: hyperparameter name\n    \"\"\"\n    parameters = [search_fit.cv_results_[\n        'params'][iter][parameter] for iter in range(\n            0,len(search_fit.cv_results_['params']))]\n    mean_test_score = search_fit.cv_results_[\n        'mean_test_score']\n    plt.scatter(parameters, mean_test_score)\n    plt.xticks(fontsize = 12)\n    plt.yticks(fontsize = 12)\n    plt.xlabel(parameter, fontsize = 12)\n    plt.ylabel('accuracy', fontsize = 12)\n    plt.show()\n```", "```py\n# Defining hyperparameter gridparameter_grid = {\"alpha\": [0, 0.1, 0.2, 0.3, 0.4, 0.5]}\n# generating the grid search\nlasso_search = GridSearchCV(Lasso(\n    random_state = random_state),\n    parameter_grid,cv = stratified_kfold_cv,n_jobs=-1)\n# fitting the grid search cross-validation\nlasso_search.fit(x, y)\nreg_search_plot(search_fit = lasso_search,\n    parameter = 'alpha')\n```", "```py\n# Defining hyperparameter gridparameter_grid = {\"C\": [0.01, 0.2, 0.4, 0.6, 0.8, 1]}\n# generating the grid search\nsvc_search = GridSearchCV(SVC(kernel = 'poly',\n    random_state = random_state),parameter_grid,\n    cv = stratified_kfold_cv,n_jobs=-1)\n# fitting the grid search cross-validation\nsvc_search.fit(x, y)\nreg_search_plot(search_fit = svc_search, parameter = 'C')\n```"]