<html><head></head><body>
		<div id="_idContainer190">
			<h1 id="_idParaDest-208" class="chapter-number"><a id="_idTextAnchor207"/>12</h1>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor208"/>Genomics</h1>
			<p><strong class="bold">Genomics</strong> is the study of an organism’s genome or genetic material. In humans, genetic materials<a id="_idIndexMarker822"/> are stored in the form of <strong class="bold">Deoxyribonucleic Acid</strong> (<strong class="bold">DNA</strong>). These are the instructions that make up a human being, and 99.9% of the genomes in humans are identical and only 0.1% is different, which accounts for the differences in physical characteristics, such as eye color. Most of these variations are harmless, but some variants can cause health conditions, such as sickle cell anemia. Therefore, the analysis of such information can be used to predict or prevent a disease or provide personalized<a id="_idIndexMarker823"/> treatment, also known as <strong class="bold">precision medicine</strong>. There are four chemical bases present in DNA, namely <strong class="bold">adenine</strong> (<strong class="bold">A</strong>), <strong class="bold">thymine</strong> (<strong class="bold">T</strong>), <strong class="bold">cytosine</strong> (<strong class="bold">C</strong>), and <strong class="bold">guanine</strong> (<strong class="bold">G</strong>). They always bond in a particular manner; for example, adenine will always bond with thymine, and cytosine with guanine. The combination of these chemical bases is what makes up a <span class="No-Break">DNA sequence.</span></p>
			<p><strong class="bold">Sequencing</strong> is at the heart <a id="_idIndexMarker824"/>of genomics. To understand what it means, the <strong class="bold">Human Genome Project</strong> (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6875757/pdf/arhw-19-3-190.pdf">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6875757/pdf/arhw-19-3-190.pdf</a>) was started in 1989 with the objective of sequencing one human genome within 15 years. This was completed in 12 years in 2001 and involved thousands of scientists. With the development of next-generation sequencing technology, the whole human genome can now be generated in about a day. A single human genome is around 3 billion base pairs long; a similar size has been seen for other organisms such as a mouse or <span class="No-Break">a cow.</span></p>
			<p>Since the time and cost of generating the genome sequence have significantly dropped, it has led to the generation of an enormous amount of data. So, in order to analyze this magnitude of data, we need powerful machines and a large amount of cost-effective storage. The good news is that DNA sequencing data is publicly available, and one of the largest repositories is the <strong class="bold">National Center for Biotechnology Information</strong> (<strong class="bold">NCBI</strong>). We can use statistical and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models to gain insights from the genome data, which can be compute intensive. This poses two major challenges: big data and a massive ML model are required to make predictions, such as the prediction of promoters or predicting the masked <span class="No-Break">DNA sequence.</span></p>
			<p>Therefore, this chapter will help navigate these challenges by covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Managing large genomics data <span class="No-Break">on AWS</span></li>
				<li>Designing architecture <span class="No-Break">for genomics</span></li>
				<li>Applying ML <span class="No-Break">to genomics</span></li>
			</ul>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor209"/>Technical requirements</h1>
			<p>You should have the following prerequisites before getting started with <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>A web browser (for the best experience, it is recommended that you use a Chrome or <span class="No-Break">Firefox browser)</span></li>
				<li>Access to the AWS account that you used in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span></li>
				<li>Access to the SageMaker Studio development environment that we created in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span></li>
				<li>Example Jupyter notebooks for this chapter are provided in the companion GitHub <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06</span></a><span class="No-Break">)</span></li>
			</ul>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor210"/>Managing large genomics data on AWS</h1>
			<p>Apart from the large size<a id="_idIndexMarker825"/> of the genomics dataset, other challenges for managing it include discoverability, accessibility, availability, and <a id="_idIndexMarker826"/>storing it in a storage system that allows for scalable data processing while keeping the critical data safe. <em class="italic">The responsible and secure sharing of genomic and health data is key to accelerating research and improving human health</em>, is a stated objective of the <strong class="bold">Global Alliance for Genomics and Health</strong> (<strong class="bold">GA4GH</strong>). This <a id="_idIndexMarker827"/>approach requires two important things: one is a deep technical understanding of the domain, and the second is access to compute and storage resources.  You can also find many genomics datasets hosted by AWS on the <strong class="bold">Registry of Open Data on </strong><span class="No-Break"><strong class="bold">AWS</strong></span><span class="No-Break"> (</span><a href="https://registry.opendata.aws/"><span class="No-Break">https://registry.opendata.aws/</span></a><span class="No-Break">).</span></p>
			<p>Before you can begin any processing <a id="_idIndexMarker828"/>on the genomics dataset using cloud services, you need to make sure that it’s transferred and stored on the AWS cloud. For storing data, we recommend using <strong class="bold">Amazon Simple Storage Services</strong> (<strong class="bold">Amazon S3</strong>), as genomics data produced by next-generation sequencers are persisted in files, and a lot of genomic data analysis tools also take files as inputs and write the output back as files. For example, using an ML <a id="_idIndexMarker829"/>model for data analysis might involve taking large DNA sequence files as<a id="_idIndexMarker830"/> input and storing the inference or prediction results in a file, for which Amazon S3 makes a <span class="No-Break">natural fit.</span></p>
			<p>You can store genomics data securely by <a id="_idIndexMarker831"/>enabling server-side encryption with either <strong class="bold">Amazon S3-managed encryption keys</strong> (<strong class="bold">SSE-S3</strong>) or <strong class="bold">AWS Key Management Service</strong> (<strong class="bold">AWS KMS</strong>) keys. Moreover, Amazon S3 also allows you to enable the data life cycle by storing the infrequently accessed data in the <strong class="bold">Amazon S3 Standard-Infrequent Access</strong> (<strong class="bold">S3 Standard-IA</strong>) class tier or archiving the data<a id="_idIndexMarker832"/> to a low-cost storage option, such as <strong class="bold">Amazon S3 Glacier Deep Archive</strong> when the data is not in use to significantly reduce the cost. This pattern is discussed in detail in the <em class="italic">Tiered storage for cost optimization</em> section of <a href="B18493_04.xhtml#_idTextAnchor074"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <span class="No-Break"><em class="italic">Data Storage.</em></span></p>
			<p>For transferring genomics data to Amazon S3, you can use the AWS DataSync service, as discussed in <a href="B18493_02.xhtml#_idTextAnchor035"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Data Management </em><span class="No-Break"><em class="italic">and Transfer.</em></span></p>
			<p>Let’s take a closer look at the detailed architecture for applying ML models to the <span class="No-Break">genomics dataset.</span></p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor211"/>Designing architecture for genomics</h1>
			<p>In this section, we will describe a sample reference architecture for transferring, storing, processing, and <a id="_idIndexMarker833"/>gaining insights on genomics datasets on the AWS cloud, in a secure and cost-effective manner. <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.1</em> shows a sample genomics data <span class="No-Break">processing workflow:</span></p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B18493_12_001.jpg" alt="Figure 12.1 – Genomics data processing workflow"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Genomics data processing workflow</p>
			<p><span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.1</em> shows the <span class="No-Break">following workflow:</span></p>
			<ol>
				<li>A scientist or a lab technician will collect sample genomic data, for example, skin cells, prepare it in <a id="_idIndexMarker834"/>a lab, and then load it into <span class="No-Break">a sequencer.</span></li>
				<li>The sequencer will then generate a sequence, which might be short DNA fragments. These are<a id="_idIndexMarker835"/> usually called <strong class="bold">reads</strong> because you are <span class="No-Break">reading DNA.</span></li>
				<li>The DNA sequence is stored in an on-premises data <span class="No-Break">storage system.</span></li>
				<li>The AWS DataSync service will then transfer the genomic data securely to the cloud; for further details, refer to <a href="B18493_02.xhtml#_idTextAnchor035"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Data Management </em><span class="No-Break"><em class="italic">and Transfer</em></span><span class="No-Break">.</span></li>
				<li>The raw genomic data is then stored on Amazon S3. You can use AWS Analytics tools for <span class="No-Break">data processing.</span></li>
				<li>Amazon Genomics CLI is a purpose-built open source tool for processing raw genomics data in the cloud at a petabyte scale. For details, please refer to this <span class="No-Break">link: </span><a href="https://aws.amazon.com/genomics-cli/"><span class="No-Break">https://aws.amazon.com/genomics-cli/</span></a><span class="No-Break">.</span></li>
				<li>Optionally, we recommend storing the processed genomics data on <strong class="bold">Amazon Feature Store</strong>, which is a fully managed service for storing, sharing, versioning, and managing ML features for training and inference to enable the reuse of features across <span class="No-Break">ML applications.</span></li>
				<li>You can add granular access control policies on genomics data stored on Amazon S3 or Amazon Feature Store by using the <strong class="bold">AWS Lake Formation</strong> service, based on <a id="_idIndexMarker836"/>your business requirements. For details on AWS Lake Formation, refer to this <span class="No-Break">link: </span><a href="https://aws.amazon.com/lake-formation"><span class="No-Break">https://aws.amazon.com/lake-formation</span></a><span class="No-Break">.</span></li>
				<li>Once the data is processed and stored on either Amazon Feature Store or Amazon S3, you can run ML models such as <strong class="bold">DNABERT</strong> (<a href="https://www.biorxiv.org/content/10.1101/2020.09.17.301879v1">https://www.biorxiv.org/content/10.1101/2020.09.17.301879v1</a>) using <strong class="bold">Amazon SageMaker</strong> to gain further insights or to predict the masked DNA sequence. The ML model can take a batch of genomic data, make inferences, and store the results back in <span class="No-Break">Amazon S3.</span></li>
				<li>Additionally, you <a id="_idIndexMarker837"/>can archive the unused data to Amazon S3 Glacier Deep Archive to have significant cost savings on <span class="No-Break">data storage.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">A detailed discussion on Amazon Genomics CLI, AWS Lake Formation, and Amazon Feature Store is out of scope for this chapter; however, we will use the DNABERT model in the <em class="italic">Applying ML to genomics</em> section of <span class="No-Break">this chapter.</span></p>
			<p>Let’s learn how to apply the ML model to genomics applications and predict masked sequences in a DNA sequence using a pretrained <span class="No-Break">ML model.</span></p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor212"/>Applying ML to genomics</h1>
			<p>Before we dive into ML <a id="_idIndexMarker838"/>model details, let’s first understand the genomic data, which is stored as DNA in every organism. There are four chemical bases present in <a id="_idIndexMarker839"/>DNA, namely <strong class="bold">Adenine</strong> (<strong class="bold">A</strong>), <strong class="bold">Thymine</strong> (<strong class="bold">T</strong>), <strong class="bold">Cytosine</strong> (<strong class="bold">C</strong>) and <strong class="bold">Guanine</strong> (<strong class="bold">G</strong>). They always bond in particular manner for example, Adenine will always bond with Thymine, and Cytosine with Guanine. The combination of these chemical bases is what makes up a DNA sequence, represented by the letters A, T, C, and G. A 20-length example of a DNA sequence is <strong class="source-inline">ACTCCACAGTACCTCCGAGA</strong>. A single complete sequence of the human genome is around 3 billion <strong class="bold">base pairs</strong> (<strong class="bold">bp</strong>) long and takes about 200 GB of data <span class="No-Break">storage (</span><a href="https://www.science.org/doi/10.1126/science.abj6987"><span class="No-Break">https://www.science.org/doi/10.1126/science.abj6987</span></a><span class="No-Break">).</span></p>
			<p>However, for analyzing the DNA sequence, we don’t need the complete human genome sequence. Usually, we analyze a part of the human DNA; for example, to determine hair growth or skin growth, a<a id="_idIndexMarker840"/> lab technician will take a small section of human skin and prepare it to run <a id="_idIndexMarker841"/>through the next-generation sequencer, which will then read the DNA and generate the DNA sequence, which are short fragments of DNA. ML models can be used for various tasks, such <a id="_idIndexMarker842"/>as DNA classification, <strong class="bold">promoter recognition</strong>, interpretation of structural variation in human genomes, precision medicine, cancer research, and <span class="No-Break">so on.</span></p>
			<p>In this section, we will showcase how to fine-tune the DNABERT model using Amazon SageMaker for the proximal promoter recognition task. DNABERT is based on the BERT model fine-tuned on DNA sequences, as outlined in the research paper <em class="italic">Supervised promoter recognition: a benchmark framework</em> (<a href="https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-022-04647-5.pdf">https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-022-04647-5.pdf</a>). Therefore, let’s take an example of deploying a pretrained DNABERT model for promoter recognition using DNA sequence data on Amazon <span class="No-Break">SageMaker service.</span></p>
			<p>Amazon SageMaker is a fully managed service by AWS to assist ML practitioners in building, training, and deploying ML models. Although it provides features and an integrated development environment for each phase of ML, it is highly modular in nature, meaning if you already have a trained model, you can use the <strong class="bold">SageMaker hosting</strong> features to deploy the model for performing inference/predictions on your model. For details on the various types of deployment options provided by SageMaker, refer to <a href="B18493_07.xhtml#_idTextAnchor128"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Deploying Machine Learning Models </em><span class="No-Break"><em class="italic">at Scale</em></span><span class="No-Break">.</span></p>
			<p>We will deploy a version of the DNABERT pretrained model provided by <strong class="bold">Hugging Face</strong>, an AI community with a library of 65K+ pretrained transformer models. Amazon SageMaker provides first-party deep learning containers for Hugging Face for both training and inference. These containers include Hugging Face pretrained transformer models, tokenizers, and dataset libraries. For a list of all the available containers, you can refer to this link: <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md">https://github.com/aws/deep-learning-containers/blob/master/available_images.md</a>. These containers are regularly maintained and updated with security patches and remove undifferentiated heavy lifting for <span class="No-Break">ML practitioners.</span></p>
			<p>With a few lines of configuration code, you can deploy a pretrained model in the Hugging Face library on Amazon SageMaker, and start making predictions using your model. Amazon SageMaker provides <a id="_idIndexMarker843"/>a lot of features for deploying ML models. For example, in the case of <strong class="bold">real-time inference</strong>, when you choose to deploy the model as an API, set up an <strong class="bold">autoscaling policy</strong> to scale up and down the<a id="_idIndexMarker844"/> number of instances on which the model is deployed based on the number of invocations on the model. Additionally, you can do <strong class="bold">blue/green deployment</strong>, add <strong class="bold">security guardrails</strong>, <strong class="bold">auto-rollback,</strong> and so on, using Amazon SageMaker hosting features. For details on deploying models for inference, refer to this link: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html">https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html</a>. Now that we have understood the <a id="_idIndexMarker845"/>benefits of using Amazon SageMaker for deploying models and Hugging Face integration, let’s see how we can deploy a pretrained DNABERT model for <span class="No-Break">promoter recognition.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The full code for deploying the model is available on <span class="No-Break">GitHub: </span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter12/dnabert.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter12/dnabert.ipynb</span></a><span class="No-Break">.</span></p>
			<p>We need to follow three steps for deploying pretrained transformer models for real-time inference provided by the Hugging Face library on <span class="No-Break">Amazon SageMaker:</span></p>
			<ol>
				<li value="1">Provide model hub configuration where we supply the Hugging Face model ID and the task – in our case, <span class="No-Break">text classification.</span></li>
				<li>Create a <strong class="source-inline">HuggingFaceModel</strong> class provided by the SageMaker API, where we provide parameters such as the transformer version, PyTorch version, Python version, hub configuration, <span class="No-Break">and role.</span></li>
				<li>Finally, we use the <strong class="source-inline">deploy()</strong> API, where we supply the number of instances and the type of instance on which to deploy <span class="No-Break">the model.</span></li>
			</ol>
			<p>The following code snippet <a id="_idIndexMarker846"/>showcases the three steps that we <span class="No-Break">just outlined:</span></p>
			<pre class="source-code">
…
from sagemaker.huggingface import HuggingFaceModel
import sagemaker
role = sagemaker.get_execution_role()
# Step 1: Hub Model configuration. <a href="https://huggingface.co/models">https://huggingface.co/models</a>
hub = {
     'HF_MODEL_ID':'AidenH20/DNABERT-500down',
     'HF_TASK':'text-classification'
}
# Step 2: create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
     transformers_version='4.17.0',
     pytorch_version='1.10.2',
     py_version='py38',
     env=hub,
     role=role,
)
# Step 3: deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
     initial_instance_count=1, # number of instances
     instance_type='ml.m5.xlarge' # ec2 instance type
)
…</pre>
			<p>Using this code snippet, we basically tell SageMaker to deploy the Hugging Face model provided in <strong class="source-inline">'HF_MODEL_ID'</strong> for the task mentioned in <strong class="source-inline">'HF_TASK'</strong>; in our case, <strong class="source-inline">text classification</strong>, as we want to classify promoter regions by providing a DNA sequence. The <strong class="source-inline">HuggingFaceModel</strong> class defines the container on which the model will be deployed. Finally, the <strong class="source-inline">deploy()</strong> API launches the Hugging Face container defined by the <strong class="source-inline">HuggingFaceModel</strong> class and loads the model provided in the hub configuration<a id="_idIndexMarker847"/> to the initial number of instances and type <a id="_idIndexMarker848"/>of instances provided by the <span class="No-Break">ML practitioner.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The number of instances on which the model is deployed as an API can be updated even after the model <span class="No-Break">is deployed.</span></p>
			<p>Once the model is deployed, you can then use the <strong class="source-inline">predict()</strong> API provided by SageMaker to make inferences or predictions on the model, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
…
dna_sequence = 'CTAATC TAATCT AATCTA ATCTAG TCTAGT CTAGTA TAGTAA AGTAAT GTAATG TAATGC AATGCC ATGCCG TGCCGC GCCGCG CCGCGT CGCGTT GCGTTG CGTTGG GTTGGT TTGGTG TGGTGG GGTGGA GTGGAA TGGAAA GGAAAG GAAAGA AAAGAC AAGACA AGACAT GACATG ACATGA CATGAC ATGACA TGACAT GACATA ACATAC CATACC ATACCT TACCTC ACCTCA CCTCAA CTCAAA TCAAAC CAAACA AAACAG AACAGC ACAGCA CAGCAG AGCAGG GCAGGG CAGGGG AGGGGG GGGGGC GGGGCG GGGCGC GGCGCC GCGCCA CGCCAT GCCATG CCATGC CATGCG ATGCGC TGCGCC GCGCCA CGCCAA GCCAAG CCAAGC CAAGCC AAGCCC AGCCCG GCCCGC CCCGCA CCGCAG CGCAGA GCAGAG CAGAGG AGAGGG GAGGGT AGGGTT GGGTTG GGTTGT GTTGTC TTGTCC TGTCCA GTCCAA TCCAAC CCAACT CAACTC AACTCC ACTCCT CTCCTA TCCTAT CCTATT CTATTC TATTCC ATTCCT'
predictor.predict({
     'inputs': dna_sequence
})
…</pre>
			<p>The output will be the label with <a id="_idIndexMarker849"/>the highest probability. In our case, it’s either <strong class="source-inline">LABEL_0</strong> or <strong class="source-inline">LABEL_1</strong>, denoting the absence or presence of a promoter region in a <span class="No-Break">DNA sequence.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding code deploys the model as an API on a long-running instance, so if you are not using the endpoint, please make sure to delete it; otherwise, you will be charged <span class="No-Break">for it.</span></p>
			<p>You can also see the <a id="_idIndexMarker850"/>endpoint details on SageMaker Studio by clicking on an orange triangle icon (<strong class="bold">SageMaker resources</strong>) in the left navigation panel and selecting <strong class="bold">Endpoints</strong>, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B18493_12_002.jpg" alt="Figure 12.2 – Accessing Endpoints on SageMaker Studio"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Accessing Endpoints on SageMaker Studio</p>
			<p>This will show all the SageMaker endpoints (models deployed for real-time inference as an API). Double-clicking on the endpoint name will show you the details by calling the <strong class="source-inline">DescribeEndpoint()</strong> API behind the scenes. The SageMaker Studio UI shows you a lot of options, such as <strong class="bold">Test inference</strong>, <strong class="bold">Data quality</strong>, <strong class="bold">Model quality</strong>, <strong class="bold">Model explainability</strong>, <strong class="bold">Model bias</strong>, <strong class="bold">Monitoring job history</strong>, and <strong class="bold">AWS settings</strong>. The data is populated on these tabs based on the features you have enabled; for example, to understand the data and model quality, you need to enable the model monitoring feature of SageMaker, which <a id="_idIndexMarker851"/>monitors the models deployed as real-time endpoints on a schedule set by you, compares it with baseline statistics, and stores the report in S3. For details on model monitoring, refer to this <span class="No-Break">link: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html</span></a><span class="No-Break">.</span></p>
			<p>The <strong class="bold">AWS settings</strong> tab, on the other hand, will always be populated with model endpoint metadata, such as <a id="_idIndexMarker852"/>the endpoint name, type, status, creation time, last <a id="_idIndexMarker853"/>updated time, <strong class="bold">Amazon Resource Name</strong> (<strong class="bold">ARN</strong>), endpoint runtime settings, endpoint configuration settings, production variants (in case you have more than one variant of the same model), instance details (type and number of instances), model name, and lineage, as applicable. <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.3</em> shows some of the metadata associated with <span class="No-Break">the endpoint:</span></p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B18493_12_003.jpg" alt="Figure 12.3 – SageMaker endpoint details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – SageMaker endpoint details</p>
			<p>Also, if you quickly want to test <a id="_idIndexMarker854"/>your model from the SageMaker Studio UI, you can click on <strong class="bold">Test inference</strong>, provide a payload (input request) in JSON format in <strong class="bold">JSON editor</strong>, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.4</em>, and quickly see the response provided by <span class="No-Break">the model:</span></p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B18493_12_004.jpg" alt="Figure 12.4 – Test inference from the SageMaker Studio UI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Test inference from the SageMaker Studio UI</p>
			<p>Now that we understand<a id="_idIndexMarker855"/> how pretrained models on the Hugging Face model library can be deployed and tested on<a id="_idIndexMarker856"/> Amazon SageMaker, let’s take another example of how to fine-tune the pretrained model in the Hugging Face library, and deploy the fine-tuned model. For this section, we will use a BERT-based model trained on protein sequences known as <strong class="bold">ProtBERT</strong>, published in this research <span class="No-Break">paper: </span><a href="https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3"><span class="No-Break">https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3</span></a><span class="No-Break">.</span></p>
			<p>The study of protein structure, functions, and<a id="_idIndexMarker857"/> interactions is called <strong class="bold">proteomics</strong>, which takes the help of genomic studies as proteins are the functional product of the genome. Both proteomics and genomics are used to prevent diseases and are an active part of drug discovery. Although there are a lot of tasks that contribute to drug discovery, protein classification, and secondary structure identification play a vital role. In the following section, we will understand how to fine-tune a large protein model (the Hugging Face library) to predict the secondary structure of a protein using the distributed training features of <span class="No-Break">Amazon SageMaker.</span></p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor213"/>Protein secondary structure prediction for protein sequences</h2>
			<p>Protein sequences are made up of 20 essential <strong class="bold">amino acids</strong>, each represented by a capital letter. They combine to form a protein sequence, which you can use to do protein classification or predict the secondary structure of the protein, among other tasks. Protein sequences assume a <a id="_idIndexMarker858"/>particular 3D structure based on constraints, which are optimized for undertaking a particular function. You can think of these constraints as rules of grammar or meaning in natural language, which<a id="_idIndexMarker859"/> allows us to use <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) techniques to <span class="No-Break">protein sequences.</span></p>
			<p>In this section, we will focus on fine-tuning the <strong class="source-inline">prot_t5_xl_uniref50</strong> model, which has around <em class="italic">11 billion parameters</em>, and you can use the same training script to fine-tune a smaller <strong class="source-inline">prot_bert_bfd</strong> model, which has close to <em class="italic">420 million parameters</em>, with different configuration to accommodate the size of the model. The code for fine-tuning the <strong class="source-inline">prot_t5_xl_uniref50</strong> model is provided in the GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter12"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter12</span></a><span class="No-Break">.</span></p>
			<p>To create a SageMaker training job using the model in the Hugging Face library, we will need a Hugging Face estimator from SageMaker SDK. The estimator class will handle all the training tasks based on the configuration that we provide. For example, to train a Hugging Face model with the basic configuration, we will need to provide the <span class="No-Break">following parameters:</span></p>
			<ul>
				<li><strong class="source-inline">entry_point</strong>: This is where we will specify the training script for training <span class="No-Break">the model.</span></li>
				<li><strong class="source-inline">source_dir</strong>: This is the name of the folder where the training script or other helper files <span class="No-Break">will reside.</span></li>
				<li><strong class="source-inline">instance_type</strong>: This is the type of machine in the cloud where the training script <span class="No-Break">will run.</span></li>
				<li><strong class="source-inline">instance_count</strong>: This is the number of machines in the cloud that will be launched for running the training job. If the count is greater than <strong class="source-inline">1</strong>, then it will automatically launch <span class="No-Break">a cluster.</span></li>
				<li><strong class="source-inline">transfomer_version</strong>, <strong class="source-inline">pytorch_version</strong>, <strong class="source-inline">py_version</strong>: These determine the version of transformers, PyTorch, and Python that will be present in the container. Based on the<a id="_idIndexMarker860"/> value of these parameters, SageMaker will fetch the appropriate container, which will be provisioned onto the instances (machines) in <span class="No-Break">the cloud.</span></li>
				<li><strong class="source-inline">hyperparameters</strong>: This defines the named arguments that will be passed to the <span class="No-Break">training script.</span></li>
			</ul>
			<p>The following code snippet formalizes these parameters into a SageMaker <span class="No-Break">training job:</span></p>
			<pre class="source-code">
...
huggingface_estimator = HuggingFace(entry_point='train.py',
                           source_dir='./code',
                           instance_type='ml.g5.12xlarge',
                           instance_count=1,
                           role=role,
                           transformers_version='4.12',
                           pytorch_version='1.9',
                           py_version='py38',
                           hyperparameters=hyperparameters)
...</pre>
			<p>Once the estimator is defined, you can then provide the S3 location (path) of your data to start the training job. SageMaker provides some very useful environment variables for a training job, which include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">SM_MODEL_DIR</strong>: This provides the path where the training job will store the model artifacts, and once the job finishes, the stored model in this folder is directly uploaded to the S3 <span class="No-Break">output location.</span></li>
				<li><strong class="source-inline">SM_NUM_GPUS</strong>: This represents the number of GPUs available to <span class="No-Break">the host.</span></li>
				<li><strong class="source-inline">SM_CHANNEL_XXXX</strong>: This represents the input data path for the specified channel. For example, <strong class="source-inline">data</strong>, in our case, will correspond to the <strong class="source-inline">SM_CHANNEL_DATA</strong> environment variable, which is used by the training script, as shown in the following <span class="No-Break">code snippet:</span><pre class="source-code">
…</pre><pre class="source-code">
# starting the train job with our uploaded datasets as input</pre><pre class="source-code">
huggingface_estimator.fit({'data': data_input_path})</pre><pre class="source-code">
…</pre></li>
			</ul>
			<p>When we call the <strong class="source-inline">fit()</strong> method on the Hugging Face estimator, SageMaker will first provision the ephemeral compute environment, and copy the training script and data to the compute environment. It will then kick off the training, save the trained model to the S3 output location provided in<a id="_idIndexMarker861"/> the estimator class, and finally, tear down all the resources so that users only pay for the amount of time the training job was <span class="No-Break">running for.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information on the SageMaker Hugging Face estimator, as well as how to leverage the SageMaker SDK to instantiate the estimator, please see the AWS documentation (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html">https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html</a>), and the SageMaker SDK <span class="No-Break">documentation (</span><a href="https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-estimator"><span class="No-Break">https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-estimator</span></a><span class="No-Break">).</span></p>
			<p>We will extend this basic configuration to fine-tune <strong class="source-inline">prot_t5_xl_uniref50</strong> with <em class="italic">11 billion parameters</em> using the distributed training features <span class="No-Break">of SageMaker.</span></p>
			<p>Before we do a deeper dive into the code, let’s first understand some of the concepts that we will leverage for training the model. Since it’s a big model, the first step is to get an idea of the size of the model, which will help us determine whether it will fit in a single GPU memory or not. The SageMaker model parallel documentation (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html">https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html</a>) gives a good idea of estimating the size of the model. For a training job that uses <strong class="bold">automatic mixed precision</strong> (<strong class="bold">AMP</strong>) with a <strong class="bold">floating-point 16-bit</strong> (<strong class="bold">FP16</strong>) size, using Adam optimizers, we can calculate the memory required per parameter using the following breakdown, which comes to about <span class="No-Break">20 bytes:</span></p>
			<ul>
				<li>An FP16 parameter ~ 16 bits or <span class="No-Break">2 bytes</span></li>
				<li>An FP16 gradient ~ 16 bits or <span class="No-Break">2 bytes</span></li>
				<li>An FP32 optimizer state ~ 64 bits or 8 bytes based on the <span class="No-Break">Adam optimizers</span></li>
				<li>An FP32 copy of parameter ~ 32 bits or 4 bytes (needed for the <strong class="bold">optimizer apply</strong> (<span class="No-Break"><strong class="bold">OA</strong></span><span class="No-Break">) operation)</span></li>
				<li>An FP32 copy of gradient ~ 32 bits or 4 bytes (needed for the <span class="No-Break">OA operation)</span></li>
			</ul>
			<p>Therefore, our model with 11 billion parameters will require more than 220 GB of memory, which is larger than <a id="_idIndexMarker862"/>the typical GPU memory currently available on a single GPU. Even if we are able to get such a machine with GPU memory greater than 220 GB, it will not be cost-effective, and we won’t be able to scale our training job. Another constraint to understand here is the batch size, as we will need at least one batch of data in the memory to begin training. Using a smaller batch size will drive down the GPU utilization and degrade the training efficiency, as the model might not be able <span class="No-Break">to converge.</span></p>
			<p>Hence, we will have to partition our model into multiple GPUs, and in order to increase the batch size, we will also need to shard the data. So, we will use a hybrid approach that will utilize both data parallel and model parallel approaches. This approach has been explained in detail in <a href="B18493_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Distributed Training of Machine </em><span class="No-Break"><em class="italic">Learning Models</em></span><span class="No-Break">.</span></p>
			<p>Since our model size is 220 GB, we will have to use a mechanism in the training job, which will optimize it for memory in order to avoid <strong class="bold">out of memory</strong> (<strong class="bold">OOM</strong>) errors. For memory optimization, the SageMaker model parallel library provides two types of model parallelism, namely, pipeline parallelism and tensor parallelism, along with memory-saving techniques, such as activation checkpointing, activation offloading, and optimizer state sharding. Let’s understand each of <span class="No-Break">these terms:</span></p>
			<ul>
				<li><strong class="bold">Pipeline parallelism</strong>: This <a id="_idIndexMarker863"/>means partitioning the model into different GPUs. This goes as a parameter to the training job, and based on the parallelism degree provided, the library will create different partitions. For example, we are training the model with 4 GPUs, and if the <strong class="source-inline">partitions</strong> parameter value provided in the configuration is <strong class="source-inline">2</strong>, then the model will be divided into 2 partitions <a id="_idIndexMarker864"/>across 4 GPUs, meaning it will have 2 model replicas. Since, in this case, the number of GPUs is greater than the number of partitions, we will have to enable <strong class="bold">distributed data parallel</strong> by setting the <strong class="source-inline">ddp</strong> parameter to <strong class="source-inline">true</strong>. Otherwise, the training<a id="_idIndexMarker865"/> job will give an error. Pipeline parallelism with <strong class="source-inline">ddp</strong> enabled is illustrated in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B18493_12_005.jpg" alt="Figure 12.5 – Showcasing the hybrid approach with model and data parallelism"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Showcasing the hybrid approach with model and data parallelism</p>
			<ul>
				<li><strong class="bold">Tensor Parallelism</strong>: This applies <a id="_idIndexMarker866"/>the same concept as pipeline parallelism and goes a step further to divide the largest layer of our model and places it on different nodes. This concept is illustrated in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/B18493_12_006.jpg" alt="Figure 12.6 – Tensor parallelism shards tensors (layers) of the model across GPUs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Tensor parallelism shards tensors (layers) of the model across GPUs</p>
			<ul>
				<li><strong class="bold">Activation checkpointing</strong>: This helps in reducing memory usage by clearing out the activations of layers and <a id="_idIndexMarker867"/>computing it again during the backward pass. For any deep learning model, the data is first passed through the intermediary layers in the forward <a id="_idIndexMarker868"/>pass, which compute the outputs, also known as activations. These activations <a id="_idIndexMarker869"/>need to be stored as they are used for computing gradients during the backward pass. Now, storing activations for a large model in memory can increase memory usage significantly and can cause bottlenecks. In order to overcome this issue, an activation checkpointing or gradient checkpointing technique comes in handy, which clears out the activations of <span class="No-Break">intermediary layers.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">Activation checkpointing results in extra computation time in order to reduce <span class="No-Break">memory usage.</span></p>
			<ul>
				<li><strong class="bold">Activation offloading</strong>: This uses<a id="_idIndexMarker870"/> activation checkpointing, where it only keeps a few activations in the GPU memory during the model training. It offloads the checkpointed activations to CPU memory during the forward pass, which are loaded back to the GPU during the backward pass of a particular micro-batch <span class="No-Break">of data.</span></li>
				<li><strong class="bold">Optimizer state sharding</strong>: This is another useful memory-saving technique that partitions the state of the <a id="_idIndexMarker871"/>optimizers described by the set of weights across the data parallel device groups. It can be used only when we are using a stateful optimizer, such as Adam <span class="No-Break">or FP16.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">Since optimizer state sharding partitions the optimizer state across the data parallel device groups, it will only become useful if the data parallel degree is greater <span class="No-Break">than one.</span></p>
			<p>Another important concept to understand with model parallelism is the <strong class="bold">micro-batch</strong>, which is a smaller subset of the batch of<a id="_idIndexMarker872"/> data. During training, you pass a certain number of records of data forward and backward through the layers, known as a batch or sometimes a mini-batch. A full <a id="_idIndexMarker873"/>pass through your dataset is called an <strong class="bold">epoch</strong>. SageMaker<a id="_idIndexMarker874"/> model parallelism shards the batch into smaller subsets, which are called micro-batches. These <a id="_idIndexMarker875"/>micro-batches are then passed through the <strong class="bold">pipeline scheduler</strong> to increase GPU utilization. The pipeline scheduler is explained in detail in <a href="B18493_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>,<em class="italic"> Distributed Training of Machine </em><span class="No-Break"><em class="italic">Learning Models</em></span><span class="No-Break">.</span></p>
			<p>So, now that we understand the memory-saving techniques used by the SageMaker model parallel library let’s see how we can use them in the code. The following code snippet wraps all the memory-saving techniques together in a <span class="No-Break">simple manner:</span></p>
			<pre class="source-code">
…
# configuration for running training on smdistributed Model Parallel
mpi_options = {
    "enabled": True,
    "processes_per_host": 8,
}
smp_options = {
    "enabled":True,
    "parameters": {
        "microbatches": 1,
        "placement_strategy": "cluster",
        "pipeline": "interleaved",
        <strong class="bold">"optimize": "memory",</strong>
        <strong class="bold">"partitions": 4,</strong>
        "ddp": True,
        # "tensor_parallel_degree": 2,
        <strong class="bold">"shard_optimizer_state": True,</strong>
<strong class="bold">        "activation_checkpointing": True,</strong>
<strong class="bold">        "activation_strategy": "each",</strong>
<strong class="bold">        "activation_offloading": True,</strong>
    }
}
distribution = {
    "smdistributed": {"modelparallel": smp_options},
    "mpi": mpi_options
}
…</pre>
			<p>All the memory-saving <a id="_idIndexMarker876"/>techniques are highlighted in the code snippet, where you first have to make sure to set the <strong class="source-inline">optimize</strong> parameter to <strong class="source-inline">memory</strong>. This instructs the SageMaker model splitting algorithm to optimize for memory consumption. Once this is set, then you can simply enable other memory-saving features by setting their value <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">true</strong></span><span class="No-Break">.</span></p>
			<p>You will then supply the <strong class="source-inline">distribution</strong> configuration to the <strong class="source-inline">HuggingFace</strong> estimator class, as shown in<a id="_idIndexMarker877"/> the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
…
huggingface_estimator = HuggingFace(entry_point='train.py',
                           source_dir='./code',
                           instance_type='ml.g5.48xlarge',
                           instance_count=1,
                           role=role,
                           transformers_version='4.12',
                           pytorch_version='1.9',
                           py_version='py38',
                           <strong class="bold">distribution= distribution</strong>,
                           hyperparameters=hyperparameters)
huggingface_estimator.fit({'data': data_input_path})
…</pre>
			<p>As you can see, we also provide the <strong class="source-inline">train.py</strong> training script as <strong class="source-inline">entry_point</strong> to the estimator. In <a href="B18493_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Distributed Training of Machine Learning Models</em>, we understood that when we are using model parallel, we have to update our training script with SageMaker model parallel constructs. In this example, since we are using the <strong class="source-inline">HuggingFace</strong> estimator and the <strong class="source-inline">Trainer</strong> API for model training, it has built-in support for SageMaker model parallel. So, we simply import the Hugging Face <strong class="source-inline">Trainer</strong> API and provide configuration related to model training, and based on the model parallel configuration provided in the <strong class="source-inline">HuggingFace</strong> estimator, it will invoke the SageMaker model parallel constructs during <span class="No-Break">model training.</span></p>
			<p>In our <strong class="source-inline">train.py</strong> script, first, we need to import the <strong class="source-inline">Trainer</strong> module, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
…
from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments
from transformers.sagemaker import SageMakerTrainer as Trainer
…</pre>
			<p>Since we are training a T5-based BERT model (<strong class="source-inline">prot_t5_xl_uniref50</strong>) we will also need to import the <strong class="source-inline">T5Tokenizer</strong> and <strong class="source-inline">T5ForConditionalGeneration</strong> modules from the <span class="No-Break"><strong class="source-inline">transformers</strong></span><span class="No-Break"> library:</span></p>
			<pre class="source-code">
…
from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForTokenClassification, BertTokenizerFast, EvalPrediction, T5Tokenizer
…</pre>
			<p>The next step is to<a id="_idIndexMarker878"/> transform the protein sequences and load them into a PyTorch DataLoader. After we have the data in <strong class="source-inline">DataLoader</strong>, we will provide <strong class="source-inline">TrainingArguments</strong>, as shown in the following code snippet, which will be used by the <span class="No-Break"><strong class="source-inline">Trainer</strong></span><span class="No-Break"> API:</span></p>
			<pre class="source-code">
…
training_args = TrainingArguments(
    output_dir='./results',         # output directory
    num_train_epochs=2,              # total number of training epochs
    per_device_train_batch_size=1,   # batch size per device during training
    per_device_eval_batch_size=1,   # batch size for evaluation
    warmup_steps=200,                # number of warmup steps for learning rate scheduler
    learning_rate=3e-05,             # learning rate
    weight_decay=0.0,                # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=200,               # How often to print logs
    do_train=True,                   # Perform training
    do_eval=True,                    # Perform evaluation
    evaluation_strategy="epoch",     # evalute after each epoch
    gradient_accumulation_steps=32,  # total number of steps before back propagation
    fp16=True,                       # Use mixed precision
    fp16_opt_level="02",             # mixed precision mode
    run_name="ProBert-T5-XL",      # experiment name
    seed=3,                         # Seed for experiment reproducibility
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",
    greater_is_better=True,
    save_strategy="epoch",
    max_grad_norm=0,
    dataloader_drop_last=True,
    )
…</pre>
			<p>As you can see, <strong class="source-inline">TrainingArguments</strong> contains <a id="_idIndexMarker879"/>a list of hyperparameters, such as the number of epochs, learning rate, weight decay, evaluation strategy, and so on, which will be used for model training. For details on the different hyperparameters for the <strong class="source-inline">TrainingArguments</strong> API, you can refer to this URL: <a href="https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.TrainingArguments">https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.TrainingArguments</a>. Make sure that when you are providing <strong class="source-inline">TrainingArguments</strong>, the value of <strong class="source-inline">dataloader_drop_last</strong> is set to <strong class="source-inline">true</strong>. This will make sure that the batch size is <a id="_idIndexMarker880"/>divisible by the micro-batches, and setting <strong class="source-inline">fp16</strong> to <strong class="source-inline">true</strong> will use the automatic mixed precision for model training, which also helps reduce the memory footprint, as floating-point 16 takes 2 bytes to store <span class="No-Break">a parameter.</span></p>
			<p>Now, we will define the <strong class="source-inline">Trainer</strong> API, which takes <strong class="source-inline">TrainingArguments</strong> and the training and validation datasets <span class="No-Break">as input:</span></p>
			<pre class="source-code">
…
trainer = Trainer(
    model_init=model_init,                # the instantiated Transformers model to be trained
    <strong class="bold">args=training_args</strong>,                   # training arguments, defined above
    train_dataset=train_dataset,          # training dataset
    eval_dataset=val_dataset,             # evaluation dataset
    compute_metrics = compute_metrics,    # evaluation metrics
    )
…</pre>
			<p>Once the <strong class="source-inline">Trainer</strong> API has been defined, the model training is kicked off with the <strong class="source-inline">train()</strong> method, and once the training is complete, we will use the <strong class="source-inline">save_model()</strong> method to save the trained model to the specified <span class="No-Break">model directory:</span></p>
			<pre class="source-code">
…
trainer.train()
trainer.save_model(args.model_dir)
…</pre>
			<p>The <strong class="source-inline">save_model()</strong> API takes the model path as a parameter, coming from the <strong class="source-inline">SM_MODEL_DIR</strong> SageMaker container environment variable and parsed as a <strong class="source-inline">model_dir</strong> variable. The model artifacts stored in this path will then be copied to the S3 path specified in the <strong class="source-inline">HuggingFace</strong> estimator, and all the resources, such as the training instance, will then be torn down so that users only pay for the duration of the <span class="No-Break">training job.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We are training a very large model, <strong class="source-inline">prot_t5_xl_uniref50</strong>, with 11 billion parameters on a very big instance, <strong class="source-inline">ml.g5.48xlarge</strong>, which is an NVIDIA A10G tensor core machine with 8 GPUs and 768 GB of GPU memory. Although we are using model parallel, the training of the model will take more than 10 hours, and you will incur a cost for it. Alternatively, you can use a smaller model, such as <strong class="source-inline">prot_bert_bfd</strong>, which has approximately 420 million parameters and is pretrained on protein sequences. Since it’s a relatively smaller model that can fit into a single GPU memory, you can only use the SageMaker distributed data parallel library, as described in <a href="B18493_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">,</em> <em class="italic">Distributed Training of Machine </em><span class="No-Break"><em class="italic">Learning Models</em></span><span class="No-Break">.</span></p>
			<p>Once the model is trained, you <a id="_idIndexMarker881"/>can deploy the model using the SageMaker <strong class="source-inline">HuggingFaceModel</strong> class, as explained in the <em class="italic">Applying ML to genomics</em> main section of this chapter, or simply use the <strong class="source-inline">huggingface_estimator.deploy()</strong> API, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
…
predictor = huggingface_estimator.deploy(1,"ml.g4dn.xlarge")
…</pre>
			<p>Once the model is deployed, you can use the <strong class="source-inline">predictor</strong> variable to <span class="No-Break">make predictions:</span></p>
			<pre class="source-code">
…
predictor.predict(input_sequence)
…</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Both the discussed deployment options will deploy the model for real-time inference as an API on a long-running instance. So, if you are not using the endpoint, make sure to delete it; otherwise, you will incur a cost <span class="No-Break">for it.</span></p>
			<p>Now that we understand the<a id="_idIndexMarker882"/> applications of ML to genomics, let’s recap what we have learned so far in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor214"/>Summary</h1>
			<p>In this chapter, we started with understanding the concepts of genomics and how you can store and manage large genomics data on AWS. We also discussed the end-to-end architecture design for transferring, storing, analyzing, and applying ML to genomics data using AWS services. We then focused on how you can deploy large state-of-the-art models for genomics, such as DNABERT, for promoter recognition tasks using Amazon SageMaker with a few lines of code and how you can test your endpoint using code and the SageMaker <span class="No-Break">Studio UI.</span></p>
			<p>We then moved on to understanding proteomics, which is the study of protein sequences, structure, and their functions. We walked through an example of predicting protein secondary structure for protein sequences using a Hugging Face pretrained model with 11 billion parameters. Since it is a large model with memory requirements greater than 220 GB, we explored various memory-saving techniques, such as activation checkpointing, activation offloading, optimizer state sharding, and tensor parallelism, provided by the SageMaker model parallel library. We then used these techniques to train our model for predicting protein structure. We also understood how SageMaker provides integration with Hugging Face and makes it simple to use state-of-the-art models, which otherwise need a lot of heavy lifting in order <span class="No-Break">to train.</span></p>
			<p>In the next chapter, we will review another domain, autonomous vehicles, and understand how high-performance computing capabilities provided by AWS can be used for training and deploying ML models <span class="No-Break">at scale.</span></p>
		</div>
		<div>
			<div id="_idContainer191" class="IMG---Figure">
			</div>
		</div>
	</body></html>