- en: Loading and Preparing the Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data preparation involves data cleaning and feature engineering. It is the
    most time-consuming part of a machine learning project. Amazon ML offers powerful
    features to transform and slice the data. In this chapter, we will create the
    datasources that Amazon ML requires to train and select models. Creating a datasource
    involves three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Making the dataset available on AWS S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Informing Amazon ML about the nature of the data using the *schema.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transforming the initial dataset with recipes for feature engineering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a second part, we will extend Amazon ML data modification capabilities in
    order to carry out powerful feature engineering and data cleansing by using Amazon
    SQL service Athena. Athena is a serverless SQL-based query service perfectly suited
    for data munging in a predictive analytics context.
  prefs: []
  type: TYPE_NORMAL
- en: Working with datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You cannot do predictive analytics without a dataset. Although we are surrounded
    by data, finding datasets that are adapted to predictive analytics is not always
    straightforward. In this section, we present some resources that are freely available.
    We then focus on the dataset we are going to work with for several chapters. The
    `Titanic` dataset is a classic introductory datasets for predictive analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Finding open datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a multitude of dataset repositories available online, from local to
    global public institutions, from non-profit organizations to data-focused start-ups.
    Here''s a small list of open dataset resources that are well suited for predictive
    analytics. This, by far, is not an exhaustive list:'
  prefs: []
  type: TYPE_NORMAL
- en: This thread on Quora points to many other interesting data sources: [https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)
    . You can also ask for specific datasets on Reddit at [https://www.reddit.com/r/datasets/.](https://www.reddit.com/r/datasets/)
  prefs: []
  type: TYPE_NORMAL
- en: '**UCI Machine Learning Repository** is a collection of datasets maintained
    by *UC Irvine* since 1987, hosting over 300 datasets related to classification,
    clustering, regression, and other ML tasks, [https://archive.ics.uci.edu/ml/](https://archive.ics.uci.edu/ml/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Stanford Large Network Dataset Collection** at [https://snap.stanford.edu/data/index.html](https://snap.stanford.edu/data/index.html)
    and other major universities also offer great collections of open datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kdnuggets** has an extensive list of open datasets at [http://www.kdnuggets.com/datasets](http://www.kdnuggets.com/datasets)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data.gov](http://Data.gov) and other US government agencies; [data.UN.org](http://data.UN.org)
    and other UN agencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open data websites** from governments across the world: CA: [http://open.canada.ca/](http://open.canada.ca/),
    FR: [http://www.data.gouv.fr/fr/](http://www.data.gouv.fr/fr/)[,](http://open.canada.ca/)
    JA: [http://www.data.go.jp/](http://www.data.go.jp/), IN: [https://data.gov.in/](https://data.gov.in/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS offers open datasets via partners at [https://aws.amazon.com/government-education/open-data/](https://aws.amazon.com/government-education/open-data/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following startups are data centered and give open access to rich data
    repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quandl** and **Quantopian** for financial datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Datahub.io](https://datahub.io/), [Enigma.com](https://www.enigma.com/), and
    [Data.world](https://data.world/) are dataset-sharing sites'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Datamarket.com](https://datamarket.com/) is great for time series datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kaggle.com](https://www.kaggle.com/), the data science competition website,
    hosts over a 100 very interesting datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS public datasets:** AWS hosts a variety of public datasets, such as the
    Million Song Dataset, the mapping of the **Human Genome**, the US Census data
    as well as many others in Astrology, Biology, Math, Economics, and so on. These
    datasets are mostly available using EBS snapshots although some are directly accessible
    on S3\. The datasets are large, from a few gigabytes to several terabytes, and
    they are not meant to be downloaded on your local machine, they are only to be
    accessible via an EC2 instance (take a look at [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html) for
    further details). AWS public datasets are accessible at [https://aws.amazon.com/public-datasets/](https://aws.amazon.com/public-datasets/).'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Titanic dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this present chapter and in [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml),
    *Model Creation *and [Chapter 6](436c43e7-4294-4e09-a90e-10c13af2b32a.xhtml),
    *Predictions and Performances*, we will use the classic `Titanic` dataset. The
    data consists of demographic and traveling information for 1309 of the Titanic
    passengers, and the goal is to predict the survival of these passengers. The full
    Titanic dataset is available from the *Department of Biostatistic*s at the *Vanderbilt
    University School of Medicine* ([http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv)) in
    several formats. The *Encyclopedia Titanica* website ([https://www.encyclopedia-titanica.org/](https://www.encyclopedia-titanica.org/))
    is the website of reference regarding the Titanic. It contains all the facts,
    history, and data surrounding the Titanic, including a full list of passengers
    and crew members. The Titanic dataset is also the subject of the introductory
    competition on Kaggle ([https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic),
    requires opening an account with Kaggle). You can also find a CSV version in this
    book's GitHub repository at [https://github.com/alexperrier/packt-aml/blob/master/ch4](https://github.com/alexperrier/packt-aml/blob/master/ch4).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Titanic data contains a mix of textual, Boolean, continuous, and categorical
    variables. It exhibits interesting characteristics such as missing values, outliers,
    and text variables ripe for text mining, a rich dataset that will allow us to
    demonstrate data transformations. Here''s a brief summary of the 14 attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pclass`: Passenger class (1 = 1st; 2 = 2nd; 3 = 3rd)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`survival`: A Boolean indicating whether the passenger survived or not (0 =
    No; 1 = Yes); this is our target'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: A field rich in information as it contains title and family names'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sex`: Male/female'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`: Age, a significant portion of values are missing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sibsp`: Number of siblings/spouses aboard'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`parch`: Number of parents/children aboard'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ticket`: Ticket number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fare`: Passenger fare (British Pound).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cabin`: Cabin. Does the location of the cabin influence chances of survival?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embarked`: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boat`: Lifeboat, many missing values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`body`: Body Identification Number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`home.dest`: Home/destination'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take a look at [http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf](http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf) for
    more details on these variables.
  prefs: []
  type: TYPE_NORMAL
- en: We have 1309 records and 14 attributes, three of which we will discard. The `home.dest`
    attribute has too few existing values, the `boat` attribute is only present for
    passengers who have survived, and the `body` attribute is only for passengers
    who have not survived. We will discard these three columns later on while using
    the data schema.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the initial raw dataset, we are going to shuffle it, split
    it into a training and a held-out subset, and load it to an S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 2](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml), *Machine
    Learning Definitions and Concepts*, in order to build and select the best model,
    we need to split the dataset into three parts: training, validation, and test,
    with the usual ratios being 60%, 20%, and 20%. The training and validation sets
    are used to build several models and select the best one while the held-out set
    is used for the final performance evaluation on previously unseen data. We will
    use the held-out subset in [Chapter 6](436c43e7-4294-4e09-a90e-10c13af2b32a.xhtml),
    *Predictions and Performances* to simulate batch predictions with the model we
    build in [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml), *Model Creation*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Amazon ML does the job of splitting the dataset used for model training
    and model evaluation into training and validation subsets, we only need to split
    our initial dataset into two parts: the global training/evaluation subset (80%)
    for model building and selection, and the held-out subset (20%) for predictions
    and final model performance evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shuffle before you split**: If you download the original data from the Vanderbilt
    University website, you will notice that it is ordered by `pclass`, the class
    of the passenger, and by alphabetical order of the `name` column. The first 323
    rows correspond to the first class followed by second (277) and third (709) class
    passengers. It is important to shuffle the data before you split it so that all
    the different variables have have similar distributions in each training and held-out
    subsets. You can shuffle the data directly in the spreadsheet by creating a new
    column, generating a random number for each row, and then ordering by that column.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find an already shuffled `titanic.csv` file for this book at [https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic.csv](https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic.csv). In
    addition to shuffling the data, we have removed punctuation in the name column:
    commas, quotes, and parenthesis, which can add confusion when parsing a CSV file.'
  prefs: []
  type: TYPE_NORMAL
- en: We end up with two files: `titanic_train.csv` with 1047 rows and `titanic_heldout.csv`
    with 263 rows. The next step is to upload these files on S3 so that Amazon ML
    can access them.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data on S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS S3 is one of the main AWS services dedicated to hosting files and managing their
    access. Files in S3 can be public and open to the Internet or have access restricted
    to specific users, roles, or services. S3 is also used extensively by AWS for
    operations such as storing log files or results (predictions, scripts, queries,
    and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'Files in S3 are organized around the notion of buckets. Buckets are placeholders
    with unique names similar to domain names for websites. A file in S3 will have
    a unique locator URI: `s3://bucket_name/{path_of_folders}/filename`. The bucket
    name is unique across S3\.  In this section, we will create a bucket for our data,
    upload the titanic training file, and open its access to Amazon ML.'
  prefs: []
  type: TYPE_NORMAL
- en: We will show in *[Chapter 7](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609),
    Command Line and SDK* and the files in S3 can be entirely managed via the command
    line. For now, we will use the S3 online interface. Go to [https://console.aws.amazon.com/s3/home](https://console.aws.amazon.com/s3/home), and
    open an S3 account if you don't have one yet.
  prefs: []
  type: TYPE_NORMAL
- en: '**S3 pricing:** S3 charges for the total volume of files you host and the volume
    of file transfers depends on the region where the files are hosted. At the time
    of writing, for less than 1TB, AWS S3 charges $0.03/GB per month in the US east
    region. All S3 prices are available at [https://aws.amazon.com/s3/pricing/](https://aws.amazon.com/s3/pricing/)
    also [http://calculator.s3.amazonaws.com/index.html](http://calculator.s3.amazonaws.com/index.html) for
    the AWS cost calculator.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a bucket
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you have created your S3 account, the next step is to create a bucket
    for your files. Click on the Create bucket button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Name and a region**: Since bucket names are unique across S3, you must choose
    a name for your bucket that has not been already taken. We chose the name `aml.packt`
    for our bucket, and we will use this bucket throughout book. Regarding the region,
    you should always select a region that is the closest to the person or application accessing
    the files in order to reduce latency and prices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Set Versioning, Logging, and Tags**: Versioning will keep a copy of every
    version of your files, which prevents accidental deletions. Since versioning and
    logging induce extra costs, we chose to disable them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Set permissions**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Review and save.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These steps are illustrated by the following screenshots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To upload the data, simply click on the upload button and select the `titanic_train.csv`
    file that we created earlier on. You should, at this point, have the training
    dataset uploaded to your AWS S3 bucket. We added a `/data` folder in our `aml.packt`
    bucket to compartmentalize our objects. It will be useful later on when the bucket
    will also contain folders created by Amazon ML.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, only the owner of the bucket (that is you) is able to access
    and modify its contents. We need to grant the Amazon ML service permissions to
    read the data and add other files to the bucket. When creating the Amazon ML datasource,
    we will be prompted to grant these permissions via the Amazon ML console. We can
    also modify the bucket's policy upfront.
  prefs: []
  type: TYPE_NORMAL
- en: Granting permissions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to edit the policy of the `aml.packt` bucket. To do so, we have to
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on your bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the Permissions tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the dropdown, select `Bucket Policy` as shown in the following screenshot.
    This will open an editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B05028_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Paste in the following JSON file. Make sure to replace `{YOUR_BUCKET_NAME}`
    with the name of your bucket and save:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Further details on this policy are available at [http://docs.aws.amazon.com/machine-learning/latest/dg/granting-amazon-ml-permissions-to-read-your-data-from-amazon-s3.html](http://docs.aws.amazon.com/machine-learning/latest/dg/granting-amazon-ml-permissions-to-read-your-data-from-amazon-s3.html).
    Once again, this step is optional since Amazon ML will prompt you for access to
    the bucket when you create the datasource.
  prefs: []
  type: TYPE_NORMAL
- en: Formatting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon ML works on comma separated values files (`.csv`), a very simple format
    where each row is an observation and each column is a variable or attribute. There
    are, however, a few conditions that should be met:'
  prefs: []
  type: TYPE_NORMAL
- en: The data must be encoded in plain text using a character set, such as ASCII,
    Unicode, or EBCDIC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All values must be separated by commas; if a value contains a comma, it should
    be enclosed by double quotes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each observation (row) must be smaller than 100k
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also conditions regarding end of line characters that separate rows.
    Special care must be taken when using Excel on *OS X (Mac)*, as explained on this
    page: [http://docs.aws.amazon.com/machine-learning/latest/dg/understanding-the-data-format-for-amazon-ml.html.](http://docs.aws.amazon.com/machine-learning/latest/dg/understanding-the-data-format-for-amazon-ml.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**What about other data file formats?**'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, Amazon ML datasources are only compatible with CSV files and
    Redshift or RDS databases and they do not accept formats such as JSON, TSV, or
    XML. However, other services such as Athena, a serverless database service, do
    accept a wider range of formats. We will see later in this chapter how to circumvent
    the Amazon ML file format restrictions using Athena.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data is on S3, the next step is to tell Amazon ML its location
    by creating a datasource.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the datasource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working with Amazon ML, the data always resides in S3 and it is not duplicated
    in Amazon ML. A datasource is the metadata that indicates the location of the
    input data allowing Amazon ML to access it. Creating a datasource also generates descriptive
    statistics related to the data and a schema with information on the nature of
    the variables. Basically, the datasource gives Amazon ML all the information it
    requires to be able to train a model. The following are the steps you need to
    follow to create a datasource:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to Amazon Machine Learning: [https://console.aws.amazon.com/machinelearning/home](https://console.aws.amazon.com/machinelearning/home).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on getting started, you will be given a choice between accessing the
    Dashboard and Standard setup. This time choose the standard setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B05028_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Perform the following steps, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose an S3 location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start typing the name of the bucket in the s3 location field, and the list folders
    and files should show up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `titanic_train.csv` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give the datasource the name Titanic training set, and click on Verify.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B05028_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you haven''t previously set up the bucket policy, you will be asked to grant
    permissions to Amazon ML to read the file in S3; click on Yes to confirm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will see a confirmation that your datasource was successfully created and
    is accessible by Amazon ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on `Continue` in order to finalize the datasource creation. At this point, Amazon
    ML has scanned the `titanic_train.csv` file and inferred the data type for each
    column. This meta information is regrouped in the schema.
  prefs: []
  type: TYPE_NORMAL
- en: '**Other datasources**: Amazon ML allows you to define a Redshift or RDS database
    as the source of your data instead of S3\. Amazon Redshift is a "*fast, fully
    managed, petabyte-scale data warehouse solution*". It is far less trivial to set
    up than S3, but it will handle much larger and more complex volumes of data. Redshift
    allows you to analyze your data with any SQL client using industry-standard ODBC/JDBC
    connections. We will come back to Redshift in *[Chapter 8](e4d85418-6fa5-43e9-940f-b6b908e47836.xhtml),
    Creating Datasources from Redshift*.'
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the data schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data schema is the meta information, the dictionary of a data file. It informs
    Amazon ML of the type of each variable in the dataset. Amazon ML will use that
    information to correctly read, interpret, analyze, and transform the data. Once
    created, the data schema can be saved and reused for other subsets of the same
    data. Although Amazon ML does a good job of guessing the nature of your dataset,
    it is always a good idea to double-check and sometimes make some necessary adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, Amazon ML assumes that the first row of your file contains an observation.
    In our case, the first row of the `titanic_train.csv` file contains the names of
    the variables. Be sure to confirm that this is the case by selecting Yes in that
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Amazon ML classifies your data according to four data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary**: 0 or 1, Yes or No, Male or Female, False or True (`survived` in
    the Titanic dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical**: Variables that can take a finite number of values with numbers
    or characters (`pclass` or `embarked`, `sibsp`, `parch`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numeric**: A quantity with continuous values (`age` or `fare`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text**: Free text (`name`, `ticket`, `home.dest`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that some variables in the `Titanic` dataset could be associated with different
    data types. `ticket`, `cabin`, or `home.dest`, for instance, could be interpreted as
    text or categorical values. The `age` attribute could be transformed from a numerical
    value into a categorical one by binning. The `sex` attribute could also be interpreted
    as being binary, categorical, or even a text type. Having the `sex` attribute as
    binary would require transforming its values from male/female to 0/1, so we'll
    keep it as categorical even though it only takes two values.
  prefs: []
  type: TYPE_NORMAL
- en: The model-building algorithm will process the variables and their values differently
    depending on their data type. It may be worthwhile to explore different data types
    for certain variables, when that makes sense, in order to improve predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the schema that Amazon ML has deduced from our
    `titanic_train.csv` data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the sampled values shown here will be different on your own schema
    since the data has been randomized.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose to keep the default choices made by Amazon ML or make some modifications.
    Consider the following for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sibsp` and `parch`, respectively the number of siblings and parents, could
    be categorical instead of numeric if we wanted to regroup passengers with similar
    numbers of family members.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pclass` should be corrected as Categorical and not Numeric since there are
    only 3 possible values for `pcalss`: 1, 2, and 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cabin` could be interpreted as categorical since there are a finite number
    of cabins. However, the data indicates that the field could be parsed further.
    It has values such as C22 C26, which seems to indicate not one cabin but two.
    A text data type would be more appropriate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon ML needs to know what attribute we aim to predict. On the next screen,
    you will be asked to select the target. Confirm that Do you plan to use this dataset
    to create or evaluate an ML model, and select the `survived` attribute, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_10_v3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Deciding between numeric and categorical**: Numeric values are ordered, categorical
    values are not. When setting a variable to numeric, you are actually telling the
    model that the values are ordered. This information is useful to the algorithm
    when building the model. Categorical variables do not imply any order between
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the example of the `sibsp` and `parch` attributes, these variables
    have a finite number of values (0 to 8 for `sibps` and 0 to 9 for `parch`) and
    could be categorical. However, the order of the values holds some important information.
    Eight siblings is more than one and indicates a big family. Therefore, it also
    makes sense to keep the values as numeric.
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical values and one-hot encoding**: In the case of linear regression,
    categorical values are one-hot encoded. They are broken down into *N-1* binary
    variables when there are *N* categories. For instance, a variable with just three
    values *A*, *B*, and *C* is broken into two binary variables *is_it_A?* and *is_it_B?*
    that only take true and false values. Note that there is no need to define a third
    *is_it_C?* binary variable as it is directly deduced from the values of *is_it_A?*
    and *is_it_B?*. In the Titanic case, we have three values for the embarked variable;
    Amazon ML will create two binary variables equivalent to *passenger embarked at
    Queenstown* and *passenger **embarked at Cherbourg*, with the third variable *passenger **embarked
    at Southhampton* inferred from the values of the two first ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The type of the target dictates the model Amazon ML will use. Since we chose
    a binary target, Amazon ML will use logistic regression to train the models. A
    numeric target would have implied linear regression and a categorical target also
    logistic regression but this time for multiclass classification. Amazon ML confirms
    the nature of the model it will use, as shown by the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: The final step consists of telling Amazon ML that our data does not contain
    a row identifier and finally reviewing our datasource. The datasource creation
    is now pending; depending on its size, it will take a few minutes to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Reusing the schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The schema is a JSON file, which implies that we can create one from scratch
    for our data or modify the one generated by Amazon ML. We will now modify the
    one created by Amazon ML, save it to S3, and use it to create a new datasource
    that does not include the `body`, `boat`, and `home.dest` variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on View Input Schema as shown in the next screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: This gives us the raw schema in JSON format. Save it on your local machine with
    the filename `titanic_train.csv.schema`. We will load this file on S3 in the same
    bucket/folder where the `titanic_train.csv` file resides. By adding `.schema`
    to the data CSV filename, we allow Amazon ML to automatically associate the schema
    file to the data file and bypass the creation of its own schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the schema file with your favorite editor and edit as such:'
  prefs: []
  type: TYPE_NORMAL
- en: Add `home.dest`*,* `body`, `boat` in the `excludedAttributeNames` field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change the datatype from `CATEGORICAL` to `NUMERIC` for `sibps` and `parch`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that although we want to remove the fields `boat`, `body`, and `home.dest`,
    we still need to declare them and their data types in the schema. Your JSON file
    should now look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Upload that modified `titanic_train.csv.schema` file to the same S3 location
    as your `titanic_train.csv` data file. Your bucket/folder should now look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us now create a new datasource. Since the schema file has the same name
    as the data file and is contained at the same location, Amazon ML will use the
    schema we provided:'
  prefs: []
  type: TYPE_NORMAL
- en: Go back to the Amazon ML dashboard and click on Create a new datasource in the
    main menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Indicate location and name the datasource; we named this new datasource Titanic
    train set 11 variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Amazon ML confirms that the schema you provided has been taken into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_14.png)'
  prefs: []
  type: TYPE_IMG
- en: Go through the remaining datasource creation steps and notice that the data
    types correspond to the ones you specified in the schema file and that the three
    fields are no longer present.
  prefs: []
  type: TYPE_NORMAL
- en: '**Schema recap**: To associate a schema to a file, it suffices to name the
    schema with the same name as the data file and add the .schema extension. For
    instance, for a data file named `my_data.csv`, the schema file should be named
    `my_data.csv.schema` and be uploaded to the same S3 location as the data file.'
  prefs: []
  type: TYPE_NORMAL
- en: Our datasource has now been created, and we can explore what type of insights
    into the data Amazon ML gives us.
  prefs: []
  type: TYPE_NORMAL
- en: Examining data statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When Amazon ML created the data source, it carried out a basic statistical
    analysis of the different variables. For each variable, it estimated the following
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: Correlation of each attribute to the target
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of invalid values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribution of numeric variables with histogram and box plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Range, mean, and median for numeric variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most and least frequent categories for categorical variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word counts for text variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of true values for binary variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Go to the Datasource dashboard, and click on the new datasource you just created
    in order to access the data summary page. The left side menu lets you access data
    statistics for the target and different attributes, grouped by data types. The
    following screenshot shows data insights for the Numeric attributes. The `age`
    and `fare` variables are worth looking at more closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Two things stand out:'
  prefs: []
  type: TYPE_NORMAL
- en: '`age` has `20%` missing values. We should replace these missing values by the
    mean or the median values of the existing values of `age`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean for `fare` is 33.28, but the range is `0-512.32`, indicating a highly
    skewed distribution. Looking at the `fare` distribution confirms that. Click on
    the Preview.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the histogram and the associated box plot for
    the `fare` attribute. We can see that most of the values are below 155, with very
    few values above 195\. This shows that the 521.32 value may well be an outlier,
    an invalid value caused by human error. Looking back at the original dataset,
    we see that four passengers from the same family with the same ticket number (*PC
    17755*) paid this price for four first class cabins *(B51, B53, B55,* and *B101).*
    Although that fare of 512.32 value is well above any other fare, it does look
    legit and not like an error of some sort. We should probably not discard or replace it.
    The following histogram shows the `fare` distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Text attributes have automatically been tokenized by Amazon ML and each
    word has been extracted from the original attribute. Punctuation has also been
    removed. Amazon ML then calculates word frequency as shown in the following screenshot
    for the `name` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_17.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature engineering with Athena
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have a decent set of variables that can help predict whether
    a passenger survived the Titanic disaster. However, that data could use a bit
    of cleaning up in order to handle outliers and missing values. We could also try
    to extract other meaningful features from existing attributes to boost our predictions.
    In other terms, we want to do some feature engineering. Feature engineering is
    the key to boosting the accuracy of your predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering is the process of using domain knowledge of the data to
    create features that make machine learning algorithms work.
  prefs: []
  type: TYPE_NORMAL
- en: '- Wikipedia'
  prefs: []
  type: TYPE_NORMAL
- en: ML offers what it calls data recipes to transform the data and adapt it to its
    linear regression and logistic regression algorithm. In Amazon ML, data recipes
    are part of building the predictive model, not creating the datasource. We study
    Amazon ML's data recipes extensively in [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml),
    *Model Creation*. Amazon ML's data recipes are mostly suited to adapt the data
    to the algorithm and is a bit limited to correcting problems in the original data
    or creating new attributes from existing ones. For instance, removing outliers,
    extracting keywords from text, or replacing missing values are not possible with
    Amazon ML's recipes. We will, therefore, use SQL queries to perform some data
    cleaning and feature creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several AWS services are based on SQL databases: Redshift, RDS, and more recently,
    **Athena**. SQL is not only widely used to query data but, as we will see, it
    is particularly well suited for data transformation and data cleaning.  Many creative
    ideas on how to squeeze out information for the original Titanic dataset can be
    found online. These online sources, for instance, offer many ideas on the subject
    of feature engineering on the Titanic dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the Titanic dataset for Kaggle competition on the Ultraviolet blog
    with code example in Python at [http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/](http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar analysis with example in R on Trevor Stephens blog: [http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/](http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Titanic Kaggle competition forums are ripe with ideas: [https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, we will focus on the following transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: Replacing the missing `age` values with the average of existing `age` values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of replacing or discarding the fare outlier values, we will create a
    new log (`fare`) variable with a distribution less skewed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting titles from the `name` field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each cabin number is referred to by three characters, where the first character
    is a letter relative to the deck level (A, B, C, ..., F) and the number of cabin
    number, on that deck; we will extract the first letter of each cabin as a new
    `deck` variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also combine `sibps` and `parch` to create a `family_size` variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will use Athena to do these data transformations.
    We will first create a database and table in Athena and fill it with the Titanic
    data. We will then create new features using standard SQL queries on the newly
    created table. Finally, we will export the results to CSV in S3 and create a new
    datasource from it.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Athena
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Athena was launched during the *AWS re:Invent* conference in *December 2016*.
    It complements other AWS SQL based services by offering a simple serverless service
    that directly interacts with S3\. According to AWS, Amazon Athena is an interactive
    query service that makes it easy to analyze data directly from Amazon S3 using
    standard SQL*. *Several attributes of Amazon Athena make it particularly adapted
    for data preparation with Amazon ML:'
  prefs: []
  type: TYPE_NORMAL
- en: Athena can generate tables directly from data available in S3 in different formats
    (CSV, JSON, TSV, amazon logs, and others). Since datasources in Amazon ML are
    also S3-based, it is easy to manipulate files, perform various data transformation,
    and create various datasets on the fly to test your models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Athena is fast, simple, and can handle massive datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Athena uses `prestodb`, a distributed SQL query engine developed by Facebook
    that offers a very rich library of SQL functions, which are well suited to do
    data transformations and feature engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon ML only accepts CSV files to create datasources from S3\. However, since
    Athena can gather data from other file formats besides CSV, (JSON, TSV, ORC, and
    others) and export query results to a CSV file, it's possible to use Athena as
    a conversion tool and expand Amazon ML sourcing capabilities that way.
  prefs: []
  type: TYPE_NORMAL
- en: Presto is an open-source distributed SQL query engine optimized for low-latency,
    ad-hoc analysis of data. It supports the ANSI SQL standard, including complex
    queries, aggregations, joins, and window functions. More information can be found
    at [https://prestodb.io/](https://prestodb.io/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will perform the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an Athena account and a database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create and populate a table directly from the S3 Titanic CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform queries on the dataset and create new features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the results to S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create new training and testing datasources with the extra features in Amazon
    ML.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Athena is simple. Let's start with a simple overview.
  prefs: []
  type: TYPE_NORMAL
- en: A brief tour of AWS Athena
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Athena, the data is not stored in a database; it remains in S3\. When you
    create a table in Athena, you are creating an information layer that tells Athena
    where to find the data, how it is structured, and what format it is in. The schema
    in Athena is a logical namespace of objects. Once the data structure and location
    are known to Athena, you can query the data via standard SQL statements.
  prefs: []
  type: TYPE_NORMAL
- en: Athena uses **Hive Data Definition Language (DDL)** to create or drop databases
    and tables (more information on Hive DDL can be found at [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL)).
    Athena can understand multiple formats (CSV, TSV, JSON, and so on) through the
    use of `serializer-deserializer (SerDes)` libraries. Athena is available either
    via the console ([https://console.aws.amazon.com/athena/](https://console.aws.amazon.com/athena/))
    or via JDBC connection ([http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html](http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html)).
    We will use the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Athena** service offers four sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Query Editor:'
  prefs: []
  type: TYPE_NORMAL
- en: Lets you write your queries, save them, and see the results as well as navigate
    among your databases and tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Saved Queries page listing all the queries you saved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A History page listing all the queries you ran.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Catalog Manager that lets you explore your stored data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These sections are self-explanatory to use, and we will let you explore them
    at your leisure.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, Amazon Athena does not have a **Command-Line interface
    (cli)**. However, Athena can be accessed via a **Java Database Connectivity (JDBC)**
    driver available on S3\. You will find more information at [http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html](http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'A few things to know:'
  prefs: []
  type: TYPE_NORMAL
- en: You can create a table by specifying the location of the data in S3 or explicitly
    via an SQL query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All tables must be created as `EXTERNAL` and it is not possible to** CREATE
    TABLE AS SELECT**. Dropping a table created with the External keyword does not
    delete the underlying data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The presto SQL functions are available in Athena. Take a look at [https://prestodb.io/docs/current/functions.html](https://prestodb.io/docs/current/functions.html) for
    a full list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only submit one query and run five concurrent queries at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Athena Pricing:** Athena charges based on the amount of data scanned by the
    query with (at the time of writing) a $5 fee per TB of data scanned with a minimum
    of 10 MB per query. You can reduce your costs by converting your data to columnar
    formats or partitioning your data. Refer to [https://aws.amazon.com/athena/pricing/](https://aws.amazon.com/athena/pricing/) for
    more information.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a titanic database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to start from scratch and go back to the original Titanic dataset
    available at [https://github.com/alexperrier/packt-aml/blob/master/ch4/original_titanic.csv](https://github.com/alexperrier/packt-aml/blob/master/ch4/original_titanic.csv). Follow
    these steps to prepare the CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the `original_titanic.csv` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the header row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove the following punctuation characters: `,"()`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The file should only contain data, not column names. This is the original file
    with 1309 rows. These rows are ordered by `pclass` and alphabetical names. The
    resulting file is available at [https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic_for_athena.csv](https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic_for_athena.csv).
    Let us create a new `athena_data` folder in our S3 bucket and upload the `titanic_for_athena.csv`
    file. Now go to the Athena console. We will create a `titanic_db` database and
    a `titanic` table with the data.
  prefs: []
  type: TYPE_NORMAL
- en: Using the wizard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two ways to create databases and tables in Athena, via the wizard
    or by running queries. The wizard is accessible by clicking on the *Add table* link
    in the Query Editor page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In four steps, the wizard allows us to create the database, the table, and
    load the data. Creating the columns in *step 3* involves manually typing the name
    of each column and specifying each column type. With 14 columns in the dataset,
    this manual approach is time-consuming. We will, therefore, not use the wizard
    and switch to creating the database and table directly in SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_20.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating the database and table directly in SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create the database, run the following query in the query editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then select `titanic_db` in the database dropdown menu on the left side as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To create the table and load the data, run the following SQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A few things to note about that query:'
  prefs: []
  type: TYPE_NORMAL
- en: The location `s3://aml.packt/athena_data/` points to the folder we have specially
    created, not to the file itself . All the files in that folder will be considered
    as data for that table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SERDE` corresponds to the CSV format with a comma as the field delimiter.
    The list of supported formats and respective SERDE is available at [https://docs.aws.amazon.com/athena/latest/ug/supported-formats.html](https://docs.aws.amazon.com/athena/latest/ug/supported-formats.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The field types are standard SQL types (string, double, tinyint, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the query has finished running, the `titanic` table name appears in the
    left section of the page. You can click on the eye icon to select the first 10
    rows of the table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_22.png)**Specifying the results location**: Athena stores
    the results of your queries in a new S3 folder. You can specify what folder you
    want these results to be stored in by going to the settings page and specifying
    the desired S3 path. We have created a new folder titled  `*athena_query_results*`
    in our `aml.packt` bucket and set the result location to  `*s3://aml.packt/athena_query_results/*`.'
  prefs: []
  type: TYPE_NORMAL
- en: Data munging in SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now define the SQL query that will correct the missing value and outlier
    problems we saw earlier and create some new variables.
  prefs: []
  type: TYPE_NORMAL
- en: Missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have missing values for the `age` and `fare` variables with respective median
    values `28` and `14.5`. We can replace all the missing values with the median
    values with this statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We also want to keep the information that there was a missing value at least
    for the `age` variable. We can do that with the query that creates a new binary variable
    that we name: `is_age_missing`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Handling outliers in the fare
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen that the `fare` had four passengers paying a much higher price
    than the others. There are several ways to deal with these values, one of which
    can be to bin the variable by defining a series of specific ranges. For instance,
    below 20; from 20 to 50; 50 to 100, ..., and over 200\. Binning can be done with
    Amazon ML recipes.  We could also cap the fare value at a specific threshold,
    such as the 95% percentile. However, we decided that these large fare values were legit
    and that we ought to keep them. We can still create a new variable, `log_fare`*,* with
    a more compact range and a less skewed distribution by taking the log of the `fare:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `log_fare` variable has a range of *0 - 9.0*, a mean of *4.3*, and a median
    of *3.95*, whereas the original `fare` variable had a range of *[0, 512.3]*, mean *32.94*,
    and median *14.5*. The distribution of the `log_fare` is closer to a Gaussian
    distribution than the distribution of the original `fare` variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Box-Cox transformation**: Because of the linear regression assumptions, it
    is better to have variables with Gaussian distributions. The Box-Cox transformation,
    also known as the power transform (see [https://en.wikipedia.org/wiki/Power_transform](https://en.wikipedia.org/wiki/Power_transform)),
    is a common method to reshape a variable into a normally-distributed one. The
    Box-Cox transformation is a generalization of the log transformation we just applied
    to the `fare` variable.'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the title from the name
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you look closely at the passenger names, you will notice that they are all
    in the `{family name}{title}{first names}`format. It would be interesting to extract
    the `title` as a new variable. The following query uses the split function, which
    returns an array. We need the second element of that array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Inferring the deck from the cabin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cabin variable has three character values, where the first character corresponds
    to the Deck number (A, B, C, D, E). This is surely important information that
    we would like to extract.  We can do so with the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Calculating family size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, it makes sense to assume that the overall family size a passenger
    belongs to might have been a decisive factor in survival. We can aggregate the
    number of siblings and the number of parents and add 1 for the passenger. The
    following simple query will create the family size variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Wrapping up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can combine all these queries while also selecting the original attributes.
    Since the data is still ordered by `pclass` and passenger `name` in alphabetical
    order, we should also randomize the results. We end up with the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us run that query. The results will be displayed in the results panel and
    also written in a CSV file in the query result location on S3\. You can also save
    it on your local machine by clicking at the icon in the upper right corner of
    the results panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_04_23.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, we want to split the data into a training and a testing set like
    we did previously and create a new Amazon ML datasource with an extended schema.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an improved datasource
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to do some manipulation on the new `Titanic` dataset before we upload
    it to S3 and create a new datasource in Amazon ML:'
  prefs: []
  type: TYPE_NORMAL
- en: Open this new Titanic dataset in your favorite editor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the first *1047* rows, and save them to a new CSV: `ext_titanic_training.csv`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the next *263* rows and the header row, and save them to a file `ext_titanic_heldout.csv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to update our schema. Open the schema file `titanic_training.csv.schema`,
    and add the following lines to the JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The new schema file as well as the training and held-out sets can be found at [https://github.com/alexperrier/packt-aml/tree/master/ch4](https://github.com/alexperrier/packt-aml/tree/master/ch4).
  prefs: []
  type: TYPE_NORMAL
- en: We then need to upload the training and the schema file to S3\. These files
    should be in the same S3 location, `{bucket}/{folders}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now ready to create a new datasource based on this extended Titanic
    training set following the exact same steps as before:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify the location of the input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the schema.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the target as `survived`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bypass the `rowID`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review and create.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We now have 19 attributes in the schema and a brand new datasource.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on what is commonly known as the **Extract Load
    Transform (ETL)** part of the data science flow with regard to the Amazon ML service.
    We saw that the Amazon ML datasource is a set of information comprised of location,
    data structure, and data analytics given to the service so that it can use that
    data to start training models. You should now feel comfortable creating an Amazon
    ML datasource from an original CSV data file made accessible via S3.
  prefs: []
  type: TYPE_NORMAL
- en: We have also explored ways to transform the data and create new features via
    the AWS Athena service using simple SQL queries. The ability to complement the
    features of Amazon ML by leveraging the AWS ecosystem is one of the main benefits
    of using Amazon ML.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a couple of Titanic datasets, the original one and the extended
    one, which are split into training and held-out subsets, and we have created the
    associated datasources.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml), *Model Creation*,
    we will use these datasets to train models, and we will see if our new features
    and data cleaning result in better models.
  prefs: []
  type: TYPE_NORMAL
