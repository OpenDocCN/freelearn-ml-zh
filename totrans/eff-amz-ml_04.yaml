- en: Loading and Preparing the Dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集并准备数据
- en: 'Data preparation involves data cleaning and feature engineering. It is the
    most time-consuming part of a machine learning project. Amazon ML offers powerful
    features to transform and slice the data. In this chapter, we will create the
    datasources that Amazon ML requires to train and select models. Creating a datasource
    involves three steps:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备涉及数据清洗和特征工程。这是机器学习项目中耗时最长的一部分。Amazon ML提供了强大的功能来转换和切片数据。在本章中，我们将创建Amazon
    ML用于训练和选择模型的所需数据源。创建数据源涉及三个步骤：
- en: Making the dataset available on AWS S3.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS S3上使数据集可用。
- en: Informing Amazon ML about the nature of the data using the *schema.*
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*模式*向Amazon ML告知数据的性质。
- en: Transforming the initial dataset with recipes for feature engineering.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用特征工程的配方转换初始数据集。
- en: In a second part, we will extend Amazon ML data modification capabilities in
    order to carry out powerful feature engineering and data cleansing by using Amazon
    SQL service Athena. Athena is a serverless SQL-based query service perfectly suited
    for data munging in a predictive analytics context.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们将扩展Amazon ML数据修改功能，以便通过使用Amazon SQL服务Athena来执行强大的特征工程和数据清洗。Athena是一个无服务器的基于SQL的查询服务，非常适合预测分析环境中的数据处理。
- en: Working with datasets
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据集
- en: You cannot do predictive analytics without a dataset. Although we are surrounded
    by data, finding datasets that are adapted to predictive analytics is not always
    straightforward. In this section, we present some resources that are freely available.
    We then focus on the dataset we are going to work with for several chapters. The
    `Titanic` dataset is a classic introductory datasets for predictive analytics.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数据集，你无法进行预测分析。尽管我们被数据包围，但找到适合预测分析的数据集并不总是那么简单。在本节中，我们介绍了一些免费可用的资源。然后，我们将重点介绍我们将用于几章的特定数据集。`Titanic`数据集是预测分析的入门级经典数据集。
- en: Finding open datasets
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找开放数据集
- en: 'There is a multitude of dataset repositories available online, from local to
    global public institutions, from non-profit organizations to data-focused start-ups.
    Here''s a small list of open dataset resources that are well suited for predictive
    analytics. This, by far, is not an exhaustive list:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在线有大量的数据集存储库，从地方到全球的公共机构，从非营利组织到数据驱动的初创公司。以下是一些适合预测分析的开放数据集资源的小列表，这绝对不是详尽的列表：
- en: This thread on Quora points to many other interesting data sources: [https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)
    . You can also ask for specific datasets on Reddit at [https://www.reddit.com/r/datasets/.](https://www.reddit.com/r/datasets/)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Quora帖子指向了许多其他有趣的数据源：[https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public](https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public)。你还可以在Reddit上请求特定的数据集，[https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/)。
- en: '**UCI Machine Learning Repository** is a collection of datasets maintained
    by *UC Irvine* since 1987, hosting over 300 datasets related to classification,
    clustering, regression, and other ML tasks, [https://archive.ics.uci.edu/ml/](https://archive.ics.uci.edu/ml/)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UCI机器学习存储库** 自1987年以来由*加州大学欧文分校*维护的数据集集合，包含超过300个与分类、聚类、回归和其他机器学习任务相关的数据集，[https://archive.ics.uci.edu/ml/](https://archive.ics.uci.edu/ml/)'
- en: '**The Stanford Large Network Dataset Collection** at [https://snap.stanford.edu/data/index.html](https://snap.stanford.edu/data/index.html)
    and other major universities also offer great collections of open datasets'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**斯坦福大型网络数据集集合** [https://snap.stanford.edu/data/index.html](https://snap.stanford.edu/data/index.html)
    以及其他主要大学也提供了大量的开放数据集'
- en: '**Kdnuggets** has an extensive list of open datasets at [http://www.kdnuggets.com/datasets](http://www.kdnuggets.com/datasets)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kdnuggets** 在 [http://www.kdnuggets.com/datasets](http://www.kdnuggets.com/datasets)
    提供了大量的开放数据集'
- en: '[Data.gov](http://Data.gov) and other US government agencies; [data.UN.org](http://data.UN.org)
    and other UN agencies'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Data.gov](http://Data.gov) 和其他美国政府机构；[data.UN.org](http://data.UN.org) 和其他联合国机构'
- en: '**Open data websites** from governments across the world: CA: [http://open.canada.ca/](http://open.canada.ca/),
    FR: [http://www.data.gouv.fr/fr/](http://www.data.gouv.fr/fr/)[,](http://open.canada.ca/)
    JA: [http://www.data.go.jp/](http://www.data.go.jp/), IN: [https://data.gov.in/](https://data.gov.in/)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自世界各地的**开放数据网站**：加拿大：[http://open.canada.ca/](http://open.canada.ca/)，法国：[http://www.data.gouv.fr/fr/](http://www.data.gouv.fr/fr/)，日本：[http://www.data.go.jp/](http://www.data.go.jp/)，印度：[https://data.gov.in/](https://data.gov.in/)
- en: AWS offers open datasets via partners at [https://aws.amazon.com/government-education/open-data/](https://aws.amazon.com/government-education/open-data/)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS通过合作伙伴在[https://aws.amazon.com/government-education/open-data/](https://aws.amazon.com/government-education/open-data/)提供开放数据集
- en: 'The following startups are data centered and give open access to rich data
    repositories:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下初创公司以数据为中心，并提供对丰富数据仓库的开放访问：
- en: '**Quandl** and **Quantopian** for financial datasets'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Quandl**和**Quantopian**提供金融数据集'
- en: '[Datahub.io](https://datahub.io/), [Enigma.com](https://www.enigma.com/), and
    [Data.world](https://data.world/) are dataset-sharing sites'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Datahub.io](https://datahub.io/)、[Enigma.com](https://www.enigma.com/)和[Data.world](https://data.world/)是数据共享网站'
- en: '[Datamarket.com](https://datamarket.com/) is great for time series datasets'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Datamarket.com](https://datamarket.com/)非常适合时间序列数据集'
- en: '[Kaggle.com](https://www.kaggle.com/), the data science competition website,
    hosts over a 100 very interesting datasets'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学竞赛网站[Kaggle.com](https://www.kaggle.com/)托管了超过100个非常有趣的数据集
- en: '**AWS public datasets:** AWS hosts a variety of public datasets, such as the
    Million Song Dataset, the mapping of the **Human Genome**, the US Census data
    as well as many others in Astrology, Biology, Math, Economics, and so on. These
    datasets are mostly available using EBS snapshots although some are directly accessible
    on S3\. The datasets are large, from a few gigabytes to several terabytes, and
    they are not meant to be downloaded on your local machine, they are only to be
    accessible via an EC2 instance (take a look at [http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html) for
    further details). AWS public datasets are accessible at [https://aws.amazon.com/public-datasets/](https://aws.amazon.com/public-datasets/).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS公共数据集**：AWS托管了各种公共数据集，如百万歌曲数据集、人类基因组映射、美国人口普查数据以及天文学、生物学、数学、经济学等多个领域的许多其他数据集。这些数据集大多可以通过EBS快照访问，尽管一些数据集可以直接在S3上访问。这些数据集很大，从几个吉字节到几个太字节不等，它们不是用于在本地机器上下载的，而是仅通过EC2实例访问（有关更多详细信息，请参阅[http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html)）。AWS公共数据集可通过[https://aws.amazon.com/public-datasets/](https://aws.amazon.com/public-datasets/)访问。'
- en: Introducing the Titanic dataset
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍泰坦尼克号数据集
- en: Throughout this present chapter and in [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml),
    *Model Creation *and [Chapter 6](436c43e7-4294-4e09-a90e-10c13af2b32a.xhtml),
    *Predictions and Performances*, we will use the classic `Titanic` dataset. The
    data consists of demographic and traveling information for 1309 of the Titanic
    passengers, and the goal is to predict the survival of these passengers. The full
    Titanic dataset is available from the *Department of Biostatistic*s at the *Vanderbilt
    University School of Medicine* ([http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv)) in
    several formats. The *Encyclopedia Titanica* website ([https://www.encyclopedia-titanica.org/](https://www.encyclopedia-titanica.org/))
    is the website of reference regarding the Titanic. It contains all the facts,
    history, and data surrounding the Titanic, including a full list of passengers
    and crew members. The Titanic dataset is also the subject of the introductory
    competition on Kaggle ([https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic),
    requires opening an account with Kaggle). You can also find a CSV version in this
    book's GitHub repository at [https://github.com/alexperrier/packt-aml/blob/master/ch4](https://github.com/alexperrier/packt-aml/blob/master/ch4).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章以及[第5章](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml)“模型创建”和[第6章](436c43e7-4294-4e09-a90e-10c13af2b32a.xhtml)“预测和性能”中，我们将使用经典的`Titanic`数据集。数据包括1309名泰坦尼克号乘客的人口统计和旅行信息，目标是预测这些乘客的生存情况。完整的泰坦尼克号数据集可以从范德比尔特大学医学院生物统计学系(*Department
    of Biostatistics*)([http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv))以多种格式获取。*Encyclopedia
    Titanica*网站([https://www.encyclopedia-titanica.org/](https://www.encyclopedia-titanica.org/))是关于泰坦尼克号的参考网站。它包含了关于泰坦尼克号的所有事实、历史和数据，包括乘客和船员的完整名单。泰坦尼克号数据集也是Kaggle上入门竞赛的主题([https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)，需要通过Kaggle开户)。您也可以在本书的GitHub仓库[https://github.com/alexperrier/packt-aml/blob/master/ch4](https://github.com/alexperrier/packt-aml/blob/master/ch4)中找到CSV版本。
- en: 'The Titanic data contains a mix of textual, Boolean, continuous, and categorical
    variables. It exhibits interesting characteristics such as missing values, outliers,
    and text variables ripe for text mining, a rich dataset that will allow us to
    demonstrate data transformations. Here''s a brief summary of the 14 attributes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克号数据包含文本、布尔、连续和分类变量的混合。它表现出有趣的特征，如缺失值、异常值和适合文本挖掘的文本变量，这是一个丰富的数据集，将使我们能够展示数据转换。以下是14个属性的简要总结：
- en: '`pclass`: Passenger class (1 = 1st; 2 = 2nd; 3 = 3rd)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pclass`: 乘客等级（1 = 一等；2 = 二等；3 = 三等）'
- en: '`survival`: A Boolean indicating whether the passenger survived or not (0 =
    No; 1 = Yes); this is our target'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`survival`: 一个布尔值，表示乘客是否幸存（0 = 否；1 = 是）；这是我们目标'
- en: '`name`: A field rich in information as it contains title and family names'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`: 一个信息丰富的字段，因为它包含头衔和姓氏'
- en: '`sex`: Male/female'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sex`: 男性/女性'
- en: '`age`: Age, a significant portion of values are missing'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age`: 年龄，其中很大一部分值是缺失的'
- en: '`sibsp`: Number of siblings/spouses aboard'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sibsp`: 船上兄弟姐妹/配偶的数量'
- en: '`parch`: Number of parents/children aboard'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parch`: 船上父母/孩子的数量'
- en: '`ticket`: Ticket number.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ticket`: 票号。'
- en: '`fare`: Passenger fare (British Pound).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fare`: 乘客票价（英镑）。'
- en: '`cabin`: Cabin. Does the location of the cabin influence chances of survival?'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cabin`: 舱位。舱位的位置是否会影响生存的机会？'
- en: '`embarked`: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embarked`: 上船港口（C = Cherbourg；Q = Queenstown；S = Southampton）'
- en: '`boat`: Lifeboat, many missing values'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boat`: 救生艇，许多值缺失'
- en: '`body`: Body Identification Number'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`body`: 身体识别号码'
- en: '`home.dest`: Home/destination'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`home.dest`: 家/目的地'
- en: Take a look at [http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf](http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf) for
    more details on these variables.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 查看更多关于这些变量的详细信息，请参阅[http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf](http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf)。
- en: We have 1309 records and 14 attributes, three of which we will discard. The `home.dest`
    attribute has too few existing values, the `boat` attribute is only present for
    passengers who have survived, and the `body` attribute is only for passengers
    who have not survived. We will discard these three columns later on while using
    the data schema.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有1309条记录和14个属性，其中三个我们将丢弃。`home.dest`属性现有的值太少，`boat`属性仅适用于幸存乘客，而`body`属性仅适用于未幸存的乘客。我们将在使用数据模式时稍后丢弃这三个列。
- en: Preparing the data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: Now that we have the initial raw dataset, we are going to shuffle it, split
    it into a training and a held-out subset, and load it to an S3 bucket.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了初始的原始数据集，我们将对其进行洗牌，分割成训练集和保留子集，并将其加载到S3存储桶中。
- en: Splitting the data
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割数据
- en: 'As we saw in [Chapter 2](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml), *Machine
    Learning Definitions and Concepts*, in order to build and select the best model,
    we need to split the dataset into three parts: training, validation, and test,
    with the usual ratios being 60%, 20%, and 20%. The training and validation sets
    are used to build several models and select the best one while the held-out set
    is used for the final performance evaluation on previously unseen data. We will
    use the held-out subset in [Chapter 6](436c43e7-4294-4e09-a90e-10c13af2b32a.xhtml),
    *Predictions and Performances* to simulate batch predictions with the model we
    build in [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml), *Model Creation*.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第2章](8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml)，“机器学习定义和概念”中看到的，为了构建和选择最佳模型，我们需要将数据集分成三部分：训练集、验证集和测试集，通常的比例是60%、20%和20%。训练集和验证集用于构建多个模型并选择最佳模型，而保留集用于对先前未见数据进行的最终性能评估。我们将在[第6章](436c43e7-4294-4e09-a90e-10c13af2b32a.xhtml)，“预测和性能”中使用保留子集来模拟使用我们在[第5章](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml)，“模型创建”中构建的模型进行的批量预测。
- en: 'Since Amazon ML does the job of splitting the dataset used for model training
    and model evaluation into training and validation subsets, we only need to split
    our initial dataset into two parts: the global training/evaluation subset (80%)
    for model building and selection, and the held-out subset (20%) for predictions
    and final model performance evaluation.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Amazon ML负责将用于模型训练和模型评估的数据集分割成训练集和验证集，我们只需要将我们的初始数据集分成两部分：用于模型构建和选择的全球训练/评估子集（80%），以及用于预测和最终模型性能评估的保留子集（20%）。
- en: '**Shuffle before you split**: If you download the original data from the Vanderbilt
    University website, you will notice that it is ordered by `pclass`, the class
    of the passenger, and by alphabetical order of the `name` column. The first 323
    rows correspond to the first class followed by second (277) and third (709) class
    passengers. It is important to shuffle the data before you split it so that all
    the different variables have have similar distributions in each training and held-out
    subsets. You can shuffle the data directly in the spreadsheet by creating a new
    column, generating a random number for each row, and then ordering by that column.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**在分割前打乱顺序**：如果您从范德堡大学网站下载原始数据，您会注意到它按`pclass`（乘客等级）和`name`列的字母顺序排序。前323行对应头等舱乘客，其次是二等舱（277）和三等舱（709）乘客。在分割数据之前打乱数据非常重要，以确保所有不同的变量在每个训练和保留子集中都有相似的分布。您可以直接在电子表格中通过创建一个新列，为每一行生成一个随机数，然后按该列排序来打乱数据。'
- en: 'You will find an already shuffled `titanic.csv` file for this book at [https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic.csv](https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic.csv). In
    addition to shuffling the data, we have removed punctuation in the name column:
    commas, quotes, and parenthesis, which can add confusion when parsing a CSV file.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic.csv](https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic.csv)找到本书的已打乱顺序的`titanic.csv`文件。除了打乱数据外，我们还从“姓名”列中移除了标点符号：逗号、引号和括号，这些在解析CSV文件时可能会引起混淆。
- en: We end up with two files: `titanic_train.csv` with 1047 rows and `titanic_heldout.csv`
    with 263 rows. The next step is to upload these files on S3 so that Amazon ML
    can access them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到两个文件：包含1047行的`titanic_train.csv`和包含263行的`titanic_heldout.csv`。下一步是将这些文件上传到S3，以便Amazon
    ML可以访问它们。
- en: Loading data on S3
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在S3上加载数据
- en: AWS S3 is one of the main AWS services dedicated to hosting files and managing their
    access. Files in S3 can be public and open to the Internet or have access restricted
    to specific users, roles, or services. S3 is also used extensively by AWS for
    operations such as storing log files or results (predictions, scripts, queries,
    and so on).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: AWS S3是AWS主要服务之一，专注于托管文件和管理其访问权限。S3中的文件可以是公开的，对互联网开放，或者仅限于特定用户、角色或服务访问。S3还被AWS广泛用于存储日志文件或结果（预测、脚本、查询等）的操作。
- en: 'Files in S3 are organized around the notion of buckets. Buckets are placeholders
    with unique names similar to domain names for websites. A file in S3 will have
    a unique locator URI: `s3://bucket_name/{path_of_folders}/filename`. The bucket
    name is unique across S3\.  In this section, we will create a bucket for our data,
    upload the titanic training file, and open its access to Amazon ML.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: S3中的文件是围绕“桶”的概念组织的。桶是具有类似域名名称的唯一占位符。S3中的一个文件将有一个唯一的定位URI：`s3://bucket_name/{文件夹路径}/filename`。桶名在S3中是唯一的。在本节中，我们将为我们的数据创建一个桶，上传titanic训练文件，并打开其访问权限以供Amazon
    ML使用。
- en: We will show in *[Chapter 7](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609),
    Command Line and SDK* and the files in S3 can be entirely managed via the command
    line. For now, we will use the S3 online interface. Go to [https://console.aws.amazon.com/s3/home](https://console.aws.amazon.com/s3/home), and
    open an S3 account if you don't have one yet.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*[第7章](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609)，命令行和SDK*中展示，S3中的文件可以通过命令行完全管理。现在，我们将使用S3在线界面。前往[https://console.aws.amazon.com/s3/home](https://console.aws.amazon.com/s3/home)，如果您还没有账户，请创建一个S3账户。
- en: '**S3 pricing:** S3 charges for the total volume of files you host and the volume
    of file transfers depends on the region where the files are hosted. At the time
    of writing, for less than 1TB, AWS S3 charges $0.03/GB per month in the US east
    region. All S3 prices are available at [https://aws.amazon.com/s3/pricing/](https://aws.amazon.com/s3/pricing/)
    also [http://calculator.s3.amazonaws.com/index.html](http://calculator.s3.amazonaws.com/index.html) for
    the AWS cost calculator.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**S3定价：** S3根据您托管文件的总容量以及文件传输量（取决于文件托管区域）进行收费。在撰写本文时，对于小于1TB的容量，AWS S3在美国东部地区的月费为每GB
    $0.03。所有S3价格信息可在[https://aws.amazon.com/s3/pricing/](https://aws.amazon.com/s3/pricing/)找到，同时也可在[http://calculator.s3.amazonaws.com/index.html](http://calculator.s3.amazonaws.com/index.html)的AWS成本计算器中查看。'
- en: Creating a bucket
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建桶
- en: 'Once you have created your S3 account, the next step is to create a bucket
    for your files. Click on the Create bucket button:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您创建了S3账户，下一步就是为您的文件创建一个桶。点击“创建桶”按钮：
- en: '![](img/B05028_04_01.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_04_01.png)'
- en: '**Name and a region**: Since bucket names are unique across S3, you must choose
    a name for your bucket that has not been already taken. We chose the name `aml.packt`
    for our bucket, and we will use this bucket throughout book. Regarding the region,
    you should always select a region that is the closest to the person or application accessing
    the files in order to reduce latency and prices.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**名称和区域**：由于S3中的存储桶名称是唯一的，您必须选择一个尚未被占用的存储桶名称。我们为我们的存储桶选择了名称`aml.packt`，并且我们将在整个书中使用此存储桶。至于区域，您应该始终选择一个与访问文件的人或应用程序最接近的区域，以减少延迟和价格。'
- en: '**Set Versioning, Logging, and Tags**: Versioning will keep a copy of every
    version of your files, which prevents accidental deletions. Since versioning and
    logging induce extra costs, we chose to disable them.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置版本控制、日志记录和标签**：版本控制将保留每个文件版本的副本，这可以防止意外删除。由于版本控制和日志记录会引发额外的成本，我们选择禁用它们。'
- en: '**Set permissions**.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置权限**。'
- en: '**Review and save.**'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**审查并保存**。'
- en: 'These steps are illustrated by the following screenshots:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图说明了这些步骤：
- en: '![](img/B05028_04_02.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_04_02.png)'
- en: Loading the data
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: To upload the data, simply click on the upload button and select the `titanic_train.csv`
    file that we created earlier on. You should, at this point, have the training
    dataset uploaded to your AWS S3 bucket. We added a `/data` folder in our `aml.packt`
    bucket to compartmentalize our objects. It will be useful later on when the bucket
    will also contain folders created by Amazon ML.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要上传数据，只需单击上传按钮并选择我们之前创建的`titanic_train.csv`文件。此时，您应该已经将训练数据集上传到您的AWS S3存储桶。我们在`aml.packt`存储桶中添加了一个`/data`文件夹来隔离我们的对象。当存储桶也包含由Amazon
    ML创建的文件夹时，这将会很有用。
- en: At this point, only the owner of the bucket (that is you) is able to access
    and modify its contents. We need to grant the Amazon ML service permissions to
    read the data and add other files to the bucket. When creating the Amazon ML datasource,
    we will be prompted to grant these permissions via the Amazon ML console. We can
    also modify the bucket's policy upfront.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，只有存储桶的所有者（即您）能够访问和修改其内容。我们需要授予Amazon ML服务读取数据和向存储桶添加其他文件的权限。在创建Amazon ML数据源时，我们将被提示通过Amazon
    ML控制台授予这些权限。我们还可以提前修改存储桶的策略。
- en: Granting permissions
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 授予权限
- en: 'We need to edit the policy of the `aml.packt` bucket. To do so, we have to
    perform the following steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要编辑`aml.packt`存储桶的策略。为此，我们必须执行以下步骤：
- en: Click on your bucket.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击您的存储桶。
- en: Select the Permissions tab.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择“权限”选项卡。
- en: 'In the dropdown, select `Bucket Policy` as shown in the following screenshot.
    This will open an editor:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下拉菜单中，选择如图所示的“存储桶策略”。这将打开一个编辑器：
- en: '![](img/B05028_04_03.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_04_03.png)'
- en: 'Paste in the following JSON file. Make sure to replace `{YOUR_BUCKET_NAME}`
    with the name of your bucket and save:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将以下JSON文件粘贴进去。确保将`{YOUR_BUCKET_NAME}`替换为您的存储桶名称并保存：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Further details on this policy are available at [http://docs.aws.amazon.com/machine-learning/latest/dg/granting-amazon-ml-permissions-to-read-your-data-from-amazon-s3.html](http://docs.aws.amazon.com/machine-learning/latest/dg/granting-amazon-ml-permissions-to-read-your-data-from-amazon-s3.html).
    Once again, this step is optional since Amazon ML will prompt you for access to
    the bucket when you create the datasource.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此政策的更多详细信息可在[http://docs.aws.amazon.com/machine-learning/latest/dg/granting-amazon-ml-permissions-to-read-your-data-from-amazon-s3.html](http://docs.aws.amazon.com/machine-learning/latest/dg/granting-amazon-ml-permissions-to-read-your-data-from-amazon-s3.html)找到。再次强调，此步骤是可选的，因为当您创建数据源时，Amazon
    ML会提示您访问存储桶。
- en: Formatting the data
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 格式化数据
- en: 'Amazon ML works on comma separated values files (`.csv`), a very simple format
    where each row is an observation and each column is a variable or attribute. There
    are, however, a few conditions that should be met:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon ML在逗号分隔值文件（`.csv`）上工作，这是一种非常简单的格式，其中每一行是一个观察值，每一列是一个变量或属性。然而，还有一些条件需要满足：
- en: The data must be encoded in plain text using a character set, such as ASCII,
    Unicode, or EBCDIC
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据必须使用字符集（如ASCII、Unicode或EBCDIC）以纯文本格式编码
- en: All values must be separated by commas; if a value contains a comma, it should
    be enclosed by double quotes
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有值必须用逗号分隔；如果值中包含逗号，则应将其用双引号括起来
- en: Each observation (row) must be smaller than 100k
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个观察值（行）的大小必须小于100k
- en: There are also conditions regarding end of line characters that separate rows.
    Special care must be taken when using Excel on *OS X (Mac)*, as explained on this
    page: [http://docs.aws.amazon.com/machine-learning/latest/dg/understanding-the-data-format-for-amazon-ml.html.](http://docs.aws.amazon.com/machine-learning/latest/dg/understanding-the-data-format-for-amazon-ml.html)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 关于分隔行的换行符也存在一些条件。在使用 Excel 在 *OS X (Mac)* 上时必须特别注意，如本页所述：[http://docs.aws.amazon.com/machine-learning/latest/dg/understanding-the-data-format-for-amazon-ml.html](http://docs.aws.amazon.com/machine-learning/latest/dg/understanding-the-data-format-for-amazon-ml.html)。
- en: '**What about other data file formats?**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于其他数据文件格式呢？**'
- en: Unfortunately, Amazon ML datasources are only compatible with CSV files and
    Redshift or RDS databases and they do not accept formats such as JSON, TSV, or
    XML. However, other services such as Athena, a serverless database service, do
    accept a wider range of formats. We will see later in this chapter how to circumvent
    the Amazon ML file format restrictions using Athena.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，Amazon ML 数据源仅兼容 CSV 文件和 Redshift 或 RDS 数据库，并且不接受 JSON、TSV 或 XML 等格式。然而，其他服务如无服务器数据库服务
    Athena，则接受更广泛的格式。我们将在本章后面看到如何使用 Athena 来绕过 Amazon ML 文件格式限制。
- en: Now that the data is on S3, the next step is to tell Amazon ML its location
    by creating a datasource.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已位于 S3 上，下一步是通过创建数据源来告诉 Amazon ML 其位置。
- en: Creating the datasource
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据源
- en: 'When working with Amazon ML, the data always resides in S3 and it is not duplicated
    in Amazon ML. A datasource is the metadata that indicates the location of the
    input data allowing Amazon ML to access it. Creating a datasource also generates descriptive
    statistics related to the data and a schema with information on the nature of
    the variables. Basically, the datasource gives Amazon ML all the information it
    requires to be able to train a model. The following are the steps you need to
    follow to create a datasource:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Amazon ML 时，数据始终位于 S3 中，并且不会在 Amazon ML 中重复。数据源是元数据，指示输入数据的位置，允许 Amazon
    ML 访问它。创建数据源还会生成与数据相关的描述性统计信息和包含变量性质信息的模式。基本上，数据源为 Amazon ML 提供了所有必要的信息，以便能够训练模型。以下是你需要遵循的创建数据源的步骤：
- en: 'Go to Amazon Machine Learning: [https://console.aws.amazon.com/machinelearning/home](https://console.aws.amazon.com/machinelearning/home).'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 Amazon 机器学习：[https://console.aws.amazon.com/machinelearning/home](https://console.aws.amazon.com/machinelearning/home)。
- en: 'Click on getting started, you will be given a choice between accessing the
    Dashboard and Standard setup. This time choose the standard setup:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“入门”，你将可以选择访问仪表板或标准设置。这次请选择标准设置：
- en: '![](img/B05028_04_04.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_04.png)'
- en: 'Perform the following steps, as shown in the following screenshot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤，如以下截图所示：
- en: Choose an S3 location.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 S3 位置。
- en: Start typing the name of the bucket in the s3 location field, and the list folders
    and files should show up.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 s3 位置字段中开始输入存储桶的名称，列表文件夹和文件应该会显示出来。
- en: Select the `titanic_train.csv` file.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 `titanic_train.csv` 文件。
- en: Give the datasource the name Titanic training set, and click on Verify.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据源命名为“Titanic 训练集”，然后点击“验证”。
- en: '![](img/B05028_04_05.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_05.png)'
- en: 'If you haven''t previously set up the bucket policy, you will be asked to grant
    permissions to Amazon ML to read the file in S3; click on Yes to confirm:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有设置存储桶策略，系统会要求你授予 Amazon ML 读取 S3 中文件的权限；点击“是”以确认：
- en: '![](img/B05028_04_06.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_06.png)'
- en: 'You will see a confirmation that your datasource was successfully created and
    is accessible by Amazon ML:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到确认信息，表明你的数据源已成功创建并可由 Amazon ML 访问：
- en: '![](img/B05028_04_07.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_07.png)'
- en: Click on `Continue` in order to finalize the datasource creation. At this point, Amazon
    ML has scanned the `titanic_train.csv` file and inferred the data type for each
    column. This meta information is regrouped in the schema.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“继续”以完成数据源创建。此时，Amazon ML 已扫描 `titanic_train.csv` 文件并推断出每列的数据类型。这些元信息被重新组合在模式中。
- en: '**Other datasources**: Amazon ML allows you to define a Redshift or RDS database
    as the source of your data instead of S3\. Amazon Redshift is a "*fast, fully
    managed, petabyte-scale data warehouse solution*". It is far less trivial to set
    up than S3, but it will handle much larger and more complex volumes of data. Redshift
    allows you to analyze your data with any SQL client using industry-standard ODBC/JDBC
    connections. We will come back to Redshift in *[Chapter 8](e4d85418-6fa5-43e9-940f-b6b908e47836.xhtml),
    Creating Datasources from Redshift*.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他数据源**：亚马逊机器学习允许您将Redshift或RDS数据库定义为数据源，而不是S3。Amazon Redshift是一个“*快速、完全托管、PB级数据仓库解决方案*”。它比S3设置起来要复杂得多，但它可以处理更大和更复杂的数据量。Redshift允许您使用行业标准ODBC/JDBC连接通过任何SQL客户端分析数据。我们将在*[第8章](e4d85418-6fa5-43e9-940f-b6b908e47836.xhtml)，从Redshift创建数据源*中回到Redshift。'
- en: Verifying the data schema
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证数据模式
- en: The data schema is the meta information, the dictionary of a data file. It informs
    Amazon ML of the type of each variable in the dataset. Amazon ML will use that
    information to correctly read, interpret, analyze, and transform the data. Once
    created, the data schema can be saved and reused for other subsets of the same
    data. Although Amazon ML does a good job of guessing the nature of your dataset,
    it is always a good idea to double-check and sometimes make some necessary adjustments.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模式是元信息，是数据文件的字典。它通知亚马逊机器学习数据集中每个变量的类型。亚马逊机器学习将使用这些信息来正确读取、解释、分析和转换数据。一旦创建，数据模式可以保存并用于同一数据的其他子集。尽管亚马逊机器学习在猜测数据集的性质方面做得很好，但始终进行双重检查并在必要时进行一些必要的调整是一个好主意。
- en: 'By default, Amazon ML assumes that the first row of your file contains an observation.
    In our case, the first row of the `titanic_train.csv` file contains the names of
    the variables. Be sure to confirm that this is the case by selecting Yes in that
    form:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，亚马逊机器学习假定您的文件的第一行包含一个观测值。在我们的例子中，`titanic_train.csv`文件的第一行包含变量的名称。请确保通过在该表单中选择是来确认这一点：
- en: '![](img/B05028_04_08.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_04_08.png)'
- en: 'Amazon ML classifies your data according to four data types:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊机器学习根据四种数据类型对您的数据进行分类：
- en: '**Binary**: 0 or 1, Yes or No, Male or Female, False or True (`survived` in
    the Titanic dataset)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二进制**：0或1，是或否，男性或女性，假或真（例如`titanic`数据集中的`survived`）'
- en: '**Categorical**: Variables that can take a finite number of values with numbers
    or characters (`pclass` or `embarked`, `sibsp`, `parch`)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：可以取有限个数值的变量，可以是数字或字符（例如`pclass`或`embarked`、`sibsp`、`parch`）'
- en: '**Numeric**: A quantity with continuous values (`age` or `fare`)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值**：具有连续值的数量（例如`age`或`fare`）'
- en: '**Text**: Free text (`name`, `ticket`, `home.dest`)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本**：自由文本（例如`name`、`ticket`、`home.dest`）'
- en: Note that some variables in the `Titanic` dataset could be associated with different
    data types. `ticket`, `cabin`, or `home.dest`, for instance, could be interpreted as
    text or categorical values. The `age` attribute could be transformed from a numerical
    value into a categorical one by binning. The `sex` attribute could also be interpreted
    as being binary, categorical, or even a text type. Having the `sex` attribute as
    binary would require transforming its values from male/female to 0/1, so we'll
    keep it as categorical even though it only takes two values.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`Titanic`数据集中的一些变量可能与不同的数据类型相关联。例如，`ticket`、`cabin`或`home.dest`可以解释为文本或分类值。`age`属性可以通过分箱从数值值转换为分类值。`sex`属性也可以解释为二进制、分类或甚至是文本类型。将`sex`属性作为二进制将需要将其值从男性/女性转换为0/1，因此尽管它只取两个值，我们仍将其保留为分类。
- en: The model-building algorithm will process the variables and their values differently
    depending on their data type. It may be worthwhile to explore different data types
    for certain variables, when that makes sense, in order to improve predictions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建算法将根据变量的数据类型以不同的方式处理变量及其值。在某些情况下，探索某些变量的不同数据类型可能值得，以便提高预测的准确性。
- en: 'The following screenshot shows the schema that Amazon ML has deduced from our
    `titanic_train.csv` data:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了亚马逊机器学习从我们的`titanic_train.csv`数据中推断出的模式：
- en: '![](img/B05028_04_09.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_04_09.png)'
- en: Note that the sampled values shown here will be different on your own schema
    since the data has been randomized.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里显示的采样值将因您的数据模式而不同，因为数据已经被随机化。
- en: 'You can choose to keep the default choices made by Amazon ML or make some modifications.
    Consider the following for instance:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择保留亚马逊机器学习默认选择的选项，或者进行一些修改。以下是一个例子：
- en: '`sibsp` and `parch`, respectively the number of siblings and parents, could
    be categorical instead of numeric if we wanted to regroup passengers with similar
    numbers of family members.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sibsp`和`parch`，分别是兄弟姐妹和父母的人数，如果我们想要重新组合具有相似家庭成员数量的乘客，它们可以是分类的而不是数值的。'
- en: '`pclass` should be corrected as Categorical and not Numeric since there are
    only 3 possible values for `pcalss`: 1, 2, and 3.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pclass`应该更正为分类而不是数值，因为`pcalss`只有三个可能的值：1、2和3。'
- en: '`cabin` could be interpreted as categorical since there are a finite number
    of cabins. However, the data indicates that the field could be parsed further.
    It has values such as C22 C26, which seems to indicate not one cabin but two.
    A text data type would be more appropriate.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cabin`可以解释为分类的，因为船舱的数量是有限的。然而，数据表明该字段可以进一步解析。它有如C22 C26之类的值，这似乎表明不是一个船舱，而是两个。文本数据类型将更合适。'
- en: 'Amazon ML needs to know what attribute we aim to predict. On the next screen,
    you will be asked to select the target. Confirm that Do you plan to use this dataset
    to create or evaluate an ML model, and select the `survived` attribute, as shown
    in the following screenshot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon ML需要知道我们旨在预测的属性。在下一屏，你将被要求选择目标。确认你是否计划使用此数据集来创建或评估一个机器学习模型，并选择`survived`属性，如下面的截图所示：
- en: '![](img/B05028_04_10_v3.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_04_10_v3.png)'
- en: '**Deciding between numeric and categorical**: Numeric values are ordered, categorical
    values are not. When setting a variable to numeric, you are actually telling the
    model that the values are ordered. This information is useful to the algorithm
    when building the model. Categorical variables do not imply any order between
    values.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**决定数值和分类之间的选择**：数值是有序的，分类值不是。当将变量设置为数值时，你实际上是在告诉模型这些值是有序的。当构建模型时，这种信息对算法是有用的。分类变量之间不暗示任何顺序。'
- en: Going back to the example of the `sibsp` and `parch` attributes, these variables
    have a finite number of values (0 to 8 for `sibps` and 0 to 9 for `parch`) and
    could be categorical. However, the order of the values holds some important information.
    Eight siblings is more than one and indicates a big family. Therefore, it also
    makes sense to keep the values as numeric.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 回到`sibsp`和`parch`属性的例子，这些变量具有有限数量的值（`sibsp`为0到8，`parch`为0到9）并且可能是分类的。然而，值的顺序包含一些重要的信息。八个兄弟姐妹多于一个，表明这是一个大家庭。因此，保留数值作为值也是有意义的。
- en: '**Categorical values and one-hot encoding**: In the case of linear regression,
    categorical values are one-hot encoded. They are broken down into *N-1* binary
    variables when there are *N* categories. For instance, a variable with just three
    values *A*, *B*, and *C* is broken into two binary variables *is_it_A?* and *is_it_B?*
    that only take true and false values. Note that there is no need to define a third
    *is_it_C?* binary variable as it is directly deduced from the values of *is_it_A?*
    and *is_it_B?*. In the Titanic case, we have three values for the embarked variable;
    Amazon ML will create two binary variables equivalent to *passenger embarked at
    Queenstown* and *passenger **embarked at Cherbourg*, with the third variable *passenger **embarked
    at Southhampton* inferred from the values of the two first ones.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类值和独热编码**：在线性回归的情况下，分类值会被独热编码。当有*N*个类别时，它们会被分解成*N-1*个二进制变量。例如，一个只有三个值*A*、*B*和*C*的变量会被分解成两个二进制变量*is_it_A?*和*is_it_B?*，它们只接受真和假值。请注意，没有必要定义第三个*is_it_C?*二进制变量，因为它可以直接从*is_it_A?*和*is_it_B?*的值中推断出来。在泰坦尼克号的案例中，我们有一个名为`embarked`的变量的三个值；Amazon
    ML将创建两个二进制变量，相当于*在皇后镇登船的乘客*和*在瑟堡登船的乘客*，第三个变量*在南安普顿登船的乘客*将从前两个变量的值中推断出来。'
- en: 'The type of the target dictates the model Amazon ML will use. Since we chose
    a binary target, Amazon ML will use logistic regression to train the models. A
    numeric target would have implied linear regression and a categorical target also
    logistic regression but this time for multiclass classification. Amazon ML confirms
    the nature of the model it will use, as shown by the following screenshot:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 目标类型决定了Amazon ML将使用的模型类型。由于我们选择了二元目标，Amazon ML将使用逻辑回归来训练模型。数值目标将意味着线性回归，而分类目标也将是逻辑回归，但这次是多类分类。Amazon
    ML会确认它将使用的模型类型，如下面的截图所示：
- en: '![](img/B05028_04_11.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_04_11.png)'
- en: The final step consists of telling Amazon ML that our data does not contain
    a row identifier and finally reviewing our datasource. The datasource creation
    is now pending; depending on its size, it will take a few minutes to finish.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是告诉亚马逊机器学习我们的数据不包含行标识符，并最终审查我们的数据源。数据源创建现在处于待定状态；根据其大小，完成可能需要几分钟。
- en: Reusing the schema
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新使用模式
- en: The schema is a JSON file, which implies that we can create one from scratch
    for our data or modify the one generated by Amazon ML. We will now modify the
    one created by Amazon ML, save it to S3, and use it to create a new datasource
    that does not include the `body`, `boat`, and `home.dest` variables.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 模式是一个JSON文件，这意味着我们可以从头开始为我们自己的数据创建一个，或者修改亚马逊机器学习生成的模式。我们现在将修改亚马逊机器学习创建的模式，将其保存到S3，并使用它创建一个新的数据源，该数据源不包含`body`、`boat`和`home.dest`变量。
- en: 'Click on View Input Schema as shown in the next screenshot:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 点击查看输入模式，如图所示：
- en: '![](img/B05028_04_12.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_12.png)'
- en: This gives us the raw schema in JSON format. Save it on your local machine with
    the filename `titanic_train.csv.schema`. We will load this file on S3 in the same
    bucket/folder where the `titanic_train.csv` file resides. By adding `.schema`
    to the data CSV filename, we allow Amazon ML to automatically associate the schema
    file to the data file and bypass the creation of its own schema.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了原始的JSON格式模式。将其保存在本地机器上，文件名为`titanic_train.csv.schema`。我们将在S3的同一存储桶/文件夹中加载此文件，该文件夹包含`titanic_train.csv`文件。通过在数据CSV文件名中添加`.schema`，我们允许亚马逊机器学习自动将模式文件与数据文件关联，并绕过其自身模式的创建。
- en: 'Open the schema file with your favorite editor and edit as such:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用您最喜欢的编辑器打开模式文件，并按如下方式编辑：
- en: Add `home.dest`*,* `body`, `boat` in the `excludedAttributeNames` field
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`excludedAttributeNames`字段中添加`home.dest`、`body`、`boat`
- en: Change the datatype from `CATEGORICAL` to `NUMERIC` for `sibps` and `parch`
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`sibsp`和`parch`的数据类型从`CATEGORICAL`更改为`NUMERIC`
- en: 'Note that although we want to remove the fields `boat`, `body`, and `home.dest`,
    we still need to declare them and their data types in the schema. Your JSON file
    should now look as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管我们想要删除字段`boat`、`body`和`home.dest`，我们仍然需要在模式中声明它们及其数据类型。您的JSON文件现在应如下所示：
- en: '[PRE1]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Upload that modified `titanic_train.csv.schema` file to the same S3 location
    as your `titanic_train.csv` data file. Your bucket/folder should now look like
    this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将修改后的`titanic_train.csv.schema`文件上传到与`titanic_train.csv`数据文件相同的S3位置。您的存储桶/文件夹现在应如下所示：
- en: '![](img/B05028_04_13.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_13.png)'
- en: 'Let us now create a new datasource. Since the schema file has the same name
    as the data file and is contained at the same location, Amazon ML will use the
    schema we provided:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个新的数据源。由于模式文件和数据文件具有相同的名称，并且位于同一位置，亚马逊机器学习将使用我们提供的模式：
- en: Go back to the Amazon ML dashboard and click on Create a new datasource in the
    main menu.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回亚马逊机器学习仪表板，并在主菜单中单击创建新的数据源。
- en: Indicate location and name the datasource; we named this new datasource Titanic
    train set 11 variables.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定位置并命名数据源；我们给这个新的数据源命名为Titanic train set 11 variables。
- en: 'Amazon ML confirms that the schema you provided has been taken into account:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊机器学习确认您提供的模式已被考虑：
- en: '![](img/B05028_04_14.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_14.png)'
- en: Go through the remaining datasource creation steps and notice that the data
    types correspond to the ones you specified in the schema file and that the three
    fields are no longer present.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 完成剩余的数据源创建步骤，并注意数据类型与您在模式文件中指定的数据类型相对应，并且三个字段不再存在。
- en: '**Schema recap**: To associate a schema to a file, it suffices to name the
    schema with the same name as the data file and add the .schema extension. For
    instance, for a data file named `my_data.csv`, the schema file should be named
    `my_data.csv.schema` and be uploaded to the same S3 location as the data file.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**模式回顾**：要将模式关联到文件，只需将模式文件命名为与数据文件相同的名称，并添加`.schema`扩展名。例如，对于名为`my_data.csv`的数据文件，模式文件应命名为`my_data.csv.schema`，并上传到与数据文件相同的S3位置。'
- en: Our datasource has now been created, and we can explore what type of insights
    into the data Amazon ML gives us.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据源现在已经创建完成，我们可以探索亚马逊机器学习为我们提供的数据洞察。
- en: Examining data statistics
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查数据统计
- en: 'When Amazon ML created the data source, it carried out a basic statistical
    analysis of the different variables. For each variable, it estimated the following
    information:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当亚马逊机器学习创建数据源时，它对不同的变量进行了基本的统计分析。对于每个变量，它估计以下信息：
- en: Correlation of each attribute to the target
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个属性与目标的相关性
- en: Number of missing values
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值的数量
- en: Number of invalid values
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无效值的数量
- en: Distribution of numeric variables with histogram and box plot
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有直方图和箱线图的数值变量分布
- en: Range, mean, and median for numeric variables
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值变量的范围、平均值和中位数
- en: Most and least frequent categories for categorical variables
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类别变量最频繁和最不频繁的类别
- en: Word counts for text variables
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本变量的词频
- en: Percentage of true values for binary variables
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元变量的真实值百分比
- en: 'Go to the Datasource dashboard, and click on the new datasource you just created
    in order to access the data summary page. The left side menu lets you access data
    statistics for the target and different attributes, grouped by data types. The
    following screenshot shows data insights for the Numeric attributes. The `age`
    and `fare` variables are worth looking at more closely:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 前往数据源仪表板，然后点击您刚刚创建的新数据源，以便访问数据摘要页面。左侧菜单允许您访问针对目标和不同属性的数据统计信息，按数据类型分组。以下截图显示了数值属性的洞察。`年龄`和`票价`变量值得更仔细地查看：
- en: '![](img/B05028_04_15.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/B05028_04_15.png)'
- en: 'Two things stand out:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 两件事引人注目：
- en: '`age` has `20%` missing values. We should replace these missing values by the
    mean or the median values of the existing values of `age`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`年龄`有`20%`的缺失值。我们应该用`年龄`现有值的平均值或中位数来替换这些缺失值。'
- en: The mean for `fare` is 33.28, but the range is `0-512.32`, indicating a highly
    skewed distribution. Looking at the `fare` distribution confirms that. Click on
    the Preview.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`票价`的平均值是33.28，但范围是`0-512.32`，表明分布高度偏斜。查看`票价`分布可以证实这一点。点击预览。'
- en: 'The following screenshot shows the histogram and the associated box plot for
    the `fare` attribute. We can see that most of the values are below 155, with very
    few values above 195\. This shows that the 521.32 value may well be an outlier,
    an invalid value caused by human error. Looking back at the original dataset,
    we see that four passengers from the same family with the same ticket number (*PC
    17755*) paid this price for four first class cabins *(B51, B53, B55,* and *B101).*
    Although that fare of 512.32 value is well above any other fare, it does look
    legit and not like an error of some sort. We should probably not discard or replace it.
    The following histogram shows the `fare` distribution:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`票价`属性的直方图和相关箱线图。我们可以看到，大多数值都低于155，而高于195的值非常少。这表明521.32这个值可能是一个异常值，是由人为错误造成的无效值。回顾原始数据集，我们看到有四名来自同一家庭、持有相同票号（*PC
    17755*）的乘客为四个头等舱（*B51、B53、B55*和*B101*）支付了此价格。尽管512.32的票价远高于其他票价，但它看起来很合法，并不像某种错误。我们可能不应该丢弃或替换它。以下直方图显示了`票价`的分布：
- en: '![](img/B05028_04_16.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/B05028_04_16.png)'
- en: 'The Text attributes have automatically been tokenized by Amazon ML and each
    word has been extracted from the original attribute. Punctuation has also been
    removed. Amazon ML then calculates word frequency as shown in the following screenshot
    for the `name` attribute:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 文本属性已经由Amazon ML自动分词，并且每个单词都从原始属性中提取出来。标点符号也已删除。Amazon ML随后计算了以下截图所示的`姓名`属性的词频：
- en: '![](img/B05028_04_17.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/B05028_04_17.png)'
- en: Feature engineering with Athena
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Athena进行特征工程
- en: At this point, we have a decent set of variables that can help predict whether
    a passenger survived the Titanic disaster. However, that data could use a bit
    of cleaning up in order to handle outliers and missing values. We could also try
    to extract other meaningful features from existing attributes to boost our predictions.
    In other terms, we want to do some feature engineering. Feature engineering is
    the key to boosting the accuracy of your predictions.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一组相当不错的变量，可以帮助预测乘客是否在泰坦尼克号灾难中幸存。然而，这些数据可能需要一些清理，以处理异常值和缺失值。我们还可以尝试从现有属性中提取其他有意义的特征，以提高我们的预测能力。换句话说，我们想要进行一些特征工程。特征工程是提高预测准确率的关键。
- en: Feature engineering is the process of using domain knowledge of the data to
    create features that make machine learning algorithms work.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是使用数据领域知识创建特征的过程，这些特征可以使机器学习算法工作。
- en: '- Wikipedia'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '- 维基百科'
- en: ML offers what it calls data recipes to transform the data and adapt it to its
    linear regression and logistic regression algorithm. In Amazon ML, data recipes
    are part of building the predictive model, not creating the datasource. We study
    Amazon ML's data recipes extensively in [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml),
    *Model Creation*. Amazon ML's data recipes are mostly suited to adapt the data
    to the algorithm and is a bit limited to correcting problems in the original data
    or creating new attributes from existing ones. For instance, removing outliers,
    extracting keywords from text, or replacing missing values are not possible with
    Amazon ML's recipes. We will, therefore, use SQL queries to perform some data
    cleaning and feature creation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习提供了它所谓的“数据食谱”来转换数据并适应其线性回归和逻辑回归算法。在 Amazon ML 中，数据食谱是构建预测模型的一部分，而不是创建数据源。我们在第
    5 章 [Model Creation](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml) 中广泛研究了 Amazon
    ML 的数据食谱。Amazon ML 的数据食谱主要适用于将数据适应到算法中，并且对于纠正原始数据中的问题或从现有数据中创建新属性有一定的局限性。例如，使用
    Amazon ML 的食谱无法去除异常值、从文本中提取关键词或替换缺失值。因此，我们将使用 SQL 查询来执行一些数据清洗和特征创建。
- en: 'Several AWS services are based on SQL databases: Redshift, RDS, and more recently,
    **Athena**. SQL is not only widely used to query data but, as we will see, it
    is particularly well suited for data transformation and data cleaning.  Many creative
    ideas on how to squeeze out information for the original Titanic dataset can be
    found online. These online sources, for instance, offer many ideas on the subject
    of feature engineering on the Titanic dataset:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 几个 AWS 服务基于 SQL 数据库：Redshift、RDS 以及最近新增的 **Athena**。SQL 不仅广泛用于查询数据，正如我们将看到的，它特别适合于数据转换和数据清洗。在网上可以找到许多关于如何从原始泰坦尼克数据集中提取信息的创意想法。例如，这些在线资源提供了关于泰坦尼克数据集特征工程主题的许多想法：
- en: Analysis of the Titanic dataset for Kaggle competition on the Ultraviolet blog
    with code example in Python at [http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/](http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在紫外光博客上对泰坦尼克数据集进行的 Kaggle 竞赛分析，其中包含 Python 代码示例，链接为 [http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/](http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/)
- en: Similar analysis with example in R on Trevor Stephens blog: [http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/](http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Trevor Stephens 的博客上使用 R 进行类似分析，示例链接：[http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/](http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/)
- en: The Titanic Kaggle competition forums are ripe with ideas: [https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泰坦尼克 Kaggle 竞赛论坛充满了创意：[https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)
- en: 'In our case, we will focus on the following transformations:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将关注以下转换：
- en: Replacing the missing `age` values with the average of existing `age` values
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用现有 `age` 值的平均值替换缺失的 `age` 值
- en: Instead of replacing or discarding the fare outlier values, we will create a
    new log (`fare`) variable with a distribution less skewed
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 而不是替换或丢弃票价异常值，我们将创建一个新的对数变量 (`fare`)，其分布更少偏斜
- en: Extracting titles from the `name` field
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 `name` 字段中提取标题
- en: Each cabin number is referred to by three characters, where the first character
    is a letter relative to the deck level (A, B, C, ..., F) and the number of cabin
    number, on that deck; we will extract the first letter of each cabin as a new
    `deck` variable
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个客舱号由三个字符组成，其中第一个字符是相对于甲板级别的字母（A、B、C、...、F）以及该甲板上的客舱号数量；我们将提取每个客舱的第一个字母作为新的
    `deck` 变量
- en: We will also combine `sibps` and `parch` to create a `family_size` variable
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将 `sibps` 和 `parch` 合并来创建 `family_size` 变量
- en: In the following section, we will use Athena to do these data transformations.
    We will first create a database and table in Athena and fill it with the Titanic
    data. We will then create new features using standard SQL queries on the newly
    created table. Finally, we will export the results to CSV in S3 and create a new
    datasource from it.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用 Athena 来进行这些数据转换。我们首先在 Athena 中创建数据库和表，并用泰坦尼克数据填充它。然后，我们将使用标准 SQL
    查询在新建的表上创建新特征。最后，我们将结果导出到 S3 中的 CSV 文件，并从中创建新的数据源。
- en: Introducing Athena
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Athena
- en: 'Athena was launched during the *AWS re:Invent* conference in *December 2016*.
    It complements other AWS SQL based services by offering a simple serverless service
    that directly interacts with S3\. According to AWS, Amazon Athena is an interactive
    query service that makes it easy to analyze data directly from Amazon S3 using
    standard SQL*. *Several attributes of Amazon Athena make it particularly adapted
    for data preparation with Amazon ML:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Athena于2016年12月的*AWS re:Invent*大会上推出。它通过提供一种简单的无服务器服务，直接与S3交互，补充了其他AWS基于SQL的服务。根据AWS的说法，Amazon
    Athena是一个交互式查询服务，它使得使用标准SQL直接从Amazon S3分析数据变得容易。*Amazon Athena的几个特性使其特别适合与Amazon
    ML进行数据准备：
- en: Athena can generate tables directly from data available in S3 in different formats
    (CSV, JSON, TSV, amazon logs, and others). Since datasources in Amazon ML are
    also S3-based, it is easy to manipulate files, perform various data transformation,
    and create various datasets on the fly to test your models.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athena可以直接从S3中不同格式的数据（CSV、JSON、TSV、亚马逊日志等）生成表格。由于Amazon ML中的数据源也是基于S3的，因此可以轻松地操作文件，执行各种数据转换，并即时创建各种数据集来测试您的模型。
- en: Athena is fast, simple, and can handle massive datasets.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athena速度快，简单，可以处理大量数据集。
- en: Athena uses `prestodb`, a distributed SQL query engine developed by Facebook
    that offers a very rich library of SQL functions, which are well suited to do
    data transformations and feature engineering.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athena使用`prestodb`，这是Facebook开发的一个分布式SQL查询引擎，它提供了一个非常丰富的SQL函数库，非常适合进行数据转换和特征工程。
- en: Amazon ML only accepts CSV files to create datasources from S3\. However, since
    Athena can gather data from other file formats besides CSV, (JSON, TSV, ORC, and
    others) and export query results to a CSV file, it's possible to use Athena as
    a conversion tool and expand Amazon ML sourcing capabilities that way.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon ML仅接受CSV文件从S3创建数据源。然而，由于Athena可以收集除CSV之外的其他文件格式的数据（JSON、TSV、ORC等），并将查询结果导出为CSV文件，因此可以使用Athena作为转换工具，从而扩展Amazon
    ML的数据源能力。
- en: Presto is an open-source distributed SQL query engine optimized for low-latency,
    ad-hoc analysis of data. It supports the ANSI SQL standard, including complex
    queries, aggregations, joins, and window functions. More information can be found
    at [https://prestodb.io/](https://prestodb.io/).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Presto是一个开源的分布式SQL查询引擎，针对低延迟、即席数据分析进行了优化。它支持ANSI SQL标准，包括复杂查询、聚合、连接和窗口函数。更多信息可以在[https://prestodb.io/](https://prestodb.io/)找到。
- en: 'In this section, we will perform the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将执行以下操作：
- en: Create an Athena account and a database.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Athena账户和一个数据库。
- en: Create and populate a table directly from the S3 Titanic CSV file.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接从S3的Titanic CSV文件创建并填充一个表。
- en: Perform queries on the dataset and create new features.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集进行查询并创建新的特征。
- en: Download the results to S3.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果下载到S3。
- en: Create new training and testing datasources with the extra features in Amazon
    ML.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Amazon ML的额外功能创建新的训练和测试数据源。
- en: Athena is simple. Let's start with a simple overview.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Athena简单。让我们从简单的概述开始。
- en: A brief tour of AWS Athena
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS Athena简要概述
- en: In Athena, the data is not stored in a database; it remains in S3\. When you
    create a table in Athena, you are creating an information layer that tells Athena
    where to find the data, how it is structured, and what format it is in. The schema
    in Athena is a logical namespace of objects. Once the data structure and location
    are known to Athena, you can query the data via standard SQL statements.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在Athena中，数据不会存储在数据库中；它仍然保留在S3中。当您在Athena中创建一个表时，您实际上是在创建一个信息层，告诉Athena数据的位置、结构以及格式。Athena中的模式是对象的逻辑命名空间。一旦Athena知道了数据结构和位置，您就可以通过标准SQL语句查询数据。
- en: Athena uses **Hive Data Definition Language (DDL)** to create or drop databases
    and tables (more information on Hive DDL can be found at [https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL)).
    Athena can understand multiple formats (CSV, TSV, JSON, and so on) through the
    use of `serializer-deserializer (SerDes)` libraries. Athena is available either
    via the console ([https://console.aws.amazon.com/athena/](https://console.aws.amazon.com/athena/))
    or via JDBC connection ([http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html](http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html)).
    We will use the console.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Athena使用**Hive数据定义语言（DDL）**来创建或删除数据库和表（有关Hive DDL的更多信息，请参阅[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL)）。Athena可以通过使用`序列化-反序列化（SerDes）`库理解多种格式（CSV、TSV、JSON等）。Athena可以通过控制台（[https://console.aws.amazon.com/athena/](https://console.aws.amazon.com/athena/））或通过JDBC连接（[http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html](http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html)）访问。我们将使用控制台。
- en: 'The **Athena** service offers four sections:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**Athena**服务提供四个部分：'
- en: '![](img/B05028_04_18.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_04_18.png)'
- en: 'The Query Editor:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 查询编辑器：
- en: Lets you write your queries, save them, and see the results as well as navigate
    among your databases and tables.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 允许您编写查询、保存它们，并查看结果，同时还可以在您的数据库和表之间导航。
- en: A Saved Queries page listing all the queries you saved.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个列出您保存的所有查询的已保存查询页面。
- en: A History page listing all the queries you ran.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个列出您运行的所有查询的历史页面。
- en: A Catalog Manager that lets you explore your stored data.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个允许您探索存储数据的目录管理器。
- en: These sections are self-explanatory to use, and we will let you explore them
    at your leisure.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这些部分的使用说明很直观，我们将让您在空闲时探索它们。
- en: At the time of writing, Amazon Athena does not have a **Command-Line interface
    (cli)**. However, Athena can be accessed via a **Java Database Connectivity (JDBC)**
    driver available on S3\. You will find more information at [http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html](http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Amazon Athena没有**命令行界面（cli）**。然而，Athena可以通过S3上可用的**Java数据库连接（JDBC）驱动程序**访问。您可以在[http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html](http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html)找到更多信息。
- en: 'A few things to know:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一些需要知道的事情：
- en: You can create a table by specifying the location of the data in S3 or explicitly
    via an SQL query.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过指定S3中的数据位置或通过SQL查询明确创建一个表。
- en: 'All tables must be created as `EXTERNAL` and it is not possible to** CREATE
    TABLE AS SELECT**. Dropping a table created with the External keyword does not
    delete the underlying data:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有表都必须以`EXTERNAL`方式创建，并且无法使用**CREATE TABLE AS SELECT**。使用外部关键字创建的表删除不会删除底层数据：
- en: '[PRE2]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The presto SQL functions are available in Athena. Take a look at [https://prestodb.io/docs/current/functions.html](https://prestodb.io/docs/current/functions.html) for
    a full list.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Presto SQL函数在Athena中可用。请参阅[https://prestodb.io/docs/current/functions.html](https://prestodb.io/docs/current/functions.html)以获取完整列表。
- en: You can only submit one query and run five concurrent queries at the same time.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您一次只能提交一个查询，同时最多运行五个并发查询。
- en: '**Athena Pricing:** Athena charges based on the amount of data scanned by the
    query with (at the time of writing) a $5 fee per TB of data scanned with a minimum
    of 10 MB per query. You can reduce your costs by converting your data to columnar
    formats or partitioning your data. Refer to [https://aws.amazon.com/athena/pricing/](https://aws.amazon.com/athena/pricing/) for
    more information.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**Athena定价：**Athena根据查询扫描的数据量收费（在撰写本文时，每TB数据扫描费用为5美元，每个查询最低为10 MB。您可以通过将数据转换为列式格式或对数据进行分区来降低成本。有关更多信息，请参阅[https://aws.amazon.com/athena/pricing/](https://aws.amazon.com/athena/pricing/)）。'
- en: Creating a titanic database
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个巨大的数据库
- en: 'We are going to start from scratch and go back to the original Titanic dataset
    available at [https://github.com/alexperrier/packt-aml/blob/master/ch4/original_titanic.csv](https://github.com/alexperrier/packt-aml/blob/master/ch4/original_titanic.csv). Follow
    these steps to prepare the CSV file:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从零开始，回到可在[https://github.com/alexperrier/packt-aml/blob/master/ch4/original_titanic.csv](https://github.com/alexperrier/packt-aml/blob/master/ch4/original_titanic.csv)找到的原始泰坦尼克号数据集。按照以下步骤准备CSV文件：
- en: Open the `original_titanic.csv` file.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`original_titanic.csv`文件。
- en: Remove the header row.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除标题行。
- en: 'Remove the following punctuation characters: `,"()`.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除以下标点符号字符：`,"()`。
- en: The file should only contain data, not column names. This is the original file
    with 1309 rows. These rows are ordered by `pclass` and alphabetical names. The
    resulting file is available at [https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic_for_athena.csv](https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic_for_athena.csv).
    Let us create a new `athena_data` folder in our S3 bucket and upload the `titanic_for_athena.csv`
    file. Now go to the Athena console. We will create a `titanic_db` database and
    a `titanic` table with the data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 文件应只包含数据，不包含列名。这是包含1309行的原始文件。这些行按`pclass`排序和按字母顺序命名。结果文件可在[https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic_for_athena.csv](https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic_for_athena.csv)找到。现在让我们在我们的S3桶中创建一个新的`athena_data`文件夹，并将`titanic_for_athena.csv`文件上传。现在转到Athena控制台。我们将创建一个`titanic_db`数据库和一个`titanic`表，并使用这些数据。
- en: Using the wizard
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用向导
- en: 'There are two ways to create databases and tables in Athena, via the wizard
    or by running queries. The wizard is accessible by clicking on the *Add table* link
    in the Query Editor page:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在Athena中创建数据库和表有两种方式，通过向导或运行查询。可以通过在查询编辑器页面点击“*添加表*”链接来访问向导：
- en: '![](img/B05028_04_19.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_19.png)'
- en: 'In four steps, the wizard allows us to create the database, the table, and
    load the data. Creating the columns in *step 3* involves manually typing the name
    of each column and specifying each column type. With 14 columns in the dataset,
    this manual approach is time-consuming. We will, therefore, not use the wizard
    and switch to creating the database and table directly in SQL:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 向导在四个步骤中允许我们创建数据库、表和加载数据。在*第3步*中创建列涉及手动输入每个列的名称并指定每个列的类型。由于数据集中有14个列，这种方法很耗时。因此，我们将不使用向导，直接在SQL中创建数据库和表：
- en: '![](img/B05028_04_20.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_20.png)'
- en: Creating the database and table directly in SQL
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接在SQL中创建数据库和表
- en: 'To create the database, run the following query in the query editor:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建数据库，请在查询编辑器中运行以下查询：
- en: '[PRE3]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then select `titanic_db` in the database dropdown menu on the left side as
    follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如下所示，在左侧的数据库下拉菜单中选择`titanic_db`：
- en: '![](img/B05028_04_21.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_04_21.png)'
- en: 'To create the table and load the data, run the following SQL query:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建表并加载数据，请运行以下SQL查询：
- en: '[PRE4]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A few things to note about that query:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 关于该查询的几点注意事项：
- en: The location `s3://aml.packt/athena_data/` points to the folder we have specially
    created, not to the file itself . All the files in that folder will be considered
    as data for that table.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置`s3://aml.packt/athena_data/`指向我们特别创建的文件夹，而不是指向文件本身。该文件夹中的所有文件都将被视为该表的数据。
- en: The `SERDE` corresponds to the CSV format with a comma as the field delimiter.
    The list of supported formats and respective SERDE is available at [https://docs.aws.amazon.com/athena/latest/ug/supported-formats.html](https://docs.aws.amazon.com/athena/latest/ug/supported-formats.html).
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SERDE`对应于以逗号作为字段分隔符的CSV格式。支持的格式及其相应的SERDE列表可在[https://docs.aws.amazon.com/athena/latest/ug/supported-formats.html](https://docs.aws.amazon.com/athena/latest/ug/supported-formats.html)找到。'
- en: The field types are standard SQL types (string, double, tinyint, and so on).
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字段类型是标准的SQL类型（字符串、双精度浮点数、小整数等）。
- en: 'Once the query has finished running, the `titanic` table name appears in the
    left section of the page. You can click on the eye icon to select the first 10
    rows of the table as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦查询运行完成，`titanic`表名就会出现在页面左侧部分。您可以点击眼睛图标来选择表的前10行，如下所示：
- en: '![](img/B05028_04_22.png)**Specifying the results location**: Athena stores
    the results of your queries in a new S3 folder. You can specify what folder you
    want these results to be stored in by going to the settings page and specifying
    the desired S3 path. We have created a new folder titled  `*athena_query_results*`
    in our `aml.packt` bucket and set the result location to  `*s3://aml.packt/athena_query_results/*`.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B05028_04_22.png)**指定结果位置**：Athena将查询结果存储在新的S3文件夹中。您可以通过访问设置页面并指定所需的S3路径来指定这些结果存储在哪个文件夹中。我们在`aml.packt`桶中创建了一个名为`*athena_query_results*`的新文件夹，并将结果位置设置为`*s3://aml.packt/athena_query_results/*`。'
- en: Data munging in SQL
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL中的数据处理
- en: We will now define the SQL query that will correct the missing value and outlier
    problems we saw earlier and create some new variables.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将定义一个SQL查询，该查询将纠正我们之前看到的缺失值和异常值问题，并创建一些新变量。
- en: Missing values
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失值
- en: 'We have missing values for the `age` and `fare` variables with respective median
    values `28` and `14.5`. We can replace all the missing values with the median
    values with this statement:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`年龄`和`票价`变量有缺失值，分别的中位数为`28`和`14.5`。我们可以使用此语句用中位数替换所有缺失值：'
- en: '[PRE5]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We also want to keep the information that there was a missing value at least
    for the `age` variable. We can do that with the query that creates a new binary variable
    that we name: `is_age_missing`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望保留至少对于`年龄`变量存在缺失值的信息。我们可以通过创建一个名为`is_age_missing`的新二进制变量来实现这一点：
- en: '[PRE6]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Handling outliers in the fare
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理票价中的异常值
- en: We have seen that the `fare` had four passengers paying a much higher price
    than the others. There are several ways to deal with these values, one of which
    can be to bin the variable by defining a series of specific ranges. For instance,
    below 20; from 20 to 50; 50 to 100, ..., and over 200\. Binning can be done with
    Amazon ML recipes.  We could also cap the fare value at a specific threshold,
    such as the 95% percentile. However, we decided that these large fare values were legit
    and that we ought to keep them. We can still create a new variable, `log_fare`*,* with
    a more compact range and a less skewed distribution by taking the log of the `fare:`
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，`票价`有四个乘客支付的价格远高于其他人。处理这些值有几种方法，其中之一可以通过定义一系列特定范围对变量进行分箱。例如，低于20；从20到50；50到100，...，以及超过200。可以使用Amazon
    ML食谱进行分箱。我们还可以将票价值限制在特定阈值，例如95%分位数。然而，我们决定这些高额票价是合法的，我们应该保留它们。我们可以通过取`票价`的对数来创建一个新的变量`log_fare`*，*，它具有更紧凑的范围和更少偏斜的分布：
- en: '[PRE7]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `log_fare` variable has a range of *0 - 9.0*, a mean of *4.3*, and a median
    of *3.95*, whereas the original `fare` variable had a range of *[0, 512.3]*, mean *32.94*,
    and median *14.5*. The distribution of the `log_fare` is closer to a Gaussian
    distribution than the distribution of the original `fare` variable.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`log_fare`变量范围为*0 - 9.0*，平均值为*4.3*，中位数为*3.95*，而原始`票价`变量范围为*[0, 512.3]*，平均值为*32.94*，中位数为*14.5*。`log_fare`的分布比原始`票价`变量的分布更接近高斯分布。'
- en: '**Box-Cox transformation**: Because of the linear regression assumptions, it
    is better to have variables with Gaussian distributions. The Box-Cox transformation,
    also known as the power transform (see [https://en.wikipedia.org/wiki/Power_transform](https://en.wikipedia.org/wiki/Power_transform)),
    is a common method to reshape a variable into a normally-distributed one. The
    Box-Cox transformation is a generalization of the log transformation we just applied
    to the `fare` variable.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**Box-Cox转换**：由于线性回归的假设，最好有高斯分布的变量。Box-Cox转换，也称为幂转换（见[https://en.wikipedia.org/wiki/Power_transform](https://en.wikipedia.org/wiki/Power_transform)），是将变量重塑为正态分布的常用方法。Box-Cox转换是对我们刚刚应用于`票价`变量的对数转换的推广。'
- en: Extracting the title from the name
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从名称中提取标题
- en: 'If you look closely at the passenger names, you will notice that they are all
    in the `{family name}{title}{first names}`format. It would be interesting to extract
    the `title` as a new variable. The following query uses the split function, which
    returns an array. We need the second element of that array:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察乘客姓名，你会注意到它们都遵循`{姓氏}{头衔}{名字}`格式。提取`头衔`作为一个新变量将很有趣。以下查询使用split函数，它返回一个数组。我们需要该数组的第二个元素：
- en: '[PRE8]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Inferring the deck from the cabin
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从客舱推断舱位
- en: 'The cabin variable has three character values, where the first character corresponds
    to the Deck number (A, B, C, D, E). This is surely important information that
    we would like to extract.  We can do so with the following query:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 舱位变量有三个字符值，其中第一个字符对应于甲板号（A，B，C，D，E）。这无疑是重要的信息，我们希望提取。我们可以使用以下查询来完成：
- en: '[PRE9]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Calculating family size
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算家庭规模
- en: 'Finally, it makes sense to assume that the overall family size a passenger
    belongs to might have been a decisive factor in survival. We can aggregate the
    number of siblings and the number of parents and add 1 for the passenger. The
    following simple query will create the family size variable:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，假设乘客所属的整体家庭规模可能是生存的决定性因素是有意义的。我们可以聚合兄弟姐妹的数量和父母数量，并为乘客加1。以下简单的查询将创建家庭规模变量：
- en: '[PRE10]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Wrapping up
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'We can combine all these queries while also selecting the original attributes.
    Since the data is still ordered by `pclass` and passenger `name` in alphabetical
    order, we should also randomize the results. We end up with the following query:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在选择原始属性的同时合并所有这些查询。由于数据仍然按`pclass`和乘客`name`的字母顺序排序，我们还应该随机化结果。我们最终得到以下查询：
- en: '[PRE11]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let us run that query. The results will be displayed in the results panel and
    also written in a CSV file in the query result location on S3\. You can also save
    it on your local machine by clicking at the icon in the upper right corner of
    the results panel:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行这个查询。结果将在结果面板中显示，并也写入S3查询结果位置的CSV文件中。你也可以通过点击结果面板右上角的图标将其保存到你的本地机器上：
- en: '![](img/B05028_04_23.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_04_23.png)'
- en: At this point, we want to split the data into a training and a testing set like
    we did previously and create a new Amazon ML datasource with an extended schema.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们想要将数据分成训练集和测试集，就像我们之前做的那样，并创建一个新的Amazon ML数据源，具有扩展的架构。
- en: Creating an improved datasource
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建改进的数据源
- en: 'We need to do some manipulation on the new `Titanic` dataset before we upload
    it to S3 and create a new datasource in Amazon ML:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将数据上传到S3并创建新的Amazon ML数据源之前，我们需要对新的`Titanic`数据集进行一些操作：
- en: Open this new Titanic dataset in your favorite editor.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你最喜欢的编辑器中打开这个新的Titanic数据集。
- en: 'Select the first *1047* rows, and save them to a new CSV: `ext_titanic_training.csv`.'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择前*1047*行，并将它们保存到一个新的CSV文件中：`ext_titanic_training.csv`。
- en: Select the next *263* rows and the header row, and save them to a file `ext_titanic_heldout.csv`.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择下一*263*行和标题行，并将它们保存到文件`ext_titanic_heldout.csv`中。
- en: 'We need to update our schema. Open the schema file `titanic_training.csv.schema`,
    and add the following lines to the JSON:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要更新我们的架构。打开架构文件`titanic_training.csv.schema`，并在JSON中添加以下行：
- en: '[PRE12]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The new schema file as well as the training and held-out sets can be found at [https://github.com/alexperrier/packt-aml/tree/master/ch4](https://github.com/alexperrier/packt-aml/tree/master/ch4).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 新的架构文件以及训练集和保留集可以在[https://github.com/alexperrier/packt-aml/tree/master/ch4](https://github.com/alexperrier/packt-aml/tree/master/ch4)找到。
- en: We then need to upload the training and the schema file to S3\. These files
    should be in the same S3 location, `{bucket}/{folders}`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来需要将训练集和架构文件上传到S3。这些文件应该在同一个S3位置，`{bucket}/{folders}`。
- en: 'We are now ready to create a new datasource based on this extended Titanic
    training set following the exact same steps as before:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好根据扩展的Titanic训练集创建新的数据源，按照之前完全相同的步骤进行：
- en: Specify the location of the input data.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定输入数据的位置。
- en: Review the schema.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查架构。
- en: Set the target as `survived`.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目标设置为`survived`。
- en: Bypass the `rowID`.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过`rowID`。
- en: Review and create.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查并创建。
- en: We now have 19 attributes in the schema and a brand new datasource.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有19个属性在架构中，以及一个全新的数据源。
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on what is commonly known as the **Extract Load
    Transform (ETL)** part of the data science flow with regard to the Amazon ML service.
    We saw that the Amazon ML datasource is a set of information comprised of location,
    data structure, and data analytics given to the service so that it can use that
    data to start training models. You should now feel comfortable creating an Amazon
    ML datasource from an original CSV data file made accessible via S3.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于通常所说的数据科学流程中Amazon ML服务的**提取、加载、转换（ETL）**部分。我们了解到Amazon ML数据源是一组信息，包括位置、数据结构和数据分析，提供给服务以便它可以使用这些数据来开始训练模型。你现在应该能够从通过S3可访问的原始CSV数据文件中创建Amazon
    ML数据源感到舒适。
- en: We have also explored ways to transform the data and create new features via
    the AWS Athena service using simple SQL queries. The ability to complement the
    features of Amazon ML by leveraging the AWS ecosystem is one of the main benefits
    of using Amazon ML.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了通过使用简单的SQL查询通过AWS Athena服务转换数据和创建新特征的方法。利用AWS生态系统来补充Amazon ML的功能是使用Amazon
    ML的主要好处之一。
- en: We now have a couple of Titanic datasets, the original one and the extended
    one, which are split into training and held-out subsets, and we have created the
    associated datasources.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有几个Titanic数据集，原始的一个和扩展的一个，它们被分割成训练集和保留集，并且我们已经创建了相关的数据源。
- en: In [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml), *Model Creation*,
    we will use these datasets to train models, and we will see if our new features
    and data cleaning result in better models.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml)“模型创建”中，我们将使用这些数据集来训练模型，并查看我们新的特征和数据清洗是否会导致更好的模型。
