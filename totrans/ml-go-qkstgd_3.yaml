- en: Supervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we learned in the first chapter, supervised learning is one of two major
    branches of machine learning. In a way, it is similar to how humans learn a new
    skill: someone else shows us what to do, and we are then able to learn by following
    their example. In the case of supervised learning algorithms, we usually need
    lots of examples, that is, lots of data providing the **input** to our algorithm
    and what the **expected output** should be. The algorithm will learn from this
    data, and then be able to **predict** the output based on new inputs that it has
    not seen before.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A surprising number of problems can be addressed using supervised learning.
    Many email systems use it to classify emails as either important or unimportant
    automatically whenever a new message arrives in the inbox. More complex examples
    include image recognition systems, which can identify what an image contains purely
    from the input pixel values^([1]). These systems start by learning from huge datasets
    of images that have been labelled manually by humans, but are then able to categorize
    completely new images automatically. It is even possible to use supervised learning
    to steer a car automatically around a racing track: the algorithm starts by learning
    how a human driver controls the vehicle, and is eventually able to replicate this
    behavior^([2]).'
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to use Go to implement two types
    of supervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**, where an algorithm must learn to classify the input into
    two or more discrete categories. We will build a simple image recognition system
    to demonstrate how this works.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**, in which the algorithm must learn to predict a continuous variable,
    for example, the price of an item for sale on a website. For our example, we will
    predict house prices based on inputs, such as the location, size, and age of the
    house.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: When to use regression and classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement regression and classification using Go machine learning libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to measure the performance of an algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will cover the two stages involved in building a supervised learning system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training**, which is the learning phase where we use labelled data to calibrate
    an algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference** or **prediction**, where we use the trained algorithm for its
    intended purpose: to make predictions from input data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When starting any supervised learning problem, the first step is to load and
    prepare the data. We are going to start by loading the **MNIST Fashion** **dataset**^([3]),
    a collection of small, grayscale images showing different items of clothing. Our
    job is to build a system that can recognize what is in each image; that is, does
    it contain a dress, a shoe, a coat, and so on?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to download the dataset by running the `download-fashion-mnist.sh`
    script in the code repository. Then, we will load it into Go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start by taking a look at a sample of the images. Each one is 28 x 28
    pixels, and each pixel has a value between 0 and 255\. We are going to use these
    pixel values as the inputs to our algorithm: our system will accept 784 inputs
    from an image and use them to classify the image according to which item of clothing
    it contains. In Jupyter, you can view an image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display one of the 28 x 28 images from the dataset, as shown in the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4ddbc0b-b7bd-4a7e-b2e0-4e8730942184.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To make this data suitable for a machine learning algorithm, we need to convert
    it into a dataframe format, as we learned in [Chapter 2](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml),
    *Setting Up the Development Environment*. To start, we will load the first 1,000
    images from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need a string array that contains the possible labels for each image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It is very important to start by reserving a small proportion of your data in
    order to test the finished algorithm. This allows us to measure how well the algorithm
    works on new data that was not used during training. If you do not do this, you
    will most likely build a system that works really well during training but performs
    badly when faced with new data. To start with, we are going to use 75% of the
    images to train our model and 25% of the images to test it.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting your data into a **training set** and a **test set** is crucial step
    when using supervised learning. It is normal to reserve 20-30% of the data for
    testing, but if your dataset is very large, you may be able to use less than this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `Split(df dataframe.DataFrame, valFraction float64)` function from
    the last chapter to prepare these two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A simple model – the logistic classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the simplest algorithms that solves our problem is a logistic classifier.
    This is what mathematicians call a **linear model**, which we can understand by
    thinking about a simple example where we are trying to classify the points on
    the following two charts as either circles or squares. A linear model will try
    to do this by drawing a straight line to separate the two types of point. This
    works very well on the left-hand chart, where the relationship between the inputs
    (on the chart axes) and the output (circle or square) is simple. However, it does
    not work on the right-hand chart, where it is not possible to split the points
    into two correct groups using a straight line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28aa78cc-b28a-4a64-bb0d-815d13704964.png)'
  prefs: []
  type: TYPE_IMG
- en: When faced with a new machine learning problem, it is advised that you start
    with a linear model as a **baseline**, and then compare other models to it. Although
    linear models ca not capture complex relationships in the input data, they are
    easy to understand and normally quick to implement and train. You might find that
    a linear model is good enough for the problem you are working on and save yourself
    time by not having to implement anything more complex. If not, you can try different
    algorithms and use the linear model to understand how much better they work.
  prefs: []
  type: TYPE_NORMAL
- en: A **baseline** is a simple model that you can use as a point of reference when
    comparing different machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to our image dataset, we are going to use a logistic classifier
    to decide whether an image contains trousers or not. First, let''s do some final
    data preparation: simplify the labels to be either trousers (`true`) or not-trousers
    (`false`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We are also going to normalize the pixel data so that, instead of being stored
    as integers between 0 and 255, it will be represented by floats between 0 and
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: Many supervised machine learning algorithms only work properly if the data is
    normalized, that is, rescaled so that it is between 0 and 1\. If you are having
    trouble getting an algorithm to train properly, make sure that you have normalized
    the data properly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After preparing the data properly, it is finally time to create a logistic
    classifier and train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Measuring performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our trained model, we need to measure how well it is performing
    by comparing the predictions it makes on each image with the ground truth (whether
    or not the image is a pair of trousers). A simple way to do this is to measure
    **accuracy**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** measures what proportion of the input data can be classified correctly
    by the algorithm, for example, 90%, if 90 out of 100 predictions from the algorithm
    are correct.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our Go code example, we can test the model by looping over the validation
    dataset and counting how many images are classified correctly. This will output
    a model accuracy of 98.8%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Precision and recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring accuracy can be very misleading. Suppose you are building a system
    to classify whether medical patients will test positive for a rare disease, and
    in the dataset only 0.1% of examples are in fact positive. A really bad algorithm
    might predict that nobody will test positive, and yet it has an accuracy of 99.9%
    simply because the disease is rare.
  prefs: []
  type: TYPE_NORMAL
- en: A dataset that has many more examples of one classification versus another is
    known as **unbalanced**. Unbalanced datasets need to be treated carefully when
    measuring algorithm performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better way to measure performance starts by putting each prediction from
    the algorithm into one of the following four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/115c28b2-7254-4bea-b5c2-10cb3bdd4daa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now define some new performance metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision** measures what fraction of the models true predictions are actually
    correct. In the following diagram, it is the true positives that are predicted
    from the model (the left-hand side of the circle) divided by all of the models
    positive predictions (everything in the circle).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall** measures how good the model is at identifying all the positive examples.
    In other words, the true positives (left-hand side of the circle) divided by all
    the datapoints that are actually positive (the entire left-hand side):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5ff88538-985a-4a66-9574-ea896bd17ba2.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows datapoints that have been predicted as true by the
    model in the central circle. The points that are actually true are on the left
    half of the diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision** and **recall** are more robust performance metrics when working
    with unbalanced datasets. Both range between 0 and 1, where 1 indicates perfect
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the code for the total count of true positives and false negatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now calculate precision and recall with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For our linear model, we get 100% precision, meaning that there are no false
    positives, and a recall of 90.3%.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to measure performance involves looking at how the classifier works
    in more detail. Inside our model, two things happen:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the model calculates a value between 0 and 1, indicating how likely it
    is that a given image should be classified as a pair of trousers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A threshold is set, so that only images scoring more than the threshold get
    classified as trousers. Setting different thresholds can improve precision at
    the expense of recall and vice versa.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we look at the model output *across all the different thresholds from 0
    to 1*, we can understand more about how useful it is. We do this using something
    called the **receiver operating characteristic** (**ROC**) curve, which is a plot
    of the true positive rate versus the false positive rate across the dataset for
    different threshold values. The following three examples show ROC curves for a
    bad, moderate, and very good classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34189d2a-5b38-4024-8e0c-0eaa87b148d0.png)'
  prefs: []
  type: TYPE_IMG
- en: By measuring the shaded area under these ROC curves, we get a simple metric
    of how good the model is, which is known as **area under curve** (**AUC)**. For
    the bad model, this is close to **0.5**, but for the very good model, it is close
    to **1.0**, indicating that the model can achieve *both* a high true positive
    rate and a low false positive rate.
  prefs: []
  type: TYPE_NORMAL
- en: The `gonum`/`stat` package provides a useful function for computing ROC curves,
    which we will use once we have extended the model to work with each of the different
    items of clothing in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The **receiver operating characteristic**, or **ROC curve**, is a plot of true
    positive rate versus false positive rate for different threshold values. It allows
    us to visualize how good the model is at classification. The AUC gives a simple
    measure of how good the classifier is.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have been using **binary classification**; that is, it should
    output `true` if the image shows a pair of trousers, and `false` otherwise. For
    some problems, such as detecting whether an email is important or not, this is
    all we need. But in this example, what we really want is a model that can identify
    all the different types of clothing in our dataset, that is, shirt, boot, dress,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'With some algorithm implementations, you will need to start by applying one-hot
    encoding to the output, as demonstrated in [Chapter 2](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml),
    *Setting Up the Development Environment*. However, for our example, we will use
    **softmax regression** in **goml/linear**, which does this step automatically.
    We can train the model by simply feeding it with the input (pixel values) and
    the integer output (0, 1, 2, ... representing t-shirt, trouser, pullover, and
    so on):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'When using this model for inference, it will output a vector of probabilities
    for each class; that is, it tells us what the probability that an input image
    is a t-shirt, trouser, and so on. This is exactly what we need for the ROC analysis,
    but, if we want a single prediction for each image, we can use the following func
    to find the class that has the *highest* probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can plot the ROC curve and the AUC for each individual class. The
    following code will loop over each example in the validation dataset and predict
    probabilities for each class using the new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now compute AUC values for each class, which shows that our model performs
    better on some classes than others:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For trousers, the AUC value is `0.96`, showing that even a simple linear model
    works really well in this case. However, shirt and pullover both score close to
    `0.6`. This makes intuitive sense: shirts and pullovers look very similar, and
    are therefore much harder for the model to recognize correctly. We can see this
    more clearly by plotting the ROC curve for each class as separate lines: the model
    clearly performs the worst on shirts and pullovers, and the best on the clothes
    that have a very distinctive shape (boots, trousers, sandals, and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code loads gonums plotting libraries, creates the ROC plot, and
    saves it as a JPEG image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If we view the plot in Jupyter, we can see that the the worst classes follow
    the lines close to the diagonal, again indicating an AUC close to `0.5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c384cecc-fbc7-4f0a-9daf-7377d5e36731.png)'
  prefs: []
  type: TYPE_IMG
- en: A non-linear model – the support vector machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To move forward, we need to use a different machine learning algorithm: one
    that is able to model more complex, non-linear relationships between the pixel
    inputs and the output classes. While some of the mainstream Go machine learning
    libraries such as Golearn have support for basic algorithms like local least squares,
    there is not a single library that supports as broad a set of algorithms as Python''s
    scikit-learn or R''s standard library. For this reason, it is often necessary
    to search for alternative libraries that implement bindings to a widely used C
    library, or that contain a configurable implementation of an algorithm that is
    suited for a particular problem. For this example, we are going to use an algorithm
    called the **support vector machine** (**SVM**).SVMs can be more difficult to
    use than linear models—they have more parameters to tune—but have the advantage
    of being able to model much more complex patterns in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: An SVMis a more advanced machine learning method that can be used both for classification
    and regression. They allow us to apply **kernels** to the input data, which means
    that they can model non-linear relationships between the inputs/outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important feature of SVM models is their ability to use a **kernel function**.
    Put simply, this means that the algorithm can apply a transformation to the input
    data so that non-linear patterns can be found. For our example, we will use the **LIBSVM**
    library to train an SVM on the image data. LIBSVM is an open source library with
    bindings for many different languages, meaning that it is also useful if you want
    to port a model that has been built in Python''s popular scikit-learn library.
    First, we need to do some data preparation to make our input/output data suitable
    for feeding into the Go library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can set up the SVM model and configure it with a **radial basis function**
    (**RBF**) **kernel**. RBF kernels are a common choice when using SVMs, but do
    take longer to train than linear models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can fit our model to the training data of 750 images, and then
    use `svm.SVMPredictProbability` to predict probabilities, like we did with the
    linear multi-class model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As we did previously, we compute the AUC and ROC curves, which demonstrate
    that this model performs much better across the board, including the difficult
    classes, like shirt and pullover:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91329032-d01f-413b-8771-7773e3317475.png)'
  prefs: []
  type: TYPE_IMG
- en: Overfitting and underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SVM model is performing much better on our validation dataset than the
    linear model, but, in order to understand what to do next, we need to introduce
    two important concepts in machine learning: **overfitting** and **underfitting**.
    These both refer to problems that can occur when training a model.'
  prefs: []
  type: TYPE_NORMAL
- en: If a model **underfits** the data, it is *too simple* to explain the patterns
    in the input data, and therefore performs poorly when evaluated against the training
    dataset and the validation dataset. Another term for this problem is that the
    model has **high bias**.If a model **overfits** the data, it is *too complex*,
    and will not generalize well to new data points that were not included as part
    of training. This means that the model will perform well when evaluated against
    the training data, but poorly when evaluated against the validation dataset. Another
    term for this problem is that the model has **high variance**.
  prefs: []
  type: TYPE_NORMAL
- en: 'An easy way to understand the difference between overfitting and underfitting
    is to look at the following simple example: when building a model, our aim is
    to build something that is just right for the dataset. The example on the left
    underfits because a straight line model can not accurately divide the circles
    and squares. The model on the right is too complex: it separates all the circles
    and squares correctly, but is unlikely to work well on new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6397e42b-9573-4cf2-b611-c61ad5259c07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our linear model suffered from underfitting: it was too simplistic to model
    the difference between all the classes. Looking at the accuracy of the SVM, we
    can see that it scores 100% on the training data, but only 82% on validation.
    This is a clear sign that it is overfitting: it is much worse at classifying new
    images compared with those on which it was trained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of dealing with overfitting is to use more training data: even a complex
    model will not be able to overfit if the training dataset is large enough. Another
    way to do this is to introduce regularization: many machine learning models have
    a parameter that you can adjust to reduce overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have improved our model''s performance using an SVM, but still face
    two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Our SVM is overfitting the training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is also difficult to scale to the full dataset of 60,000 images: try training
    the last example with more images and you will find that it gets *much slower*.
    If we double the number of datapoints, the SVM algorithm takes *more than double*
    the amount of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we are going to tackle this problem using a **deep neural network**.
    These types of model have been able to achieve state-of-the-art performance on
    image classification tasks, as well many other machine learning problems. They
    are able to model complex non-linear patterns, and also scale well to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists will often use Python to develop and train neural networks because
    it has access to extremely well-supported deep learning frameworks such as **TensorFlow**
    and **Keras**. These frameworks make it easier than ever to build complex neural
    networks and train them on large datasets. They are usually the best choice for
    building sophisticated deep learning models. In [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using
    Pre-Trained Models*, we will look at how to export a trained model from Python
    and then call it from Go for inference. In this section, we will build a much
    simpler neural network from scratch using the `go-deep` library to demonstrate
    the key concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basic building block of a neural network is a **neuron** (also known as
    a **perceptron**). This is actually just the same as our simple linear model:
    it combines all of its inputs, that is, *x[1],x[2],x[3]... *and so on into a single
    output, *y*, according to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02e2f192-2e94-4abe-8e73-bdabca624535.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The magic of neural networks comes from what happens when we combine these
    simple neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a **layer** of many neurons into which we feed the input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the output of each neuron, we introduce an **activation function**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of this **input layer** is then fed to another layer of neurons and
    activations, known as a **hidden layer**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This gets repeated for multiple hidden layers—the more layers there are, the
    **deeper** the network is said to be.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A final **output** layer of neurons combines the result of the network into
    the final output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a technique known as **backpropagation**, we can train the network by
    finding the weights, *w[0],w[1],w[2]...*, for each neural network that allows
    the whole network to fit the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following diagram shows this layout: the arrows represent the output of
    each neuron, which are feeding into the input of the neurons in the next layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d68fe958-c58f-4f05-b0d9-e351c81c13da.png)'
  prefs: []
  type: TYPE_IMG
- en: The neurons in this network are said to be arranged **fully-connected** or **dense**
    layers. Recent advances in both computing power and software have allowed researchers
    to build and train more complex neural network architectures than ever before.
    For instance, a state-of-the-art image recognition system might contain millions
    of individual weights, and require many days of computing time to train all of
    these parameters to fit a large dataset. They often contain different arrangements
    of neurons, for instance, in **convolutional layers**, which perform more specialized
    learning in these types of systems.
  prefs: []
  type: TYPE_NORMAL
- en: Much of the skill that is required to use deep learning successfully in practice
    involves a broad understanding of how to select and tune a network to get good
    performance. There are many blogs and online resources that provide more detail
    on how these networks work and the types of problems that they have been applied
    to.
  prefs: []
  type: TYPE_NORMAL
- en: A **fully-connected** layer in a neural network is one where the inputs of each
    neuron are connected to the outputs of all the neurons in the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: A simple deep learning model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Much of the skill in building a successful deep learning model involves choosing
    the correct model architecture: the number/size/type of layers, and the activation
    functions for each neuron. Before starting, it is worth researching to see if
    someone else has already tackled a similar problem to yours using deep learning
    and published an architecture that works well. As always, it is best to start
    with something simple and then modify the network iteratively to improve its performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will start with the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: An input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two hidden layers containing 128 neurons each
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output layer of 10 neurons (one for each output class in the dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each neuron in the hidden layer will use a **rectified linear unit** (**ReLU**)
    as its output function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLUs are a common choice of activation function in neural networks. They are
    a very simple way to introduce non-linearity into a model. Other common activation
    functions include the **logistic** function and the **tanh** function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `go-deep` library lets us build this architecture very quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Neural network training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a neural network is another area in which you need to make skillful
    adjustments in order to get good results. The training algorithm works by calculating
    how well the model fits a small **batch** of training data (known as the **loss**),
    and then making small adjustments to the weights to improve the fit. This process
    then gets repeated over and over again on different batches of training data.
    The **learning rate** is an important parameter that controls how quickly the
    algorithm will adjust the neuron weights.
  prefs: []
  type: TYPE_NORMAL
- en: When training a neural network, the algorithm will feed all of the input data
    into the network repeatedly, and adjust the network weights as it goes. Each full
    pass through the data is known as an **epoch**.
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a neural network, monitor the **accuracy** and **loss** of the
    network after each epoch (accuracy should increase, while loss should decrease).
    If the accuracy is not improving, try lowering the learning rate. Keep training
    the network until accuracy stops improving: at this point, the network is said
    to have **converged**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code trains our model using a learning rate of `0.006` for `500`
    iterations and prints out the accuracy after each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This neural network provides an accuracy of 80% on both the training and validation
    datasets, a good sign that the model is not overfitting. See if you can improve
    its performance by adjusting the network architecture and retraining. In [Chapter
    5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using Pre-Trained Models*, we
    will revisit this example by building a more sophisticated neural network in Python
    and then exporting it to Go.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having mastered many of the key machine learning concepts in the *Classification*
    section, in this section, we will apply what we have learned to a regression problem.
    We will be using a dataset containing information about groups of houses in different
    locations in California^([4]). Our goal will be to predict the median house price
    in each group using input data such as the latitude/longitude location, median
    house size, age, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `download-housing.sh` script to download the dataset and then load
    it into Go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to carry out some data preparation to create columns in the dataframe
    that represent the average number of rooms and bedrooms for houses in each area,
    along with the average occupancy. We will also rescale the median house value
    into units of $100,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Like we did previously, we need to split this data into training and validation
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like the classification example, we are going to start by using a linear model
    as a baseline. This time, though, we are predicting a **continuous output variable**,
    so we need a different performance metric. A common metric to use for regression
    is the **mean squared error** (**MSE**), that is, the sum of the squared differences
    between the model predictions and the true values. By using a *squared* error,
    we are making sure that the value increases for underestimates and overestimates
    are of the true value.
  prefs: []
  type: TYPE_NORMAL
- en: A common alternative to MSE for regression problems is the **mean absolute error**
    (**MAE**). This can be useful when your input data contains outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a Golang regression library, we can train the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can calculate the mean squared error from the validation set as
    `0.51`. This provides a benchmark level of performance that we can refer to when
    comparing other models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Random forest regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that house prices vary according to location, often in complicated ways
    that our linear model is unlikely to be able to capture. Therefore, we are going
    to introduce **random forest regression** as an alternative model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random forest regression** is an example of an **ensemble model**: it works
    by training a large number of simple **base models** and then uses statistical
    averaging to output a final prediction. With random forests, the base models are
    decision trees, and, by adjusting the parameters of these trees and the number
    of models in the ensemble, you can control overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `RF.go` library, we can train a random forest on the house price
    data. First, let''s do some data preparation on the training and validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can fit a random forest containing 25 underlying decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This gives a much improved MSE of `0.29` on the validation set, but shows signs
    of overfitting with an error of only `0.05` on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Other regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many other regression models you can try out on this dataset. In fact,
    the SVM and deep learning models that we used in the previous example can also
    be adapted for use on regression problems. See if you can improve on the performance
    of the random forest by using a different model. Remember that some of these models
    will require the data to be normalized so that they can be trained properly.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered a lot of ground in this chapter, and introduced many important
    machine learning concepts. The first step in tackling a supervised learning problem
    is to collect and preprocess the data, making sure that it is normalized, and
    split into training and validation sets. We covered a range of different algorithms
    for both classification and regression. In each example, there were two phases:
    training the algorithm, followed by inference; that is, using the trained model
    to make predictions from new input data. Whenever you try a new machine learning
    technique on your data, it is important to keep track of its performance against
    the training and validation datasets. This serves two main purposes: it helps
    you diagnose underfitting/overfitting and also provides an indication of how well
    your model is working.'
  prefs: []
  type: TYPE_NORMAL
- en: It is usually best to choose the simplest model that provides good enough performance
    for the task that you are working on. Simple models are usually faster and easier
    to implement and use. In each example, we started with a simple linear model,
    and then evaluated more sophisticated techniques against this baseline.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different implementations of machine learning models for Go that
    are available online. As we have done in this chapter, it is usually quicker to
    find and use an existing library rather than implementing an algorithm completely
    from scratch. Often, these libraries have slightly different requirements in terms
    of data preparation and tuning parameters, so be sure to read the documentation
    carefully in each case.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will reuse many of the techniques for data loading and preparation
    that we have implemented here, but, instead, will focus on unsupervised machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Further readings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/). Retrieved
    March 24, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://blogs.nvidia.com/blog/2016/05/06/self-driving-cars-3/](https://blogs.nvidia.com/blog/2016/05/06/self-driving-cars-3/).
    Retrieved March 24, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist). Retrieved
    March 24, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://colah.github.io/](http://colah.github.io/). Retrieved May 15, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://karpathy.github.io/](https://karpathy.github.io/). Retrieved May 15,
    2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). Retrieved
    March 24,  2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
