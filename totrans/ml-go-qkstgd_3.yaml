- en: Supervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'As we learned in the first chapter, supervised learning is one of two major
    branches of machine learning. In a way, it is similar to how humans learn a new
    skill: someone else shows us what to do, and we are then able to learn by following
    their example. In the case of supervised learning algorithms, we usually need
    lots of examples, that is, lots of data providing the **input** to our algorithm
    and what the **expected output** should be. The algorithm will learn from this
    data, and then be able to **predict** the output based on new inputs that it has
    not seen before.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第一章中学到的，监督学习是机器学习的两个主要分支之一。从某种意义上说，它与人类学习新技能的方式相似：有人向我们展示该怎么做，然后我们通过模仿他们的例子来学习。在监督学习算法的情况下，我们通常需要大量的例子，即大量的数据提供算法的**输入**以及**预期输出**应该是什么。算法将从这些数据中学习，然后能够根据它之前未见过的新输入**预测**输出。
- en: 'A surprising number of problems can be addressed using supervised learning.
    Many email systems use it to classify emails as either important or unimportant
    automatically whenever a new message arrives in the inbox. More complex examples
    include image recognition systems, which can identify what an image contains purely
    from the input pixel values^([1]). These systems start by learning from huge datasets
    of images that have been labelled manually by humans, but are then able to categorize
    completely new images automatically. It is even possible to use supervised learning
    to steer a car automatically around a racing track: the algorithm starts by learning
    how a human driver controls the vehicle, and is eventually able to replicate this
    behavior^([2]).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使用监督学习可以解决大量问题。许多电子邮件系统会自动将新消息分类为重要或不重要，每当新消息到达收件箱时就会使用它。更复杂的例子包括图像识别系统，这些系统可以仅从输入像素值中识别图像内容^([1])。这些系统最初是通过学习大量由人类手动标记的图像数据集来学习的，但随后能够自动对全新的图像进行分类。甚至可以使用监督学习来自动驾驶赛车：算法首先学习人类驾驶员如何控制车辆，最终能够复制这种行为^([2])。
- en: 'By the end of this chapter, you will be able to use Go to implement two types
    of supervised learning:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够使用Go实现两种类型的监督学习：
- en: '**Classification**, where an algorithm must learn to classify the input into
    two or more discrete categories. We will build a simple image recognition system
    to demonstrate how this works.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**，其中算法必须学习将输入分类到两个或多个离散类别中。我们将构建一个简单的图像识别系统来展示这是如何工作的。'
- en: '**Regression**, in which the algorithm must learn to predict a continuous variable,
    for example, the price of an item for sale on a website. For our example, we will
    predict house prices based on inputs, such as the location, size, and age of the
    house.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**，其中算法必须学习预测一个连续变量，例如，在网站上出售的商品的价格。在我们的例子中，我们将根据输入预测房价，例如房屋的位置、大小和年龄。'
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: When to use regression and classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时使用回归和分类
- en: How to implement regression and classification using Go machine learning libraries
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Go机器学习库实现回归和分类
- en: How to measure the performance of an algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何衡量算法的性能
- en: 'We will cover the two stages involved in building a supervised learning system:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖构建监督学习系统涉及的两个阶段：
- en: '**Training**, which is the learning phase where we use labelled data to calibrate
    an algorithm'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**，这是使用标记数据校准算法的学习阶段'
- en: '**Inference** or **prediction**, where we use the trained algorithm for its
    intended purpose: to make predictions from input data'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理**或**预测**，即我们使用训练好的算法来实现其预期目的：从输入数据中进行预测'
- en: Classification
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: When starting any supervised learning problem, the first step is to load and
    prepare the data. We are going to start by loading the **MNIST Fashion** **dataset**^([3]),
    a collection of small, grayscale images showing different items of clothing. Our
    job is to build a system that can recognize what is in each image; that is, does
    it contain a dress, a shoe, a coat, and so on?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始任何监督学习问题之前，第一步是加载数据并准备数据。我们将从加载**MNIST Fashion** **数据集**^([3])开始，这是一个包含不同服装的小型、灰度图像集合。我们的任务是构建一个能够识别每张图像内容的系统；也就是说，它是否包含连衣裙、鞋子、外套等？
- en: 'First, we need to download the dataset by running the `download-fashion-mnist.sh`
    script in the code repository. Then, we will load it into Go:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要通过在代码仓库中运行`download-fashion-mnist.sh`脚本来下载数据集。然后，我们将将其加载到Go中：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s start by taking a look at a sample of the images. Each one is 28 x 28
    pixels, and each pixel has a value between 0 and 255\. We are going to use these
    pixel values as the inputs to our algorithm: our system will accept 784 inputs
    from an image and use them to classify the image according to which item of clothing
    it contains. In Jupyter, you can view an image as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看图像的样本。每个图像都是28 x 28像素，每个像素的值在0到255之间。我们将使用这些像素值作为算法的输入：我们的系统将从图像中接受784个输入，并使用它们来根据包含的衣物项目对图像进行分类。在Jupyter中，您可以按以下方式查看图像：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will display one of the 28 x 28 images from the dataset, as shown in the
    following image:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示数据集中28 x 28像素的图像之一，如下面的图像所示：
- en: '![](img/a4ddbc0b-b7bd-4a7e-b2e0-4e8730942184.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4ddbc0b-b7bd-4a7e-b2e0-4e8730942184.png)'
- en: 'To make this data suitable for a machine learning algorithm, we need to convert
    it into a dataframe format, as we learned in [Chapter 2](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml),
    *Setting Up the Development Environment*. To start, we will load the first 1,000
    images from the dataset:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些数据适合机器学习算法，我们需要将其转换为我们在[第2章](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml)，“设置开发环境”中学到的dataframe格式。首先，我们将从数据集中加载前1000张图像：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also need a string array that contains the possible labels for each image:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个包含每个图像可能标签的字符串数组：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It is very important to start by reserving a small proportion of your data in
    order to test the finished algorithm. This allows us to measure how well the algorithm
    works on new data that was not used during training. If you do not do this, you
    will most likely build a system that works really well during training but performs
    badly when faced with new data. To start with, we are going to use 75% of the
    images to train our model and 25% of the images to test it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先保留一小部分数据以测试最终算法非常重要。这使我们能够衡量算法在训练过程中未使用的新数据上的表现。如果您不这样做，您很可能会构建一个在训练期间表现良好但在面对新数据时表现不佳的系统。首先，我们将使用75%的图像来训练我们的模型，25%的图像来测试它。
- en: Splitting your data into a **training set** and a **test set** is crucial step
    when using supervised learning. It is normal to reserve 20-30% of the data for
    testing, but if your dataset is very large, you may be able to use less than this.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用监督学习时，将数据分成**训练集**和**测试集**是一个关键步骤。通常，我们会保留20-30%的数据用于测试，但如果您的数据集非常大，您可能可以使用更少的比例。
- en: 'Use the `Split(df dataframe.DataFrame, valFraction float64)` function from
    the last chapter to prepare these two datasets:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上一章中的`Split(df dataframe.DataFrame, valFraction float64)`函数来准备这两个数据集：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A simple model – the logistic classifier
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的模型——逻辑分类器
- en: 'One of the simplest algorithms that solves our problem is a logistic classifier.
    This is what mathematicians call a **linear model**, which we can understand by
    thinking about a simple example where we are trying to classify the points on
    the following two charts as either circles or squares. A linear model will try
    to do this by drawing a straight line to separate the two types of point. This
    works very well on the left-hand chart, where the relationship between the inputs
    (on the chart axes) and the output (circle or square) is simple. However, it does
    not work on the right-hand chart, where it is not possible to split the points
    into two correct groups using a straight line:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 解决我们问题的最简单算法之一是逻辑分类器。这是数学家所说的**线性模型**，我们可以通过考虑一个简单的例子来理解它，在这个例子中，我们试图将以下两个图表上的点分类为圆圈或正方形。线性模型将尝试通过画一条直线来分隔这两种类型的点。这在左边的图表上效果很好，其中输入（图表轴上的）与输出（圆圈或正方形）之间的关系很简单。然而，它不适用于右边的图表，在右边的图表中，无法使用直线将点分成两个正确的组：
- en: '![](img/28aa78cc-b28a-4a64-bb0d-815d13704964.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28aa78cc-b28a-4a64-bb0d-815d13704964.png)'
- en: When faced with a new machine learning problem, it is advised that you start
    with a linear model as a **baseline**, and then compare other models to it. Although
    linear models ca not capture complex relationships in the input data, they are
    easy to understand and normally quick to implement and train. You might find that
    a linear model is good enough for the problem you are working on and save yourself
    time by not having to implement anything more complex. If not, you can try different
    algorithms and use the linear model to understand how much better they work.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 面对一个新的机器学习问题时，建议你从一个线性模型作为**基线**开始，然后将其与其他模型进行比较。尽管线性模型无法捕捉输入数据中的复杂关系，但它们易于理解，通常实现和训练速度也很快。你可能发现线性模型对于你正在解决的问题已经足够好，从而节省了时间，无需实现更复杂的模型。如果不是这样，你可以尝试不同的算法，并使用线性模型来了解它们的效果有多好。
- en: A **baseline** is a simple model that you can use as a point of reference when
    comparing different machine learning algorithms.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**基线**是一个简单的模型，你可以将其用作比较不同机器学习算法时的参考点。'
- en: 'Going back to our image dataset, we are going to use a logistic classifier
    to decide whether an image contains trousers or not. First, let''s do some final
    data preparation: simplify the labels to be either trousers (`true`) or not-trousers
    (`false`):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的图像数据集，我们将使用逻辑分类器来决定一张图片是否包含裤子。首先，让我们做一些最终的数据准备：将标签简化为“裤子”（`true`）或“非裤子”（`false`）：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We are also going to normalize the pixel data so that, instead of being stored
    as integers between 0 and 255, it will be represented by floats between 0 and
    1:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将对像素数据进行归一化，使其不再是存储在0到255之间的整数，而是表示为0到1之间的浮点数：
- en: Many supervised machine learning algorithms only work properly if the data is
    normalized, that is, rescaled so that it is between 0 and 1\. If you are having
    trouble getting an algorithm to train properly, make sure that you have normalized
    the data properly.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 许多监督式机器学习算法只有在数据归一化（即缩放，使其在0到1之间）的情况下才能正常工作。如果你在训练算法时遇到困难，请确保你已经正确归一化了数据。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After preparing the data properly, it is finally time to create a logistic
    classifier and train it:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在正确准备数据之后，现在终于到了创建逻辑分类器并对其进行训练的时候了：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Measuring performance
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量性能
- en: Now that we have our trained model, we need to measure how well it is performing
    by comparing the predictions it makes on each image with the ground truth (whether
    or not the image is a pair of trousers). A simple way to do this is to measure
    **accuracy**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了模型，我们需要通过将模型对每张图片的预测与真实情况（图片是否是一双裤子）进行比较来衡量其表现的好坏。一个简单的方法是测量**准确率**。
- en: '**Accuracy** measures what proportion of the input data can be classified correctly
    by the algorithm, for example, 90%, if 90 out of 100 predictions from the algorithm
    are correct.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**准确率**衡量算法能够正确分类输入数据的比例，例如，如果算法的100个预测中有90个是正确的，那么准确率为90%。'
- en: 'In our Go code example, we can test the model by looping over the validation
    dataset and counting how many images are classified correctly. This will output
    a model accuracy of 98.8%:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的Go代码示例中，我们可以通过遍历验证数据集并计算正确分类的图片数量来测试模型。这将输出模型准确率为98.8%：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Precision and recall
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精确率和召回率
- en: Measuring accuracy can be very misleading. Suppose you are building a system
    to classify whether medical patients will test positive for a rare disease, and
    in the dataset only 0.1% of examples are in fact positive. A really bad algorithm
    might predict that nobody will test positive, and yet it has an accuracy of 99.9%
    simply because the disease is rare.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 测量准确率可能会非常误导。假设你正在构建一个系统来分类医疗患者是否会测试出罕见疾病，而在数据集中只有0.1%的例子实际上是阳性的。一个非常差的算法可能会预测没有人会测试出阳性，然而它仍然有99.9%的准确率，仅仅因为这种疾病很罕见。
- en: A dataset that has many more examples of one classification versus another is
    known as **unbalanced**. Unbalanced datasets need to be treated carefully when
    measuring algorithm performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类比另一个分类有更多示例的数据集被称为**不平衡**。在衡量算法性能时，需要仔细处理不平衡数据集。
- en: 'A better way to measure performance starts by putting each prediction from
    the algorithm into one of the following four categories:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好的衡量性能的方法是从将算法的每个预测放入以下四个类别之一开始：
- en: '![](img/115c28b2-7254-4bea-b5c2-10cb3bdd4daa.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/115c28b2-7254-4bea-b5c2-10cb3bdd4daa.png)'
- en: 'We can now define some new performance metrics:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义一些新的性能指标：
- en: '**Precision** measures what fraction of the models true predictions are actually
    correct. In the following diagram, it is the true positives that are predicted
    from the model (the left-hand side of the circle) divided by all of the models
    positive predictions (everything in the circle).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall** measures how good the model is at identifying all the positive examples.
    In other words, the true positives (left-hand side of the circle) divided by all
    the datapoints that are actually positive (the entire left-hand side):'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5ff88538-985a-4a66-9574-ea896bd17ba2.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows datapoints that have been predicted as true by the
    model in the central circle. The points that are actually true are on the left
    half of the diagram.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision** and **recall** are more robust performance metrics when working
    with unbalanced datasets. Both range between 0 and 1, where 1 indicates perfect
    performance.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the code for the total count of true positives and false negatives:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can now calculate precision and recall with the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For our linear model, we get 100% precision, meaning that there are no false
    positives, and a recall of 90.3%.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: ROC curves
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to measure performance involves looking at how the classifier works
    in more detail. Inside our model, two things happen:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: First, the model calculates a value between 0 and 1, indicating how likely it
    is that a given image should be classified as a pair of trousers.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A threshold is set, so that only images scoring more than the threshold get
    classified as trousers. Setting different thresholds can improve precision at
    the expense of recall and vice versa.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we look at the model output *across all the different thresholds from 0
    to 1*, we can understand more about how useful it is. We do this using something
    called the **receiver operating characteristic** (**ROC**) curve, which is a plot
    of the true positive rate versus the false positive rate across the dataset for
    different threshold values. The following three examples show ROC curves for a
    bad, moderate, and very good classifier:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34189d2a-5b38-4024-8e0c-0eaa87b148d0.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: By measuring the shaded area under these ROC curves, we get a simple metric
    of how good the model is, which is known as **area under curve** (**AUC)**. For
    the bad model, this is close to **0.5**, but for the very good model, it is close
    to **1.0**, indicating that the model can achieve *both* a high true positive
    rate and a low false positive rate.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The `gonum`/`stat` package provides a useful function for computing ROC curves,
    which we will use once we have extended the model to work with each of the different
    items of clothing in the dataset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The **receiver operating characteristic**, or **ROC curve**, is a plot of true
    positive rate versus false positive rate for different threshold values. It allows
    us to visualize how good the model is at classification. The AUC gives a simple
    measure of how good the classifier is.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class models
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have been using **binary classification**; that is, it should
    output `true` if the image shows a pair of trousers, and `false` otherwise. For
    some problems, such as detecting whether an email is important or not, this is
    all we need. But in this example, what we really want is a model that can identify
    all the different types of clothing in our dataset, that is, shirt, boot, dress,
    and so on.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'With some algorithm implementations, you will need to start by applying one-hot
    encoding to the output, as demonstrated in [Chapter 2](532d8304-b31d-41ef-81c1-b13f4c692824.xhtml),
    *Setting Up the Development Environment*. However, for our example, we will use
    **softmax regression** in **goml/linear**, which does this step automatically.
    We can train the model by simply feeding it with the input (pixel values) and
    the integer output (0, 1, 2, ... representing t-shirt, trouser, pullover, and
    so on):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When using this model for inference, it will output a vector of probabilities
    for each class; that is, it tells us what the probability that an input image
    is a t-shirt, trouser, and so on. This is exactly what we need for the ROC analysis,
    but, if we want a single prediction for each image, we can use the following func
    to find the class that has the *highest* probability:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we can plot the ROC curve and the AUC for each individual class. The
    following code will loop over each example in the validation dataset and predict
    probabilities for each class using the new model:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can now compute AUC values for each class, which shows that our model performs
    better on some classes than others:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For trousers, the AUC value is `0.96`, showing that even a simple linear model
    works really well in this case. However, shirt and pullover both score close to
    `0.6`. This makes intuitive sense: shirts and pullovers look very similar, and
    are therefore much harder for the model to recognize correctly. We can see this
    more clearly by plotting the ROC curve for each class as separate lines: the model
    clearly performs the worst on shirts and pullovers, and the best on the clothes
    that have a very distinctive shape (boots, trousers, sandals, and so on).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code loads gonums plotting libraries, creates the ROC plot, and
    saves it as a JPEG image:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we view the plot in Jupyter, we can see that the the worst classes follow
    the lines close to the diagonal, again indicating an AUC close to `0.5`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c384cecc-fbc7-4f0a-9daf-7377d5e36731.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: A non-linear model – the support vector machine
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To move forward, we need to use a different machine learning algorithm: one
    that is able to model more complex, non-linear relationships between the pixel
    inputs and the output classes. While some of the mainstream Go machine learning
    libraries such as Golearn have support for basic algorithms like local least squares,
    there is not a single library that supports as broad a set of algorithms as Python''s
    scikit-learn or R''s standard library. For this reason, it is often necessary
    to search for alternative libraries that implement bindings to a widely used C
    library, or that contain a configurable implementation of an algorithm that is
    suited for a particular problem. For this example, we are going to use an algorithm
    called the **support vector machine** (**SVM**).SVMs can be more difficult to
    use than linear models—they have more parameters to tune—but have the advantage
    of being able to model much more complex patterns in the data.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续前进，我们需要使用不同的机器学习算法：一种能够对像素输入和输出类别之间的更复杂、非线性关系进行建模的算法。虽然一些主流的围棋机器学习库，如 Golearn，支持基本算法，如局部最小二乘法，但没有一个库支持像
    Python 的 scikit-learn 或 R 的标准库那样广泛的算法集。因此，通常需要寻找实现绑定到广泛使用的 C 库的替代库，或者包含适用于特定问题的算法的可配置实现。对于这个例子，我们将使用一个称为**支持向量机**（**SVM**）的算法。与线性模型相比，SVM
    可能更难使用——它们有更多的参数需要调整——但它们的优势在于能够对数据中的更复杂模式进行建模。
- en: An SVMis a more advanced machine learning method that can be used both for classification
    and regression. They allow us to apply **kernels** to the input data, which means
    that they can model non-linear relationships between the inputs/outputs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 是一种更高级的机器学习方法，可用于分类和回归。它们允许我们对输入数据应用**核**，这意味着它们可以建模输入/输出之间的非线性关系。
- en: 'An important feature of SVM models is their ability to use a **kernel function**.
    Put simply, this means that the algorithm can apply a transformation to the input
    data so that non-linear patterns can be found. For our example, we will use the **LIBSVM**
    library to train an SVM on the image data. LIBSVM is an open source library with
    bindings for many different languages, meaning that it is also useful if you want
    to port a model that has been built in Python''s popular scikit-learn library.
    First, we need to do some data preparation to make our input/output data suitable
    for feeding into the Go library:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 模型的一个重要特性是它们能够使用**核函数**。简单来说，这意味着算法可以对输入数据进行变换，以便找到非线性模式。在我们的例子中，我们将使用**LIBSVM**库在图像数据上训练
    SVM。LIBSVM 是一个开源库，具有多种语言的绑定，这意味着如果你想在 Python 的流行 scikit-learn 库中移植模型，它也非常有用。首先，我们需要做一些数据准备，使我们的输入/输出数据适合输入到
    Go 库中：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we can set up the SVM model and configure it with a **radial basis function**
    (**RBF**) **kernel**. RBF kernels are a common choice when using SVMs, but do
    take longer to train than linear models:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以设置 SVM 模型，并使用**径向基函数**（**RBF**）**核**对其进行配置。RBF 核在 SVM 中是一个常见的选择，但训练时间比线性模型要长：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we can fit our model to the training data of 750 images, and then
    use `svm.SVMPredictProbability` to predict probabilities, like we did with the
    linear multi-class model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们的模型拟合到 750 张图像的训练数据上，然后使用 `svm.SVMPredictProbability` 来预测概率，就像我们之前对线性多类模型所做的那样：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we did previously, we compute the AUC and ROC curves, which demonstrate
    that this model performs much better across the board, including the difficult
    classes, like shirt and pullover:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所做的那样，我们计算了 AUC 和 ROC 曲线，这表明该模型在各个方面的表现都更好，包括像衬衫和套头衫这样的困难类别：
- en: '![](img/91329032-d01f-413b-8771-7773e3317475.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/91329032-d01f-413b-8771-7773e3317475.png)'
- en: Overfitting and underfitting
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过度拟合和欠拟合
- en: 'The SVM model is performing much better on our validation dataset than the
    linear model, but, in order to understand what to do next, we need to introduce
    two important concepts in machine learning: **overfitting** and **underfitting**.
    These both refer to problems that can occur when training a model.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 模型在我们的验证数据集上的表现比线性模型要好得多，但为了了解下一步该做什么，我们需要介绍机器学习中的两个重要概念：**过度拟合**和**欠拟合**。这两个概念都指的是在训练模型时可能发生的问题。
- en: If a model **underfits** the data, it is *too simple* to explain the patterns
    in the input data, and therefore performs poorly when evaluated against the training
    dataset and the validation dataset. Another term for this problem is that the
    model has **high bias**.If a model **overfits** the data, it is *too complex*,
    and will not generalize well to new data points that were not included as part
    of training. This means that the model will perform well when evaluated against
    the training data, but poorly when evaluated against the validation dataset. Another
    term for this problem is that the model has **high variance**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型**欠拟合**数据，它对输入数据中的模式解释得太简单，因此在评估训练数据集和验证数据集时表现不佳。这个问题还有另一个术语，即模型有**高偏差**。如果一个模型**过拟合**数据，它太复杂了，不能很好地推广到训练中没有包含的新数据点。这意味着当评估训练数据时，模型表现良好，但当评估验证数据集时表现不佳。这个问题还有另一个术语，即模型有**高方差**。
- en: 'An easy way to understand the difference between overfitting and underfitting
    is to look at the following simple example: when building a model, our aim is
    to build something that is just right for the dataset. The example on the left
    underfits because a straight line model can not accurately divide the circles
    and squares. The model on the right is too complex: it separates all the circles
    and squares correctly, but is unlikely to work well on new data:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 理解过拟合和欠拟合之间的区别的一个简单方法是看看以下简单的例子：在构建模型时，我们的目标是构建适合数据集的东西。左边的例子欠拟合，因为直线模型无法准确地将圆和正方形分开。右边的模型太复杂了：它正确地分离了所有的圆和正方形，但不太可能在新的数据上工作得很好：
- en: '![](img/6397e42b-9573-4cf2-b611-c61ad5259c07.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6397e42b-9573-4cf2-b611-c61ad5259c07.png)'
- en: 'Our linear model suffered from underfitting: it was too simplistic to model
    the difference between all the classes. Looking at the accuracy of the SVM, we
    can see that it scores 100% on the training data, but only 82% on validation.
    This is a clear sign that it is overfitting: it is much worse at classifying new
    images compared with those on which it was trained.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的线性模型受到了欠拟合的影响：它太简单，无法模拟所有类别的差异。查看SVM的准确率，我们可以看到它在训练数据上得分为100%，但在验证数据上只有82%。这是一个明显的迹象表明它过拟合了：与训练数据相比，它在分类新图像方面表现得更差。
- en: 'One way of dealing with overfitting is to use more training data: even a complex
    model will not be able to overfit if the training dataset is large enough. Another
    way to do this is to introduce regularization: many machine learning models have
    a parameter that you can adjust to reduce overfitting.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 处理过拟合的一种方法是用更多的训练数据：即使是一个复杂的模型，如果训练数据集足够大，也不会过拟合。另一种方法是引入正则化：许多机器学习模型都有一个可以调整的参数，以减少过拟合。
- en: Deep learning
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: 'So far, we have improved our model''s performance using an SVM, but still face
    two problems:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用支持向量机（SVM）提高了我们模型的性能，但仍然面临两个问题：
- en: Our SVM is overfitting the training data.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的SVM过度拟合了训练数据。
- en: 'It is also difficult to scale to the full dataset of 60,000 images: try training
    the last example with more images and you will find that it gets *much slower*.
    If we double the number of datapoints, the SVM algorithm takes *more than double*
    the amount of time.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也很难扩展到包含60,000张图像的全数据集：尝试用更多的图像训练最后一个示例，你会发现它变得**慢得多**。如果我们将数据点的数量加倍，SVM算法所需的时间将**超过加倍**。
- en: In this section, we are going to tackle this problem using a **deep neural network**.
    These types of model have been able to achieve state-of-the-art performance on
    image classification tasks, as well many other machine learning problems. They
    are able to model complex non-linear patterns, and also scale well to large datasets.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用**深度神经网络**来解决这个问题。这类模型已经在图像分类任务上实现了最先进的性能，以及许多其他机器学习问题。它们能够模拟复杂的非线性模式，并且在大数据集上扩展良好。
- en: Data scientists will often use Python to develop and train neural networks because
    it has access to extremely well-supported deep learning frameworks such as **TensorFlow**
    and **Keras**. These frameworks make it easier than ever to build complex neural
    networks and train them on large datasets. They are usually the best choice for
    building sophisticated deep learning models. In [Chapter 5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using
    Pre-Trained Models*, we will look at how to export a trained model from Python
    and then call it from Go for inference. In this section, we will build a much
    simpler neural network from scratch using the `go-deep` library to demonstrate
    the key concepts.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basic building block of a neural network is a **neuron** (also known as
    a **perceptron**). This is actually just the same as our simple linear model:
    it combines all of its inputs, that is, *x[1],x[2],x[3]... *and so on into a single
    output, *y*, according to the following formula:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02e2f192-2e94-4abe-8e73-bdabca624535.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'The magic of neural networks comes from what happens when we combine these
    simple neurons:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: First, we create a **layer** of many neurons into which we feed the input data.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the output of each neuron, we introduce an **activation function**.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of this **input layer** is then fed to another layer of neurons and
    activations, known as a **hidden layer**.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This gets repeated for multiple hidden layers—the more layers there are, the
    **deeper** the network is said to be.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A final **output** layer of neurons combines the result of the network into
    the final output.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a technique known as **backpropagation**, we can train the network by
    finding the weights, *w[0],w[1],w[2]...*, for each neural network that allows
    the whole network to fit the training data.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following diagram shows this layout: the arrows represent the output of
    each neuron, which are feeding into the input of the neurons in the next layer:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d68fe958-c58f-4f05-b0d9-e351c81c13da.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: The neurons in this network are said to be arranged **fully-connected** or **dense**
    layers. Recent advances in both computing power and software have allowed researchers
    to build and train more complex neural network architectures than ever before.
    For instance, a state-of-the-art image recognition system might contain millions
    of individual weights, and require many days of computing time to train all of
    these parameters to fit a large dataset. They often contain different arrangements
    of neurons, for instance, in **convolutional layers**, which perform more specialized
    learning in these types of systems.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Much of the skill that is required to use deep learning successfully in practice
    involves a broad understanding of how to select and tune a network to get good
    performance. There are many blogs and online resources that provide more detail
    on how these networks work and the types of problems that they have been applied
    to.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: A **fully-connected** layer in a neural network is one where the inputs of each
    neuron are connected to the outputs of all the neurons in the previous layer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: A simple deep learning model architecture
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Much of the skill in building a successful deep learning model involves choosing
    the correct model architecture: the number/size/type of layers, and the activation
    functions for each neuron. Before starting, it is worth researching to see if
    someone else has already tackled a similar problem to yours using deep learning
    and published an architecture that works well. As always, it is best to start
    with something simple and then modify the network iteratively to improve its performance.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we will start with the following architecture:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: An input layer
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two hidden layers containing 128 neurons each
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output layer of 10 neurons (one for each output class in the dataset)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each neuron in the hidden layer will use a **rectified linear unit** (**ReLU**)
    as its output function
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLUs are a common choice of activation function in neural networks. They are
    a very simple way to introduce non-linearity into a model. Other common activation
    functions include the **logistic** function and the **tanh** function.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'The `go-deep` library lets us build this architecture very quickly:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Neural network training
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a neural network is another area in which you need to make skillful
    adjustments in order to get good results. The training algorithm works by calculating
    how well the model fits a small **batch** of training data (known as the **loss**),
    and then making small adjustments to the weights to improve the fit. This process
    then gets repeated over and over again on different batches of training data.
    The **learning rate** is an important parameter that controls how quickly the
    algorithm will adjust the neuron weights.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: When training a neural network, the algorithm will feed all of the input data
    into the network repeatedly, and adjust the network weights as it goes. Each full
    pass through the data is known as an **epoch**.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'When training a neural network, monitor the **accuracy** and **loss** of the
    network after each epoch (accuracy should increase, while loss should decrease).
    If the accuracy is not improving, try lowering the learning rate. Keep training
    the network until accuracy stops improving: at this point, the network is said
    to have **converged**.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code trains our model using a learning rate of `0.006` for `500`
    iterations and prints out the accuracy after each epoch:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This neural network provides an accuracy of 80% on both the training and validation
    datasets, a good sign that the model is not overfitting. See if you can improve
    its performance by adjusting the network architecture and retraining. In [Chapter
    5](815e42bb-64e4-4f04-9dbd-c58af28f2580.xhtml), *Using Pre-Trained Models*, we
    will revisit this example by building a more sophisticated neural network in Python
    and then exporting it to Go.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having mastered many of the key machine learning concepts in the *Classification*
    section, in this section, we will apply what we have learned to a regression problem.
    We will be using a dataset containing information about groups of houses in different
    locations in California^([4]). Our goal will be to predict the median house price
    in each group using input data such as the latitude/longitude location, median
    house size, age, and so on.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `download-housing.sh` script to download the dataset and then load
    it into Go:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We need to carry out some data preparation to create columns in the dataframe
    that represent the average number of rooms and bedrooms for houses in each area,
    along with the average occupancy. We will also rescale the median house value
    into units of $100,000:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Like we did previously, we need to split this data into training and validation
    sets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Linear regression
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like the classification example, we are going to start by using a linear model
    as a baseline. This time, though, we are predicting a **continuous output variable**,
    so we need a different performance metric. A common metric to use for regression
    is the **mean squared error** (**MSE**), that is, the sum of the squared differences
    between the model predictions and the true values. By using a *squared* error,
    we are making sure that the value increases for underestimates and overestimates
    are of the true value.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: A common alternative to MSE for regression problems is the **mean absolute error**
    (**MAE**). This can be useful when your input data contains outliers.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a Golang regression library, we can train the model as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we can calculate the mean squared error from the validation set as
    `0.51`. This provides a benchmark level of performance that we can refer to when
    comparing other models:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Random forest regression
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that house prices vary according to location, often in complicated ways
    that our linear model is unlikely to be able to capture. Therefore, we are going
    to introduce **random forest regression** as an alternative model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '**Random forest regression** is an example of an **ensemble model**: it works
    by training a large number of simple **base models** and then uses statistical
    averaging to output a final prediction. With random forests, the base models are
    decision trees, and, by adjusting the parameters of these trees and the number
    of models in the ensemble, you can control overfitting.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `RF.go` library, we can train a random forest on the house price
    data. First, let''s do some data preparation on the training and validation sets:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we can fit a random forest containing 25 underlying decision trees:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This gives a much improved MSE of `0.29` on the validation set, but shows signs
    of overfitting with an error of only `0.05` on the training data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Other regression models
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many other regression models you can try out on this dataset. In fact,
    the SVM and deep learning models that we used in the previous example can also
    be adapted for use on regression problems. See if you can improve on the performance
    of the random forest by using a different model. Remember that some of these models
    will require the data to be normalized so that they can be trained properly.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered a lot of ground in this chapter, and introduced many important
    machine learning concepts. The first step in tackling a supervised learning problem
    is to collect and preprocess the data, making sure that it is normalized, and
    split into training and validation sets. We covered a range of different algorithms
    for both classification and regression. In each example, there were two phases:
    training the algorithm, followed by inference; that is, using the trained model
    to make predictions from new input data. Whenever you try a new machine learning
    technique on your data, it is important to keep track of its performance against
    the training and validation datasets. This serves two main purposes: it helps
    you diagnose underfitting/overfitting and also provides an indication of how well
    your model is working.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: It is usually best to choose the simplest model that provides good enough performance
    for the task that you are working on. Simple models are usually faster and easier
    to implement and use. In each example, we started with a simple linear model,
    and then evaluated more sophisticated techniques against this baseline.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: There are many different implementations of machine learning models for Go that
    are available online. As we have done in this chapter, it is usually quicker to
    find and use an existing library rather than implementing an algorithm completely
    from scratch. Often, these libraries have slightly different requirements in terms
    of data preparation and tuning parameters, so be sure to read the documentation
    carefully in each case.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will reuse many of the techniques for data loading and preparation
    that we have implemented here, but, instead, will focus on unsupervised machine
    learning.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Further readings
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/). Retrieved
    March 24, 2019.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://blogs.nvidia.com/blog/2016/05/06/self-driving-cars-3/](https://blogs.nvidia.com/blog/2016/05/06/self-driving-cars-3/).
    Retrieved March 24, 2019.'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist). Retrieved
    March 24, 2019.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://colah.github.io/](http://colah.github.io/). Retrieved May 15, 2019.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://karpathy.github.io/](https://karpathy.github.io/). Retrieved May 15,
    2019.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html](http://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). Retrieved
    March 24,  2019.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
