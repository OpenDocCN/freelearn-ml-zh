<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer093">
<h1 class="chapter-number" id="_idParaDest-175"><a id="_idTextAnchor177"/>11</h1>
<h1 id="_idParaDest-176"><a id="_idTextAnchor178"/>Distributed and GPU-Based Learning with LightGBM</h1>
<p>This chapter looks at training LightGBM models on distributed computing clusters and GPUs. Distributed computing can significantly speed up training workloads and enable the training of much larger datasets than the memory available on a single machine. We’ll look at leveraging Dask for distributed computing and LightGBM’s support for <span class="No-Break">GPU-based training.</span></p>
<p>The topics covered in the chapter are <span class="No-Break">as follows:</span></p>
<ul>
<li>Distributed learning with LightGBM <span class="No-Break">and Dask</span></li>
<li>GPU training <span class="No-Break">for LightGBM</span></li>
</ul>
<h1 id="_idParaDest-177"><a id="_idTextAnchor179"/>Technical requirements</h1>
<p>The chapter includes examples of training and running LightGBM models on distributed computing clusters and GPUs. A Dask environment and GPUs are required to run the examples. Complete code examples are available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-11"><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-11</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor180"/>Distributed learning with LightGBM and Dask</h1>
<p>Dask<a id="_idIndexMarker713"/> is an open-source Python library for distributed computing. It’s designed to integrate seamlessly with existing Python libraries and tools, including scikit-learn and LightGBM. This section<a id="_idIndexMarker714"/> looks at running distributed training workloads for LightGBM <span class="No-Break">using Dask.</span></p>
<p>Dask (<a href="https://www.dask.org/">https://www.dask.org/</a>) allows<a id="_idIndexMarker715"/> you to set up clusters on both a single machine and across many machines. Running Dask on a single machine is the default and requires no setup. However, workloads that run on a single-machine cluster (or scheduler) can readily be run with a <span class="No-Break">distributed scheduler.</span></p>
<p>Dask offers many ways to run a distributed cluster, including integrating Kubernetes, MPI, or automatic provisioning into a hyperscalar such as AWS or Google <span class="No-Break">Cloud Platform.</span></p>
<p>When running on a single machine, Dask still distributes the workload across multiple threads, which can significantly speed <span class="No-Break">up workloads.</span></p>
<p>Dask provides cluster<a id="_idIndexMarker716"/> management utility classes to set up a cluster easily. A local cluster can be run <span class="No-Break">as follows:</span></p>
<pre class="source-code">
cluster = LocalCluster(n_workers=4, threads_per_worker=2)
client = Client(cluster)</pre>
<p>The preceding code creates a local cluster with four workers, configuring each worker to run two threads. The cluster runs on localhost, with the scheduler running on port <strong class="source-inline">8786</strong> by default. The host IP and port can be configured using parameters. In addition to running the scheduler, Dask also starts a diagnostic dashboard that’s implemented using Bokeh (<a href="https://docs.bokeh.org/en/latest/">https://docs.bokeh.org/en/latest/</a>). By default, the dashboard runs on port <strong class="source-inline">8787</strong>. We can check the <strong class="bold">Workers</strong> page to see the status of our running cluster, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.1.</em></span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 11.1 – Dask diagnostics dashboard showing four running workers with some technical statistics for each" height="421" src="image/B16690_11_01.jpg" width="1020"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Dask diagnostics dashboard showing four running workers with some technical statistics for each</p>
<p>With a cluster up and running, we can prepare our data for use on the <span class="No-Break">distributed cluster.</span></p>
<p>Dask offers <a id="_idIndexMarker717"/>its own implementation of a data frame called the Dask DataFrame. A Dask DataFrame<a id="_idIndexMarker718"/> comprises many smaller pandas DataFrames, which are split based on the index. Each part can be stored on disk or distributed across a network, which allows working with much larger datasets than can fit into a single machine’s memory. Operations performed on the Dask DataFrame are automatically distributed to the <span class="No-Break">pandas DataFrames.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">When your dataset fits into RAM, using standard pandas DataFrames instead of a Dask DataFrame <span class="No-Break">is recommended.</span></p>
<p>We can create a Dask DataFrame by loading a CSV file. Note that the CSV file may be located on S3 or HDFS and can be very large. The following code creates a Dask DataFrame from a <span class="No-Break">CSV file:</span></p>
<pre class="source-code">
import dask.dataframe as dd
df = dd.read_csv("covtype/covtype.csv", blocksize="64MB")</pre>
<p>Here, we also specify the block size for loading the CSV file. The block size sets the chunks the dataset is divided into and gives us granular control over the memory of individual DataFrame parts. When calling <strong class="source-inline">df.shape</strong>, we get an <span class="No-Break">interesting result:</span></p>
<pre class="source-code">
df.shape
# (Delayed('int-a0031d1f-945d-42b4-af29-ea5e40148f3f'), 55)</pre>
<p>The number of columns is returned as a number. However, looking at the number of rows, we got a wrapper class called <strong class="source-inline">Delayed</strong>. This illustrates that even though we’ve created the DataFrame, the data is not loaded into memory. Instead, Dask loads the data as needed on the workers that use the data. We can force Dask to compute the row count <span class="No-Break">as follows:</span></p>
<pre class="source-code">
df.shape[0].compute()
# 581012</pre>
<p>With the data available in a DataFrame, we can prepare it for training. We split our data into a training and test set using the <strong class="source-inline">train_test_split</strong> function <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">dask_ml</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
X = df.iloc[:, :-1]
y = df.iloc[:, -1]
X_train, X_test, y_train, y_test = dask_ml.model_selection.train_test_split(X, y)</pre>
<p>Although the function from <strong class="source-inline">dask_ml</strong> mirrors the functionality of scikit-learn’s <strong class="source-inline">train_test_split</strong>, the Dask version maintains the distributed nature of the underlying <span class="No-Break">Dask DataFrame.</span></p>
<p>Our cluster is <a id="_idIndexMarker719"/>now set up, and the data is prepared for training. We can now look toward training our <span class="No-Break">LightGBM model.</span></p>
<p>The LightGBM library <a id="_idIndexMarker720"/>team offers and maintains Dask versions of each available learning algorithm: <strong class="source-inline">DaskLGBMRegressor</strong>, <strong class="source-inline">DaskLGBMClassifier</strong>, and <strong class="source-inline">DaskLGBMRanker</strong>. These are wrappers around the standard LightGBM scikit-learn interface with additional functionality to specify the Dask cluster client <span class="No-Break">to use.</span></p>
<p>When LightGBM runs on a Dask cluster, training occurs with one LightGBM worker per Dask worker. LightGBM concatenates all data partitions on a single worker into a single dataset, and each LightGBM worker uses the local <span class="No-Break">dataset independently.</span></p>
<p>Each LightGBM worker then works in concert to train a single LightGBM model, using the Dask cluster to communicate. When data parallel training is performed (as is the case with Dask), LightGBM uses <a id="_idIndexMarker721"/>a <span class="No-Break"><strong class="bold">Reduce-Scatter</strong></span><span class="No-Break"> strategy:</span></p>
<ol>
<li>During the histogram-building phase, each worker builds histograms for different non-overlapping features. Then, a <strong class="bold">Reduce-Scatter</strong> operation is performed: each worker shares a part of its histogram with each <span class="No-Break">other worker.</span></li>
<li>After the <strong class="bold">Reduce-Scatter</strong>, each worker has a complete histogram for a subset of features and then finds the best split for <span class="No-Break">these features.</span></li>
<li>Finally, a gathering operation is performed: each worker shares its best split with all other workers, so all workers have all the <span class="No-Break">best splits.</span></li>
</ol>
<p>The best feature split is chosen, and the data is <span class="No-Break">partitioned accordingly.</span></p>
<p>Fortunately, the complexity of the distributed algorithm is hidden from us, and the training code is identical to the scikit-learn training code we’re <span class="No-Break">used to:</span></p>
<pre class="source-code">
dask_model = lgb.DaskLGBMClassifier(n_estimators=200, client=client)
dask_model.fit(X_train, y_train)</pre>
<p>Running the <a id="_idIndexMarker722"/>preceding code<a id="_idIndexMarker723"/> trains the LightGBM model, and we can see the progress by checking the <strong class="bold">Status</strong> page of the Dask dashboard, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 11.2 – Status page of the Dask dashboard showing the task stream while a LightGBM model is training" height="946" src="image/B16690_11_02.jpg" width="1261"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Status page of the Dask dashboard showing the task stream while a LightGBM model is training</p>
<p>Dask LightGBM <a id="_idIndexMarker724"/>models can be fully serialized using <strong class="bold">Pickle</strong> or <strong class="bold">joblib</strong>, and<a id="_idIndexMarker725"/> we<a id="_idIndexMarker726"/> can save the model to disk <span class="No-Break">as follows:</span></p>
<pre class="source-code">
with open("dask-model.pkl", "wb") as f:
        pickle.dump(dask_model, f)</pre>
<p>Predictions can be made by calling the <strong class="source-inline">predict</strong> method of the model. Note that the Dask model expects a Dask DataFrame <span class="No-Break">or array:</span></p>
<pre class="source-code">
predictions = dask_model.predict(X_test)</pre>
<p>Similar to getting the shape of a Dask DataFrame, the prediction operation is also delayed and only calculated when needed. We can use <strong class="source-inline">compute</strong> to get the <span class="No-Break">prediction values:</span></p>
<pre class="console">
<strong class="source-inline">predictions.compute()</strong></pre>
<p>This concludes our look at leveraging <a id="_idIndexMarker727"/>Dask for distributed training with LightGBM. With Dask, LightGBM can train models on massive datasets well beyond the computing power of a single server. Dask scales alongside your needs, so you can start with <a id="_idIndexMarker728"/>your local laptop and move to a high-performance computing environment or cloud infrastructure as your data grows. Further, as shown previously, Dask is designed to work harmoniously with established Python libraries such as pandas, NumPy, and scikit-learn, providing a familiar environment for data scientists while extending the capabilities of <span class="No-Break">these tools.</span></p>
<p>Next, we’ll look at speeding up LightGBM training when large models need to be trained using <span class="No-Break">the GPU.</span></p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor181"/>GPU training for LightGBM</h1>
<p>The LightGBM library has <a id="_idIndexMarker729"/>native support for training the model on a GPU [<em class="italic">1</em>]. Two GPU platforms are supported: GPU via OpenCL and CUDA. Leveraging the GPU via OpenCL offers support for the broadest range of GPUs (including AMD GPUs) and is significantly faster than running the model on a CPU. However, the CUDA platform offers the fastest runtime if you have an NVIDIA <span class="No-Break">GPU available.</span></p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor182"/>Setting up LightGBM for the GPU</h2>
<p>Setting up <a id="_idIndexMarker730"/>your environment to use the GPU can be a bit tricky, but<a id="_idIndexMarker731"/> we'll review the core <span class="No-Break">steps here.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The GPU setup steps discussed here are offered as a guide and overview of the process of setting up your environment. The exact version number of libraries and drivers listed here may be outdated, and it’s recommended that you review the official documentation for up-to-date <span class="No-Break">versions: </span><a href="https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.xhtml"><span class="No-Break">https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.xhtml</span></a><span class="No-Break">.</span></p>
<p>In order to use the GPU, we have to <em class="italic">compile and build the LightGBM library from the source code</em>. The following instructions assume an Ubuntu Linux build environment; steps for other platforms are similar. Before we can build the library, we must install a <span class="No-Break">few dependencies.</span></p>
<p>Importantly, first, install <a id="_idIndexMarker732"/>the GPU drivers for your environment. If you<a id="_idIndexMarker733"/> have an NVIDIA GPU, also install CUDA. Instructions for doing so are available from the respective <span class="No-Break">vendor sites:</span></p>
<ul>
<li><a href="https://docs.nvidia.com/cuda/"><span class="No-Break">https://docs.nvidia.com/cuda/</span></a></li>
<li><span class="No-Break">https://www.amd.com/en/support</span></li>
</ul>
<p>Next, we need to install the <span class="No-Break">OpenCL headers:</span></p>
<pre class="source-code">
sudo apt install --no-install-recommends
sudo apt install --no-install-recommends nvidia-opencl-dev opencl-headers</pre>
<p>Finally, install the library <span class="No-Break">build dependencies:</span></p>
<pre class="source-code">
sudo apt install --no-install-recommends git cmake build-essential libboost-dev libboost-system-dev libboost-filesystem-dev</pre>
<p>We are now ready to compile the LightGBM library with GPU support. Clone the repository and build the library, setting the <span class="No-Break"><strong class="source-inline">USE_GPU</strong></span><span class="No-Break"> flag:</span></p>
<pre class="source-code">
git clone --recursive https://github.com/microsoft/LightGBM
cd LightGBM
mkdir build
cd build
cmake -DUSE_GPU=1 ..
make -j$(nproc)
cd ..</pre>
<p>As mentioned in <a href="B16690_02.xhtml#_idTextAnchor036"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Ensemble Learning – Bagging and Boosting,</em> LightGBM is a C++ library with a<a id="_idIndexMarker734"/> Python interface. With the preceding instructions, we<a id="_idIndexMarker735"/> have built the library with GPU support, but we must build and install the Python package to use the library from Python (including the <span class="No-Break">scikit-learn API):</span></p>
<pre class="source-code">
cd python-package/
python setup.py install --user --precompile</pre>
<h2 id="_idParaDest-181"><a id="_idTextAnchor183"/>Running LightGBM on the GPU</h2>
<p>Running training <a id="_idIndexMarker736"/>code on the GPU is straightforward. We set the device <a id="_idIndexMarker737"/>parameter to either <strong class="source-inline">gpu</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">cuda</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
model = lgb.LGBMClassifier(
        n_estimators=150,
        device="cuda",
        is_enable_sparse=False
)
model = model.fit(X_train, y_train)</pre>
<p>As shown in the preceding code, we turn off LightGBM’s sparse matrix optimization by setting <strong class="source-inline">is_enable_sparse</strong> to <strong class="source-inline">False</strong>. LightGBM’s sparse features are not supported on GPU devices. Further, depending on your dataset, you might get the following warning stating that <strong class="source-inline">multi_logloss</strong> is <span class="No-Break">not implemented:</span></p>
<pre class="source-code">
Metric multi_logloss is not implemented in cuda version. Fall back to evaluation on CPU.</pre>
<p>Notably, the fallback performed is only for evaluation and not training; training is still performed on the GPU. We can validate that the GPU is used by checking <strong class="source-inline">nvidia-smi</strong> (for <span class="No-Break">NVIDIA GPUs):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="Figure 11.3 – nvidia-smi output while LightGBM training is running (as we can see, the GPU utilization is at 40%)" height="223" src="image/B16690_11_03.jpg" width="652"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – nvidia-smi output while LightGBM training is running (as we can see, the GPU utilization is at 40%)</p>
<p>The speed-up<a id="_idIndexMarker738"/> achieved depends on your GPU. The training time was <a id="_idIndexMarker739"/>reduced from 171 s to 11 s (a 15-times speed-up) for 150 iterations on the Forest <span class="No-Break">Cover dataset.</span></p>
<p>The immense performance gain stemming from using a GPU is especially useful when performing parameter tuning. We can use GPU-based training with, for instance, Optuna to significantly accelerate the search for optimal parameters. All that’s needed is to move the model training in the <strong class="source-inline">objective</strong> function to the GPU device. When defining the objective function, we specify our Optuna parameter ranges as <span class="No-Break">per usual</span><span class="No-Break">:</span></p>
<pre class="source-code">
def objective(trial):
        lambda_l1 = trial.suggest_float(
                'lambda_l1', 1e-8, 10.0, log=True),
        lambda_l2 = trial.suggest_float(
                'lambda_l2', 1e-8, 10.0, log=True),
...</pre>
<p>We then create the model with the Optuna parameters and make sure to specify the device as <strong class="source-inline">cuda</strong> (<span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">gpu</strong></span><span class="No-Break">):</span></p>
<pre class="source-code">
model = lgb.LGBMClassifier(
    force_row_wise=True,
    boosting_type=boosting_type,
    n_estimators=n_estimators,
    lambda_l1=lambda_l1,
    lambda_l2=lambda_l2,
...
    learning_rate=learning_rate,
    max_bin=max_bin,
    device="cuda")</pre>
<p>The last part <a id="_idIndexMarker740"/>of the objective function is to return the <span class="No-Break">cross-validated </span><span class="No-Break"><a id="_idIndexMarker741"/></span><span class="No-Break">scores:</span></p>
<pre class="source-code">
scores = cross_val_score(model, X_train, y_train, scoring="f1_macro")
return scores.mean()</pre>
<p>We can then run a parameter study as we <span class="No-Break">would normally:</span></p>
<pre class="source-code">
sampler = optuna.samplers.TPESampler()
pruner = optuna.pruners.HyperbandPruner(
        min_resource=10, max_resource=400, reduction_factor=3)
study = optuna.create_study(
        direction='maximize', sampler=sampler,
        pruner=pruner
)
study.optimize(objective(), n_trials=10, gc_after_trial=True, n_jobs=1)</pre>
<p>Importantly, we also set the <strong class="source-inline">n_jobs</strong> parameter to <strong class="source-inline">1</strong> here, as running parallel jobs leveraging the GPU could cause unnecessary contention <span class="No-Break">and overhead.</span></p>
<p>There are a <a id="_idIndexMarker742"/>few noteworthy best practices for getting the best performance <a id="_idIndexMarker743"/>when training on <span class="No-Break">a GPU:</span></p>
<ul>
<li>Always verify that the GPU is being used. LightGBM returns to CPU training if the GPU is unavailable despite setting <strong class="source-inline">device=gpu</strong>. A good way of checking is with a tool such as <strong class="source-inline">nvidia-smi</strong>, as shown previously, or comparing training times to <span class="No-Break">reference benchmarks.</span></li>
<li>Use a much smaller <strong class="source-inline">max_bin</strong> size. Large datasets reduce the impact of a smaller <strong class="source-inline">max_bin</strong> size, and the smaller number of bins benefits training on the GPU. Similarly, use single-precision floats for increased performance if your GPU <span class="No-Break">supports it.</span></li>
<li>GPU training works best for large, dense datasets. Data needs to be moved to the GPU’s VRAM for training, and if the dataset is too small, the overhead involved with moving the data is <span class="No-Break">too significant.</span></li>
<li>Avoid one-hot encoding of feature columns, as this leads to sparse feature matrices, which do not work well on <span class="No-Break">the GPU.</span></li>
</ul>
<p>This concludes our section on how to use the GPU with LightGBM. Although the setup might be more complex, GPUs offer a significant boost in training speed due to their ability to handle thousands of threads simultaneously, allowing for efficient parallel processing, especially with large datasets. The massive parallelism of GPUs is particularly beneficial for histogram-based algorithms in LightGBM, making operations such as building histograms more efficient <span class="No-Break">and effective.</span></p>
<h1 id="_idParaDest-182"><a id="_idTextAnchor184"/>Summary</h1>
<p>In this chapter, we discussed two ways of accelerating computing with LightGBM. The first is large-scale distributed training across many machines using the Python library Dask. We showed how to set up a Dask cluster, how data can be distributed to the cluster using the Dask DataFrame, and how to run LightGBM on <span class="No-Break">the cluster.</span></p>
<p>Second, we also looked at how to leverage the GPU with LightGBM. Notably, the GPU setup is complex, but significant speed-up can be achieved when it’s available. We also discussed some best practices for training LightGBM models on <span class="No-Break">the GPU.</span></p>
<h1 id="_idParaDest-183"><a id="_idTextAnchor185"/>References</h1>
<table class="No-Table-Style" id="table001-10">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">1]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">H. Zhang, S. Si, and C.-J. Hsieh, GPU-acceleration for Large-scale Tree </em><span class="No-Break"><em class="italic">Boosting, 2017.</em></span></p>
</td>
</tr>
</tbody>
</table>
</div>
</div></body></html>