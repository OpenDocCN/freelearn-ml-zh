<html><head></head><body>
		<div id="_idContainer958">
			<h1 class="chapter-number" id="_idParaDest-275"><a id="_idTextAnchor702"/>14</h1>
			<h1 id="_idParaDest-276"><a id="_idTextAnchor703"/>Deploying Models on a Mobile Platform</h1>
			<p>In this chapter, we’ll discuss deploying <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models on mobile devices running on the Android operating system. ML can be used to improve the user experience on mobile devices, especially since we can create more autonomous features that allow our devices to learn and adapt to user behavior. For example, ML can be used for image recognition, allowing devices to identify objects in photos and videos. This feature can be useful for applications such as augmented reality or photo editing tools. Additionally, ML-powered speech recognition can enable voice assistants to better understand and respond to natural language commands. Another important benefit of the autonomous features development is that they can work without an internet connection. This is particularly useful in situations where connectivity is limited or unreliable, such as when traveling in remote areas or during <span class="No-Break">natural disasters.</span></p>
			<p>Using C++ on mobile devices allows us to make programs faster and more compact. We can utilize as many computational resources as possible because modern compilers can optimize the program concerning the target CPU architecture. C++ doesn’t use an additional garbage collector for memory management, which can have a significant impact on program performance. Program size can be reduced because C++ doesn’t use an additional <strong class="bold">Virtual Machine</strong> (<strong class="bold">VM</strong>) and is compiled directly into machine code. Also, the use of C++ can help optimize battery life by more precise resource usage and adjusting accordingly. These facts make C++ the right choice for mobile devices with a limited amount of resources and can be used to solve heavy <span class="No-Break">computational tasks.</span></p>
			<p>By the end of the chapter, you will learn how to implement real-time object detection using a camera on an Android mobile platform using PyTorch and YOLOv5. But this chapter is not a comprehensive introduction to Android development; rather, it can be used as a starting point for experiments with ML and computer vision on an Android platform. It provides a complete minimal example of the project that you will be able to extend for <span class="No-Break">your task.</span></p>
			<p>This chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Creating the minimal required project for Android <span class="No-Break">C++ development</span></li>
				<li>Implementing the minimal required Kotlin functionality for <span class="No-Break">object detection</span></li>
				<li>Initializing the image-capturing session in the C++ part of <span class="No-Break">the project</span></li>
				<li>Using OpenCV to process native camera images and <span class="No-Break">draw results</span></li>
				<li>Using PyTorch script to launch the YOLOv5 model on the <span class="No-Break">Android platform</span></li>
			</ul>
			<h1 id="_idParaDest-277"><a id="_idTextAnchor704"/>Technical requirements</h1>
			<p>The following are the technical requirements for <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Android Studio, <strong class="bold">Android Software Development Kit</strong> (<strong class="bold">SDK</strong>), and Android <strong class="bold">Native Development </strong><span class="No-Break"><strong class="bold">Kit</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NDK</strong></span><span class="No-Break">)</span></li>
				<li>The <span class="No-Break">PyTorch library</span></li>
				<li>A modern C++ compiler with <span class="No-Break">C++20 support</span></li>
				<li>CMake build system version &gt;= <span class="No-Break">3.22</span></li>
			</ul>
			<p>The code files for this chapter can be found at the following GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-edition/tree/main/Chapter14"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-Second-edition/tree/main/Chapter14</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-278"><a id="_idTextAnchor705"/>Developing object detection on Android</h1>
			<p>There are many approaches regarding how to deploy an ML model to a mobile device with Android. We can use PyTorch, ExecuTorch, TensorFlow Lite, NCNN, ONNX Runtime, or others. We’ll use the PyTorch<a id="_idIndexMarker1551"/> framework in this chapter since we have discussed it in the previous chapters, and because it allows us to use almost any <a id="_idIndexMarker1552"/>PyTorch model with minimal functional restrictions. Unfortunately, we will be able to use only the target device CPU for inference. Other<a id="_idIndexMarker1553"/> frameworks, such as ExecuTorch, TensorFlow Lite, NCNN, and ONNX Runtime, allow you to use other inference backends, such as onboard GPU or <strong class="bold">Neural Processing Unit</strong> (<strong class="bold">NPU</strong>). However, this<a id="_idIndexMarker1554"/> option also comes with a notable restriction, which is the lack of certain operators or functions, which can limit the types of models that can be deployed on mobile devices. Dynamic shape support is usually limited, making it difficult to handle data with <span class="No-Break">varying dimensions.</span></p>
			<p>Another challenge is restricted control flow, which limits the ability to use models with dynamic computational graphs and implement advanced algorithms. These restrictions can make it more challenging to deploy ML models on mobile platforms using the frameworks described earlier. So, there is a trade-off between the model’s functionality and the required performance <a id="_idIndexMarker1555"/>when you deploy ML models on mobile devices. To balance functionality and performance, developers must carefully evaluate their <a id="_idIndexMarker1556"/>requirements and choose a <a id="_idIndexMarker1557"/>framework that meets their <span class="No-Break">specific needs.</span></p>
			<h2 id="_idParaDest-279"><a id="_idTextAnchor706"/>The mobile version of the PyTorch framework</h2>
			<p>There is an available binary distribution of PyTorch for mobile devices available in the Maven repository named <strong class="source-inline">org.pytorch:pytorch_android_lite</strong>. However, this distribution is outdated. So, to use the<a id="_idIndexMarker1558"/> most recent version, we need to build it from source code. We can do this in the same way as we compile its regular version but with <a id="_idIndexMarker1559"/>additional CMake parameters to enable mobile mode. You also have to install the Android NDK, which includes an appropriate version of the C/C++ compiler and the Android native libraries that are required to build <span class="No-Break">the application.</span></p>
			<p>The simplest way to install Android development tools is to download the Android Studio IDE and use the SDK Manager tool from that. You can find the SDK Manager under the <strong class="bold">Tools</strong> | <strong class="bold">SDK Manager</strong> menu. You can use this manager to install appropriate Android SDK versions. You can install the corresponding NDKs by using the <strong class="bold">SDK Tools</strong> tab in the manager’s window. You can also use this tab to install the CMake utility. Another way to get NDK and build tools for Android is to use the <strong class="source-inline">cmdline-tools</strong> package. However, you need to have Java in your system; for Ubuntu, you can install Java <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
sudo apt install default-jre</pre>			<p>The following command line script shows you how to install all required packages for <span class="No-Break">CLI development:</span></p>
			<pre class="source-code">
# make the folder where to install components
mkdir android
cd android
# download command line tools
wget https://dl.google.com/android/repository/commandlinetools-linux-9477386_latest.zip
# unzip them and move to the correct folder
unzip commandlinetools-linux-9477386_latest.zip
mv cmdline-tools latest
mkdir cmdline-tools
mv latest cmdline-tools
# install SDK, NDK and build tools for Android using sdkmanager utility
yes | ./cmdline-tools/latest/bin/sdkmanager --licenses
yes | ./cmdline-tools/latest/bin/sdkmanager "platform-tools"
yes | ./cmdline-tools/latest/bin/sdkmanager "platforms;android-35"
yes | ./cmdline-tools/latest/bin/sdkmanager "build-tools;35.0.0"
yes | ./cmdline-tools/latest/bin/sdkmanager "system-images;android-35;google_apis;arm64-v8a"
yes | ./cmdline-tools/latest/bin/sdkmanager --install "ndk;26.1.10909125"</pre>			<p>Here, we used the <strong class="source-inline">sdkmanager</strong> manager utility <a id="_idIndexMarker1560"/>to install all required components <a id="_idIndexMarker1561"/>with appropriate versions. Using this script, the path to NDK will be <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
android/ndk/26.1.10909125/</pre>			<p>Having installed build tools and NDK, we can move on to the PyTorch mobile <span class="No-Break">version compilation.</span></p>
			<p>The following code snippet shows <a id="_idIndexMarker1562"/>you how to use the command line environment to check out PyTorch and <span class="No-Break">build it:</span></p>
			<pre class="console">
cd /home/[USER]
git clone https://github.com/pytorch/pytorch.git
cd pytorch/
git checkout v2.3.1
git submodule update --init --recursive
export ANDROID_NDK=[Path to the installed NDK]
export ANDROID_ABI='arm64-v8a'
export ANDROID_STL_SHARED=1
$START_DIR/android/pytorch/scripts/build_android.sh \
-DBUILD_CAFFE2_MOBILE=OFF \
-DBUILD_SHARED_LIBS=ON \
-DUSE_VULKAN=OFF \
-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())') \
-DPYTHON_EXECUTABLE=$(python -c 'import sys; print(sys.executable)') \</pre>			<p>Here, we assumed that <strong class="source-inline">/home/[USER]</strong> is the <a id="_idIndexMarker1563"/>user’s home directory. The main requirement when it comes to building the mobile version of PyTorch is to declare the <strong class="source-inline">ANDROID_NDK</strong> environmental variable, which should<a id="_idIndexMarker1564"/> point to the Android NDK installation directory. The <strong class="source-inline">ANDROID_ABI</strong> environment variable can be <a id="_idIndexMarker1565"/>used to specify the <strong class="bold">ARM</strong> (<strong class="bold">Advanced RISC Machines</strong>) CPU architecture’s compatibility for the compiler to generate architecture-specific code. In this example, we used the <span class="No-Break"><strong class="source-inline">arm64-v8a</strong></span><span class="No-Break"> architecture.</span></p>
			<p>We used the <strong class="source-inline">build_android.sh</strong> script from the PyTorch source code distribution to build mobile PyTorch binaries. This script uses the CMake command internally, which is why it takes CMake parameter definitions as arguments. Notice that we passed the <strong class="source-inline">BUILD_CAFFE2_MOBILE=OFF</strong> parameter to disable building the mobile version of <strong class="source-inline">Caffe2</strong>, which is hard to use in the current version because the library is deprecated. The second important parameter we used was <strong class="source-inline">BUILD_SHARED_LIBS=ON</strong>, which enabled us to build shared libraries. Also, we disabled the Vulkan API support by using <strong class="source-inline">DUSE_VULKAN=OFF</strong> because it’s still experimental and has some compilation problems. The other parameters that were configured were the Python installation paths for intermediate build <span class="No-Break">code generation.</span></p>
			<p>Now that we have the mobile PyTorch libraries, that is, <strong class="source-inline">libc10.so</strong> and <strong class="source-inline">libtorch.so</strong>, we can start developing the application. We are going to build an object detection application based on the YOLOv5 neural <span class="No-Break">network architecture.</span></p>
			<p>YOLOv5 is an object detection model <a id="_idIndexMarker1566"/>based on the <strong class="bold">You Only Look Once</strong> (<strong class="bold">YOLO</strong>) architecture. It’s a state-of-the-art deep learning model that can detect objects in images and videos with high accuracy <a id="_idIndexMarker1567"/>and speed. The model is relatively small and lightweight, making it easy to deploy on resource-constrained devices. Also, it’s fast enough, which is important for real-time applications that analyze a real-time video stream. It’s open source software, which means that developers can freely access the code and modify it to suit <span class="No-Break">their needs.</span></p>
			<h2 id="_idParaDest-280"><a id="_idTextAnchor707"/>Using TorchScript for a model snapshot</h2>
			<p>In this section, we will discuss how to<a id="_idIndexMarker1568"/> get the YOLOv5 model TorchScript file so that we can use it in our mobile application. In the previous chapters, we discussed how to save and load model parameters and how to use the ONNX format to share models<a id="_idIndexMarker1569"/> between frameworks. When we use the PyTorch framework, there is another method we can use to share models between the Python <a id="_idIndexMarker1570"/>API and C++ API <span class="No-Break">called </span><span class="No-Break"><strong class="bold">TorchScript</strong></span><span class="No-Break">.</span></p>
			<p>This method uses real-time model tracing to get a special type of model definition that can be executed by the PyTorch engine, regardless of API. In PyTorch, only the Python API can create such definitions, but we can use the C++ API to load the model and execute it. Also, the mobile version of the PyTorch framework doesn’t allow us to program neural networks with a full-featured C++ API. However, as was said earlier, TorchScript allows us to export and run models with complex control flow and dynamic shapes, which is not fully possible now for ONNX and other formats used in other <span class="No-Break">mobile frameworks.</span></p>
			<p>For now, the YOLOv5 PyTorch model can be directly exported only into TorchScript for inference on mobile CPUs. For example, there are YOLOv5 models adapted for TensorFlow Lite and NCNN frameworks, but we will not discuss these cases because we are using PyTorch mostly. I have to say that using NCNN will allow you to use a mobile GPU though the Vulkan API and <a id="_idIndexMarker1571"/>using TensorFlow Lite or ONNX Runtime for Android will allow you to use a mobile NPU for some devices. However, you will need to adapt the model into another format by reducing <a id="_idIndexMarker1572"/>some functionality or developing it <span class="No-Break">with TensorFlow.</span></p>
			<p>So, in this example, we are going to use the TorchScript model to perform object detection. To get the YOLOv5 model, we have to perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Clone the model repository from GitHub and install dependencies; run these commands in <span class="No-Break">the terminal:</span><pre class="source-code">
<strong class="bold">git clone https://github.com/ultralytics/yolov5</strong>
<strong class="bold">cd yolov5</strong>
<strong class="bold">pip install -r requirements.txt</strong></pre></li>				<li>Run the export script in the terminal to get the PyTroch <strong class="source-inline">jit</strong> script of the model optimized <span class="No-Break">for mobile:</span><pre class="source-code">
python export.py --weights yolov5s.torchscript --include torchscript --optimize</pre></li>			</ol>
			<p>The script from the second step automatically traces the model and saves the TorchScript file for us. After we made these steps, there will be the <strong class="source-inline">yolo5s.torchscript</strong> file, which we will be able to load and use <span class="No-Break">in C++.</span></p>
			<p>Now, we have all the prerequisites to move on and make an Android Studio project for <span class="No-Break">our application.</span></p>
			<h1 id="_idParaDest-281"><a id="_idTextAnchor708"/>The Android Studio project</h1>
			<p>In this section, we will use the Android <a id="_idIndexMarker1573"/>Studio IDE to create our mobile application. We can use a default <strong class="bold">native C++</strong> wizard in the Android Studio IDE to create an application stub. If we<a id="_idIndexMarker1574"/> name the project <strong class="source-inline">objectdetection</strong> and select <strong class="bold">Kotlin</strong> as the programming language, then Android Studio will create a particular project structure; the following sample shows the most valuable parts <span class="No-Break">of it:</span></p>
			<pre class="source-code">
app
|--src
|  `--main
|    |--cpp
|    |  |—CmakeLists.txt
|    |  `—native-lib.cpp
|    |--java
|    |  `--com
|    |    `--example
|    |       `--objectdetection
|    |         `--MainActivity.kt
|    |--res
|    |  `--layout
|    |    `--activity_main.xml
|    |--values
|       |--colors.xml
|       |--strings.xml
|       |--styles.xml
|          `—…
|--build.gradle
`--...</pre>			<p>The <strong class="source-inline">cpp</strong> folder contains the C++ part of the whole project. In this project, the Android Studio IDE created the C++ part as a native shared library project that had been configured with the CMake build generation system. The <strong class="source-inline">java</strong> folder contains the Kotlin part of the project. In our case, it is a <a id="_idIndexMarker1575"/>single file that defines the main activity—the object that’s used as a connection between the UI elements and event handlers. The <strong class="source-inline">res</strong> folder contains project resources, such as UI elements and <span class="No-Break">string definitions.</span></p>
			<p>We also need to create the <strong class="source-inline">jniLibs</strong> folder, under the <strong class="source-inline">main</strong> folder, with the <span class="No-Break">following structure:</span></p>
			<pre class="source-code">
app
|--src
|  |--main
|  |--…
|  |--JniLibs
|     `--arm64-v8a
|        |--libc10.so
|        |--libtorch_cpu.so
|        |--libtorch_global_deps.so
|        `—libtorch.so
`...</pre>			<p>Android Studio requires us to place additional native libraries in such folders to correctly package them into the final application. It also <a id="_idIndexMarker1576"/>allows the <strong class="bold">Java Native Interface</strong> (<strong class="bold">JNI</strong>) system to be able to find these libraries. Notice that we placed PyTorch libraries in the <strong class="source-inline">arm64-v8a</strong> folder because they have only been compiled for this CPU architecture. If you have libraries for other architectures, you have to create folders with <span class="No-Break">corresponding names.</span></p>
			<p>Also, in the previous subsection, we<a id="_idIndexMarker1577"/> learned how to get the YOLOv5 torch script model. The model file and corresponding file, along with the class IDs, should be placed in the <strong class="source-inline">assets</strong> folder. This folder should be created beside the <strong class="source-inline">JniLibs</strong> folder on the same folder level, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
app
|--src
|  `--main
|     |--...
|     |--cpp
|     |--JniLibs
|     |--assests
|     |  |--yolov5.torchscript
|     |  `--classes.txt
|     `—...
`...</pre>			<p>The file that maps string class names to numerical IDs, which the model returns, can be downloaded <span class="No-Break">from </span><a href="https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml"><span class="No-Break">https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml</span></a><span class="No-Break">.</span></p>
			<p>In our example, we simply convert the YAML file into the text one to make its <span class="No-Break">parsing simpler.</span></p>
			<p>The IDE uses the Gradle build system for project configuration, so there are two files named <strong class="source-inline">build.gradle.kts</strong>, one for the application module and another one for the project properties. Look at the <strong class="source-inline">build.gradle</strong> file for the application module in our example. There are two variables<a id="_idIndexMarker1578"/> that define paths to the PyTorch source code folder and to the OpenCV Android SDK folder. You need to update their values if you change these paths. The prebuilt OpenCV Android SDK can be downloaded from the official GitHub repository (<a href="https://github.com/opencv/opencv/releases">https://github.com/opencv/opencv/releases</a>) and <span class="No-Break">simply unpacked.</span></p>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor709"/>The Kotlin part of the project</h2>
			<p>In this project, we are going to <a id="_idIndexMarker1579"/>use the native C++ part to draw the captured picture with bounding boxes and class labels for detected objects. So, there will be no UI code and declarations in the Kotlin part. However, the Kotlin part will used to request and check required camera access permissions. Also, it will start a camera capture session if permissions are granted. All Kotlin code will be in the <span class="No-Break"><strong class="source-inline">MainActivity.kt</strong></span><span class="No-Break"> file.</span></p>
			<h3>Preserving camera orientation</h3>
			<p>In our project, we skip the implementation<a id="_idIndexMarker1580"/> of device rotation handling to make code simpler and show just the most interesting parts of working with the object detection model. So, to make our code stable, we have to disable the landscape mode, which can be done in the <strong class="source-inline">AndroidManifest.xml</strong> file, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
…
&lt;activity
    …
    android:screenOrientation="portrait"&gt;
…</pre>			<p>We added the screen orientation instruction to the activity entity. This is not a good solution because there are devices that work only in landscape mode and our application will not work with them. In <a id="_idIndexMarker1581"/>a real production-ready application, you should handle different orientation modes; for example, for most smartphones, this dirty solution <span class="No-Break">should work.</span></p>
			<h3>Handling camera permission requests</h3>
			<p>There are no C++ APIs in <a id="_idIndexMarker1582"/>Android NDK to request permissions. We can request the required permission only from the Java/Kotlin side or with JNI from C++. It’s simpler to write the Kotlin code to request the camera permission than to write <span class="No-Break">JNI calls.</span></p>
			<p>The first step is modifying the declaration of the <strong class="source-inline">MainActivity</strong> class to be able to process permission request results. It’s done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
class MainActivity
    : NativeActivity(),
      ActivityCompat.OnRequestPermissionsResultCallback {
  …
}</pre>			<p>Here, we inherited the <strong class="source-inline">MainActivity</strong> class from the <strong class="source-inline">OnRequestPermissionsResultCallback</strong> interface. It gives us the possibility to override the <strong class="source-inline">onRequestPermissionsResult</strong> method where we will be able to check a result. However, to get a result, we<a id="_idIndexMarker1583"/> have to make a request first, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
override fun onResume() {
  super.onResume() val cameraPermission =
      android.Manifest.permission
          .CAMERA if (checkSelfPermission(
                          cameraPermission) !=
                      PackageManager.PERMISSION_GRANTED) {
    requestPermissions(arrayOf(cameraPermission),
                       CAM_PERMISSION_CODE)
  }
  else {
    val camId =
        getCameraBackCameraId() if (camId.isEmpty()){
            Toast
                .makeText(
                    this,
                    "Camera probably won't work on this
                    device !",
                    Toast.LENGTH_LONG)
                .show() finish()} initObjectDetection(camId)
  }
}</pre>			<p>We overrode the <strong class="source-inline">onResume</strong> method of the <strong class="source-inline">Activity</strong> class. This method is called every time when our application starts to work or is resumed from the background. We initialized the <strong class="source-inline">cameraPermission</strong> variable with the required camera permission constant value. Then, we checked <a id="_idIndexMarker1584"/>whether we already granted this permission using the <strong class="source-inline">checkSelfPermission</strong> method. If we don’t have the camera permission, we ask for it with the <span class="No-Break"><strong class="source-inline">requestPermissions</strong></span><span class="No-Break"> method.</span></p>
			<p>Notice that we used the <strong class="source-inline">CAM_PERMISSION_CODE</strong> code to identify our request in the callback method. If we were granted access to a camera, we tried to get the back-facing camera ID and initialize the object detection pipeline for this camera. If we can’t get access to a camera, we finish the Android activity with the <strong class="source-inline">finish</strong> method and the corresponding message. In the <strong class="source-inline">onRequestPermissionsResult</strong> method, we check if the required permission <a id="_idIndexMarker1585"/>was granted, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
override fun onRequestPermissionsResult(requestCode
                                        : Int, permissions
                                        : Array&lt;out String&gt;,
                                          grantResults
                                        : IntArray) {
  super.onRequestPermissionsResult(requestCode,
                                   permissions,
                                   grantResults)
    if (requestCode == CAM_PERMISSION_CODE &amp;&amp;
      grantResults[0] != PackageManager.PERMISSION_GRANTED)
    {
       Toast.makeText(this,
                     "This app requires camera permission",
                     Toast.LENGTH_SHORT).show()
                     finish()
    }
}</pre>			<p>At first, we called the parent method to preserve the standard application behavior. Then, we checked the permission identification code, <strong class="source-inline">CAM_PERMISSION_CODE</strong>, and whether or not the permission<a id="_idIndexMarker1586"/> was granted. In the failure case, we just show the error message and finish the <span class="No-Break">Android activity.</span></p>
			<p>As we said before, in the success case, we looked for the back-facing camera ID, which is done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
private fun getCameraBackCameraId(): String {
  val camManager = getSystemService(
      Context.CAMERA_SERVICE)as CameraManager
  for (camId in camManager.cameraIdList) {
      val characteristics =
          camManager.getCameraCharacteristics(camId)
      val hwLevel = characteristics.get(
     CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL)
      val facing = characteristics.get(
          CameraCharacteristics.LENS_FACING)
      if (hwLevel != INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY &amp;&amp;
          facing == LENS_FACING_BACK) {
              return camId
      }
  }
  return ""
}</pre>			<p>We got the instance of the <strong class="source-inline">CameraManager</strong> object and used this object to iterate over every camera on a device. For <a id="_idIndexMarker1587"/>each camera object, we asked for its characteristics, supported hardware level, and where this camera faces. If a camera is a regular legacy device and faces back, we return its ID. If we didn’t find a suitable device, we returned an <span class="No-Break">empty string.</span></p>
			<p>Having granted the camera access permission and the camera ID, we called the <strong class="source-inline">initObjectDetection</strong> function to start image capturing and object detection. This and the <strong class="source-inline">stopObjectDetection</strong> function are functions provided through the JNI from the C++ part to the Kotlin part. The <strong class="source-inline">stopObjectDetection</strong> function is used to stop the camera capturing session, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
override fun onPause() {
  super.onPause()
  stopObjectDetection()
}</pre>			<p>In the overridden <strong class="source-inline">onPause</strong> activity<a id="_idIndexMarker1588"/> method, we just stopped the camera capturing session. This method is called every time the Android application is closed or goes into <span class="No-Break">the background.</span></p>
			<h3>Native library loading</h3>
			<p>There are two methods, <strong class="source-inline">initObjectDetection</strong> and <strong class="source-inline">stopObjectDetection</strong>, which are JNI calls to the native library functions that are implemented with C++. To connect the native library<a id="_idIndexMarker1589"/> with the Java or Kotlin code, we use JNI. This is a standard mechanism that’s used for calling C/C++ functions from Kotlin <span class="No-Break">or Java.</span></p>
			<p>First, we have to load the native library with the <strong class="source-inline">System.LoadLibrary</strong> call and place it in the companion object for our activity. Then, we have to define the methods that are implemented in the native library by declaring them as <strong class="source-inline">external</strong>. The following snippet shows how to define these methods <span class="No-Break">in Kotlin:</span></p>
			<pre class="source-code">
private external fun initObjectDetection(camId: String)
private external fun stopObjectDetection()
companion object {
  init {
      System.loadLibrary("object-detection")
  }
}</pre>			<p>Such declarations allow Kotlin to find the corresponding native library binary, load it, and access the functions. JNI works by providing a set of APIs that allow Java code to call into native code and vice versa. The JNI API consists of a number of functions that can be called from Java or native code. These functions allow you to perform tasks, such as creating and accessing Java objects from native code, calling Java methods from native code, and accessing native data structures <span class="No-Break">from Java.</span></p>
			<p>Internally, JNI works by mapping Java objects and types to their corresponding native counterparts. This mapping is done using the <strong class="source-inline">JNIEnv</strong> interface, which provides access to the <strong class="bold">Java Virtual Machine</strong>(<strong class="bold">JVM</strong>) internal state. When a<a id="_idIndexMarker1590"/> Java method is called from native code, <strong class="source-inline">JNIEnv</strong> is used to find the corresponding native method and pass it the necessary arguments. Similarly, when a native method returns a value, <strong class="source-inline">JNIEnv</strong> is used to convert the native value to a Java object. The JVM manages memory for both Java and native objects. However, native code must explicitly allocate and free its own memory. JNI provides functions <a id="_idIndexMarker1591"/>for allocating and freeing memory, as well as for copying data between Java and native memory. JNI code must be thread-safe. This means that any data accessed by JNI must be properly synchronized to avoid race conditions. Using JNI can have performance implications. Native code is typically faster than Java code, but there is overhead associated with calling into native code <span class="No-Break">through JNI.</span></p>
			<p>In the next section, we will discuss the C++ part of <span class="No-Break">the project.</span></p>
			<h2 id="_idParaDest-283"><a id="_idTextAnchor710"/>The native C++ part of the project</h2>
			<p>The main functionality of this example project is implemented in the native C++ part. It’s designed to use the OpenCV library to deal <a id="_idIndexMarker1592"/>with camera images and the PyTorch framework for object detection model inference. Such an approach allows you to port this solution to another platform if needed and allows you to use standard desktop instruments, such as OpenCV and<a id="_idIndexMarker1593"/> PyTorch, to develop and debug algorithms that will be used on <span class="No-Break">mobile platforms.</span></p>
			<p>There are two main C++ classes in this project. The <strong class="source-inline">Detector</strong> class is the application facade that implements a connection with the Android activity image-capturing pipeline, and delegates object detection to the second class, <strong class="source-inline">YOLO</strong>. The <strong class="source-inline">YOLO</strong> class implements the object detection model loading and <span class="No-Break">its inference.</span></p>
			<p>The following subsections will describe the implementation details of <span class="No-Break">these classes.</span></p>
			<h3>Initialization of object detection with JNI</h3>
			<p> We finished our discussion of the Kotlin part by talking about the JNI function declarations. The corresponding C++ implementation for <strong class="source-inline">initObjectDetection</strong> and <strong class="source-inline">stopObjectDetection</strong> are located in the <strong class="source-inline">native-lib.cpp</strong> file. This file is automatically created by the Android <a id="_idIndexMarker1594"/>Studio IDE for the native activity projects. The following code snippet shows the <strong class="source-inline">initObjectDetection</strong> <span class="No-Break">function definition:</span></p>
			<pre class="source-code">
#include &lt;jni.h&gt;
...
std::shared_ptr&lt;ObjectDetector&gt; object_detector_;
extern "C" JNIEXPORT void JNICALL
Java_com_example_objectdetection_MainActivity_initObjectDetection(
    JNIEnv* env,
    jobject /* this */,
    jstring camId) {
  auto camera_id = env-&gt;GetStringUTFChars(camId, nullptr);
        
LOGI("Camera ID: %s", camera_id);
  if (object_detector_) {
    object_detector_-&gt;allow_camera_session(camera_id);
    object_detector_-&gt;configure_resources();
  } else
    LOGE("Object Detector object is missed!");
}</pre>			<p>We followed JNI rules to make the function declaration correct and visible from the Java/Kotlin part. The name of the function includes the full Java package name, including namespaces, and our first two required parameters are the <strong class="source-inline">JNIEnv*</strong> and <strong class="source-inline">jobject</strong> types. The third parameter is the string and corresponds to the camera ID; this is the parameter that exists in the Kotlin declaration of <span class="No-Break">the function.</span></p>
			<p>In the function implementation, we checked whether the <strong class="source-inline">ObjectDetector</strong> object was already instantiated and, in this case, we called the <strong class="source-inline">allow_camera_session</strong> method with the camera ID and then called the <strong class="source-inline">configure_resources</strong> method. These calls make the <strong class="source-inline">ObjectDetector</strong> object remember what camera to use and initialize, configure the output window, and initialize<a id="_idIndexMarker1595"/> the <span class="No-Break">image-capturing pipeline.</span></p>
			<p>The second function we used in the Kotlin part is the <strong class="source-inline">stopObjectDetection</strong>, and its implementation is done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
extern "C" JNIEXPORT void JNICALL
Java_com_example_objectdetection_MainActivity_stopObjectDetection(
    JNIEnv*,
    jobject /* this */) {
  if (object_detector_) {
    object_detector_-&gt;release_resources();
  } else
    LOGE("Object Detector object is missed!");
}</pre>			<p>Here, we just released resources used for the image-capturing pipeline because when the application is suspended, access to the camera device is blocked. When the application is activated again, the <strong class="source-inline">initObjectDetection</strong> function will be called and the image-capturing pipeline will be <span class="No-Break">initialized again.</span></p>
			<p>You can see that we used the <strong class="source-inline">LOGI</strong> and the <strong class="source-inline">LOGE</strong> functions, which are defined <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
#include &lt;android/log.h&gt;
#define LOG_TAG "OBJECT-DETECTION"
#define LOGI(...) __android_log_print(ANDROID_LOG_INFO,
                                      LOG_TAG, __VA_ARGS__)
#define LOGW(...) __android_log_print(ANDROID_LOG_WARN,
                                      LOG_TAG, __VA_ARGS__)
#define LOGE(...) __android_log_print(ANDROID_LOG_ERROR,
                                      LOG_TAG, __VA_ARGS__)
#define ASSERT(cond, fmt, ...)                                \
  if (!(cond))
  {                                              \
     __android_log_assert(#cond, LOG_TAG, fmt, ##__VA_ARGS__); \
  }</pre>			<p>We defined these functions to log messages into the Android <strong class="source-inline">logcat</strong> subsystem more easily. This series of functions uses the<a id="_idIndexMarker1596"/> same tag for logging and has fewer arguments than the original <strong class="source-inline">__android_log_xxx</strong> functions. Also, the log level was encoded in the <span class="No-Break">function name.</span></p>
			<h3>Main application loop</h3>
			<p>This project will use the Native App Glue<a id="_idIndexMarker1597"/> library. This is a library for Android developers that helps to create native applications. It provides an abstraction layer between the Java code and the native code, making it easier to develop applications using <span class="No-Break">both languages.</span></p>
			<p>The use of this library allows us to have the standard <strong class="source-inline">main</strong> function with a loop that runs continuously, updating the UI, processing user input, and responding to system events. The following code snippet shows how we implemented the main function in the <span class="No-Break"><strong class="source-inline">native-lib.cpp</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
extern "C" void android_main(struct android_app* app) {
  LOGI("Native entry point");
  object_detector_ = std::make_shared&lt;ObjectDetector&gt;(app);
  app-&gt;onAppCmd = ProcessAndroidCmd;
  while (!app-&gt;destroyRequested) {
    struct android_poll_source* source = nullptr;
    auto result = ALooper_pollOnce(0, nullptr, nullptr,
                                   (void**)&amp;source);
    ASSERT(result != ALOOPER_POLL_ERROR,
           "ALooper_pollOnce returned an error");
    if (source != nullptr) {
      source-&gt;process(app, source);
    }
    if (object_detector_)
      object_detector_-&gt;draw_frame();
  }
  object_detector_.reset();
}</pre>			<p>This <strong class="source-inline">android_main</strong> function takes the instance of the <strong class="source-inline">android_app</strong> type, instead of regular <strong class="source-inline">argc</strong> and <strong class="source-inline">argv</strong> parameters. The <strong class="source-inline">android_app</strong> is a C++ class that provides access to the Android framework <a id="_idIndexMarker1598"/>and allows you to interact with system services. Also, you can use it to access the device hardware, such as sensors <span class="No-Break">and cameras.</span></p>
			<p>The <strong class="source-inline">android_main</strong> main function is the starting point for our native module. So, we initialized the global <strong class="source-inline">object_detector_</strong> object here, and it became available for the <strong class="source-inline">initObjectDetection</strong> and <strong class="source-inline">stopObjectDetection</strong> functions. For initialization, the <strong class="source-inline">ObjectDetector</strong> instance takes the pointer to the <span class="No-Break"><strong class="source-inline">android_app</strong></span><span class="No-Break"> object.</span></p>
			<p>Then, we attached the command processing function to the Android application object. Finally, we started the main loop, and it worked until the application was destroyed (closed). In this loop, we used the <strong class="source-inline">ALooper_pollOnce</strong> Android NDK function to get a pointer to the commands (events) <span class="No-Break">poller object.</span></p>
			<p>We called the <strong class="source-inline">process</strong> method of this object to dispatch the current command to our <strong class="source-inline">ProcessAndroidCmd</strong> function through the <strong class="source-inline">app</strong> object. At the end of the loop, we used our object detector object to grab the current camera picture and process it in the <span class="No-Break"><strong class="source-inline">draw_frame</strong></span><span class="No-Break"> method.</span></p>
			<p>The <strong class="source-inline">ProcessAndroidCmd</strong> function is implemented <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
 static void ProcessAndroidCmd(struct android_app* /*app*/,
                              int32_t cmd) {
  if (object_detector_) {
  switch (cmd) {
    case APP_CMD_INIT_WINDOW:
      object_detector_-&gt;configure_resources();
      break;
    case APP_CMD_TERM_WINDOW:
      object_detector_-&gt;release_resources();
      break;
  }
}</pre>			<p>Here, we processed only two commands that correspond to the application window initialization and termination. We <a id="_idIndexMarker1599"/>used them to initialize and clear the image-capturing pipeline in the object detector. When the window is created, we configure its dimensions according to the capturing resolution. The window termination command allows us to clear capturing resources to prevent access to the already blocked <span class="No-Break">camera device.</span></p>
			<p>That is all the information about the <strong class="source-inline">native-lib.cpp</strong> file. The next subsections will look at the <strong class="source-inline">ObjectDetector</strong> class <span class="No-Break">implementation details.</span></p>
			<h3>The ObjectDetector class overview</h3>
			<p>This is the main facade of the whole object detection pipeline of our application. The following list shows the functionality items <span class="No-Break">it implements:</span></p>
			<ul>
				<li>Camera device <a id="_idIndexMarker1600"/><span class="No-Break">access management</span></li>
				<li>Application window <span class="No-Break">dimensions configuration</span></li>
				<li>Image-capturing <span class="No-Break">pipeline management</span></li>
				<li>Camera image converting into OpenCV <span class="No-Break">matrix objects</span></li>
				<li>Drawing an object detection result into the <span class="No-Break">application window</span></li>
				<li>Delegating the object detection to the YOLO <span class="No-Break">inference object</span></li>
			</ul>
			<p>Before we start looking at these item details, let’s see how the constructor, the destructor, and some helper methods are implemented. The constructor implementation is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
ObjectDetector::ObjectDetector(android_app *app) : android_app_(app) {
  yolo_ = std::make_shared&lt;YOLO&gt;(app-&gt;activity-&gt;assetManager);
}</pre>			<p>We just saved the pointer to the <strong class="source-inline">android_app</strong> object and created the <strong class="source-inline">YOLO</strong> class inference object. Also, we used the <strong class="source-inline">android_app</strong> object to get a pointer to the <strong class="source-inline">AssetManager</strong> object, which is used to load files packaged into the <strong class="bold">Android Application Package</strong> (<strong class="bold">APK</strong>). The destructor is<a id="_idIndexMarker1601"/> implemented <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
ObjectDetector::~ObjectDetector() {
  release_resources();
  LOGI("Object Detector was destroyed!");
}
void ObjectDetector::release_resources() {
  delete_camera();
  delete_image_reader();
  delete_session();
}</pre>			<p>We called the <strong class="source-inline">release_resources</strong> method, which is where we close the opened camera device and clear capturing pipeline objects. The following code snippet shows the methods that are used from the <a id="_idIndexMarker1602"/>Kotlin part through the <span class="No-Break"><strong class="source-inline">initObjectDetection</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
void ObjectDetector::allow_camera_session(std::string_view camera_id) {
  camera_id_ = camera_id;
}</pre>			<p>In <strong class="source-inline">allow_camera_session</strong>, we saved the camera ID string; the device with this ID will be opened in the <strong class="source-inline">configure_resources</strong> method. As we already know, the camera ID will be passed to <strong class="source-inline">ObjectDetector</strong> only if the required permission is granted and there is a back-facing <a id="_idIndexMarker1603"/>camera on the Android device. So, we defined <strong class="source-inline">is_session_allowed</strong> <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
bool ObjectDetector::is_session_allowed() const {
  return !camera_id_.empty();
}</pre>			<p>Here, we just checked if a camera ID is <span class="No-Break">not empty.</span></p>
			<p>The following subsections will show the main functionality items <span class="No-Break">in detail.</span></p>
			<h3>Camera device and application window configuration</h3>
			<p>There is the <strong class="source-inline">create_camera</strong> method in the <strong class="source-inline">ObjectDetection</strong> class that implements the creation of a <a id="_idIndexMarker1604"/>camera manager<a id="_idIndexMarker1605"/> object and a camera device opening <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
void ObjectDetector::create_camera() {
  camera_mgr_ = ACameraManager_create();
  ASSERT(camera_mgr_, "Failed to create Camera Manager");
  ACameraManager_openCamera(camera_mgr_, camera_id_.c_str(),
                            &amp;camera_device_callbacks,
                            &amp;camera_device_);
  ASSERT(camera_device_, "Failed to open camera");
}</pre>			<p><strong class="source-inline">camera_mgr_</strong> is the <strong class="source-inline">ObjectDetector</strong> member variable and after initialization, it is used to open a camera device. The pointer to the opened camera device will be stored in the <strong class="source-inline">camera_device_</strong> member<a id="_idIndexMarker1606"/> variable. Also, notice that we used the camera ID string to open the <a id="_idIndexMarker1607"/>particular device. The <strong class="source-inline">camera_device_callbacks</strong> variable is defined <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
namespace {
void onDisconnected(
    [[maybe_unused]] void* context,
    [[maybe_unused]] ACameraDevice* device) {
  LOGI("Camera onDisconnected");
}
void onError([[maybe_unused]] void* context,
             [[maybe_unused]] ACameraDevice* device,
             int error) {
  LOGE("Camera error %d", error);
}
ACameraDevice_stateCallbacks camera_device_callbacks = {
    .context = nullptr,
    .onDisconnected = onDisconnected,
    .onError = onError,
};
}  // namespace</pre>			<p>We defined the <strong class="source-inline">ACameraDevice_stateCallbacks</strong> structure object with references to functions that simply report if the camera is opened or closed. These handlers can do some more useful work in other applications, but we can’t initialize them with nulls due to the <span class="No-Break">API requirements.</span></p>
			<p>The <strong class="source-inline">create_camera</strong> method is <a id="_idIndexMarker1608"/>called in the <strong class="source-inline">configure_resources</strong> method of the <strong class="source-inline">ObjectDetection</strong> class. This method is called every time the application is activated and it has the <a id="_idIndexMarker1609"/><span class="No-Break">following implementation:</span></p>
			<pre class="source-code">
void ObjectDetector::configure_resources() {
  if (!is_session_allowed() || !android_app_ ||
      !android_app_-&gt;window) {
          LOGE("Can't configure output window!");
      return;
  }
  if (!camera_device_)
      create_camera();
  // configure output window size and format
  ...
  if (!image_reader_ &amp;&amp; !session_output_) {
      create_image_reader();
      create_session();
  }
}</pre>			<p>In the beginning, we checked that there are all required resources: the camera ID, the <strong class="source-inline">android_app</strong> object, and that this object has a pointer to the application window. Then, we created a camera manager object and opened a camera device. Using the camera manager, we got the camera sensor orientation to configure the appropriate width and height for the application window. Also, using values for image capture width and height, we configured the window dimensions, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
ACameraMetadata *metadata_obj{nullptr};
ACameraManager_getCameraCharacteristics(camera_mgr_,
                                        camera_id_.c_str(),
                                        &amp;metadata_obj);
ACameraMetadata_const_entry entry;
ACameraMetadata_getConstEntry(metadata_obj,
                              ACAMERA_SENSOR_ ORIENTATION,
                              &amp;entry);
orientation_ = entry.data.i32[0];
bool is_horizontal = orientation_ == 0 || orientation_ == 270;
auto out_width = is_horizontal ? width_ : height_;
auto out_height = is_horizontal ? height_ : width_;
ANativeWindow_setBuffersGeometry(android_app_-&gt;window,
                                 out_width,
                                 out_height,
                                 WINDOW_FORMAT_RGBA_8888);</pre>			<p>Here, we used the <strong class="source-inline">ACameraManager_getCameraCharacteristics</strong> function to get the camera metadata characteristics<a id="_idIndexMarker1610"/> object. Then, we read the <strong class="source-inline">ACAMERA_SENSOR_ORIENTATION</strong> property <a id="_idIndexMarker1611"/>with the <strong class="source-inline">ACameraMetadata_getConstEntry</strong> function. After, we chose the appropriate width and height order based on the orientation used with the <strong class="source-inline">ANativeWindow_setBuffersGeometry</strong> function to set application output window dimensions and rendering <span class="No-Break">buffer format.</span></p>
			<p>The format we set is <strong class="source-inline">32</strong>-bit <strong class="bold">RGBA</strong> (<strong class="bold">Red Green Blue Alpha</strong>). If the orientation is horizontal, we swapped the width and <a id="_idIndexMarker1612"/>height. The exact width and height values are defined in the header file and are equal to <strong class="source-inline">800</strong> for height and <strong class="source-inline">600</strong> for width in portrait mode. This orientation handling is<a id="_idIndexMarker1613"/> very simple and is needed only to work with output window buffers correctly. Previously, we disabled the landscape mode for our application so we will ignore the<a id="_idIndexMarker1614"/> camera sensor orientation in the camera <span class="No-Break">image decoding.</span></p>
			<p>At the end of the <strong class="source-inline">configure_resources</strong> method, we created the camera reader object and initialized the <span class="No-Break">capturing pipeline.</span></p>
			<h3>Image-capturing pipeline construction</h3>
			<p>Previously, we saw that before the capturing <a id="_idIndexMarker1615"/>pipeline initialization, we created the image reader object. It’s done in the <strong class="source-inline">create_image_reader</strong> method, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
void ObjectDetector::create_image_reader() {
  constexpr int32_t MAX_BUF_COUNT = 4;
  auto status = AImageReader_new(
      width_, height_, AIMAGE_FORMAT_YUV_420_888,
      MAX_BUF_COUNT, &amp;image_reader_);
  ASSERT(image_reader_ &amp;&amp; status == AMEDIA_OK,
         "Failed to create AImageReader");
}</pre>			<p>We used <strong class="source-inline">AImageReader_new</strong> to create the <strong class="source-inline">AImageReader</strong> object with a particular width and height, the YUV format, and four image buffers. The width and height values we used were the same that were used for the output window dimensions configuration. The YUV format was used because it’s the native image format for most camera devices. Four image buffers were used to make image capturing sightly independent from their processing. It means that the image reader will fill one image buffer with camera data while we are reading another buffer and <span class="No-Break">processing it.</span></p>
			<p>The capture session initialization is<a id="_idIndexMarker1616"/> a complex process that requires several objects’ instantiation and their connection with each other. The <strong class="source-inline">create_session</strong> method implements it <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
void ObjectDetector::create_session() {
  ANativeWindow* output_ native_window;
  AImageReader_getWindow(image_reader_,
                         &amp;output_ native_window);
  ANativeWindow_acquire(output_native_window);
  ACaptureSessionOutputContainer_create(&amp;output_container_);
  ACaptureSessionOutput_create(output_native_window,
                               &amp;session_ output_);
  ACaptureSessionOutputContainer_add(output_container_,
                                     session_output_);
  ACameraOutputTarget_create(output_native_window,
                             &amp;output_target_);
  ACameraDevice_createCaptureRequest(
      camera_ device_, TEMPLATE_PREVIEW, &amp;capture_request_);
  ACaptureRequest_ addTarget(capture_request_,
                             output_target_);
  ACameraDevice_createCaptureSession(camera_device_,
                                     output_container_,
                                     &amp;session_callbacks,
                                     &amp;capture_session_);
  // Start capturing continuously
  ACameraCaptureSession_setRepeatingRequest(capture_session_,
                                            nullptr,
                                            1,
                                            &amp;capture_request_,
                                            nullptr);
}</pre>			<p>We started with getting a native <a id="_idIndexMarker1617"/>window from the image reader object and acquiring it. The window acquisition means that we took the reference to the window and the system should not delete it. This image reader window will be used as output for the capturing pipeline, so camera images will be drawn <span class="No-Break">into it.</span></p>
			<p>Then, we created the session output object and the container for the session output. The capturing session can have several outputs and they should be placed into a container. Every session output is a connection object for a concrete surface or a window output; in our case, it’s the image <span class="No-Break">reader window.</span></p>
			<p>Having configured session outputs, we created the capture request object and made sure that its output target was the image reader window. We configured the capture request for our opened camera device and the preview mode. After that, we instantiated the capturing session object and pointed it to the opened camera device, which had the container with the outputs we <span class="No-Break">created earlier.</span></p>
			<p>Finally, we started the capturing by setting the repeated request for the session. The connection between the session and capture request is the follows: we created the capturing session that was configured with a list of possible outputs, and the capture request specifies what surfaces will actually be used. There can be several capture requests and several outputs. In our case, we have a single capture request with a single output that will be continuously repeated. So, in general, we will capture real-time pictures for the camera like a video stream. The following picture shows the logical scheme of an image data flow in the <span class="No-Break">capturing session:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer954">
					<img alt="Figure 14.1 – The logical data flow in a capturing session" src="image/B19849_14_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – The logical data flow in a capturing session</p>
			<p>This is not the actual data flow scheme but the logical one that shows how the capture session objects are connected. The <a id="_idIndexMarker1618"/>dotted line shows the request path and the solid line shows the logical image <span class="No-Break">data path.</span></p>
			<h3>The capture image and the output window buffer management</h3>
			<p>When we discussed the<a id="_idIndexMarker1619"/> main application loop, we mentioned the <strong class="source-inline">draw_frame</strong> method, which is called in this loop after command processing. This method is used to take a captured image from the image reader <a id="_idIndexMarker1620"/>object, then detect objects on it and draw the detection results in the application window. The following code snippet shows the <strong class="source-inline">draw_fame</strong> <span class="No-Break">method implementation:</span></p>
			<pre class="source-code">
void ObjectDetector::draw_frame() {
  if (image_reader_ == nullptr)
      return;
  AImage *image = nullptr;
  auto status = AImageReader_acquireNextImage(image_reader_, &amp;image);
      
  if (status != AMEDIA_OK) {
      return;
  }
ANativeWindow_acquire(android_app_-&gt;window);
ANativeWindow_Buffer buf;
if (ANativeWindow_lock(android_app_-&gt;window,
                       &amp;buf,
                       nullptr) &lt; 0) {
  AImage_delete(image);
  return;
}
    process_image(&amp;buf, image);
    AImage_delete(image);
    ANativeWindow_unlockAndPost(android_app_-&gt;window);
    ANativeWindow_release(android_app_-&gt;window);
}</pre>			<p>We acquired the next image received by<a id="_idIndexMarker1621"/> the image reader object. Remember that we initialized it to have four image buffers. So, we acquire images from these buffers one by one in the main loop, and while we process one image, the capturing session fills another one that has already been <a id="_idIndexMarker1622"/>processed. It’s done in a circular manner. Having the image from a camera, we acquired and locked the application window, but if the lock fails, we delete the current image reference, stop processing, and go to the next iteration of the main loop. Otherwise, if we successfully lock the application window, we process the current image, detect objects on it, and draw detection results into an application window—this is done in the <strong class="source-inline">process_image</strong> method. This method takes the <strong class="source-inline">AImage</strong> and the <span class="No-Break"><strong class="source-inline">ANativeWindow_Buffer</strong></span><span class="No-Break"> objects.</span></p>
			<p>When we lock the application window, we get the pointer to the internal buffer that will be used for drawing. After we process the image and draw results, we unlock the application window to make its buffer available for the system, release the reference to the window, and delete the<a id="_idIndexMarker1623"/> reference to the image object. So, this method is mostly about resource management, and the real image<a id="_idIndexMarker1624"/> processing is done in the <strong class="source-inline">process_image</strong> method, which we will discuss in the <span class="No-Break">following subsection.</span></p>
			<h3>The captured image processing</h3>
			<p>The <strong class="source-inline">process_image</strong> method implements<a id="_idIndexMarker1625"/> the <span class="No-Break">following tasks:</span></p>
			<ol>
				<li>Convert Android YUV image data into the <span class="No-Break">OpenCV matrix.</span></li>
				<li>Dispatch the image matrix to the YOLO <span class="No-Break">object detector.</span></li>
				<li>Draw detection results into the <span class="No-Break">OpenCV matrix.</span></li>
				<li>Copy the OpenCV results matrix into the RGB (red, blue, green) <span class="No-Break">window buffer.</span></li>
			</ol>
			<p>Let’s see implementations for these tasks one by one. The <strong class="source-inline">process_image</strong> method signature looks <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
void ObjectDetector::process_image(
    ANativeWindow_Buffer* buf,
    AImage* image);</pre>			<p>This method takes the application window buffer object for results drawing and the image object for actual processing. To be able to process an image, we have to convert it into some appropriate data-structure format; in our case, this is the OpenCV matrix. We start with image format properties checking <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
int32_t src_format = -1;
AImage_getFormat(image, &amp;src_format);
ASSERT(AIMAGE_FORMAT_YUV_420_888 == src_format,
       "Unsupported image format for displaying");
int32_t num_src_planes = 0;
AImage_getNumberOfPlanes(image, &amp;num_src_planes);
ASSERT(num_src_planes == 3,
      "Image for display has unsupported number of planes");
int32_t src_height;
AImage_getHeight(image, &amp;src_height);
int32_t src_width;
AImage_getWidth(image, &amp;src_width);</pre>			<p>We checked that the image format is YUV (Luminance (Y), blue luminance (U), and red luminance (V)) and the image has three planes, so <a id="_idIndexMarker1626"/>we can proceed with its conversion. Then, we got image dimensions, which will be used later. After that, we verified the input data we extracted from the YUV plane data <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
int32_t y_stride{0};
AImage_getPlaneRowStride(image, 0, &amp;y_stride);
int32_t uv_stride1{0};
AImage_getPlaneRowStride(image, 1, &amp;uv_stride1);
int32_t uv_stride2{0};
AImage_getPlaneRowStride(image, 1, &amp;uv_stride2);
uint8_t *y_pixel{nullptr}, *uv_pixel1{nullptr}, *uv_pixel2{nullptr};
int32_t y_len{0}, uv_len1{0}, uv_len2{0};
AImage_getPlaneData(image, 0, &amp;y_pixel, &amp;y_len);
AImage_getPlaneData(image, 1, &amp;uv_pixel1, &amp;uv_len1);
AImage_getPlaneData(image, 2, &amp;uv_pixel2, &amp;uv_len2);</pre>			<p>We got strides, data sizes, and pointers to the actual YUV plane data. In this format, the image data is split into three components: luma (<strong class="source-inline">y</strong>), representing brightness, and two chroma components (<strong class="source-inline">u</strong> and <strong class="source-inline">v</strong>), which represent color information. The <strong class="source-inline">y</strong> component is usually stored at full resolution, while the <strong class="source-inline">u</strong> and <strong class="source-inline">v</strong> components may be subsampled. This allows for more efficient storage and <a id="_idIndexMarker1627"/>transmission of video data. The Android YUV image uses the half-sized resolution for <strong class="source-inline">u</strong> and <strong class="source-inline">v</strong>. The strides will allow us to correctly access the row data in the plane buffers; these strides depend on the image resolution and a data <span class="No-Break">memory layout.</span></p>
			<p>Having the YUV plane data and its strides and lengths, we convert them into OpenCV matrix objects, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
cv::Size actual_size(src_width, src_height);
cv::Size half_size(src_width / 2, src_height / 2);
cv::Mat y(actual_size, CV_8UC1, y_pixel, y_stride);
cv::Mat uv1(half_size, CV_8UC2, uv_pixel1, uv_stride1);
cv::Mat uv2(half_size, CV_8UC2, uv_pixel2, uv_stride2);</pre>			<p>We created the two <strong class="source-inline">cv::Size</strong> objects to store the original image size for the Y plane and the half size for the <strong class="source-inline">u</strong> and <strong class="source-inline">v</strong> planes. Then, we used these sizes, pointers to data, and strides to create an OpenCV matrix for every plane. We didn’t copy actual data into the OpenCV matrix objects; they will use data pointers that were passed for initialization. Such a view-creation approach saves memory and computational resources. The <strong class="source-inline">y</strong>-plane matrix has the 8-bit single-channel type but the <strong class="source-inline">u</strong> and <strong class="source-inline">v</strong> matrices have the 8-bit 2-channel type. We can use these matrices with the OpenCV <strong class="source-inline">cvtColorTwoPlane</strong> function to convert them into the RGBA format <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
cv::mat rgba_img_;
...
long addr_diff = uv2.data - uv1.data;
if (addr_diff &gt; 0) {
  cvtColorTwoPlane(y, uv1, rgba_img_, cv::COLOR_YUV2RGBA_NV12);
} else {
  cvtColorTwoPlane(y, uv2, rgba_img_, cv::COLOR_YUV2RGBA_NV21);
}</pre>			<p>We used the address difference to determine the ordering of the u and v planes: a positive difference indicates the NV12 format, while a negative difference indicates the NV21 format. <strong class="source-inline">NV12</strong> and <strong class="source-inline">NV21</strong> are types of the YUV format that differ in the order of the <strong class="source-inline">u</strong> and <strong class="source-inline">v</strong> components in the chroma <a id="_idIndexMarker1628"/>plane. In <strong class="source-inline">NV12</strong>, the <strong class="source-inline">u</strong> component precedes the <strong class="source-inline">v</strong> component, while in <strong class="source-inline">NV21</strong>, it’s the opposite. Such plane ordering plays a role in memory consumption and image processing performance, so the choice of which to use depends on the actual task and project. Also, the format can depend on the actual camera device, which is why we added <span class="No-Break">this detection.</span></p>
			<p>The <strong class="source-inline">cvtColorTwoPlane</strong> function takes the <strong class="source-inline">y</strong>-plane and <strong class="source-inline">uv</strong>-plane matrices as input arguments and outputs the RGBA image matrix into the <strong class="source-inline">rgba_img_</strong> variable. The last argument is the flag that tells the function what actual conversion it should perform. Now, this function can convert only YUV formats into RGB or <span class="No-Break">RGBA formats.</span></p>
			<p>As we said before, our application works only in portrait mode, but to make the image look normal, we need to rotate it <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
cv::rotate(rgba_img_, rgba_img_, cv::ROTATE_90_CLOCKWISE);</pre>			<p>Android camera sensors return camera images rotated even if we fixed our orientation, so we used the <strong class="source-inline">cv::rotate</strong> function to make it <span class="No-Break">look vertical.</span></p>
			<p>Having prepared the RGBA image, we pass it to the <strong class="source-inline">YOLO</strong> object detector and get the detection results. For every result item, we draw rectangles and labels on the image matrix we have already used for detection. These steps are implemented <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto results = yolo_-&gt;detect(rgba_img_);
for (auto&amp; result : results) {
  int thickness = 2;
  rectangle(rgba_img_, result.rect.tl(), result.rect.br(),
            cv::Scalar(255, 0, 0, 255), thickness,
            cv::LINE_4);
  cv::putText(rgba_ img_, result.class_name,
              result.rect.tl(), cv::FONT_HERSHEY_DUPLEX,
              1.0, CV_RGB(0, 255, 0), 2);
}</pre>			<p>We called the <strong class="source-inline">detect</strong> method of the <strong class="source-inline">YOLO</strong> object and got the <strong class="source-inline">results</strong> container. This method will be discussed later. Then, for each<a id="_idIndexMarker1629"/> item in the container, we draw a bounding box and a text label for the detected object. We used the OpenCV <strong class="source-inline">rectangle</strong> function with the <strong class="source-inline">rgba_img_</strong> destination image argument. Also, the text was rendered into the <strong class="source-inline">rgba_img_</strong> object. The detection result is the structure defined in the <strong class="source-inline">yolo.h</strong> header file <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
struct YOLOResult {
  int class_index;
  std::string class_name;
  float score;
  cv::Rect rect;
};</pre>			<p>So, a detection result has the class index and name properties, the model confidence score, and the bounding box in the image coordinates. For our results visualization, we used only rectangle and class <span class="No-Break">name properties.</span></p>
			<p>The last task that the <strong class="source-inline">process_image</strong> method <a id="_idIndexMarker1630"/>does is to render the resulting image into the application window buffer. It’s implemented <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
cv::Mat buffer_mat(src_width,
                   src_height,
                   CV_8UC4,
                   buf-&gt;bits,
                   buf-&gt;stride * 4);
rgba_img_.copyTo(buffer_mat);</pre>			<p>We created the OpenCV <strong class="source-inline">buffer_mat</strong> matrix to wrap the given window buffer. Then, we simply used the OpenCV <strong class="source-inline">copyTo</strong> method to put the RGBA image with rendered rectangles and class labels into the <strong class="source-inline">buffer_mat</strong> object. <strong class="source-inline">buffer_mat</strong> is the OpenCV view for the Android window buffer. We created it to follow the window buffer format we configured in the <strong class="source-inline">configure_resources</strong> method, the <strong class="source-inline">WINDOW_FORMAT_RGBA_8888</strong> format. So, we created the OpenCV matrix with the 8-bit 4-channel type and used the buffer stride information to satisfy memory layout access. Such a view allows us to <a id="_idIndexMarker1631"/>write less code and use OpenCV routines for <span class="No-Break">memory management.</span></p>
			<p>We discussed the main facade of our object detection application and in the following subsections, we will discuss details of how the YOLO model inference is implemented and how its results are parsed into the <span class="No-Break"><strong class="source-inline">YOLOResult</strong></span><span class="No-Break"> structures.</span></p>
			<h3>The YOLO wrapper initialization</h3>
			<p>There is only the constructor and the <strong class="source-inline">detect</strong> method in the <strong class="source-inline">YOLO</strong> class public API. We already saw that the <strong class="source-inline">YOLO</strong> object is<a id="_idIndexMarker1632"/> initialized in the <strong class="source-inline">ObjectDetector</strong> class constructor, and the <strong class="source-inline">detect</strong> method is used in the <strong class="source-inline">process_image</strong> method. The <strong class="source-inline">YOLO</strong> class constructor takes only the asset manager object as a single argument and is implemented <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
YOLO::YOLO(AAssetManager* asset_manager) {
  const std::string model_file_name = "yolov5s.torchscript";
  auto model_buf = read_asset(asset_manager,
                           model_file_name);
  model_ = torch::jit::_load_for_mobile(
      std::make_unique&lt;ReadAdapter&gt;(model_buf));
  const std::string classes_file_name = "classes.txt";
  auto classes_buf = read_asset( asset_manager,
                             classes_file_name);
  VectorStreamBuf&lt;char&gt; stream_buf(classes_buf);
  std::istream is(&amp;stream_buf);
  load_classes(is);
}</pre>			<p>Remember that we added the <strong class="source-inline">yolov5s.torchscript</strong> and the <strong class="source-inline">classes.txt</strong> files to the <strong class="source-inline">assets</strong> folder of our project. These files can be accessed in the application with the <strong class="source-inline">AAssetManager</strong> class object; this object was taken from the <strong class="source-inline">android_app</strong> object in the <strong class="source-inline">android_main</strong> function. So, in the constructor, we loaded the model binary and classes list file with a call to the <strong class="source-inline">read_asset</strong> function. Then, the model binary data was used to load and initialize the PyTorch script module with the <span class="No-Break"><strong class="source-inline">torch::jit::_load_for_mobile</strong></span><span class="No-Break"> function.</span></p>
			<p>Notice that the scripted model should be <a id="_idIndexMarker1633"/>saved with optimization for mobile and loaded with the corresponding function. When PyTorch for mobile was compiled, the regular <strong class="source-inline">torch::jit::load</strong> functionality was automatically disabled. Let’s look at the <strong class="source-inline">read_asset</strong> function that reads assets from the application bundle as <strong class="source-inline">std::vector&lt;char&gt;</strong> objects. The following code shows <span class="No-Break">its implementation:</span></p>
			<pre class="source-code">
std::vector&lt;char&gt; read_asset(AAssetManager* asset_manager,
                             const std::string&amp; name) {
  std::vector&lt;char&gt; buf;
  AAsset* asset = AAssetManager_open(
      asset_manager, name.c_str(), AASSET_MODE_UNKNOWN);
  if (asset != nullptr) {
    LOGI("Open asset %s OK", name.c_str());
    off_t buf_size = AAsset_getLength(asset);
    buf.resize(buf_size + 1, 0);
    auto num_read =AAsset_read(
                      asset, buf.data(), buf_size);
    LOGI("Read asset %s OK", name.c_str());
    if (num_read == 0)
      buf.clear();
    AAsset_close(asset);
    LOGI("Close asset %s OK", name.c_str());
  }
  return buf;
}</pre>			<p>There are four Android framework functions that we used to read an asset from the application bundle. The <strong class="source-inline">AAssetManager_open</strong> function opened the asset and returned the not null pointer to the <strong class="source-inline">AAsset</strong> object. This function assumes that the path to the asset is in the file path format and that the root of this path is the <strong class="source-inline">assets</strong> folder. After we opened the asset, we used the <strong class="source-inline">AAsset_getLength</strong> function to get the file size and allocated the <a id="_idIndexMarker1634"/>memory for <strong class="source-inline">std::vector&lt;char&gt;</strong> with the <strong class="source-inline">std::vector::resize</strong> method. Then, we used the <strong class="source-inline">AAsset_read()</strong> function to read the whole file to the <span class="No-Break"><strong class="source-inline">buf</strong></span><span class="No-Break"> object.</span></p>
			<p>This function does <span class="No-Break">the following:</span></p>
			<ul>
				<li>It takes the pointer to the asset object to <span class="No-Break">read from</span></li>
				<li>It takes the <strong class="source-inline">void*</strong> pointer to the memory buffer to <span class="No-Break">read in</span></li>
				<li>It measures the size of the bytes <span class="No-Break">to read</span></li>
			</ul>
			<p>So, as you can see, the assets API is pretty much the same as the standard C library API for file operations. When we’d finished working with the asset object, we used the <strong class="source-inline">AAsset_close</strong> function to notify the system that <a id="_idIndexMarker1635"/>we didn’t need access to this asset anymore. If your assets are in the <strong class="source-inline">.zip</strong> archive format, you should check the number of bytes returned by the <strong class="source-inline">AAsset_read</strong> function because the Android framework reads archives chunk <span class="No-Break">by chunk.</span></p>
			<p>You might see that we didn’t pass the vector of chars directly to the <strong class="source-inline">torch::jit::_load_for_mobile</strong> function. This function doesn’t work with standard C++ streams and types; instead, it accepts a pointer to an object of the <strong class="source-inline">caffe2::serialize::ReadAdapterInterface</strong> class. The following code shows how you to make the concrete <a id="_idIndexMarker1636"/>implementation of the <strong class="source-inline">caffe2::serialize::ReadAdapterInterface</strong> class, which wraps the <span class="No-Break"><strong class="source-inline">std::vector&lt;char&gt;</strong></span><span class="No-Break"> object:</span></p>
			<pre class="source-code">
class ReadAdapter
    : public caffe2::serialize::ReadAdapterInterface {
 public:
  explicit ReadAdapter(const std::vector&lt;char&gt;&amp; buf)
      : buf_(&amp;buf) {}
  size_t size() const override { return buf_-&gt;size(); }
  size_t read(uint64_t pos,
              void* buf,
              size_t n,
              const char* what) const override {
    std::copy_n(buf_-&gt;begin() + pos, n,
                reinterpret_cast&lt;char*&gt;(buf));
    return n;
  }
 private:
  const std::vector&lt;char&gt;* buf_;
};</pre>			<p>The <strong class="source-inline">ReaderAdapter</strong> class overrides two methods, <strong class="source-inline">size</strong> and <strong class="source-inline">read</strong>, from the <strong class="source-inline">caffe2::serialize::ReadAdapterInterface</strong> base class. Their implementations are pretty obvious: the <strong class="source-inline">size</strong> method returns the size of the underlying vector object, while the <strong class="source-inline">read</strong> method copies the <strong class="source-inline">n</strong> bytes (chars) from the vector to the destination buffer with the standard algorithm function, that <span class="No-Break">is, </span><span class="No-Break"><strong class="source-inline">std::copy_n</strong></span><span class="No-Break">.</span></p>
			<p>To load class information, we used the <strong class="source-inline">VectorStreamBuf</strong> adapter class to convert <strong class="source-inline">std::vector&lt;char&gt;</strong> into the <strong class="source-inline">std::istream</strong> type object. It was done because the <strong class="source-inline">YOLO::load_classes</strong> method takes an<a id="_idIndexMarker1637"/> object of the <strong class="source-inline">std::istream</strong> type. The <strong class="source-inline">VectorStreamBuf</strong> implementation is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
template&lt;typename CharT, typename TraitsT = std::char_traits&lt;CharT&gt; &gt;
struct VectorStreamBuf : public std::basic_streambuf&lt;CharT, TraitsT&gt; {
  explicit VectorStreamBuf(std::vector&lt;CharT&gt;&amp; vec) {
    this-&gt;setg(vec.data(), vec.data(),
               vec.data() + vec.size());
  }
}</pre>			<p>We inherited from the <strong class="source-inline">std::basic_streambuf</strong> class and in the constructor, we initialized the <strong class="source-inline">streambuf</strong> internal data with char values from the input vector. Then, we used an object of this adapter class as regular C++ input stream. You can see it in the <strong class="source-inline">load_classes</strong> method implementation, which is shown in the <span class="No-Break">following snippet:</span></p>
			<pre class="source-code">
void YOLO::load_classes(std::istream&amp; stream) {
  LOGI("Init classes start OK");
  classes_.clear();
  if (stream) {
    std::string line;
    std::string id;
    std::string label;
    size_t idx = 0;
    while (std::getline(stream, line)) {
      auto pos = line.find_first_of(':');
      id = line.substr(0, pos);
      label = line.substr(pos + 1);
      classes_.insert({idx, label});
      ++idx;
    }
  }
  LOGI("Init classes finish OK");
}</pre>			<p>The lines in <strong class="source-inline">classes.txt</strong> are in the <span class="No-Break">following format:</span></p>
			<pre class="source-code">
[ID] space character [class name]</pre>			<p>So, we read this file line by line and split <a id="_idIndexMarker1638"/>each line at the position of the first space character. The first part of each line is the class identifier, while the second one is the class name. To match the model’s evaluation result with the correct class name, we created the dictionary (map) object, where the key is the <strong class="source-inline">id</strong> value and the value is <strong class="source-inline">label</strong> (e.g., the <span class="No-Break">class name)s.</span></p>
			<h3>The YOLO detection inference</h3>
			<p>The <strong class="source-inline">detect</strong> method of the <strong class="source-inline">YOLO</strong> class is the place where we do the actual object detection. This method takes the OpenCV<a id="_idIndexMarker1639"/> matrix object that represents the RGB image as an argument and its implementation is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::vector&lt;YOLOResult&gt; YOLO::detect(const cv::Mat&amp; image) {
  constexpr int input_width = 640;
  constexpr int input_height = 640;
  cv::cvtColor(image, rgb_img_, cv::COLOR_RGBA2RGB);
  cv::resize(rgb_img_, rgb_img_,
             cv::Size(input_width, input_height));
  auto img_scale_x =
      static_cast&lt;float&gt;(image.cols) / input_width;
  auto img_scale_y =
      static_cast&lt;float&gt;(image.rows) / input_height;
  auto input_tensor = mat2tensor(rgb_img_);
  std::vector&lt;torch::jit::IValue&gt; inputs;
  inputs.emplace_back(input_tensor);
  auto output = model_.forward(inputs).toTuple() - &gt;
                elements()[0].toTensor().squeeze(0);
  output2results(output, img_scale_x, img_scale_y);
  return non_max_suppression();
}</pre>			<p>We defined constants that represent the width and height of the model input; it’s <strong class="source-inline">640</strong> x <strong class="source-inline">640</strong> because the YOLO model was trained on images of this size. Using these constants, we resized the input image. Also, we removed the alpha channel and made the RGB image. We calculated scale<a id="_idIndexMarker1640"/> factors for image dimensions, as they will be used to re-scale detected object boundaries to the original image size. Having scaled the image, we converted the OpenCV matrix into a PyTorch Tensor object using the <strong class="source-inline">mat2tensor</strong> function, whose implementation we will discuss later. The object type cast of the PyTorch Tensor object we added to the container of the <strong class="source-inline">torch::jit::IValue</strong> values was done automatically. There is a single element in this <strong class="source-inline">inputs</strong> container since the YOLO model takes a single RGB <span class="No-Break">image input.</span></p>
			<p>Then, we used the <strong class="source-inline">forward</strong> function of the YOLO <strong class="source-inline">model_</strong> object to perform inference. The PyTorch API script modules return the <strong class="source-inline">torch::jit::Tuple</strong> type. So, we explicitly cast the returned <strong class="source-inline">torch::jit::IValue</strong> object to the tuple and took the first element. This element was cast to the PyTorch <strong class="source-inline">Tensor</strong> object and the batch dimension was removed from it with the <strong class="source-inline">squeeze</strong> method. So, we got the <strong class="source-inline">torch::Tensor</strong> type <strong class="source-inline">output</strong> object of size <strong class="source-inline">25200</strong> x <strong class="source-inline">85</strong>. Here, <strong class="source-inline">25200</strong> is the number of detected objects and we <a id="_idIndexMarker1641"/>will apply the non-max suppression algorithm to get the final reduced output. The <strong class="source-inline">85</strong> means <strong class="source-inline">80</strong> class scores, <strong class="source-inline">4</strong> bounding box locations (x, y, width, height), and <strong class="source-inline">1</strong> confidence score. The resulting tensor was parsed into the <strong class="source-inline">YOLOResult</strong> structures in the <strong class="source-inline">output2results</strong> method. As we said, we used the <strong class="source-inline">non_max_suppression</strong> method to select the best <span class="No-Break">detection results.</span></p>
			<p>Let’s see the details of all the intermediate functions we used <span class="No-Break">for inference.</span></p>
			<h3>Converting OpenCV matrix into torch::Tensor</h3>
			<p>The <strong class="source-inline">mat2tensor</strong> function <a id="_idIndexMarker1642"/>convents an OpenCV <strong class="source-inline">mat</strong> object into a <strong class="source-inline">torch::Tensor</strong> object and is implemented <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
torch::Tensor mat2tensor(const cv::Mat&amp; image) {
  ASSERT(image.channels() == 3, "Invalid image format");
  torch::Tensor tensor_image = torch::from_blob(
      image.data,
      {1, image.rows, image.cols, image.channels()},
      at::kByte);
  tensor_image = tensor_image.to(at::kFloat) / 255.;
  tensor_image = torch::transpose(tensor_image, 1, 2);
  tensor_image = torch::transpose(tensor_image, 1, 3);
  return tensor_image;
}</pre>			<p>We used the <strong class="source-inline">torch::from_blob</strong> function to create the torch <strong class="source-inline">Tensor</strong> object from the raw data. The data pointer is what we took from the OpenCV object with the <strong class="source-inline">data</strong> property. The shape we used, <strong class="source-inline">[HIGHT, WIDTH, CHANNELS]</strong>, follows the OpenCV memory layout where the last dimension is the channel number dimension. Then, we made tensor float and normalized it to the <strong class="source-inline">[0,1]</strong> interval. PyTorch and the YOLO model use different shape<a id="_idIndexMarker1643"/> layouts to <strong class="source-inline">[CHANNELS, HEIGHT, WIDTH]</strong>. So, we transpose the tensor <span class="No-Break">channels appropriately.</span></p>
			<h3>Processing model output tensor</h3>
			<p>The next function we <a id="_idIndexMarker1644"/>used is <strong class="source-inline">output2results</strong>, which converts the output Tensor object into the vector of the <strong class="source-inline">YOLOResult</strong> structures. It has the <span class="No-Break">following implementation:</span></p>
			<pre class="source-code">
void YOLO::output2results(const torch::Tensor &amp;output,
                          float img_scale_x,
                          float img_scale_y) {
  auto outputs = output.accessor&lt;float, 2&gt;();
  auto output_row = output.size(0);
  auto output_column = output.size(1);
  results_.clear();
  for (int64_t i = 0; i &lt; output_row; i++) {
    auto score = outputs[i][4];
    if (score &gt; threshold) {
      // read the bounding box
      // calculate the class id
      results_.push_back(YOLOResult{
          .class_index = cls,
          .class_name = classes_[cls],
          .score = score,
          .rect = cv::Rect(left, top, bw, bh),
      });
    }
  }</pre>			<p>In the beginning, we used the <strong class="source-inline">accessor&lt;float, 2&gt;</strong> method of the torch Tensor object to get a very useful accessor for the tensor. This accessor allowed us to use the square brackets operator to access the <a id="_idIndexMarker1645"/>elements in a multidimensional tensor. The number <strong class="source-inline">2</strong> means that the tensor is 2D. Then, we made a loop over tensor rows because every row corresponds to a single detection result. Inside the loop, we did the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>We read the confidence score from the element with row <span class="No-Break">index </span><span class="No-Break"><strong class="source-inline">4</strong></span><span class="No-Break">.</span></li>
				<li>We continued result row processing if the confidence score was greater than <span class="No-Break">the threshold.</span></li>
				<li>We read the <strong class="source-inline">0, 1, 2, 3</strong> elements, which are the [x, y, width, height] coordinates of the <span class="No-Break">bounding rectangle.</span></li>
				<li>Using the previously calculated scale factors, we converted these coordinates into the [left, top, width, <span class="No-Break">height] format.</span></li>
				<li>We read the elements 5-84, which are class probabilities, and selected the class with the <span class="No-Break">maximum value.</span></li>
				<li>We created the <strong class="source-inline">YOLOResult</strong> structure with calculated values and inserted it into the <span class="No-Break"><strong class="source-inline">results_</strong></span><span class="No-Break"> container.</span></li>
			</ol>
			<p>The bounding box calculation was done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
float cx = outputs[i][0];
float cy = outputs[i][1];
float w = outputs[i][2];
float h = outputs[i][3];
int left = static_cast&lt;int&gt;(img_scale_x * (cx - w / 2));
int top = static_cast&lt;int&gt;(img_scale_y * (cy - h / 2));
int bw = static_cast&lt;int&gt;(img_scale_x * w);
int bh = static_cast&lt;int&gt;(img_scale_y * h);</pre>			<p>The YOLO model returns X and Y coordinates<a id="_idIndexMarker1646"/> for the center of a rectangle so we converted them into the image(screen) coordinate system: to the <span class="No-Break">top-left point.</span></p>
			<p>The class ID selection was implemented <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
float max = outputs[i][5];
int cls = 0;
for (int64_t j = 0; j &lt; output_column - 5; j++) {
if (outputs[i][5 + j] &gt; max) {
  max = outputs[i][5 + j];
  cls = static_cast&lt;int&gt;(j);
}
}</pre>			<p>We used the loop over the last elements that represent 79 class probabilities to select the index of the maximum value. This index was used as a <span class="No-Break">class ID.</span></p>
			<h3>NMS and IoU</h3>
			<p><strong class="bold">Non-Maximum Suppression</strong> (<strong class="bold">NMS</strong>) and <strong class="bold">Intersection over Union</strong> (<strong class="bold">IoU</strong>) are two key algorithms used in YOLO <a id="_idIndexMarker1647"/>for refining and filtering the output predictions to get the <span class="No-Break">best</span><span class="No-Break"><a id="_idIndexMarker1648"/></span><span class="No-Break"> results.</span></p>
			<p>NMS is used to suppress or<a id="_idIndexMarker1649"/> eliminate duplicate detections that overlap with each other. It works by comparing the predicted bounding boxes from the network<a id="_idIndexMarker1650"/> and removing those that have high overlaps with others. For example, if there are two bounding boxes predicted for the same object, NMS will keep only the one with the highest confidence score and discard the rest. The following picture shows how <span class="No-Break">NMS works:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer955">
					<img alt="Figure 14.2 – NMS" src="image/B19849_14_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – NMS</p>
			<p>IoU is another algorithm used in conjunction with NMS to measure the overlap between bounding boxes. IoU calculates the ratio of an intersection area to a union area between two boxes. The <strong class="bold">intersection area</strong> refers to <a id="_idIndexMarker1651"/>the area where boxes overlap. The <strong class="bold">union area</strong>, on the other hand, refers to <a id="_idIndexMarker1652"/>the total area <a id="_idIndexMarker1653"/>covered by both boxes. The IoU value ranges from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>, where <strong class="source-inline">0</strong> means no <a id="_idIndexMarker1654"/>overlap, and <strong class="source-inline">1</strong> indicates perfect overlap. The following picture shows how <span class="No-Break">IoU works:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer956">
					<img alt="Figure 14.3 – IoU" src="image/B19849_14_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – IoU</p>
			<p>We implemented NMS in the <strong class="source-inline">non_max_suppression</strong> method <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::vector&lt;YOLOResult&gt; YOLO::non_max_suppression() {
  // do an sort on the confidence scores, from high to low.
    std::sort(results_.begin(), results_.end(), [](
          auto &amp;r1, auto &amp;r2) {
        return r1.score &gt; r2.score;
    });
    std::vector&lt;YOLOResult&gt; selected;
    std::vector&lt;bool&gt; active(results_.size(), true);
    int num_active = static_cast&lt;int&gt;(active.size());
    bool done = false;
    for (size_t i = 0; i &lt; results_.size() &amp;&amp; !done; i++) {
  if (active[i]) {
    const auto&amp; box_a = results_[i];
    selected.push_back(box_a);
    if (selected.size() &gt;= nms_limit)
      break;
    for (size_t j = i + 1; j &lt; results_.size(); j++) {
      if (active[j]) {
        const auto&amp; box_b = results_[j];
        if (IOU(box_a.rect, box_b.rect) &gt; threshold) {
          active[j] = false;
          num_active -= 1;
          if (num_active &lt;= 0) {
            done = true;
            break;
          }
        }
      }
    }
  }
}
  return selected;
}</pre>			<p>At first, we sorted all detection results by confidence score in descending order. We marked all results as active. If a <a id="_idIndexMarker1655"/>detection result is active, then we can compare another result with it, otherwise, the result is already suppressed. Then, every active detection result was sequentially compared with the following active results; remember that the container is sorted. The comparison was done by calculating the IoU <a id="_idIndexMarker1656"/>value for bounding boxes and comparing the IoU value with a threshold. If the IoU value is greater than the threshold, we marked the result with a lower confidence value as non-active; we suppressed it. So, we defined the nested comparison loop. In the outer loop, we ignored suppressed results too. Also, this nested loop has the check for the maximum allowed number of results; refer to the use of the <span class="No-Break"><strong class="source-inline">nms_limit</strong></span><span class="No-Break"> value.</span></p>
			<p>The IoU algorithm for two bounding boxes is implemented in the <strong class="source-inline">IOU</strong> function <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
float IOU(const cv::Rect&amp; a, const cv::Rect&amp; b) {
  if (a.empty() &lt;= 0.0)
    return 0.0f;
  if (b.empty() &lt;= 0.0)
    return 0.0f;
  auto min_x = std::max(a.x, b.x);
  auto min_y = std::max(a.y, b.y);
  auto max_x = std::min(a.x + a.width, b.x + b.width);
  auto max_y = std::min(a.y + a.height, b.y + b.height);
  auto area = std::max(max_y - min_y, 0) *
              std::max(max_x - min_x, 0);
  return static_cast&lt;float&gt;(area) /
         static_cast&lt;float&gt;(a.area() + b.area() - area);
}</pre>			<p>At first, we checked bounding boxes for emptiness; if one of the boxes is empty, the IoU value is zero. Then, we calculated the intersection area. This was done by finding the minimum and maximum values for X and Y, considering both bounding boxes, and then taking the product of the difference between these values. The union area was calculated by summing the areas of two bounding boxes minus the intersection area. You can see this calculation in the return statement where we calculated the <span class="No-Break">area’s ratio.</span></p>
			<p>Together, NMS and IoU help improve the accuracy and precision of YOLO by discarding false positives and ensuring that only relevant detections are included in the <span class="No-Break">final output.</span></p>
			<p>In this section, we looked at the<a id="_idIndexMarker1657"/> implementation of object detection applications for the Android system. We learned how to export a pre-trained model from a Python <a id="_idIndexMarker1658"/>program as a PyTorch script file. Then, we delved into developing a mobile application with Android Studio IDE and the mobile version of the PyTorch C++ library. In the following figure, you can see an example of the application <span class="No-Break">output window:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer957">
					<img alt="Figure 14.4 – Object detection application output" src="image/B19849_14_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – Object detection application output</p>
			<p>In this figure, you can see that our application successfully detected a laptop and a computer mouse in front of the <a id="_idIndexMarker1659"/>smartphone camera. Every detection <a id="_idIndexMarker1660"/>result was marked with the bounding box and <span class="No-Break">corresponding label.</span></p>
			<h1 id="_idParaDest-284"><a id="_idTextAnchor711"/>Summary</h1>
			<p>In this chapter, we discussed how to deploy ML models, especially neural networks, to mobile platforms. We examined that, on these platforms, we usually need a customized build of the ML framework that we used in our project. Mobile platforms use different CPUs, and sometimes, they have specialized neural network accelerator devices, so you need to compile your application and ML framework in regard to these architectures. These architectures differ from development environments, and you often use them for two different purposes. The first case is to use powerful machine configuration with GPUs to accelerate the ML training process, so you need to build your application while taking the use of one or multiple GPUs into account. The other case is using a device for inference only. In this case, you typically don’t need a GPU at all because a modern CPU can, in many cases, satisfy your <span class="No-Break">performance requirements.</span></p>
			<p>In this chapter, we developed an object detection application for the Android platform. We learned how to connect the Kotlin module with the native C++ library through JNI. Then, we examined how to build the PyTorch C++ library for Android using the NDK and saw what limitations there are to using the <span class="No-Break">mobile version.</span></p>
			<p>This was the last chapter of the book; I hope you have enjoyed this book and found it helpful in your journey to mastering the use of C++ for ML. I hope that by now, you have gained a solid understanding of how to leverage the power of C++ to build robust and efficient ML models. Throughout the book, I have aimed to provide clear explanations of complex concepts, practical examples, and step-by-step guides to help you get started with C++ for ML. I have also included tips and best practices to help you avoid common pitfalls and optimize your models for performance. I want to remind you that the possibilities are endless when it comes to using C++ <span class="No-Break">for ML.</span></p>
			<p>Whether you are a beginner or an experienced developer, there is always something new to learn and explore. With that in mind, I encourage you to continue to push your boundaries and experiment with different approaches and techniques. The world of ML is constantly evolving, and by staying up to date with the latest trends and developments, you can stay ahead of the curve and build cutting-edge models that can solve <span class="No-Break">complex problems.</span></p>
			<p>Thank you again for choosing my book and for taking the time to learn about using C++ for ML. I hope that you find it to be a valuable resource and that it helps you on your journey toward becoming a skilled and successful <span class="No-Break">ML developer.</span></p>
			<h1 id="_idParaDest-285"><a id="_idTextAnchor712"/>Further reading</h1>
			<ul>
				<li>PyTorch C++ <span class="No-Break">API: </span><a href="https://pytorch.org/cppdocs/"><span class="No-Break">https://pytorch.org/cppdocs/</span></a></li>
				<li>Documentation for app <span class="No-Break">developers: </span><a href="https://developer.android.com/develop"><span class="No-Break">https://developer.android.com/develop</span></a></li>
				<li>Android <span class="No-Break">NDK: </span><a href="https://developer.android.com/ndk"><span class="No-Break">https://developer.android.com/ndk</span></a></li>
				<li>PyTorch guides for mobile <span class="No-Break">development: </span><a href="https://pytorch.org/mobile/android/"><span class="No-Break">https://pytorch.org/mobile/android/</span></a></li>
				<li>PyTorch guide for optimized mobile script <span class="No-Break">exporting: </span><a href="https://pytorch.org/tutorials/recipes/script_optimized.html"><span class="No-Break">https://pytorch.org/tutorials/recipes/script_optimized.html</span></a></li>
				<li>OpenCV Android SDK <span class="No-Break">tutorial: </span><a href="https://docs.opencv.org/4.x/d5/df8/tutorial_dev_with_OCV_on_Android.html"><span class="No-Break">https://docs.opencv.org/4.x/d5/df8/tutorial_dev_with_OCV_on_Android.html</span></a></li>
				<li>ExcuTorch – a new framework for running PyTorch on embedded <span class="No-Break">devices: </span><a href="https://pytorch.org/executorch/stable/index.html"><span class="No-Break">https://pytorch.org/executorch/stable/index.html</span></a></li>
			</ul>
		</div>
	</body></html>