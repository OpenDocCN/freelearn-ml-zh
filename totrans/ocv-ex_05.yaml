- en: Chapter 5. Automated Optical Inspection, Object Segmentation, and Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about histograms and filters that allowed
    us to understand image manipulation and create a photo application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce you to the basic concepts of object segmentation
    and detection, which means isolation the objects that appear in an image for future
    processing and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Noise removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The basics of light/background removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thresholding operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A connected component for object segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding contours for object segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The industry sector uses complex Computer Vision systems and hardware. Computer
    Vision tries to detect the problems and minimizes errors produced in the production
    process and increases the quality of final products.
  prefs: []
  type: TYPE_NORMAL
- en: In this sector, the name for Computer Vision tasks is **Automated Optical Inspection**
    or AOI. This name appears in the inspection of printed circuit board manufacturers,
    where one or more cameras scan each circuit to detect critical failures and quality
    defects. This nomenclature was used by other manufacturers to use optical camera
    systems and Computer Vision algorithms to increase the product quality. Nowadays,
    the use of optical inspection using different camera types such as infrared, 3D
    cameras, and so on depends on the problem requirements, such as measure objects,
    detect surface effects, and so on; and complex algorithms are used in thousands
    of industries for different purposes, such as defects detection, recognition,
    classification, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Isolating objects in a scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce you to the first step of any AOI algorithm,
    that is, isolating different parts or objects in a scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take the example of object detection and classification of three object
    types: a screw, a packing ring, and a nut and develop these in this chapter and
    [Chapter 6](ch06.html "Chapter 6. Learning Object Classification"), *Learning
    Object Classification*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we are in a company that produces these three objects. All of them
    are in the same carrier tape, and our objective is to detect each object in the
    carrier tape and classify each one to allow a robot to put each object on the
    correct shelf:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Isolating objects in a scene](img/B04283_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we will isolate each object and detect its position in the
    image in pixels. In the next chapter, we will classify each isolated object to
    check whether it is a nut, a screw, or a packing ring.
  prefs: []
  type: TYPE_NORMAL
- en: In the following image, we show our desired result where there are a few objects
    in the left-hand side image, and in the right-hand side image, we draw each one
    in different colors. We can show different features such as the area, height,
    width, countour size, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Isolating objects in a scene](img/B04283_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To achieve this result, we will follow different steps that allow us to better
    understand and organize our algorithm, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Isolating objects in a scene](img/B04283_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Our application is divided into two chapters. In this chapter, we will develop
    and understand the preprocessing and segmentation steps. In [Chapter 6](ch06.html
    "Chapter 6. Learning Object Classification"), *Learning Object Classification,*
    we will extract the characteristics of each segmented object and train our machine
    learning system/algorithm to identify each object class to allow you to classify
    our objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our preprocessing steps are divided into three more substeps, which are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Noise removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lighting removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the segmentation step, we will use two different algorithms, which are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The contour detection algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The connected component extraction (labeling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see these substeps in the following diagram along with the application
    flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Isolating objects in a scene](img/B04283_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, it's time to start the preprocessing step to get the best binarization
    image by removing the noise and lighting effects in order to minimize the possible
    detection errors.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an application for AOI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create our new application, we require a few input parameters when the user
    executes them; all of them are optional, excluding the input image to be processed:'
  prefs: []
  type: TYPE_NORMAL
- en: An input image to be processed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The light image pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The light operation, where the user can choose between difference or division
    operations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the input value of the user is set to `0`, then a difference operation is
    applied
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the input value of the user is set to `1`, then a division operation is applied
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Segmentation, where the user can choose between connected components with or
    without statistics and `findContours` methods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the input value of the user is set to `1`, then the connected `components`
    method for the segment is applied
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the input value of the user is set to `2`, then the connected components
    with the statistics area is applied
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the input value of the user is set to `3`, then the `findContours` method
    is applied to the segmentation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To enable this user selection, we will use the command line `parser` class
    with these keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the command line `parser` class that checks the parameters in the `main`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After `parser` class our command line user data, we check whether the input
    image is correctly loaded, and then we load the image and check whether it has
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to create our AOI process of segmentation. We will start with
    the preprocessing task.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the input image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section introduces you to some of the most common techniques that can be
    applied to preprocess images in the context of object segmentation/detection.
    The preprocess is the first change that we make in a new image before we start
    with our work and extract the information that we require from it.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, in the preprocessing step, we try to minimize the image noise, light
    conditions, or image deformations due to the camera lens. These steps minimize
    the errors when you try to detect objects or segment our image.
  prefs: []
  type: TYPE_NORMAL
- en: Noise removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we don''t remove the noise, we can detect more objects than we expect because
    normally noise is represented as a small point in the image and can be segmented
    as an object. The sensor and scanner circuit normally produce this noise. This
    variation of brightness or color can be represented in different types, such as
    Gaussian noise, spike noise, and shot noise. There are different techniques that
    can be used to remove the noise. We will use a smooth operation, but depending
    of the type on the noise, we will use some that are better than others. For example,
    a median filter is normally used to remove the salt-pepper noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Noise removal](img/B04283_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The left-hand side image is the original input with a salt-pepper noise. If
    we apply a median blur, we get an awesome result where we lose small details.
    For example, the borders of a screw for which we maintain the perfect edges. Refer
    to the top-right figure. If we apply a box filter or a Gaussian filter, the noise
    if not removed. It is just smoothed and the details of objects are loosed and
    smoothed as well. Refer to the bottom-right figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV provides us with the `medianBlur` function that requires the following
    three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: An input image with a 1, 3, or 4 channel image. When the kernel size is greater
    than `5`, the image depth can only be `CV_8U`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output image,which is the resulting image, that has the same type and depth
    as that of the input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel size that has the aperture size greater than `1` and an odd value.
    For example, 3, 5, 7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This piece of code used to remove the noise looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Removing the background using the light pattern for segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will develop a basic algorithm that enables us to remove
    the background using a light pattern. This preprocessing gives us better segmentation.
    Refer to the following figures. The top-left figure is the input image without
    noise, and the top-right figure is the result of applying a thresholding operation;
    we can see the top artifact. The bottom-left figure is the input image after the
    removal of the background, the bottom-right figure is the thresholding result
    where there are no artifacts in it and it's better to segment it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing the background using the light pattern for segmentation](img/B04283_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'How can we remove the light from our image? It is very simple; we only need
    a picture of our scenario without any object that is taken from exactly the same
    position from where the other images have been taken, and to have the same light
    conditions. This is a very common technique in AOI because the external conditions
    are supervised and known. The image result of our case is similar to the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing the background using the light pattern for segmentation](img/B04283_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, with a simple mathematical operation, we can remove this light pattern.
    There are two options to remove it, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Difference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Division
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The difference images are the simplest approach. If we have the light pattern
    *L* and the image picture *I,* the removal *R* result is the difference between
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This division is a bit more complex but simple at the same time. If we have
    the light pattern matrix *L* and the image picture matrix *I,* the removal *R*
    result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we divide the image by the light pattern. We make the assumption
    that if our light pattern is white and the objects are darker than the background
    carrier tape, then the image pixel values will always remain the same or will
    be lower than the light pixel values. Then, the result that we obtain from *I/L*
    is between `0` and `1`. Finally, we invert the result of this division to get
    the same color direction range and multiply it by 255 to get values between the
    `0-255` range.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our code, we will create a new function called `removeLight` with the following
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: An input image to remove the light/background
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Light pattern mat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Method, `0` is difference, `1` division
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is a new image matrix without the light/background.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code implements the background removal using the light pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's try to understand this. After creating the `aux` variable, in order to
    save the result, we select the method that is chosen by the user and passed via
    a parameter to the function. If the method selected is `1`, we apply the `division`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `division` method requires a 32-bit float image to allow us to divide the
    images. The first step is to convert the image and light pattern mat to 32-bit
    depth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can perform the mathematical operations in our matrix, as described,
    dividing the image by the pattern and inverting the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have the result, but we need to return an 8-bit depth image, and then,
    use the `convert` function, as we did previously, to convert it to a 32-bit float:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can return the `aux` variable with the result. For the difference method,
    the development is very easy because we don''t have to convert our images, we
    only need to perform the difference and return. If we don''t assume that the pattern
    is equal to or greater than the image, then we will require a few checks and truncate
    values that can be less than `0` or greater than `255`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure is the result of applying the image light pattern to our
    input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing the background using the light pattern for segmentation](img/B04283_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the results that we obtain, we can check how the light gradient is removed
    and the possible artifacts are removed as well.
  prefs: []
  type: TYPE_NORMAL
- en: However, what happens when we don't have a light/background pattern? There are
    a few different techniques to do this, and we are going to present the most basic
    one. Using a filter, we can create one that can be used, but there are better
    algorithms from which you can learn the background from a few images, where the
    pieces appear in different areas. This technique sometimes requires a background
    estimation image initialization, where our basic approach can play very well.
    These advanced techniques are explored in the video surveillance chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To estimate the background image, we will use a blur with a large kernel size
    that is applied to our input image. This is a common technique used in OCR where
    the letters are thin and small relative to the whole document, and allows us to
    perform an approximation of the light patterns in the image. We can see the light/background
    pattern reconstruction on the left-hand side figure and the ground truth on the
    right-hand side figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing the background using the light pattern for segmentation](img/B04283_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there are minor differences in the light patterns, but this
    result is enough to remove the background, and we can see the result in the following
    figure using difference images.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, we can see the result of applying the image difference
    between the original input image and the estimated background image that are computed
    with the previous approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Removing the background using the light pattern for segmentation](img/B04283_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `calculateLightPattern` function creates this light pattern or background
    approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This `basic` function applies a blur to an input image using a big kernel size
    relative to the image size. From the code, it is one-third of the original width
    and height.
  prefs: []
  type: TYPE_NORMAL
- en: The thresholding operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After removing the background, we only have to binarize the image for future
    segmentation. Now, we will apply the `threshold` function using two different
    threshold values: a very low value when we remove the light/background because
    all non-interest regions are black or very low values, and a medium value when
    we do not use a light removal method because we have a white background and the
    object images have lower values. This last option allows us to check the results
    with and without the background removal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will continue with the most important part of our application: the
    segmentation. We will use two different approaches or algorithms: connected components
    and contours.'
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting our input image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will introduce you to the following two techniques used to segment
    our thresholded image:'
  prefs: []
  type: TYPE_NORMAL
- en: The connected components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `findContours` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these two techniques, we will be allowed to extract each region of interest
    of our image where our target objects appear; in our case, a nut, screw, and ring.
  prefs: []
  type: TYPE_NORMAL
- en: The connected component algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The connected component is a very common algorithm used to segment and identify
    parts in binary images. A connected component is an iterative algorithm used for
    the purpose of labeling an image using an 8- or 4-connectivity pixel. Two pixels
    are connected if they have the same value and are neighbors. In the following
    figure, each pixel has eight neighbor pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The connected component algorithm](img/B04283_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A 4-connectivity means that only the 2, 4, 5, and 7 neighbors can be connected
    to the center if they have the same value. In the case of 8-connectivity, 1, 2,
    3, 4, 5, 6, 7, and 8 can be connected if they have the same value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we can see the difference between an eight and four
    connectivity algorithm. We will apply each algorithm to the next binarized image.
    We used a small 9 X 9 image and zoomed it to show how connected components, and
    the difference between an 4- and 8-connectivity, work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The connected component algorithm](img/B04283_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The 4-connectivity algorithm detects two objects, as shown on the left-hand
    side image. The 8-connectivity algorithm detects only one object (the right-hand
    side image) because two diagonal pixels are connected, whereas in a 4-connectivity
    algorithm, only vertical and horizontal pixels are connected. We can see the result
    in the following figure, where each object has a different gray color value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The connected component algorithm](img/B04283_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'OpenCV 3 introduces you to the connected components algorithm with the following
    two different functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`connectedComponents(image, labels, connectivity=8, type=CV_32S)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connectedComponentsWithStats(image, labels, stats, centroids, connectivity=8,
    ltype=CV_32S)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the functions return an integer with the number of detected labels, where
    the label `0` represents the background.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference between these two functions is basically the information that
    returns each one. Let''s check the parameters of each one. The `connectedComponents`
    function give us the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image**: This is the input image to be labeled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labels**: This is a `mat` output with the same size of an input image, where
    each pixel has the value of its label, and all `0`''s represent the background,
    the pixels that have `1` as values represent the first connected component object,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connectivity**: This has two possible values: `8` or `4` that represents
    the connectivity we want to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type**: This is the type of the label image that we would want to use: only
    two types are allowed, `CV32_S` or `CV16_U`. By default, it is `CV32_S`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `connectedComponentsWithStats` function has two more parameters that are
    defined: stats and centroids parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Stats`: This is an output parameter for each label, including the background
    label. The following statistics values can be accessed via stats (label, column),
    where columns are defined as well, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_LEFT`: This is the leftmost x coordinate of a connected component
    object'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_TOP`: This is the topmost y coordinate of a connected component object'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_WIDTH`: This is the width of a connected component object defined
    by its bounding box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_HEIGHT`: This is the height of a connected component object defined
    by its bounding box'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_AREA`: This is the number of pixels (area) of the connected component
    object'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Centroids`: The centroid points in `float` type for each label inclusive of
    the background'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our example application, we will create two functions that are to be applied
    to these two OpenCV algorithms and show the user the obtained result in a new
    image with colored objects in the basic algorithm and draw the area of the stats
    algorithm for each object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the basic drawing of the connected `component` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'First of all, we call the OpenCV `connectedComponents` function that returns
    the number of objects detected. If the number of objects is less than two, this
    means that only the background object is detected, and then, we don''t need to
    draw anything and finish. If the algorithm detects more than one object, then
    we show the number of objects detected via the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will draw all the detected objects in a new image with different colors,
    and then we need to create a new black image with the same input size and three
    channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to loop over each label, except the *0* value because it''s the
    background label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract each object from the label image, we need to create a mask for each
    label `i` using a comparison, and save it in a new image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set a pseudo-random color to the output image using the mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After we loop all images, we have all the objects with different colors in
    our output image, and we only have to show the output image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result where each object is painted with a different color or gray
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The connected component algorithm](img/B04283_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will explain how to use the connected components with the stats OpenCV
    algorithm and show some more information in the output result image. The following
    function implements this functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s understand the code, as we did in the non-stats function. We call the
    connected components algorithm; but, in this case, using the `stats` function,
    we check whether we can detect more than one object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have two more output results: the `stats` and `centroids` variables.
    Then, for each label that we detect, we will show its centroid and area via the
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You can check the call to the `stats` variable in order to extract the area
    using the `stats.at<int>(I, CC_STAT_AREA)` column constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, as mentioned earlier, we paint the output image of the object labeled
    with the `i` number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to add over the image, in the centroid of the object segmented,
    some info like the area. To do this, we use the `stats` and `centroid` variables
    using the `putText` function. First, we need to create a stringstream to add the
    stats area information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use the `putText` using the centroid as the text position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this function looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The connected component algorithm](img/B04283_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The findContours algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `findContours` algorithm is one of the most frequently used OpenCV algorithms
    to segment objects. This algorithm has been included in OpenCV since its first
    version and provides more information and descriptors, such as shapes, topological
    organizations, and so on, to the developers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain each parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image**: This is the input binary image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contours**: This is the contours output where each detected contour is a
    vector of points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchy**: This is the optional output vector where we store the hierarchy
    of contours. This is the topology of the image where we can get the relations
    between each contour.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mode**: This is the method used to retrieve the contours:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETR_EXTERNAL`: This retrieves only the external contours.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETR_LIST`: This retrieves all the contours without establishing the hierarchy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETR_CCOMP`: This retrieves all the contours with two levels of hierarchy:
    external and holes. If another object is inside one hole, then this is put on
    the top of the hierarchy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETR_TREE`: This retrieves all the contours that create a full hierarchy between
    contours.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method**: This allows you to perform the approximation method to retrieve
    the contours'' shapes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CV_CHAIN_APPROX_NONE:` This does not apply any approximation to the contours
    and stores all the contours points.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CV_CHAIN_APPROX_SIMPLE`: This compresses all the horizontal, vertical, and
    diagonal segments that store only the start and end points.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CV_CHAIN_APPROX_TC89_L1,CV_CHAIN_APPROX_TC89_KCOS` This applies the Teh-Chin
    chain approximation algorithm.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offset**: This is the optional point value used to shift all the contours.
    This is very useful when we work in a ROI and is required to retrieve the global
    positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: The input image is modified by the `findContours` function. Create a copy of
    your image before it is sent to this function if you need it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know the parameters of the `findContours` function, let''s apply
    them to our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Let's understand our implementation line by line.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we don''t require any hierarchy, so we will retrieve only the
    external `contours` of all possible objects. To do this, we use the `RETR_EXTERNAL`
    mode, and we use the basic contour encoding scheme using the `CHAIN_APPROX_SIMPLE`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the connected components examples mentioned earlier, we first check
    how many `contours` we have retrieved. If there are none, then we exit from our
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we draw each detected `contour` that we detect, and we draw it in
    our output image with a different color. To do this, OpenCV provides us with a
    function to draw the result of the find `contours` image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `drawContours` function allows the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image**: This is the output image used to draw the contours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contours**: This is the vector of contours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contour index**: This is a number that indicates the contour to be drawn;
    if it is negative, all the contours are drawn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Color**: This is the color used to draw the contour.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Thickness**: If this is negative, then the contour is filled with the color
    chosen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line type**: This is used when we want draw with antialiasing, or other drawing
    methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchy**: This is an optional parameter and is only needed if you want
    to draw only some of the contours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max level**: This is an optional parameter and taken into account only when
    the hierarchy parameter is available. If it is set to 0, only the specified contour
    is drawn, and if it is set to 1, the function draws the current contour and the
    nested as well. If it is set to 2, then the algorithm draws all the specified
    contour hierarchies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offset**: This is an optional parameter used to shift the contours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result of our example can be shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The findContours algorithm](img/B04283_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After a binarized image, we can see the three different algorithms that are
    used to divide and separate each object of an image, allowing us to isolate each
    object in order to manipulate or extract features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the entire process in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The findContours algorithm](img/B04283_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the basics of object segmentation in a controlled
    situation, where a camera take pictures of different objects.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to remove the background and light in order to allow us to binarize
    our image by minimizing the noise and also three different algorithms used to
    divide and separate each object of an image, allowing us to isolate each object
    in order to manipulate or extract features. Finally, we extracted all the objects
    on an image, where we are going to extract characteristics of each of these objects
    to train a machine learning system.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to predict the class of any of objects in
    an image, and then call to a robot or any other system to pick any of them, or
    detect an object that is not in the correct carrier tape, and then notify to a
    person to pick it up.
  prefs: []
  type: TYPE_NORMAL
