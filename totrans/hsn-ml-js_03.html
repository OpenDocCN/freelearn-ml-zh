<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Tour of Machine Learning Algorithms</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to explore the different ways to categorize the types of tasks that <strong>machine learning</strong> (<strong>ML</strong>) can accomplish, and categorize the ML algorithms themselves. There are many different ways to organize the ML landscape; we can categorize algorithms by the type of training data we give them, we can categorize by the type of output we expect from the algorithms, we can categorize algorithms by their specific methods and tactics, we can categorize them by the format of the data they work with, and so on.</p>
<p>As we discuss the different types and categories of ML tasks and algorithms throughout this chapter, we'll also introduce many of the algorithms that you'll encounter throughout this book. Only the high-level concepts of algorithms will be discussed in this chapter, allowing us to go into detail in later chapters. The topics that we will be covering in this chapter are as follows:</p>
<ul>
<li>Introduction to machine learning</li>
<li>Types of learning—unsupervised learning, supervised learning, and reinforcement learning</li>
<li>Categories of algorithms—clustering, classification, regression, dimensionality reduction, optimization, natural language processing, and image processing</li>
</ul>
<p>At the end of this chapter, you should have an understanding of supervised learning versus unsupervised learning, and should understand the overall landscape of the algorithms that we'll apply throughout this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to machine learning</h1>
                </header>
            
            <article>
                
<p>In general, ML is the name we give to the practice of making computers learn without explicitly programming insights into the algorithm. The converse practice—that is, programming an algorithm with a set of instructions that it can apply to datasets—is often called<span> </span><strong>heuristics</strong>. This is our first classification of algorithms: machine learning versus heuristic algorithms. If you are managing a firewall and are manually maintaining a blacklist of IP address ranges to block, you can be said to have developed a heuristic for your firewall. On the other hand, if you develop an algorithm that analyzes patterns in web traffic,<span> </span>infers<span> </span>from those patterns, and automatically maintains your blacklist, you can be said to have developed an ML approach to firewalls.</p>
<p>We can, of course, further subcategorize our ML firewall approach. If your algorithm is designed with no<span> </span><em>a priori</em><span> </span>knowledge (knowledge beforehand), that is, if the algorithm<span> </span><em>starts from scratch</em>, then it can be called an <strong>unsupervised learning</strong> algorithm. On the other hand, if you train the algorithm by showing it examples of requests from sources that should be blocked and expect it to learn by example, then the algorithm can be called a<span> </span><strong>supervised learning</strong> algorithm.</p>
<p>The specific algorithm you implement may also fall into yet another subcategory. Your algorithm may rely on<span> </span><em>clustering</em><span> </span>similar requests in order to determine which cluster a given request might belong to, or your algorithm may use Bayesian statistics to determine the probability that a request should be<span> </span><em>classified</em><span> </span>good or bad, or your algorithm may use a combination of techniques such as clustering, classification, and heuristics! Like many other taxonomical systems, there is often ambiguity in classifying special cases, but for the most part, algorithms can be divided into different categories.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of learning</h1>
                </header>
            
            <article>
                
<p>All ML algorithms consume data as an input and are expected to generate insights, predictions, classifications, or analyses as an output. Some algorithms have an additional <em>training</em> step, where the algorithm is trained on some data, tested to make sure that they have learned from the training data, and at a future date given a new data point or set of data for which you desire insights.</p>
<p>All ML algorithms that use training data expect the data to be <em>labeled</em>, or somehow marked with the desired result for that data. For instance, when building a spam filter, you must first teach or train the algorithm on what spam looks like as compared to what normal messages (called <strong>ham</strong>) look like. You must first train the spam filter on a number of messages, each labeled either <em>spam</em> or <em>ham</em>, so that the algorithm can learn to distinguish between the two. Once the algorithm is trained, you can present it with a new, never-before-seen message, and expect it to guess whether that message is ham or spam. In this example, the set of messages you train the algorithm with is called the <strong>training data</strong> or <strong>training set</strong>, the labels in use are <em>spam</em> and <em>ham</em>, and the guesswork that the algorithm performs is called <strong>inference</strong>. This practice of training an algorithm on a set of prelabeled training data is called <strong>supervised learning</strong>.</p>
<p>Other algorithms do not require training, or can inspect a dataset without any labels and develop insights directly from the data. This is called <strong>unsupervised learning</strong>, and this classification is marked by the lack of labels on the data. If you work in a scientific laboratory and are developing an image processing algorithm to inspect pictures of bacterial cultures in Petri dishes, with the goal of the algorithm telling you how many distinct bacterial colonies are seen in the photograph, you have developed an unsupervised learning algorithm. In this case, you do not need to train the algorithm with training data that has the number of colonies prelabeled; the algorithm is expected to work from scratch to find patterns and structures in the data. The inputs and outputs are similar to the supervised learning example, in that you give the data to the algorithm and expect to receive insights as output, but these inputs and outputs are different in that there is no training step or <em>a</em> <em>priori</em> knowledge required by the algorithm.</p>
<p>There are further classifications that fall within a spectrum between supervised and unsupervised learning. For instance, in <em>semi-supervised </em>learning, an algorithm receives a prelabeled training set, but not every label is represented by the training data. In this case, the algorithm is expected to fit examples to the trained labels where applicable, but also expected to generate new labels when appropriate.</p>
<p>Another mode of learning is <strong>reinforcement learning</strong>. Reinforcement learning is similar to both supervised learning and unsupervised learning in various ways. In reinforcement learning, the training data does not have explicit labels, but the results that the algorithm generates may be associated with a certain penalty or reward; the goal of the algorithm is to eventually optimize its results such that the penalty is minimized. Reinforcement learning is often used in conjunction with supervised learning. An algorithm may be initially trained on some labeled training data, but then is expected to update its model based on feedback about the decisions it has made.</p>
<p>For the most part, you will find that supervised and unsupervised learning are the two major categories of algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p>In unsupervised learning, the goal is to infer structure or patterns from data without needing any prior labeling of the data. Because the data is unlabeled, there is typically no way to evaluate the accuracy of the learning algorithm, a major distinction from supervised learning. Unsupervised learning algorithms typically are not given any <em>a</em> <em>priori</em> knowledge of the data, except perhaps indirectly by the tuning parameters given to the algorithm itself.</p>
<p>Unsupervised learning is commonly used for problems that might be solvable by eye if the data had very few dimensions, but the large dimensionality of the data makes this impossible or very difficult for a human to infer. Unsupervised learning can also be used for lower-dimension problems that may be solved intuitively by a human, but where there is a lot of data to be processed, it is unreasonable to do manually.</p>
<p>Imagine that you're writing an algorithm that looks at satellite imagery data and the task is to identify buildings and cluster them into geographically-separated neighborhoods. If you have just one image, or a handful of images, this is easy to accomplish by hand. A researcher would mark all the buildings on a photo and visually inspect the photo to determine clusters of buildings. The researcher then records the latitude and longitude of the neighborhood's center and puts the results in a spreadsheet. Great, the head scientist says, only three million more images to go! This is an example of a low-dimensional problem (there are only two dimensions, <em>latitude</em> and <em>longitude</em>, to consider) that is made implausible by the sheer volume of the task. Clearly a more sophisticated solution is required.</p>
<p>To develop an unsupervised learning approach to this problem, a researcher might divide the problem into two stages: <strong>preprocessing</strong> and <strong>analysis</strong>. In the preprocessing step, each image should be run through an algorithm that detects buildings in the photograph and returns their latitude/longitude coordinates. This preprocessing step can be managed in several ways: one approach would be to send the images to a team of interns to be manually marked; another approach could be a non-machine learning edge detection algorithm that looks for rectangular shapes; and a third approach could be a <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>) that is trained to identify images of buildings.</p>
<p>Once the preprocessing has been done and a list of building coordinates is on hand, the coordinates can then be run through an unsupervised clustering algorithm, such as the k-means algorithm, which we'll explore later. The unsupervised algorithm does not need to know what a <em>building</em> is, it doesn't need to know about any existing neighborhoods or clusters of buildings, and doesn't need any other <em>a priori</em> knowledge of the problem. The algorithm is simply able to read a list of millions or billions of latitude/longitude coordinates, and group them into geographically-centered clusters.</p>
<p>Because unsupervised algorithms cannot judge the accuracy of their results, there is no guarantee that this algorithm will generate neighborhoods that match up with census data or that the algorithm's concept of <em>neighborhood</em> will be semantically correct. It's possible that a single town or neighborhood may be considered two separate neighborhoods if, for instance, a wide highway divides two halves of the town. It's also possible that the algorithm may combine two neighborhoods that are considered by their residents to be distinct into one single cluster, if there is no clear separation between the two neighborhoods.</p>
<p>In many cases, this type of semantic error is acceptable; the benefit of this approach to the problem is that it can process millions or billions of data points quickly and provides at least a logical sense of clustering. The results of the unsupervised clustering can be further postprocessed, either by another algorithm or reviewed by hand, to add semantic information to the results.</p>
<p>Unsupervised algorithms can also find patterns in high-dimensional datasets that humans are unable to visualize intuitively. In the building clustering problem, it's easy for a researcher to visually inspect the two-dimensional map and identify clusters by eye. Imagine now that you have a set of data points, each existing in a 100-dimensional space (that is, data that has 100 distinct features). If the amount of data you possess is non-trivial, for example, more than 100 or 1,000 data points, it may be nearly impossible for a human to interpret the data, because the relationships between the features are too difficult to visualize in a 100-dimensional space.</p>
<p>As a contrived example of the preceding problem, imagine you're a psychologist and your task is to interpret a thousand surveys given to participants that ask 100 different questions, each on a 1-10 scale. Each question is designed to rate a different aspect of the participant's personality. Your goal in interpreting this data is to determine how many distinct personality types are represented by the respondents.</p>
<p>Processing only 1,000 data points by hand is certainly achievable, and common practice <span><span>in</span></span> many fields. In this case, however, the high dimensionality of the data makes it very difficult to discover patterns. Two respondents may have answered some questions very similarly, but answered other questions differently; are these two respondents similar enough to be considered of the same personality type? And how similar is that personality type to any other given personality type? The same algorithm we used previously to detect clusters of buildings can be applied to this problem in order to detect clusters of respondents and their personality types (my apologies to any actual psychologists reading this; I know I have grossly oversimplified the problem!).</p>
<p>In this case, the unsupervised clustering algorithm does not have any difficultly with <em>visualizing</em> the 100 dimensions involved, and will perform similarly to the two-dimensional neighborhood clustering problem. The same caveats apply, as well: there is no guarantee that the clusters the algorithm detects will be psychologically correct, nor that the questions themselves were designed correctly to appropriately capture all the distinct personality types. The only promise this algorithm makes is that it will identify clusters of similar data points.</p>
<p>In <a href="94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml" target="_blank">Chapter 2</a>, <em>Data Exploration,</em> we discussed the importance of preprocessing data before giving it to a ML algorithm. We are now beginning to understand the importance of postprocessing and interpreting results, especially when looking at unsupervised algorithms. Because unsupervised algorithms can only judge their overall statistical distribution (that is, the average distance from any point to its cluster center in this case), rather than their semantic error (that is, how many data points are actually <strong>correct</strong>), the algorithm cannot make any claims as to its semantic correctness. Looking at metrics such as root-mean-squared error or standard deviation may give you a hint as to how the algorithm performed, but this cannot be used as a judgment of the algorithm's accuracy, and can only be used to describe the statistical properties of the dataset. Looking at these metrics won't tell you if the results are correct, and will only tell you how clustered or unclustered the data is (some neighborhoods are sparse, other neighborhoods are dense, and so on).</p>
<p>So far we have considered unsupervised learning in the context of clustering algorithms, which is indeed a major family of unsupervised learning algorithms, but there are also many others. For instance, our discussion of outlier detection from <a href="94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml" target="_blank">Chapter 2</a>, <em>Data Exploration</em>, would fall under the category of unsupervised learning; we are looking at unlabeled data with no <em>a priori</em> knowledge, and attempting to glean insights from that data.</p>
<p>Another example of a popular unsupervised learning technique is <strong>Principal Component Analysis</strong> (<strong>PCA</strong>), which we briefly introduced in <a href="94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml" target="_blank">Chapter 2</a>, <em>Data Exploration</em>. PCA is an unsupervised learning algorithm commonly used during preprocessing for feature detection and dimensionality reduction, and this algorithm fits the use case of interpreting high dimensional data. Unlike clustering algorithms, which aim to tell you how many logical clusters of data points exist in a dataset, PCA aims to tell you which features or dimensions of a dataset can be neatly combined into statistically significant derived features. In some sense, PCA can be thought of as the clustering of features or dimensions, rather than the clustering of data points.</p>
<p>An algorithm such as PCA does not necessarily need to be used exclusively for preprocessing, and can in fact be used as the primary ML algorithm from which you want to gain insight.</p>
<p>Let's return to our psychological survey example. Rather than clustering survey respondents, we might want to analyze the questions themselves with a PCA. The results of the algorithm would tell you which survey questions are most significantly correlated to one another, and this insight can help you rewrite the actual survey questions so that they better target the personality traits that you wish to study. Additionally, the dimensionality reduction that PCA provides can help a researcher visualize the relationship between the questions, respondents, and results. The algorithm will convert your 100-dimensional, highly interconnected feature space that's impossible to visualize into distinct, lower-dimensional spaces that can actually be graphed and visually inspected.</p>
<p>As with all unsupervised learning algorithms, there is no guarantee that the principal component algorithm will be semantically correct, there is only a guarantee that the algorithm will be able to statistically determine the relationships between features. This means that some results may seem nonsensical or unintuitive; the algorithm might combine questions that don't seem to make intuitive sense when combined. In a situation like this, it's up to the researcher to postprocess and interpret the results of the analysis, potentially modifying the questions or changing their approach for the next round of surveys.</p>
<p>There are many other examples of unsupervised learning algorithms, including the autoencoder neural network, which we will discuss in a later chapter. The most important feature of an unsupervised learning algorithm is the lack of labels in its input data, which results in the inability to determine the semantic correctness of its results. Do not make the mistake of dismissing unsupervised learning algorithms as being <em>lesser</em> than other algorithms, however, as they are very important in data preprocessing and many other types of data exploration tasks. Just as a wrench is no more and no less valuable than a screwdriver, each tool has its place and purpose in the world of ML.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p>Like unsupervised learning, the goal of a supervised learning algorithm is to interpret input data and generate insights as its output. Unlike unsupervised learning, supervised learning algorithms are first trained on labeled training examples. The training examples are used by the algorithm to build a <em>model</em>, or an internal representation of the relationships between the data's properties and its label, and the model is then applied to new, unlabeled data points that you wish to glean insight from.</p>
<p>Supervised learning is often more exciting to ML students, as this category of algorithms aims to provide semantically correct results. When a supervised learning algorithm works well, the results almost seem magical! You can train an algorithm on 1,000 prelabeled data points and then use that model to process millions of future data points, with some expectation of semantic accuracy in the results.</p>
<p>Because supervised learning algorithms aim to be semantically correct, we must first discuss how this correctness is measured. First, we must introduce the concepts of <em>true positives</em>, <em>false positives</em>, <em>true negatives</em>, and <em>false negatives</em>, then we will introduce the concepts of accuracy, precision, and recall.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Measuring accuracy</h1>
                </header>
            
            <article>
                
<p>Imagine you are developing a spam filter for a commenting system you've developed for your blog. Spam filters are a type of supervised learning algorithm, as the algorithm must first be told what constitutes spam versus ham. You train your spam system on many examples of ham and spam messages, and then release it into production and allow it to classify all new comments, automatically blocking spam messages and letting genuine ham messages go through.</p>
<p>Let us consider a <em>positive</em> to be a comment that the algorithm identifies as spam (we're calling this <em>positive</em> because we're calling the algorithm a spam filter; this is only a semantic distinction, as we could call the filter a <em>ham filter</em> and instead use <em>positive</em> to denote suspected ham messages). Let's consider a <em>negative</em> to be a comment identified as a genuine (ham) comment.</p>
<p>If your algorithm categorizes a comment as spam (positive), and does so semantically correctly (that is, when you read the message, you also determine that it is spam), the algorithm has generated a <em>true positive</em>, or a positive result that is truly and correctly a positive result. If, on the other hand, a genuine comment incorrectly gets identified as spam and blocked, that is considered a <em>false positive</em>, or a positive result that is not actually positive. Similarly, a genuine ham message that is identified as ham is a <em>true negative</em>, and a spam comment that is identified as ham and let through is considered a <em>false negative</em>. It is unreasonable to expect an algorithm to provide 100% correct results, so there will always be some amount of false positives and false negatives in practice.</p>
<p>If we take our four classifications of result accuracy, we can count the number of instances of each classification and determine a rate for each: we can easily calculate the false positive rate, the true positive rate, the false negative rate, and the true negative rate. However, these four rates may be clumsy to discuss if we treat them independently, so we can also combine these rates into other categories.</p>
<p>For instance, the <em>recall</em>, or <em>sensitivity,</em> of an algorithm is its true positive rate, or the percentage of times that a positive classification is a true positive. In our spam example, the recall therefore refers to the percentage of spam messages correctly identified out of all actual spam messages. This can be calculated as either <em>true positives divided by actual positives</em>, or alternatively <em>true positives divided by true positives plus false negatives</em> (recall that false negatives are comments that are actually spam, but incorrectly identified as ham). Recall, in this case, refers to the algorithm's ability to correctly detect a spam comment, or put simply, <em>of all the actual spam messages there are, how many did we identify?</em></p>
<p>Specificity is similar to recall, except that it represents the algorithm's true negative rate. Specificity asks the question <em>of all actual ham messages, how many did we correctly identify?</em></p>
<p>Precision, on the other hand, is defined as the number of true positives divided by the sum of true positives and false positives. In terms of our spam example, precision answers the question <em>of all the messages we think are spam, how many guesses did we get correctly?</em> The distinction between the two metrics lies in whether we are considering all <em>actual</em> spam messages, or considering messages we <em>think</em> are spam.</p>
<p>Accuracy is distinct from both precision and recall, and focuses on overall correct results. It is defined as the rate of true positives and true negatives divided by the total number of trials (that is, how many guesses were correct overall). A common mistake that students of ML often make is to focus on accuracy alone, because it is intuitively easier to understand, but accuracy often is not sufficient when evaluating the performance of an algorithm.</p>
<p>To demonstrate this, we must consider the impact of the performance of our spam filter on our real-world results. In some cases, you want a spam filter that never ever lets a single spam message through, even if that means incorrectly blocking some ham messages. In other cases, it's better to make sure that all ham messages are allowed, even if a few spam messages evade your filter. It's possible for two different spam filters to have the same exact <em>accuracy</em>,<em> </em>but totally different characteristics of precision and recall. For this reason, accuracy (while very useful) cannot always be the only performance metric you consider.</p>
<p>Because the previous mathematical definitions may be a little difficult to internalize, let's put numbers to the example. Consider 100 messages, 70 of which are genuinely ham and 30 of which are genuinely spam:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td/>
<td>
<p><strong>30</strong> Actual Spam (Positive)</p>
</td>
<td>
<p><strong>70</strong> Actual Ham (Negative)</p>
</td>
</tr>
<tr>
<td>
<p><strong>26</strong> Guessed Spam</p>
</td>
<td>
<p>22 (True Positive)</p>
</td>
<td>
<p>4 (False Positive)</p>
</td>
</tr>
<tr>
<td>
<p><strong>74</strong> Guessed Ham</p>
</td>
<td>
<p>8 (False Negative)</p>
</td>
<td>
<p>66 (True Negative)</p>
</td>
</tr>
</tbody>
</table>
<p>To calculate the accuracy of the algorithm, we add up the correct guesses: <kbd>22</kbd> true positives and <kbd>66</kbd> true negatives, which equals 88 correct guesses in total. Our accuracy is therefore 88%.</p>
<div class="packt_tip">As an aside: 88% accuracy would be considered very good for advanced algorithms on difficult problems, but a little poor for a spam filter.</div>
<p>The recall or sensitivity of the algorithm is the <em>true positive rate</em>, or the number of times we guessed correctly when looking at examples that are <em>actually</em> spam. This means that we only consider the left hand column in the preceding table. The recall of the algorithm is the number of true positives among the actual positives, that is, the number of true positives divided by the true positives plus the false negatives. In this case, we have 22 true positives and 30 actual spam messages, so the recall of our algorithm is 22/30, or 73%.</p>
<p>The precision of the algorithm relates not to the messages that are <em>actually</em> spam, but instead to the messages that we <em>guessed</em> are spam. In this case, we only consider the top row, or the true positives divided by the sum of true positives and false positives; that is, the true positives divided by the guessed positives. In our case, there are 22 true positives and 26 total guessed positives, so our precision is 22/26, or 84%.</p>
<p>Note that this algorithm is more precise than it is sensitive. This means that its spam guesses are 84% correct <em>when it guesses spam</em>, but the algorithm also has a tendency to lean towards guessing ham, and misses a good number of actual spam messages. Also note that the total accuracy is 88%, but both its precision and recall are lower than that figure.</p>
<p>Another way to think about these performance metrics intuitively is as follows: precision is the algorithm's ability to guess correctly when it guesses positive, but recall is the algorithm's ability to remember what a spam message looks like. High precision and low recall would mean that an algorithm is very selective when guessing that a message is spam; the algorithm really needs to be convinced that a message is spam before identifying it as spam.</p>
<div class="packt_infobox">The algorithm is very <em>precise</em> about saying that a message is spam.</div>
<p>It, therefore, might favor letting ham messages through at the cost of accidentally letting some spam through. A low-precision, high-recall algorithm, on the other hand, will tend to more aggressively identify messages as spam, however, it will also incorrectly block a number of ham messages (the algorithm better <em>recalls</em> what spam looks like, it is more <em>sensitive</em> to spam, therefore it thinks more messages are spam and will act accordingly).</p>
<p>Of course, some algorithms can have high accuracy, precision, and recall—but more realistically, the way you train your algorithms will involve trade-offs between precision and recall, and you must balance these trade-offs against the desired goals of your system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning algorithms</h1>
                </header>
            
            <article>
                
<p>Now that we have developed an understanding of accuracy, precision, and recall, we can continue with the topic at hand: supervised learning algorithms. The key distinction between supervised and unsupervised learning algorithms is the presence of prelabeled data, typically introduced during the training phase of the algorithm. A supervised learning algorithm should be able to learn from labeled training data and then analyze a new, unlabeled data point and guess that data's label.</p>
<p>Supervised learning algorithms further divide into two subcategories: <strong>classification</strong> and <strong>regression</strong>. Classification algorithms aim to predict the label of an unseen data point, based on the generalized patterns that it has learned from the training data, as described previously. Regression algorithms aim to predict the value of a new point, again based on the generalized patterns that it has learned during training. While classification and regression feel different in practice, the preceding description betrays how similar the two categories actually are; the major distinction between the two is that regression algorithms typically work with continuous data, for instance, time-series or coordinate data. For the rest of this section, however, we will discuss only classification tasks.</p>
<p>Because the algorithm builds a model from labeled data, it is expected that the algorithm can generate <em>semantically</em> correct results, as opposed to the <em>statistically</em> correct results that unsupervised algorithms generate. A semantically correct result is a result that would hold up to external scrutiny, using the same techniques by which the training data was labeled. In a spam filter, a semantically correct result is a guess that the algorithm makes that a human would agree with.</p>
<p>The ability to generate semantically correct results is enabled by the prelabeled training data. The training data itself represents the semantics of the problem, and is how the algorithm learns to generate its semantically correct results. Notice that this entire discussion—and the entire discussion of accuracy, precision, and recall—hinges on the ability to introduce externally validated information to the model. You can only know if an individual guess is correct if an external entity independently validates the result, and you can only teach an algorithm to make semantically correct guesses if an external entity has provided enough data points with their correct labels to train the algorithm on. You can think of the training data for a supervised learning algorithm as the source of truth from which all guesses originate.</p>
<p>While supervised learning algorithms may indeed seem like magic when they're working well, there are many potential pitfalls. Because the training data is of crucial importance to the algorithm, your results will only be as good as your training data and your training methods. Some noise in training data can often be tolerated, but if there is a source of systemic error in the training data, you will also have systemic errors in your results. These may be difficult to detect, since the validation of a model typically uses a subset of the training data that you set aside—the same data that has the systemic error is used to validate the model, so you will think that the model is running well!</p>
<p>Another potential pitfall is not having enough training data. If the problem you're solving is highly-dimensional, you will need a correspondingly large amount of training data; the training data must be sufficient to actually present all of the various patterns to the machine learning algorithm. You wouldn't expect to train a spam filter on only 10 emails and also expect great results.</p>
<p>These factors often present a sort of startup cost to supervised learning. Some amount of investment needs to be made in procuring or generating an appropriate amount of and in the distribution of training examples. The training data typically, though not always, needs to be generated by human knowledge and evaluation. This can be costly, especially in the case of image processing and object detection, which generally need many labeled training examples. In a world where ML algorithms are becoming ever more accessible, the true competition lies in having the best data to work with.</p>
<p>In the case of our spam filter, the need for training data means that you cannot simply write and launch the spam filter. You'll also need to spend some time manually recording which emails are spam and ham (or have your users report this). Before deploying your spam filter, you should make sure that you have enough training data to both train and validate the algorithm with, and that could mean having to wait until you have hundreds or thousands of examples of spam messages flagged by a human.</p>
<p>Assuming you have an appropriate amount of high-quality training data, it's also possible to mismanage the training process and cause bad results with good data. ML novices often believe that more training is categorically better, but this is not the case.</p>
<p>There are two new concepts to introduce at this point: <strong>bias</strong> and <strong>variance</strong>. When training a ML model, your hope is that the model will learn the <em>general</em> attributes of the training data and be able to extrapolate from there. If an algorithm has made significant incorrect assumptions about the structure of the data, it can be said to be highly biased and therefore <em>underfitted</em>. On the other hand, a model can demonstrate high variance, or a high sensitivity to small differences in training data. This is called <strong>overfitting</strong>, and can be thought of as the algorithm learning to identify individual examples, or the specific noise in individual examples, rather than the general trend of the data.</p>
<p>Overtraining models can easily lead to overfitting. Imagine that you use the same keyboard every day for 10 years, but the keyboard is actually a strange model with an odd layout and lots of quirks. It's to be expected that you'd become very good at typing on such a keyboard after so much time. Then, unexpectedly, the keyboard breaks and you get a new standard keyboard only to find that you have no idea how to type on it! The muscle memory that you've trained over a decade of typing is used to the period key being <em>just so</em>, the letter <em>o</em> being shifted off a little further to the right, and so on. When using the new keyboard, you find that you can't type a single word without a typo. A decade of overtraining on a bad keyboard has only taught you how to type <em>on that keyboard</em>, and you haven't been able to generalize your skill to other keyboards. Overfitting a model is the same concept: your algorithm gets very good at identifying your training data <em>and nothing else.</em></p>
<p>For this reason, training a model is not as simple as plugging in training data and letting the algorithm train for an arbitrary amount of time. One crucial step in the process is to divide your training data into two parts: one set for training the algorithm, and another part used <em>only</em> to validate the results of your model. You should not train the algorithm on your validation data, because you run the risk of training the model on how to identify your validation data, rather than training it and then using the validation data to independently verify the accuracy of your algorithm. The need for a validation set increases the cost of generating training data. If you determine that you need 1,000 examples to train your algorithm on, you may actually need to generate 1,500 examples in total in order to have a reasonable validation set.</p>
<p>Validation data is not just used to test the overall accuracy of the algorithm. You also often use validation data to determine when to <em>stop</em> training. During the training process, you should periodically test the algorithm with your validation data. Over time you will find that the accuracy of the validation will increase, as expected, and then at a certain point the validation accuracy may actually <em>decrease.</em> This change in direction is the point at which your model has begun overfitting your training data. The algorithm will always continue to get more accurate when you present it with an example from your training set (those are the examples it's learning directly), but once the model begins to overfit the training data, it'll begin to lose the ability to generalize and therefore perform worse—not better—with data it has not been trained on. For this reason, maintaining an independent set of validation data is crucial. If you ever train an algorithm and it has 100% accuracy when testing its own training data, the odds are you've overfitted the data and it will likely perform very poorly on unseen data. The algorithm has gone past learning the general trends in the data and is starting to memorize specific examples, including the various bits of noise in the data.</p>
<p>Aside from maintaining a validation set, proper preprocessing of your data will also combat overfitting. The various noise reduction, feature selection, feature extraction, and dimensionality reduction techniques we discussed in <a href="94c3773e-b3a5-4c82-a542-80dd5cf5c094.xhtml" target="_blank">Chapter 2</a><em>, Data Exploration</em>, will all serve to help generalize your model and avoid overfitting.</p>
<p>Finally, because the semantic correctness of your algorithm's inferences can only be determined by an external source, it's often impossible to know whether a guess is actually correct (unless you receive user feedback on a specific guess). At best, you can only infer from the precision, recall, and accuracy values you've calculated during your training and validation stage what the overall effectiveness of the algorithm is. Fortunately, many supervised learning algorithms present their results in a probabilistic manner (for example, <em>I think there's a 92% chance this is spam</em>), so you can have some indication of the algorithm's confidence in an inference, however, when you combine this confidence level with the precision and recall of the model and the fact that your training data may have systemic errors, even the confidence level that comes with an inference is questionable.</p>
<p>Despite these potential pitfalls, supervised learning is a very powerful technique. The ability to extrapolate from only a few thousand training examples in a complex problem domain and quickly make inferences on millions of unseen data points is both impressive and highly valuable.</p>
<p>As with unsupervised learning, there are many types of supervised learning algorithms, each with their own strengths and weaknesses. Neural networks, Bayesian classifiers, k-nearest neighbor, decision trees, and random forests are all examples of supervised learning techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p>While supervised and unsupervised learning are the two primary subclassifications of machine learning algorithms, they are in fact part of a spectrum and there are other modes of learning. The next most significant learning mode in the context of this book is reinforcement learning, which in some ways can be considered a hybrid of supervised and unsupervised learning; however, most would categorize reinforcement learning as an unsupervised learning algorithm. This is one of those cases where the taxonomy becomes a little vague!</p>
<p>In unsupervised learning, almost nothing is known about the data to be processed and the algorithm must infer patterns from a blank slate. In supervised learning, significant resources are dedicated to training the algorithm on known examples. In reinforcement learning, <em>something</em> is known (or can be known) about the data, but the knowledge of the data is not an explicit labeling or a categorization. Instead, the <em>something</em> that is known (or can be known) is the result of an action based on a decision made with the data. Reinforcement learning is considered by many to be an unsupervised learning algorithm, because the algorithm <em>starts from scratch</em>, however reinforcement also <em>closes the loop</em> and continually retrains itself based on its own actions, which has some similarities to training in supervised learning.</p>
<p>To use an absurd and contrived example, imagine that you're writing an algorithm that is supposed to replace the function of government. The algorithm will receive as its input the current state of affairs of the country and must, as an output, develop new policies and laws in order to optimize the country in many dimensions: citizen happiness, economic health, low crime, and so on. The reinforcement learning approach to this problem starts from scratch, knowing nothing about how its laws and policies will affect the country. The algorithm then implements a law or set of laws; because it has just started, the law it implements will be completely arbitrary. After the law has taken some time to go into effect and make its impact on society, the algorithm will once again read the state of affairs of the country and may discover that it has turned the country into a chaotic wasteland. The algorithm learns from this feedback, adjusts itself, and implements a new set of laws. Over time, and using the initial laws it implements as experiments, the algorithm will come to understand the cause and effect of its policies and begin to optimize. Given enough time, this approach may develop a near-perfect society—if it doesn't accidentally destroy the society with its initial failed experiments.</p>
<p>Reinforcement learning techniques are distinct from supervised and unsupervised algorithms in that they directly interact with their environment and monitor the effects of their decisions in order to update their models. Rather than aiming to detect patterns or to classify data, most reinforcement learning aims to optimize some cost or reward within an environment. The environment in question can either be a real-world environment, as is often the case in the field of control systems, or it can be a virtual environment, as is the case with genetic algorithms. In either case, the algorithm must have some way of characterizing an overall <em>cost</em>/<em>penalty</em> or <em>reward</em>, and will work to optimize that value. Reinforcement learning is an important optimization technique, especially in highly dimensional problem spaces, since a brute-force trial-and-error approach is often impossible to achieve in a reasonable amount of time.</p>
<p>Examples of reinforcement learning algorithms include genetic algorithms, which we will discuss in depth in a later chapter, Monte Carlo methods, and gradient descent (which we will discuss alongside neural networks).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categories of algorithms</h1>
                </header>
            
            <article>
                
<p>We've categorized ML algorithms by their learning mode, but that's not the only way to categorize algorithms. Another approach is to categorize them by their task or function. In this section we will briefly present the basic functions of ML algorithms and name some example algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p>Clustering algorithms aim to identify groups of data points that are similar to one another. The definition of <em>similar</em> depends on the type of data, the problem domain, and the algorithm used. The easiest way to intuitively understand clustering algorithms is to visualize points on an <em>x/y</em> grid. A clustering algorithm's aim is typically to draw circles around groups of similar points; each set of circled points is taken to be a cluster. The clusters are generally not known beforehand, so clustering algorithms are generally classified as unsupervised learning problems.</p>
<p>Some examples of clustering algorithms include:</p>
<ul>
<li>k-means, and variants such as k-medians</li>
<li>Gaussian mixture models</li>
<li>Mean-shift</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification</h1>
                </header>
            
            <article>
                
<p>Classification is a very broad (and very popular) category of supervised learning algorithms, with the goal of trying to identify a data point as belonging to some classification (spam or ham; male or female; animal, mineral or vegetable, and so on). A multitude of algorithms for classification exists, including:</p>
<ul>
<li>k-nearest neighbor</li>
<li>Logistic regression</li>
<li>Naive Bayes classifier</li>
<li>Support Vector Machines</li>
<li>(Most) neural networks</li>
<li>Decision trees</li>
<li>Random forests</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression</h1>
                </header>
            
            <article>
                
<p>Regression algorithms aim to determine and characterize the relationship between variables. In the most simple case of two-dimensional linear regression, the algorithm's goal is to determine the line that can be drawn most closely through a set of points, however, higher-degree and higher-dimensional regressions can generate significant insights and make predictions concerning complex data. Because these algorithms necessarily require known data points, they are considered to be supervised learning algorithms. Some examples:</p>
<ul>
<li>Linear regression</li>
<li>Polynomial regression</li>
<li>Bayesian linear regression</li>
<li>Least absolute deviation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality reduction</h1>
                </header>
            
            <article>
                
<p>Dimensionality reduction is a family of techniques whose purpose is to convert data with a high number of dimensions into data with a lower number of dimensions. Used as a general term, this can mean either discarding dimensions entirely (such as feature selection), or to create new individual dimensions that simultaneously represent multiple original dimensions, with some loss of resolution (<span><span>such as </span></span>feature extraction).</p>
<p>Some algorithms that can be used for dimensionality reduction include:</p>
<ul>
<li>Various types of regressions</li>
<li>PCA</li>
<li>Image transformations (for example, converting an image to grayscale)</li>
<li>Stemming and lemmatization (in natural language processing)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimization</h1>
                </header>
            
            <article>
                
<p>Optimization algorithms have the goal of selecting a set of parameters, or the values for a set of parameters, such that the cost or error of a system is minimized (alternatively, such that the reward of a system is maximized). Feature selection and feature extraction is actually a form of optimization; you are modifying parameters with the purpose of reducing dimensionality while preserving important data. In the most basic optimization technique, a brute-force search, you simply try every possible combination of parameters and select the combination with the best results. In practice, most problems are complex enough that a brute-force search may take an unreasonable amount of time (that is, millions of years on a modern computer). Some optimization techniques include:</p>
<ul>
<li>A brute force search (also known as an <em>exhaustive search</em>)</li>
<li>Gradient descent</li>
<li>Simulated annealing</li>
<li>Genetic algorithms</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Natural language processing</h1>
                </header>
            
            <article>
                
<p><strong>Natural language processing</strong> (<strong>NLP</strong>) is an entire field on its own and contains many techniques that are not considered in machine learning. However, NLP is often used in concert with ML algorithms, as the two fields combined are necessary to achieve generalized artificial intelligence. Many ML classification algorithms operate on text rather than numbers (such as our spam filter), and in those situations, we rely on techniques from the field of NLP: stemming, in particular, is a quick and easy dimensionality reduction technique for text classifiers. Some NLP techniques relevant to ML include:</p>
<ul>
<li>Tokenization</li>
<li>String distance</li>
<li>Stemming or lemmatization</li>
<li>TF-IDF</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image processing</h1>
                </header>
            
            <article>
                
<p>Like NLP, image processing is its own field of study that has overlapped with ML but is not fully encompassed by ML. As with NLP, we may often use image processing techniques to reduce dimensionality before applying an ML algorithm to an image. Some image processing techniques relevant to machine learning include:</p>
<ul>
<li>Edge detection</li>
<li>Scale invariant transformations</li>
<li>Color space transformations</li>
<li>Object detection</li>
<li>Recurrent neural networks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've discussed the various ways we can categorize machine learning techniques. In particular, we discussed the difference between unsupervised learning, supervised learning, and reinforcement learning, presenting various examples of each.</p>
<p>We also discussed different ways to judge the accuracy of machine learning algorithms, in particular, the concepts of accuracy, precision, and recall as applied to supervised learning techniques. We also discussed the importance of the training step in supervised learning algorithms, and illustrated the concepts of bias, variance, generalization, and overfitting.</p>
<p>Finally, we looked at how machine learning algorithms can be categorized not by learning mode but instead by task or technique, and presented a number of algorithms that fit into the categories of clustering, classification, regression, dimensionality reduction, natural language processing, and image processing.</p>
<p>In the next chapter, we'll get our hands dirty and take a deep dive into clustering algorithms.</p>


            </article>

            
        </section>
    </body></html>