- en: '*Chapter 11*: Mapping Our Environments'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few fundamental things that your self-driving car needs to navigate
    the world.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, you need to have a map of your environment. This map is very similar
    to the map you use on your phone for getting to your favorite restaurant.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, you need a way to localize your position on that map in the real world.
    On your phone, this is the blue dot localized by the GPS.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about various ways for your self-driving car
    to map and localize through its environment, so it knows where it is in the world.
    You can imagine why this is important since the entire reason for making a self-driving
    car is to go places!
  prefs: []
  type: TYPE_NORMAL
- en: 'You will learn the following topics to help you build a self-driving car worthy
    of being called Magellan:'
  prefs: []
  type: TYPE_NORMAL
- en: Why you need maps and localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of mapping and localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source mapping tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SLAM with an Ouster lidar and Google Cartographer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The chapter requires the following software:'
  prefs: []
  type: TYPE_NORMAL
- en: Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ROS Melodic: [http://wiki.ros.org/melodic/Installation/Ubuntu](http://wiki.ros.org/melodic/Installation/Ubuntu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Python 3.7: [https://www.python.org/downloads/release/python-370/](https://www.python.org/downloads/release/python-370/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google Cartographer ROS: [https://github.com/cartographer-project/cartographer_ros](https://github.com/cartographer-project/cartographer_ros)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ouster_example_cartographer`: [https://github.com/Krishtof-Korda/ouster_example_cartographer](https://github.com/Krishtof-Korda/ouster_example_cartographer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for the chapter can be found at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code in Action videos for the chapter can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/2IVkJVZ](https://bit.ly/2IVkJVZ)'
  prefs: []
  type: TYPE_NORMAL
- en: Why you need maps and localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn the importance of maps and localization, and
    the combination of them. Maps and localization are things we often take for granted
    in the modern world, but as you will see, they are very important, especially
    for self-driving cars, where the amazing human brain is not utilized.
  prefs: []
  type: TYPE_NORMAL
- en: Maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Take a moment and imagine a world without cell phones, without MapQuest (yup,
    I'm an elder millennial), without paper maps, and without Anaximander of Greece!
  prefs: []
  type: TYPE_NORMAL
- en: How well do you think you could navigate from your home to a city you have never
    been to, let alone the new Trader Joe's that just opened a few cities away? I
    am sure you could do it, but you would probably stop every few kilometers and
    ask a local for the next few directions to get you closer to that bold and earthy
    Two Buck Chuck. But you can see why maps really make our lives easier and open
    up possibilities to venture to new places with little fear of getting lost and
    ending up at Walley World.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you are very fortunate that companies such as Google and Apple have painstakingly
    mapped every street, alleyway, and side street you can think of. That is a huge
    task and we benefit from it every day. Hooray maps!
  prefs: []
  type: TYPE_NORMAL
- en: Localization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Okay, now imagine that you are teleported here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Monkey Face in Russia. Image source: http://bit.ly/6547672-17351141](img/Figure_11.1_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1 – Monkey Face in Russia. Image source: [http://bit.ly/6547672-17351141](http://bit.ly/6547672-17351141)'
  prefs: []
  type: TYPE_NORMAL
- en: You have been given a map of the area and need to find your way to the nearest
    body of water. The first thing you need to do – after you stop shaking from being
    teleported – is figure out where in the world you are on the map you have. You
    would likely look around you to pick out landmarks nearby, and then try to find
    those landmarks on your map. *Monkey Face, I'm right in the middle of the Monkey
    Face!* Congratulations, you just localized yourself in the map and can use it
    to find the elixir of life!
  prefs: []
  type: TYPE_NORMAL
- en: Now you see why both a map and localization are necessary for navigating the
    world and environments.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you say, *But wait, what if something has changed in the world since the
    map was generated?!*
  prefs: []
  type: TYPE_NORMAL
- en: Surely you have been driving along, following the sweet voice of navigation
    from your phone when *wham!* You just rolled up to some road construction that
    has closed the road and is forcing you to take a 30-minute detour. You blurt out
    to your phone, *Curse you, nav voice from oblivion! How did you not know that
    there was construction?*
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that your dear navigator voice, no matter how up to date, will
    always miss real-time information about the world. Just imagine some ducks crossing
    the road; the voice will never warn you about that. In the next section, you will
    learn the many ways to save the ducks with various types of mapping and localization.
  prefs: []
  type: TYPE_NORMAL
- en: Types of mapping and localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of localization and mapping is absolutely full of amazing research
    and is continually growing. The advancement of GPUs and computer processing speeds
    has led to the development of some very exciting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Quickly, let's get back to saving our ducks! Recall in the previous section
    that our dear sat-nav voice did not see the ducks crossing the road in front of
    us. A map will never be completely accurate since the world is ever-changing and
    morphing. Therefore, we must have a way to not only localize using a pre-built
    map but also build a map in real time so that we can see when new obstacles appear
    in our map and navigate around them. Introducing SLAM for the ducks (not dunks).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there are independent methods for mapping and localization, in this
    chapter, we will focus on **Simultaneous Localization and Mapping** (**SLAM**).
    If you are curious, though, the following is a quick breakdown of the most commonly
    used algorithms for independent localization and mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: Particle filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grid localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extended Kalman filters for range-bearing localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalman filters for dead reckoning (odometry)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can read more about localization here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://www.cs.cmu.edu/~motionplanning/lecture/Chap8-Kalman-Mapping_howie.pdf](https://www.cs.cmu.edu/~motionplanning/lecture/Chap8-Kalman-Mapping_howie.pdf)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[http://robots.stanford.edu/papers/thrun.pf-in-robotics-uai02.pdf](http://robots.stanford.edu/papers/thrun.pf-in-robotics-uai02.pdf)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1999_3/fox_dieter_1999_3.pdf](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1999_3/fox_dieter_1999_3.pdf)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Some example types of mapping are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Occupancy grid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature-based (landmark)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topological (graph-based)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual teach and repeat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To read more about mapping, refer to the following links:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://www.cs.cmu.edu/~motionplanning/lecture/Chap8-Kalman-Mapping_howie.pdf](https://www.cs.cmu.edu/~motionplanning/lecture/Chap8-Kalman-Mapping_howie.pdf)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1996_8/thrun_sebastian_1996_8.pdf](https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1996_8/thrun_sebastian_1996_8.pdf)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There is a lot of great information on these algorithms and implementations,
    but for this book, we will focus on the most widely used form of localization
    and mapping, the simultaneous kind: SLAM.'
  prefs: []
  type: TYPE_NORMAL
- en: Simultaneous localization and mapping (SLAM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's jump back into our imagination for a moment.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you suddenly woke up one night and there was absolutely no light, no
    moon, no glow worms – just pitch black! Fear not, you will use the magic of SLAM
    to navigate from your bed to get that tasty midnight snack!
  prefs: []
  type: TYPE_NORMAL
- en: You fumble your left hand around until you feel the edge of your bed. Boom,
    you just localized yourself on the bed and have mapped the left edge of the bed
    in your mind. You make the assumption that you didn't flip vertically in bed while
    sleeping, so this really is the left side of your bed.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you swing your legs over the edge of the bed, slowly lowering yourself
    until you feel the floor. Blam, you just mapped a portion of your floor. Now,
    you carefully stand up and put your arms out in front of you. You sway your arms
    in a Lissajous curve in front of you, like Helen Keller searching for a spider's
    web. Simultaneously, you carefully sweep your feet across the floor, like a modern
    interpretive dancer looking for any steps, transitions, edges, and pitfalls, so
    you don't trip.
  prefs: []
  type: TYPE_NORMAL
- en: Each time you move forward, you carefully keep track in your mind which direction
    you are facing and how far you have stepped (**odometry**). All the time, you
    are building a mental map of the room and using your hands and feet as range sensors,
    giving you a sense of where you are in the room (**localizing**). Each time you
    find an obstacle, you store that in your mental map and navigate gingerly around
    it. You are SLAMing!
  prefs: []
  type: TYPE_NORMAL
- en: 'SLAM typically uses some kind of range-finding sensor, such as a lidar sensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – OS1-128 digital lidar sensor, courtesy of Ouster, Inc.](img/Figure_11.2_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – OS1-128 digital lidar sensor, courtesy of Ouster, Inc.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you were navigating your room, your arms and legs were acting as your
    range finders. Lidar sensors use laser light, which illuminates the environment
    and bounces off objects. The time of flight between the light leaving and returning
    is used to estimate the range to the object using the speed of light. Lidar sensors,
    such as the OS1-128, produce rich and dense point clouds with highly accurate
    distance information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Lidar point cloud in an urban setting, courtesy of Ouster,
    Inc.](img/Figure_11.3_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Lidar point cloud in an urban setting, courtesy of Ouster, Inc.
  prefs: []
  type: TYPE_NORMAL
- en: This distance information is what SLAM algorithms use to localize and map the
    world.
  prefs: []
  type: TYPE_NORMAL
- en: An **Inertial Measurement Unit** (**IMU**) is also needed to help estimate the
    pose of the vehicle and estimate the distance traveled between successive measurements.
    One reason that Ouster lidar sensors are popular for map creation is that they
    come with an IMU built in, which lets you start mapping with a single device.
    Later in the chapter, you will learn how to map with an Ouster lidar sensor and
    Google Cartographer.
  prefs: []
  type: TYPE_NORMAL
- en: 'SLAM is the concept of building a map on the fly with no *a priori* information
    and simultaneously localizing in the map as it is being built. You can imagine
    that this is very difficult and is a bit of a *chicken or egg* problem. To localize,
    you need a map (the egg) to localize from, but at the same time, in order to build
    your map on the fly, you need to localize (the chicken) and know where you are
    on the map you are trying to build. This is like a problem from a time travel
    movie: surviving to live long enough to go back in time to save yourself in the
    first place. Does your head hurt yet?'
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that this field has been studied for over 30 years and has
    borne beautiful fruit in the form of algorithms for robotics and self-driving
    cars. Let's see what lies ahead!
  prefs: []
  type: TYPE_NORMAL
- en: Types of SLAM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following is a brief list of some state-of-the-art algorithms used throughout
    robotics, drone mapping, and self-driving industries. Each of these algorithms
    has different applications. RGB-D SLAM, for example, is used for camera-based
    SLAM, while LIO SAM is specific to lidar sensors. Kinetic fusion is another interesting
    form of SLAM used to map complex objects indoors. A more complete list can be
    found on the KITTI website at [http://www.cvlibs.net/datasets/kitti/eval_odometry.php](http://www.cvlibs.net/datasets/kitti/eval_odometry.php):'
  prefs: []
  type: TYPE_NORMAL
- en: '**LIO SAM**: [https://arxiv.org/pdf/2007.00258.pdf](https://arxiv.org/pdf/2007.00258.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LOAM**: [https://ri.cmu.edu/pub_files/2014/7/Ji_LidarMapping_RSS2014_v8.pdf](https://ri.cmu.edu/pub_files/2014/7/Ji_LidarMapping_RSS2014_v8.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RGB-D SLAM**: [https://felixendres.github.io/rgbdslam_v2/](https://felixendres.github.io/rgbdslam_v2/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kinetic fusion**: [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will learn about a very important method of reducing error in a SLAM
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Loop closure in SLAM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One thing to consider with mapping and localization is that nothing is perfect.
    You will never find a sensor that is perfectly accurate. All sensors are probabilistic
    with some distribution containing a mean and variance of a measurement. These
    are determined empirically at the factory during the calibration process and then
    provided in the datasheet. You may ask, *Why do I care?*
  prefs: []
  type: TYPE_NORMAL
- en: Good question! The fact that the sensors always have some error means the longer
    you navigate using these sensors, the more your map and the estimation of your
    position within that map will drift from reality.
  prefs: []
  type: TYPE_NORMAL
- en: 'SLAM algorithms almost universally have a trick up their sleeve to combat this
    drift: **loop closure**! Loop closure works like this. Let''s say you pass by
    the Aldar building on your trip to Abu Dhabi:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – The Aldar headquarters building, Abu Dhabi, UAE](img/Figure_11.4_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – The Aldar headquarters building, Abu Dhabi, UAE
  prefs: []
  type: TYPE_NORMAL
- en: You register this magnificent circular building into your map and continue on
    your way. Then, sometime later, perhaps after you grabbed lunch at Li Beirut,
    you drive back, passing the Aldar building a second time. Now when you pass it,
    you measure your distance from it and compare that to where you think you are
    relative to when you first registered it on your map. You realize that you are
    not where you expect to be relative to it. Snap! The algorithm takes this information
    and iteratively corrects the entire map to represent where you really are in the
    world.
  prefs: []
  type: TYPE_NORMAL
- en: SLAM is constantly doing this with every feature it maps and returns to later.
    You will see this in action when you play with the open source SLAM in the next
    few sections. Before that, let's quickly show you some of the available open source
    mapping tools available to you for your mapping pleasure.
  prefs: []
  type: TYPE_NORMAL
- en: Open source mapping tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SLAM is quite complicated to implement and understand, but fortunately, there
    are plenty of open source solutions that you can use in your self-driving car.
    The website *Awesome Open Source* ([https://awesomeopensource.com/projects/slam](https://awesomeopensource.com/projects/slam))
    has a treasure trove of SLAM algorithms that you can use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a curated selection to whet your appetite:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cartographer by Google** ([https://github.com/cartographer-project/cartographer](https://github.com/cartographer-project/cartographer))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LIO-SAM by TixiaoShan** ([https://github.com/TixiaoShan/LIO-SAM](https://github.com/TixiaoShan/LIO-SAM))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LeGO-LOAM by RobustFieldAutonomy** ([https://github.com/RobustFieldAutonomyLab/LeGO-LOAM](https://github.com/RobustFieldAutonomyLab/LeGO-LOAM))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since Cartographer is by far the most popular and supported, you will get to
    play with and experience all it has to offer in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: SLAM with an Ouster lidar and Google Cartographer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the moment you have been waiting for: building maps with hands-on experience
    using Cartographer and an Ouster lidar sensor!'
  prefs: []
  type: TYPE_NORMAL
- en: An Ouster lidar was chosen for this hands-on example because it has a built-in
    **IMU**, which is needed to perform SLAM. This means that you don't need to purchase
    another sensor to provide the inertial data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example you will see is the offline processing of data collected from an
    Ouster sensor and is adapted from the work of Wil Selby. Please visit Wil Selby''s
    website home page for more cool projects and ideas: [https://www.wilselby.com/](https://www.wilselby.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Selby also has a related project that performs the SLAM online (in real time)
    for a DIY driverless car in ROS: [https://github.com/wilselby/diy_driverless_car_ROS](https://github.com/wilselby/diy_driverless_car_ROS).'
  prefs: []
  type: TYPE_NORMAL
- en: Ouster sensor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn more about the Ouster data format and usage of the sensor from
    the OS1 user guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/blob/master/Chapter11/OS1-User-Guide-v1.14.0-beta.12.pdf](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/blob/master/Chapter11/OS1-User-Guide-v1.14.0-beta.12.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Don't worry, you don't need to buy a sensor to get your hands dirty in this
    chapter. We have provided you with some sample data collected from an OS1-128
    for you to use. You will see later how to download the data.
  prefs: []
  type: TYPE_NORMAL
- en: The repo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will find the code for this chapter in the `ouster_example_cartographer`
    submodule at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter11](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter11)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that you have the latest code in the submodule, you can run the following
    command from within the `Chapter11` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Getting started with cartographer_ros
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we dive into the code, you are encouraged to learn the basics of Cartographer
    by reading the algorithm walkthrough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://google-cartographer-ros.readthedocs.io/en/latest/algo_walkthrough.html](https://google-cartographer-ros.readthedocs.io/en/latest/algo_walkthrough.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin with a quick overview of the Cartographer configuration files needed
    to make it work using your sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Cartographer_ros configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cartographer needs the following configuration files to understand about your
    sensor, robot, transforms, and so on. The files can be found in the `ouster_example_cartographer/cartographer_ros/`
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`configuration_files/demo_3d.rviz`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`configuration_files/cart_3d.lua`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`urdf/os_sensor.urdf`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`launch/offline_cart_3d.launch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`configuration_files/assets_writer_cart_3d.lua`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`configuration_files/transform.lua`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The files referenced here are for performing offline SLAM on a bag collected
    from an Ouster sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's step through each file and explain how it contributes to making SLAM
    work inside ROS.
  prefs: []
  type: TYPE_NORMAL
- en: demo_3d.rviz
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This file sets the configuration of the `rviz` GUI window. It''s based on the
    example file provided in the `cartographer_ros` source files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/cartographer-project/cartographer_ros/blob/master/cartographer_ros/configuration_files/demo_3d.rviz](https://github.com/cartographer-project/cartographer_ros/blob/master/cartographer_ros/configuration_files/demo_3d.rviz)'
  prefs: []
  type: TYPE_NORMAL
- en: 'It specifies the frames of reference. The details of the various reference
    frames are available at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.ros.org/reps/rep-0105.html](https://www.ros.org/reps/rep-0105.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet is where you will add your frame names based on
    the sensor you are using for your project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the definitions of each frame from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`base_link` is the coordinate frame of your robot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`map` is the fixed coordinate frame of the world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`odom` is a world-fixed frame that is computed based on odometry from the IMU,
    wheel encoders, visual odometry, and so on. This can drift over time, but can
    be useful in maintaining continuous smooth position information without discrete
    jumps. Cartographer uses this frame to publish non-loop-closing local SLAM results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`os` is the coordinate frame of the Ouster sensor or any other lidar sensor
    you have chosen for your project. This is used to transform lidar range readings
    to the `base_link` frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`os_imu` is the coordinate frame of the IMU in the Ouster sensor or any other
    IMU you have chosen for your project. This is the frame that Cartographer will
    track during SLAM. It will also be transformed back to the `base_link` frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, the hierarchy `tf` transform tree of frames is defined so that you can
    transform between any of the frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the `os` and `os_imu` frames are both related to `base_link`
    (the vehicle frame). This means you cannot directly transform from `os` (the lidar
    frame) to `os_imu` (the IMU frame). Instead, you would transform both into the
    `base_link` frame. From there, you can transform up the `tf` tree all the way
    to the map frame. This is what Cartographer will do when building the map using
    the lidar range measurements and IMU pose measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Next, `RobotModel` is configured to display the links (meaning sensors, arms,
    or anything that has a coordinate frame on the robot that you want to track) in
    their correct pose according to the `tf` transform tree previously defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows you where to put your link names previously
    defined in the `Frames` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can see `base_link`, the `os` lidar, and `os_imu` links are added here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, `rviz/PointCloud2` is mapped to the topic for the `PointCloud2` lidar
    points data, which for an Ouster lidar sensor bag file is stored in the `/os_cloud_node/points`
    topic. If you are using any other lidar sensor, you would place that lidar''s
    topic name in the `Topic:` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the topic from the lidar is mapped as a `PointCloud2` type.
  prefs: []
  type: TYPE_NORMAL
- en: That wraps up the specific configurations for the lidar and IMU sensors in `rviz`.
    Next, you will see how the `cart_3d.lua` file is modified to match your robot-specific
    layout.
  prefs: []
  type: TYPE_NORMAL
- en: cart_3d.lua
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This file sets the configuration of the robot SLAM tuning parameters. A `.lua`
    file should be robot-specific, rather than bag-specific. It is based on the example
    file provided in the `cartographer_ros` source files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/cartographer-project/cartographer_ros/blob/master/cartographer_ros/configuration_files/backpack_3d.lua](https://github.com/cartographer-project/cartographer_ros/blob/master/cartographer_ros/configuration_files/backpack_3d.lua)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are encouraged to tune the parameters in the `.lua` file based on your
    specific application. A guide for tuning is available at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://google-cartographer-ros.readthedocs.io/en/latest/algo_walkthrough.html](https://google-cartographer-ros.readthedocs.io/en/latest/algo_walkthrough.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will touch quickly on some options that you can configure for your
    self-driving car:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding options are configured for offline SLAM from the bag file provided
    on the Ouster website at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://data.ouster.io/downloads/os1_townhomes_cartographer.zip](https://data.ouster.io/downloads/os1_townhomes_cartographer.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://data.ouster.io/downloads/os1_townhomes_cartographer.zip](https://data.ouster.io/downloads/os1_townhomes_cartographer.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to modify the highlighted ones if you are doing online (real-time)
    SLAM on your self-driving car:'
  prefs: []
  type: TYPE_NORMAL
- en: '`odom_frame = "base_link"`: This should be set to `odom` so that Cartographer
    publishes the non-loop-closing continuous pose as `odom_frame`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`provide_odom_frame = false`: This should be set to `true` so that Cartographer
    knows that the `odom_frame` is published.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_laser_scans = 0`: This should be set to `1` so that the lidar sensor''s
    scan data is used straight from the sensor, rather than from point clouds from
    a bag file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_point_clouds = 1`: This should be set to `0` if not using a bag file and
    you are instead using a live lidar scan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will see how the sensor `urd` file is configured.
  prefs: []
  type: TYPE_NORMAL
- en: os_sensor.urdf
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This file is used to configure the physical transforms of your self-driving
    car. Each sensor you mount on the vehicle will be a link. Think of links as rigid
    bodies, like links in a chain. Each link is rigid in a chain, but the links can
    move relative to each other and each have their own coordinate frames.
  prefs: []
  type: TYPE_NORMAL
- en: In this file, you will see that we have set up the Ouster sensor as the robot,
    `<robot name="os_sensor">`.
  prefs: []
  type: TYPE_NORMAL
- en: We added links that represent the lidar coordinate frame, `<link name="os_lidar">`,
    and the IMU coordinate frame, `<link name="os_imu">`, of the sensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how we provide the transforms from each frame back
    to the `base_link` frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that `os_sensor` is placed at the center of the `base_link` coordinate
    frame, while `os_imu` and `os_lidar` are given their respective translations and
    rotations relative to `os_sensor`. These translations and rotations are provided
    in the Ouster sensor user guide under *Section 8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Krishtof-Korda/ouster_example_cartographer/blob/master/OS1-User-Guide-v1.14.0-beta.12.pdf](https://github.com/Krishtof-Korda/ouster_example_cartographer/blob/master/OS1-User-Guide-v1.14.0-beta.12.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn how the launch file is configured to call all the previous
    configuration files and launch the SLAM process.
  prefs: []
  type: TYPE_NORMAL
- en: offline_cart_3d.launch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This file is used to call all the configuration files previously discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also remaps the `points2` and `imu` topics to the bag file Ouster `os_cloud_node`
    topics. If you are using another type of lidar sensor, simply use the topic name
    of that sensor in place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, you will learn how the `assets_writer_cart_3d.lua` file is used to save
    the map data.
  prefs: []
  type: TYPE_NORMAL
- en: assets_writer_cart_3d.lua
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This file is used to configure the options for generating the fully aggregated
    point cloud that will be output in `.ply` format.
  prefs: []
  type: TYPE_NORMAL
- en: You can set the `VOXEL_SIZE` value that should be used to downsample the points
    and only take the centroid. This is important since without down sampling, you
    would need tremendous processing cycles.
  prefs: []
  type: TYPE_NORMAL
- en: VOXEL_SIZE = 5e-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You also set `min_max_range_filter`, which only keeps points that are within
    a specified range from the lidar sensor. This is usually based on the specs in
    the datasheet of the lidar sensor. The Ouster OS1 datasheet can be found on the
    Ouster ([https://outser.com/](https://outser.com/)) website.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows where you can configure the range filter options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you will learn how the `transform.lua` file is used to do 2D projections.
  prefs: []
  type: TYPE_NORMAL
- en: The transform.lua file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This file is a generic file for performing transforms and is used in the previous
    file to create the 2D map x-ray and probability grid images.
  prefs: []
  type: TYPE_NORMAL
- en: Fantastic, now that you understand what each configuration file does, it's time
    for you to see it in action! The next section will guide you through running SLAM
    using a prebuilt Docker image. This will hopefully get you SLAMing quicker than
    you can say *The cars of the future will drive us!*
  prefs: []
  type: TYPE_NORMAL
- en: Docker image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Docker image has been created for you to download. This will help to ensure
    that all the required packages are installed and minimizes the time you need to
    get everything working.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are running on a Linux operating system, you can simply run [install-docker.sh](http://install-docker.sh),
    located in the `ouster_example_cartographer` submodule, with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are on another operating system (Windows 10 or macOS), you can download
    and install Docker directly from their website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify that Docker was installed correctly with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Great! Hopefully, things have gone smoothly, and you are ready to run the Docker
    image in a container. It is highly recommended to use a Linux machine with an
    Nvidia graphics card in order to make the code and Docker image work. The `run-docker.sh`
    script is provided to help start Docker with the correct options for your graphics
    processor. It is highly recommended to use a Nvidia GPU to process the SLAM efficiently.
    You can use other GPUs but the support for them is low.
  prefs: []
  type: TYPE_NORMAL
- en: The following section will provide you some troubleshooting steps for connecting
    Docker with your Nvidia GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Nvidia troubleshooting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Depending on the Nvidia setup on your Linux machine, you may need to perform
    the following commands before connecting to your Docker container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can run Docker and connect it to your GPU with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This script will pull the latest Docker image from Docker Hub and run the image
    either with the Nvidia runtime, if available, or simply on the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: This file also has many useful commands in the comments for running Cartographer
    in 2D or 3D mode. You will learn about 3D mode here.
  prefs: []
  type: TYPE_NORMAL
- en: The next few sections will walk you through the steps of performing SLAM on
    the data you will download from Ouster.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the sample data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sample data that you will be SLAMing is available from the Ouster website.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download it with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Sourcing the workspace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You will need to source the `catkin` workspace to ensure it is set up with
    ROS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Validating rosbag
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is a good idea to validate `rosbag` using the built-in cartographer bag
    validation tool. This will ensure that the bag has continuous data and will produce
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Preparing to launch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To run your offline SLAM on the bag, you first need to get to the launchpad:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Launching offline on the bag
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, you are ready to launch the offline SLAM. This will create a `.pbstream`
    file that will be used later to write your assets, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.ply`, the point cloud file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 2D x-ray image of the mapped space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 2D probability grid image of an open versus an occupied area
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following command will launch the offline SLAM process on your bag file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an `rviz` window open that looks something like that in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – The rviz window Cartographer launch](img/Figure_11.5_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – The rviz window Cartographer launch
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can sit back and watch in wonder as Cartographer meticulously performs
    SLAM.
  prefs: []
  type: TYPE_NORMAL
- en: First, it will make smaller local submaps. Then, it will scan match the submap
    to the global map. You will notice that it snaps the point cloud every few seconds
    when it has collected enough data to match the global map.
  prefs: []
  type: TYPE_NORMAL
- en: When the process is complete, you will have a file in the `/root/bags` folder
    named `os1_townhomes_cartographer.bag.pbstream`. You will use this file to write
    your assets.
  prefs: []
  type: TYPE_NORMAL
- en: Writing your sweet, sweet assets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I hope you are ready because you are about to get the final product from SLAM
    – a map of some random street you have never seen before. Just what you dreamed
    of, right?
  prefs: []
  type: TYPE_NORMAL
- en: Run the following command to collect your prize!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This will take a while; go grab a bite of your favorite comfort food. We will
    see you back here in an hour.
  prefs: []
  type: TYPE_NORMAL
- en: Welcome back! Feast your eyes on your prizes!
  prefs: []
  type: TYPE_NORMAL
- en: Opening your first prize
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Voila! Your very own x-ray 2D map!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the output appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – 2D x-ray map of townhomes](img/Figure_11.6_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – 2D x-ray map of townhomes
  prefs: []
  type: TYPE_NORMAL
- en: Opening your second prize
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shazam! Your very own probability grid 2D map!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the output appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – 2D probability grid map of townhomes](img/Figure_11.7_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – 2D probability grid map of townhomes
  prefs: []
  type: TYPE_NORMAL
- en: Your final prize
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You will find a file in the `/root/bags` folder that is named `os1_townhomes_cartographer.bag_points.ply`.
    This prize will take a little more effort to truly appreciate.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use any tool that is capable of opening a `.ply` file. CloudCompare
    is a **FOSS** (that is, a **free open source software**) tool for this and can
    be downloaded from the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.danielgm.net/cc/](https://www.danielgm.net/cc/)'
  prefs: []
  type: TYPE_NORMAL
- en: You can also use CloudCompare to save your `.ply` file into other formats, such
    as XYZ, XYZRGB, CGO, ASC, CATIA ASC, PLY, LAS, PTS, or PCD.
  prefs: []
  type: TYPE_NORMAL
- en: '`unitycoder` has good instructions for making the conversion available at the
    following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/unitycoder/UnityPointCloudViewer/wiki/Converting-Points-Clouds-with-CloudCompare](https://github.com/unitycoder/UnityPointCloudViewer/wiki/Converting-Points-Clouds-with-CloudCompare)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how the output appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Point cloud 3D map viewed in CloudCompare](img/Figure_11.8_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Point cloud 3D map viewed in CloudCompare
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at *Figure 11.8* and *Figure 11.9*, which show what the 3D map
    of points looks like using the CloudCompare viewer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Point cloud 3D map viewed in CloudCompare, top-view](img/Figure_11.9_B16322.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Point cloud 3D map viewed in CloudCompare, top-view
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on making your first of what we hope are many maps! This is
    just the beginning of your journey and we can't wait to see what you make with
    your newfound skills! Next, we will summarize everything you learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wow, you have come a long way in this chapter and book. You began with nothing
    but a mobile phone and a blue GPS dot. You traveled across the globe to Russia
    and found the life-juice at the Monkey Face. You grabbed some snacks by SLAMing
    your way through your Cimmerian dark home. You learned the difference between
    maps and localization, and the various types of each. You picked up some open
    source tools and lashed them to your adventure belt for future use.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how to apply the open source Cartographer on Ouster OS1-128
    lidar sensor data, coupled with the built-in IMU to generate dense and tangible
    maps of some really nice townhomes that you manipulated using CloudCompare. Now
    you know how to create maps and can go out and map your own spaces and localize
    within them! The world is your Ouster (pardon me, oyster)! We can't wait to see
    what you build next with your creativity and knowhow!
  prefs: []
  type: TYPE_NORMAL
- en: We really hope that you enjoyed learning with us; we certainly enjoyed sharing
    this knowledge with you and hope you are inspired to build the future!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should be able to answer the following questions now:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between mapping and localization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What frame does Cartographer typically use as the tracking frame?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is SLAM needed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In which file do you set `min_max_range_filter`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'W. Hess, D. Kohler, H. Rapp, and D. Andor, *Real-Time Loop Closure in 2D LIDAR
    SLAM*: [https://opensource.googleblog.com/2016/10/introducing-cartographer.html](https://opensource.googleblog.com/2016/10/introducing-cartographer.html)
    ([https://research.google/pubs/pub45466/](https://research.google/pubs/pub45466/)),
    in *Robotics and Automation (ICRA)*, 2016 IEEE International Conference on. IEEE,
    2016\. pp. 1271–1278.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cartographer: [https://github.com/cartographer-project/cartographer_ros](https://github.com/cartographer-project/cartographer_ros)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More on Cartographer: [https://google-cartographer-ros.readthedocs.io/en/latest/compilation.html](https://google-cartographer-ros.readthedocs.io/en/latest/compilation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Localization types: [https://www.cpp.edu/~ftang/courses/CS521/notes/Localization.pdf](https://www.cpp.edu/~ftang/courses/CS521/notes/Localization.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RGB-D SLAM: [https://felixendres.github.io/rgbdslam_v2/](https://felixendres.github.io/rgbdslam_v2/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Probabilistic algorithms in robotics: [http://robots.stanford.edu/papers/thrun.probrob.pdf](http://robots.stanford.edu/papers/thrun.probrob.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
