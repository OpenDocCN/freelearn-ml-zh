<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Analyzing Visitor Patterns to Make Recommendations</h1>
                </header>
            
            <article>
                
<p>This chapter will focus on the problem of finding similar visitors based on the theme park attractions they attend, to make improved marketing recommendations. Collaborative filtering methods will be introduced with examples showing how to train and obtain custom recommendations both in Apache Spark (EMR) and through the AWS SageMaker built-in algorithms. Many companies leverage the kinds of algorithms we describe in this chapter to improve the engagement of their customers by recommending products that have a proven record of being relevant to similar customers.  </p>
<p><span>We </span><span>will</span><span> </span>cover<span> the following topics in</span><span> </span>this chapter:</p>
<ul>
<li>Making theme park attraction recommendations through Flickr data</li>
<li>Finding recommendations through Apache Spark's Alternating Least Squares method</li>
<li>Recommending attractions through SageMaker Factorization Machines</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making theme park attraction recommendations through Flickr data</h1>
                </header>
            
            <article>
                
<p>Throughout this chapter, we will make use of the dataset from <a href="https://sites.google.com/site/limkwanhui/datacode">https://sites.google.com/site/limkwanhui/datacode</a>, which consists of Flickr data from users who take photos at different locations, these photos are then mapped to known theme park attractions. Flickr is <span>an image-hosting service. Let's assume</span> Flickr wants to create a plug-in on their mobile app that, as users take photos on the different attractions, identifies user preferences and provides recommendations on other attractions that might be of interest to them.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's also suppose that the number of photos a user takes on a particular attraction is an indicator of their interest in the attraction. Our goal is to analyze a dataset with triples of the <em>user ID, attraction, number of photos taken</em> form so that given an arbitrary set of attractions visited by a user, the model is able to recommend new attractions that similar users found interesting.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collaborative filtering</h1>
                </header>
            
            <article>
                
<p class="mce-root">Collaborative filtering is a process for providing recommendations to users based on their behavior by analyzing the behaviors of a lot of users. We observe the effects of this algorithm in our day-to-day life in a large number of applications. For example, when you are using streaming services, such as Netflix or YouTube, it recommends videos that you may be interested in based on your streaming history. Social networks, such as Twitter and LinkedIn, suggest people for you to follow or connect with based on your current contacts. Services such as Instagram and Facebook curate posts from your friends and tailor your timeline based on the posts that you read or like. As a data scientist, collaborative filtering algorithms are really useful when you are building recommendation systems based on a large amount of user data. </p>
<p>There are various ways in which collaborative filtering can be implemented on a dataset. In this chapter, we will be discussing the memory-based approach and the model-based approach. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Memory-based approach</h1>
                </header>
            
            <article>
                
<p>In the memory-based approach, we generate recommendations in two phases. Consider a situation where we are trying to generate recommendations for a given user based on their interests. In the first phase, we discover users who are similar to the given user based on their interests. We rank all the users based on how similar they are to a given user. In the second phase, we discover the top interests among the group of users that are most similar to a given user. The top interests are ranked based on their similarity to the set of top-ranked users. This ranked list of interests is then presented to the original user as recommendations. </p>
<p><span>For example, in the process of movie recommendations, we look at the movies a user is interested in or has watched recently and discover other users who have watched similar movies. Based on the top-ranked list of similar users, we look at the movies they have watched recently and rank them based on the similarity to the list of ranked users. Then, the top-ranked movies are then presented as recommendations to the user. </span></p>
<p>To find a similarity between users, we use functions called similarity measures. Similarity measures are popularly used in search engines to rank a similarity between query terms and documents. In this section, we discuss the cosine similarity measure, which is commonly used in collaborative filtering. We treat each user's interest as a vector. To discover users with similar interests, we calculate the cosine of the angle between two users' interest vector. It can be represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e44e8858-e65a-442f-86e8-cc65325e8e62.png" style="width:22.17em;height:3.42em;"/></p>
<p>Based on the similarity between a given user and all users in the dataset, we select the top k users. We then aggregate the interest vectors of all users to discover the top-ranked interests and recommend it to the user. </p>
<p>Note that memory-based models do not use any modeling algorithms that were discussed in the previous chapters. They only rely on simple arithmetic to generate recommendations for users based on their interests. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-based approach</h1>
                </header>
            
            <article>
                
<p>For the model-based approach, we use machine learning techniques to train a model that can predict the probability of each interest being relevant to a given user. Various algorithms, such as Bayesian models or clustering models, can be applied for model-based collaborative filtering. However, in this chapter, we focus on the matrix-factorization-based approach. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Matrix factorization</h1>
                </header>
            
            <article>
                
<p>The matrix factorization approach works by decomposing a matrix of users and the interests of the users. In this methodology, we map the user data and the information about the interests to a set of factors. The score of a user to interest is calculated by taking a dot product of the vector scores for the user and the interest.  These factors can be inferred from the user ratings or from the external information about the interests in the algorithm. </p>
<p>For example, consider a matrix where one dimension represents the users and the other dimension represents the movies the users have rated:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 81px"/>
<td style="width: 117.333px"><strong>Avengers</strong></td>
<td style="width: 122.667px"><strong>Jumanji</strong></td>
<td style="width: 107px"><strong>Spiderman</strong></td>
</tr>
<tr>
<td style="width: 81px">User 1</td>
<td style="width: 117.333px">4</td>
<td style="width: 122.667px"/>
<td style="width: 107px">4</td>
</tr>
<tr>
<td style="width: 81px">User 2</td>
<td style="width: 117.333px">1</td>
<td style="width: 122.667px">5</td>
<td style="width: 107px">2</td>
</tr>
<tr>
<td style="width: 81px">User n</td>
<td style="width: 117.333px">5</td>
<td style="width: 122.667px">2</td>
<td style="width: 107px">4</td>
</tr>
</tbody>
</table>
<p> </p>
<p><span>The values in the matrix are the ratings provided by the users. The matrix factorization methodology maps the movies into shorter vectors that represent concepts, such as the genre of the movie. These vectors are known as latent factors. We map the movies to genres and also map what genres users are interested in based on how they rate movies in each genre. Using this information, we can calculate the similarity between a user and movie based on the dot product (multiplying the interest of the user in genres by the likelihood of the movie to belong to genres) between both the vectors. Thus, the unknown ratings in the matrix can be predicted using the knowledge of known ratings by consolidating the users and interests to less granular items (that is, genres).</span> <span><span>In our previous example, we assume that we already have a known mapping of movies to genre. However, we cannot make an assumption that we will always have explicit data to generate such mappings to latent factors</span></span><span>.</span></p>
<p><span>Hence, we explore methodologies that can help us generate such mappings automatically based on the data.  </span>Matrix factorization models therefore need to be able to generate a map between users and interests through a latent factors vector. To ensure we can generate a dot product between the latent factors of a user and the item, the length of the latent factors is set to <span><span>a fixed value</span></span>. Each interest item, <img class="fm-editor-equation" src="assets/b1751de6-aac4-4c85-b8e3-cda4ce8defca.png" style="width:0.50em;height:1.17em;"/>, is represented by a vector, <img class="fm-editor-equation" src="assets/d82d8f27-75f4-420f-82a4-670c39aff92a.png" style="width:1.08em;height:1.00em;"/>, and each user, <img class="fm-editor-equation" src="assets/0e7f0f13-8c2c-4560-9ff1-528999b61e2a.png" style="width:0.75em;height:0.83em;"/>, is represented by a vector, <img class="fm-editor-equation" src="assets/7a61b9cd-ac9f-4912-b6f4-68d4ea25bac9.png" style="width:1.42em;height:1.08em;"/>. The <img class="fm-editor-equation" src="assets/f16968da-4c4d-4caf-bfbe-beffe6d887e2.png" style="width:1.08em;height:1.00em;"/> <span>and </span><img class="fm-editor-equation" src="assets/760e2953-72f9-4f46-915e-7da0de7cd402.png" style="width:1.42em;height:1.08em;"/> vectors are both latent factors that are derived from the data. The rating for an item, <img class="fm-editor-equation" src="assets/bb19f614-4e7e-4d27-95ad-737235fd23e8.png" style="width:0.50em;height:1.17em;"/>, for a user, <img class="fm-editor-equation" src="assets/2cf02bfe-0617-47cc-865a-32914451a6bb.png" style="width:0.75em;height:0.83em;"/>, is represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ef81c489-1e91-4258-9b3f-b17578757613.png" style="width:7.00em;height:1.75em;"/></p>
<p> In general, if we already have a partial set of ratings from users to interests, we can use that to model ratings between other users and interests. We use optimization techniques to calculate this. Our objective is to predict the values of <img class="fm-editor-equation" src="assets/0b81e02c-349c-4ee0-be51-a825132cce01.png" style="width:1.42em;height:1.08em;"/> and <img class="fm-editor-equation" src="assets/edfabc37-59ef-44c3-88bb-61f64d4263e8.png" style="width:1.08em;height:1.00em;"/>. Hence, we do that by minimizing the regularized error when predicting these vectors by using the known ratings. This is represented in the following formula: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/08e98314-a4eb-49c1-bd23-0cb4916391f8.png" style="width:28.08em;height:3.50em;"/></p>
<p class="mce-root"/>
<p>Here, <em>k</em> is a set of <img class="fm-editor-equation" src="assets/b89e6751-a35a-462f-9921-9fbbd6021e15.png" style="width:2.50em;height:1.42em;"/> where the rating, <img class="fm-editor-equation" src="assets/9e086582-82c0-4fda-b029-dee387107679.png" style="width:1.50em;height:1.00em;"/>, is known. Now, let's look at study two approaches for minimizing the preceding equation. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stochastic gradient descent</h1>
                </header>
            
            <article>
                
<p>We studied the stochastic gradient-descent algorithm in <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms</em> regarding linear regression. A similar methodology is used to minimize the function to predict the correct latent factors for each user and interest. We use an iterative approach, where during each iteration, we calculate the error of predicting <img class="fm-editor-equation" src="assets/a7ba6505-3b19-432d-a187-5f83f41d271c.png" style="width:1.08em;height:1.00em;"/> and <img class="fm-editor-equation" src="assets/695327bb-3516-4b2f-8d6d-0511dc04cd6d.png" style="width:1.42em;height:1.08em;"/> based on all the known ratings:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/55123d63-6338-4689-a390-47ed3a1e378b.png" style="width:9.33em;height:1.58em;"/></p>
<p>Based on the magnitude of error, we update the values of <span> </span><img class="fm-editor-equation" src="assets/629216ac-47e5-43be-b576-bb9812831018.png" style="width:1.08em;height:1.00em;"/> <span>and </span><img class="fm-editor-equation" src="assets/bb88a91e-797e-45b5-bd34-99e1820eb0a2.png" style="width:1.42em;height:1.08em;"/> in the opposite direction of the error:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b4231d69-9439-4118-af42-1eeb63226ff3.png" style="width:13.92em;height:1.50em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3653b777-1627-4d63-98e9-6983a00d2316.png" style="width:13.67em;height:1.42em;"/></p>
<p class="mce-root">We stop the iterations after the values of <img class="fm-editor-equation" src="assets/a7ba6505-3b19-432d-a187-5f83f41d271c.png" style="width:1.08em;height:1.00em;"/> <span>and </span><img class="fm-editor-equation" src="assets/695327bb-3516-4b2f-8d6d-0511dc04cd6d.png" style="width:1.42em;height:1.08em;"/> converge. Stochastic gradient descent is also used in algorithms such as <strong>Factorization Machines </strong>(<strong>FMs</strong>), which uses it to compute values of vectors. FMs are a variant of <strong>support vector machine</strong> (<strong>SVM</strong>) models that can be applied in a collaborative filtering framework. We do not explain support vector machines or FMs in detail in this book, but encourage you to understand how they work. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alternating Least Squares </h1>
                </header>
            
            <article>
                
<p>One of the challenges of minimizing the optimization function to predict the values of both <img class="fm-editor-equation" src="assets/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png" style="width:1.08em;height:1.00em;"/> <span>and </span><img class="fm-editor-equation" src="assets/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png" style="width:1.42em;height:1.08em;"/> is that the equation is not convex. This is because we are trying to optimize two values at the same time. However, if we used a constant for one of the values, or <img class="fm-editor-equation" src="assets/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png" style="width:1.08em;height:1.00em;"/> <span>or</span> <img class="fm-editor-equation" src="assets/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png" style="width:1.42em;height:1.08em;"/>, we can solve the equation optimally for the other variable. Hence, in the Alternating Least Squares technique, we alternatively set the values of <img class="fm-editor-equation" src="assets/c5e3c13e-f99f-4f26-90be-89c5eb603d61.png" style="width:1.08em;height:1.00em;"/> <span>and </span><img class="fm-editor-equation" src="assets/39b18f7c-0914-4b9a-8e92-464d3b246e6d.png" style="width:1.42em;height:1.08em;"/> as constant while optimizing for the other vector.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Hence, in the first step, we set base values for both the vectors. Assuming that one of the values is constant, we use linear programming to optimize the other vector. In the next step, we set the value of the optimized vector as constant and optimize for the other variable. We will not explain how linear programming is used to optimize for quadratic questions as it is an entire field of study and not in the scope of this book. This methodology optimizes each vector until convergence. </p>
<p>The advantage of stochastic gradient descent is that it is faster than the ALS method, as it depends on predicting the values of both the vectors in each step while modifying the vectors based on the proportion of errors. However, in the ALS methodology, the system calculates the values of each vector independently, and hence leads to better optimization. Moreover, when the matrix is dense, the gradient descent methodology has to learn from each set of data, making it less efficient than the ALS methodology. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding recommendations through Apache Spark's ALS</h1>
                </header>
            
            <article>
                
<p>In this section, we will go through the process of creating recommendations in Apache Spark using <strong>Alternating Least Squares</strong> (<strong>ALS</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data gathering and exploration</h1>
                </header>
            
            <article>
                
<p>The first step is to download the data from <a href="https://sites.google.com/site/limkwanhui/datacode">https://sites.google.com/site/limkwanhui/datacode</a> . We will be using the <kbd>poiList-sigir17</kbd> dataset with photos taken by users at different theme park attractions (identified as points of interest by Flickr). There are following two datasets we're interested in:</p>
<ul>
<li>The list of points of interests, which captures the names and other properties of each attraction:</li>
</ul>
<pre style="padding-left: 60px">poi_df = spark.read.csv(SRC_PATH + 'data-sigir17/poiList-sigir17', <br/>                        header=True, inferSchema=True, sep=';')</pre>
<p style="padding-left: 60px">The following screenshot shows the first few lines of the <kbd>poi_df</kbd> dataframe:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bf8ca937-b9e2-4f10-919c-8acb409538fd.png" style="font-size: 1em;"/></p>
<ul>
<li>The photos taken by Flickr users at different points of interest:</li>
</ul>
<pre style="padding-left: 60px">visits_path = SRC_PATH+'data-sigir17/userVisits-sigir17'<br/>visits_df = spark.read.csv(visits_path, <br/>                           header=True,<br/>                           inferSchema=True, sep=';')</pre>
<p style="padding-left: 60px">The following screenshot shows a sample of the <kbd>visits_df</kbd> dataframe:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/eb169bd1-9b0d-4330-bb02-2deae5faff7e.png" style="font-size: 1em;width:40.92em;height:11.83em;"/></p>
<p style="padding-left: 60px">In this dataset, we will be using the <kbd>nsid</kbd> field (indicating the user taking the photo) and <kbd>poiID</kbd>, which indicates the actual point of interest or attraction visited while taking the photo. For our purposes, we will ignore the rest of the fields.</p>
<p>Let's do some basic inspection on our dataset. The dataset has about 300,000 rows of data. By taking a sample of 1,000 entries, we can see that there are 36 unique Flickr users:</p>
<pre>sample_df = visits_df.limit(1000).toPandas()<br/>sample_df.describe()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The output of the preceding <kbd>describe()</kbd> command is as follows:</p>
<pre>count 1000<br/> unique 36<br/> top 10182842@N08<br/> freq 365</pre>
<p>This is important, as we need to have enough entries per user to ensure we have enough information about users to make predictions. Furthermore, it's actually more relevant to know whether users visit different attractions. One the nice things about Apache Spark is that one can work on datasets using SQL. Finding the number of distinct attractions users see on average can easily be done with SQL.</p>
<p>In order to work with SQL, we first need to give a table name to the dataset. This is done by registering a temp table:</p>
<pre>poi_df.createOrReplaceTempView('points')<br/>visits_df.createOrReplaceTempView('visits')</pre>
<p>Once we register the tables, we can do queries, such as finding the number of unique attractions:</p>
<pre>spark.sql('select distinct poiID from visits').count()<br/>31</pre>
<p>Or we can combine SQL with other dataset operations, such as <kbd>.describe()</kbd>:</p>
<pre>spark.sql('select nsid,count(distinct poiID) as cnt from visits group by nsid').describe().show()</pre>
<p>The following screenshot contains the result of the output of the <kbd>show()</kbd> command:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a6020f4a-0a8b-4555-afe9-877e3e6886b5.png" style="font-size: 1em;width:24.08em;height:12.67em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The preceding SQL command finds the number of distinct attractions each user visits. The describe dataset operation finds statistics on these users, which tells us that, on average, users visit about five different locations. This is important as we need to have enough attractions per user to be able to correctly identify user patterns.</p>
<p>Similarly, we should look at the number of photos users take at each location, to validate that in fact we can use the number of photos taken as an indicator of the user's interest. We do that through the following command:</p>
<pre>spark.sql('select nsid,poiID,count(*) from visits group by nsid,poiID').describe().show()</pre>
<p>The output of the preceding command is shown by the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/92832e20-1d60-4f6d-8293-1c2682b54201.png" style="width:27.75em;height:11.08em;"/></p>
<p>The SQL command counts the number of entries for each user and attraction, and then we find a statistical summary using the describe. We can conclude therefore that on average, each user takes about eight pictures at every location they visit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p>To train our model, we will construct a dataset that computes the number of photos taken by each user at each location:</p>
<pre>train_df = spark.sql('select hash(nsid) as user_hash_id, poiID, count(*) as pictures_taken from visits group by 1,2')</pre>
<p>The following screenshot shows the first few lines of the <kbd>train_df</kbd> dataframe:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fdcd6859-698d-47f8-bf86-8cec71cca1b0.png" style="width:14.75em;height:6.67em;"/></p>
<p class="mce-root"/>
<div class="packt_tip">We hash the user because the ALS trainer just supports numerical values as features.</div>
<p>To train the model, we simply need to construct an instance of ALS and provide the user column, item column (in this case the attraction IDs), and the rating column (in this case, <kbd>pictures_takes</kbd> is used as a proxy for rating). <kbd>coldStartStrategy</kbd> is set to drop as we're not interested in making predictions for users or attractions not present in the dataset (that is, predictions for such entries will be dropped rather than returning NaN):</p>
<pre>from pyspark.ml.recommendation import ALS<br/><br/>recommender = ALS(userCol="user_hash_id", <br/>                  itemCol="poi_hash_id", <br/>                  ratingCol="pictures_taken", <br/>                  coldStartStrategy="drop")<br/><br/>model = recommender.fit(train_df)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting recommendations</h1>
                </header>
            
            <article>
                
<p>Once we build a model, we can generate predictions for all users in our dataset:</p>
<pre>recommendations = model.recommendForAllUsers(10)</pre>
<p>The preceding command will pick the top 10 recommendations for each user. Note that because of how ALS works, it might actually recommend attractions already visited by the user, so we need to discard that for our purposes, as we will see later on.</p>
<p>The recommendations look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/debd84dc-dfbb-44cb-bce9-2a1bef310250.png" style="width:33.33em;height:9.75em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Each user gets a list of tuples with the recommended attraction as well as the score for the recommendation. In this case, the score represents the estimated number of photos we would expect each user to take at the recommended location. Even though the model just provides the IDs of the attractions, we would like to inspect a few of these recommendations to make sure they are good. In order to do that, we will construct a dictionary of IDs to attraction names (point of interest names) by collecting the result of a query that finds the name of each attraction in the points table:</p>
<pre>row_list = spark.sql('select distinct p.poiName, p.poiID from visits v join points p on (p.poiID=v.poiID) ').collect()<br/>id_to_poi_name = dict(map(lambda x: (x.poiID, x.poiName), row_list))</pre>
<p><span>The map contains the following entries:</span></p>
<pre>{1: 'Test Track',<br/> 10: 'Golden Zephyr',<br/> 19: "Tarzan's Treehouse",<br/> 22: 'Country Bear Jamboree'<br/> ....<br/> }</pre>
<p>For each user, we want to remove the recommendations for already-visited sites and output the recommendations. To do that, we need to process the list of tuples on each row. Apache Spark provides a convenient way to do this by allowing users to create custom SQL functions, or <strong>user-defined functions</strong> (<strong>UDFs</strong>). We will define and register a UDF that is capable of extracting the names of each recommended attraction through the use of the preceding map:</p>
<pre>def poi_names(recommendations, visited_pois):<br/>   visited_set = set([id_to_poi_name[poi] for poi in visited_pois])<br/>   recommended = str([(id_to_poi_name[poi], weight) \<br/>                      for (poi,weight) in recommendations<br/>                      if id_to_poi_name[poi] not in visited_set])<br/>   return "recommended: %s ; visited: %s "%(recommended, visited_set)<br/><br/>spark.udf.register("poi_names", poi_names)</pre>
<p>The <kbd>poi_names</kbd> function receives the recommendations tuple for a user as well as the attractions visited and then returns a string that contains all recommended attraction names that were not in the set of visited, as well as an enumeration of the visited attractions.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>We then register the recommendations as a table so it can be used in our next query:</p>
<pre>recommendations.createOrReplaceTempView('recommendations')<br/><br/>recommendation_sample = spark.sql('select user_hash_id, collect_list(poiID), poi_names(max(recommendations), collect_list(poiID)) as recommendation from recommendations r join visits v on (r.user_hash_id = hash(v.nsid)) group by 1')\<br/>   .sample(fraction=0.1, withReplacement=False) \<br/>   .collect()<br/><br/></pre>
<p>The preceding query joins the user recommendations table with the visits table and joins by user, collecting all points of interest visited by each user, and through the UDF it outputs the recommended attractions as well as the names of the already-visited attractions. We sample and collect a few instances of the table to inspect. In the companion notebook, we can observe the entries:</p>
<pre>print(recommendation_sample[0].recommendation)<br/><br/>recommended: [("It's A Small World", 31.352962493896484), ('Walt Disney World Railroad', 23.464025497436523), ('Pirates of the Caribbean', 21.36219596862793), ('Buzz Lightyear Astro Blasters', 17.21680450439453), ('Haunted Mansion', 15.873616218566895), ('Country Bear Jamboree', 9.63521957397461), ('Astro Orbiter', 9.164801597595215), ('The Great Movie Ride', 8.167647361755371)] ; visited: {"California Screamin'", 'Sleeping Beauty Castle Walkthrough', 'Voyage of The Little Mermaid', "Tarzan's Treehouse", 'Main Street Cinema', 'The Many Adventures of Winnie the Pooh', 'Jungle Cruise', 'Tom Sawyer Island', 'Test Track', 'The Twilight Zone Tower of Terror'}</pre>
<p><span>We can observe that this user visited a number of adventure-like attractions and the model recommended a few more. Here, the reader can inspect a couple more recommendations:</span></p>
<pre>print(recommendation_sample[200].recommendation)<br/><br/>recommended: [('Splash Mountain', 0.9785523414611816), ('Sleeping Beauty Castle Walkthrough', 0.8383632302284241), ("Pinocchio's Daring Journey", 0.7456990480422974), ('Journey Into Imagination With Figment', 0.4501221477985382), ("California Screamin'", 0.44446268677711487), ('Tom Sawyer Island', 0.41949236392974854), ("It's A Small World", 0.40130260586738586), ('Astro Orbiter', 0.37899214029312134), ('The Twilight Zone Tower of Terror', 0.3728359639644623)] ; visited: {"Snow White's Scary Adventures"}<br/><br/>print(recommendation_sample[600].recommendation)<br/><br/>recommended: [('Fantasmic!', 20.900590896606445), ('Pirates of the Caribbean', 9.25596809387207), ("It's A Small World", 8.825133323669434), ('Buzz Lightyear Astro Blasters', 5.474684715270996), ('Main Street Cinema', 5.1001691818237305), ('Country Bear Jamboree', 4.3145904541015625), ("California Screamin'", 3.717888832092285), ("It's A Small World", 3.6027705669403076), ('The Many Adventures of Winnie the Pooh', 3.429044246673584)] ; visited: {'Haunted Mansion', 'The Twilight Zone Tower of Terror', 'Journey Into Imagination With Figment'}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recommending attractions through SageMaker Factorization Machines</h1>
                </header>
            
            <article>
                
<p>FMs are one of the most widely used algorithms for making recommendations when it comes to very sparse input. It is similar to the <strong>stochastic gradient descent</strong> (<strong>SGD</strong>) algorithm we discussed under the model-based matrix factorization methodology. In this section, we will show how to use AWS' built-in algorithm implementation of FMs to get recommendations for our theme park visitors.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the dataset for learning</h1>
                </header>
            
            <article>
                
<p>In order to use such an algorithm, we need to prepare our dataset in a different way. We will pose the recommendation problem as a regression problem in which the input are a pair of user and attraction, and the output is the expected level of interest this user will have toward the attraction. The training dataset must have the actual empirical interest (measured by the number of photos taken) for each pair of user and attraction. With this data, the FM model will then be able to predict the interest of an arbitrary attraction for any user. Hence, to obtain recommendations for a user, we just need to find the list of attractions that yields the highest predicted level of interest.</p>
<p><strong>So then how do we encode the user and the attractions in a dataset?</strong></p>
<p>Given that FMs are extremely good at dealing with high-dimensional features, we can one-hot encode our input. Since there are 8,903 users and 31 attractions, our input vector will be of length 8,934 where the first 31 vector components will correspond to the 31 different attractions, and the remaining positions correspond to each user. The vector will always have zeros except for the positions corresponding to the user and attraction, which will have a value of 1. The target feature (label) used in our model will be the level of interest, which we will discretize to a value of 1 to 5 by normalizing the number of pictures taken according to their corresponding quantile. </p>
<p>The following figure shows how such a training dataset could look:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/351fd038-4fe8-4e44-adfc-5d92549a6256.png" style="width:32.17em;height:16.33em;"/></p>
<p>As you can imagine, this matrix is extremely sparse, therefore we need to encode our rows using a sparse representation. Like most SageMaker algorithms, we must drop our data in S3 to allow SageMaker to train the data. In past chapters, we used CSV as an input. However, CSV is not a good representation for our dataset; given its sparse nature, it would occupy too much space (with a lot of repeated zeros!). In fact, at the time of writing, SageMaker doesn't even support CSV as an input format. In a sparse representation, each vector must indicate the following three values:</p>
<ul>
<li style="font-weight: 400">The size of the vector</li>
<li style="font-weight: 400">The positions in which we have a value other than 0</li>
<li style="font-weight: 400">The values at each of these non-zero positions</li>
</ul>
<p>For example, the sparse representation for the first row in the preceding figure would be the following:</p>
<ul>
<li style="font-weight: 400">Vector size = 8934</li>
<li style="font-weight: 400">Non-zero positions = [1, 33]</li>
<li style="font-weight: 400">Values at non-sero positions = [1, 1]</li>
</ul>
<p>The only input format FMs currently supports is called protobuf recordIO. Protobuf, short for <strong>Protocol buffers</strong>, is a language-neutral, platform-neutral extensible mechanism for serializing structured data initially developed by Google. In our case, the structure will be the sparse representation of our matrix. Each record in the protobuf file we store in S3 will have all three items necessary for sparse representation, as well as the target feature (label).</p>
<p class="mce-root"/>
<p>Following, we will go through the process of preparing the dataset and uploading it to S3.</p>
<p>We will start with the Spark dataframe that we used for training in the previous section (<kbd>train_df</kbd>) and apply a <kbd>Pipeline</kbd> that does the one-hot encoding as well as normalizing the photos-taken target feature:</p>
<pre>from pyspark.ml.feature import OneHotEncoder<br/>from pyspark.ml.feature import StringIndexer<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml.feature import QuantileDiscretizer<br/>from pyspark.ml.feature import VectorAssembler<br/><br/>pipeline = Pipeline(stages = [<br/>   StringIndexer(inputCol='user_hash_id', <br/>                 outputCol="user_hash_id_index", <br/>                 handleInvalid='keep'),<br/>   OneHotEncoder(inputCol='user_hash_id_index', <br/>                 outputCol='user_hash_id_encoded'),<br/>   StringIndexer(inputCol='poiID', <br/>                 outputCol='poi_id_indexed', <br/>                 handleInvalid='keep'),<br/>   OneHotEncoder(inputCol='poi_id_indexed', <br/>                 outputCol='poi_id_encoded'),<br/>   QuantileDiscretizer(numBuckets=5, <br/>                       inputCol='pictures_taken', <br/>                       outputCol='interest_level'),<br/>   VectorAssembler(inputCols=['poi_id_encoded', 'user_hash_id_encoded'],<br/>                   outputCol='features'),<br/>])<br/><br/>model = pipeline.fit(train_df)</pre>
<p>The pipeline is similar to pipelines we've built in the previous chapters, the difference being that we have not included a machine learning algorithm as a final step (since this stage will run through SageMaker's FMs once the dataset is in S3). We first string index the user and attraction (point of interest) features, and then chain them into a one-hot encoder. The quantile discretizer will reduce the photos taken feature into five buckets according to their percentile. We will name this feature <kbd>interest_level</kbd>. Additionally, we will assemble a vector with these encoded attractions and user vectors.</p>
<p>Next, we transform the training dataset by applying the model:</p>
<pre>sparse_df = model.transform(train_df)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This will produce a dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/771a08f9-086d-423d-8188-f2bcca0be7b9.png"/></p>
<p>Note how the encoded fields (<kbd>user_hash_id_encoded</kbd>, <kbd>poi_id_encoded</kbd>, and features) show the sparse representation of the vectors.</p>
<p>Once we have this encoded dataset, we can split them into testing and training. SageMaker will use the training dataset for fitting and the test dataset for finding the validation errors at each epoch upon training. We need to convert each of these datasets into recordio format and upload them to s3.</p>
<p>If we were working in Scala (the native programming language used by Spark), we could do something like this:</p>
<pre>sagemaker_train_df.write.format("sagemaker") \<br/>  .option("labelColumnName", "interest_level") \<br/>  .option("featuresColumnName", "features") \<br/>  .save("s3://mastering-ml-aws/chapter6/train-data")</pre>
<p>Unfortunately, <kbd>pyspark</kbd> does not support writing a dataframe directly into recordio format at the time of this writing. Instead we will collect all our spark dataframes in memory and convert each row to a sparse vector, and then upload it to S3.</p>
<p>The following <kbd>spark_vector_to_sparse_matrix</kbd> function does exactly that. It takes a Spark dataframe row and converts it into a sparse <kbd>csr_matrix</kbd> (from <kbd>scipy</kbd>, a Python library with scientific utilities). The <kbd>upload_matrices_to_s3</kbd> function receives a Spark dataset (either training or testing), collects each row, builds a sparse vector with the features, and stacks them into a matrix. Additionally, it builds a target feature vector with all the interest levels. Given this matrix and label vector, we use the utility function <kbd>write_spmatrix_to_sparse_tensor</kbd>, of the <span><kbd>sagemaker</kbd> library </span>to write the data in recordio format. Finally, we upload that object to S3. To do this, let's first import all the necessary dependencies:</p>
<pre>from scipy.sparse import csr_matrix<br/>import numpy as np<br/>import boto3<br/>import io<br/>import numpy as np<br/>import scipy.sparse as sp<br/>import sagemaker.amazon.common as smac</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root">Next, let's define two auxiliary functions: <kbd>spark_vector_to_sparse_matrix</kbd>, which will take a row and produce a <kbd>scipy</kbd> sparse matrix, and <kbd>upload_matrices_to_s3</kbd>, which is responsible for uploading the test or training dataset to s3:</p>
<pre>def spark_vector_to_sparse_matrix(row):<br/>   vect = row['features']<br/>   return csr_matrix((vect.values, vect.indices, np.array([0, vect.values.size])),<br/>                     (1, vect.size), <br/>                      dtype=np.float32)<br/><br/>def upload_matrices_to_s3(dataframe, dataset_name):<br/>   features_matrices = <br/>        dataframe.select("features") \<br/>                 .rdd.map(spark_vector_to_sparse_matrix).collect()<br/>   interest_levels = <br/>        dataframe.select("interest_level") \<br/>                 .rdd.map(lambda r: r['interest_level']).collect()<br/><br/>   interest_level_vector = np.array(interest_levels, dtype=np.float32)<br/>   buffer = io.BytesIO()<br/>   smac.write_spmatrix_to_sparse_tensor(buffer, \<br/>                                        sp.vstack(features_matrices), \<br/>                                        interest_level_vector)<br/>   buffer.seek(0)<br/>   bucket = boto3.resource('s3').Bucket('mastering-ml-aws')<br/>   bucket.Object('chapter6/%s-data.protobuf'%dataset_name).upload_fileobj(buffer)</pre>
<p>Finally, we need to upload the training and testing dataset by calling the <kbd>upload_matrices_to_s3</kbd> method on both variables:</p>
<pre>upload_matrices_to_s3(sagemaker_train_df, 'train')<br/>upload_matrices_to_s3(sagemaker_test_df, 'test')</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p>Now that we have the data in S3 in the right format for learning, we can start training our model to get recommendations.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will instantiate the SageMaker session and define the paths where to read and write the data:</p>
<pre>import sagemaker<br/>from sagemaker import get_execution_role<br/>import json<br/>import boto3<br/><br/>sess = sagemaker.Session()<br/>role = get_execution_role()<br/>container = sagemaker.amazon.amazon_estimator.get_image_uri('us-east-1', <br/>     "factorization-machines", <br/>     "latest")<br/><br/>s3_train_data = 's3://mastering-ml-aws/chapter6/train-data.protobuf'<br/>s3_test_data = 's3://mastering-ml-aws/chapter6/train-data.protobuf'<br/>s3_output_location = 's3://mastering-ml-aws/chapter6/sagemaker/output/'</pre>
<p>With the session, we can instantiate the SageMaker estimator by setting the number and type of computers to use. We also specify the hyperparameters. Two important parameters to consider are the feature dim (which is the length of our training vectors) and the predictor type. Since our problem is posed as a regression, we will use regressor. If instead of interest level, we had modeled it as a presence/no presence of interest, we would have used the <kbd>binary_classifier</kbd> value:</p>
<pre>from sagemaker.session import s3_input<br/><br/>recommender = sagemaker.estimator.Estimator(container,<br/>                                            role,<br/>                                            train_instance_count=1,<br/>                                            train_instance_type='ml.c4.xlarge',<br/>                                            output_path=s3_output_location,<br/>                                            sagemaker_session=sess)<br/><br/>recommender.set_hyperparameters(predictor_type='regressor',<br/>                                feature_dim=8934,<br/>                                epochs=200,<br/>                                mini_batch_size=100,<br/>                                num_factors=128)<br/><br/>recommender.fit({'train': s3_input(s3_train_data), \<br/>                 'test': s3_input(s3_test_data)})</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The logs will show some validation stats and a confirmation for when the model has completed:</p>
<div>
<pre>[02/23/2019 22:01:02 INFO 140697667364672] #test_score (algo-1) : ('rmse', 0.19088356774389661)<br/>2019-02-23 22:01:11 Uploading - Uploading generated training model<br/> 2019-02-23 22:01:11 Completed - Training job completed</pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting recommendations</h1>
                </header>
            
            <article>
                
<p>Once the model is fitted, we can launch a predictor web service:</p>
<pre>predictor = recommender.deploy(instance_type='ml.c5.xlarge', initial_instance_count=1)</pre>
<p>This will launch the web service endpoint that hosts the trained model and is now ready to receive requests with predictions. Let's take one user from our recommendations made with Spark's ALS and compare it to the predictions made by SageMaker:</p>
<pre>print(recommendation_sample[1].user_hash_id)<br/>-525385694</pre>
<p>We can collect the features of that user:</p>
<pre><br/>sagemaker_test_df.select('features').where('user_hash_id=-525385694') \<br/>                 .rdd.map(build_request).collect()<br/><br/>[{'data': {'features': {'shape': [8934],<br/>   'keys': [4, 3297],<br/>   'values': [1.0, 1.0]}}}]</pre>
<p>Here, <kbd>build_request</kbd> is a convenient function to create a JSON request compatible with how SageMaker expects the sparse-encoded requests:</p>
<pre>def build_request(row):<br/>   vect = row['features']<br/>   return {'data':{ 'features': {'shape':[int(vect.size)], <br/>                                 'keys':list(map(int,vect.indices)),<br/>                                 'values':list(vect.values)}}}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>As we know, the user ID position in the vector is <kbd>3297</kbd> and the attraction position is <kbd>4</kbd>. We can call the service to get a prediction for the service:</p>
<pre>import json<br/><br/>predictor.content_type = 'application/json'<br/>predictor.predict(json.dumps({'instances': [<br/>    {'data': {'features': {'shape': [8934], 'keys': [4, 3297], <br/>              'values': [1, 1]}}}]}))</pre>
<p>Here's the output:</p>
<pre>{'predictions': [{'score': 0.8006305694580078}]}</pre>
<div class="packt_infobox">
<p>More details about the formats of the JSON requests and responses can be found here: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html">https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html</a>.</p>
</div>
<p>Since we can ask the predictor for the score for an arbitrary pair of (user, attraction), we'll find the scores of all 31 attractions for the user in question and then sort by score:</p>
<pre>def predict_poi(poi_position):<br/>   prediction = predictor.predict( <br/>           json.dumps({'instances': [{'data': <br/>                        {'features': {'shape': [8934], <br/>                                      'keys': [poi_position, 3297], <br/>                                      'values': [1, 1]}}}]}))<br/>   return prediction['predictions'][0]['score']<br/><br/>predictions = [(poi_position, predict_poi(poi_position)) for poi_position in range(0,31)]<br/>predictions.sort(key=lambda x:x[1], reverse=True)</pre>
<p>Given those scores, we can find the names of the highest-ranking attractions, excluding those already visited:</p>
<pre>user_visited_pois = <br/>     [id_to_poi_name[x] for x in set(recommendation_sample[1]['collect_list(poiID)'])]<br/><br/>for (poi_position, score) in predictions[:10]:<br/>  recommended_poi = id_to_poi_name[int(model.stages[2].labels[poi_position])]<br/>  if recommended_poi not in user_visited_pois:<br/>       print(recommended_poi)</pre>
<p>The output is as follows:</p>
<pre>Test Track<br/> Walt Disney World Railroad<br/> Main Street Cinema<br/> Tom Sawyer Island<br/> Tarzan's Treehouse<br/> Mark Twain Riverboat<br/> Sleeping Beauty Castle Walkthrough<br/> Snow White's Scary Adventures</pre>
<p>Let's compare this with the recommendations made by Spark:</p>
<div>
<pre>print(recommendation_sample[1].recommendation)<br/>recommended: [("Pinocchio's Daring Journey", 3.278768539428711), ('Tom Sawyer Island', 2.78713321685791), ('Splash Mountain', 2.114530324935913), ("Tarzan's Treehouse", 2.06896710395813), ('Fantasmic!', 1.9648514986038208), ("Snow White's Scary Adventures", 1.8940000534057617), ('Main Street Cinema', 1.6671074628829956), ('Mark Twain Riverboat', 1.314055323600769), ('Astro Orbiter', 1.3135600090026855)] ; visited: {'The Many Adventures of Winnie the Pooh', 'Rose &amp; Crown Pub Musician', 'Golden Zephyr', "It's A Small World"}</pre></div>
<p>As the reader might notice, there are many overlapping recommendations. For a more thorough analysis regarding the quality of the model and its predictive power, we can use the evaluation methods discussed in <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms,</em> as this problem is posed as a regression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we studied a new type of machine learning algorithm called collaborative filtering. This algorithm is used in recommendation systems. We looked at memory-based approaches that use similarity measures to find users similar to a given user and discover recommendations based on the collective interests of the top-ranked similar users. We also studied a model-based approach called matrix factorization, that maps users and interests to latent factors and generate recommendations based on these factors. We also studied the implementations of various collaborative filtering approaches in Apache Spark and SageMaker.</p>
<p>In the next chapter, we will focus on a very popular topic: deep learning. We will cover the theory behind this advanced field as well as a few modern applications. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<ol>
<li>Find an example of a recommendation system that is not described in this chapter. Evaluate which approach of collaborative filtering would fit that approach.</li>
<li>For a movie-recommendation engine, explore how the issue of sparsity of data affects each algorithm listed in this chapter. </li>
</ol>


            </article>

            
        </section>
    </body></html>