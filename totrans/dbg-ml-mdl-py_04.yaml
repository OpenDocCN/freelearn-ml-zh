- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Detecting Performance and Efficiency Issues in Machine Learning Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测机器学习模型中的性能和效率问题
- en: One of the main objectives we must keep in mind is how to build a high-performance
    machine learning model with minimal errors on new data we want to use the model
    for. In this chapter, you will learn how to properly assess the performance of
    your models and identify opportunities for decreasing their errors.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须牢记的主要目标之一是，如何在新的数据上构建一个高性能的机器学习模型，这些数据是我们希望使用模型的情况。在本章中，你将学习如何正确评估你的模型性能，并识别减少它们错误的机会。
- en: This chapter includes many figures and code examples to help you better understand
    these concepts and start benefiting from them in your projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含许多图表和代码示例，以帮助你更好地理解这些概念，并在你的项目中开始从中受益。
- en: 'We will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Performance and error assessment measures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能和错误评估措施
- en: Visualization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化
- en: Bias and variance diagnosis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差和方差诊断
- en: Model validation strategy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型验证策略
- en: Error analysis
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误分析
- en: Beyond performance
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除此之外
- en: By the end of this chapter, you will have learned about how to assess the performance
    of machine learning models and the benefits, limitations, and wrong usage of visualization
    in different machine learning problems. You will have also learned about bias
    and variance diagnosis and error analysis to help you identify opportunities so
    that you can improve your models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解如何评估机器学习模型的性能，以及可视化在不同机器学习问题中的好处、局限性和错误使用。你还将了解偏差和方差诊断以及错误分析，以帮助你识别机会，以便你可以改进你的模型。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，以下要求应予以考虑，因为它们将帮助你更好地理解概念，在你的项目中使用它们，并使用提供的代码进行实践：
- en: 'Python library requirements:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库要求：
- en: '`sklearn` >= 1.2.2'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` >= 1.2.2'
- en: '`numpy` >= 1.22.4'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` >= 1.22.4'
- en: '`pandas` >= 1.4.4'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` >= 1.4.4'
- en: '`matplotlib` >= 3.5.3'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` >= 3.5.3'
- en: '`collections` >= 3.8.16'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collections` >= 3.8.16'
- en: '`xgboost` >= 1.7.5'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xgboost` >= 1.7.5'
- en: You should have basic knowledge of model validation and testing, as well as
    classification, regression, and clustering in machine learning
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该具备模型验证和测试的基本知识，以及机器学习中的分类、回归和聚类
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到本章的代码文件，网址为[https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04)。
- en: Performance and error assessment measures
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能和错误评估措施
- en: The metrics we use to assess the performance and calculate errors in our models,
    and how we interpret their values, determine the models we select, the decisions
    we make to improve a component of our machine learning life cycle, and determine
    if we have a reliable model to bring into production. Although many performance
    metrics can be used in one line of Python code to calculate errors and performance,
    we shouldn’t blindly use them or try to improve our performance reports by implementing
    many of them together without knowing their limitations and how to correctly interpret
    them. In this section, we will talk about metrics for assessing the performance
    of classification, regression, and clustering models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来评估模型性能和计算错误的指标，以及我们如何解释它们的值，决定了我们选择的模型，我们做出的改进机器学习生命周期组件的决定，以及我们是否有一个可靠的生产模型。尽管许多性能指标可以用一行Python代码来计算错误和性能，但我们不应该盲目使用它们，或者试图通过实现许多指标来提高我们的性能报告，而不了解它们的限制和如何正确解释它们。在本节中，我们将讨论用于评估分类、回归和聚类模型性能的指标。
- en: Classification
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: 'Each classification model, either binary or multi-class, returns the probability
    of predictions, a number between 0 and 1, which then gets transformed into class
    labels. There are two major categories of performance metrics: **label-based performance
    metrics**, which rely on predicted labels, and **probability-based performance
    metrics**, which use the probability of predictions for performance or error calculation.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分类模型，无论是二分类还是多分类，都会返回预测的概率，一个介于 0 和 1 之间的数字，然后将其转换为类别标签。性能指标主要有两大类：**基于标签的性能指标**，它依赖于预测标签，以及**基于概率的性能指标**，它使用预测的概率进行性能或错误计算。
- en: Label-based performance metrics
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于标签的性能指标
- en: 'The predicted probabilities of classification models get transformed into class
    labels by the Python classes we use for modeling. We can then use a confusion
    matrix, as shown in *Figure 4**.1*, to identify four groups of data points, including
    **true positives** (**TPs**), **false positives** (**FPs**), **false negatives**
    (**FNs**), and **true negatives** (**TNs**) for binary classification problems:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型的预测概率通过我们用于建模的 Python 类转换为类别标签。然后，我们可以使用如图 *图 4.1* 所示的混淆矩阵来识别四组数据点，包括**真正例**（**TPs**）、**假正例**（**FPs**）、**假反例**（**FNs**）和**真反例**（**TNs**）对于二分类问题：
- en: '![Figure 4.1 – Confusion matrix for binary classification](img/B16369_04_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 二分类的混淆矩阵](img/B16369_04_01.jpg)'
- en: Figure 4.1 – Confusion matrix for binary classification
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 二分类的混淆矩阵
- en: 'We can use `sklearn.metrics.confusion_matrix()` to extract these four groups
    of data points and then calculate performance metrics such as specificity according
    to the following mathematical definition:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `sklearn.metrics.confusion_matrix()` 提取这四组数据点，然后根据以下数学定义计算性能指标，如特异性：
- en: '![](img/B16369_04_001.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16369_04_001.jpg)'
- en: 'Here is the Python implementation of extracting specificity, precision, and
    recall from a confusion matrix:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用混淆矩阵提取特异性、精确率和召回率的 Python 实现：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can calculate other performance metrics, such as precision and recall, using
    TP, TN, FP, and FN, which have been extracted from the confusion matrix, or directly
    use functions available in Python (*Table 4.1*). In addition to the Python functions
    to calculate some of the common performance metrics for classification models,
    you can also find the mathematical definitions of the metrics and their interpretations
    in *Table 4.1*. This extra information will help you understand how to interpret
    each of these metrics and when to use them:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用从混淆矩阵中提取的 TP、TN、FP 和 FN 计算其他性能指标，如精确率和召回率，或者直接使用 Python 中可用的函数（*表 4.1*）。除了用于计算分类模型的一些常见性能指标的
    Python 函数外，您还可以在 *表 4.1* 中找到指标的数学定义及其解释。这些额外信息将帮助您理解如何解释这些指标以及何时使用它们：
- en: '| **Metric** | **Python** **Function** | **Formula** | **Description** |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| **指标** | **Python** **函数** | **公式** | **描述** |'
- en: '| Accuracy | `metrics.accuracy_score()` |  TP + TN _ n n: Number of data points
    | Number of correct predictions over the total number of data pointsRange: [0,
    1]Higher values mean higher performance |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | `metrics.accuracy_score()` |  TP + TN _ n n: 数据点数量 | 在总数据点中正确预测的数量范围：[0,
    1]值越高表示性能越好 |'
- en: '| **Precision or positive predictive** **value** (**PPV**) | `metrics.precision_score()`
    |  TP _ TP + FP  | Fraction of predicted positives that are positiveRange:[0,
    1]Higher values mean higher performance |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **精确率或阳性预测值**（**PPV**） | `metrics.precision_score()` |  TP _ TP + FP  | 预测为阳性的预测中实际为阳性的比例范围：[0,
    1]值越高表示性能越好 |'
- en: '| Recall, sensitivity, or **true positive** **rate** (**TPR**) | `metrics.recall_score()`
    |  TP _ TP + FN  | Fraction of positives that are predicted as positiveRange:[0,
    1]Higher values mean higher performance |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 召回率、灵敏度或**真正例率**（**TPR**） | `metrics.recall_score()` |  TP _ TP + FN  | 被预测为阳性的正例的比例范围：[0,
    1]值越高表示性能越好 |'
- en: '| F1 score and its derivatives | `metrics.f1_score()` |  Precision * Recall  ____________ Precision
    + Recall _ 2   | The harmonic mean of precision and recallRange:[0, 1]Higher values
    mean higher performance |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| F1 分数及其衍生指标 | `metrics.f1_score()` |  精确率 * 召回率  ____________ 精确率 + 召回率 _ 2  
    | 精确率和召回率的调和平均值范围：[0, 1]值越高表示性能越好 |'
- en: '| Balanced accuracy | `metrics.balanced_accuracy_score()` |  Recall + Specificity  _____________ 2 
    | Average of the fraction of positives and negatives that are truly predictedRange:[0,
    1]Higher values mean higher performance |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 平衡准确率 | `metrics.balanced_accuracy_score()` |  召回率 + 特异性  _____________ 2 
    | 真正预测的正负比例的平均值范围：[0, 1]值越高表示性能越好 |'
- en: '| **Matthews correlation** **coefficient** (**MCC**) | `sklearn.metrics.matthews_corrcoef()`
    |  TP * TN − FP * FN  ______________________________   √ __________________________________    (TP
    + FP)(FP + TN)(TN + FN)(FN + TP)   | The numerator aims to maximize diagonal and
    minimize off-diagonal elements of a confusion matrixRange: [ − 1, 1]Higher values
    mean higher performance |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **马修斯相关系数** **系数**（**MCC**） | `sklearn.metrics.matthews_corrcoef()` |  TP
    * TN − FP * FN  ______________________________   √ __________________________________    (TP
    + FP)(FP + TN)(TN + FN)(FN + TP)   | 分子旨在最大化混淆矩阵的对角线元素并最小化非对角线元素范围：[ − 1, 1]值越高表示性能越好
    |'
- en: Table 4.1 – Common metrics for assessing the performance of classification models
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 – 评估分类模型性能的常用指标
- en: One aspect of selecting performance metrics for model selection and reporting
    is their relevance to the target problem. For example, if you are building a model
    for cancer detection, you could aim to maximize recall by maximizing the identification
    of all positive class members (that is, cancer patients) while controlling them
    for precision. This strategy helps you make sure patients with cancer will not
    remain undiagnosed with a deadly disease, although it would be ideal to have a
    model with high precision and recall at the same time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性能指标用于模型选择和报告的一个方面是它们与目标问题的相关性。例如，如果你正在构建一个用于癌症检测的模型，你可以通过最大化识别所有正类成员（即癌症患者）来最大化召回率，同时控制精确度。这种策略可以帮助你确保患有癌症的患者不会因致命疾病而未得到诊断，尽管同时拥有高精确度和召回率的模型会更理想。
- en: Selecting performance metrics depends on whether we care about the true prediction
    of all classes with the same level of importance or whether there are one or more
    classes that would be more important. There are algorithmic ways to enforce the
    model to care more about one or multiple classes. Also, in reporting performance
    and model selection, we need to consider this imbalance between the classes and
    not solely rely on performance metrics that summarize the prediction performance
    of all classes with equal weights.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性能指标取决于我们是否关心所有类别的真实预测具有相同的重要性水平，或者是否有一个或多个类别更为重要。有一些算法方法可以强制模型更加关注一个或多个类别。此外，在报告性能和模型选择时，我们需要考虑类别之间的这种不平衡，而不仅仅依赖于总结所有类别预测性能的等权重性能指标。
- en: We also have to note that we define positive and negative classes in the case
    of binary classification. The data we generate or collect usually does not have
    such labeling. For example, your dataset could have “fraud” versus “not fraud,”
    “cancer” versus “healthy,” or digit names in strings such as “one,” “two,” and
    “three.” So, we need to select the performance metrics according to our definition
    of classes if there are one or more we care more or less about.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须注意，在二元分类的情况下，我们需要定义正类和负类。我们生成或收集的数据通常没有这样的标签。例如，你的数据集可能有“欺诈”与“非欺诈”、“癌症”与“健康”，或者字符串中的数字名称，如“一”、“二”和“三”。因此，如果有我们更关心或更少关心的一个或多个类别，我们需要根据我们对类别的定义来选择性能指标。
- en: The other aspect of selecting performance metrics is their reliability, and
    if they have biases that depend on the data, we use them for training, validation,
    or testing. For example, accuracy, one of the widely used performance metrics
    for classification models, should not be used on an imbalanced dataset. Accuracy
    is defined as the total number of correct predictions over the total number of
    data points (*Table 4.1*). Hence, if a model predicts all data points as the majority
    class, it returns a high value, even if it might not be a good model. *Figure
    4**.2* shows the values of different performance metrics, including accuracy,
    for a model that predicts all data points as negatives. The accuracy of this bad
    model is 0.8 if 80% of the data points in the dataset are negative (*Figure 4**.2*).
    However, alternative performance metrics such as balanced accuracy or **Matthews
    correlation coefficient** (**MCC**) remain unchanged for such a bad model across
    datasets with different positive data point fractions. Data balance is only one
    of the parameters, although an important one, in selecting performance metrics
    for classification models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性能指标的其他方面是它们的可靠性，如果它们有依赖于数据的偏差，我们就会在训练、验证或测试中使用它们。例如，准确率，作为分类模型广泛使用的性能指标之一，不应在不平衡的数据集上使用。准确率定义为正确预测的总数除以数据点的总数（*表4.1*）。因此，如果一个模型将所有数据点预测为多数类，即使它可能不是一个好的模型，它也会返回一个高值。*图4.2*显示了不同性能指标，包括准确率，对于一个将所有数据点预测为负数的模型的值。如果数据集中有80%的数据点是负数，那么这个糟糕模型的准确率是0.8（*图4.2*）。然而，平衡准确率或**马修斯相关系数**（**MCC**）等替代性能指标在具有不同正数据点分数的数据集上对这样一个糟糕的模型来说保持不变。数据平衡只是选择分类模型性能指标时考虑的参数之一，尽管它很重要。
- en: 'Some of the performance metrics have derivatives that better behave in situations
    such as imbalanced data classification. For example, F1 is a widely used metric
    that is not the best choice when dealing with imbalanced data classification (*Figure
    4**.2*):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一些性能指标具有更好的行为，适用于不平衡数据分类等情境。例如，F1是一个广泛使用的指标，但在处理不平衡数据分类时并不是最佳选择（*图4.2*）：
- en: '![Figure 4.2 – Values of common classification metrics across different real
    positive fractions for a model that returns all predictions as negatives](img/B16369_04_02.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – 对于一个将所有预测返回为负数的模型，在不同真实正分数下的常见分类指标值](img/B16369_04_02.jpg)'
- en: Figure 4.2 – Values of common classification metrics across different real positive
    fractions for a model that returns all predictions as negatives
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – 对于一个将所有预测返回为负数的模型，在不同真实正分数下的常见分类指标值
- en: 'However, it has a general form of F β where a parameter, β, is used as a weight
    for increasing the effect of precision according to its mathematical definition.
    You can use the `sklearn.metrics.fbeta_score()` function to calculate this metric
    using true and predicted labels of a list of data points:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它有一个通用的形式Fβ，其中参数β用作根据其数学定义增加精度的效果的权重。你可以使用`sklearn.metrics.fbeta_score()`函数来计算这个指标，使用数据点的真实标签和预测标签：
- en: '![](img/B16369_04_018.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16369_04_018.jpg)'
- en: Probability-based performance metrics
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于概率的性能指标
- en: 'The probability outputs of classification models can be directly used to assess
    the performance of models, without the need for transformation to predict labels.
    An example of such a performance measure is **logistic loss**, known as **log-loss**
    or **cross-entropy loss**, which calculates the total loss over a dataset using
    probabilities of prediction for each data point and its true label, as follows.
    Log-loss is also a loss function that’s used to train classification models:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型的概率输出可以直接用来评估模型性能，无需将预测标签进行转换。这种性能度量的一种例子是**逻辑损失**，也称为**对数损失**或**交叉熵损失**，它使用每个数据点的预测概率及其真实标签来计算数据集上的总损失，如下所示。对数损失也是一个用于训练分类模型的损失函数：
- en: L log(y, p) = − (ylog(p) + (1 − y)log(1 − p))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: L log(y, p) = − (ylog(p) + (1 − y)log(1 − p))
- en: 'There are other types of probability-based performance assessment methods such
    as the **receiver operating characteristic** (**ROC**) curve and the **precision
    recall** (**PR**) curve that consider different cutoffs for transforming probabilities
    into labels to predict the true positive rate, false positive rate, precision,
    and recall. Then, these values, across different cutoffs, get used to generate
    ROC and PR curves (*Figure 4**.3*):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他基于概率的性能评估方法，如**接收者操作特征**（**ROC**）曲线和**精确率召回率**（**PR**）曲线，它们考虑了将概率转换为标签的不同截止点，以预测真正例率、假正例率、精确率和召回率。然后，这些值在不同截止点被用来生成ROC和PR曲线（**图4.3**）：
- en: '![Figure 4.3 – Schematic illustration of ROC and PR curves](img/B16369_04_03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – ROC和PR曲线的示意图](img/B16369_04_03.jpg)'
- en: Figure 4.3 – Schematic illustration of ROC and PR curves
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – ROC和PR曲线的示意图
- en: It is common to use the area under these curves, referred to as ROC-AUC and
    PR-AUC, to assess the performance of classification models. ROC-AUC and PR-AUC
    range from 0 to 1, with 1 being the performance of a perfect model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些曲线下的面积，称为ROC-AUC和PR-AUC，来评估分类模型的性能是很常见的。ROC-AUC和PR-AUC的范围从0到1，其中1表示完美模型的性能。
- en: In *Figure 4**.2*, you saw how some performance metrics return high-performance
    values for a bad model that predicts everything as negative due to data imbalance.
    We can see the extension of this analysis in *Figure 4**.4* for different fractions
    of positive data points among true labels and predicted labels. There is no training
    here and the data points are randomly generated to result in the specified fraction
    of positive data points in each panel of *Figure 4**.4*. The randomly generated
    probabilities are then transformed into labels so that they can be compared with
    true labels using different performance metrics.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.2*中，你看到了一些性能指标如何为预测所有数据点为负的坏模型返回高性能值，这是由于数据不平衡。我们可以在*图4.4*中看到这种分析的扩展，它展示了真正例标签和预测标签中不同正数据点的比例。这里没有训练，数据点是随机生成的，以在*图4.4*的每个面板中产生指定的正数据点比例。然后，随机生成的概率被转换为标签，以便可以使用不同的性能指标与真正例进行比较。
- en: '*Figures 4.4* and *4.5* show different biases in the performance metrics of
    classification models. For example, the median precision of random predictions
    is equal to the fraction of true positive data points, while the median recall
    of random predictions is equal to the fraction of positive predicted labels. You
    can also check the behavior of other performance metrics in *Figures 4.4* and
    *4.5* for different fractions of true or predicted positives:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.4*和*图4.5*显示了分类模型性能指标中的不同偏差。例如，随机预测的中位精确率等于真正例数据点的比例，而随机预测的中位召回率等于预测标签中正标签的比例。你还可以检查*图4.4*和*图4.5*中其他性能指标在不同真正例或预测正例比例下的行为：'
- en: '![Figure 4.4 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 1)](img/Image97668.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 1,000个随机二元预测在1,000个数据点上的性能分布（第一部分）](img/Image97668.jpg)'
- en: Figure 4.4 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 1)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 1,000个随机二元预测在1,000个数据点上的性能分布（第一部分）
- en: '![Figure 4.5 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 2)](img/B16369_04_05.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 1,000个随机二元预测在1,000个数据点上的性能分布（第二部分）](img/B16369_04_05.jpg)'
- en: Figure 4.5 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 2)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 1,000个随机二元预测在1,000个数据点上的性能分布（第二部分）
- en: A combination of ROC-AUC and PR-AUC, or the use of MCC or balanced accuracy,
    are common approaches to have a low bias in performance assessment for classification
    models. But if you know your objectives, such as if you care more about precision
    than recall, then you can choose the performance metrics that would add the necessary
    information for decision-making. But avoid reporting 10 performance metrics for
    your models just for the sake of counting how many of them are better in one model
    versus another.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ROC-AUC和PR-AUC的组合，或使用MCC或平衡准确率，是降低分类模型性能评估偏差的常见方法。但如果你知道你的目标，例如如果你更关心精确率而不是召回率，那么你可以选择添加决策所需必要信息的性能指标。但避免仅仅为了计数模型中哪些性能指标更好而报告10个性能指标。
- en: Regression
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: 'You can assess the performance of your regression models using metrics that
    evaluate either the difference between the continuous predictions of your models
    and true values, such as **Root Mean Squared Error** (**RMSE**), or the agreement
    between the predictions and true values, such as the coefficient of determination
    R 2 (*Table 4.2*). Each of the metrics for regression model performance assessment
    has its assumptions, interpretation, and limitations. For example, R 2 doesn’t
    take into account data dimensionality (that is, the number of features, inputs,
    or independent variables). So, if you have a regression model with multiple features,
    you should use adjusted R 2 instead of R 2\. By adding new features, R 2 could
    increase but might not necessarily correspond to a better model. However, adjusted
    R 2 increases when the new inputs improve model performance more than expectation
    by chance. This is an important consideration, especially if you want to compare
    models with different numbers of inputs for the sample problem:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Python Function** | **Formula** | **Description** |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| **Root Mean Squared** **Error** (**RMSE**)**Mean Squared** **Error** (**MSE**)
    | `sklearn.metrics.mean_squared_error()` | MSE =  1 _ n  ∑ i=1 n (y i −  ˆ y  i) 2RMSE
    = √ _ MSE n: Number of data pointsy i: The true value of the data point, i ˆ y  i:
    The predicted value of the data point, i | Range: [0, ∞)Lower values mean higher
    performance |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| **Mean Absolute** **Error** (**MAE**) | `sklearn.metrics.mean_absolute_error()`
    | MAE =  1 _ n  ∑ i=1 n &#124;y i − ˆ y i&#124; | Range: [0, ∞)Lower values mean
    higher performance |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| Coefficient of determination (R 2) | `sklearn.metrics.r2_score()` | R 2 =
    1 −  ∑ i=1 n  (y i −  ˆ y  i) 2 _ ∑ i=1 n  (y i − y _) 2  ;y _ =  1 _ n  ∑ i=1 n y iy _:
    Mean of the true valuesn: Number of data pointsy i: The true value of the data
    point, i ˆ y  i: The predicted value of the data point, i | Range: [0, 1]Higher
    values mean higher performanceThe proportion of the dependent variable that can
    be explained by the independent variables |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| Adjusted R 2 | Use `sklearn.metrics.r2_score()` to calculate R 2, then calculate
    the adjusted version using its formula. | Adj R 2 = 1 −  (1 − R 2)(n − 1) ___________ n
    − m − 1 n: Number of data pointsm: Number of features | Adjusts to the number
    of featuresCould be greater than 1 or less than 0 if m is close to nHigher values
    mean higher performance |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – Common metrics for assessing the performance of regression models
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Correlation coefficients are also used to report on the performance of regression
    models. Correlation coefficients use the predicted and true continuous values,
    or a transformation of those, and report values commonly between -1 and 1, with
    1 corresponding to an ideal prediction with 100% agreement and -1 with full disagreement
    (*Table 4.3*). Correlation coefficients also have their own assumptions and cannot
    be selected randomly for reporting on the performance of regression models. For
    example, Pearson correlation is a parametric test that assumes a linear relationship
    between predicted and true continuous values, which does not always hold. Alternatively,
    the Spearman and Kendall rank correlations are non-parametric without such assumptions
    behind the relationship of variables or the distribution of each variable in comparison.
    Both the Spearman and Kendall rank correlations rely on the rank of predicted
    and true outputs instead of their actual values:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '| **Correlation Coefficient** | **Python Function** | **Formula** | **Description**
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| Pearson correlation coefficient or Pearson’s *r* | `scipy.stats.pearsonr()`
    | r =  ∑ i=1 n  ( ˆ y  i −  ˆ y  _)(y i − y _)  ________________  √ _________________  ∑ i=1 n  ( ˆ y  i
    −  ˆ y  _) 2 (y i − y _) 2  n: Number of data pointsy i: The true value of the
    data point, iy _: Mean of the true values ˆ y  i: The predicted value of the data
    point, i ˆ y  _: Mean of the predicted values | ParametricLooks for a linear relationship
    between predictions and true valuesRange:[ − 1, 1] |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| Spearman’s rank correlation coefficient or Spearman correlation coefficient
    | `scipy.stats.spearmanr()` | ρ = 1 −  6∑ i=1 n  d i 2 _ n(n 2 − 1) n: Number
    of data pointsd i: The difference between the rank of the data point, i, among
    true values and predicted values | Non-parametricLooks for a monotonic relationship
    between predictions and true valuesRange:[ − 1, 1] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| Kendall rank correlation coefficient or Kendall’s τ coefficient | `scipy.stats.kendalltau()`
    | τ =  C − D __________________  √ ___________________  (C + D + T)(C + D + c)  C:
    Number of concordant pairs (for example, y i > y j and  ˆ y  i >  ˆ y  j; or y i
    < y j and  ˆ y  i <  ˆ y  j)D: Number of discordant pairs (for example, y i >
    y j and  ˆ y  i <  ˆ y  j; or y i < y j and  ˆ y  i >  ˆ y  j)T: Number of ties
    only in predicted valuesU: Number of ties only in true values | Non-parametricLooks
    for a monotonic relationship between predictions and true valuesRange:[ − 1, 1]
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – Common correlation coefficients used for assessing the performance
    of regression models
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clustering is an unsupervised learning approach to identify groupings of data
    points using their feature values. However, to assess the performance of a clustering
    model, we need to have a dataset or example data points with available true labels.
    We don’t use these labels when training the clustering model, as in supervised
    learning; instead, we use them to assess how well similar data points are grouped
    and separated from dissimilar data points. You can find some of the common metrics
    for assessing the performance of clustering models in *Table 4.4*. These metrics
    do not inform you about the quality of the clustering. For example, homogeneity
    tells you if the data points that are clustered together are similar to each other
    while completeness informs you if similar data points in your dataset are clustered
    together. There are also metrics such as V-measure and adjusted mutual information
    that try to assess both qualities at the same time:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Python Function** | **Formula** | **Description** |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| Homogeneity | `sklearn.metrics.homogeneity_score()` | Formula (*1*) provided
    in Rosenberg et al., EMNLP-CoNLL 2007 | Measures how many data points within the
    same clusters are similar to each otherRange: [0, 1]Higher values mean higher
    performance |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| Completeness | `sklearn.metrics.completeness_score()` | Formula (*2*) provided
    in Rosenberg et al., EMNLP-CoNLL 2007 | Measures how similar the data points that
    are clustered together areRange: [0, 1]Higher values mean higher performance |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| V-measure or normalized mutual information score | `sklearn.metrics.v_measure_score()`
    | v =  (1 + β) × h × c ___________ (β × h + c) h: Homogeneityc: Completenessβ:
    Ratio of weight attributed to homogeneity versus completeness | Measures both
    homogeneity and completeness at the same timeRange: [0, 1]Higher values mean higher
    performance |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| Mutual information | `sklearn.metrics.mutual_info_score()` | MI(U, V) = ∑ i=1 &#124;U&#124; ∑ j=1 &#124;V&#124;  &#124;U i
    ∩ V j&#124; _ N  log  &#124;U i ∩ V j&#124; _ &#124;U i&#124;&#124;V j&#124; 
    | Range:[0, 1]Higher values mean higher performance |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| Adjusted mutual information | `sklearn.metrics.adjusted_mutual_info_score()`
    | AMI(U, V)=  [MI(U, V) − E(MI(U, V))]  ________________________   [avg(H(U),
    H(V)) − E(MI(U, V))]  | Range:[0, 1]Higher values mean higher performance |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: Table 4.4 – Common metrics for assessing the performance of clustering models
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed the different performance measures for assessing
    the performance of machine learning models. But there are other important aspects
    of performance assessment to consider, such as data visualization, which we will
    discuss next.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Visualization for performance assessment
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualization is an important tool that helps us not only understand the characteristics
    of our data for modeling but also better assess the performance of our models.
    Visualization could provide complementary information to the aforementioned model
    performance metrics.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Summary metrics are not enough
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are summary statistics such as ROC-AUC and PR-AUC that provide a one-number
    summary of their corresponding curves for assessing the performance of classification
    models. Although these summaries are more reliable than many other metrics such
    as accuracy, they do not completely capture the characteristics of their corresponding
    curves. For example, two different models with different ROC curves can have the
    same or very close ROC-AUCs (*Figure 4**.6*):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Comparison of two arbitrary models with the same ROC-AUCs and
    different ROC curves](img/B16369_04_06.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Comparison of two arbitrary models with the same ROC-AUCs and different
    ROC curves
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Comparing ROC-AUCs alone could result in deciding the equivalence of these models.
    However, they have different ROC curves and in most applications, a red curve
    is preferred over a blue one as it results in a higher true positive rate for
    low false positive rates such as *FPR*1.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Visualizations could be misleading
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the proper visualization technique for your results is the key to analyzing
    the results of your models and reporting their performances. Plotting your data
    without having the model objective in mind could be misleading. For example, you
    might see time series plots such as the one shown in *Figure 4**.7* that overlay
    predictions and real values over time in many blog posts. For such time series
    models, we want predictions and real values to be as close to each other as possible
    for each time point. Although the lines might seem to agree with each other in
    *Figure 4**.7*, there is a two-time unit delay in predictions shown in orange
    compared to the true values shown in blue. This lag in predictions could have
    serious consequences in many applications such as stock price prediction:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.7 – Laying two time series diagrams on top of each other is misleading\
    \ – the orange and blue curves represent predictions and true values for arbitrary\
    \ time series da\uFEFFta](img/B16369_04_07.jpg)"
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Laying two time series diagrams on top of each other is misleading
    – the orange and blue curves represent predictions and true values for arbitrary
    time series data
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Don’t interpret your plots as you wish
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each visualization has its assumptions and right way of interpretation. For
    example, if you want to compare the numerical values of data points in a 2D plot,
    you need to pay attention to the units of the *x* and *y* axes. Or when we use
    **t-distributed Stochastic Neighbor Embedding** (**t-SNE**), a dimensionality
    reduction method designed to help in visualizing high dimensional data in low
    dimensional space, we have to remind ourselves that large distances between data
    points and densities of each group are not representative of the distances and
    densities in the original high-dimensional space (*Figure 4**.8*):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Schematic t-SNE plots showing (A) three groups of data points
    with different distances and (B) two groups with different densities in two dimensions](img/B16369_04_08.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Schematic t-SNE plots showing (A) three groups of data points with
    different distances and (B) two groups with different densities in two dimensions
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: You can use different performance measures to assess if your models are trained
    well and generalizable to new data points, which is the next topic in this chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Bias and variance diagnosis
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We aim to have a model with high performance, or low error, in the training
    set (that is, a low bias model) while keeping the performance high, or error low,
    for new data points (that is, a low variance model). As we don’t have access to
    unseen new data points, we must use validation and test sets to assess the variance
    or generalizability of our models. Model complexity is one of the important factors
    in determining the bias and variance of machine learning models. By increasing
    complexity, we let a model learn more complex patterns in training data that could
    reduce training errors or model bias (*Figure 4**.9*):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Error versus model complexity for (A) high bias, (B) high variance,
    and (C, D) two different cases of low bias and low variance models](img/B16369_04_09.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Error versus model complexity for (A) high bias, (B) high variance,
    and (C, D) two different cases of low bias and low variance models
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: This decrease in error helps build a better model, even for new data points.
    However, this trend changes after a point, and higher complexities could cause
    overfitting or higher variance and lower performance in validation and test sets
    compared to the training set (*Figure 4**.9*). Assessing bias and variance concerning
    parameters such as model complexity or dataset size could help us identify opportunities
    for model performance improvements in training, validation, and test sets.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Four of the possible dependencies of model error in training and validation
    sets to model complexity are shown in *Figure 4**.9*. Although the validation
    error is usually higher than the training error, you might experience a lower
    error in validation sets because of the data points you have in your training
    and validation sets. For example, a multiclass classifier could have a lower error
    in the validation set because of being better at predicting classes that form
    the majority of the data points in the validation set. In such cases, you need
    to investigate the distribution of data points in the training and validation
    sets before reporting performance assessments on training and validation datasets
    and deciding which model to select for production.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s practice a bias and variance analysis. You can find the results of training
    random forest models with different maximum depths on the breast cancer dataset
    from `scikit-learn` (*Figure 4**.10*). The breast cancer data from `scikit-learn`
    is used for training and validating model performance, with 30% of the data randomly
    separated as the validation set and the rest kept as the training set. By increasing
    the maximum depth of the random forest models, log-loss error in the training
    set decreases while balanced accuracy as a measure of model performance increases.
    Validation errors also decrease up to a maximum depth of three and start increasing
    after that as a sign of overfitting. Although error decreases after the maximum
    depth of three, balanced accuracy can still be increased by increasing the maximum
    depth to four and five. The reason is the difference in the definition of log-loss
    based on the probability of predictions and the balanced accuracy on predicted
    labels:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Balanced accuracy (top) and log-loss (bottom) in training and
    validation sets separated from the breast cancer dataset of scikit-learn for a
    random forest model](img/B16369_04_10.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Balanced accuracy (top) and log-loss (bottom) in training and
    validation sets separated from the breast cancer dataset of scikit-learn for a
    random forest model
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for the results shown in *Figure 4**.10*. First, we must import
    the necessary Python libraries and load the breast cancer dataset:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we must split the data into train and test sets and train multiple random
    forest models with different maximum depths allowed for their decision trees:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that you’ve learned about the concepts of bias and variance, we will introduce
    different techniques that you can use to validate your models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Model validation strategy
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To validate our models, we can use separate datasets or split the dataset we
    have into training and validation sets using different techniques, as explained
    in *Table 4.5* and illustrated in *Figure 4**.11*. In cross-validation strategies,
    we split the data into different subsets, then the performance score or error
    for each subset, since the validation set is calculated using the predictions
    of the model trained on the rest of the data. Then, we can use the mean of the
    performance across the subsets as the cross-validation performance:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Techniques for separating the validation and training sets
    within one dataset](img/B16369_04_11.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Techniques for separating the validation and training sets within
    one dataset
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Each of these validation techniques has its advantages and limitations. Using
    cross-validation techniques instead of hold-out validation has the benefit of
    covering all or the majority of the data in at least one validation subset. Stratified
    k-fold **cross-validation** (**CV**) is also a better choice compared to k-fold
    CV or leave-one-out CV as it keeps the same balance across the validation subsets
    as in the whole dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification or regression hold-out or CV methods don’t work for time
    series data. As the order of data points is important in time series data, shuffling
    the data or randomly selecting data points is not suitable in the process of training
    and validation subset selection. Randomly selecting data points for validation
    and training sets results in models trained on some future data points to predict
    the outcome in the past, which is not the intention of time series models. Rolling
    or time series CV is an appropriate validation technique for time series models
    as it rolls the validation set over time instead of randomly selecting the data
    point (*Table 4.5*):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '| **Validation Method** | **Python Function** | **Description** |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| Hold-out validation | `sklearn.model_selection.train_test_split()` | This
    splits all the data into one training and one validation set. 20-40% of the data
    commonly gets selected as a validation set but this percentage could be lower
    for large datasets. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
- en: '| k-fold cross-validation | `sklearn.model_selection.KFold()` | This method
    splits the data into *k* different subsets and uses each as a validation set and
    the remaining data points as a training set. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: '| Stratified k-fold cross-validation | `sklearn.model_selection.StratifiedKFold()`
    | This is similar to k-fold CV but preserves the percentage of samples for each
    class, as in the whole dataset, in each of the *k* subsets. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
- en: '| **Leave-p-out** **cross-validation** (**LOCV**) | `sklearn.model_selection.LeavePOut()`
    | This is similar to k-fold CV, with each subset having *p* data points instead
    of splitting the dataset into *k* subsets. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
- en: '| **Leave-one-out cross-validation** (**LOOCV**) | `sklearn.model_selection.LeaveOneOut()`
    | This works exactly as k-fold CV, with *k* being equal to the total number of
    data points. Each validation subset has one data point that uses LOOCV. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
- en: '| Monte Carlo or random permutation cross-validation | `sklearn.model_selection.ShuffleSplit()`
    | This splits the data randomly into a training and a validation set, similar
    to hold-out validation, and repeats this process many times. More iterations result
    in a better assessment of performance, although it increases the computational
    cost of validation. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| Rolling or time-based cross-validation | `sklearn.model_selection.TimeSeriesSplit()`
    | A small subset of data gets selected as the training set and a smaller subset
    gets selected as the validation set. The validation set gets shifted in time and
    the data points that were previously considered for validation get added to the
    training set. |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: Table 4.5 – Common validation techniques that use one dataset
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Here is the Python implementation of hold-out, k-fold CV, and stratified k-fold
    CV to help you start using these methods in your projects.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the necessary libraries, load the breast cancer dataset,
    and initialize a random forest model:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we must train and validate different random forest models using each
    validation technique:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Error analysis is another technique you can benefit from when seeking to develop
    reliable machine learning models, which we will introduce next.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Error analysis
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use error analysis to find common characteristics between data points
    with incorrectly predicted outputs. For example, the majority of images that are
    misclassified in image classification models might have darker backgrounds, or
    a disease diagnostic model might have lower performance for men compared to women.
    Although manually investigating the data points with incorrect predictions could
    be insightful, this process could cost you a lot of time. Instead, you can try
    to reduce the cost programmatically.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Here, we want to practice with a simple case of error analysis in which the
    number of misclassified data points from each class is counted for a random forest
    model that’s been trained and validated using a 5-fold CV. For error analysis,
    only predictions for validation subsets are used.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the necessary Python libraries and load the wine dataset:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we must initialize a random forest model and 5-fold CV object:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, for each fold, we must train a random forest model using all the data,
    excluding that fold, and validate the model on the chunk of data considered in
    that fold:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This analysis shows that class 1 has nine misclassified data points, while classes
    2 and 0 have only three and two misclassified examples, respectively. This simple
    example helps you start practicing with error analysis. But error analysis is
    not only about identifying misclassification count per class. You can also identify
    patterns in feature values for misclassified examples by comparing feature values
    between misclassified data points and the whole dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: There are other important factors, such as computational cost and time, that
    also need to be considered when developing machine learning models. Here, we will
    briefly talk about this important topic, but the details are beyond the scope
    of this book.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Beyond performance
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Paying any price for improving the performance of machine learning models is
    not the objective of modeling as part of bigger pipelines at the industrial level.
    Increasing the performance of models by a tenth of a percent could help you win
    machine learning competitions or publish papers by beating state-of-the-art models.
    But not all improvements result in models worth deploying to production. An example
    of such efforts, which has been common in machine learning competitions, is model
    stacking. Model stacking is about using the output of multiple models to train
    a secondary model, which could increase the cost of inference by orders of magnitude.
    Python’s implementation of stacking of the logistic regression, k-nearest neighbor,
    random forest, support vector machine, and XGBoost classification models on the
    breast cancer dataset from `scikit-learn` is shown here. A secondary logistic
    regression model uses predictions of each of these primary models as input to
    come up with the final prediction of the stacked model:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this example, the performance of the stacked model is less than 1% better
    than the best individual model, while the inference time could be more than 20
    times higher, depending on the hardware and software configurations you have.
    Although inference time could be less important, such as in the case of disease
    diagnosis or scientific discoveries, it could be of critical importance if your
    model needs to provide the output in real time, such as in recommending products
    to consumers. So, you need to consider other factors, such as inference or prediction
    time, when you’re deciding to bring a model into production or planning for new
    expensive computational experiments or data collection.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Although inference time or other factors need to be considered in your model
    building and selection, it doesn’t mean that you cannot use complex models for
    real-time output generation. Depending on the application and your budget, you
    can use better configurations, for example, on your cloud-based system, to eliminate
    the issues that arise due to higher performance but slower models.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about different performance and error metrics for
    supervised and unsupervised learning models. We discussed the limitations of each
    metric and the right way of interpreting them. We also reviewed bias and variance
    analysis and different validation and cross-validation techniques for assessing
    the generalizability of models. We also presented error analysis as an approach
    for detecting the components of a model that contribute to model overfitting.
    We went through Python code examples for these topics to help you practice with
    them and be able to quickly use them in your projects.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will review techniques to improve the generalizability
    of machine learning models, such as synthetic data addition to training data,
    removing data inconsistencies, and regularization methods.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classifier is designed to identify if patients of a clinic need to go through
    the rest of the diagnostic steps after the first round of testing. What classification
    metric would be more or less appropriate? Why?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A classifier is designed to assess the risk of investment for different investment
    options, for a specific amount of money, and is going to be used to suggest investment
    opportunities to your clients. What classification metric would be more or less
    appropriate? Why?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the calculated ROC-AUCs of two binary classification models on the same validation
    set are the same, does it mean that the models are the same?
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If model A has a lower log-loss compared to model B on the same test set, does
    it always mean that the MCC of model A is also higher than model B?
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If model A has a higher R 2 on the same number of data points compared to model
    B, could we claim that model A is better than model B? How does the number of
    features affect our comparison between the two models?
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If model A has higher performance than model B, does it mean that choosing model
    A is the right one to bring into production?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rosenberg, Andrew, and Julia Hirschberg. *V-measure: A conditional entropy-based
    external cluster evaluation measure*. Proceedings of the 2007 joint conference
    on empirical methods in natural language processing and computational natural
    language learning (EMNLP-CoNLL). 2007.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinh, Nguyen Xuan, Julien Epps, and James Bailey. *Information theoretic measures
    for clusterings comparison: is a correction for chance necessary?* Proceedings
    of the 26th annual international conference on machine learning. 2009.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andrew Ng, *Stanford CS229: Machine Learning Course*, Autumn 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van der Maaten, Laurens, and Geoffrey Hinton. *Visualizing data using t-SNE*.
    Journal of machine learning research 9.11 (2008).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McInnes, Leland, John Healy, and James Melville. *Umap: Uniform manifold approximation
    and projection for dimension reduction*. arXiv preprint arXiv:1802.03426 (2018).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
