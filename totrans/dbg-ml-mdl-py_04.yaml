- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting Performance and Efficiency Issues in Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main objectives we must keep in mind is how to build a high-performance
    machine learning model with minimal errors on new data we want to use the model
    for. In this chapter, you will learn how to properly assess the performance of
    your models and identify opportunities for decreasing their errors.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter includes many figures and code examples to help you better understand
    these concepts and start benefiting from them in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance and error assessment measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias and variance diagnosis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model validation strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about how to assess the performance
    of machine learning models and the benefits, limitations, and wrong usage of visualization
    in different machine learning problems. You will have also learned about bias
    and variance diagnosis and error analysis to help you identify opportunities so
    that you can improve your models.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` >= 1.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` >= 1.22.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` >= 1.4.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` >= 3.5.3'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collections` >= 3.8.16'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xgboost` >= 1.7.5'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You should have basic knowledge of model validation and testing, as well as
    classification, regression, and clustering in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter04).
  prefs: []
  type: TYPE_NORMAL
- en: Performance and error assessment measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The metrics we use to assess the performance and calculate errors in our models,
    and how we interpret their values, determine the models we select, the decisions
    we make to improve a component of our machine learning life cycle, and determine
    if we have a reliable model to bring into production. Although many performance
    metrics can be used in one line of Python code to calculate errors and performance,
    we shouldn’t blindly use them or try to improve our performance reports by implementing
    many of them together without knowing their limitations and how to correctly interpret
    them. In this section, we will talk about metrics for assessing the performance
    of classification, regression, and clustering models.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each classification model, either binary or multi-class, returns the probability
    of predictions, a number between 0 and 1, which then gets transformed into class
    labels. There are two major categories of performance metrics: **label-based performance
    metrics**, which rely on predicted labels, and **probability-based performance
    metrics**, which use the probability of predictions for performance or error calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: Label-based performance metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The predicted probabilities of classification models get transformed into class
    labels by the Python classes we use for modeling. We can then use a confusion
    matrix, as shown in *Figure 4**.1*, to identify four groups of data points, including
    **true positives** (**TPs**), **false positives** (**FPs**), **false negatives**
    (**FNs**), and **true negatives** (**TNs**) for binary classification problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Confusion matrix for binary classification](img/B16369_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Confusion matrix for binary classification
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `sklearn.metrics.confusion_matrix()` to extract these four groups
    of data points and then calculate performance metrics such as specificity according
    to the following mathematical definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16369_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the Python implementation of extracting specificity, precision, and
    recall from a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can calculate other performance metrics, such as precision and recall, using
    TP, TN, FP, and FN, which have been extracted from the confusion matrix, or directly
    use functions available in Python (*Table 4.1*). In addition to the Python functions
    to calculate some of the common performance metrics for classification models,
    you can also find the mathematical definitions of the metrics and their interpretations
    in *Table 4.1*. This extra information will help you understand how to interpret
    each of these metrics and when to use them:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Python** **Function** | **Formula** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | `metrics.accuracy_score()` |  TP + TN _ n n: Number of data points
    | Number of correct predictions over the total number of data pointsRange: [0,
    1]Higher values mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: '| **Precision or positive predictive** **value** (**PPV**) | `metrics.precision_score()`
    |  TP _ TP + FP  | Fraction of predicted positives that are positiveRange:[0,
    1]Higher values mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: '| Recall, sensitivity, or **true positive** **rate** (**TPR**) | `metrics.recall_score()`
    |  TP _ TP + FN  | Fraction of positives that are predicted as positiveRange:[0,
    1]Higher values mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: '| F1 score and its derivatives | `metrics.f1_score()` |  Precision * Recall  ____________ Precision
    + Recall _ 2   | The harmonic mean of precision and recallRange:[0, 1]Higher values
    mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: '| Balanced accuracy | `metrics.balanced_accuracy_score()` |  Recall + Specificity  _____________ 2 
    | Average of the fraction of positives and negatives that are truly predictedRange:[0,
    1]Higher values mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: '| **Matthews correlation** **coefficient** (**MCC**) | `sklearn.metrics.matthews_corrcoef()`
    |  TP * TN − FP * FN  ______________________________   √ __________________________________    (TP
    + FP)(FP + TN)(TN + FN)(FN + TP)   | The numerator aims to maximize diagonal and
    minimize off-diagonal elements of a confusion matrixRange: [ − 1, 1]Higher values
    mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Common metrics for assessing the performance of classification models
  prefs: []
  type: TYPE_NORMAL
- en: One aspect of selecting performance metrics for model selection and reporting
    is their relevance to the target problem. For example, if you are building a model
    for cancer detection, you could aim to maximize recall by maximizing the identification
    of all positive class members (that is, cancer patients) while controlling them
    for precision. This strategy helps you make sure patients with cancer will not
    remain undiagnosed with a deadly disease, although it would be ideal to have a
    model with high precision and recall at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting performance metrics depends on whether we care about the true prediction
    of all classes with the same level of importance or whether there are one or more
    classes that would be more important. There are algorithmic ways to enforce the
    model to care more about one or multiple classes. Also, in reporting performance
    and model selection, we need to consider this imbalance between the classes and
    not solely rely on performance metrics that summarize the prediction performance
    of all classes with equal weights.
  prefs: []
  type: TYPE_NORMAL
- en: We also have to note that we define positive and negative classes in the case
    of binary classification. The data we generate or collect usually does not have
    such labeling. For example, your dataset could have “fraud” versus “not fraud,”
    “cancer” versus “healthy,” or digit names in strings such as “one,” “two,” and
    “three.” So, we need to select the performance metrics according to our definition
    of classes if there are one or more we care more or less about.
  prefs: []
  type: TYPE_NORMAL
- en: The other aspect of selecting performance metrics is their reliability, and
    if they have biases that depend on the data, we use them for training, validation,
    or testing. For example, accuracy, one of the widely used performance metrics
    for classification models, should not be used on an imbalanced dataset. Accuracy
    is defined as the total number of correct predictions over the total number of
    data points (*Table 4.1*). Hence, if a model predicts all data points as the majority
    class, it returns a high value, even if it might not be a good model. *Figure
    4**.2* shows the values of different performance metrics, including accuracy,
    for a model that predicts all data points as negatives. The accuracy of this bad
    model is 0.8 if 80% of the data points in the dataset are negative (*Figure 4**.2*).
    However, alternative performance metrics such as balanced accuracy or **Matthews
    correlation coefficient** (**MCC**) remain unchanged for such a bad model across
    datasets with different positive data point fractions. Data balance is only one
    of the parameters, although an important one, in selecting performance metrics
    for classification models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the performance metrics have derivatives that better behave in situations
    such as imbalanced data classification. For example, F1 is a widely used metric
    that is not the best choice when dealing with imbalanced data classification (*Figure
    4**.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Values of common classification metrics across different real
    positive fractions for a model that returns all predictions as negatives](img/B16369_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Values of common classification metrics across different real positive
    fractions for a model that returns all predictions as negatives
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it has a general form of F β where a parameter, β, is used as a weight
    for increasing the effect of precision according to its mathematical definition.
    You can use the `sklearn.metrics.fbeta_score()` function to calculate this metric
    using true and predicted labels of a list of data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16369_04_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Probability-based performance metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The probability outputs of classification models can be directly used to assess
    the performance of models, without the need for transformation to predict labels.
    An example of such a performance measure is **logistic loss**, known as **log-loss**
    or **cross-entropy loss**, which calculates the total loss over a dataset using
    probabilities of prediction for each data point and its true label, as follows.
    Log-loss is also a loss function that’s used to train classification models:'
  prefs: []
  type: TYPE_NORMAL
- en: L log(y, p) = − (ylog(p) + (1 − y)log(1 − p))
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other types of probability-based performance assessment methods such
    as the **receiver operating characteristic** (**ROC**) curve and the **precision
    recall** (**PR**) curve that consider different cutoffs for transforming probabilities
    into labels to predict the true positive rate, false positive rate, precision,
    and recall. Then, these values, across different cutoffs, get used to generate
    ROC and PR curves (*Figure 4**.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Schematic illustration of ROC and PR curves](img/B16369_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Schematic illustration of ROC and PR curves
  prefs: []
  type: TYPE_NORMAL
- en: It is common to use the area under these curves, referred to as ROC-AUC and
    PR-AUC, to assess the performance of classification models. ROC-AUC and PR-AUC
    range from 0 to 1, with 1 being the performance of a perfect model.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.2*, you saw how some performance metrics return high-performance
    values for a bad model that predicts everything as negative due to data imbalance.
    We can see the extension of this analysis in *Figure 4**.4* for different fractions
    of positive data points among true labels and predicted labels. There is no training
    here and the data points are randomly generated to result in the specified fraction
    of positive data points in each panel of *Figure 4**.4*. The randomly generated
    probabilities are then transformed into labels so that they can be compared with
    true labels using different performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figures 4.4* and *4.5* show different biases in the performance metrics of
    classification models. For example, the median precision of random predictions
    is equal to the fraction of true positive data points, while the median recall
    of random predictions is equal to the fraction of positive predicted labels. You
    can also check the behavior of other performance metrics in *Figures 4.4* and
    *4.5* for different fractions of true or predicted positives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 1)](img/Image97668.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 1)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 2)](img/B16369_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Distribution of performance of 1,000 random binary predictions
    on 1,000 data points (part 2)
  prefs: []
  type: TYPE_NORMAL
- en: A combination of ROC-AUC and PR-AUC, or the use of MCC or balanced accuracy,
    are common approaches to have a low bias in performance assessment for classification
    models. But if you know your objectives, such as if you care more about precision
    than recall, then you can choose the performance metrics that would add the necessary
    information for decision-making. But avoid reporting 10 performance metrics for
    your models just for the sake of counting how many of them are better in one model
    versus another.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can assess the performance of your regression models using metrics that
    evaluate either the difference between the continuous predictions of your models
    and true values, such as **Root Mean Squared Error** (**RMSE**), or the agreement
    between the predictions and true values, such as the coefficient of determination
    R 2 (*Table 4.2*). Each of the metrics for regression model performance assessment
    has its assumptions, interpretation, and limitations. For example, R 2 doesn’t
    take into account data dimensionality (that is, the number of features, inputs,
    or independent variables). So, if you have a regression model with multiple features,
    you should use adjusted R 2 instead of R 2\. By adding new features, R 2 could
    increase but might not necessarily correspond to a better model. However, adjusted
    R 2 increases when the new inputs improve model performance more than expectation
    by chance. This is an important consideration, especially if you want to compare
    models with different numbers of inputs for the sample problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Python Function** | **Formula** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| **Root Mean Squared** **Error** (**RMSE**)**Mean Squared** **Error** (**MSE**)
    | `sklearn.metrics.mean_squared_error()` | MSE =  1 _ n  ∑ i=1 n (y i −  ˆ y  i) 2RMSE
    = √ _ MSE n: Number of data pointsy i: The true value of the data point, i ˆ y  i:
    The predicted value of the data point, i | Range: [0, ∞)Lower values mean higher
    performance |'
  prefs: []
  type: TYPE_TB
- en: '| **Mean Absolute** **Error** (**MAE**) | `sklearn.metrics.mean_absolute_error()`
    | MAE =  1 _ n  ∑ i=1 n &#124;y i − ˆ y i&#124; | Range: [0, ∞)Lower values mean
    higher performance |'
  prefs: []
  type: TYPE_TB
- en: '| Coefficient of determination (R 2) | `sklearn.metrics.r2_score()` | R 2 =
    1 −  ∑ i=1 n  (y i −  ˆ y  i) 2 _ ∑ i=1 n  (y i − y _) 2  ;y _ =  1 _ n  ∑ i=1 n y iy _:
    Mean of the true valuesn: Number of data pointsy i: The true value of the data
    point, i ˆ y  i: The predicted value of the data point, i | Range: [0, 1]Higher
    values mean higher performanceThe proportion of the dependent variable that can
    be explained by the independent variables |'
  prefs: []
  type: TYPE_TB
- en: '| Adjusted R 2 | Use `sklearn.metrics.r2_score()` to calculate R 2, then calculate
    the adjusted version using its formula. | Adj R 2 = 1 −  (1 − R 2)(n − 1) ___________ n
    − m − 1 n: Number of data pointsm: Number of features | Adjusts to the number
    of featuresCould be greater than 1 or less than 0 if m is close to nHigher values
    mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – Common metrics for assessing the performance of regression models
  prefs: []
  type: TYPE_NORMAL
- en: 'Correlation coefficients are also used to report on the performance of regression
    models. Correlation coefficients use the predicted and true continuous values,
    or a transformation of those, and report values commonly between -1 and 1, with
    1 corresponding to an ideal prediction with 100% agreement and -1 with full disagreement
    (*Table 4.3*). Correlation coefficients also have their own assumptions and cannot
    be selected randomly for reporting on the performance of regression models. For
    example, Pearson correlation is a parametric test that assumes a linear relationship
    between predicted and true continuous values, which does not always hold. Alternatively,
    the Spearman and Kendall rank correlations are non-parametric without such assumptions
    behind the relationship of variables or the distribution of each variable in comparison.
    Both the Spearman and Kendall rank correlations rely on the rank of predicted
    and true outputs instead of their actual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Correlation Coefficient** | **Python Function** | **Formula** | **Description**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pearson correlation coefficient or Pearson’s *r* | `scipy.stats.pearsonr()`
    | r =  ∑ i=1 n  ( ˆ y  i −  ˆ y  _)(y i − y _)  ________________  √ _________________  ∑ i=1 n  ( ˆ y  i
    −  ˆ y  _) 2 (y i − y _) 2  n: Number of data pointsy i: The true value of the
    data point, iy _: Mean of the true values ˆ y  i: The predicted value of the data
    point, i ˆ y  _: Mean of the predicted values | ParametricLooks for a linear relationship
    between predictions and true valuesRange:[ − 1, 1] |'
  prefs: []
  type: TYPE_TB
- en: '| Spearman’s rank correlation coefficient or Spearman correlation coefficient
    | `scipy.stats.spearmanr()` | ρ = 1 −  6∑ i=1 n  d i 2 _ n(n 2 − 1) n: Number
    of data pointsd i: The difference between the rank of the data point, i, among
    true values and predicted values | Non-parametricLooks for a monotonic relationship
    between predictions and true valuesRange:[ − 1, 1] |'
  prefs: []
  type: TYPE_TB
- en: '| Kendall rank correlation coefficient or Kendall’s τ coefficient | `scipy.stats.kendalltau()`
    | τ =  C − D __________________  √ ___________________  (C + D + T)(C + D + c)  C:
    Number of concordant pairs (for example, y i > y j and  ˆ y  i >  ˆ y  j; or y i
    < y j and  ˆ y  i <  ˆ y  j)D: Number of discordant pairs (for example, y i >
    y j and  ˆ y  i <  ˆ y  j; or y i < y j and  ˆ y  i >  ˆ y  j)T: Number of ties
    only in predicted valuesU: Number of ties only in true values | Non-parametricLooks
    for a monotonic relationship between predictions and true valuesRange:[ − 1, 1]
    |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – Common correlation coefficients used for assessing the performance
    of regression models
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Clustering is an unsupervised learning approach to identify groupings of data
    points using their feature values. However, to assess the performance of a clustering
    model, we need to have a dataset or example data points with available true labels.
    We don’t use these labels when training the clustering model, as in supervised
    learning; instead, we use them to assess how well similar data points are grouped
    and separated from dissimilar data points. You can find some of the common metrics
    for assessing the performance of clustering models in *Table 4.4*. These metrics
    do not inform you about the quality of the clustering. For example, homogeneity
    tells you if the data points that are clustered together are similar to each other
    while completeness informs you if similar data points in your dataset are clustered
    together. There are also metrics such as V-measure and adjusted mutual information
    that try to assess both qualities at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Python Function** | **Formula** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Homogeneity | `sklearn.metrics.homogeneity_score()` | Formula (*1*) provided
    in Rosenberg et al., EMNLP-CoNLL 2007 | Measures how many data points within the
    same clusters are similar to each otherRange: [0, 1]Higher values mean higher
    performance |'
  prefs: []
  type: TYPE_TB
- en: '| Completeness | `sklearn.metrics.completeness_score()` | Formula (*2*) provided
    in Rosenberg et al., EMNLP-CoNLL 2007 | Measures how similar the data points that
    are clustered together areRange: [0, 1]Higher values mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: '| V-measure or normalized mutual information score | `sklearn.metrics.v_measure_score()`
    | v =  (1 + β) × h × c ___________ (β × h + c) h: Homogeneityc: Completenessβ:
    Ratio of weight attributed to homogeneity versus completeness | Measures both
    homogeneity and completeness at the same timeRange: [0, 1]Higher values mean higher
    performance |'
  prefs: []
  type: TYPE_TB
- en: '| Mutual information | `sklearn.metrics.mutual_info_score()` | MI(U, V) = ∑ i=1 &#124;U&#124; ∑ j=1 &#124;V&#124;  &#124;U i
    ∩ V j&#124; _ N  log  &#124;U i ∩ V j&#124; _ &#124;U i&#124;&#124;V j&#124; 
    | Range:[0, 1]Higher values mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: '| Adjusted mutual information | `sklearn.metrics.adjusted_mutual_info_score()`
    | AMI(U, V)=  [MI(U, V) − E(MI(U, V))]  ________________________   [avg(H(U),
    H(V)) − E(MI(U, V))]  | Range:[0, 1]Higher values mean higher performance |'
  prefs: []
  type: TYPE_TB
- en: Table 4.4 – Common metrics for assessing the performance of clustering models
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed the different performance measures for assessing
    the performance of machine learning models. But there are other important aspects
    of performance assessment to consider, such as data visualization, which we will
    discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization for performance assessment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualization is an important tool that helps us not only understand the characteristics
    of our data for modeling but also better assess the performance of our models.
    Visualization could provide complementary information to the aforementioned model
    performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Summary metrics are not enough
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are summary statistics such as ROC-AUC and PR-AUC that provide a one-number
    summary of their corresponding curves for assessing the performance of classification
    models. Although these summaries are more reliable than many other metrics such
    as accuracy, they do not completely capture the characteristics of their corresponding
    curves. For example, two different models with different ROC curves can have the
    same or very close ROC-AUCs (*Figure 4**.6*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Comparison of two arbitrary models with the same ROC-AUCs and
    different ROC curves](img/B16369_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Comparison of two arbitrary models with the same ROC-AUCs and different
    ROC curves
  prefs: []
  type: TYPE_NORMAL
- en: Comparing ROC-AUCs alone could result in deciding the equivalence of these models.
    However, they have different ROC curves and in most applications, a red curve
    is preferred over a blue one as it results in a higher true positive rate for
    low false positive rates such as *FPR*1.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizations could be misleading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the proper visualization technique for your results is the key to analyzing
    the results of your models and reporting their performances. Plotting your data
    without having the model objective in mind could be misleading. For example, you
    might see time series plots such as the one shown in *Figure 4**.7* that overlay
    predictions and real values over time in many blog posts. For such time series
    models, we want predictions and real values to be as close to each other as possible
    for each time point. Although the lines might seem to agree with each other in
    *Figure 4**.7*, there is a two-time unit delay in predictions shown in orange
    compared to the true values shown in blue. This lag in predictions could have
    serious consequences in many applications such as stock price prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.7 – Laying two time series diagrams on top of each other is misleading\
    \ – the orange and blue curves represent predictions and true values for arbitrary\
    \ time series da\uFEFFta](img/B16369_04_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Laying two time series diagrams on top of each other is misleading
    – the orange and blue curves represent predictions and true values for arbitrary
    time series data
  prefs: []
  type: TYPE_NORMAL
- en: Don’t interpret your plots as you wish
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each visualization has its assumptions and right way of interpretation. For
    example, if you want to compare the numerical values of data points in a 2D plot,
    you need to pay attention to the units of the *x* and *y* axes. Or when we use
    **t-distributed Stochastic Neighbor Embedding** (**t-SNE**), a dimensionality
    reduction method designed to help in visualizing high dimensional data in low
    dimensional space, we have to remind ourselves that large distances between data
    points and densities of each group are not representative of the distances and
    densities in the original high-dimensional space (*Figure 4**.8*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Schematic t-SNE plots showing (A) three groups of data points
    with different distances and (B) two groups with different densities in two dimensions](img/B16369_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Schematic t-SNE plots showing (A) three groups of data points with
    different distances and (B) two groups with different densities in two dimensions
  prefs: []
  type: TYPE_NORMAL
- en: You can use different performance measures to assess if your models are trained
    well and generalizable to new data points, which is the next topic in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Bias and variance diagnosis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We aim to have a model with high performance, or low error, in the training
    set (that is, a low bias model) while keeping the performance high, or error low,
    for new data points (that is, a low variance model). As we don’t have access to
    unseen new data points, we must use validation and test sets to assess the variance
    or generalizability of our models. Model complexity is one of the important factors
    in determining the bias and variance of machine learning models. By increasing
    complexity, we let a model learn more complex patterns in training data that could
    reduce training errors or model bias (*Figure 4**.9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Error versus model complexity for (A) high bias, (B) high variance,
    and (C, D) two different cases of low bias and low variance models](img/B16369_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Error versus model complexity for (A) high bias, (B) high variance,
    and (C, D) two different cases of low bias and low variance models
  prefs: []
  type: TYPE_NORMAL
- en: This decrease in error helps build a better model, even for new data points.
    However, this trend changes after a point, and higher complexities could cause
    overfitting or higher variance and lower performance in validation and test sets
    compared to the training set (*Figure 4**.9*). Assessing bias and variance concerning
    parameters such as model complexity or dataset size could help us identify opportunities
    for model performance improvements in training, validation, and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Four of the possible dependencies of model error in training and validation
    sets to model complexity are shown in *Figure 4**.9*. Although the validation
    error is usually higher than the training error, you might experience a lower
    error in validation sets because of the data points you have in your training
    and validation sets. For example, a multiclass classifier could have a lower error
    in the validation set because of being better at predicting classes that form
    the majority of the data points in the validation set. In such cases, you need
    to investigate the distribution of data points in the training and validation
    sets before reporting performance assessments on training and validation datasets
    and deciding which model to select for production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s practice a bias and variance analysis. You can find the results of training
    random forest models with different maximum depths on the breast cancer dataset
    from `scikit-learn` (*Figure 4**.10*). The breast cancer data from `scikit-learn`
    is used for training and validating model performance, with 30% of the data randomly
    separated as the validation set and the rest kept as the training set. By increasing
    the maximum depth of the random forest models, log-loss error in the training
    set decreases while balanced accuracy as a measure of model performance increases.
    Validation errors also decrease up to a maximum depth of three and start increasing
    after that as a sign of overfitting. Although error decreases after the maximum
    depth of three, balanced accuracy can still be increased by increasing the maximum
    depth to four and five. The reason is the difference in the definition of log-loss
    based on the probability of predictions and the balanced accuracy on predicted
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Balanced accuracy (top) and log-loss (bottom) in training and
    validation sets separated from the breast cancer dataset of scikit-learn for a
    random forest model](img/B16369_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Balanced accuracy (top) and log-loss (bottom) in training and
    validation sets separated from the breast cancer dataset of scikit-learn for a
    random forest model
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for the results shown in *Figure 4**.10*. First, we must import
    the necessary Python libraries and load the breast cancer dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must split the data into train and test sets and train multiple random
    forest models with different maximum depths allowed for their decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now that you’ve learned about the concepts of bias and variance, we will introduce
    different techniques that you can use to validate your models.
  prefs: []
  type: TYPE_NORMAL
- en: Model validation strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To validate our models, we can use separate datasets or split the dataset we
    have into training and validation sets using different techniques, as explained
    in *Table 4.5* and illustrated in *Figure 4**.11*. In cross-validation strategies,
    we split the data into different subsets, then the performance score or error
    for each subset, since the validation set is calculated using the predictions
    of the model trained on the rest of the data. Then, we can use the mean of the
    performance across the subsets as the cross-validation performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Techniques for separating the validation and training sets
    within one dataset](img/B16369_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Techniques for separating the validation and training sets within
    one dataset
  prefs: []
  type: TYPE_NORMAL
- en: Each of these validation techniques has its advantages and limitations. Using
    cross-validation techniques instead of hold-out validation has the benefit of
    covering all or the majority of the data in at least one validation subset. Stratified
    k-fold **cross-validation** (**CV**) is also a better choice compared to k-fold
    CV or leave-one-out CV as it keeps the same balance across the validation subsets
    as in the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification or regression hold-out or CV methods don’t work for time
    series data. As the order of data points is important in time series data, shuffling
    the data or randomly selecting data points is not suitable in the process of training
    and validation subset selection. Randomly selecting data points for validation
    and training sets results in models trained on some future data points to predict
    the outcome in the past, which is not the intention of time series models. Rolling
    or time series CV is an appropriate validation technique for time series models
    as it rolls the validation set over time instead of randomly selecting the data
    point (*Table 4.5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Validation Method** | **Python Function** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Hold-out validation | `sklearn.model_selection.train_test_split()` | This
    splits all the data into one training and one validation set. 20-40% of the data
    commonly gets selected as a validation set but this percentage could be lower
    for large datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| k-fold cross-validation | `sklearn.model_selection.KFold()` | This method
    splits the data into *k* different subsets and uses each as a validation set and
    the remaining data points as a training set. |'
  prefs: []
  type: TYPE_TB
- en: '| Stratified k-fold cross-validation | `sklearn.model_selection.StratifiedKFold()`
    | This is similar to k-fold CV but preserves the percentage of samples for each
    class, as in the whole dataset, in each of the *k* subsets. |'
  prefs: []
  type: TYPE_TB
- en: '| **Leave-p-out** **cross-validation** (**LOCV**) | `sklearn.model_selection.LeavePOut()`
    | This is similar to k-fold CV, with each subset having *p* data points instead
    of splitting the dataset into *k* subsets. |'
  prefs: []
  type: TYPE_TB
- en: '| **Leave-one-out cross-validation** (**LOOCV**) | `sklearn.model_selection.LeaveOneOut()`
    | This works exactly as k-fold CV, with *k* being equal to the total number of
    data points. Each validation subset has one data point that uses LOOCV. |'
  prefs: []
  type: TYPE_TB
- en: '| Monte Carlo or random permutation cross-validation | `sklearn.model_selection.ShuffleSplit()`
    | This splits the data randomly into a training and a validation set, similar
    to hold-out validation, and repeats this process many times. More iterations result
    in a better assessment of performance, although it increases the computational
    cost of validation. |'
  prefs: []
  type: TYPE_TB
- en: '| Rolling or time-based cross-validation | `sklearn.model_selection.TimeSeriesSplit()`
    | A small subset of data gets selected as the training set and a smaller subset
    gets selected as the validation set. The validation set gets shifted in time and
    the data points that were previously considered for validation get added to the
    training set. |'
  prefs: []
  type: TYPE_TB
- en: Table 4.5 – Common validation techniques that use one dataset
  prefs: []
  type: TYPE_NORMAL
- en: Here is the Python implementation of hold-out, k-fold CV, and stratified k-fold
    CV to help you start using these methods in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the necessary libraries, load the breast cancer dataset,
    and initialize a random forest model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must train and validate different random forest models using each
    validation technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Error analysis is another technique you can benefit from when seeking to develop
    reliable machine learning models, which we will introduce next.
  prefs: []
  type: TYPE_NORMAL
- en: Error analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use error analysis to find common characteristics between data points
    with incorrectly predicted outputs. For example, the majority of images that are
    misclassified in image classification models might have darker backgrounds, or
    a disease diagnostic model might have lower performance for men compared to women.
    Although manually investigating the data points with incorrect predictions could
    be insightful, this process could cost you a lot of time. Instead, you can try
    to reduce the cost programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we want to practice with a simple case of error analysis in which the
    number of misclassified data points from each class is counted for a random forest
    model that’s been trained and validated using a 5-fold CV. For error analysis,
    only predictions for validation subsets are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import the necessary Python libraries and load the wine dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must initialize a random forest model and 5-fold CV object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for each fold, we must train a random forest model using all the data,
    excluding that fold, and validate the model on the chunk of data considered in
    that fold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This analysis shows that class 1 has nine misclassified data points, while classes
    2 and 0 have only three and two misclassified examples, respectively. This simple
    example helps you start practicing with error analysis. But error analysis is
    not only about identifying misclassification count per class. You can also identify
    patterns in feature values for misclassified examples by comparing feature values
    between misclassified data points and the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There are other important factors, such as computational cost and time, that
    also need to be considered when developing machine learning models. Here, we will
    briefly talk about this important topic, but the details are beyond the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Paying any price for improving the performance of machine learning models is
    not the objective of modeling as part of bigger pipelines at the industrial level.
    Increasing the performance of models by a tenth of a percent could help you win
    machine learning competitions or publish papers by beating state-of-the-art models.
    But not all improvements result in models worth deploying to production. An example
    of such efforts, which has been common in machine learning competitions, is model
    stacking. Model stacking is about using the output of multiple models to train
    a secondary model, which could increase the cost of inference by orders of magnitude.
    Python’s implementation of stacking of the logistic regression, k-nearest neighbor,
    random forest, support vector machine, and XGBoost classification models on the
    breast cancer dataset from `scikit-learn` is shown here. A secondary logistic
    regression model uses predictions of each of these primary models as input to
    come up with the final prediction of the stacked model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the performance of the stacked model is less than 1% better
    than the best individual model, while the inference time could be more than 20
    times higher, depending on the hardware and software configurations you have.
    Although inference time could be less important, such as in the case of disease
    diagnosis or scientific discoveries, it could be of critical importance if your
    model needs to provide the output in real time, such as in recommending products
    to consumers. So, you need to consider other factors, such as inference or prediction
    time, when you’re deciding to bring a model into production or planning for new
    expensive computational experiments or data collection.
  prefs: []
  type: TYPE_NORMAL
- en: Although inference time or other factors need to be considered in your model
    building and selection, it doesn’t mean that you cannot use complex models for
    real-time output generation. Depending on the application and your budget, you
    can use better configurations, for example, on your cloud-based system, to eliminate
    the issues that arise due to higher performance but slower models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about different performance and error metrics for
    supervised and unsupervised learning models. We discussed the limitations of each
    metric and the right way of interpreting them. We also reviewed bias and variance
    analysis and different validation and cross-validation techniques for assessing
    the generalizability of models. We also presented error analysis as an approach
    for detecting the components of a model that contribute to model overfitting.
    We went through Python code examples for these topics to help you practice with
    them and be able to quickly use them in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will review techniques to improve the generalizability
    of machine learning models, such as synthetic data addition to training data,
    removing data inconsistencies, and regularization methods.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classifier is designed to identify if patients of a clinic need to go through
    the rest of the diagnostic steps after the first round of testing. What classification
    metric would be more or less appropriate? Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A classifier is designed to assess the risk of investment for different investment
    options, for a specific amount of money, and is going to be used to suggest investment
    opportunities to your clients. What classification metric would be more or less
    appropriate? Why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the calculated ROC-AUCs of two binary classification models on the same validation
    set are the same, does it mean that the models are the same?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If model A has a lower log-loss compared to model B on the same test set, does
    it always mean that the MCC of model A is also higher than model B?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If model A has a higher R 2 on the same number of data points compared to model
    B, could we claim that model A is better than model B? How does the number of
    features affect our comparison between the two models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If model A has higher performance than model B, does it mean that choosing model
    A is the right one to bring into production?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rosenberg, Andrew, and Julia Hirschberg. *V-measure: A conditional entropy-based
    external cluster evaluation measure*. Proceedings of the 2007 joint conference
    on empirical methods in natural language processing and computational natural
    language learning (EMNLP-CoNLL). 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinh, Nguyen Xuan, Julien Epps, and James Bailey. *Information theoretic measures
    for clusterings comparison: is a correction for chance necessary?* Proceedings
    of the 26th annual international conference on machine learning. 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andrew Ng, *Stanford CS229: Machine Learning Course*, Autumn 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van der Maaten, Laurens, and Geoffrey Hinton. *Visualizing data using t-SNE*.
    Journal of machine learning research 9.11 (2008).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McInnes, Leland, John Healy, and James Melville. *Umap: Uniform manifold approximation
    and projection for dimension reduction*. arXiv preprint arXiv:1802.03426 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
