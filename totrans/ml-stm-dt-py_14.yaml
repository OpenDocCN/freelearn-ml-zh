- en: '*Chapter 11*: Catastrophic Forgetting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two chapters, we started to look at a number of auxiliary tasks
    for online machine learning and working with streaming data. [*Chapter 9*](B18335_09_ePub.xhtml#_idTextAnchor184)
    covered drift detection and solutions and [*Chapter 10*](B18335_10_ePub.xhtml#_idTextAnchor201)
    covered feature transformation and scaling in a streaming context. The current
    chapter introduces a third and final topic to this list of auxiliary tasks, namely
    catastrophic forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: Catastrophic forgetting, also known as catastrophic interference, is the tendency
    of machine learning models to forget what they have learned upon new updates,
    wrongly de-learning correctly learned older tendencies as new tendencies are learned
    from new data.
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen a lot of examples of online models throughout this book, you
    will understand that continuous updating of the models creates a large risk of
    this learning going wrong. It has already been touched upon briefly, in the chapter
    on drift and drift detection, that model learning going wrong can also be seen
    as a real risk of performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Drift, however, tends to be used for pointing out drift in either the independent
    variables (data drift) or in the relations between independent variables and dependent
    variables (concept drift). As catastrophic forgetting is really a problem inside
    the coefficients of the model, we could not really consider catastrophic forgetting
    to be a part of drift.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models, especially online machine learning models, are often
    used in a relatively black-box manner, meaning that we look at their outcomes
    but do not necessarily spend much time looking at the inside mechanisms. This
    becomes a problem when detecting wrongly learned patterns. Machine learning explicability
    is therefore also related to the topic of catastrophic forgetting and will be
    covered as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the problem of machine learning models updating in
    the wrong manner, which we call catastrophic forgetting or catastrophic inference,
    with the following chapters being covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining catastrophic forgetting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detection of catastrophic forgetting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model explicability versus catastrophic forgetting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code for this book on GitHub at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python).
    If you are not yet familiar with Git and GitHub, the easiest way to download the
    notebooks and code samples is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the link of the repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the green **Code** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Download zip**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you download the ZIP file, you unzip it in your local environment, and
    you will be able to access the code through your preferred Python editor.
  prefs: []
  type: TYPE_NORMAL
- en: Python environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow along with this book, you can download the code in the repository
    and execute it using your preferred Python editor.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not yet familiar with Python environments, I would advise you to
    check out either Anaconda ([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)),
    which comes with the Jupyter Notebook and JupyterLab, which are both great for
    executing notebooks. It also comes with Spyder and VS Code for editing scripts
    and programs.
  prefs: []
  type: TYPE_NORMAL
- en: If you have difficulty installing Python or the associated programs on your
    machine, you can check out Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    or Kaggle Notebooks ([https://www.kaggle.com/code](https://www.kaggle.com/code)),
    which both allow you to run Python code in online notebooks for free, without
    any setup.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The code in the book will generally use Colab and Kaggle Notebooks with Python
    version 3.7.13 and you can set up your own environment to mimic this.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing catastrophic forgetting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Catastrophic forgetting was initially defined as a problem that occurs on (deep)
    neural networks. Deep neural networks are a set of very complex machine learning
    models that, thanks to their extreme complexity, are able to learn very complex
    patterns. Of course, this is the case only when there is enough data.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks have been studied for multiple decades. They used to be mathematically
    interesting but practically infeasible to execute due to the lack of computing
    power. The current-day progress in computing power has made it possible for neural
    networks to gain the popularity that they are currently observing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complexity of neural networks also makes them sensitive to the problem
    of catastrophic forgetting. The way a neural network learns (from a high point
    of view) is by making many update passes to the coefficients and at every update,
    the model should fit a little bit better to the data. A schematic overview of
    a neural network''s parameters can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Schematic overview of the number of coefficients in a neural
    network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – Schematic overview of the number of coefficients in a neural network
  prefs: []
  type: TYPE_NORMAL
- en: In this schematic drawing, you see that even for a very small neural network
    there are many coefficients. The larger the number of nodes becomes, the larger
    the number of parameters to estimate. When comparing this to traditional statistical
    methods, you can see that the idea of making so many passes is relatively different
    and causes different problems than those that were common in traditional statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Catastrophic forgetting is one such problem. It was first observed in a study
    in 1989, in which an experiment was presented. This experiment trained neural
    networks on the task of doing additions (from 1 + 1 = 2 to 1 + 9 = 10). A sequential
    method was tested, in which the model first learned only the first task, and then
    a new task was added once the first one was mastered.
  prefs: []
  type: TYPE_NORMAL
- en: The conclusion of this and other experiments was that adding new tasks after
    the first one has been learned will cause interference with the original learned
    model. They observed that the newer information has to be learned, the larger
    this disruption will be. Finally, they found out that the problem occurs in sequential
    learning only. If you learn all tasks at the same time, there is not really any
    re-learning happening so forgetting cannot really happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more detailed, scientific resources on catastrophic forgetting in the specific
    case of online learning using neural networks, I recommend checking out the two
    links here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://proceedings.neurips.cc/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf](https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now see how catastrophic forgetting affects online models in general.
  prefs: []
  type: TYPE_NORMAL
- en: Catastrophic forgetting in online models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although catastrophic forgetting was initially identified as a problem for neural
    networks, you can imagine that online machine learning has the same problem of
    continuous re-learning. The problem of catastrophic forgetting, or catastrophic
    inference, is therefore also present and needs to be mastered.
  prefs: []
  type: TYPE_NORMAL
- en: If models are updated at every new data point, it is expected that coefficients
    will change over time. Yet as modern-day machine learning algorithms are very
    complex and have huge numbers of coefficients or trees, it is a fairly difficult
    task to keep a close eye on them.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, the most beneficial goal would probably be to try and avoid
    any wrong learning in your machine learning at all. One way to do this is to keep
    a close eye on model performance and keep tight versioning systems in place to
    make sure that even if your model is wrongly learning anything, it does not get
    deployed in a production system. We will go into this topic shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Another solution that is possible is to work with drift detection methods, as
    you saw in [*Chapter 9*](B18335_09_ePub.xhtml#_idTextAnchor184). When you closely
    follow your model's performance and the distributions of your data, and other
    KPIs and descriptive statistics, you should be able to detect problems rather
    soon, which will allow you to intervene rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: As a third tool for managing catastrophic forgetting, you will see more tools
    for model explicability in this chapter. One of the problems of catastrophic forgetting
    is that the models are too much of a black box. Using tools from the domain of
    model explicability will help you to have a peek inside those black-box models.
    This will allow you to detect catastrophic forgetting and catastrophic inference
    based more on business logic rather than technical logic. The combination of business
    and technical logic together will be a strong combination to prepare against catastrophic
    forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting catastrophic forgetting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to look at two different approaches that you
    could use to detect catastrophic forgetting. The first approach is to implement
    a system that can detect problems with a model just after it has learned something.
    To do this, we are going to implement a Python example in multiple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a model training loop with online learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add direct evaluation to this model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add longer-term evaluation to this model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a system to avoid model updating in case of wrong learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Python to detect catastrophic forgetting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To work through this example, let''s start by implementing an online regression
    model, just like you have already seen earlier on in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we first need to generate some data. The code to generate the data
    for this example is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you look at this code, you can see that there is a shift occurring in the
    pattern. In the first 15 observations, `y` is defined as `x + random.randint()`,
    meaning just the same value as `x` but with some random variation. After the 15th
    observation, this shift changes and becomes `x * 2 + random.randint`, meaning
    the double of `x` with some added random variation. This example will be perfect
    to see how a model needs to update with time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now make a quick plot of this data to have a better idea of what this
    shift actually looks like. This can be done with the code that is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting graph is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – The scatter plot resulting from the preceding code block'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 – The scatter plot resulting from the preceding code block
  prefs: []
  type: TYPE_NORMAL
- en: The first linear trend clearly holds from x = 1 to x = 5, but a different, steeper
    function starts at x = 6 and goes on to x = 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to use River in this example, so it will be necessary to get the
    data in the right format. You should by now have mastered the data formats for
    the River library, but you can refer to the following code if necessary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this code block should be something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – The output resulting from the preceding code block'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 – The output resulting from the preceding code block
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s add a `KNNRegressor` function from the River library to this loop,
    and at each new data point, use the `learn_one` method to update the model. This
    is done using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-4
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compute the final training error of this model to have a general idea
    of the amount of errors that we should expect. The following code does exactly
    that:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-4
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the current example, this computes a mean absolute error of 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now have a more detailed look into the step-by-step learning quality
    of the model. We can do this by using continuous evaluation. This means that every
    time we learn, we will evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will plot those errors over time to see how the model is
    learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot results from this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – The plot resulting from the preceding code block'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 – The plot resulting from the preceding code block
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the model seems to obtain a perfect score every time that we
    see a new value for x, then the second time that the same x value occurs, we have
    a perfect score again, but the third time, we have a larger error!
  prefs: []
  type: TYPE_NORMAL
- en: 'It would be great to compare this with the final error, which was not computed
    step by step but just at once, using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from this code block is shown hereafter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – The plot resulting from the preceding code block'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.5 – The plot resulting from the preceding code block
  prefs: []
  type: TYPE_NORMAL
- en: You can clearly observe that when evaluating the model step by step, the error
    on each data point does not seem too big. However, when evaluating all at the
    end, you see that the model has actually forgotten the first data points! This
    is a good example of how catastrophic forgetting can be observed in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final step, let''s add a small evaluation to the model loop to help you
    in realizing that the model has forgotten your first scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this code block, a rule was made to detect forgetting as soon as the error
    was larger than the original error. Of course, this is a really severe detection
    mechanism, and you could imagine other approaches in the place of this one. For
    example, this could be a percentage change or an absolute number that must not
    be surpassed. This all depends on your business case.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have seen an approach for detecting catastrophic forgetting using
    alarm mechanisms based on model performance, let's go on to the next part of this
    chapter, in which you'll see how to use model explicability to detect catastrophic
    forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: Model explicability versus catastrophic forgetting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at model performance is generally a good way to keep track of your model
    and it will definitely help you to detect that something, somewhere in the model,
    has gone wrong. Generally, this will be enough of an alerting mechanism and will
    help you to manage your models in production.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to understand exactly what has gone wrong, however, you'll need
    to dig deeper into your model. Looking at performance only is more of a black-box
    approach, whereas we can also extract things such as trees, coefficients, variable
    importance, and the like to see what has actually changed inside the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no one-size-fits-all method for deep diving into models. All model
    categories have their own specific method for fitting the data, and an inspection
    of their fit would therefore be strongly dependent on the model itself. In the
    remainder of this section, however, we will look at two very common structures
    in machine learning: linear models with coefficients and trees.'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining models using linear coefficients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this first example, we''ll build a linear regression on some sample data
    and extract coefficients of the model to give an interpretation of them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create the data for this example using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is shown here in a dataframe format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – The plot resulting from the preceding code block'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.6 – The plot resulting from the preceding code block
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create two scatter plots to have a better visual idea of how ice cream
    sales are related to temperature and price (in this fictitious example). The following
    code shows how to create the first scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – The plot resulting from the preceding code block'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.7 – The plot resulting from the preceding code block
  prefs: []
  type: TYPE_NORMAL
- en: 'The second scatter plot can be created as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – The plot resulting from the preceding code block'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.8 – The plot resulting from the preceding code block
  prefs: []
  type: TYPE_NORMAL
- en: You can clearly see that sales are higher when the temperature is higher, and
    sales are lower when the temperature is lower. Also, higher prices are correlated
    with lower sales, and lower prices are correlated with higher sales.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are two logical and explainable factors in ice cream sales, but this
    is not yet a model. Let''s use a `LinearRegression` function to model this straightforward
    linear relationship:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can evaluate the (in-sample) fit of this model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This model yields a training R2 score of 0.98, meaning that the model fits really
    well to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now at the step where we need to go deeper into the model than just
    looking at performance. With the linear regression, we need to look at coefficients
    to be able to interpret what they have fitted. The coefficients are extracted
    in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – The coefficients resulting from the preceding code block'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.9 – The coefficients resulting from the preceding code block
  prefs: []
  type: TYPE_NORMAL
- en: 'You can interpret this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Every additional degree Celsius will increase ice cream sales by 0.15, given
    a constant price.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every euro added to the price will decrease ice cream sales by 1.3, given a
    constant temperature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining models using dendrograms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While looking at coefficients is great for linear models, some models do not
    have any coefficients. Examples of this are basically any models that use trees.
    Trees have nodes and these nodes are split based on yes/no questions. Although
    you cannot extract coefficients from trees, the advantage is that you can simply
    print out the entire tree as a graph! We''ll look at that in the next example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we need to fit a `DecisionTreeRegressor` function on the same
    data as the one we used before, using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a general idea whether the model fits, let''s compute an R2 score on
    the training set, just like we did before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The result is 1.0, which means that the decision tree has obtained a perfect
    fit on the training data. Nothing guarantees that this will generalize out-of-sample,
    but that is not necessarily a problem for explaining the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To extract the tree as an image, you can simply use the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 11-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out the entire tree and give you perfect insight into how the
    predictions have been made:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10 – The resulting dendrogram from the preceding code block'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.10 – The resulting dendrogram from the preceding code block
  prefs: []
  type: TYPE_NORMAL
- en: Explaining models using variable importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a third and final method for explaining models, you can look at variable
    importance. Again, this is something that will not work for all machine learning
    models. Yet, for rather complex models it is often too difficult to look at all
    dendrograms and variable importance estimates are a great replacement for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s extract the variable importance from the decision tree model that was
    built previously. This can be done using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 11-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting dataframe looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 – The importance value'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_11_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.11 – The importance value
  prefs: []
  type: TYPE_NORMAL
- en: This tells us that the decision tree has used degrees Celsius more than it has
    used the price as a predictor variable.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have seen how catastrophic forgetting can cause bad performance
    in your model, especially when data arrives in a sequential manner. Especially
    when one trend is learned first and a second trend follows, the risk of forgetting
    the first trend is real and needs to be controlled.
  prefs: []
  type: TYPE_NORMAL
- en: Although there is no one-stop solution for these issues, there are many things
    that can be done to avoid bad models from going into production systems. You have
    seen how to implement continuous evaluation metrics and you have seen how you
    would be able to detect that some trends have been forgotten.
  prefs: []
  type: TYPE_NORMAL
- en: Performance-based metrics are great for detecting problems but are not able
    to tell you what exactly has gone wrong inside the model. You have seen three
    methods of model explanation that can help you deep-dive further into most models.
    By extracting from the model which trends or relationships the model has learned,
    you can identify whether this corresponds to an already known business logic or
    common sense.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter of this book, we will conclude the different topics
    that have been presented and consider a number of best practices to keep in mind
    while working on online models and streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'KNNRegressor: [https://riverml.xyz/latest/api/neighbors/KNNRegressor/](https://riverml.xyz/latest/api/neighbors/KNNRegressor/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LinearRegression: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DecisionTree: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tree_plot: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
