- en: '*Chapter 11*: Catastrophic Forgetting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：灾难性遗忘'
- en: In the previous two chapters, we started to look at a number of auxiliary tasks
    for online machine learning and working with streaming data. [*Chapter 9*](B18335_09_ePub.xhtml#_idTextAnchor184)
    covered drift detection and solutions and [*Chapter 10*](B18335_10_ePub.xhtml#_idTextAnchor201)
    covered feature transformation and scaling in a streaming context. The current
    chapter introduces a third and final topic to this list of auxiliary tasks, namely
    catastrophic forgetting.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们开始探讨在线机器学习和处理流数据的许多辅助任务。[第9章](B18335_09_ePub.xhtml#_idTextAnchor184)
    讨论了漂移检测和解决方案，[第10章](B18335_10_ePub.xhtml#_idTextAnchor201) 讨论了在流环境中的特征转换和缩放。当前章节介绍了辅助任务列表中的第三个也是最后一个主题，即灾难性遗忘。
- en: Catastrophic forgetting, also known as catastrophic interference, is the tendency
    of machine learning models to forget what they have learned upon new updates,
    wrongly de-learning correctly learned older tendencies as new tendencies are learned
    from new data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 灾难性遗忘，也称为灾难性干扰，是机器学习模型在更新后忘记他们所学内容的一种趋势，错误地重新学习正确学习的老旧倾向，因为新数据中学习了新的倾向。
- en: As you have seen a lot of examples of online models throughout this book, you
    will understand that continuous updating of the models creates a large risk of
    this learning going wrong. It has already been touched upon briefly, in the chapter
    on drift and drift detection, that model learning going wrong can also be seen
    as a real risk of performance degradation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在这本书中你已经看到了许多在线模型的例子，你会明白模型的持续更新会带来学习出错的大风险。在关于漂移和漂移检测的章节中已经简要提到，模型学习出错也可以被视为性能退化的真实风险。
- en: Drift, however, tends to be used for pointing out drift in either the independent
    variables (data drift) or in the relations between independent variables and dependent
    variables (concept drift). As catastrophic forgetting is really a problem inside
    the coefficients of the model, we could not really consider catastrophic forgetting
    to be a part of drift.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，漂移通常用于指出独立变量（数据漂移）或独立变量与依赖变量之间的关系（概念漂移）中的漂移。由于灾难性遗忘实际上是模型系数内部的问题，我们实际上不能将灾难性遗忘视为漂移的一部分。
- en: Machine learning models, especially online machine learning models, are often
    used in a relatively black-box manner, meaning that we look at their outcomes
    but do not necessarily spend much time looking at the inside mechanisms. This
    becomes a problem when detecting wrongly learned patterns. Machine learning explicability
    is therefore also related to the topic of catastrophic forgetting and will be
    covered as well.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型，尤其是在线机器学习模型，通常以相对黑盒的方式使用，这意味着我们关注它们的输出，但并不一定花很多时间查看内部机制。当检测到错误学习到的模式时，这就会成为一个问题。因此，机器学习可解释性也与灾难性遗忘的主题相关，并将被涵盖。
- en: 'This chapter will cover the problem of machine learning models updating in
    the wrong manner, which we call catastrophic forgetting or catastrophic inference,
    with the following chapters being covered:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍机器学习模型错误更新的问题，我们称之为灾难性遗忘或灾难性推理，以下章节将涵盖：
- en: Defining catastrophic forgetting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义灾难性遗忘
- en: Detection of catastrophic forgetting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灾难性遗忘的检测
- en: Model explicability versus catastrophic forgetting
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可解释性 versus 灾难性遗忘
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find all the code for this book on GitHub at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python).
    If you are not yet familiar with Git and GitHub, the easiest way to download the
    notebooks and code samples is the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下链接的GitHub上找到这本书的所有代码：[https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python)。如果你还不熟悉Git和GitHub，下载笔记本和代码示例的最简单方法是以下步骤：
- en: Go to the link of the repository.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往存储库的链接。
- en: Go to the green **Code** button.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击绿色 **代码** 按钮。
- en: Select **Download zip**.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **下载ZIP**。
- en: When you download the ZIP file, you unzip it in your local environment, and
    you will be able to access the code through your preferred Python editor.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当你下载ZIP文件后，你可以在本地环境中解压它，然后你可以通过你偏好的Python编辑器访问代码。
- en: Python environment
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python环境
- en: To follow along with this book, you can download the code in the repository
    and execute it using your preferred Python editor.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随这本书的内容，你可以下载仓库中的代码，并使用你喜欢的Python编辑器执行它。
- en: If you are not yet familiar with Python environments, I would advise you to
    check out either Anaconda ([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)),
    which comes with the Jupyter Notebook and JupyterLab, which are both great for
    executing notebooks. It also comes with Spyder and VS Code for editing scripts
    and programs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还不熟悉Python环境，我建议你查看Anaconda（[https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)），它包含了Jupyter
    Notebook和JupyterLab，这两个都是执行笔记本的绝佳选择。它还包含了Spyder和VS Code，用于编辑脚本和程序。
- en: If you have difficulty installing Python or the associated programs on your
    machine, you can check out Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    or Kaggle Notebooks ([https://www.kaggle.com/code](https://www.kaggle.com/code)),
    which both allow you to run Python code in online notebooks for free, without
    any setup.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你安装Python或相关程序有困难，你可以查看Google Colab（[https://colab.research.google.com/](https://colab.research.google.com/)）或Kaggle
    Notebooks（[https://www.kaggle.com/code](https://www.kaggle.com/code)），这两个都允许你免费在线笔记本中运行Python代码，无需任何设置。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The code in the book will generally use Colab and Kaggle Notebooks with Python
    version 3.7.13 and you can set up your own environment to mimic this.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的代码通常使用Colab和Kaggle Notebooks，Python版本为3.7.13，你可以设置自己的环境来模拟这种情况。
- en: Introducing catastrophic forgetting
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入灾难性遗忘
- en: Catastrophic forgetting was initially defined as a problem that occurs on (deep)
    neural networks. Deep neural networks are a set of very complex machine learning
    models that, thanks to their extreme complexity, are able to learn very complex
    patterns. Of course, this is the case only when there is enough data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 灾难性遗忘最初被定义为在（深度）神经网络上发生的问题。深度神经网络是一组非常复杂的机器学习模型，由于它们的极端复杂性，能够学习非常复杂的模式。当然，只有在有足够数据的情况下才会是这样。
- en: Neural networks have been studied for multiple decades. They used to be mathematically
    interesting but practically infeasible to execute due to the lack of computing
    power. The current-day progress in computing power has made it possible for neural
    networks to gain the popularity that they are currently observing.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经被研究了几十年。它们过去在数学上很有趣，但由于计算能力的缺乏，实际上无法执行。当前计算能力的进步使得神经网络能够获得它们目前所观察到的普及度。
- en: 'The complexity of neural networks also makes them sensitive to the problem
    of catastrophic forgetting. The way a neural network learns (from a high point
    of view) is by making many update passes to the coefficients and at every update,
    the model should fit a little bit better to the data. A schematic overview of
    a neural network''s parameters can be seen here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的复杂性也使得它们对灾难性遗忘问题很敏感。从高角度来看，神经网络的学习方式是通过多次更新系数，并且在每次更新时，模型应该更好地拟合数据。一个神经网络参数的示意图概述如下：
- en: '![Figure 11.1 – Schematic overview of the number of coefficients in a neural
    network'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1 – 神经网络中系数数量的示意图'
- en: '](img/B18335_11_1.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18335_11_1.jpg]'
- en: Figure 11.1 – Schematic overview of the number of coefficients in a neural network
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 神经网络中系数数量的示意图
- en: In this schematic drawing, you see that even for a very small neural network
    there are many coefficients. The larger the number of nodes becomes, the larger
    the number of parameters to estimate. When comparing this to traditional statistical
    methods, you can see that the idea of making so many passes is relatively different
    and causes different problems than those that were common in traditional statistics.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示意图中，你可以看到即使是对于一个非常小的神经网络，也有许多系数。节点数量越多，需要估计的参数数量就越大。当与传统统计方法进行比较时，你可以看到进行如此多次遍历的想法相对不同，并且与传统统计学中常见的那些问题不同。
- en: Catastrophic forgetting is one such problem. It was first observed in a study
    in 1989, in which an experiment was presented. This experiment trained neural
    networks on the task of doing additions (from 1 + 1 = 2 to 1 + 9 = 10). A sequential
    method was tested, in which the model first learned only the first task, and then
    a new task was added once the first one was mastered.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 灾难性遗忘就是这样一个问题。它在1989年的一项研究中首次被观察到，其中进行了一个实验。这个实验训练神经网络进行加法任务（从1 + 1 = 2到1 +
    9 = 10）。测试了一种顺序方法，其中模型首先学习第一个任务，然后在第一个任务掌握后添加一个新任务。
- en: The conclusion of this and other experiments was that adding new tasks after
    the first one has been learned will cause interference with the original learned
    model. They observed that the newer information has to be learned, the larger
    this disruption will be. Finally, they found out that the problem occurs in sequential
    learning only. If you learn all tasks at the same time, there is not really any
    re-learning happening so forgetting cannot really happen.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验以及其他实验的结论是，在第一个任务学习之后添加新任务将干扰原始学习模型。他们观察到，新信息需要学习，这种干扰就会越大。最后，他们发现这个问题只发生在顺序学习中。如果你同时学习所有任务，实际上并没有发生再学习，所以遗忘实际上是不会发生的。
- en: 'For more detailed, scientific resources on catastrophic forgetting in the specific
    case of online learning using neural networks, I recommend checking out the two
    links here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关于在线学习神经网络中灾难性遗忘的更详细、更科学的资源，我建议查看这里提供的两个链接：
- en: '[https://proceedings.neurips.cc/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://proceedings.neurips.cc/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/54ee290e80589a2a1225c338a71839f5-Paper.pdf)'
- en: '[https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf](https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf](https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf)'
- en: Let's now see how catastrophic forgetting affects online models in general.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看灾难性遗忘是如何影响在线模型的。
- en: Catastrophic forgetting in online models
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线模型中的灾难性遗忘
- en: Although catastrophic forgetting was initially identified as a problem for neural
    networks, you can imagine that online machine learning has the same problem of
    continuous re-learning. The problem of catastrophic forgetting, or catastrophic
    inference, is therefore also present and needs to be mastered.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管灾难性遗忘最初被识别为神经网络的问题，你可以想象在线机器学习也有持续再学习的相同问题。因此，灾难性遗忘的问题，或者说灾难性推理的问题，也是存在的，并且需要被掌握。
- en: If models are updated at every new data point, it is expected that coefficients
    will change over time. Yet as modern-day machine learning algorithms are very
    complex and have huge numbers of coefficients or trees, it is a fairly difficult
    task to keep a close eye on them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在每一个新的数据点上更新，预计系数会随时间变化。然而，由于现代机器学习算法非常复杂，具有大量的系数或树，密切关注它们是一项相当困难的任务。
- en: In an ideal world, the most beneficial goal would probably be to try and avoid
    any wrong learning in your machine learning at all. One way to do this is to keep
    a close eye on model performance and keep tight versioning systems in place to
    make sure that even if your model is wrongly learning anything, it does not get
    deployed in a production system. We will go into this topic shortly.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个理想的世界里，最有益的目标可能是尽量避免在机器学习中出现任何错误的学习。做到这一点的一种方法是通过密切关注模型性能并实施严格的版本控制系统，以确保即使模型错误地学习任何内容，也不会在生产系统中部署。我们将很快讨论这个话题。
- en: Another solution that is possible is to work with drift detection methods, as
    you saw in [*Chapter 9*](B18335_09_ePub.xhtml#_idTextAnchor184). When you closely
    follow your model's performance and the distributions of your data, and other
    KPIs and descriptive statistics, you should be able to detect problems rather
    soon, which will allow you to intervene rapidly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能的解决方案是与漂移检测方法一起工作，正如你在[*第9章*](B18335_09_ePub.xhtml#_idTextAnchor184)中看到的。当你密切关注你的模型性能、数据分布以及其他KPI和描述性统计时，你应该能够很快地发现问题，这将允许你迅速干预。
- en: As a third tool for managing catastrophic forgetting, you will see more tools
    for model explicability in this chapter. One of the problems of catastrophic forgetting
    is that the models are too much of a black box. Using tools from the domain of
    model explicability will help you to have a peek inside those black-box models.
    This will allow you to detect catastrophic forgetting and catastrophic inference
    based more on business logic rather than technical logic. The combination of business
    and technical logic together will be a strong combination to prepare against catastrophic
    forgetting.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作为管理灾难性遗忘的第三个工具，你将在本章中看到更多关于模型可解释性的工具。灾难性遗忘的一个问题是模型过于像一个黑盒。使用模型可解释性领域的工具将帮助你窥视那些黑盒模型。这将允许你基于业务逻辑而不是技术逻辑来检测灾难性遗忘和灾难性推理。将业务逻辑和技术逻辑结合起来，将是一个强大的组合，以预防灾难性遗忘。
- en: Detecting catastrophic forgetting
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测灾难性遗忘
- en: 'In this chapter, we are going to look at two different approaches that you
    could use to detect catastrophic forgetting. The first approach is to implement
    a system that can detect problems with a model just after it has learned something.
    To do this, we are going to implement a Python example in multiple steps:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨两种不同的方法，您可以使用这些方法来检测灾难性遗忘。第一种方法是实现一个系统，可以在模型学习到新内容后立即检测到问题。为此，我们将分多个步骤实现一个
    Python 示例：
- en: Develop a model training loop with online learning.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一个带有在线学习的模型训练循环。
- en: Add direct evaluation to this model.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向此模型添加直接评估。
- en: Add longer-term evaluation to this model.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向此模型添加长期评估。
- en: Add a system to avoid model updating in case of wrong learning.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个系统以避免错误学习时的模型更新。
- en: Using Python to detect catastrophic forgetting
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 检测灾难性遗忘
- en: 'To work through this example, let''s start by implementing an online regression
    model, just like you have already seen earlier on in this book:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这个例子，让我们首先实现一个在线回归模型，就像你在本书前面已经看到的那样：
- en: 'To do this, we first need to generate some data. The code to generate the data
    for this example is shown here:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要做到这一点，我们首先需要生成一些数据。本例中生成数据的代码如下所示：
- en: Code Block 11-1
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-1
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you look at this code, you can see that there is a shift occurring in the
    pattern. In the first 15 observations, `y` is defined as `x + random.randint()`,
    meaning just the same value as `x` but with some random variation. After the 15th
    observation, this shift changes and becomes `x * 2 + random.randint`, meaning
    the double of `x` with some added random variation. This example will be perfect
    to see how a model needs to update with time.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看这个代码，你可以看到模式中发生了一个变化。在前 15 个观测值中，`y` 被定义为 `x + random.randint()`，这意味着与 `x`
    相同的值，但有一些随机变化。在第 15 个观测值之后，这种变化改变并变为 `x * 2 + random.randint`，这意味着 `x` 的两倍加上一些额外的随机变化。这个例子将完美地展示模型如何随着时间更新。
- en: 'Let''s now make a quick plot of this data to have a better idea of what this
    shift actually looks like. This can be done with the code that is shown here:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们快速绘制这些数据，以便更好地了解这种变化实际上看起来是什么样子。这可以通过以下代码实现：
- en: Code Block 11-2
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-2
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The resulting graph is shown here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表如下所示：
- en: '![Figure 11.2 – The scatter plot resulting from the preceding code block'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.2 – 前一个代码块生成的散点图](img/B18335_11_2.jpg)'
- en: '](img/B18335_11_2.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18335_11_2.jpg](img/B18335_11_2.jpg)'
- en: Figure 11.2 – The scatter plot resulting from the preceding code block
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 前一个代码块生成的散点图
- en: The first linear trend clearly holds from x = 1 to x = 5, but a different, steeper
    function starts at x = 6 and goes on to x = 10.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条线性趋势在 x = 1 到 x = 5 之间明显成立，但从 x = 6 开始，有一个不同且更陡峭的函数继续到 x = 10。
- en: 'We are going to use River in this example, so it will be necessary to get the
    data in the right format. You should by now have mastered the data formats for
    the River library, but you can refer to the following code if necessary:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用 River，因此需要将数据格式化为正确格式。到现在为止，你应该已经掌握了 River 库的数据格式，但如果需要，可以参考以下代码：
- en: Code Block 11-3
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-3
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result of this code block should be something like the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码块的结果应该类似于以下内容：
- en: '![Figure 11.3 – The output resulting from the preceding code block'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.3 – 前一个代码块生成的输出](img/B18335_11_3.jpg)'
- en: '](img/B18335_11_3.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18335_11_3.jpg](img/B18335_11_3.jpg)'
- en: Figure 11.3 – The output resulting from the preceding code block
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 前一个代码块生成的输出
- en: 'Now, let''s add a `KNNRegressor` function from the River library to this loop,
    and at each new data point, use the `learn_one` method to update the model. This
    is done using the following code:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将 River 库中的 `KNNRegressor` 函数添加到这个循环中，并在每个新的数据点处使用 `learn_one` 方法更新模型。这是通过以下代码完成的：
- en: Code Block 11-4
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-4
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can compute the final training error of this model to have a general idea
    of the amount of errors that we should expect. The following code does exactly
    that:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以计算这个模型的最终训练误差，以大致了解我们应该期望多少误差。以下代码正是这样做的：
- en: Code Block 11-4
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-4
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the current example, this computes a mean absolute error of 10.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前示例中，这计算出一个平均绝对误差为 10。
- en: 'Let''s now have a more detailed look into the step-by-step learning quality
    of the model. We can do this by using continuous evaluation. This means that every
    time we learn, we will evaluate the model:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地看看模型的逐步学习质量。我们可以通过使用持续评估来实现这一点。这意味着每次我们学习时，我们都会评估模型：
- en: Code Block 11-5
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-5
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following code will plot those errors over time to see how the model is
    learning:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码将绘制这些误差随时间的变化，以查看模型是如何学习的：
- en: Code Block 11-6
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-6
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following plot results from this code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图是此代码的结果：
- en: '![Figure 11.4 – The plot resulting from the preceding code block'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 11.4 – 上述代码块产生的图'
- en: '](img/B18335_11_4.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18335_11_4.jpg]'
- en: Figure 11.4 – The plot resulting from the preceding code block
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 上述代码块产生的图
- en: Interestingly, the model seems to obtain a perfect score every time that we
    see a new value for x, then the second time that the same x value occurs, we have
    a perfect score again, but the third time, we have a larger error!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，模型似乎每次我们看到 x 的新值时都会获得满分，然后当相同的 x 值再次出现时，我们再次获得满分，但第三次，我们有一个更大的误差！
- en: 'It would be great to compare this with the final error, which was not computed
    step by step but just at once, using the following code:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与最终误差进行比较将非常棒，这个最终误差不是逐步计算的，而是通过以下代码一次性计算的：
- en: Code Block 11-7
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-7
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output from this code block is shown hereafter:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块的结果如下所示：
- en: '![Figure 11.5 – The plot resulting from the preceding code block'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 11.5 – 上述代码块产生的图'
- en: '](img/B18335_11_5.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18335_11_5.jpg]'
- en: Figure 11.5 – The plot resulting from the preceding code block
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 上述代码块产生的图
- en: You can clearly observe that when evaluating the model step by step, the error
    on each data point does not seem too big. However, when evaluating all at the
    end, you see that the model has actually forgotten the first data points! This
    is a good example of how catastrophic forgetting can be observed in practice.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以清楚地观察到，当逐步评估模型时，每个数据点的误差似乎并不太大。然而，当最终评估时，你会发现模型实际上已经忘记了最初的数据点！这是一个很好的例子，说明了灾难性遗忘如何在实践中被观察到。
- en: 'As a final step, let''s add a small evaluation to the model loop to help you
    in realizing that the model has forgotten your first scores:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为最后一步，让我们在模型循环中添加一个小型评估，帮助你意识到模型已经忘记了你的第一个分数：
- en: Code Block 11-8
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-8
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this code block, a rule was made to detect forgetting as soon as the error
    was larger than the original error. Of course, this is a really severe detection
    mechanism, and you could imagine other approaches in the place of this one. For
    example, this could be a percentage change or an absolute number that must not
    be surpassed. This all depends on your business case.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码块中，制定了一条规则，当误差大于原始误差时立即检测到遗忘。当然，这是一个非常严重的检测机制，你可以想象用其他方法来替代这个方法。例如，这可以是一个百分比变化或一个绝对数值，不能超过。这完全取决于你的业务案例。
- en: Now that you have seen an approach for detecting catastrophic forgetting using
    alarm mechanisms based on model performance, let's go on to the next part of this
    chapter, in which you'll see how to use model explicability to detect catastrophic
    forgetting.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经看到了使用基于模型性能的警报机制来检测灾难性遗忘的方法，那么让我们继续本章的下一部分，你将看到如何使用模型可解释性来检测灾难性遗忘。
- en: Model explicability versus catastrophic forgetting
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型可解释性与灾难性遗忘
- en: Looking at model performance is generally a good way to keep track of your model
    and it will definitely help you to detect that something, somewhere in the model,
    has gone wrong. Generally, this will be enough of an alerting mechanism and will
    help you to manage your models in production.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 查看模型性能通常是一个跟踪模型的好方法，它肯定会帮助你检测到模型中某个地方出现了错误。通常，这将足够作为一个警报机制，并帮助你管理生产中的模型。
- en: If you want to understand exactly what has gone wrong, however, you'll need
    to dig deeper into your model. Looking at performance only is more of a black-box
    approach, whereas we can also extract things such as trees, coefficients, variable
    importance, and the like to see what has actually changed inside the model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你想确切地了解出了什么问题，你需要更深入地挖掘你的模型。仅仅查看性能更像是一种黑盒方法，而我们可以提取诸如树、系数、变量重要性等东西，以了解模型内部实际发生了什么变化。
- en: 'There is no one-size-fits-all method for deep diving into models. All model
    categories have their own specific method for fitting the data, and an inspection
    of their fit would therefore be strongly dependent on the model itself. In the
    remainder of this section, however, we will look at two very common structures
    in machine learning: linear models with coefficients and trees.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一种适合所有情况的深入模型研究方法。所有模型类别都有自己特定的方法来拟合数据，因此对它们的拟合检查将强烈依赖于模型本身。然而，在本节的剩余部分，我们将探讨机器学习中的两种非常常见的结构：具有系数的线性模型和树。
- en: Explaining models using linear coefficients
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性系数解释模型
- en: 'In this first example, we''ll build a linear regression on some sample data
    and extract coefficients of the model to give an interpretation of them:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个例子中，我们将对一些样本数据建立线性回归，并提取模型的系数来解释它们：
- en: 'You can create the data for this example using the following code:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用以下代码创建此示例的数据：
- en: Code Block 11-9
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块11-9
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The data is shown here in a dataframe format:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以数据框格式在此显示：
- en: '![Figure 11.6 – The plot resulting from the preceding code block'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.6 – 前一个代码块生成的图表'
- en: '](img/B18335_11_6.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18335_11_6.jpg)'
- en: Figure 11.6 – The plot resulting from the preceding code block
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 前一个代码块生成的图表
- en: 'Let''s create two scatter plots to have a better visual idea of how ice cream
    sales are related to temperature and price (in this fictitious example). The following
    code shows how to create the first scatter plot:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建两个散点图，以更好地了解冰淇淋销量与温度和价格（在这个虚构的例子中）的关系。以下代码显示了如何创建第一个散点图：
- en: Code Block 11-10
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块11-10
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This results in the following graph:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下图表：
- en: '![Figure 11.7 – The plot resulting from the preceding code block'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.7 – 前一个代码块生成的图表'
- en: '](img/B18335_11_7.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18335_11_7.jpg)'
- en: Figure 11.7 – The plot resulting from the preceding code block
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – 前一个代码块生成的图表
- en: 'The second scatter plot can be created as follows:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个散点图可以创建如下：
- en: Code Block 11-11
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块11-11
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This results in the following graph:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下图表：
- en: '![Figure 11.8 – The plot resulting from the preceding code block'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.8 – 前一个代码块生成的图表'
- en: '](img/B18335_11_8.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18335_11_8.jpg)'
- en: Figure 11.8 – The plot resulting from the preceding code block
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8 – 前一个代码块生成的图表
- en: You can clearly see that sales are higher when the temperature is higher, and
    sales are lower when the temperature is lower. Also, higher prices are correlated
    with lower sales, and lower prices are correlated with higher sales.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以清楚地看到，当温度较高时，销售额也较高，而当温度较低时，销售额较低。此外，高价与低销量相关，低价与高销量相关。
- en: 'These are two logical and explainable factors in ice cream sales, but this
    is not yet a model. Let''s use a `LinearRegression` function to model this straightforward
    linear relationship:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些是冰淇淋销量中的两个逻辑上可解释的因素，但这还不是模型。让我们使用`LinearRegression`函数来模拟这种直接的线性关系：
- en: Code Block 11-12
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块11-12
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can evaluate the (in-sample) fit of this model as follows:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以如下评估此模型的（样本内）拟合度：
- en: Code Block 11-13
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块11-13
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This model yields a training R2 score of 0.98, meaning that the model fits really
    well to the training data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型产生一个训练R2分数为0.98，这意味着模型与训练数据拟合得非常好。
- en: 'We are now at the step where we need to go deeper into the model than just
    looking at performance. With the linear regression, we need to look at coefficients
    to be able to interpret what they have fitted. The coefficients are extracted
    in the following code:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在处于需要比仅仅查看性能更深入地了解模型的步骤。在线性回归中，我们需要查看系数来解释它们拟合的内容。系数在以下代码中提取：
- en: Code Block 11-14
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块11-14
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This gives the following output:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '![Figure 11.9 – The coefficients resulting from the preceding code block'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.9 – 前一个代码块生成的系数'
- en: '](img/B18335_11_9.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18335_11_9.jpg)'
- en: Figure 11.9 – The coefficients resulting from the preceding code block
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9 – 前一个代码块生成的系数
- en: 'You can interpret this as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样解释：
- en: Every additional degree Celsius will increase ice cream sales by 0.15, given
    a constant price.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在恒定价格下，每增加一度摄氏度将使冰淇淋销量增加 0.15。
- en: Every euro added to the price will decrease ice cream sales by 1.3, given a
    constant temperature.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在恒定温度下，每增加一欧元的价格将使冰淇淋销量减少 1.3。
- en: Explaining models using dendrograms
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用树状图解释模型
- en: 'While looking at coefficients is great for linear models, some models do not
    have any coefficients. Examples of this are basically any models that use trees.
    Trees have nodes and these nodes are split based on yes/no questions. Although
    you cannot extract coefficients from trees, the advantage is that you can simply
    print out the entire tree as a graph! We''ll look at that in the next example:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看系数对于线性模型来说很棒时，一些模型没有任何系数。这类模型的例子基本上是任何使用树的模型。树有节点，这些节点基于是/否问题进行分割。虽然您不能从树中提取系数，但优势是您可以简单地打印出整个树作为图形！我们将在下一个示例中查看这一点：
- en: 'To get started, we need to fit a `DecisionTreeRegressor` function on the same
    data as the one we used before, using the following code:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始，我们需要在之前使用的数据上拟合一个 `DecisionTreeRegressor` 函数，使用以下代码：
- en: Code Block 11-15
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-15
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To get a general idea whether the model fits, let''s compute an R2 score on
    the training set, just like we did before:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了得到模型是否拟合的一般概念，让我们在训练集上计算一个 R2 分数，就像我们之前做的那样：
- en: Code Block 11-16
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-16
- en: '[PRE16]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The result is 1.0, which means that the decision tree has obtained a perfect
    fit on the training data. Nothing guarantees that this will generalize out-of-sample,
    but that is not necessarily a problem for explaining the model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 1.0，这意味着决策树在训练数据上获得了完美的拟合。没有任何保证这将在样本外泛化，但这对于解释模型来说不一定是问题。
- en: 'To extract the tree as an image, you can simply use the code here:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将树提取为图像，您只需使用这里的代码：
- en: Code Block 11-17
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-17
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will print out the entire tree and give you perfect insight into how the
    predictions have been made:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出整个树，并让您深入了解预测是如何做出的：
- en: '![Figure 11.10 – The resulting dendrogram from the preceding code block'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.10 – 前一个代码块生成的树状图'
- en: '](img/B18335_11_10.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18335_11_10.jpg)'
- en: Figure 11.10 – The resulting dendrogram from the preceding code block
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – 前一个代码块生成的树状图
- en: Explaining models using variable importance
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用变量重要性解释模型
- en: As a third and final method for explaining models, you can look at variable
    importance. Again, this is something that will not work for all machine learning
    models. Yet, for rather complex models it is often too difficult to look at all
    dendrograms and variable importance estimates are a great replacement for this.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解释模型的第三种和最后一种方法，您可以查看变量重要性。再次强调，这并不是所有机器学习模型都适用的。然而，对于相当复杂的模型来说，查看所有树状图和变量重要性估计通常是一个很好的替代方案。
- en: 'Let''s extract the variable importance from the decision tree model that was
    built previously. This can be done using the following code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从之前构建的决策树模型中提取变量重要性。这可以通过以下代码完成：
- en: Code Block 11-18
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 11-18
- en: '[PRE18]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The resulting dataframe looks as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数据框如下所示：
- en: '![Figure 11.11 – The importance value'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.11 – 重要性值'
- en: '](img/B18335_11_11.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18335_11_11.jpg)'
- en: Figure 11.11 – The importance value
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11 – 重要性值
- en: This tells us that the decision tree has used degrees Celsius more than it has
    used the price as a predictor variable.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，决策树比价格更使用了摄氏度作为预测变量。
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you have seen how catastrophic forgetting can cause bad performance
    in your model, especially when data arrives in a sequential manner. Especially
    when one trend is learned first and a second trend follows, the risk of forgetting
    the first trend is real and needs to be controlled.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经看到了灾难性遗忘如何导致模型性能不佳，尤其是在数据以序列方式到达时。特别是当首先学习一个趋势，然后跟随第二个趋势时，忘记第一个趋势的风险是真实的，需要得到控制。
- en: Although there is no one-stop solution for these issues, there are many things
    that can be done to avoid bad models from going into production systems. You have
    seen how to implement continuous evaluation metrics and you have seen how you
    would be able to detect that some trends have been forgotten.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有一劳永逸的解决方案来处理这些问题，但有许多事情可以做到，以避免不良模型进入生产系统。您已经看到了如何实现连续评估指标，以及您将能够检测到一些趋势已经被遗忘。
- en: Performance-based metrics are great for detecting problems but are not able
    to tell you what exactly has gone wrong inside the model. You have seen three
    methods of model explanation that can help you deep-dive further into most models.
    By extracting from the model which trends or relationships the model has learned,
    you can identify whether this corresponds to an already known business logic or
    common sense.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 基于性能的指标非常适合检测问题，但无法告诉你模型内部究竟发生了什么具体错误。你已经看到了三种模型解释的方法，这些方法可以帮助你进一步深入到大多数模型中。通过从模型中提取模型学习到的趋势或关系，你可以确定这是否对应于已知的业务逻辑或常识。
- en: In the next and final chapter of this book, we will conclude the different topics
    that have been presented and consider a number of best practices to keep in mind
    while working on online models and streaming data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章和最后一章中，我们将总结所介绍的不同主题，并考虑在处理在线模型和流数据时需要牢记的一些最佳实践。
- en: Further reading
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'KNNRegressor: [https://riverml.xyz/latest/api/neighbors/KNNRegressor/](https://riverml.xyz/latest/api/neighbors/KNNRegressor/)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'KNNRegressor: [https://riverml.xyz/latest/api/neighbors/KNNRegressor/](https://riverml.xyz/latest/api/neighbors/KNNRegressor/)'
- en: 'LinearRegression: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LinearRegression: [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)'
- en: 'DecisionTree: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DecisionTree: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)'
- en: 'Tree_plot: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tree_plot: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)'
