- en: '*Chapter 4*: LIME for Model Interpretability'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we discussed the various technical concepts of **Explainable
    AI** (**XAI**) that are needed to build trustworthy AI systems. Additionally,
    we looked at certain practical examples and demonstrations using various Python
    frameworks to implement the concepts of practical problem solving, which are given
    in the GitHub code repository of this chapter. XAI has been an important research
    topic for quite some time, but it is only very recently that all organizations
    have started to adopt XAI as a part of the solution life cycle for solving business
    problems using AI. One such popular approach is **Local Interpretable Model-Agnostic
    Explanations** (**LIME**), which has been widely adopted to provide model-agnostic
    local explainability. The LIME Python library is a robust framework that provides
    human-friendly explanations to tabular, text, and image data and helps in interpreting
    black-box supervised machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to the LIME framework, which has made
    a significant impact in the field of XAI. We will discuss the workings of the
    LIME algorithm for global and local model explainability. Also, I will demonstrate
    an example in which the LIME Python framework can be used in practice. I will
    cover the limitations of this framework that you should be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this chapter, we will discuss the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive understanding of LIME
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What makes LIME a good model explainer?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submodular pick (SP-LIME)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A practical example of using LIME for classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential pitfalls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without further ado, let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is slightly more technical than the previous chapters covered in
    this book. The code and dataset resources can be downloaded or cloned from the
    GitHub repository for this chapter, which is located at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04).
    Similar to the previous chapters, we will be using Python and Jupyter notebooks
    to run the code and generate the necessary outputs. Other important Python frameworks
    that are necessary to run the code will be mentioned in the notebooks with further
    relevant details to understand the code implementation of these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitive understanding of LIME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LIME is a novel, model-agnostic, local explanation technique used for interpreting
    black-box models by learning a local model around the predictions. LIME provides
    an intuitive global understanding of the model, which is helpful for non-expert
    users, too. The technique was first proposed in the research paper *"Why Should
    I Trust You?" Explaining the Predictions of Any Classifier* by *Ribeiro et al*.
    (https://arxiv.org/abs/1602.04938). The Python library can be installed from the
    GitHub repository at [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime).
    The algorithm does a pretty good job of interpreting any classifier or regressor
    in faithful ways by using approximated local interpretable models. It provides
    a global perspective to establish trust for any black-box model; therefore, it
    allows you to identify interpretable models over human-interpretable representation,
    which is locally faithful to the algorithm. So, it mainly functions by *learning
    interpretable data representations*, *maintaining a balance in a fidelity-interpretability
    trade-off*, and *searching for local explorations*. Let's look at each one of
    them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Learning interpretable data representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LIME does a pretty good job in differentiating between impactful features and
    choosing interpretable data representations that are understandable to any non-expert
    user regardless of the actual complex features used by the algorithm. For example,
    when explaining models trained on unstructured data such as images, the actual
    algorithm might use complex numerical feature vectors for its decision-making
    process, but these numerical feature values are incomprehensible to any non-technical
    end user. In comparison, if the explainability is provided in terms of the presence
    or absence of a region of interest or superpixel (that is, a continuous patch
    of pixels) within the image, that is a human-interpretable way of providing explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for text data, instead of using word-embedding vector values to interpret
    models, a better way to provide a human-interpretable explanation is by using
    examples of the presence or absence of certain words used to describe the target
    outcome of the model. So, mathematically speaking, the original representation
    of a data instance being explained is denoted by ![](img/B18216_04_001.png), where
    *d* is the entire dimension of data. A binary vector of interpretable data representations
    is mathematically denoted by ![](img/B18216_04_002.png). Intuitively speaking,
    the algorithm tries to denote the presence or absence of human-interpretable data
    representations to explain any black-box model.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.1* shows how LIME tries to divide the input image data into human-interpretable
    components that are later used to explain black-box models in a manner that is
    understandable to any non-technical user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – How LIME transforms an image into human-interpretable components'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_04_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – How LIME transforms an image into human-interpretable components
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's discuss how to maintain the fidelity-interpretability trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining a balance in the fidelity-interpretability trade-off
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LIME makes use of inherently interpretable models such as decision trees, linear
    models, and rule-based heuristic models to provide explanations to non-expert
    users with visual or textual artifacts. Mathematically speaking, this explanation
    is a model that can be denoted by ![](img/B18216_04_003.png), where ![](img/B18216_04_004.png)
    is the entire set of potentially interpretable models and the domain of ![](img/B18216_04_005.png)
    is represented with another binary vector, ![](img/B18216_04_006.png), which represents
    the presence or absence of interpretable components. Additionally, the algorithm
    tries to measure the *complexity* of an explanation along with its *interpretability*.
    For example, even in interpretable models such as decision trees, the depth of
    the tree is a measure of its complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically speaking, the complexity of an interpretable model is denoted
    by ![](img/B18216_04_007.png). LIME tries to maintain **local fidelity** while
    providing explanations. This means that the algorithm tries to replicate the behavior
    of the model in proximity to the individual data instance being predicted. So,
    mathematically, the inventors of this algorithm used a function, ![](img/B18216_04_008.png),
    to measure the proximity between any data instances, ![](img/B18216_04_009.png),
    thus defining the locality around the original representation, ![](img/B18216_04_010.png).
    Now, if the probability function, ![](img/B18216_04_011.png), defines the probability
    that ![](img/B18216_04_012.png) belongs to a certain class, then to approximate
    ![](img/B18216_04_013.png), the LIME algorithm tries to measure how unfaithful
    ![](img/B18216_04_014.png) is with a proximity function, ![](img/B18216_04_015.png).
    This entire operation is denoted by the ![](img/B18216_04_016.png) function. Therefore,
    the algorithm tries to minimize the locality-aware loss function, ![](img/B18216_04_017.png),
    while maintaining ![](img/B18216_04_018.png) to be a low value. This is so that
    it is easily explainable to any non-expert user. The measure of an interpretability
    local fidelity trade-off is approximated by the following mathematical function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18216_04_019.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, this trade-off measure depends on the interpretable models, ![](img/B18216_04_020.png),
    the fidelity function, ![](img/B18216_04_021.png), and the complexity measure,
    ![](img/B18216_04_022.png).
  prefs: []
  type: TYPE_NORMAL
- en: Searching for local explorations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LIME algorithm is *model-agnostic*. This means when we try to minimize the
    *locality-aware loss function*, ![](img/B18216_04_023.png), without any assumption
    about *f*. Also, LIME maintains local fidelity by taking samples that are weighted
    by ![](img/B18216_04_024.png) while approximating ![](img/B18216_04_025.png).
    Nonzero samples of ![](img/B18216_04_026.png) are drawn uniformly at random to
    sample instances around ![](img/B18216_04_0261.png). Let's suppose there is a
    perturbed sample containing fractions of nonzero elements of ![](img/B18216_04_027.png),
    which is denoted by ![](img/B18216_04_029.png). The algorithm tries to recover
    samples from the original representation, ![](img/B18216_04_030.png), to approximate
    ![](img/B18216_04_031.png). Then, ![](img/B18216_04_032.png) is used as a label
    for the explanation model, ![](img/B18216_04_033.png).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.2* represents an example presented in the original paper of the LIME
    framework at [https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf),
    which intuitively explains the working of the algorithm using a visual representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Explaining the working of the LIME algorithm intuitively'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_04_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Explaining the working of the LIME algorithm intuitively
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4.2*, the curve separating the light blue and pink backgrounds is
    considered a complex ![](img/B18216_04_034.png) decision function of a black-box
    model. Since the decision function is not linear, approximating it using linear
    models is not efficient. The crosses and the dots represent training data belonging
    to two different classes. The bold cross represents the inference data instance
    being explained. The algorithm functions by sampling instances to get predictions
    using *f*. Then, the algorithm assigns weight by the proximity to the data instance
    being explained. In the preceding diagram, based on the proximity of the data
    instance, the sizes of the red crosses and blue dots are varied. So, the instances
    that are sampled are both in closer proximity to *x*, having a higher weight from
    ![](img/B18216_04_035.png), and far away from it, thus having a lower weight of
    ![](img/B18216_04_036.png). The original black-box model might be too complex
    to provide a global explanation, but the LIME framework can provide explanations
    that are appropriate for the local data instance, ![](img/B18216_04_037.png).
    The learned explanation is illustrated by the dashed line, which is locally faithful
    with a global perspective.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.3* illustrates a far more intuitive understanding of the LIME algorithm.
    From the original image, the algorithm generates a set of perturbed data instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Predictions being explained using LIME'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_04_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – Predictions being explained using LIME
  prefs: []
  type: TYPE_NORMAL
- en: The perturbed instances, as shown in *Figure 4.3*, are created by switching
    some of the interpretable components off. In the case of images, as shown in the
    preceding diagram, it is done by turning certain components gray. Then, the black-box
    model is applied to each of the perturbed instances that are generated, and the
    probability of the instance being predicted as the final outcome of the model
    is calculated. Then, an interpretable model (such as a simple locally weighted
    linear model) is learned on the dataset, and finally, the superpixels having the
    maximum positive weights are considered for the final explanation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let's discuss why LIME is a good model explainer.
  prefs: []
  type: TYPE_NORMAL
- en: What makes LIME a good model explainer?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LIME enables non-expert users to understand the working of untrustworthy black-box
    models. The following properties of LIME make it a good model explainer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Human interpretable**: As discussed in the previous section, LIME provides
    explanations that are easy to understand, as it provides a qualitative way to
    compare the components of the input data with the model outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model-agnostic**: In the previous chapters, although you have learned about
    various model-specific explanation methods, it is always an advantage if the explanation
    method can be used to provide explainability for any black-box model. LIME does
    not make any assumptions about the model while providing the explanations and
    can work with any model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local fidelity**: LIME tries to replicate the behavior of the entire model
    by exploring the proximity of the data instance being predicted. So, it provides
    local explainability to the data instance being used for prediction. This is important
    for any non-technical user to understand the exact reason for the model''s decision-making
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global intuition**: Although the algorithm provides local explainability,
    it does try to explain a representative set to the end users, thereby providing
    a global perspective to the functioning of the model. SP-LIME provides a global
    understanding of the model by explaining a collection of data instances. This
    will be covered in more detail in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the key advantages of the LIME framework, in the next
    section, let's discuss the submodular pick algorithm of LIME, which is used for
    extracting global explainability.
  prefs: []
  type: TYPE_NORMAL
- en: SP-LIME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to make explanation methods more trustworthy, providing an explanation
    to a single data instance (that is, a local explanation) is not always sufficient,
    and the end user might want a global understanding of the model to have higher
    reliability on the robustness of the model. So, the SP-LIME algorithm tries to
    run the explanations on multiple diverse, yet carefully selected, sets of instances
    and returns non-redundant explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let me provide an intuitive understanding of the SP-LIME algorithm. The
    algorithm considers that the time required to go through all the individual local
    explanations is limited and is a constraint. So, the number of explanations that
    the end users are willing to examine to explain a model is the budget of the algorithm
    denoted by *B*. Let's suppose that *X* denotes the set of instances; the task
    of selecting *B* instances for the end user to analyze for model explainability
    is defined as the **pick step**. The pick step is independent of the existence
    of the explanation and it needs to provide *non-redundant explanations* by picking
    up a diverse representative set of instances to explain how the model is behaving
    considering a global perspective. Therefore, the algorithm tries to avoid picking
    up instances with similar explanations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this idea is represented using the *Explanation Matrix* (*W*),
    in which *W* *= n * d''*, such that *n* is the number of samples and *d''* is
    the human interpretable features. The algorithm also uses a *Global importance
    component matrix* (*I*), in which for each component of *j*, *I(j)* represent
    the global importance in the explanation space. Intuitively speaking, *I* is formulated
    in a way to assign higher scores to features, which explains many instances of
    the data. The set of important features that are considered for the explanations
    is denoted by *V*. So, combining all these parameters, the algorithm tries to
    learn a *non-redundant coverage intuition function*, *c(V,W,I)*. The non-redundant
    coverage intuition tries to compute the collective importance of all features
    that appear in at least one instance in set *V*. However, the *pick problem* is
    about *maximizing the weighted coverage function*. This is denoted by the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18216_04_038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The details about the algorithm that we just covered in this section might
    be slightly overwhelming to understand for certain readers. However, intuitively,
    the algorithm tries to cover the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The explanation model is run on all instances (*x*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The global importance of all individual components is computed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the algorithm tries to maximize the non-redundant coverage intuition function
    (*c*) by iteratively adding instances with the highest maximum coverage gain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the algorithm tries to obtain the representative non-redundant explanation
    set (*V*) and return it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, we will cover how the LIME Python framework can be used
    for classification problems using code examples.
  prefs: []
  type: TYPE_NORMAL
- en: A practical example of using LIME for classification problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have covered most of the in-depth conceptual understanding that
    is needed regarding the LIME algorithm. In this section, we will try to explore
    the LIME Python framework for explaining classification problems. The framework
    is available as an open source project on GitHub at [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime).
    Installing LIME in Python can be done easily using the `pip` installer inside
    the Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete notebook version of the tutorial is accessible from the GitHub
    repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb).
    However, for now, I will try to walk you through the entire code so that you understand
    the code in detail. Once the LIME framework has been installed, quickly verify
    whether the installation was successful or not by importing the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If the import was successful, you can easily proceed with the next steps; otherwise,
    you need to check what went wrong while installing the framework. But usually,
    you should not face any errors or any dependency conflicts as installing the library
    is quite straightforward. For this tutorial, we will use the *Titanic dataset*
    ([https://www.openml.org/search?type=data&sort=runs&id=40945&status=active](https://www.openml.org/search?type=data&sort=runs&id=40945&status=active)).
    This is one of the classic machine learning datasets used for predicting the survival
    of passengers on the Titanic. So, this is a binary classification problem that
    can be solved using machine learning. Although this is a classic dataset that
    is not very complex, it contains all types of features such as *Categorical*,
    *Ordinal*, *Continuous*, and even certain *identifiers* that are not relevant
    for the classification, thereby making this an interesting dataset to work with.
    To make it easier for you to execute notebooks, I have downloaded and provided
    the dataset after some slight modifications in the code repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04/dataset](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04/dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Titanic dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The original Titanic dataset, describing the survival status of individual
    passengers on the Titanic. The titanic data does not contain information from
    the crew, but it does contain actual ages of half of the passengers. The principal
    source for data about Titanic passengers is the Encyclopedia Titanica. The datasets
    used here were begun by a variety of researchers. One of the original sources
    is Eaton & Haas (1994) Titanic: Triumph and Tragedy, Patrick Stephens Ltd, which
    includes a passenger list created by many researchers and edited by Michael A.
    Findlay.'
  prefs: []
  type: TYPE_NORMAL
- en: Thomas Cason of UVa has greatly updated and improved the titanic data frame
    using the Encyclopedia Titanica and created the dataset here. Some duplicate passengers
    have been dropped, many errors corrected, many missing ages filled in, and new
    variables created.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing and importing all the required modules, first, we will start
    by loading the dataset from the directory as a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When you try to visualize the DataFrame using the `head` method from pandas,
    you will get a glimpse of the dataset, as shown in *Figure 4.4*. Often, this step
    helps you to get a quick idea about how to understand your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows a glimpse of the pandas DataFrame used for this
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Displaying the dataset as a pandas DataFrame (left-hand side)
    and a data dictionary (right-hand side)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_04_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Displaying the dataset as a pandas DataFrame (left-hand side) and
    a data dictionary (right-hand side)
  prefs: []
  type: TYPE_NORMAL
- en: 'For this particular example, we are not concerned about getting a highly efficient
    machine learning model, but rather our focus is on using LIME to produce human-friendly
    explanations in a few lines of code. So, we will skip doing rigorous **Exploratory
    Data Analysis** (**EDA**) or feature engineering steps. However, I do highly encourage
    all of you to perform these steps as a good practice. As we can see from the dataset,
    certain features such as *Passenger ID* and *Ticket Number* are identifiers that
    can be ignored. The *Cabin Number* feature is an interesting feature, especially
    as it could indicate a certain wing, floor, or side of the ship that is more vulnerable.
    But this feature is a sparse categorical feature, which, alone, will not be very
    helpful and might require some advanced transformation or feature engineering.
    So, to build a simple model, we will drop this feature. Also, the *passenger names*
    are not useful for the predictive model, and hence, we can remove them. There
    are some categorical features that need to be transformed for better model results.
    If you want to try out some more ideas for feature engineering, the following
    article might be helpful: [https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the lines of code for data preparation before the model training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformed DataFrame is shown in *Figure 4.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – DataFrame display after basic preprocessing and feature engineering'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_04_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – DataFrame display after basic preprocessing and feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for the model training part, we will use an XGBoost classifier. This is
    an ensemble learning algorithm and is not inherently interpretable. Based on the
    number of estimators, the complexity of the algorithm can vary. It can also be
    installed easily using the `pip` installer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The code to train the model after dividing into training and testing is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s define ![](img/B18216_04_039.png) as the prediction probability
    score, which will be later utilized by the LIME framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To provide model explanations, we can define the LIME object and explain the
    required data instance with just a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the visualizations provided by LIME for model explainability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Visualizations provided by the LIME framework to explain the
    model outcome'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_04_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Visualizations provided by the LIME framework to explain the model
    outcome
  prefs: []
  type: TYPE_NORMAL
- en: 'From *Figure 4.6*, we can see the explanations provided by the LIME framework
    with only a few lines of code. Now, let''s try to understand what the visualization
    is telling us:'
  prefs: []
  type: TYPE_NORMAL
- en: The leftmost bar plot is showing us the prediction probabilities, which can
    be treated as the model's confidence level in making the prediction. In *Figure
    4.6*, for the selected data instance, the model is 100% confident that the particular
    passenger would *survive*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second visualization from the left is probably the most important visualization
    that provides maximum explainability. It tells us that the most important feature,
    with a feature importance score of 38%, is the `Sex` feature, followed by `Age,`
    with a feature importance score of 26%. However, as illustrated in *Figure 4.6*,
    for the selected data instance, the `Sex`, `Pclass` (Passenger Class), `Fare`,
    and `Embarked_C` (Port of Embarkation as Cherbourg) features contribute toward
    the model outcome of *survival* along with their threshold scores learned from
    the entire dataset. In comparison, the `Age` feature, which is highlighted in
    blue, was more inclined toward predicting the outcome as *Did not Survive* as
    the particular passenger's age was 38 and, usually, passengers above the age of
    38 have lower chances of surviving the disaster. The threshold feature values
    learned by the LIME model are also in alignment with our own common sense and
    *a prior* knowledge. Even in the case of the actual incident of the sinking of
    the Titanic, which happened over 100 years ago, women and children were given
    the first preference to escape the sinking ship using the limited lifeboats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, first-class passengers who had paid higher ticket fares got a higher
    preference to take the lifeboats and, therefore, had higher chances of survival.
    So, the model explanation provided is human-friendly and consistent with our prior
    beliefs.
  prefs: []
  type: TYPE_NORMAL
- en: The third visualization from the left shows the top five features and their
    respective values. Here, the features highlighted in orange are contributing toward
    class 1, while features highlighted in blue are contributing toward class 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rightmost visualization is almost the same as the second visualization,
    except that it is presented in a different format, and it also provides local
    explanations for the particular data instance selected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we discussed in the previous section, LIME also provides a global understanding
    of the model alongside the local explanations. This is provided using the SP-LIME
    algorithm. This can be implemented using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 4.7* shows the visualizations obtained using SP-LIME:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Visualizations of diverse explanations obtained from SP-LIME
    to get a global understanding of the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_04_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Visualizations of diverse explanations obtained from SP-LIME to
    get a global understanding of the model
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.7* shows the output of the SP-LIME code. SP-LIME provides a diverse
    representative sample set of local explanations considering different instances
    of the model to get a global perspective of the black-box model. These visualizations
    show us the important features, the feature-important scores, and even the range
    of values for each of those features and how these features contribute toward
    either of the classes. All these properties and features of the entire LIME framework
    make it a powerful approach in which to provide model-agnostic human-understandable
    model interpretability to black-box models. Additionally, the framework is also
    very robust so the entire algorithm can be implemented with only a few lines of
    code.'
  prefs: []
  type: TYPE_NORMAL
- en: Although LIME has many advantages, unfortunately, there are some drawbacks of
    this algorithm that we should be aware of. Let's discuss them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Potential pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we learned how easily the LIME Python framework can
    be used to explain black-box models for a classification problem. But unfortunately,
    the algorithm does have certain limitations, and there are a few scenarios in
    which the algorithm is not effective:'
  prefs: []
  type: TYPE_NORMAL
- en: While providing interpretable explanations, a particular choice of interpretable
    data representation and interpretable model might still have a lot of limitations.
    While the underlying trained model might still be considered a black-box model,
    there is no assumption about the model that is made during the explanation process.
    However, certain representations are not powerful enough to represent some complex
    behaviors of the model. For example, if we are trying to build an image classifier
    to distinguish between black and white images and colored images, then the presence
    or absence of superpixels will not be useful to provide the explanations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As discussed earlier, LIME learns an interpretable model to provide local explanations.
    Usually, these interpretable models are linear and non-complex. However, suppose
    that the underlying black-box model is not linear, even in the locality of the
    prediction, so the LIME algorithm is not effective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME explanations are highly sensitive to any change in input data. Even a slight
    change in the input data can drastically alter the explanation instance provided
    by LIME.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For certain datasets, LIME explanations are not robust as, even for similar
    data instances, the explanations provided can be completely different. This might
    prevent end users from completely relying on the explanations provided by LIME.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is extremely prone to data drifts and label drifts. A slight drift
    between the training and the inference data can completely produce inconsistent
    explanations. The authors of the paper named *A study of data and label shift
    in the LIME framework*, *Rahnama* and *Boström* (https://arxiv.org/abs/1910.14421),
    mention certain experiments that can be used to evaluate the impact of data drift
    in the LIME framework. Due to this limitation, the goodness of approximation of
    the LIME explanations (also referred to as *fidelity*) is considered to be low.
    This is not expected in a good explanation method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explanations provided by LIME depend on the choice of the hyperparameters of
    the algorithm. Similar to most of the algorithms, even for the LIME algorithm,
    the choice of the hyperparameters can determine the quality of the explanations
    provided. Hyperparameter tuning is also difficult for the LIME algorithm as, usually,
    qualitative methods are adopted to evaluate the quality of the LIME explanations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many research works that indicate the other limitations of the LIME
    algorithm. I have mentioned some of these research works in the *References* section.
    I would strongly recommend that you go through those papers to get more details
    about certain limitations of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This brings us to the end of the chapter. In this chapter, we discussed LIME,
    one of the most widely adopted frameworks in XAI. Throughout this chapter, we
    discussed the intuition behind the workings of the algorithm and some important
    properties of the algorithm that make the generated explanations human-friendly.
    Additionally, we saw an end-to-end tutorial on how to use LIME for a practical
    use case to provide explainability to a black-box classification model. Even though
    we discussed some limitations of the LIME algorithm, due to its simplicity, LIME
    is still one of the most popular and widely used XAI frameworks. Hence, it is
    very important for us to discuss this algorithm and have a thorough understanding
    of the workings of the framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will apply the LIME framework to solve other types of
    machine learning problems using different types of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For additional information, please refer to the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Why Should I Trust You?" Explaining the Predictions of Any Classifier* by
    *Ribeiro et al*: [https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LIME - Local Interpretable Model-Agnostic Explanations*: [https://homes.cs.washington.edu/~marcotcr/blog/lime/](https://homes.cs.washington.edu/~marcotcr/blog/lime/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The LIME GitHub project: [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A study of data and label shift in the LIME framework* by *Rahnama* and *Boström*:
    [https://arxiv.org/abs/1910.14421](https://arxiv.org/abs/1910.14421)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What''s Wrong with LIME*: [https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612](https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Why model why? Assessing the strengths and limitations of LIME* by *Dieber*
    and *Kirrane* (2020): [https://arxiv.org/pdf/2012.00093.pdf](https://arxiv.org/pdf/2012.00093.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
