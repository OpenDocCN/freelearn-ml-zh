- en: '*Chapter 4*: LIME for Model Interpretability'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：LIME用于模型可解释性'
- en: In the previous chapters, we discussed the various technical concepts of **Explainable
    AI** (**XAI**) that are needed to build trustworthy AI systems. Additionally,
    we looked at certain practical examples and demonstrations using various Python
    frameworks to implement the concepts of practical problem solving, which are given
    in the GitHub code repository of this chapter. XAI has been an important research
    topic for quite some time, but it is only very recently that all organizations
    have started to adopt XAI as a part of the solution life cycle for solving business
    problems using AI. One such popular approach is **Local Interpretable Model-Agnostic
    Explanations** (**LIME**), which has been widely adopted to provide model-agnostic
    local explainability. The LIME Python library is a robust framework that provides
    human-friendly explanations to tabular, text, and image data and helps in interpreting
    black-box supervised machine learning algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了构建可信赖人工智能系统所需的各种**可解释人工智能**（**XAI**）的技术概念。此外，我们还查看了一些使用各种Python框架实现的实际示例和演示，这些示例和演示在本书本章的GitHub代码库中给出。XAI已经是一个重要的研究课题有一段时间了，但直到最近，所有组织才开始将XAI作为解决使用人工智能解决商业问题的解决方案生命周期的一部分来采用。其中一种流行的方法是**局部可解释模型无关解释**（**LIME**），它已被广泛采用以提供模型无关的局部可解释性。LIME
    Python库是一个健壮的框架，它为表格、文本和图像数据提供人性化的解释，并有助于解释黑盒监督机器学习算法。
- en: In this chapter, you will be introduced to the LIME framework, which has made
    a significant impact in the field of XAI. We will discuss the workings of the
    LIME algorithm for global and local model explainability. Also, I will demonstrate
    an example in which the LIME Python framework can be used in practice. I will
    cover the limitations of this framework that you should be aware of.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解LIME框架，它在XAI领域产生了重大影响。我们将讨论LIME算法在全局和局部模型可解释性方面的作用。此外，我将演示一个示例，说明LIME
    Python框架在实际中的应用。我将介绍这个框架的限制，你应该注意这些限制。
- en: 'So, in this chapter, we will discuss the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将讨论以下主要主题：
- en: An intuitive understanding of LIME
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对LIME的直观理解
- en: What makes LIME a good model explainer?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么使LIME成为一个好的模型解释器？
- en: Submodular pick (SP-LIME)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子模块选择（SP-LIME）
- en: A practical example of using LIME for classification problems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LIME解决分类问题的实际示例
- en: Potential pitfalls
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能的陷阱
- en: Without further ado, let's get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，让我们开始吧。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter is slightly more technical than the previous chapters covered in
    this book. The code and dataset resources can be downloaded or cloned from the
    GitHub repository for this chapter, which is located at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04).
    Similar to the previous chapters, we will be using Python and Jupyter notebooks
    to run the code and generate the necessary outputs. Other important Python frameworks
    that are necessary to run the code will be mentioned in the notebooks with further
    relevant details to understand the code implementation of these concepts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术性比本书前几章的内容要高一些。本章的代码和数据集资源可以从位于[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04)的GitHub代码库中下载或克隆。与前面的章节类似，我们将使用Python和Jupyter笔记本来运行代码并生成必要的输出。在笔记本中还将提到运行代码所需的其他重要Python框架，并附带进一步的相关细节，以理解这些概念代码的实现。
- en: Intuitive understanding of LIME
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对LIME的直观理解
- en: LIME is a novel, model-agnostic, local explanation technique used for interpreting
    black-box models by learning a local model around the predictions. LIME provides
    an intuitive global understanding of the model, which is helpful for non-expert
    users, too. The technique was first proposed in the research paper *"Why Should
    I Trust You?" Explaining the Predictions of Any Classifier* by *Ribeiro et al*.
    (https://arxiv.org/abs/1602.04938). The Python library can be installed from the
    GitHub repository at [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime).
    The algorithm does a pretty good job of interpreting any classifier or regressor
    in faithful ways by using approximated local interpretable models. It provides
    a global perspective to establish trust for any black-box model; therefore, it
    allows you to identify interpretable models over human-interpretable representation,
    which is locally faithful to the algorithm. So, it mainly functions by *learning
    interpretable data representations*, *maintaining a balance in a fidelity-interpretability
    trade-off*, and *searching for local explorations*. Let's look at each one of
    them in detail.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 是一种新颖的、模型无关的局部解释技术，通过学习预测周围的局部模型来解释黑盒模型。LIME 为模型提供了一个直观的全局理解，这对非专家用户也很有帮助。这项技术最初在
    Ribeiro 等人撰写的科研论文 *"为什么我应该相信你？解释任何分类器的预测"* 中被提出。（https://arxiv.org/abs/1602.04938）。Python
    库可以从 GitHub 仓库 [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)
    安装。该算法通过使用近似局部可解释模型，以忠实的方式解释任何分类器或回归器，做得相当不错。它提供了一个全局视角，以建立对任何黑盒模型的信任；因此，它允许你识别在人类可解释表示中可解释的模型，这些表示在局部上是忠实于算法的。所以，它主要通过
    *学习可解释的数据表示*、*在保真度-可解释性权衡中保持平衡* 和 *寻找局部探索* 来发挥作用。让我们详细看看每一个方面。
- en: Learning interpretable data representations
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习可解释的数据表示
- en: LIME does a pretty good job in differentiating between impactful features and
    choosing interpretable data representations that are understandable to any non-expert
    user regardless of the actual complex features used by the algorithm. For example,
    when explaining models trained on unstructured data such as images, the actual
    algorithm might use complex numerical feature vectors for its decision-making
    process, but these numerical feature values are incomprehensible to any non-technical
    end user. In comparison, if the explainability is provided in terms of the presence
    or absence of a region of interest or superpixel (that is, a continuous patch
    of pixels) within the image, that is a human-interpretable way of providing explainability.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 在区分影响特征和选择任何非专家用户都能理解的可解释数据表示方面做得相当不错，无论算法实际使用的复杂特征如何。例如，当解释在图像等非结构化数据上训练的模型时，实际算法可能使用复杂的数值特征向量进行决策过程，但这些数值特征值对任何非技术终端用户来说都是难以理解的。相比之下，如果解释是通过图像中感兴趣区域或超像素（即像素的连续块）的存在或不存在来提供的，那么这是一种人类可解释的解释方式。
- en: Similarly, for text data, instead of using word-embedding vector values to interpret
    models, a better way to provide a human-interpretable explanation is by using
    examples of the presence or absence of certain words used to describe the target
    outcome of the model. So, mathematically speaking, the original representation
    of a data instance being explained is denoted by ![](img/B18216_04_001.png), where
    *d* is the entire dimension of data. A binary vector of interpretable data representations
    is mathematically denoted by ![](img/B18216_04_002.png). Intuitively speaking,
    the algorithm tries to denote the presence or absence of human-interpretable data
    representations to explain any black-box model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于文本数据，与其使用词嵌入向量值来解释模型，不如通过使用描述模型目标结果的某些单词的存在或不存在来提供一种更好的、人类可解释的解释方式。从数学上讲，被解释的数据实例的原始表示用
    ![](img/B18216_04_001.png) 表示，其中 *d* 是数据的整个维度。一个可解释数据表示的二进制向量在数学上表示为 ![](img/B18216_04_002.png)。直观地说，算法试图表示人类可解释数据表示的存在或不存在，以解释任何黑盒模型。
- en: '*Figure 4.1* shows how LIME tries to divide the input image data into human-interpretable
    components that are later used to explain black-box models in a manner that is
    understandable to any non-technical user:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.1* 展示了 LIME 如何尝试将输入图像数据划分为人类可解释的组件，这些组件随后被用来以任何非技术用户都能理解的方式解释黑盒模型：'
- en: '![Figure 4.1 – How LIME transforms an image into human-interpretable components'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1 – LIME 如何将图像转换为人类可解释的组件'
- en: '](img/B18216_04_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18216_04_001.jpg](img/B18216_04_001.jpg)'
- en: Figure 4.1 – How LIME transforms an image into human-interpretable components
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – LIME如何将图像转换为人类可解释的组件
- en: Next, let's discuss how to maintain the fidelity-interpretability trade-off.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论如何维持保真度-可解释性权衡。
- en: Maintaining a balance in the fidelity-interpretability trade-off
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维持保真度-可解释性权衡的平衡
- en: LIME makes use of inherently interpretable models such as decision trees, linear
    models, and rule-based heuristic models to provide explanations to non-expert
    users with visual or textual artifacts. Mathematically speaking, this explanation
    is a model that can be denoted by ![](img/B18216_04_003.png), where ![](img/B18216_04_004.png)
    is the entire set of potentially interpretable models and the domain of ![](img/B18216_04_005.png)
    is represented with another binary vector, ![](img/B18216_04_006.png), which represents
    the presence or absence of interpretable components. Additionally, the algorithm
    tries to measure the *complexity* of an explanation along with its *interpretability*.
    For example, even in interpretable models such as decision trees, the depth of
    the tree is a measure of its complexity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LIME利用固有的可解释模型，如决策树、线性模型和基于规则的启发式模型，通过视觉或文本工件向非专家用户提供解释。从数学上讲，这种解释是一个可以用![](img/B18216_04_003.png)表示的模型，其中![](img/B18216_04_004.png)是所有可能的可解释模型的全集，而![](img/B18216_04_005.png)的域用另一个二进制向量，![](img/B18216_04_006.png)表示，它表示可解释组件的存在或不存在。此外，算法还试图衡量解释的**复杂度**及其**可解释性**。例如，即使在可解释模型如决策树中，树的深度是其复杂度的一个度量。
- en: 'Mathematically speaking, the complexity of an interpretable model is denoted
    by ![](img/B18216_04_007.png). LIME tries to maintain **local fidelity** while
    providing explanations. This means that the algorithm tries to replicate the behavior
    of the model in proximity to the individual data instance being predicted. So,
    mathematically, the inventors of this algorithm used a function, ![](img/B18216_04_008.png),
    to measure the proximity between any data instances, ![](img/B18216_04_009.png),
    thus defining the locality around the original representation, ![](img/B18216_04_010.png).
    Now, if the probability function, ![](img/B18216_04_011.png), defines the probability
    that ![](img/B18216_04_012.png) belongs to a certain class, then to approximate
    ![](img/B18216_04_013.png), the LIME algorithm tries to measure how unfaithful
    ![](img/B18216_04_014.png) is with a proximity function, ![](img/B18216_04_015.png).
    This entire operation is denoted by the ![](img/B18216_04_016.png) function. Therefore,
    the algorithm tries to minimize the locality-aware loss function, ![](img/B18216_04_017.png),
    while maintaining ![](img/B18216_04_018.png) to be a low value. This is so that
    it is easily explainable to any non-expert user. The measure of an interpretability
    local fidelity trade-off is approximated by the following mathematical function:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来说，一个可解释模型的复杂度用 ![](img/B18216_04_007.png) 表示。LIME试图在提供解释的同时保持**局部保真度**。这意味着算法试图复制模型在预测的个体数据实例附近的的行为。因此，从数学上讲，该算法的发明者使用了一个函数，![](img/B18216_04_008.png)，来衡量任何数据实例，![](img/B18216_04_009.png)，之间的接近程度，从而定义了原始表示，![](img/B18216_04_010.png)周围的局部性。现在，如果概率函数，![](img/B18216_04_011.png)，定义了![](img/B18216_04_012.png)属于某个类别的概率，那么为了近似![](img/B18216_04_013.png)，LIME算法试图用一个接近函数，![](img/B18216_04_014.png)，来衡量![](img/B18216_04_015.png)的不忠实程度。这个整个操作用![](img/B18216_04_016.png)函数表示。因此，算法试图最小化局部感知损失函数，![](img/B18216_04_017.png)，同时保持![](img/B18216_04_018.png)为一个低值。这样，它就可以很容易地向任何非专家用户解释。可解释性局部保真度权衡的度量可以用以下数学函数近似：
- en: '![](img/B18216_04_019.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18216_04_019.png)'
- en: Hence, this trade-off measure depends on the interpretable models, ![](img/B18216_04_020.png),
    the fidelity function, ![](img/B18216_04_021.png), and the complexity measure,
    ![](img/B18216_04_022.png).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个权衡度量依赖于可解释模型，![](img/B18216_04_020.png)，保真度函数，![](img/B18216_04_021.png)，和复杂度度量，![](img/B18216_04_022.png)。
- en: Searching for local explorations
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找局部探索
- en: The LIME algorithm is *model-agnostic*. This means when we try to minimize the
    *locality-aware loss function*, ![](img/B18216_04_023.png), without any assumption
    about *f*. Also, LIME maintains local fidelity by taking samples that are weighted
    by ![](img/B18216_04_024.png) while approximating ![](img/B18216_04_025.png).
    Nonzero samples of ![](img/B18216_04_026.png) are drawn uniformly at random to
    sample instances around ![](img/B18216_04_0261.png). Let's suppose there is a
    perturbed sample containing fractions of nonzero elements of ![](img/B18216_04_027.png),
    which is denoted by ![](img/B18216_04_029.png). The algorithm tries to recover
    samples from the original representation, ![](img/B18216_04_030.png), to approximate
    ![](img/B18216_04_031.png). Then, ![](img/B18216_04_032.png) is used as a label
    for the explanation model, ![](img/B18216_04_033.png).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LIME算法是**模型无关的**。这意味着当我们尝试最小化**局部感知损失函数**，![](img/B18216_04_023.png)，对**f**没有任何假设时。同时，LIME通过取权重为![](img/B18216_04_024.png)的样本来保持局部保真度，同时近似![](img/B18216_04_025.png)。![](img/B18216_04_026.png)的非零样本被随机均匀抽取，以采样![](img/B18216_04_0261.png)周围的实例。假设存在一个包含![](img/B18216_04_027.png)非零元素分数的扰动样本，用![](img/B18216_04_029.png)表示。算法试图从原始表示，![](img/B18216_04_030.png)，中恢复样本，以近似![](img/B18216_04_031.png)。然后，![](img/B18216_04_032.png)被用作解释模型，![](img/B18216_04_033.png)的标签。
- en: '*Figure 4.2* represents an example presented in the original paper of the LIME
    framework at [https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf),
    which intuitively explains the working of the algorithm using a visual representation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.2*展示了在[https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf)原始LIME框架论文中的一个例子，它通过视觉表示直观地解释了算法的工作原理：'
- en: '![Figure 4.2 – Explaining the working of the LIME algorithm intuitively'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.2 – 直观解释LIME算法的工作原理'
- en: '](img/B18216_04_002.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_04_002.jpg)'
- en: Figure 4.2 – Explaining the working of the LIME algorithm intuitively
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – 直观解释LIME算法的工作原理
- en: In *Figure 4.2*, the curve separating the light blue and pink backgrounds is
    considered a complex ![](img/B18216_04_034.png) decision function of a black-box
    model. Since the decision function is not linear, approximating it using linear
    models is not efficient. The crosses and the dots represent training data belonging
    to two different classes. The bold cross represents the inference data instance
    being explained. The algorithm functions by sampling instances to get predictions
    using *f*. Then, the algorithm assigns weight by the proximity to the data instance
    being explained. In the preceding diagram, based on the proximity of the data
    instance, the sizes of the red crosses and blue dots are varied. So, the instances
    that are sampled are both in closer proximity to *x*, having a higher weight from
    ![](img/B18216_04_035.png), and far away from it, thus having a lower weight of
    ![](img/B18216_04_036.png). The original black-box model might be too complex
    to provide a global explanation, but the LIME framework can provide explanations
    that are appropriate for the local data instance, ![](img/B18216_04_037.png).
    The learned explanation is illustrated by the dashed line, which is locally faithful
    with a global perspective.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.2*中，分隔浅蓝色和粉红色的曲线被认为是黑盒模型的复杂![](img/B18216_04_034.png)决策函数。由于决策函数是非线性的，使用线性模型来近似它并不高效。十字和点代表属于两个不同类别的训练数据。粗十字代表正在解释的推理数据实例。算法通过采样实例来获取使用*f*的预测。然后，算法根据与被解释数据实例的邻近度分配权重。在前面的图中，根据数据实例的邻近度，红色十字和蓝色点的尺寸被调整。因此，采样的实例既更接近*x*，从![](img/B18216_04_035.png)中获得更高的权重，又远离它，因此具有较低的![](img/B18216_04_036.png)权重。原始的黑盒模型可能过于复杂，无法提供全局解释，但LIME框架可以提供适合局部数据实例，![](img/B18216_04_037.png)的解释。通过虚线表示的学习解释在局部上是忠诚的，具有全局视角。
- en: '*Figure 4.3* illustrates a far more intuitive understanding of the LIME algorithm.
    From the original image, the algorithm generates a set of perturbed data instances:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.3*展示了LIME算法的更直观理解。从原始图像中，算法生成了一组扰动数据实例：'
- en: '![Figure 4.3 – Predictions being explained using LIME'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.3 – 使用LIME解释的预测'
- en: '](img/B18216_04_003.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_04_003.jpg)'
- en: Figure 4.3 – Predictions being explained using LIME
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 使用LIME解释的预测
- en: The perturbed instances, as shown in *Figure 4.3*, are created by switching
    some of the interpretable components off. In the case of images, as shown in the
    preceding diagram, it is done by turning certain components gray. Then, the black-box
    model is applied to each of the perturbed instances that are generated, and the
    probability of the instance being predicted as the final outcome of the model
    is calculated. Then, an interpretable model (such as a simple locally weighted
    linear model) is learned on the dataset, and finally, the superpixels having the
    maximum positive weights are considered for the final explanation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4.3*所示，扰动实例是通过关闭一些可解释组件创建的。在图像的情况下，如前图所示，这是通过将某些组件变为灰色来实现的。然后，将黑盒模型应用于生成的每个扰动实例，并计算实例被预测为模型最终结果的概率。然后，在数据集上学习一个可解释的模型（例如简单的局部加权线性模型），最后，考虑具有最大正权重的超像素作为最终解释。
- en: In the next section, let's discuss why LIME is a good model explainer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们讨论为什么LIME是一个好的模型解释器。
- en: What makes LIME a good model explainer?
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么使LIME成为一个好的模型解释器？
- en: 'LIME enables non-expert users to understand the working of untrustworthy black-box
    models. The following properties of LIME make it a good model explainer:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LIME使非专家用户能够理解不可信的黑盒模型的工作原理。以下LIME的特性使其成为一个好的模型解释器：
- en: '**Human interpretable**: As discussed in the previous section, LIME provides
    explanations that are easy to understand, as it provides a qualitative way to
    compare the components of the input data with the model outcome.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类可解释性**：如前节所述，LIME提供易于理解的解释，因为它提供了一种定性的方法来比较输入数据的组件与模型结果。'
- en: '**Model-agnostic**: In the previous chapters, although you have learned about
    various model-specific explanation methods, it is always an advantage if the explanation
    method can be used to provide explainability for any black-box model. LIME does
    not make any assumptions about the model while providing the explanations and
    can work with any model.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型无关性**：在前几章中，尽管你已经学习了各种特定模型的解释方法，但如果解释方法可以用于为任何黑盒模型提供可解释性，这始终是一个优势。LIME在提供解释时不对模型做任何假设，并且可以与任何模型一起工作。'
- en: '**Local fidelity**: LIME tries to replicate the behavior of the entire model
    by exploring the proximity of the data instance being predicted. So, it provides
    local explainability to the data instance being used for prediction. This is important
    for any non-technical user to understand the exact reason for the model''s decision-making
    process.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部保真度**：LIME通过探索被预测的数据实例的邻近性来尝试复制整个模型的行为。因此，它为用于预测的数据实例提供局部可解释性。这对于任何非技术用户理解模型决策过程的准确原因非常重要。'
- en: '**Global intuition**: Although the algorithm provides local explainability,
    it does try to explain a representative set to the end users, thereby providing
    a global perspective to the functioning of the model. SP-LIME provides a global
    understanding of the model by explaining a collection of data instances. This
    will be covered in more detail in the next section.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局直觉**：尽管该算法提供了局部可解释性，但它确实试图向最终用户解释一个代表性集合，从而为模型的运作提供全局视角。SP-LIME通过解释一组数据实例来提供对模型的全局理解。这将在下一节中更详细地介绍。'
- en: Now that we understand the key advantages of the LIME framework, in the next
    section, let's discuss the submodular pick algorithm of LIME, which is used for
    extracting global explainability.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了LIME框架的关键优势，那么在下一节中，让我们讨论LIME的子模选择算法，该算法用于提取全局可解释性。
- en: SP-LIME
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SP-LIME
- en: In order to make explanation methods more trustworthy, providing an explanation
    to a single data instance (that is, a local explanation) is not always sufficient,
    and the end user might want a global understanding of the model to have higher
    reliability on the robustness of the model. So, the SP-LIME algorithm tries to
    run the explanations on multiple diverse, yet carefully selected, sets of instances
    and returns non-redundant explanations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使解释方法更加可信，对单个数据实例（即局部解释）的解释并不总是足够的，最终用户可能希望对模型有全局的理解，以便对模型的鲁棒性有更高的可靠性。因此，SP-LIME算法试图在多个多样化、但仔细选择的数据实例集上运行解释，并返回非冗余的解释。
- en: Now, let me provide an intuitive understanding of the SP-LIME algorithm. The
    algorithm considers that the time required to go through all the individual local
    explanations is limited and is a constraint. So, the number of explanations that
    the end users are willing to examine to explain a model is the budget of the algorithm
    denoted by *B*. Let's suppose that *X* denotes the set of instances; the task
    of selecting *B* instances for the end user to analyze for model explainability
    is defined as the **pick step**. The pick step is independent of the existence
    of the explanation and it needs to provide *non-redundant explanations* by picking
    up a diverse representative set of instances to explain how the model is behaving
    considering a global perspective. Therefore, the algorithm tries to avoid picking
    up instances with similar explanations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我提供一个关于SP-LIME算法的直观理解。算法认为，通过所有单个局部解释所需的时间是有限的，并且是一个约束。因此，最终用户愿意检查以解释模型的数量是算法的预算，用*B*表示。假设*X*表示实例集合；选择*B*个实例供最终用户分析以进行模型可解释性的任务定义为**选择步骤**。选择步骤与解释的存在无关，并且它需要通过选择一个多样化的代表性实例集来提供*非冗余解释*，以解释模型从全局角度的行为。因此，算法试图避免选择具有相似解释的实例。
- en: 'Mathematically, this idea is represented using the *Explanation Matrix* (*W*),
    in which *W* *= n * d''*, such that *n* is the number of samples and *d''* is
    the human interpretable features. The algorithm also uses a *Global importance
    component matrix* (*I*), in which for each component of *j*, *I(j)* represent
    the global importance in the explanation space. Intuitively speaking, *I* is formulated
    in a way to assign higher scores to features, which explains many instances of
    the data. The set of important features that are considered for the explanations
    is denoted by *V*. So, combining all these parameters, the algorithm tries to
    learn a *non-redundant coverage intuition function*, *c(V,W,I)*. The non-redundant
    coverage intuition tries to compute the collective importance of all features
    that appear in at least one instance in set *V*. However, the *pick problem* is
    about *maximizing the weighted coverage function*. This is denoted by the following
    equation:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这个想法是通过*解释矩阵*（*W*）来表示的，其中*W* = n * d'，其中*n*是样本数量，*d'*是可解释的人类特征。算法还使用一个*全局重要性组件矩阵*（*I*），其中对于*I*的每个组件*j*，*I(j)*代表解释空间中的全局重要性。直观地说，*I*是以一种方式制定的，为解释数据中的许多实例分配更高的分数。考虑用于解释的重要特征集合用*V*表示。因此，结合所有这些参数，算法试图学习一个*非冗余覆盖直觉函数*，*c(V,W,I)*。非冗余覆盖直觉试图计算至少在集合*V*中的一个实例中出现的所有特征的集体重要性。然而，*选择问题*是关于*最大化加权覆盖函数*的。这由以下方程表示：
- en: '![](img/B18216_04_038.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18216_04_038.png)'
- en: 'The details about the algorithm that we just covered in this section might
    be slightly overwhelming to understand for certain readers. However, intuitively,
    the algorithm tries to cover the following steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中刚刚覆盖的算法的细节可能对某些读者来说理解起来有些令人困惑。然而，直观地说，算法试图覆盖以下步骤：
- en: The explanation model is run on all instances (*x*).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释模型在所有实例（*x*）上运行。
- en: The global importance of all individual components is computed.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有单个组件的全局重要性。
- en: Then, the algorithm tries to maximize the non-redundant coverage intuition function
    (*c*) by iteratively adding instances with the highest maximum coverage gain.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，算法通过迭代添加具有最高最大覆盖增益的实例来尝试最大化非冗余覆盖直觉函数（*c*）。
- en: Finally, the algorithm tries to obtain the representative non-redundant explanation
    set (*V*) and return it.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，算法试图获得代表性的非冗余解释集（*V*）并将其返回。
- en: In the next section, we will cover how the LIME Python framework can be used
    for classification problems using code examples.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过代码示例介绍如何使用LIME Python框架来解决分类问题。
- en: A practical example of using LIME for classification problems
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LIME解决分类问题的实际示例
- en: 'So far, we have covered most of the in-depth conceptual understanding that
    is needed regarding the LIME algorithm. In this section, we will try to explore
    the LIME Python framework for explaining classification problems. The framework
    is available as an open source project on GitHub at [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime).
    Installing LIME in Python can be done easily using the `pip` installer inside
    the Jupyter notebook:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了关于LIME算法所需的大部分深入概念理解。在本节中，我们将尝试探索用于解释分类问题的LIME Python框架。该框架可在GitHub上作为开源项目找到[https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)。在Jupyter笔记本中使用`pip`安装程序可以轻松安装LIME：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The complete notebook version of the tutorial is accessible from the GitHub
    repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb).
    However, for now, I will try to walk you through the entire code so that you understand
    the code in detail. Once the LIME framework has been installed, quickly verify
    whether the installation was successful or not by importing the library:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 教程的完整笔记本版本可在GitHub仓库[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb)中找到。然而，现在，我将尝试带你详细了解整个代码。一旦LIME框架安装完成，通过导入库快速验证安装是否成功：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If the import was successful, you can easily proceed with the next steps; otherwise,
    you need to check what went wrong while installing the framework. But usually,
    you should not face any errors or any dependency conflicts as installing the library
    is quite straightforward. For this tutorial, we will use the *Titanic dataset*
    ([https://www.openml.org/search?type=data&sort=runs&id=40945&status=active](https://www.openml.org/search?type=data&sort=runs&id=40945&status=active)).
    This is one of the classic machine learning datasets used for predicting the survival
    of passengers on the Titanic. So, this is a binary classification problem that
    can be solved using machine learning. Although this is a classic dataset that
    is not very complex, it contains all types of features such as *Categorical*,
    *Ordinal*, *Continuous*, and even certain *identifiers* that are not relevant
    for the classification, thereby making this an interesting dataset to work with.
    To make it easier for you to execute notebooks, I have downloaded and provided
    the dataset after some slight modifications in the code repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04/dataset](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04/dataset).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果导入成功，你可以轻松地继续下一步；否则，你需要检查在安装框架时出了什么问题。但通常情况下，你不会遇到任何错误或任何依赖冲突，因为安装库相当直接。对于这个教程，我们将使用*泰坦尼克数据集*([https://www.openml.org/search?type=data&sort=runs&id=40945&status=active](https://www.openml.org/search?type=data&sort=runs&id=40945&status=active))。这是用于预测泰坦尼克号乘客生存的经典机器学习数据集之一。因此，这是一个可以用机器学习解决的问题的二分类问题。尽管这是一个不太复杂的经典数据集，但它包含了所有类型的特征，如*分类*、*有序*、*连续*，甚至某些对分类不相关的*标识符*，这使得这个数据集成为很有趣的工作对象。为了使你更容易执行笔记本，我在代码仓库[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04/dataset](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04/dataset)中进行了一些轻微修改后下载并提供了数据集。
- en: Titanic dataset
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克数据集
- en: 'The original Titanic dataset, describing the survival status of individual
    passengers on the Titanic. The titanic data does not contain information from
    the crew, but it does contain actual ages of half of the passengers. The principal
    source for data about Titanic passengers is the Encyclopedia Titanica. The datasets
    used here were begun by a variety of researchers. One of the original sources
    is Eaton & Haas (1994) Titanic: Triumph and Tragedy, Patrick Stephens Ltd, which
    includes a passenger list created by many researchers and edited by Michael A.
    Findlay.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 描述泰坦尼克号上个别乘客生存状态的原始泰坦尼克数据集。泰坦尼克数据不包含船员的信息，但它包含了半数乘客的实际年龄。关于泰坦尼克号乘客的数据主要来源于《泰坦尼克号百科全书》。这里使用的数据集由各种研究人员开始。其中一个原始来源是Eaton
    & Haas (1994)的《泰坦尼克号：胜利与悲剧》，由Patrick Stephens Ltd出版，其中包括由许多研究人员创建并由Michael A.
    Findlay编辑的乘客名单。
- en: Thomas Cason of UVa has greatly updated and improved the titanic data frame
    using the Encyclopedia Titanica and created the dataset here. Some duplicate passengers
    have been dropped, many errors corrected, many missing ages filled in, and new
    variables created.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: UVa的Thomas Cason使用《泰坦尼克号百科全书》对titanic数据框进行了大量更新和改进，并在此创建了数据集。一些重复的乘客已被删除，许多错误得到纠正，许多缺失的年龄得到填补，并创建了新的变量。
- en: 'After installing and importing all the required modules, first, we will start
    by loading the dataset from the directory as a pandas DataFrame:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装和导入所有必需的模块后，首先，我们将从目录中加载数据集作为pandas DataFrame：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When you try to visualize the DataFrame using the `head` method from pandas,
    you will get a glimpse of the dataset, as shown in *Figure 4.4*. Often, this step
    helps you to get a quick idea about how to understand your data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试使用pandas的`head`方法可视化DataFrame时，你会看到数据集的一个快照，如图*图4.4*所示。通常，这一步可以帮助你快速了解如何理解你的数据：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following diagram shows a glimpse of the pandas DataFrame used for this
    example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表展示了用于此例的pandas DataFrame的一个快照：
- en: '![Figure 4.4 – Displaying the dataset as a pandas DataFrame (left-hand side)
    and a data dictionary (right-hand side)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4 – 以pandas DataFrame（左侧）和数据字典（右侧）显示数据集'
- en: '](img/B18216_04_004.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_04_004.jpg)'
- en: Figure 4.4 – Displaying the dataset as a pandas DataFrame (left-hand side) and
    a data dictionary (right-hand side)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 以pandas DataFrame（左侧）和数据字典（右侧）显示数据集
- en: 'For this particular example, we are not concerned about getting a highly efficient
    machine learning model, but rather our focus is on using LIME to produce human-friendly
    explanations in a few lines of code. So, we will skip doing rigorous **Exploratory
    Data Analysis** (**EDA**) or feature engineering steps. However, I do highly encourage
    all of you to perform these steps as a good practice. As we can see from the dataset,
    certain features such as *Passenger ID* and *Ticket Number* are identifiers that
    can be ignored. The *Cabin Number* feature is an interesting feature, especially
    as it could indicate a certain wing, floor, or side of the ship that is more vulnerable.
    But this feature is a sparse categorical feature, which, alone, will not be very
    helpful and might require some advanced transformation or feature engineering.
    So, to build a simple model, we will drop this feature. Also, the *passenger names*
    are not useful for the predictive model, and hence, we can remove them. There
    are some categorical features that need to be transformed for better model results.
    If you want to try out some more ideas for feature engineering, the following
    article might be helpful: [https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的例子，我们并不关心获得一个高度高效的机器学习模型，而是我们的重点是使用LIME在几行代码中生成人类友好的解释。因此，我们将跳过进行严格的**探索性数据分析**（**EDA**）或特征工程步骤。然而，我非常鼓励你们所有人都将这些步骤作为良好的实践。正如我们可以从数据集中看到的那样，某些特征，如*乘客ID*和*票号*是标识符，可以忽略。*船舱号*特征是一个有趣的特征，特别是因为它可能表明船的某个翼、楼层或侧面更容易受到损害。但是，这个特征是一个稀疏分类特征，单独来看，可能不会非常有帮助，可能需要一些高级的转换或特征工程。因此，为了构建一个简单的模型，我们将删除这个特征。此外，*乘客姓名*对于预测模型没有用，因此我们可以将其删除。有一些分类特征需要转换以获得更好的模型结果。如果你想尝试更多关于特征工程的想法，以下文章可能有所帮助：[https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/).
- en: 'Here are the lines of code for data preparation before the model training:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是模型训练前的数据准备代码行：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The transformed DataFrame is shown in *Figure 4.5*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的DataFrame在*图4.5*中显示：
- en: '![Figure 4.5 – DataFrame display after basic preprocessing and feature engineering'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.5 – 基本预处理和特征工程后的DataFrame显示'
- en: '](img/B18216_04_005.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_04_005.jpg)'
- en: Figure 4.5 – DataFrame display after basic preprocessing and feature engineering
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 基本预处理和特征工程后的DataFrame显示
- en: 'Now, for the model training part, we will use an XGBoost classifier. This is
    an ensemble learning algorithm and is not inherently interpretable. Based on the
    number of estimators, the complexity of the algorithm can vary. It can also be
    installed easily using the `pip` installer:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于模型训练部分，我们将使用XGBoost分类器。这是一个集成学习算法，本身不具有可解释性。根据估计器的数量，算法的复杂性可以有所不同。它也可以使用`pip`安装程序轻松安装：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The code to train the model after dividing into training and testing is as
    follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下代码用于在划分为训练集和测试集后训练模型：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, let''s define ![](img/B18216_04_039.png) as the prediction probability
    score, which will be later utilized by the LIME framework:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将![图片 B18216_04_039.png]定义为预测概率分数，该分数将随后被LIME框架利用：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To provide model explanations, we can define the LIME object and explain the
    required data instance with just a few lines of code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供模型解释，我们可以通过几行代码定义LIME对象，并解释所需的数据实例：
- en: '[PRE29]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following diagram shows the visualizations provided by LIME for model explainability:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了LIME为模型可解释性提供的可视化：
- en: '![Figure 4.6 – Visualizations provided by the LIME framework to explain the
    model outcome'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – LIME框架提供的模型结果可视化'
- en: '](img/B18216_04_006.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18216_04_006.jpg]'
- en: Figure 4.6 – Visualizations provided by the LIME framework to explain the model
    outcome
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – LIME框架提供的模型结果可视化
- en: 'From *Figure 4.6*, we can see the explanations provided by the LIME framework
    with only a few lines of code. Now, let''s try to understand what the visualization
    is telling us:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 4.6*中，我们可以看到LIME框架仅用几行代码提供的解释。现在，让我们尝试理解这个可视化在告诉我们什么：
- en: The leftmost bar plot is showing us the prediction probabilities, which can
    be treated as the model's confidence level in making the prediction. In *Figure
    4.6*, for the selected data instance, the model is 100% confident that the particular
    passenger would *survive*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最左侧的条形图显示了预测概率，这可以被视为模型在做出预测时的置信水平。在*图 4.6*中，对于所选数据实例，模型有100%的信心认为该乘客会*幸存*。
- en: The second visualization from the left is probably the most important visualization
    that provides maximum explainability. It tells us that the most important feature,
    with a feature importance score of 38%, is the `Sex` feature, followed by `Age,`
    with a feature importance score of 26%. However, as illustrated in *Figure 4.6*,
    for the selected data instance, the `Sex`, `Pclass` (Passenger Class), `Fare`,
    and `Embarked_C` (Port of Embarkation as Cherbourg) features contribute toward
    the model outcome of *survival* along with their threshold scores learned from
    the entire dataset. In comparison, the `Age` feature, which is highlighted in
    blue, was more inclined toward predicting the outcome as *Did not Survive* as
    the particular passenger's age was 38 and, usually, passengers above the age of
    38 have lower chances of surviving the disaster. The threshold feature values
    learned by the LIME model are also in alignment with our own common sense and
    *a prior* knowledge. Even in the case of the actual incident of the sinking of
    the Titanic, which happened over 100 years ago, women and children were given
    the first preference to escape the sinking ship using the limited lifeboats.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从左数第二个可视化可能是最重要的可视化，它提供了最大的可解释性。它告诉我们最重要的特征是`Sex`特征，其特征重要性得分为38%，其次是`Age`，其特征重要性得分为26%。然而，如*图
    4.6*所示，对于所选数据实例，`Sex`、`Pclass`（乘客等级）、`Fare`和`Embarked_C`（从瑟堡港出发）特征与从整个数据集中学习到的阈值分数一起，对模型的*幸存*结果做出了贡献。相比之下，蓝色的`Age`特征更倾向于预测结果为*未幸存*，因为该乘客的年龄是38岁，通常来说，38岁以上的乘客在灾难中幸存的机会较低。LIME模型学习到的阈值特征值也与我们的常识和*先验*知识一致。即使在100多年前发生的泰坦尼克号沉船事件中，妇女和儿童被优先考虑使用有限的救生艇逃离沉船。
- en: Similarly, first-class passengers who had paid higher ticket fares got a higher
    preference to take the lifeboats and, therefore, had higher chances of survival.
    So, the model explanation provided is human-friendly and consistent with our prior
    beliefs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，支付了更高票价的一等舱乘客有更高的优先权使用救生艇，因此有更高的生存机会。所以，提供的模型解释是人性化的，并且与我们之前的信念一致。
- en: The third visualization from the left shows the top five features and their
    respective values. Here, the features highlighted in orange are contributing toward
    class 1, while features highlighted in blue are contributing toward class 0.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从左数第三个可视化显示了前五个特征及其相应的值。在这里，橙色突出显示的特征对类别1有贡献，而蓝色突出显示的特征对类别0有贡献。
- en: The rightmost visualization is almost the same as the second visualization,
    except that it is presented in a different format, and it also provides local
    explanations for the particular data instance selected.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最右侧的可视化几乎与第二个可视化相同，只是它以不同的格式呈现，并且它还为所选特定数据实例提供了局部解释。
- en: 'As we discussed in the previous section, LIME also provides a global understanding
    of the model alongside the local explanations. This is provided using the SP-LIME
    algorithm. This can be implemented using the following lines of code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了 LIME 也提供了对模型的全面理解，同时提供了局部解释。这是通过 SP-LIME 算法实现的。这可以通过以下几行代码实现：
- en: '[PRE38]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '*Figure 4.7* shows the visualizations obtained using SP-LIME:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.7* 展示了使用 SP-LIME 获得的可视化：'
- en: '![Figure 4.7 – Visualizations of diverse explanations obtained from SP-LIME
    to get a global understanding of the model'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 从 SP-LIME 获得的多样化解释的可视化，以获得对模型的全面理解](img/B18216_04_007.jpg)'
- en: '](img/B18216_04_007.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_04_007.jpg)'
- en: Figure 4.7 – Visualizations of diverse explanations obtained from SP-LIME to
    get a global understanding of the model
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 从 SP-LIME 获得的多样化解释的可视化，以获得对模型的全面理解
- en: '*Figure 4.7* shows the output of the SP-LIME code. SP-LIME provides a diverse
    representative sample set of local explanations considering different instances
    of the model to get a global perspective of the black-box model. These visualizations
    show us the important features, the feature-important scores, and even the range
    of values for each of those features and how these features contribute toward
    either of the classes. All these properties and features of the entire LIME framework
    make it a powerful approach in which to provide model-agnostic human-understandable
    model interpretability to black-box models. Additionally, the framework is also
    very robust so the entire algorithm can be implemented with only a few lines of
    code.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.7* 展示了 SP-LIME 代码的输出。SP-LIME 提供了一个多样化的代表性样本集，考虑了模型的不同实例，以获得对黑盒模型的全面视角。这些可视化显示了重要的特征、特征重要性得分，以及每个特征的值范围，以及这些特征如何对任一类别做出贡献。LIME
    框架的所有这些属性和特性使其成为向黑盒模型提供模型无关、人类可理解模型可解释性的强大方法。此外，该框架也非常稳健，因此整个算法只需几行代码即可实现。'
- en: Although LIME has many advantages, unfortunately, there are some drawbacks of
    this algorithm that we should be aware of. Let's discuss them in the next section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LIME 有许多优点，但不幸的是，这个算法也有一些缺点，我们应该意识到。让我们在下一节中讨论它们。
- en: Potential pitfalls
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在的陷阱
- en: 'In the previous section, we learned how easily the LIME Python framework can
    be used to explain black-box models for a classification problem. But unfortunately,
    the algorithm does have certain limitations, and there are a few scenarios in
    which the algorithm is not effective:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何轻松地使用 LIME Python 框架来解释分类问题的黑盒模型。但不幸的是，该算法确实存在某些局限性，并且有一些场景中该算法并不有效：
- en: While providing interpretable explanations, a particular choice of interpretable
    data representation and interpretable model might still have a lot of limitations.
    While the underlying trained model might still be considered a black-box model,
    there is no assumption about the model that is made during the explanation process.
    However, certain representations are not powerful enough to represent some complex
    behaviors of the model. For example, if we are trying to build an image classifier
    to distinguish between black and white images and colored images, then the presence
    or absence of superpixels will not be useful to provide the explanations.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提供可解释的解释时，特定的可解释数据表示和可解释模型的选择可能仍然存在很多限制。尽管底层训练模型可能仍然被视为黑盒模型，但在解释过程中并没有对模型做出任何假设。然而，某些表示可能不足以表示模型的某些复杂行为。例如，如果我们试图构建一个图像分类器来区分黑白图像和彩色图像，那么超像素的存在与否对于提供解释可能没有帮助。
- en: As discussed earlier, LIME learns an interpretable model to provide local explanations.
    Usually, these interpretable models are linear and non-complex. However, suppose
    that the underlying black-box model is not linear, even in the locality of the
    prediction, so the LIME algorithm is not effective.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，LIME 学习一个可解释的模型来提供局部解释。通常，这些可解释的模型是线性和非复杂的。然而，如果底层黑盒模型不是线性的，即使在预测的局部区域，那么
    LIME 算法可能就不有效。
- en: LIME explanations are highly sensitive to any change in input data. Even a slight
    change in the input data can drastically alter the explanation instance provided
    by LIME.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME 解释对输入数据的任何变化都高度敏感。即使输入数据的一点点变化也可能极大地改变 LIME 提供的解释实例。
- en: For certain datasets, LIME explanations are not robust as, even for similar
    data instances, the explanations provided can be completely different. This might
    prevent end users from completely relying on the explanations provided by LIME.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于某些数据集，LIME解释并不稳健，即使是对于相似的数据实例，提供的解释也可能完全不同。这可能会阻止最终用户完全依赖LIME提供的解释。
- en: The algorithm is extremely prone to data drifts and label drifts. A slight drift
    between the training and the inference data can completely produce inconsistent
    explanations. The authors of the paper named *A study of data and label shift
    in the LIME framework*, *Rahnama* and *Boström* (https://arxiv.org/abs/1910.14421),
    mention certain experiments that can be used to evaluate the impact of data drift
    in the LIME framework. Due to this limitation, the goodness of approximation of
    the LIME explanations (also referred to as *fidelity*) is considered to be low.
    This is not expected in a good explanation method.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法极其容易受到数据漂移和标签漂移的影响。训练数据和推理数据之间的一点点漂移可能会导致完全不一致的解释。论文《在LIME框架中研究数据和标签漂移》的作者*Rahnama*和*Boström*（[https://arxiv.org/abs/1910.14421](https://arxiv.org/abs/1910.14421)）提到了一些可以用来评估LIME框架中数据漂移影响的实验。由于这种限制，LIME解释的近似良好性（也称为*保真度*）被认为是低的。在一个好的解释方法中这是不应该出现的。
- en: Explanations provided by LIME depend on the choice of the hyperparameters of
    the algorithm. Similar to most of the algorithms, even for the LIME algorithm,
    the choice of the hyperparameters can determine the quality of the explanations
    provided. Hyperparameter tuning is also difficult for the LIME algorithm as, usually,
    qualitative methods are adopted to evaluate the quality of the LIME explanations.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME提供的解释依赖于算法超参数的选择。与大多数算法类似，即使是LIME算法，超参数的选择也会决定所提供解释的质量。对于LIME算法来说，超参数调整也是困难的，因为通常采用定性方法来评估LIME解释的质量。
- en: There are many research works that indicate the other limitations of the LIME
    algorithm. I have mentioned some of these research works in the *References* section.
    I would strongly recommend that you go through those papers to get more details
    about certain limitations of the algorithm.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多研究工作指出了LIME算法的其他局限性。我在*参考文献*部分提到了其中一些研究工作。我强烈建议您阅读这些论文，以获取有关算法某些局限性的更多详细信息。
- en: Summary
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This brings us to the end of the chapter. In this chapter, we discussed LIME,
    one of the most widely adopted frameworks in XAI. Throughout this chapter, we
    discussed the intuition behind the workings of the algorithm and some important
    properties of the algorithm that make the generated explanations human-friendly.
    Additionally, we saw an end-to-end tutorial on how to use LIME for a practical
    use case to provide explainability to a black-box classification model. Even though
    we discussed some limitations of the LIME algorithm, due to its simplicity, LIME
    is still one of the most popular and widely used XAI frameworks. Hence, it is
    very important for us to discuss this algorithm and have a thorough understanding
    of the workings of the framework.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这就带我们结束了这一章。在本章中，我们讨论了LIME，这是XAI中最广泛采用的框架之一。在本章中，我们讨论了算法工作背后的直觉以及使生成的解释对人类友好的算法的一些重要特性。此外，我们还看到了一个端到端的教程，展示了如何使用LIME为黑盒分类模型提供可解释性。尽管我们讨论了LIME算法的一些局限性，但由于其简单性，LIME仍然是XAI中最受欢迎和最广泛使用的框架之一。因此，对我们来说，讨论这个算法并深入了解框架的工作原理非常重要。
- en: In the next chapter, we will apply the LIME framework to solve other types of
    machine learning problems using different types of datasets.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将应用LIME框架，使用不同类型的数据集来解决其他类型的机器学习问题。
- en: References
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For additional information, please refer to the following resources:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更多信息，请参阅以下资源：
- en: '*"Why Should I Trust You?" Explaining the Predictions of Any Classifier* by
    *Ribeiro et al*: [https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*"我应该为什么相信你？"解释任何分类器的预测* by *Ribeiro 等人*：[https://arxiv.org/pdf/1602.04938.pdf](https://arxiv.org/pdf/1602.04938.pdf)'
- en: '*LIME - Local Interpretable Model-Agnostic Explanations*: [https://homes.cs.washington.edu/~marcotcr/blog/lime/](https://homes.cs.washington.edu/~marcotcr/blog/lime/)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LIME - 本地可解释模型无关解释*：[https://homes.cs.washington.edu/~marcotcr/blog/lime/](https://homes.cs.washington.edu/~marcotcr/blog/lime/)'
- en: 'The LIME GitHub project: [https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME GitHub项目：[https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)
- en: '*A study of data and label shift in the LIME framework* by *Rahnama* and *Boström*:
    [https://arxiv.org/abs/1910.14421](https://arxiv.org/abs/1910.14421)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Rahnama* 和 *Boström* 的研究论文《LIME 框架中的数据与标签偏移研究》: [https://arxiv.org/abs/1910.14421](https://arxiv.org/abs/1910.14421)'
- en: '*What''s Wrong with LIME*: [https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612](https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LIME 的问题在哪里*：[https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612](https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612)'
- en: '*Why model why? Assessing the strengths and limitations of LIME* by *Dieber*
    and *Kirrane* (2020): [https://arxiv.org/pdf/2012.00093.pdf](https://arxiv.org/pdf/2012.00093.pdf)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*为什么模型要解释？评估 LIME 的优势和局限性*，作者 *Dieber* 和 *Kirrane*（2020年）：[https://arxiv.org/pdf/2012.00093.pdf](https://arxiv.org/pdf/2012.00093.pdf)'
