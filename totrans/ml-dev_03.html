<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Clustering</h1>
                </header>
            
            <article>
                
<p>Congratulations! You have finished this book's introductory section, in which you have explored a great number of topics, and if you were able to follow it, you are prepared to start the journey of understanding the inner workings of many machine learning models.</p>
<p>In this chapter, we will explore some effective and simple approaches for automatically finding interesting data conglomerates, and so begin to research the reasons for natural groupings in data.</p>
<p>This chapter will covers the following topics:</p>
<ul>
<li>A line-by-line implementation of an example of <span>the K-means algorithm</span>, with explanations of the data structures and routines</li>
<li>A thorough explanation of the <strong>k-nearest neighbors (K-NN)</strong> algorithm, using a code example to explain the whole process</li>
<li>Additional methods of determining the optimal number of groups representing a set of samples</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grouping as a human activity</h1>
                </header>
            
            <article>
                
<p class="western">Humans typically tend to agglomerate everyday elements into groups of similar features. This feature of the human mind can also be replicated by an algorithm. Conversely, one of the simplest operations that can be initially applied to any unlabeled dataset is to group elements around common features.</p>
<p>As we have described, in this stage of the development of the discipline, clustering is taught as an introductory theme that's applied to the simplest categories of element sets.</p>
<p>But as an author, I recommend researching this domain, because the community is hinting that <span>the current model's performance </span>will all reach a plateau, before aiming for the full generalization of tasks in AI. And what kinds of method are the main candidates for the next stages of crossing the frontier towards AI? Unsupervised methods, in the form of very sophisticated variations of the methods explained here.</p>
<p>But let's not digress for now, and let's begin with the simplest of grouping criteria, the distance to a common center, which is called <strong>K-means</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Automating the clustering process</h1>
                </header>
            
            <article>
                
<p>The grouping of information for clustering follows a common pattern for all techniques. Basically, we have an initialization stage, followed by the iterative insertion of new elements, after which the new group relationships are updated. This process continues until the stop criteria is met, where the group characterization is finished. The following flow diagram illustrates this process:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="336" width="179" src="assets/e8b3b8ac-4c99-4c46-85f3-2cc3a19196df.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">General scheme for a clustering algorithm</div>
<p>After we get a clear sense of the overall process, let's start working with several cases where this scheme is applied, starting with <strong>K-means</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding a common center - K-means</h1>
                </header>
            
            <article>
                
<p>Here we go! After some necessary preparation review, we will finally start to learn from data; in this case, we are looking to label data we observe in real life.</p>
<p>In this case, we have the following elements:</p>
<ul>
<li>A set of N-dimensional elements of numeric type</li>
<li>A predetermined number of groups (this is tricky because we have to make an educated guess)</li>
<li>A set of common representative points for each group (called <strong>centroids</strong>)</li>
</ul>
<p class="western">The main objective of this method is to <span>split the dataset into</span> <span>an arbitrary number of clusters, each of which can be represented by the mentioned centroids.</span></p>
<p>The word centroid comes from the mathematics world, and has been translated to calculus and physics. Here we find a classical representation of the analytical calculation of a triangle's centroid:</p>
<div class="CDPAlignCenter CDPAlign"><img height="194" width="241" src="assets/5c4e6e22-e42e-40f1-ac57-e3e33a1d3fb3.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Graphical depiction of the centroid finding scheme for a triangle</div>
<p>The centroid of a finite set of <em>k</em> points, <em>x<sub>1 ,</sub>x<sub>2</sub>, ..., x<sub>k</sub></em> in <em>R<sup>n</sup></em>, is as follows:</p>
<div style="padding-left: 180px"><img src="assets/e3fa38b1-903b-45f5-8b83-9110e30a4696.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Analytic definition of centroid</div>
<p>So, now that we have defined this central metric, let's ask the question, "<em>What does it have to do with the grouping of data elements?</em>"</p>
<p>To answer this, we must first understand the concept of <strong>distance to a centroid.</strong> Distance has many definitions, which could be linear, quadratic, and other forms.</p>
<p>So, let's review some of the main distance types, and then we will mention which one is normally used:</p>
<div class="packt_tip">In this review, we will work with 2D variables when defining the measure types, as a mean of simplification.</div>
<p>Let's take a look at the following distance types:</p>
<ul>
<li><strong>Euclidean distance:</strong> <span>This distance metric calculates the distance in the form of a straight line between two points, or has the following formula:</span></li>
</ul>
<div style="padding-left: 210px"><img height="39" width="171" src="assets/1878fb81-cfab-4c9a-8d97-a795402cd224.png"/></div>
<ul>
<li><strong>Chebyshev distance</strong>: This distance is equivalent to the maximum distance, along any of the axes. It's also called the <strong>chess</strong> distance, because it gives the minimum quantity of moves a king needs to get from the initial point to the final point. Its defined by the following formula:</li>
</ul>
<div style="padding-left: 180px" class="mce-root"><img height="37" width="187" src="assets/370b76dd-41f1-4534-b64b-02e57756ac1d.png"/></div>
<ul>
<li><strong>Manhattan distance</strong>: This distance is the equivalent to going from one point to another in a city, with unit squares. This L1-type distance sums the number of horizontal units advanced, and the number of vertical ones. Its formula is as follows:</li>
</ul>
<div style="padding-left: 150px" class="mce-root"><img height="35" width="149" src="assets/1f39a02e-79b1-4e3d-b620-4899a5a8e9ae.png"/></div>
<p>The following diagram further explains the formulas for the different types of distance:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="295" width="694" src="assets/864586ba-845d-4477-b165-21f2c4472c38.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Graphical representation of some of the most well known distance types</div>
<p>The distance metric chosen for K-means is the Euclidean distance, which is easy to compute and scales well to many dimensions.</p>
<p>Now that we have all the elements, it's time to define the criteria we will use to define which label we will assign to any given sample. Let's summarize the learning rule with the following statement:</p>
<div class="packt_quote">"A sample will be assigned to the group represented by the closest centroid."</div>
<p>The goal of this method is to minimize the sum of squared distances from the cluster’s members to the actual centroid of all clusters that contain samples. This is also known as <strong>minimization of inertia.</strong></p>
<p class="western"><span>In the following diagram, we can see the results of a typical K-means algorithm applied to a sample population of blob-like groups, with a preset number of clusters of 3:</span></p>
<div class="western CDPAlignCenter CDPAlign"><img height="301" width="401" src="assets/0bedcf60-5ed3-453b-91d1-aa195356380f.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Typical result of a clustering process using K-means, with a seed of 3 centroids</div>
<p><span>K-means is a simple and effective algorithm that can be used to obtain a quick idea of how a dataset is organized. Its main differentiation is that objects belonging to the same class will share a common distance center, which will be incrementally upgraded with the addition of each new sample.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pros and cons of K-means</h1>
                </header>
            
            <article>
                
<p class="western"><span>The advantages of this method are as follows:</span></p>
<ul>
<li>
<p class="western"><span>It scales very well (most of the calculations can be run in parallel)</span></p>
</li>
</ul>
<ul>
<li class="western"><span>It has been used in a very large range of applications</span></li>
</ul>
<p>But simplicity also has some costs (the no silver bullet rule applies):</p>
<ul>
<li>
<p class="western"><span>It requires a priori knowledge (the number of possible clusters should be known beforehand)</span></p>
</li>
<li>
<p class="western"><span>The outlier values can skew</span> <span>the values of the centroids, as they have the same weight as any other sample</span></p>
</li>
<li>
<p class="western"><span>As we assume that the figure is convex and isotropic, it doesn’t work very well with non blob alike clusters</span></p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means algorithm breakdown</h1>
                </header>
            
            <article>
                
<p>The mechanism of the K-means algorithm can be summarized with the following flowchart:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="382" width="204" src="assets/f85cd455-dc0a-4cff-8f3b-db941c744c8b.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Flowchart of the K-means process</div>
<p>Let's describe the process in more detail:</p>
<p>We start with the unclassified samples and take K elements as the starting centroids. There are also possible simplifications of this algorithm that take the first element in the element list for the sake of brevity.</p>
<p>We then calculate the distances between the samples and the first chosen samples, and so we get the first calculated centroids (or other representative values). You can see in the moving centroids in the illustration a shift toward a more intuitive (and mathematically correct) center location.</p>
<p>After the centroids change, their displacement will cause the individual distances to change, and so the cluster membership may change.</p>
<p>This is the time when we recalculate the centroids and repeat the first steps, in case the stop condition isn’t met.</p>
<p>The stopping conditions can be of various types:</p>
<ul>
<li>After n-iterations. It could be that either we chose a very large number, and we’ll have unnecessary rounds of computing, or it could converge slowly, and we will have very unconvincing results if the centroid doesn't have a very stable mean.</li>
<li>A possible better criterion for the convergence of the iterations is to take a look at the changes of the centroids, whether in total displacement or total cluster element switches. The last one is employed normally, so we will stop the process once there are no more elements changing from their current cluster to another one.</li>
</ul>
<div class="packt_infobox">The N iterations condition could also be used as a last resort, because it could lead to very long processes where no observable change is observed on a large number of iterations.</div>
<p>Let's try to summarize the process of K-NN clustering visually, going through a few steps and looking at how the clusters evolve through time:</p>
<div class="CDPAlignCenter CDPAlign"><img height="310" width="331" src="assets/ce222ab9-8ca8-44e8-a91f-45ef5c908a75.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Graphical example of the cluster reconfiguration loop</div>
<p class="mce-root">In subfigure 1, we start seeding the clusters with possible centroids at random places, to which we assign the closest data elements; and then in subfigure 2, we reconfigure the centroids to the center of the new clusters, which in turn reconfigures the clusters again (subfigure 3), until we reach a stationary status. The element aggregation could also be made element by element, which will trigger softer reconfigurations. This will be the implementation strategy for the practical part of this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means implementations</h1>
                </header>
            
            <article>
                
<div class="cell border-box-sizing text_cell rendered">
<p>In this section, we will review the concept of K-means with a practical sample, from the very basic concepts.</p>
<p>First, we will import the libraries we need. In order to improve the understanding of the algorithms, we will use the <kbd>numpy</kbd> library. Then we will use the well-known <kbd>matplotlib</kbd> library for the graphical representation of the algorithms:</p>
<pre>import numpy as np<br/><br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/><br/>%matplotlib inline</pre>
<p>These will be a number of 2D elements, and will then generate the candidate centers, which will be of four 2D elements.</p>
<p class="inner_cell">In order to generate a dataset, normally a random number generator is used, but in this case we want to set the samples to predetermined numbers, for convenience, and also to allow you to repeat these procedures manually:</p>
<pre>samples=np.array([[1,2],[12,2],[0,1],[10,0],[9,1],[8,2],[0,10],[1,8],[2,9],[9,9],[10,8],[8,9] ], dtype=np.float)<br/>centers=np.array([[3,2], [2,6], [9,3], [7,6]], dtype=np.float)<br/>N=len(samples)</pre>
<p>Let's represent the sample's center. First, we will initialize a new <kbd>matplotlib</kbd> figure, with the corresponding axes. The <kbd>fig</kbd> object will allow us to change the parameters of all the figures.</p>
<p>The <kbd>plt</kbd> and <kbd>ax</kbd> variable names are a standarized way to refer to the plot in general and one of the plots' axes.</p>
<p>So let’s try to have an idea of what the samples look like. This will be done through the <kbd>scatter</kbd> drawing type of the <kbd>matplotlib</kbd> library. It takes as parameters the <em>x</em> coordinates, the <em>y</em> coordinates, size (in points squared), the marker type, and color.</p>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html packt_infobox">There are a variety of markers to choose from, such as point (<kbd>.</kbd>), circle (<kbd>o</kbd>), square (<kbd>s</kbd>). To see the full list, visit <a href="https://matplotlib.org/api/markers_api.html">https://matplotlib.org/api/markers_api.html<span>.</span></a></div>
<p>Let's take a look at the following code snippet:</p>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<pre>        fig, ax = plt.subplots()<br/>        ax.scatter(samples.transpose()[0], samples.transpose()[1], marker = <br/>        'o', s = 100 )<br/>        ax.scatter(centers.transpose()[0], centers.transpose()[1], marker = <br/>        's', s = 100, color='black')<br/>        plt.plot()</pre></div>
</div>
</div>
<p>Let's now take a look at the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="270" width="396" class="alignnone size-medium wp-image-484 image-border" src="assets/c394d54d-8441-4e7d-a7d7-df3090a154c0.png"/></div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell CDPAlignCenter CDPAlign packt_figref">Initial clusters status, Centers as black squares</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let’s define a function that, given a new sample, will return a list with the distances to all the current centroids in order to assign this new sample to one of them, and afterward, recalculate the centroids:</p>
<pre>    def distance (sample, centroids):<br/>        distances=np.zeros(len(centroids))<br/>        for i in range(0,len(centroids)):<br/>            dist=np.sqrt(sum(pow(np.subtract(sample,centroids[i]),2)))<br/>            distances[i]=dist<br/>        return distances</pre>
<p>Then we need a function that will build, one by one, the step-by-step graphic of our application.</p>
<p>It expects a maximum of 12 subplots, and the <kbd>plotnumber</kbd> parameter will determine the position on the 6 x 2 matrix (<kbd>620</kbd> will be the upper-left subplot, 621 the following to the right, and so on writing order).<br/>
After that, for each picture we will do a scatterplot of the clustered samples, and then of the current centroid position:</p>
<pre>    def showcurrentstatus (samples, centers, clusters, plotnumber):<br/>        plt.subplot(620+plotnumber)<br/>        plt.scatter(samples.transpose()[0], samples.transpose()[1], marker = <br/>        'o', s = 150 , c=clusters)<br/>        plt.scatter(centers.transpose()[0], centers.transpose()[1], marker = <br/>        's', s = 100, color='black')<br/>        plt.plot()</pre>
<p>The following function, called <kbd>kmeans</kbd>, will use the previous distance function to store the centroid that the samples are assigned to (it will be a number from <kbd>1</kbd> to <kbd>K</kbd>).<br/>
The main loop will go from sample <kbd>0</kbd> to <kbd>N</kbd>, and for each one, it will look for the closest centroid, assign the centroid number to index <kbd>n</kbd> of the clusters array, and sum the samples' coordinates to its currently assigned centroid.<br/>
Then, to get the sample, we use the <kbd>bincount</kbd> method to count the number of samples for each centroid, and by building a <kbd>divisor</kbd> array, we divide the sum of a class elements by the previous <kbd>divisor</kbd> array, and there we have the new centroids:</p>
<pre>    def kmeans(centroids, samples, K, plotresults):<br/>        plt.figure(figsize=(20,20))<br/>        distances=np.zeros((N,K))<br/>        new_centroids=np.zeros((K, 2))<br/>        final_centroids=np.zeros((K, 2))<br/>        clusters=np.zeros(len(samples), np.int)<br/><br/>        for i in range(0,len(samples)):<br/>            distances[i] = distance(samples[i], centroids)<br/>            clusters[i] = np.argmin(distances[i])<br/>            new_centroids[clusters[i]] += samples[i]        <br/>            divisor = np.bincount(clusters).astype(np.float)<br/>            divisor.resize([K])<br/>            for j in range(0,K):<br/>            final_centroids[j] = np.nan_to_num(np.divide(new_centroids[j] , <br/>            divisor[j]))<br/>            if (i&gt;3 and plotresults==True):<br/>                showcurrentstatus(samples[:i], final_centroids, <br/>                clusters[:i], i-3)<br/>            return final_centroids</pre>
<p>Now it’s time to kickstart the K-means process, using the initial samples and centers we set up at first. The current algorithm will show how the clusters are evolving, starting from a few elements, into their final state:</p>
<pre>    finalcenters=kmeans (centers, samples, 4, True)</pre></div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="output_wrapper">
<div class="output CDPAlignCenter CDPAlign">
<p class="output_area">Let's take a look at the following screenshot:</p>
<div><img height="364" width="553" src="assets/bccc3f75-1b2f-4dbd-891f-8879acc742d9.png"/></div>
</div>
</div>
</div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Depiction of the clustering process, with the centroids represented as black squares</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Nearest neighbors</h1>
                </header>
            
            <article>
                
<p class="western">K-NN is another classical method of clustering. It builds groups of samples, supposing that each new sample will have the same class as its neighbors, without looking for a global representative central sample. Instead, it looks at the environment, looking for the most frequent class on each new sample's environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mechanics of K-NN</h1>
                </header>
            
            <article>
                
<p><span>K-NN can be implemented in many configurations, but in this chapter we will use the</span> <span>s</span><span>emi-supervised</span> <span>approach, starting from a certain number of already assigned samples, and later guessing the cluster membership using the main criteria.</span></p>
<p>In the following diagram, we have a breakdown of the algorithm. It can be summarized with the following steps:</p>
<div class="CDPAlignCenter CDPAlign"><img height="451" width="205" src="assets/d9e62a74-25fe-4835-af90-154861815e43.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Flowchart for the K-NN clustering process</div>
<p>Let's go over all the following involved steps, in a simplified form:</p>
<ol>
<li>We place the previously known samples on the data structures.</li>
<li>We then read the next sample to be classified, and calculate the Euclidean distance from the new sample to every sample in the training set.</li>
<li>We decide the class of the new element by selecting the class of the nearest sample, by Euclidean distance. The K-NN method requires the vote of the K closest samples.</li>
<li>We repeat the procedure until there are no more remaining samples.</li>
</ol>
<p>This picture will give us a idea of how the new samples are being added. In this case, we use a <kbd>K</kbd> of <kbd>1</kbd>, for simplicity:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="315" width="336" src="assets/7a5f405c-465a-4427-9cda-db942851fb95.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Sample application of a K-NN loop</div>
<p class="mce-root">K-NN can be implemented in more than one of the configurations that we have learned, but in this chapter, we will use the semi-supervised approach; we will start from a certain number of already assigned samples, and we will later guess the cluster membership based on the characteristics of the training set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pros and cons of K-NN</h1>
                </header>
            
            <article>
                
<p class="western">The advantages of this method are as follows:</p>
<ul>
<li>
<p class="western"><strong>Simplicity</strong>: There's n<span>o need to tune parameters</span></p>
</li>
<li>
<p class="western"><strong>No formal training needed</strong>: We just need more training examples to improve the model</p>
</li>
</ul>
<p>The disadvantage is as follows:</p>
<p><span>It i</span><span>s computationally expensive - in a naive approach, all distances between points and every new sample have to be calculated, except when caching is implemented.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-NN sample implementation</h1>
                </header>
            
            <article>
                
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this simple implementation of the K-NN method, we will use the NumPy and Matplotlib libraries. Also, as we will be generating a synthetic dataset for better comprehension, we will use the <kbd>make_blobs</kbd> method from scikit-learn, which will generate well-defined and separated groups of information so we have a sure reference for our implementation.</p>
<p>Importing the required libraries:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="kn">    import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span><br/><br/><span class="kn">    import</span> <span class="nn">matplotlib</span><br/><span class="kn">    import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span><br/><br/><span class="kn">    from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span><br/><span class="o">    %</span><span class="k">matplotlib</span> inline</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So, it's time to generate the data samples for this example. The parameters of <kbd>make_blobs</kbd> are the number of samples, the number of features or dimensions, the quantity of centers or groups, whether the samples have to be shuffled, and the standard deviation of the cluster, to control how dispersed the group samples are:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="n">    data</span><span class="p">,</span> <span class="n">features</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <br/><span class="n">    shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">, </span><span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span><br/><span class="n">    fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span><br/><span class="n">    ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">transpose</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="o">.</span><span class="n">transpose</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">features</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span> <br/>    <span class="s1">'o'</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">100</span> <span class="p">)</span><br/><span class="n">    pl</span></pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<p class="output_area"><span>Here is a representation of the generated sample blobs:</span></p>
<div class="output_area">
<div class="output_png output_subarea CDPAlignCenter CDPAlign"><img height="183" width="269" class="alignnone size-medium wp-image-467 image-border" src="assets/994e5376-ce65-4ba5-a01c-5c77a6b94ac0.png"/></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Firstly, let's define our <kbd>distance</kbd> function, which will be necessary to find the neighbors of all the new elements. We basically provide one sample, and return the distance between the provided new element and all their counterparts:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="k">    def</span> <span class="nf">distance</span> <span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span><br/>        <span class="n">distances</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span><br/>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span><br/>            <span class="n">dist</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span><span class="mi">2</span><span class="p">)))</span><br/>            <span class="n">distances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">dist</span><br/>        <span class="k">return</span> <span class="n">distances</span></pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <kbd>add_sample</kbd> function will receive a new 2D sample, the current dataset, and an array marking the group of the corresponding sample (from <kbd>0</kbd> to <kbd>3</kbd> in this case). In this case, we use <kbd>argpartition</kbd> to get the indices of the three nearest neighbors of the new sample, and then we use them to extract a subset of the <kbd>features</kbd> array. Then, <kbd>bincount</kbd> will return the count of any of the different classes on that three-element subset, and then with <kbd>argmax</kbd>, we will choose the index (in this case, the class number) of the group with the most elements in that two-element set:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="k">    def</span> <span class="nf">add_sample</span><span class="p">(</span><span class="n">newsample</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span><br/>        <span class="n">distances</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span><br/>        <span class="c1">#calculate the distance of the new sample and the current data</span><br/>        <span class="n">distances</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">newsample</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span><br/>        <span class="n">closestneighbors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argpartition</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="mi">3</span><span class="p">)[:</span><span class="mi">3</span><span class="p">]</span><br/>        <span class="n">closestgroups</span><span class="o">=</span><span class="n">features</span><span class="p">[</span><span class="n">closestneighbors</span><span class="p">]</span><br/>        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">closestgroups</span><span class="p">))</span></pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we define our main <kbd>knn</kbd> function, which takes the new data to be added and uses the original classified data, represented by the <kbd>data</kbd> and <kbd>features</kbd> parameters, to decide the classes of the new elements:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="k">    def</span> <span class="nf">knn</span> <span class="p">(</span><span class="n">newdata</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span><br/>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">newdata</span><span class="p">:</span><br/>            <span class="n">test</span><span class="o">=</span><span class="n">add_sample</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">features</span><span class="p">);</span><br/>            <span class="n">features</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="p">[</span><span class="n">test</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><br/>            <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><br/>        <span class="k">return</span> <span class="n">data</span><span class="p">,</span><span class="n">features</span></pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, it's time to kickstart the process. For this reason, we define a set of new samples in the range of -10, 10 on both the <kbd>x</kbd> and <kbd>y</kbd> dimensions, and we will call our <kbd>knn</kbd> routine with it<span>:</span></p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="n">        newsamples</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="mi">20</span><span class="o">-</span><span class="mf">8.</span><br/>    &gt;<span class="n">    finaldata</span><span class="p">,</span> <span class="n">finalfeatures</span><span class="o">=</span><span class="n">knn</span> <span class="p">(</span><span class="n">newsamples</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span></pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now it's time to represent the final results. First, we will represent the initial samples, which are much more well-formed than our random values, and then our final values, represented by an empty square (<kbd>c='none'</kbd>), so that they will serve as a marker of those samples:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="n">    fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span><br/><span class="n">    ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">finaldata</span><span class="o">.</span><span class="n">transpose</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">finaldata</span><span class="o">.</span><span class="n">transpose</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <br/><span class="n">    c</span><span class="o">=</span><span class="n">finalfeatures</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span> <span class="s1">'o'</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">100</span> <span class="p">)</span><br/><span class="n">    ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">newsamples</span><span class="o">.</span><span class="n">transpose</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">newsamples</span><span class="o">.</span><span class="n">transpose</span><span class="p">()<br/>    [</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="s1">'none'</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span>  <br/><span class="s1">    's'</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">100</span> <span class="p">)</span><br/><span class="n">    plt</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span></pre></div>
</div>
</div>
</div>
</div>
<p>Let's take a look at the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="192" width="285" src="assets/2fadc21f-1aac-4680-b539-0c6efa4933d6.png"/></div>
<div class="cell border-box-sizing code_cell rendered">
<div class="output_wrapper">
<div class="output CDPAlignCenter CDPAlign packt_figref">Final clustering status (new classified items are marked with a square)</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the preceding graph, we can see how our simple model of three neighbors works well to qualify and reform the grouping as the process advances. As the graph shows, the new groupings aren't necessarily of a circular form; they change according to the way the incoming data progresses.</p>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Going beyond the basics</h1>
                </header>
            
            <article>
                
<p>Now that we are done reviewing illustrative cases of the two main clustering techniques, let's explore some more advanced metrics and techniques so we can have them in our toolbox.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Elbow method</h1>
                </header>
            
            <article>
                
<p>One of the questions that may have arisen when implementing K-means could have been "how do I know that the target number of clusters is the best or most representative for the dataset?"</p>
<p>For this task, we have the <strong>Elbow</strong> method. It consists of a unique statistical measure of the total group dispersion in a grouping. It works by repeating the K-means procedure, using an increasing number of initial clusters, and calculating the total intra-cluster internal distance for all the groups.</p>
<p>Normally, the method will start with a very high value (except if we start with the right number of centroids), and then we will observe that the total intra-cluster distance drops quite quickly, until we reach a point where it doesn't change significantly. Congratulations, we have found the Elbow point, so-called for being an inflection on the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="369" width="398" src="assets/16f79328-bc5a-400c-86cb-a4a631832f49.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Graphical depiction of the error evolution, as the number of clusters increases, and the inflection point.</div>
<p>Regarding the accuracy of this indicator, the Elbow method is, as you can see, a heuristic and not mathematically determined, but can be of use if you want to make a quick estimate of the right number of clusters, especially when the curve changes very abruptly at some point.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have covered the simplest but still very practical machine learning models in an eminently practical way to get us started on the complexity scale.</p>
<p>In the following chapter, where we will cover several regression techniques, it will be time to go and solve a new type of problem that we have not worked on, even if it's possible to solve the problem with clustering methods (regression), using new mathematical tools for approximating unknown values. In it, we will model past data using mathematical functions, and try to model new output based on those modeling functions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Thorndike, Robert L, <em>Who belongs in the family?,</em></span> Psychometrika<span>18.4 (1953): 267-276.</span></li>
<li>Steinhaus, H, <em>Sur la division des corp materiels en parties.</em> Bull. Acad. Polon. Sci 1 (1956): 801–804.</li>
<li><span>MacQueen, James, <em>Some methods for classification and analysis of multivariate observations.</em></span> Proceedings of the fifth Berkeley symposium on mathematical statistics and probability<span>. Vol. 1. No. 14. 1967.</span></li>
<li><span><em>Cover, Thomas, and Peter Hart</em>, <em>Nearest neighbor pattern classification.</em></span> IEEE transactions on information theory<span> </span><span>13.1 (1967): 21-27.</span></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>