<html><head></head><body>
		<div id="_idContainer134">
			<h1 id="_idParaDest-176" class="chapter-number"><a id="_idTextAnchor175"/>9</h1>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor176"/>Performance Optimization for Real-Time Inference</h1>
			<p><strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) and <strong class="bold">Deep Learning</strong> (<strong class="bold">DL</strong>) models are used in almost every industry, such as e-commerce, manufacturing, life sciences, and finance. Due to this, there have been meaningful innovations to improve the performance of these models. Since the introduction of transformer-based models in 2018, which were initially developed for <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) applications, the size of the models and the datasets required to train the models has grown exponentially. <strong class="bold">Transformer-based</strong> models are now used for forecasting as well as <strong class="bold">computer vision</strong> applications, in addition <span class="No-Break">to NLP.</span></p>
			<p>Let’s travel back in time a little to understand the growth in size of these models. <strong class="bold">Embeddings from Language Models</strong> (<strong class="bold">ELMo</strong>), which was introduced in 2018, had <em class="italic">93.6 million parameters</em>, while the <strong class="bold">Generative Pretrained Transformer</strong> model (also known as <strong class="bold">GPT-3</strong>), in 2020, had <em class="italic">175 billion parameters</em>. Today, we have DL models such as <strong class="bold">Switch Transformers</strong> (<a href="https://arxiv.org/pdf/2101.03961.pdf">https://arxiv.org/pdf/2101.03961.pdf</a>) with more than <em class="italic">1 trillion parameters</em>. However, the speed of innovation of hardware to train and deploy such models is not catching up with the speed of innovation of large models. Therefore, we need sophisticated techniques to train and deploy these models in a cost-effective yet <span class="No-Break">performant way.</span></p>
			<p>One way to address this is to think in terms of reducing the memory footprint of the model. Moreover, many inference workloads must provide flexibility, high availability, and the ability to scale as enterprises serve millions or billions of users, especially for real-time or near real-time use cases. We need to understand which instance type and how many instances to use for deployment. We should also understand the key metrics based on which we will optimize <span class="No-Break">the models.</span></p>
			<p>Therefore, to collectively address the preceding scenarios, in this chapter, we will dive deep into the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Reducing the memory footprint of <span class="No-Break">DL models</span></li>
				<li>Key metrics for <span class="No-Break">optimizing models</span></li>
				<li>Choosing instance type, load testing, and performance tuning <span class="No-Break">for models</span></li>
				<li><span class="No-Break">Observing results</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">For details on training large DL models, please refer to <a href="B18493_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Distributed Training of Machine Learning Models</em>, where we cover the topic in detail along with <span class="No-Break">an example.</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/>Technical requirements</h1>
			<p>You should have the following prerequisites before getting started with <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>A web browser (for the best experience, it is recommended that you use the Chrome or <span class="No-Break">Firefox browser)</span></li>
				<li>Access to the AWS account that you used in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span></li>
				<li>Access to the SageMaker Studio development environment that we created in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span></li>
				<li>Example Jupyter notebooks for this chapter are provided in the companion GitHub <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter09"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter09</span></a><span class="No-Break">)</span></li>
			</ul>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor178"/>Reducing the memory footprint of DL models</h1>
			<p>Once we have<a id="_idIndexMarker672"/> trained the model, we need to deploy the model to get predictions, which are then used to provide business insights. Sometimes, our model can be bigger than the size of the single GPU memory available on the market today. In that case, you have two options – either to reduce the memory footprint of the model or use distributed deployment techniques. Therefore, in this section, we will discuss the following techniques to reduce the memory footprint of <span class="No-Break">the model:</span></p>
			<ul>
				<li><span class="No-Break">Pruning</span></li>
				<li><span class="No-Break">Quantization</span></li>
				<li><span class="No-Break">Model compilation</span></li>
			</ul>
			<p>Let’s dive deeper into each<a id="_idIndexMarker673"/> of these techniques, starting <span class="No-Break">with pruning.</span></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor179"/>Pruning</h2>
			<p><strong class="bold">Pruning</strong> is the<a id="_idIndexMarker674"/> technique of eliminating weights and parameters within a DL model that have little or no impact on the performance <a id="_idIndexMarker675"/>of the model but a significant impact on the inference speed and size of the model. The idea behind pruning methods is to make the model’s memory and power efficient, reducing the storage requirement and latency of the model. A DL model is basically a neural network with many hidden layers connected to each other. As the size of the model increases, the number of hidden layers, parameters, and weight connections between the layers also increases. Therefore, pruning methods tend to remove unused parameters and weight connections without too much bearing on the accuracy of the model, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em> and <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em> shows a neural network <span class="No-Break">before pruning:</span></p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B18493_09_001.jpg" alt="Figure 9.1 – Simple neural network before pruning"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Simple neural network before pruning</p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em> shows the same <a id="_idIndexMarker676"/>neural network <span class="No-Break">after </span><span class="No-Break"><a id="_idIndexMarker677"/></span><span class="No-Break">pruning:</span></p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B18493_09_002.jpg" alt="Figure 9.2 – Simple neural network after pruning"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Simple neural network after pruning</p>
			<p>Now that we’ve covered pruning, let’s take a look at <span class="No-Break">quantization next.</span></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor180"/>Quantization</h2>
			<p>To train a neural<a id="_idIndexMarker678"/> network, data is first passed through the network in a forward pass, which calculates the activations, and then a backward pass, which uses the activations to calculate the gradients. The activations and gradients are usually stored in floating point 32, which takes 4 bytes of memory. When you have models with billions or trillions of parameters, this number<a id="_idIndexMarker679"/> is pretty significant. Therefore, <strong class="bold">quantization</strong> is a technique that reduces the model size by decreasing the precision of weights, biases, and activations of the<a id="_idIndexMarker680"/> model, such as floating point 16 or 8, or even to integer 8, which takes significantly less memory. For example, the GPT-J-6B model, which has 6 billion trainable parameters, takes about 23 GB of memory, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.3</em> shows the number of parameters and size of the model when loaded from the Hugging <span class="No-Break">Face library:</span></p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B18493_09_003.jpg" alt="Figure 9.3 – The model size and number of parameters for the GPT-J-6B model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – The model size and number of parameters for the GPT-J-6B model</p>
			<p>The same model when loaded with 16-bit floating point precision takes about 11 GB of memory, which is a memory reduction of about half, and can fit into a single GPU memory for inference as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B18493_09_004.jpg" alt="Figure 9.4 – The model size and number of parameters for the GPT-J-6B model with FP16 precision"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – The model size and number of parameters for the GPT-J-6B model with FP16 precision</p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.3</em> and <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.4</em>, quantization can be a useful technique for reducing the memory footprint of<a id="_idIndexMarker681"/> <span class="No-Break">the model.</span></p>
			<p>Now, let’s take a look at <a id="_idIndexMarker682"/>another technique, called <span class="No-Break">model compilation.</span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor181"/>Model compilation</h2>
			<p>Before we go into<a id="_idIndexMarker683"/> model compilation, let’s first <a id="_idIndexMarker684"/><span class="No-Break">understand compilation.</span></p>
			<p><strong class="bold">Compilation</strong> is the process of <a id="_idIndexMarker685"/>converting a human-readable program that is a set of instructions into a machine-readable program. This core idea is used as a backbone for many programming languages, such as C, C++, and Java. This process also introduces efficiencies in the runtime environment of the program, such as making it platform-independent, reducing the memory size of the program, and so on. Most programming languages come with a <strong class="bold">compiler</strong>, which is <a id="_idIndexMarker686"/>used to compile the code into a machine-readable form as writing a compiler is a tedious process and requires a deep understanding of the programming language as well as <span class="No-Break">the hardware.</span></p>
			<p>A similar idea is used for compiling ML models. With ML, compilers place the core operations of the neural network on GPUs in a way that minimizes overhead. This reduces the memory footprint of the model, improves the performance efficiency, and makes it hardware <a id="_idIndexMarker687"/>agnostic. The concept is depicted in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.5</em>, where the compiler takes a model developed in PyTorch, TensorFlow, XGBoost, and so on, converts it into an intermediate representation that is language agnostic, and then converts it into <span class="No-Break">machine-readable code:</span></p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B18493_09_005.jpg" alt="Figure 9.5 – High-level model compilation process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – High-level model compilation process</p>
			<p>Model compilation eliminates the effort required to fine-tune the model for the specific hardware and software configurations of each platform. There are many frameworks available today using which you can compile your models, such as <strong class="bold">TVM</strong> and <strong class="bold">ONNX</strong>, each with its own pros <span class="No-Break">and cons.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Discussing compilers in detail is out of the scope of this book. For details on TVM, refer to this link: <a href="https://tvm.apache.org/">https://tvm.apache.org/</a>. And for details about ONNX, refer to this <span class="No-Break">link: </span><a href="https://onnx.ai/"><span class="No-Break">https://onnx.ai/</span></a><span class="No-Break">.</span></p>
			<p>Let’s discuss a feature of Amazon SageMaker <a id="_idIndexMarker688"/>called <strong class="bold">SageMaker Neo</strong>, which is used to optimize ML models for inference on multiple platforms. Neo automatically optimizes models written in various frameworks, such as Gluon, Keras, PyTorch, TensorFlow, and so on, for inference on different platforms, such as Linux and Windows, as well as many different processors. For a complete list of frameworks or processors supported by Neo, please refer to this <span class="No-Break">link: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge.html</span></a><span class="No-Break">.</span></p>
			<p>The way Neo works is that it will first read the model, convert the framework-specific operations and functions into an intermediate representation that is framework agnostic, and finally, apply <a id="_idIndexMarker689"/>a series of optimizations. It will then generate binary code for optimized operations, write it to the shared object library and save the model definition and parameters in separate <span class="No-Break">files (</span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html</span></a><span class="No-Break">).</span></p>
			<p>It also provides a runtime for each target platform that loads and executes the compiled model. Moreover, it can optimize the models with parameters either in floating point 32 (<strong class="source-inline">FP32</strong>), quantized into integer 8 (<strong class="source-inline">INT8</strong>), or in floating point 16 (<strong class="source-inline">FP16</strong>). It can improve the model’s performance up to 25 times with less than one-tenth of the footprint of a DL framework such as TensorFlow or PyTorch. To understand this further, let’s take a pretrained image classification model from PyTorch and optimize it using <span class="No-Break">SageMaker Neo.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">We touched on SageMaker Neo in <a href="B18493_08.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Optimizing and Managing Machine Learning Models for Edge Deployment</em>. Here, we will be using the same example to explain it in detail. For the full code, refer to the GitHub <span class="No-Break">link: </span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/sagemaker_notebook/1_compile_resnet_model_egde_manager.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/blob/main/Chapter08/sagemaker_notebook/1_compile_resnet_model_egde_manager.ipynb</span></a><span class="No-Break">.</span></p>
			<p>Follow these steps to optimize a pretrained model using <span class="No-Break">SageMaker Neo:</span></p>
			<ol>
				<li>First, we will download a pretrained model from the PyTorch library as shown in the following <span class="No-Break">code snippet:</span><pre class="source-code">
…</pre><pre class="source-code">
from torchvision.models import resnet18, ResNet18_Weights</pre><pre class="source-code">
#initialize the model</pre><pre class="source-code">
weights = ResNet18_Weights.DEFAULT</pre><pre class="source-code">
model = resnet18(weights)</pre><pre class="source-code">
…</pre></li>
				<li>Next, we need to save the model in <strong class="source-inline">model.tar.gz</strong> format, which SageMaker requires, specify the<a id="_idIndexMarker690"/> input data shape, and upload the model to <span class="No-Break"><strong class="bold">Amazon S3</strong></span><span class="No-Break">:</span><pre class="source-code">
…</pre><pre class="source-code">
torch.save(model.state_dict(),</pre><pre class="source-code">
           './output/resnet18-model.pt')</pre><pre class="source-code">
with tarfile.open("model.tar.gz", "w:gz") as f:</pre><pre class="source-code">
    f.add("model.pth")</pre><pre class="source-code">
input_tensor = torch.zeros([1, 3, 224, 224])</pre><pre class="source-code">
model_uri = sagemaker_session.upload_data(</pre><pre class="source-code">
    path="model.tar.gz", key_prefix=key_prefix)</pre><pre class="source-code">
print("S3 Path for Model: ", model_uri)</pre><pre class="source-code">
…</pre></li>
				<li>Once we have the model in SageMaker format, we will prepare the parameters required for model compilation. Most importantly, you need to mention the <strong class="source-inline">target_device</strong> parameter, as based on it, SageMaker Neo will compile the model for the particular hardware on which the model will <span class="No-Break">be deployed:</span><pre class="source-code">
…</pre><pre class="source-code">
compilation_job_name = name_from_base("image-classification-neo")</pre><pre class="source-code">
prefix = key_prefix+'/'+compilation_job_name+"/model"</pre><pre class="source-code">
data_shape = '{"input0":[1,3,224,224]}'</pre><pre class="source-code">
target_device = "ml_c5"</pre><pre class="source-code">
framework = "PYTORCH"</pre><pre class="source-code">
framework_version = "1.8"</pre><pre class="source-code">
compiled_model_path = "s3://{}/{}/output".format(bucket, compilation_job_name)</pre><pre class="source-code">
print("S3 path for compiled model: ", compiled_model_path)</pre><pre class="source-code">
…</pre></li>
				<li>Next, we will declare the <strong class="source-inline">PyTorchModel</strong> object provided by SageMaker, which will have<a id="_idIndexMarker691"/> the necessary configurations, such as the model’s S3 path, the framework version, the inference script, the Python version, and <span class="No-Break">so on:</span><pre class="source-code">
…</pre><pre class="source-code">
from sagemaker.pytorch.model import PyTorchModel</pre><pre class="source-code">
from sagemaker.predictor import Predictor</pre><pre class="source-code">
sagemaker_model = PyTorchModel(</pre><pre class="source-code">
    model_data=model_uri,</pre><pre class="source-code">
    predictor_cls=Predictor,</pre><pre class="source-code">
    framework_version=framework_version,</pre><pre class="source-code">
    role=role,</pre><pre class="source-code">
    sagemaker_session=sagemaker_session,</pre><pre class="source-code">
    entry_point="inference.py",</pre><pre class="source-code">
    source_dir="code",</pre><pre class="source-code">
    py_version="py3",</pre><pre class="source-code">
    env={"MMS_DEFAULT_RESPONSE_TIMEOUT": "500"},</pre><pre class="source-code">
)</pre><pre class="source-code">
…</pre></li>
				<li>Finally, we will use the <strong class="source-inline">PyTorchModel</strong> object to create the compilation job and deploy the<a id="_idIndexMarker692"/> compiled model to the <strong class="source-inline">ml.c5.2xlarge</strong> instance, since the model was compiled for <strong class="source-inline">ml.c5</strong> as the <span class="No-Break">target device:</span><pre class="source-code">
…</pre><pre class="source-code">
sagemaker_client = boto3.client("sagemaker",</pre><pre class="source-code">
    region_name=region)</pre><pre class="source-code">
target_arch = "X86_64"</pre><pre class="source-code">
target_os = 'LINUX'</pre><pre class="source-code">
response = sagemaker_client.create_compilation_job(</pre><pre class="source-code">
    CompilationJobName=compilation_job_name,</pre><pre class="source-code">
    RoleArn=role,</pre><pre class="source-code">
    InputConfig={</pre><pre class="source-code">
        "S3Uri": sagemaker_model.model_data,</pre><pre class="source-code">
        "DataInputConfig": data_shape,</pre><pre class="source-code">
        "Framework": framework,</pre><pre class="source-code">
    },</pre><pre class="source-code">
    OutputConfig={</pre><pre class="source-code">
        "S3OutputLocation": compiled_model_path,</pre><pre class="source-code">
        "TargetDevice": 'jetson_nano',</pre><pre class="source-code">
        "TargetPlatform": {</pre><pre class="source-code">
            "Arch": target_arch,</pre><pre class="source-code">
            "Os": target_os</pre><pre class="source-code">
        },</pre><pre class="source-code">
    },</pre><pre class="source-code">
    StoppingCondition={"MaxRuntimeInSeconds": 900},</pre><pre class="source-code">
)</pre><pre class="source-code">
print(response)</pre><pre class="source-code">
…</pre></li>
				<li>Once the model has <a id="_idIndexMarker693"/>finished compiling, you can then deploy the compiled model to make inference. In this<a id="_idIndexMarker694"/> case, we are deploying the model for <strong class="bold">real-time inference</strong> as <span class="No-Break">an endpoint:</span><pre class="source-code">
…</pre><pre class="source-code">
predictor = compiled_model.deploy(</pre><pre class="source-code">
    initial_instance_count=1,</pre><pre class="source-code">
    instance_type="ml.c5.2xlarge")</pre><pre class="source-code">
…</pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">For more details on different deployment options provided by SageMaker, refer to <a href="B18493_07.xhtml#_idTextAnchor128"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Deploying Machine Learning Models </em><span class="No-Break"><em class="italic">at Scale</em></span><span class="No-Break">.</span></p>
			<ol>
				<li value="7">Now, once the model is deployed, we can invoke the endpoint for inference as shown in <a id="_idIndexMarker695"/>the following <span class="No-Break">code snippet:</span><pre class="source-code">
…</pre><pre class="source-code">
import numpy as np</pre><pre class="source-code">
import json</pre><pre class="source-code">
with open("horse_cart.jpg", "rb") as f:</pre><pre class="source-code">
    payload = f.read()</pre><pre class="source-code">
    payload = bytearray(payload)</pre><pre class="source-code">
response = runtime.invoke_endpoint(</pre><pre class="source-code">
    EndpointName=ENDPOINT_NAME,</pre><pre class="source-code">
    ContentType='application/octet-stream',</pre><pre class="source-code">
    Body=payload,</pre><pre class="source-code">
    Accept = 'application/json')</pre><pre class="source-code">
result = response['Body'].read()</pre><pre class="source-code">
result = json.loads(result)</pre><pre class="source-code">
print(result)</pre><pre class="source-code">
…</pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Since we have deployed the model as a real-time endpoint, you will be charged for the instance on which the model is deployed. Therefore, if you are not using the endpoint, make sure to delete it using the following <span class="No-Break">code snippet.</span></p>
			<ol>
				<li value="8">Use the<a id="_idIndexMarker696"/> following code snippet to delete the endpoint, if you are not <span class="No-Break">using it:</span><pre class="source-code">
…</pre><pre class="source-code">
# delete endpoint after testing the inference</pre><pre class="source-code">
import boto3</pre><pre class="source-code">
# Create a low-level SageMaker service client.</pre><pre class="source-code">
sagemaker_client = boto3.client('sagemaker',</pre><pre class="source-code">
                                region_name=region)</pre><pre class="source-code">
# Delete endpoint</pre><pre class="source-code">
sagemaker_client.delete_endpoint(</pre><pre class="source-code">
    EndpointName=ENDPOINT_NAME)</pre><pre class="source-code">
…</pre></li>
			</ol>
			<p>Now that we understand how to optimize a model using SageMaker Neo for inference, let’s talk about some of the key metrics that you should consider when trying to improve the latency of models. The ideas covered in the next section apply to DL models, even when you are not using SageMaker Neo, as you might not be able to compile all models with Neo. You can see the supported models and frameworks for SageMaker Neo <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor182"/>Key metrics for optimizing models</h1>
			<p>When it comes<a id="_idIndexMarker697"/> to real-time inference, optimizing a model for performance usually includes metrics such as latency, throughput, and model size. To optimize the model size, the process typically involves having a trained model, checking the size of the model, and if it does not fit into single CPU/GPU memory, you can choose any of the techniques discussed in the <em class="italic">Reducing the memory footprint of DL models</em> section to prepare it <span class="No-Break">for deployment.</span></p>
			<p>For deployment, one of the best practices is to standardize the environment. This will involve the use of <a id="_idIndexMarker698"/>containers to deploy the model, irrespective of whether you are deploying it on your own server or using Amazon SageMaker. The process is illustrated in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B18493_09_006.jpg" alt="Figure 9.6 – Model deployed as a real-time endpoint"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Model deployed as a real-time endpoint</p>
			<p>To summarize, we will first prepare the model for deployment, select or create a container to standardize the environment, followed by deploying the container on an instance(s). Therefore, to optimize the model’s performance, it is important to look for both inference and instance metrics. Inference metrics include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Invocations</strong>: The number of <a id="_idIndexMarker699"/>requests sent to the model endpoint. You can get the total number of requests by using the sum statistics in <strong class="bold">Amazon CloudWatch</strong>, which <a id="_idIndexMarker700"/>monitors the AWS resources or applications that you run on AWS in real time, including <span class="No-Break">SageMaker (</span><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html"><span class="No-Break">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/WhatIsCloudWatch.html</span></a><span class="No-Break">).</span></li>
				<li><strong class="bold">Invocations per instance</strong>: If your model is deployed on more than one machine, then it’s important to<a id="_idIndexMarker701"/> understand the number of invocations sent to a model on <span class="No-Break">each instance.</span></li>
				<li><strong class="bold">Model latency</strong>: This is the time<a id="_idIndexMarker702"/> interval taken by a model to respond. It includes the local communication time taken to send the request, for the model to complete the inference in the container, and to get the response from the container of <span class="No-Break">the model:</span></li>
			</ul>
			<p><em class="italic">model latency = request time + inference time taken by the model + response time from </em><span class="No-Break"><em class="italic">the container</em></span></p>
			<ul>
				<li><strong class="bold">Overhead latency</strong>: This is the time taken to respond after the endpoint has received the request<a id="_idIndexMarker703"/> minus the model latency. It can depend on<a id="_idIndexMarker704"/> multiple factors, such as request size, request frequency, authentication/authorization of the request, and response <span class="No-Break">payload size.</span></li>
				<li><strong class="bold">Maximum Invocations</strong>: This is the <a id="_idIndexMarker705"/>maximum number of requests to an endpoint <span class="No-Break">per minute.</span></li>
				<li><strong class="bold">Cost per hour</strong>: This gives the <a id="_idIndexMarker706"/>estimated cost per hour for <span class="No-Break">your endpoint.</span></li>
				<li><strong class="bold">Cost per inference</strong>: This provides the <a id="_idIndexMarker707"/>estimated cost per inference on <span class="No-Break">your endpoint.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">Cost metrics provide the cost in <span class="No-Break">US dollars.</span></p>
			<p>In addition to inference metrics, you should also consider optimizing for instance metrics such as GPU utilization, GPU memory, CPU utilization, and CPU memory based on the instance type selected. Amazon SageMaker offers more than 70 instances from which you can choose to deploy your model. This brings up an additional question on how to determine which instance type and the number of instances to select for deploying the model in order to achieve your performance requirements. Let’s discuss the approach for selecting the optimal instance for your model in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor183"/>Choosing the instance type, load testing, and performance tuning for models</h1>
			<p>Traditionally, based on the model type (ML model or DL model) and model size, you can make a heuristic guess to test the model’s performance on a few instances. This approach is fast but might not be the best approach. Therefore, in order to optimize this process, alternatively, you can <a id="_idIndexMarker708"/>use the <strong class="bold">Inference Recommender</strong> feature of Amazon SageMaker (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html</a>), which automates the load testing and model tuning process across the SageMaker ML instances. It helps you to deploy the ML models on the optimized hardware, based on your performance requirements, at the lowest <span class="No-Break">possible cost.</span></p>
			<p>Let’s take an example by using a pretrained image classification model to understand how Inference Recommender works. The following steps outline the process of using <span class="No-Break">Inference Recommender:</span></p>
			<ol>
				<li value="1">Determine the ML <a id="_idIndexMarker709"/>model details, such as framework and domain. The following is a code snippet <span class="No-Break">for this:</span><pre class="source-code">
…</pre><pre class="source-code">
# ML model details</pre><pre class="source-code">
ml_domain = "COMPUTER_VISION"</pre><pre class="source-code">
ml_task = "IMAGE_CLASSIFICATION"</pre><pre class="source-code">
…</pre></li>
				<li>Take a pretrained model and package it in the compressed TAR file (<strong class="source-inline">*.tar.gz</strong>) format, which SageMaker understands, and upload the model to Amazon S3. If you have trained the model on SageMaker, then you can skip <span class="No-Break">this step:</span><pre class="source-code">
…</pre><pre class="source-code">
from torchvision.models import resnet18, ResNet18_Weights</pre><pre class="source-code">
#initialize the model</pre><pre class="source-code">
weights = ResNet18_Weights.DEFAULT</pre><pre class="source-code">
model = resnet18(weights)</pre><pre class="source-code">
torch.save(model.state_dict(), './output/resnet18-model.pt')</pre><pre class="source-code">
with tarfile.open("model.tar.gz", "w:gz") as f:</pre><pre class="source-code">
    f.add("model.pth")</pre><pre class="source-code">
input_tensor = torch.zeros([1, 3, 224, 224])</pre><pre class="source-code">
model_uri = sagemaker_session.upload_data(path="model.tar.gz", key_prefix=key_prefix)</pre><pre class="source-code">
print("S3 Path for Model: ", model_uri)</pre><pre class="source-code">
…</pre></li>
				<li>Select the inference <a id="_idIndexMarker710"/>container, which can be a prebuilt Docker container provided by AWS or your own custom container. In our example, we are fetching a prebuilt PyTorch container image provided <span class="No-Break">by AWS:</span><pre class="source-code">
…</pre><pre class="source-code">
instance_type = "ml.c5.xlarge"  # Note: you can use any CPU-based instance type here, this is just to get a CPU tagged image</pre><pre class="source-code">
container_uri = image_uris.retrieve(</pre><pre class="source-code">
    "pytorch",</pre><pre class="source-code">
    region,</pre><pre class="source-code">
    version=framework_version,</pre><pre class="source-code">
    py_version="py3",</pre><pre class="source-code">
    instance_type=instance_type,</pre><pre class="source-code">
    image_scope="inference",</pre><pre class="source-code">
)</pre><pre class="source-code">
container_uri</pre><pre class="source-code">
…</pre></li>
				<li>Create a sample payload. In our example, we have images in <strong class="source-inline">.jpg</strong> format, compress them in a TAR file, and upload it to Amazon S3. In this example, we are only using four <a id="_idIndexMarker711"/>images, but it’s recommended to add a variety of samples, which is reflective of your <span class="No-Break">actual payloads:</span><pre class="source-code">
<strong class="bold">!wget </strong><a href="https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/019/1390196df443f2cf614f2255ae75fcf8.jpg">https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/019/1390196df443f2cf614f2255ae75fcf8.jpg</a><strong class="bold"> -P /sample-payload</strong></pre><pre class="source-code">
<strong class="bold">!wget </strong><a href="https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/015/1390157d4caaf290962de5c5fb4c42.jpg">https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/015/1390157d4caaf290962de5c5fb4c42.jpg</a><strong class="bold"> -P /sample-payload</strong></pre><pre class="source-code">
<strong class="bold">!wget </strong><a href="https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/020/1390207be327f4c4df1259c7266473.jpg">https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/020/1390207be327f4c4df1259c7266473.jpg</a>  -P<strong class="bold"> /sample-payload</strong></pre><pre class="source-code">
<strong class="bold">!wget </strong><a href="https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/028/139028d865bafa3de66568eeb499f4a6.jpg">https://multimedia-commons.s3-us-west-2.amazonaws.com/data/images/139/028/139028d865bafa3de66568eeb499f4a6.jpg</a>  -P<strong class="bold"> /sample-payload</strong></pre></li>
			</ol>
			<p>Compress the payload in TAR format as shown in the following code snippet and upload it <span class="No-Break">to S3:</span></p>
			<pre class="source-code">
<strong class="bold">cd ./sample-payload/ &amp;&amp; tar czvf ../{payload_archive_name} *</strong></pre>
			<p>Once, we have the payload in TAR format, let’s use the following code snippet to upload it to <span class="No-Break">Amazon S3:</span></p>
			<pre class="source-code">
…
sample_payload_url = sagemaker_session.upload_data(path=payload_archive_name, key_prefix="tf_payload")
…</pre>
			<ol>
				<li value="5">Register the model in the <strong class="bold">model registry</strong>, which is<a id="_idIndexMarker712"/> used to catalog models for production, manage model versions, associate metadata, manage the approval status of the model, deploy<a id="_idIndexMarker713"/> models to production, and automate the model deployment process (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html">https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html</a>). Registering a model in the model registry is a two-step process, as <span class="No-Break">shown here:</span><ol><li>Create a model package group, which will have all the versions of <span class="No-Break">the model:</span><pre class="source-code">
…</pre><pre class="source-code">
model_package_group_input_dict = {</pre><pre class="source-code">
"ModelPackageGroupName": model_package_group_name,</pre><pre class="source-code">
"ModelPackageGroupDescription": model_package_group_description,</pre><pre class="source-code">
}</pre><pre class="source-code">
create_model_package_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)</pre><pre class="source-code">
…</pre></li><li>Register a model version to the model package group. To get the recommended instance type, you have two options – either you can specify a list of instances that you want Inference Recommender to use or you can not provide the instance list and it will pick the right instance based on the ML domain and ML task. For our example, we will use a list of common instance types used for image classification algorithms. This involves <span class="No-Break">three steps.</span></li><li>First, create an input dictionary with configuration for registering the model, as <a id="_idIndexMarker714"/>shown in the following <span class="No-Break">code snippet:</span><pre class="source-code">
…</pre><pre class="source-code">
model_approval_status = "PendingManualApproval"</pre><pre class="source-code">
# provide an input dictionary with configuration for registering the model</pre><pre class="source-code">
model_package_input_dict = {</pre><pre class="source-code">
    "ModelPackageGroupName": model_package_group_name,</pre><pre class="source-code">
    "Domain": ml_domain.upper(),</pre><pre class="source-code">
    "Task": ml_task.upper(),</pre><pre class="source-code">
    "SamplePayloadUrl": sample_payload_url,</pre><pre class="source-code">
    "ModelPackageDescription": model_package_description,</pre><pre class="source-code">
    "ModelApprovalStatus": model_approval_status,</pre><pre class="source-code">
}# optional – provide a list of instances</pre><pre class="source-code">
supported_realtime_inference_types = ["ml.c5.xlarge", "ml.m5.large", "ml.inf1.xlarge"]</pre><pre class="source-code">
...</pre></li><li>The second step is to create a model inference specification object, which will consist of providing details about the container, framework, model input, content type, and the S3 path of the <span class="No-Break">trained model:</span><pre class="source-code">
#create model inference specification object</pre><pre class="source-code">
modelpackage_inference_specification = {</pre><pre class="source-code">
    "InferenceSpecification": {</pre><pre class="source-code">
        "Containers": [</pre><pre class="source-code">
            {</pre><pre class="source-code">
                "Image": container_uri,</pre><pre class="source-code">
                "Framework": "PYTORCH",</pre><pre class="source-code">
                "FrameworkVersion": framework_version,</pre><pre class="source-code">
                "ModelInput": {"DataInputConfig": data_input_configuration},</pre><pre class="source-code">
            }</pre><pre class="source-code">
        ],</pre><pre class="source-code">
        "SupportedContentTypes": "application/image",</pre><pre class="source-code">
        "SupportedRealtimeInferenceInstanceTypes": supported_realtime_inference_types,  # optional</pre><pre class="source-code">
    }</pre><pre class="source-code">
}</pre><pre class="source-code">
# Specify the model data</pre><pre class="source-code">
modelpackage_inference_specification["InferenceSpecification"]["Containers"][0]["ModelDataUrl"] = model_url</pre><pre class="source-code">
create_model_package_input_dict.update(modelpackage_inference_specification)</pre><pre class="source-code">
...</pre></li><li>Finally, after <a id="_idIndexMarker715"/>providing the inference specification, we will then create the model package. Once the model package is created, you can then get the model package ARN, and it will also be <a id="_idIndexMarker716"/>visible in the SageMaker Studio UI, under <span class="No-Break"><strong class="bold">Model Registry</strong></span><span class="No-Break">:</span><pre class="source-code">
create_mode_package_response = sm_client.create_model_package(**model_package_input_dict)</pre><pre class="source-code">
model_package_arn = create_mode_package_response["ModelPackageArn"]</pre><pre class="source-code">
…</pre></li></ol></li>
				<li>Now, that the model has been registered, we will create an Inference Recommender job. There are two options – either you can create a default job to get instance recommendations or you can use an advanced job, where you can provide your inference requirements, tune environment variables, and perform more extensive load tests. An advanced job takes more time than a default job and depends on your traffic pattern and the number of instance types on which it will run the <span class="No-Break">load tests.</span></li>
			</ol>
			<p>In this example, we will create a default job, which will return a list of instance type recommendations including environment variables, cost, throughput, model latency, and the maximum number <span class="No-Break">of invocations.</span></p>
			<p>The following code snippet shows how you can create a <span class="No-Break">default job:</span></p>
			<pre class="source-code">
…
response = sagemaker_client.create_inference_recommendations_job(
    JobName=str(default_job),
    JobDescription="",
    JobType="Default",
    RoleArn=role,
    InputConfig={"ModelPackageVersionArn": model_package_arn},
)
print(response)
…</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Code to create a custom load test is provided in the GitHub <span class="No-Break">repository: </span><a href="http://Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/Chapter09/1_inference_recommender_custom_load_test.ipynb"><span class="No-Break">Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/Chapter09/1_inference_recommender_custom_load_test.ipynb</span></a><span class="No-Break">.</span></p>
			<p>In the next section, we<a id="_idIndexMarker717"/> will discuss the results provided by the Inference <span class="No-Break">Recommender job.</span></p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor184"/>Observing the results</h1>
			<p>The recommendation<a id="_idIndexMarker718"/> provided by Inference Recommender includes instance metrics, performance metrics, and <span class="No-Break">cost metrics.</span></p>
			<p>Instance metrics<a id="_idIndexMarker719"/> include <strong class="source-inline">InstanceType</strong>, <strong class="source-inline">InitialInstanceCount</strong>, and <strong class="source-inline">EnvironmentParameters</strong>, which are tuned according to the job for <span class="No-Break">better performance.</span></p>
			<p>Performance metrics<a id="_idIndexMarker720"/> include <strong class="source-inline">MaxInvocations</strong> and <strong class="source-inline">ModelLatency</strong>, whereas <a id="_idIndexMarker721"/>cost metrics include <strong class="source-inline">CostPerHour</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">CostPerInference</strong></span><span class="No-Break">.</span></p>
			<p>These metrics enable you to make informed trade-offs between cost and performance. For example, if your business requirement is overall price performance with an emphasis on throughput, then you should focus on <strong class="source-inline">CostPerInference</strong>. If your requirement is a balance between latency and throughput, then you should focus on <strong class="source-inline">ModelLatency</strong> and <span class="No-Break"><strong class="source-inline">MaxInvocations</strong></span><span class="No-Break"> metrics.</span></p>
			<p>You can view the results of the Inference Recommender job either through an API call or in the SageMaker <span class="No-Break">Studio UI.</span></p>
			<p>The following is the code snippet for observing <span class="No-Break">the results:</span></p>
			<pre class="source-code">
…
data = [
    {**x["EndpointConfiguration"], **x["ModelConfiguration"], **x["Metrics"]}
    for x in inference_recommender_job["InferenceRecommendations"]
]
df = pd.DataFrame(data)
…</pre>
			<p>You can observe the<a id="_idIndexMarker722"/> results from the SageMaker Studio UI by logging into SageMaker Studio, clicking on the orange triangle icon, and selecting <strong class="bold">Model registry</strong> from the <span class="No-Break">drop-down menu:</span></p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B18493_09_007.jpg" alt="Figure 9.7 – Inference Recommender results"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Inference Recommender results</p>
			<p>Now that we understand <a id="_idIndexMarker723"/>how the Inference Recommender feature of Amazon SageMaker can be used to get the right instance type and instance count, let’s take a look at the topics covered in this chapter in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor185"/>Summary</h1>
			<p>In this chapter, we discussed various techniques for optimizing ML and DL models for real-time inference. We talked about different ways to reduce the memory footprint of DL models, such as pruning and quantization, followed by a deeper dive into model compilation. We then discussed key metrics that can help in evaluating the performance of models. Finally, we did a deep dive into how you can select the right instance, run load tests, and automatically perform model tuning using SageMaker Inference <span class="No-Break">Recommender’s capability.</span></p>
			<p>In the next chapter, we will discuss visualizing and exploring large amounts of data <span class="No-Break">on AWS.</span></p>
		</div>
		<div>
			<div id="_idContainer135" class="IMG---Figure">
			</div>
		</div>
	</body></html>