["```py\nX, y = make_classification(\n    n_samples=50000, n_features=2, n_redundant=0, class_sep=2,\\\n    weights=[0.99], random_state=1, n_clusters_per_class=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\\\n    test_size = 0.2, random_state = 0, stratify=y)\nprint('y_train: ', Counter(y_train))\nprint('y_test: ', Counter(y_test))\nplot_dataset(X_train, y_train)\n```", "```py\ny_train:  Counter({0: 39404, 1: 596})\ny_test:  Counter({0: 9851, 1: 149})\n```", "```py\nimport numpy as np\ndef sigmoid(x):\n     s = 1/(1+np.exp(-x))\n     return s\n```", "```py\nlr = LogisticRegression(random_state=0, max_iter=150).fit(\n    X_train, y_train)\nplot_decision_boundary(X_train, y_train, lr, 'LogisticRegression')\nplt.show()\nPrecisionRecallDisplay.from_estimator(\n    lr, X_test, y_test, ax = plt.gca(),name = \"LogisticRegression\")\n```", "```py\ncompute_scores(lr, X_test, y_test)\n```", "```py\nf2-score: 0.921 precision: 0.926 recall: 0.919\n```", "```py\nLogisticRegression(class_weight={0: 0.5, 1:0.5})\n```", "```py\nlr_weighted = LogisticRegression(class_weight='balanced', \\\n    random_state=0, max_iter=150).fit(X_train, y_train)\nplot_decision_boundary(X_train, y_train, lr_weighted, \\\n    'LogisticRegression')\nplt.show()\nPrecisionRecallDisplay.from_estimator(lr_weighted, X_test,\\\n    y_test, ax = plt.gca(),name = \"LogisticRegressionWeighted\")\n```", "```py\ncompute_scores(lr_weighted, X_test, y_test)\n```", "```py\nf2-score: 0.873 precision: 0.587 recall: 0.993\n```", "```py\nfrom sklearn.metrics import make_scorer, fbeta_score\ndef f2_func(y_true, y_pred):\n    f2_score = fbeta_score(y_true, y_pred, beta=2.)\n    return f2_score\ndef f2_scorer():\n    return make_scorer(f2_func)\n# Define the parameter grid\nparam_grid = {\n    'class_weight': [\n        {0: x, 1: 1.0-x} for x in np.linspace(0.05, 0.95, 20)]\n}\n# Instantiate the grid search model\ngrid_search =GridSearchCV(\n    LogisticRegression(),param_grid,\\\n    cv=3, scoring=f2_scorer(), n_jobs=-1\n)\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\n# Get the best parameters\nbest_params = grid_search.best_params_\nbest_params\n```", "```py\n{'class_weight': {0: 0.14473684210526316, 1: 0.8552631578947368}}\n```", "```py\nf2-score: 0.930 precision: 0.892 recall: 0.940\n```", "```py\nsvm.SVC(class_weight= {-1: 1.0, 0: 1.0, 1: 1.0})\n```", "```py\nXGBClassifier(scale_pos_weight)\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import PrecisionRecallDisplay\ndt_clf = DecisionTreeClassifier(random_state=0, max_depth=6).fit(\n    X_train, y_train)\nplot_decision_boundary(X,y,dt_clf,'DecisionTreeClassifier')\nplt.show()\nPrecisionRecallDisplay.from_estimator(\n    dt_clf, X_test, y_test, ax = plt.gca(),\\\n    name = \"DecisionTreeClassifier\")\nprint(classification_report_imbalanced(\n    y_test,\\\n    dt_clf.predict(X_test), \\\n    target_names=['class 0', 'class 1']\n    )\n)\ncomputescores(dt_clf, X_test, y_test)\n```", "```py\nf2-score: 0.932 precision: 0.892 recall: 0.94\n```", "```py\ndt_clf_tuned = DecisionTreeClassifier(\n    class_weight = 'balanced', random_state=0, max_depth=6\n).fit(X_train, y_train)\n```", "```py\nf2-score: 0.934 precision: 0.770 recall: 0.987\n```", "```py\nC = np.array([[0, 1], [1, 0]])\n```", "```py\nC = np.array([[0, 66], [1, 0]])\n```", "```py\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\ny_pred = lr.predict_proba(X_test)\ny_pred = y_pred[:, 1]\n```", "```py\nnum_pos, num_neg = Counter(y)[1], Counter(y)[0]\nprior_threshold = num_pos /(num_pos + num_neg)\nprint('Prior threshold=%f'% prior_threshold)\n# Find the closest threshold from thresholds from ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nmin_threshold_index = np.absolute( \\\n    thresholds-prior_threshold).argmin()\nprint('Best threshold using prior threshold from ROC \\\n    function=%f'% thresholds[min_threshold_index])\nprint(\"TPR at threshold=%f\" % tpr[min_threshold_index])\nprint(\"FPR at threshold=%f\" % fpr[min_threshold_index])\nprint(\"TNR at threshold=%f\" % (1-fpr[min_threshold_index]))\n```", "```py\nPrior threshold=0.014900\nBest threshold using prior threshold from ROC function=0.014232\nTPR value at the threshold=0.675676\nFPR value at the threshold=0.147990\nTNR value at the threshold=0.852010\n```", "```py\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\nyouden_index = tpr - fpr\nmax_youden_index = np.argmax(youden_index)\nbest_thresh = thresholds[max_youden_index]\nprint('Best threshold using Youden index=%f'% best_thresh)\nprint('Max Youden index value=%f'% youden_index[max_youden_index])\nprint(\"TPR value at the threshold=%f\" % tpr[max_youden_index])\nprint(\"FPR value at the threshold=%f\" % fpr[max_youden_index])\nprint(\"TNR value at the threshold=%f\" % (1-fpr[max_youden_index]))\n```", "```py\nBest threshold using Youden index=0.098879\nMax Youden index value=0.622143\nTPR value at the threshold=0.635135\nFPR value at the threshold=0.012992\nTNR value at the threshold=0.987008\n```", "```py\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\ngmean = np.sqrt(tpr*(1-fpr))\nmax_gmean_index = np.argmax(gmean)\nbest_thresh = thresholds[max_gmean_index]\nprint('Best threshold using G-mean=%f'% (best_thresh))\nprint('Max G-mean value=%f'% (gmean[max_gmean_index]))\nprint(\"TPR value at the threshold=%f\" % tpr[max_gmean_index])\nprint(\"FPR value at the threshold=%f\" % fpr[max_gmean_index])\nprint(\"TNR value at the threshold=%f\" % (1 - fpr[max_youden_index]))\n```", "```py\nBest threshold using G-mean=0.098879\nMax G-mean value=0.791760\nTPR value at the threshold=0.635135\nFPR value at the threshold=0.012992\nTNR value at the threshold=0.987008\n```", "```py\nfrom numpy import argmax\nfrom sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred)\nfscore = 2*precision*recall/(precision+recall)\nmax_fscore_idx = argmax(fscore)\nprint('max_fscore_idx==%d' % max_fscore_idx)\nprint('best threshold using PR curve=%f' % thresholds[max_fscore_idx])\n```", "```py\nmax_fscore_idx==4950\nbest threshold using PR curve=0.258727\nmax(fscore)= 0.661290\n```", "```py\npyplot.plot(recall, precision, marker='.', \\\n    label='precision vs recall for various thresholds')\nresult = pyplot.scatter(recall[max_fscore_idx], \\\n    precision[max_fscore_idx], \\\n    marker='x', color='red', \\\n    label='Best threshold with highest\\\n    f1-score')\nplt.legend(handles=result.legend_elements()[0], \\\n    labels=\"legend\", loc='upper center', \\\n    bbox_to_anchor=(1, 1))\npyplot.xlabel('recall')\npyplot.ylabel('precision')\n```", "```py\nfrom sklearn.metrics import f1_score, auc\ndef custom_metric(y_test, y_pred):\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    sensitivity = tpr\n    specificity = 1-fpr #same as tnr\n    roc_auc = auc(fpr, tpr)\n    index_of_union = abs(sensitivity-roc_auc) + \\\n        abs(specificity-roc_auc)\n    return index_of_union\nscores = custom_metric(y_test, y_pred)\nmin_score_idx = np.argmin(scores)\nprint('max_score_idx=%d' % min_score_idx)\nprint('best threshold=%f'% thresholds[min_score_idx])\nprint('minimum IU-value at the best threshold=%f' % \\\n    scores[min_score_idx])\n```", "```py\nmax_score_idx=34\nbest threshold=0.000042\nminimum IU-value at the best threshold=0.112514\n```"]