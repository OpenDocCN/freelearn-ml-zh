<html><head></head><body>
<div id="_idContainer112" class="Content">
<p class="hidden" data-amznremoved-m8="true" data-amznremoved="mobi7">&gt;</p>
</div>
<div id="_idContainer113" class="Content">
<h1 id="_idParaDest-186"><a id="_idTextAnchor200"></a>
 Appendix</h1>
</div>
<div id="_idContainer114" class="Content">
<h2>About</h2>
<p>This section is included to assist the students to perform the activities in the book. It includes detailed steps that are to be performed by the students to achieve the objectives of the activities.</p>
</div>
<div id="_idContainer128" class="Content">
<h2 id="_idParaDest-187"><a id="_idTextAnchor201"></a>
 Chapter 1: Principles of AI</h2>
<p>In the code, backslash (\) indicates a line break, where the code does not fit a line. A backslash at the end of the line escapes the newline character. This means that the content in the line following the backslash should be read as if it started where the backslash character is.</p>
<h3 id="_idParaDest-188"><a id="_idTextAnchor202"></a>
 Activity 1: Generating All Possible Sequences of Steps in the tic-tac-toe Game</h3>
<p>This section will explore the combinatoric explosion possible when two players play randomly. We will be using a program, building on the previous results that generate all possible sequences of moves between a computer player and a human player. Determine the number of different wins, losses, and draws in terms of action sequences. Assume that the human player may make any possible move. In this example, given that the computer player is playing randomly, we will examine the wins, losses, and draws belonging to two randomly playing players:</p>
<ol>
<li value="1">Create a function that maps the <strong class="inline _idGenCharOverride-1">all_moves_from_board</strong>
 function on each element of a list of boards. This way, we will have all of the nodes of a decision tree in each depth:<p class="snippet _idGenParaOverride-1">def all_moves_from_board(board, sign):</p>
<p class="snippet _idGenParaOverride-1">    move_list = []</p>
<p class="snippet _idGenParaOverride-1">    for i, v in enumerate(board):</p>
<p class="snippet _idGenParaOverride-1">        if v == EMPTY_SIGN:</p>
<p class="snippet _idGenParaOverride-1">            move_list.append(board[:i] + sign + board[i+1:])</p>
<p class="snippet _idGenParaOverride-1">    return move_list</p>
</li>
<li value="2">The decision tree starts with <strong class="inline _idGenCharOverride-1">[ EMPTY_SIGN * 9 ]</strong>
 , and expands after each move:<p class="snippet _idGenParaOverride-1">all_moves_from_board_list( [ EMPTY_SIGN * 9 ], AI_SIGN )</p>
</li>
<li value="3">The output is as follows:<p class="snippet _idGenParaOverride-1"> ['X........',</p>
<p class="snippet _idGenParaOverride-1"> '.X.......',</p>
<p class="snippet _idGenParaOverride-1"> '..X......',</p>
<p class="snippet _idGenParaOverride-1"> '...X.....',</p>
<p class="snippet _idGenParaOverride-1"> '....X....',</p>
<p class="snippet _idGenParaOverride-1"> '.....X...',</p>
<p class="snippet _idGenParaOverride-1"> '......X..',</p>
<p class="snippet _idGenParaOverride-1"> '.......X.',</p>
<p class="snippet _idGenParaOverride-1"> '........X']</p>
<p class="snippet _idGenParaOverride-1"> ['XO.......',</p>
<p class="snippet _idGenParaOverride-1"> 'X.O......',</p>
<p class="snippet _idGenParaOverride-1"> 'X..O.....',</p>
<p class="snippet _idGenParaOverride-1"> 'X...O....',</p>
<p class="snippet _idGenParaOverride-1"> 'X....O...',</p>
<p class="snippet _idGenParaOverride-1"> 'X.....O..',</p>
<p class="snippet _idGenParaOverride-1"> 'X......O.',</p>
<p class="snippet _idGenParaOverride-1">.</p>
<p class="snippet _idGenParaOverride-1">.</p>
<p class="snippet _idGenParaOverride-1">.</p>
<p class="snippet _idGenParaOverride-1">.</p>
<p class="snippet _idGenParaOverride-1">'......OX.',</p>
<p class="snippet _idGenParaOverride-1"> '.......XO',</p>
<p class="snippet _idGenParaOverride-1"> 'O.......X',</p>
<p class="snippet _idGenParaOverride-1"> '.O......X',</p>
<p class="snippet _idGenParaOverride-1"> '..O.....X',</p>
<p class="snippet _idGenParaOverride-1"> '...O....X',</p>
<p class="snippet _idGenParaOverride-1"> '....O...X',</p>
<p class="snippet _idGenParaOverride-1"> '.....O..X',</p>
<p class="snippet _idGenParaOverride-1"> '......O.X',</p>
<p class="snippet _idGenParaOverride-1"> '.......OX']</p>
</li>
<li value="4">Let's create a <strong class="inline _idGenCharOverride-1">filter_wins</strong>
 function that takes the ended games out from the list of moves and appends them in an array containing the board states won by the AI player and the opponent player:<p class="snippet _idGenParaOverride-1">def filter_wins(move_list, ai_wins, opponent_wins):</p>
<p class="snippet _idGenParaOverride-1">    for board in move_list:</p>
<p class="snippet _idGenParaOverride-1">        won_by = game_won_by(board)</p>
<p class="snippet _idGenParaOverride-1">        if won_by == AI_SIGN:</p>
<p class="snippet _idGenParaOverride-1">            ai_wins.append(board)</p>
<p class="snippet _idGenParaOverride-1">            move_list.remove(board)</p>
<p class="snippet _idGenParaOverride-1">        elif won_by == OPPONENT_SIGN:</p>
<p class="snippet _idGenParaOverride-1">            opponent_wins.append(board)</p>
<p class="snippet _idGenParaOverride-1">            move_list.remove(board)</p>
</li>
<li class="_idGenParaOverride-2" value="5">In this function, the three lists can be considered as reference types. This means that the function does not return a value, instead but it manipulating these three lists without returning them.<div></div>
</li>
<li value="6">Let's finish this section. Then with a <strong class="inline _idGenCharOverride-1">count_possibilities</strong>
 function that prints the number of decision tree leaves that ended with a draw, won by the first player, and won by the second player:<p class="snippet _idGenParaOverride-1">def count_possibilities():</p>
<p class="snippet _idGenParaOverride-1">    board = EMPTY_SIGN * 9</p>
<p class="snippet _idGenParaOverride-1">    move_list = [board]</p>
<p class="snippet _idGenParaOverride-1">    ai_wins = []</p>
<p class="snippet _idGenParaOverride-1">    opponent_wins = []</p>
<p class="snippet _idGenParaOverride-1">    for i in range(9):</p>
<p class="snippet _idGenParaOverride-1">        print('step ' + str(i) + '. Moves: ' + \        str(len(move_list)))</p>
<p class="snippet _idGenParaOverride-1">        sign = AI_SIGN if i % 2 == 0 else OPPONENT_SIGN</p>
<p class="snippet _idGenParaOverride-1">        move_list = all_moves_from_board_list(move_list, sign)</p>
<p class="snippet _idGenParaOverride-1">        filter_wins(move_list, ai_wins, opponent_wins)</p>
<p class="snippet _idGenParaOverride-1">    print('First player wins: ' + str(len(ai_wins)))</p>
<p class="snippet _idGenParaOverride-1">    print('Second player wins: ' + str(len(opponent_wins)))</p>
<p class="snippet _idGenParaOverride-1">    print('Draw', str(len(move_list)))</p>
<p class="snippet _idGenParaOverride-1">    print('Total', str(len(ai_wins) + len(opponent_wins) + \    len(move_list)))</p>
</li>
<li value="7">We have up to 9 steps in each state. In the 0th, 2nd, 4th, 6th, and 8th iteration, the AI player moves. In all other iterations, the opponent moves. We create all possible moves in all steps and take out the ended games from the move list.</li>
<li value="8">Then execute the number of possibilities to experience the combinatoric explosion.<p class="snippet _idGenParaOverride-1">count_possibilities()</p>
</li>
<li value="9">The output is as follows:<p class="snippet _idGenParaOverride-1">step 0. Moves: 1</p>
<p class="snippet _idGenParaOverride-1">step 1. Moves: 9</p>
<p class="snippet _idGenParaOverride-1">step 2. Moves: 72</p>
<p class="snippet _idGenParaOverride-1">step 3. Moves: 504</p>
<p class="snippet _idGenParaOverride-1">step 4. Moves: 3024</p>
<p class="snippet _idGenParaOverride-1">step 5. Moves: 13680</p>
<p class="snippet _idGenParaOverride-1">step 6. Moves: 49402</p>
<p class="snippet _idGenParaOverride-1">step 7. Moves: 111109</p>
<p class="snippet _idGenParaOverride-1">step 8. Moves: 156775</p>
<p class="snippet _idGenParaOverride-1">First player wins: 106279</p>
<p class="snippet _idGenParaOverride-1">Second player wins: 68644</p>
<p class="snippet _idGenParaOverride-1">Draw 91150</p>
<p class="snippet _idGenParaOverride-3">Total 266073</p>
<div></div>
</li>
</ol>
<p>As you can see, the tree of board states consists of 266,073 leaves. The <strong class="inline _idGenCharOverride-1">count_possibilities</strong>
 function essentially implements a breadth first search algorithm to traverse all the possible states of the game. Notice that we do count these states multiple times, because placing an X on the top-right corner on step 1 and placing an X on the top-left corner on step 3 leads to similar possible states as starting with the top-left corner and then placing an X on the top-right corner. If we implemented a detection of duplicate states, we would have to check less nodes. However, at this stage, due to the limited depth of the game, we omit this step.</p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor203"></a>
 Chapter 2: AI with Search Techniques and Games</h2>
<h3 id="_idParaDest-190"><a id="_idTextAnchor204"></a>
 Activity 2: Teach the agent realize situations when it defends against losses</h3>
<p>Follow these steps to complete the activity:</p>
<ol>
<li class="ParaOverride-1" value="1">Create a function <strong class="inline _idGenCharOverride-1">player_can_win</strong>
 such that it takes all moves from the board using the <strong class="inline _idGenCharOverride-1">all_moves_from_board</strong>
 function and iterates over it using a variable <strong class="inline _idGenCharOverride-1">next_move</strong>
 . On each iteration, it checks if the game can be won by the sign, then it return true else false.<p class="snippet _idGenParaOverride-1">def player_can_win(board, sign):</p>
<p class="snippet _idGenParaOverride-1">    next_moves = all_moves_from_board(board, sign)</p>
<p class="snippet _idGenParaOverride-1">    for next_move in next_moves:</p>
<p class="snippet _idGenParaOverride-1">        if game_won_by(next_move) == sign:</p>
<p class="snippet _idGenParaOverride-1">            return True</p>
<p class="snippet _idGenParaOverride-1">    return False</p>
</li>
<li value="2">We will extend the AI move such that it prefers making safe moves. A move is safe if the opponent cannot win the game in the next step.<p class="snippet _idGenParaOverride-1">def ai_move(board):</p>
<p class="snippet _idGenParaOverride-1">    new_boards = all_moves_from_board(board, AI_SIGN)</p>
<p class="snippet _idGenParaOverride-1">    for new_board in new_boards:</p>
<p class="snippet _idGenParaOverride-1">        if game_won_by(new_board) == AI_SIGN:</p>
<p class="snippet _idGenParaOverride-1">            return new_board</p>
<p class="snippet _idGenParaOverride-1">    safe_moves = []</p>
<p class="snippet _idGenParaOverride-1">    for new_board in new_boards:</p>
<p class="snippet _idGenParaOverride-1">        if not player_can_win(new_board, OPPONENT_SIGN):</p>
<p class="snippet _idGenParaOverride-1">            safe_moves.append(new_board)</p>
<p class="snippet _idGenParaOverride-3">    return choice(safe_moves) if len(safe_moves) &gt; 0 else \        new_boards[0]</p>
<div></div>
</li>
<li value="3">You can test our new application. You will find the AI has made the correct move.</li>
<li value="4">We will now place this logic in the state space generator and check how well the computer player is doing by generating all the possible games.<p class="snippet _idGenParaOverride-1">def all_moves_from_board( board, sign ):</p>
</li>
<li value="5">We will now place this logic in the state space generator and check how well the computer player is doing by generating all the possible games.<p class="snippet _idGenParaOverride-1">def all_moves_from_board(board, sign):</p>
<p class="snippet _idGenParaOverride-1">    move_list = []</p>
<p class="snippet _idGenParaOverride-1">    for i, v in enumerate(board):</p>
<p class="snippet _idGenParaOverride-1">        if v == EMPTY_SIGN:</p>
<p class="snippet _idGenParaOverride-1">            new_board = board[:i] + sign + board[i+1:]</p>
<p class="snippet _idGenParaOverride-1">            move_list.append(new_board)</p>
<p class="snippet _idGenParaOverride-1">            if game_won_by(new_board) == AI_SIGN:</p>
<p class="snippet _idGenParaOverride-1">                return [new_board]</p>
<p class="snippet _idGenParaOverride-1">    if sign == AI_SIGN:</p>
<p class="snippet _idGenParaOverride-1">        safe_moves = []</p>
<p class="snippet _idGenParaOverride-1">        for move in move_list:</p>
<p class="snippet _idGenParaOverride-1">            if not player_can_win(move, OPPONENT_SIGN):</p>
<p class="snippet _idGenParaOverride-1">                safe_moves.append(move)</p>
<p class="snippet _idGenParaOverride-1">        return safe_moves if len(safe_moves) &gt; 0 else \            move_list[0:1]</p>
<p class="snippet _idGenParaOverride-1">    else:</p>
<p class="snippet _idGenParaOverride-1">        return move_list</p>
</li>
<li value="6">Count the possibilities that as possible.<p class="snippet _idGenParaOverride-1">count_possibilities()</p>
</li>
<li value="7">The output is as follows:<p class="snippet _idGenParaOverride-1">step 0. Moves: 1</p>
<p class="snippet _idGenParaOverride-1">step 1. Moves: 9</p>
<p class="snippet _idGenParaOverride-1">step 2. Moves: 72</p>
<p class="snippet _idGenParaOverride-1">step 3. Moves: 504</p>
<p class="snippet _idGenParaOverride-1">step 4. Moves: 3024</p>
<p class="snippet _idGenParaOverride-1">step 5. Moves: 5197</p>
<p class="snippet _idGenParaOverride-1">step 6. Moves: 18606</p>
<p class="snippet _idGenParaOverride-1">step 7. Moves: 19592</p>
<p class="snippet _idGenParaOverride-3">step 8. Moves: 30936</p>
<div></div>
<p class="snippet _idGenParaOverride-1">First player wins: 20843</p>
<p class="snippet _idGenParaOverride-1">Second player wins: 962</p>
<p class="snippet _idGenParaOverride-1">Draw 20243</p>
<p class="snippet _idGenParaOverride-1">Total 42048</p>
</li>
</ol>
<p>We are doing better than before. We not only got rid of almost 2/3 of possible games again, but most of the time, the AI player either wins or settles for a draw. Despite our effort to make the AI better, it can still lose in 962 ways. We will eliminate all these losses in the next activity.</p>
<h3 id="_idParaDest-191"><a id="_idTextAnchor205"></a>
 Activity 3: Fix the first and second moves of the AI to make it invincible</h3>
<p>Follow these steps to complete the activity:</p>
<ol>
<li class="ParaOverride-1" value="1">We will count the number of empty fields in the board and make a hard-coded move in case there are 9 or 7 empty fields. You can experiment with different hard coded moves. We found that occupying any corner, then occupying the opposite corner leads to no losses. If the opponent occupied the opposite corner, making a move in the middle results in no losses.<p class="snippet _idGenParaOverride-1">def all_moves_from_board(board, sign):</p>
<p class="snippet _idGenParaOverride-1">    if sign == AI_SIGN:</p>
<p class="snippet _idGenParaOverride-1">        empty_field_count = board.count(EMPTY_SIGN)</p>
<p class="snippet _idGenParaOverride-1">        if empty_field_count == 9:</p>
<p class="snippet _idGenParaOverride-1">            return [sign + EMPTY_SIGN * 8]</p>
<p class="snippet _idGenParaOverride-1">        elif empty_field_count == 7:</p>
<p class="snippet _idGenParaOverride-1">            return [</p>
<p class="snippet _idGenParaOverride-1">                board[:8] + sign if board[8] == \                    EMPTY_SIGN else</p>
<p class="snippet _idGenParaOverride-1">                board[:4] + sign + board[5:]</p>
<p class="snippet _idGenParaOverride-1">            ]</p>
<p class="snippet _idGenParaOverride-1">    move_list = []</p>
<p class="snippet _idGenParaOverride-1">    for i, v in enumerate(board):</p>
<p class="snippet _idGenParaOverride-1">        if v == EMPTY_SIGN:</p>
<p class="snippet _idGenParaOverride-1">            new_board = board[:i] + sign + board[i+1:]</p>
<p class="snippet _idGenParaOverride-1">            move_list.append(new_board)</p>
<p class="snippet _idGenParaOverride-1">            if game_won_by(new_board) == AI_SIGN:</p>
<p class="snippet _idGenParaOverride-1">                return [new_board]</p>
<p class="snippet _idGenParaOverride-3">    if sign == AI_SIGN:</p>
<div></div>
<p class="snippet _idGenParaOverride-1">        safe_moves = []</p>
<p class="snippet _idGenParaOverride-1">        for move in move_list:</p>
<p class="snippet _idGenParaOverride-1">            if not player_can_win(move, OPPONENT_SIGN):</p>
<p class="snippet _idGenParaOverride-1">                safe_moves.append(move)</p>
<p class="snippet _idGenParaOverride-1">        return safe_moves if len(safe_moves) &gt; 0 else \            move_list[0:1]</p>
<p class="snippet _idGenParaOverride-1">    else:</p>
<p class="snippet _idGenParaOverride-1">        return move_list</p>
</li>
<li value="2">Let's verify the state space<p class="snippet _idGenParaOverride-1">countPossibilities()</p>
</li>
<li value="3">The output is as follows:<p class="snippet _idGenParaOverride-1">step 0. Moves: 1</p>
<p class="snippet _idGenParaOverride-1">step 1. Moves: 1</p>
<p class="snippet _idGenParaOverride-1">step 2. Moves: 8</p>
<p class="snippet _idGenParaOverride-1">step 3. Moves: 8</p>
<p class="snippet _idGenParaOverride-1">step 4. Moves: 48</p>
<p class="snippet _idGenParaOverride-1">step 5. Moves: 38</p>
<p class="snippet _idGenParaOverride-1">step 6. Moves: 108</p>
<p class="snippet _idGenParaOverride-1">step 7. Moves: 76</p>
<p class="snippet _idGenParaOverride-1">step 8. Moves: 90</p>
<p class="snippet _idGenParaOverride-1">First player wins: 128</p>
<p class="snippet _idGenParaOverride-1">Second player wins: 0</p>
<p class="snippet _idGenParaOverride-1">Draw 60</p>
</li>
<li value="4">After fixing the first two steps, we only need to deal with 8 possibilities instead of 504. We also guided the AI into a state, where the hard-coded rules were sufficient for never losing a game.</li>
<li value="5">Fixing the steps is not important because we would give the AI hard coded steps to start with, but it is important, because it is a tool to evaluate and compare each step.</li>
<li class="_idGenParaOverride-2" value="6">After fixing the first two steps, we only need to deal with 8 possibilities instead of 504. We also guided the AI into a state, where the hard-coded rules were sufficient for never losing a game.<div></div>
</li>
</ol>
<h3 id="_idParaDest-192"><a id="_idTextAnchor206"></a>
 Activity 4: Connect Four</h3>
<p>This section will practice using the <strong class="inline _idGenCharOverride-1">EasyAI</strong>
 library and develop a heuristic. We will be using connect four game. The game board is seven cells wide and cells high. When you make a move, you can only select the column in which you drop your token. Then gravity pulls the token down to the lowest possible empty cell. Your objective is to connect four of your own tokens horizontally, vertically, or diagonally, before your opponent does this, or you run out of empty spaces. The rules of the game can be found at: <a href="https://en.wikipedia.org/wiki/Connect_Four">https://en.wikipedia.org/wiki/Connect_Four</a>
</p>
<ol>
<li class="ParaOverride-1" value="1">Let's set up the TwoPlayersGame framework:<p class="snippet _idGenParaOverride-1">from easyAI import TwoPlayersGame</p>
<p class="snippet _idGenParaOverride-1">from easyAI.Player import Human_Player</p>
<p class="snippet _idGenParaOverride-1">class ConnectFour(TwoPlayersGame):</p>
<p class="snippet _idGenParaOverride-1">    def __init__(self, players):</p>
<p class="snippet _idGenParaOverride-1">        self.players = players</p>
<p class="snippet _idGenParaOverride-1">    def possible_moves(self):</p>
<p class="snippet _idGenParaOverride-1">        return []</p>
<p class="snippet _idGenParaOverride-1">    def make_move(self, move):</p>
<p class="snippet _idGenParaOverride-1">        return</p>
<p class="snippet _idGenParaOverride-1">    def unmake_move(self, move):</p>
<p class="snippet _idGenParaOverride-1"># optional method (speeds up the AI)</p>
<p class="snippet _idGenParaOverride-1">        return</p>
<p class="snippet _idGenParaOverride-1">    def lose(self):</p>
<p class="snippet _idGenParaOverride-1">        return False</p>
<p class="snippet _idGenParaOverride-1">    def is_over(self):</p>
<p class="snippet _idGenParaOverride-1">        return (self.possible_moves() == []) or self.lose()</p>
<p class="snippet _idGenParaOverride-1">    def show(self):</p>
<p class="snippet _idGenParaOverride-1">        print ('board')</p>
<p class="snippet _idGenParaOverride-1">    def scoring(self):</p>
<p class="snippet _idGenParaOverride-1">        return -100 if self.lose() else 0</p>
<p class="snippet _idGenParaOverride-1">    </p>
<p class="snippet _idGenParaOverride-1">if __name__ == "__main__":</p>
<p class="snippet _idGenParaOverride-1">    from easyAI import AI_Player, Negamax</p>
<p class="snippet _idGenParaOverride-3">    ai_algo = Negamax(6)</p>
<div></div>
</li>
<li value="2">We can leave a few functions from the definition intact. We have to implement the following methods:<p class="snippet _idGenParaOverride-1">__init__</p>
<p class="snippet _idGenParaOverride-1">possible_moves</p>
<p class="snippet _idGenParaOverride-1">make_move</p>
<p class="snippet _idGenParaOverride-1">unmake_move (optional)</p>
<p class="snippet _idGenParaOverride-1">lose</p>
<p class="snippet _idGenParaOverride-1">show</p>
</li>
<li value="3">We will reuse the basic scoring function from tic-tac-toe. Once you test out the game, you will see that the game is not unbeatable, but plays surprisingly well, even though we are only using basic heuristics.</li>
<li value="4">Let's write the init method. We will define the board as a one-dimensional list, similar to the tic-tac-toe example. We could use a two-dimensional list too, but modeling will not get much easier or harder. Beyond making initializations like we did in the tic-tac-toe game, we will work a bit ahead. We will generate all of the possible winning combinations in the game and save them for future use:<p class="snippet _idGenParaOverride-1">def __init__(self, players):</p>
<p class="snippet _idGenParaOverride-1">        self.players = players</p>
<p class="snippet _idGenParaOverride-1">        # 0 1 2 3 4 5 6</p>
<p class="snippet _idGenParaOverride-1">        # 7 8 9 10 11 12 13</p>
<p class="snippet _idGenParaOverride-1">        # ...</p>
<p class="snippet _idGenParaOverride-1">        # 35 36 37 38 39 40 41</p>
<p class="snippet _idGenParaOverride-1">        self.board = [0 for i in range(42)]</p>
<p class="snippet _idGenParaOverride-1">        self.nplayer = 1 # player 1 starts.</p>
<p class="snippet _idGenParaOverride-1">        def generate_winning_tuples():</p>
<p class="snippet _idGenParaOverride-1">            tuples = []</p>
<p class="snippet _idGenParaOverride-1">            # horizontal</p>
<p class="snippet _idGenParaOverride-1">            tuples += [</p>
<p class="snippet _idGenParaOverride-1">                list(range(row*7+column, row*7+column+4, 1))</p>
<p class="snippet _idGenParaOverride-1">                for row in range(6)</p>
<p class="snippet _idGenParaOverride-1">                for column in range(4)]</p>
<p class="snippet _idGenParaOverride-1">            # vertical</p>
<p class="snippet _idGenParaOverride-1">            tuples += [</p>
<p class="snippet _idGenParaOverride-1">                list(range(row*7+column, row*7+column+28, 7))</p>
<p class="snippet _idGenParaOverride-1">                for row in range(3)</p>
<p class="snippet _idGenParaOverride-1">                for column in range(7)</p>
<p class="snippet _idGenParaOverride-3">            ]</p>
<div></div>
<p class="snippet _idGenParaOverride-1">            # diagonal forward</p>
<p class="snippet _idGenParaOverride-1">            tuples += [</p>
<p class="snippet _idGenParaOverride-1">                list(range(row*7+column, row*7+column+32, 8))</p>
<p class="snippet _idGenParaOverride-1">                for row in range(3)</p>
<p class="snippet _idGenParaOverride-1">                for column in range(4)</p>
<p class="snippet _idGenParaOverride-1">            ]</p>
<p class="snippet _idGenParaOverride-1">            # diagonal backward</p>
<p class="snippet _idGenParaOverride-1">            tuples += [</p>
<p class="snippet _idGenParaOverride-1">                list(range(row*7+column, row*7+column+24, 6))</p>
<p class="snippet _idGenParaOverride-1">                for row in range(3)</p>
<p class="snippet _idGenParaOverride-1">                for column in range(3, 7, 1)</p>
<p class="snippet _idGenParaOverride-1">            ]</p>
<p class="snippet _idGenParaOverride-1">            return tuples</p>
<p class="snippet _idGenParaOverride-1">self.tuples=generate_winning_tuples()</p>
</li>
<li value="5">Let's handle the moves. The possible moves function is a simple enumeration. Notice we are using column indices from 1 to 7 in the move names, because it is more convenient to start column indexing with 1 in the human player interface than with zero. For each column, we check if there is an unoccupied field. If there is one, we will make the column a possible move.<p class="snippet _idGenParaOverride-1">def possible_moves(self):</p>
<p class="snippet _idGenParaOverride-1">        return [column+1</p>
<p class="snippet _idGenParaOverride-1">                for column in range(7)</p>
<p class="snippet _idGenParaOverride-1">                if any([</p>
<p class="snippet _idGenParaOverride-1">                    self.board[column+row*7] == 0</p>
<p class="snippet _idGenParaOverride-1">                    for row in range(6)</p>
<p class="snippet _idGenParaOverride-1">                ])</p>
<p class="snippet _idGenParaOverride-3">                ]</p>
<div></div>
</li>
<li value="6">Making a move is similar to the possible moves function. We check the column of the move, and find the first empty cell starting from the bottom. Once we find it, we occupy it. You can also read the implementation of the dual of the make_move function: unmake_move. In the unmake_move function, we check the column from top to down, and we remove the move at the first non-empty cell. Notice we rely on the internal representation of easyAi so that it does not undo moves that it had not made. Otherwise, this function would remove a token of the other player without checking whose token got removed.<p class="snippet _idGenParaOverride-1">def make_move(self, move):</p>
<p class="snippet _idGenParaOverride-1">        column = int(move) - 1</p>
<p class="snippet _idGenParaOverride-1">        for row in range(5, -1, -1):</p>
<p class="snippet _idGenParaOverride-1">            index = column + row*7</p>
<p class="snippet _idGenParaOverride-1">            if self.board[index] == 0:</p>
<p class="snippet _idGenParaOverride-1">                self.board[index] = self.nplayer</p>
<p class="snippet _idGenParaOverride-1">                return</p>
<p class="snippet _idGenParaOverride-1">    def unmake_move(self, move):</p>
<p class="snippet _idGenParaOverride-1"># optional method (speeds up the AI)</p>
<p class="snippet _idGenParaOverride-1">        column = int(move) - 1</p>
<p class="snippet _idGenParaOverride-1">        for row in range(6):</p>
<p class="snippet _idGenParaOverride-1">            index = column + row*7</p>
<p class="snippet _idGenParaOverride-1">            if self.board[index] != 0:</p>
<p class="snippet _idGenParaOverride-1">                self.board[index] = 0</p>
<p class="snippet _idGenParaOverride-1">                return</p>
</li>
<li value="7">As we already have the tuples that we have to check, we can mostly reuse the lose function from the tic-tac-toe example.<p class="snippet _idGenParaOverride-1">def lose(self):</p>
<p class="snippet _idGenParaOverride-1">        return any([all([(self.board[c] == self.nopponent)</p>
<p class="snippet _idGenParaOverride-1">                         for c in line])</p>
<p class="snippet _idGenParaOverride-1">                    for line in self.tuples])</p>
<p class="snippet _idGenParaOverride-1">    def is_over(self):</p>
<p class="snippet _idGenParaOverride-3">        return (self.possible_moves() == []) or self.lose()</p>
<div></div>
</li>
<li value="8">Our last task is the show method that prints the board. We will reuse the tic-tac-toe implementation, and just change the variables.<p class="snippet _idGenParaOverride-1">def show(self):</p>
<p class="snippet _idGenParaOverride-1">        print('\n'+'\n'.join([</p>
<p class="snippet _idGenParaOverride-1">            ' '.join([['.', 'O', 'X'][self.board[7*row+column]]</p>
<p class="snippet _idGenParaOverride-1">                     for column in range(7)]</p>
<p class="snippet _idGenParaOverride-1">                     )</p>
<p class="snippet _idGenParaOverride-1">            for row in range(6)])</p>
<p class="snippet _idGenParaOverride-1">        )</p>
</li>
</ol>
<p>Now that all functions are complete, you can try out the example. Feel free to play a round or two against the opponent. You can see that the opponent is not perfect, but it plays reasonably well. If you have a strong computer, you can increase the parameter of the Negamax algorithm. I encourage you to come up with a better heuristic.</p>
<h3 id="_idParaDest-193"><a id="_idTextAnchor207"></a>
 Chapter 3: Regression</h3>
<h3 id="_idParaDest-194"><a id="_idTextAnchor208"></a>
 Activity 5: Predicting Population</h3>
<p>You are working at the government office of Metropolis, trying to forecast the need for elementary school capacity. Your task is to figure out a 2025 and 2030 prediction for the number of children starting elementary school. Past data are as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer115" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00073.jpg" alt="" />
</div>
</div>
<p><a id="_idTextAnchor209"></a>
</p>
<h6 class="_idGenParaOverride-2">Figure 3.21 Data of Elementary School</h6>
<div></div>
<p>Plot tendencies on a two-dimensional chart. Use linear regression.</p>
<p>Our features are the years ranging from 2001 to 2018. For simplicity, we can indicate 2001 as year 1, and 2018 as year 18.</p>
<p class="snippet">x = np.array(range(1, 19))</p>
<p class="snippet">y = np.array([</p>
<p class="snippet">    147026,</p>
<p class="snippet">    144272,</p>
<p class="snippet">    140020,</p>
<p class="snippet">    143801,</p>
<p class="snippet">    146233,</p>
<p class="snippet">    144539,</p>
<p class="snippet">    141273,</p>
<p class="snippet">    135389,</p>
<p class="snippet">    142500,</p>
<p class="snippet">    139452,</p>
<p class="snippet">    139722,</p>
<p class="snippet">    135300,</p>
<p class="snippet">    137289,</p>
<p class="snippet">    136511,</p>
<p class="snippet">    132884,</p>
<p class="snippet">    125683,</p>
<p class="snippet">    127255,</p>
<p class="snippet">    124275</p>
<p class="snippet">])</p>
<p>Use np.polyfit to determine the coefficients of the regression line.</p>
<p class="snippet">[a, b] = np.polyfit(x, y, 1)</p>
<p class="snippet">[-1142.0557275541753, 148817.5294117646]</p>
<p>Plot the results using matplotlib.pyplot to determine future tendencies.</p>
<p class="snippet">import matplotlib.pyplot as plot</p>
<p class="snippet">plot.scatter( x, y )</p>
<p class="snippet">plot.plot( [0, 30], [b, 30*a+b] )</p>
<p class="snippet _idGenParaOverride-2">plot.show()</p>
<div></div>
<h3 id="_idParaDest-195"><a id="_idTextAnchor210"></a>
 Activity 6: Stock Price Prediction with Quadratic and Cubic Linear Polynomial Regression with Multiple Variables</h3>
<p>This section will discuss how to perform linear, polynomial, and support vector regression with scikit-learn. We will also learn to predict the best fit model for a given task. We will be assuming that you are a software engineer at a financial institution and your employer wants to know whether linear regression, or support vector regression is a better fit for predicting stock prices. You will have to load all data of the S&amp;P 500 from a data source. Then build a regressor using linear regression, cubic polynomial linear regression, and a support vector regression with a polynomial kernel of degree 3. Then separate training and test data. Plot the test labels and the prediction results and compare them with the <strong class="inline _idGenCharOverride-1">y</strong>
 =<strong class="inline _idGenCharOverride-1">x</strong>
 line. And finally, compare how well the three models score.</p>
<p>Let's load the S&amp;P 500 index data using <strong class="inline _idGenCharOverride-1">Quandl</strong>
 , then prepare the data for prediction. You can read the process in the Predicting the Future section of the topic Linear Regression with Multiple Variables.</p>
<p class="snippet">import quandl</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">from sklearn import preprocessing</p>
<p class="snippet">from sklearn import model_selection</p>
<p class="snippet">from sklearn import linear_model</p>
<p class="snippet">from sklearn.preprocessing import PolynomialFeatures</p>
<p class="snippet">from matplotlib import pyplot as plot</p>
<p class="snippet">from sklearn import svm</p>
<p class="snippet">data_frame = quandl.get("YALE/SPCOMP")</p>
<p class="snippet">data_frame[['Long Interest Rate', 'Real Price',</p>
<p class="snippet">           'Real Dividend', 'Cyclically Adjusted PE Ratio']]</p>
<p class="snippet">data_frame.fillna(-100, inplace=True)</p>
<p class="snippet"># We shift the price data to be predicted 20 years forward</p>
<p class="snippet">data_frame['Real Price Label'] = data_frame['RealPrice'].shift(-240)</p>
<p class="snippet"># Then exclude the label column from the features</p>
<p class="snippet">features = np.array(data_frame.drop('Real Price Label', 1))</p>
<p class="snippet"># We scale before dropping the last 240 rows from the features</p>
<p class="snippet">scaled_features = preprocessing.scale(features)</p>
<p class="snippet"># Save the last 240 rows before dropping them</p>
<p class="snippet">scaled_features_latest240 = scaled_features[-240:]</p>
<p class="snippet"># Exclude the last 240 rows from the data used for # # modelbuilding</p>
<p class="snippet">scaled_features = scaled_features[:-240]</p>
<p class="snippet"># Now we can drop the last 240 rows from the data frame</p>
<p class="snippet">data_frame.dropna(inplace=True)</p>
<p class="snippet"># Then build the labels from the remaining data</p>
<p class="snippet">label = np.array(data_frame['Real Price Label'])</p>
<p class="snippet"># The rest of the model building stays</p>
<p class="snippet">(features_train,</p>
<p class="snippet">    features_test,</p>
<p class="snippet">    label_train,</p>
<p class="snippet">    label_test</p>
<p class="snippet">) = model_selection.train_test_split(</p>
<p class="snippet">    scaled_features,</p>
<p class="snippet">    label,</p>
<p class="snippet">   test_size=0.1</p>
<p class="snippet _idGenParaOverride-2">)</p>
<div></div>
<p>Let's first use a polynomial of degree 1 for the evaluation of the model and for the prediction. We are still recreating the main example from the second topic.</p>
<p class="snippet">model = linear_model.LinearRegression()</p>
<p class="snippet">model.fit(features_train, label_train)</p>
<p class="snippet">model.score(features_test, label_test)</p>
<ol>
<li class="ParaOverride-1" value="1">The output is as follows:<p class="snippet _idGenParaOverride-1"> 0.8978136465083912</p>
</li>
<li value="2">The output always depends on the test data, so the values may differ after each run.<p class="snippet _idGenParaOverride-1">label_predicted = model.predict(features_test)</p>
<p class="snippet _idGenParaOverride-1">plot.plot(</p>
<p class="snippet _idGenParaOverride-1">    label_test, label_predicted, 'o',</p>
<p class="snippet _idGenParaOverride-1">    [0, 3000], [0, 3000]</p>
<p class="snippet _idGenParaOverride-1">)</p>
</li>
</ol>
<div class="_idGenObjectLayout-1">
<div id="_idContainer116" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00074.jpg" alt="" />
</div>
</div>
<h6 class="_idGenParaOverride-2">Fig 3.22: Graph showing the output</h6>
<div></div>
<p>The closer the dots are to the y=x line, the less error the model works with.</p>
<p class="snippet">It is now time to perform a linear multiple regression with quadratic polynomials. The only change is in the Linear Regression model</p>
<p class="snippet">poly_regressor = PolynomialFeatures(degree=3)</p>
<p class="snippet">poly_scaled_features = poly_regressor.fit_transform(scaled_features)</p>
<p class="snippet">(poly_features_train,</p>
<p class="snippet"> poly_features_test,</p>
<p class="snippet"> poly_label_train,</p>
<p class="snippet"> poly_label_test) = model_selection.train_test_split(</p>
<p class="snippet">    poly_scaled_features,</p>
<p class="snippet">    label,</p>
<p class="snippet">    test_size=0.1)</p>
<p class="snippet">model = linear_model.LinearRegression()</p>
<p class="snippet">model.fit(poly_features_train, poly_label_train)</p>
<p class="snippet">print('Polynomial model score: ', model.score(</p>
<p class="snippet">    poly_features_test, poly_label_test))</p>
<p class="snippet">print('\n')</p>
<p class="snippet">poly_label_predicted = model.predict(poly_features_test)</p>
<p class="snippet">plot.plot(</p>
<p class="snippet">    poly_label_test, poly_label_predicted, 'o',</p>
<p class="snippet">    [0, 3000], [0, 3000]</p>
<p class="snippet">)</p>
<p class="_idGenParaOverride-2">The model is performing surprisingly well on test data. Therefore, we can already suspect our polynomials are overfitting for scenarios used in training and testing.</p>
<div></div>
<p>We will now perform a Support Vector regression with a polynomial kernel of degree 3.</p>
<p class="snippet">model = svm.SVR(kernel='poly')</p>
<p class="snippet">model.fit(features_train, label_train)</p>
<p class="snippet">label_predicted = model.predict(features_test)</p>
<p class="snippet">plot.plot(</p>
<p class="snippet">    label_test, label_predicted, 'o',</p>
<p class="snippet">    [0,3000], [0,3000]</p>
<p class="snippet">)</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer117" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00075.jpg" alt="" />
</div>
</div>
<h6>Fig 3.23: Graph showing the output</h6>
<p class="snippet">model.score(features_test, label_test)</p>
<p>The output will be <strong class="inline _idGenCharOverride-1">0.06388628722032952</strong>
 .</p>
<p>We will now perform a Support Vector regression with a polynomial kernel of degree 3.</p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor211"></a>
 Chapter 4: Classification</h2>
<h3 id="_idParaDest-197"><a id="_idTextAnchor212"></a>
 Activity 7: Preparing Credit Data for Classification</h3>
<p>This section will discuss how to prepare data for a classifier. We will be using german.data from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/">https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</a>
 , as an example and prepare the data for training and testing a classifier. Make sure all your labels are numeric, and the values are prepared for classification. Use 80% of the data points as training data.</p>
<ol>
<li class="ParaOverride-1" value="1">Save german.data from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/">https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</a>
 , and open it in a text editor like Sublime Text or Atom. Add the following first row to it:<p class="snippet _idGenParaOverride-1">CheckingAccountStatus DurationMonths CreditHistory CreditPurpose CreditAmount SavingsAccount EmploymentSince DisposableIncomePercent PersonalStatusSex OtherDebtors PresentResidenceMonths Property Age OtherInstallmentPlans Housing NumberOfExistingCreditsInBank Job LiabilityNumberOfPeople Phone ForeignWorker CreditScore</p>
</li>
<li value="2">Import the data file using pandas and replace NA values with an outlier value:<p class="snippet _idGenParaOverride-1">import pandas</p>
<p class="snippet _idGenParaOverride-1">data_frame = pandas.read_csv('german.data', sep=' ')</p>
<p class="snippet _idGenParaOverride-1">data_frame.replace('NA', -1000000, inplace=True)</p>
</li>
<li value="3">Perform label encoding. We need to transform all labels in the data frame to integers. We could create all labels in a one dimensional array. However, this would be highly ineffective, because each label occurs in exactly one column. It makes a lot more sense to group our labels per column:<p class="snippet _idGenParaOverride-1">labels = {</p>
<p class="snippet _idGenParaOverride-1"> 'CheckingAccountStatus': ['A11', 'A12', 'A13', 'A14'],</p>
<p class="snippet _idGenParaOverride-1"> 'CreditHistory': ['A30', 'A31', 'A32', 'A33', 'A34'],</p>
<p class="snippet _idGenParaOverride-1"> 'CreditPurpose': ['A40', 'A41', 'A42', 'A43', 'A44', 'A45', 'A46', 'A47', 'A48', 'A49', 'A410'],</p>
<p class="snippet _idGenParaOverride-1"> 'SavingsAccount': ['A61', 'A62', 'A63', 'A64', 'A65'],</p>
<p class="snippet _idGenParaOverride-1"> 'EmploymentSince': ['A71', 'A72', 'A73', 'A74', 'A75'],</p>
<p class="snippet _idGenParaOverride-1"> 'PersonalStatusSex': ['A91', 'A92', 'A93', 'A94', 'A95'],</p>
<p class="snippet _idGenParaOverride-1"> 'OtherDebtors': ['A101', 'A102', 'A103'],</p>
<p class="snippet _idGenParaOverride-1"> 'Property': ['A121', 'A122', 'A123', 'A124'],</p>
<p class="snippet _idGenParaOverride-1"> 'OtherInstallmentPlans': ['A141', 'A142', 'A143'],</p>
<p class="snippet _idGenParaOverride-1"> 'Housing': ['A151', 'A152', 'A153'],</p>
<p class="snippet _idGenParaOverride-1"> 'Job': ['A171', 'A172', 'A173', 'A174'],</p>
<p class="snippet _idGenParaOverride-1"> 'Phone': ['A191', 'A192'],</p>
<p class="snippet _idGenParaOverride-1"> 'ForeignWorker': ['A201', 'A202']</p>
<p class="snippet _idGenParaOverride-1">}</p>
</li>
<li value="4">Let's create a label encoder for each column and encode the values:<p class="snippet _idGenParaOverride-1">from sklearn import preprocessing</p>
<p class="snippet _idGenParaOverride-1">label_encoders = {}</p>
<p class="snippet _idGenParaOverride-1">data_frame_encoded = pandas.DataFrame()</p>
<p class="snippet _idGenParaOverride-1">for column in data_frame:</p>
<p class="snippet _idGenParaOverride-1">    if column in labels:</p>
<p class="snippet _idGenParaOverride-1">        label_encoders[column] = preprocessing.LabelEncoder()</p>
<p class="snippet _idGenParaOverride-1">        label_encoders[column].fit(labels[column])</p>
<p class="snippet _idGenParaOverride-1">        data_frame_encoded[column] = label_encoders[</p>
<p class="snippet _idGenParaOverride-1">            column].transform(data_frame[column])</p>
<p class="snippet _idGenParaOverride-1">    else:</p>
<p class="snippet _idGenParaOverride-1">        data_frame_encoded[column] = data_frame[column]</p>
<p class="_idGenParaOverride-4">Let's verify that we did everything correctly:</p>
</li>
</ol>
<p class="snippet">data_frame_encoded.head()</p>
<p class="snippet">CheckingAccountStatus DurationMonths CreditHistory CreditPurpose \</p>
<p class="snippet">0                     0             6             4             4</p>
<p class="snippet">1                     1             48             2             4</p>
<p class="snippet">2                     3             12             4             7</p>
<p class="snippet">3                     0             42             2             3</p>
<p class="snippet">4                     0             24             3             0</p>
<p class="snippet">   CreditAmount SavingsAccount EmploymentSince DisposableIncomePercent \</p>
<p class="snippet">0         1169             4                4                        4</p>
<p class="snippet">1         5951             0                2                        2</p>
<p class="snippet">2         2096             0                3                        2</p>
<p class="snippet">3         7882             0                3                        2</p>
<p class="snippet">4         4870             0                2                        3</p>
<p class="snippet">   PersonalStatusSex OtherDebtors     ...     Property Age \</p>
<p class="snippet">0                 2             0     ...             0 67</p>
<p class="snippet">1                 1             0     ...             0 22</p>
<p class="snippet">2                 2             0     ...             0 49</p>
<p class="snippet">3                 2             2     ...             1 45</p>
<p class="snippet">4                 2             0     ...             3 53</p>
<p class="snippet">   OtherInstallmentPlans Housing NumberOfExistingCreditsInBank Job \</p>
<p class="snippet">0                     2        1                             2    2</p>
<p class="snippet">1                     2        1                             1    2</p>
<p class="snippet">2                     2        1                             1    1</p>
<p class="snippet">3                     2        2                             1    2</p>
<p class="snippet">4                     2        2                             2    2</p>
<p class="snippet">   LiabilityNumberOfPeople Phone ForeignWorker CreditScore</p>
<p class="snippet">0                        1     1             0            1</p>
<p class="snippet">1                        1     0             0            2</p>
<p class="snippet">2                        2     0             0            1</p>
<p class="snippet">3                        2     0             0            1</p>
<p class="snippet">4                        2     0             0            2</p>
<p class="snippet">[5 rows x 21 columns]</p>
<p class="snippet">label_encoders</p>
<p class="snippet">{'CheckingAccountStatus': LabelEncoder(),</p>
<p class="snippet"> 'CreditHistory': LabelEncoder(),</p>
<p class="snippet"> 'CreditPurpose': LabelEncoder(),</p>
<p class="snippet"> 'EmploymentSince': LabelEncoder(),</p>
<p class="snippet"> 'ForeignWorker': LabelEncoder(),</p>
<p class="snippet"> 'Housing': LabelEncoder(),</p>
<p class="snippet"> 'Job': LabelEncoder(),</p>
<p class="snippet"> 'OtherDebtors': LabelEncoder(),</p>
<p class="snippet"> 'OtherInstallmentPlans': LabelEncoder(),</p>
<p class="snippet"> 'PersonalStatusSex': LabelEncoder(),</p>
<p class="snippet"> 'Phone': LabelEncoder(),</p>
<p class="snippet"> 'Property': LabelEncoder(),</p>
<p class="snippet"> 'SavingsAccount': LabelEncoder()}</p>
<p>All the 21 columns are available, and the label encoders have been saved in an object too. Our data are now pre-processed.</p>
<p>You don't need to save these label encoders if you don't wish to decode the encoded values. We just saved them for the sake of completeness.</p>
<ol>
<li class="ParaOverride-1" value="1">It is time to separate features from labels. We can apply the same method as the one we saw in the theory section:<p class="snippet _idGenParaOverride-1">import numpy as np</p>
<p class="snippet _idGenParaOverride-1">features = np.array(</p>
<p class="snippet _idGenParaOverride-1">    data_frame_encoded.drop(['CreditScore'], 1)</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="snippet _idGenParaOverride-1">label = np.array(data_frame_encoded['CreditScore'])</p>
<p class="_idGenParaOverride-4">Our features are not yet scaled. This is a problem, because the credit amount distances can be significantly higher than the differences in age for instance.</p>
<p class="_idGenParaOverride-4">We must perform scaling of the training and testing data together, therefore, the latest step when we can still perform scaling is before we split training data from testing data.</p>
</li>
<li value="2">Let's use a Min-Max scaler from scikit's Preprocessing library:<p class="snippet _idGenParaOverride-1">scaled_features = preprocessing.MinMaxScaler(</p>
<p class="snippet _idGenParaOverride-1">feature_range=(0,1)).fit_transform(features)</p>
</li>
<li value="3">The final step is cross-validation. We will shuffle our data, and use 80% of all data for training, 20% for testing.<p class="snippet _idGenParaOverride-1">from sklearn import model_selection</p>
<p class="snippet _idGenParaOverride-1">features_train, features_test, label_train,</p>
<p class="snippet _idGenParaOverride-1">label_test = model_selection.train_test_split(</p>
<p class="snippet _idGenParaOverride-1">    scaled_features,</p>
<p class="snippet _idGenParaOverride-1">    label,</p>
<p class="snippet _idGenParaOverride-1">    test_size = 0.2</p>
<p class="snippet _idGenParaOverride-3">)</p>
<div></div>
</li>
</ol>
<h3 id="_idParaDest-198"><a id="_idTextAnchor213"></a>
 Activity 8: Increase the accuracy of credit scoring</h3>
<p>This section will learn how the parametrization of the k-nearest neighbor classifier affects the end result. The accuracy of credit scoring is currently quite low: 66.5%. Find a way to increase it by a few percentage points. And to ensure that it happens correctly, you will need to do the previous exercises.</p>
<p>There are many ways to accomplish this exercise. In this solution, I will show you one way to increase the credit score by changing the parametrization.</p>
<p>You must have completed Exercise 13, to be able to complete this activity.</p>
<ol>
<li class="ParaOverride-1" value="1">Increase the K-value of the k-nearest neighbor classifier from the default 5 to 10, 15, 25, and 50. Evaluate the results:<p class="snippet _idGenParaOverride-1">You must have completed Exercise 13, to be able to complete this activity</p>
<p class="snippet _idGenParaOverride-1">classifier = neighbors.KNeighborsClassifier(n_neighbors=10)</p>
<p class="snippet _idGenParaOverride-1">classifier.fit(</p>
<p class="snippet _idGenParaOverride-1">    features_train,label_train</p>
<p class="snippet _idGenParaOverride-1">    )</p>
<p class="snippet _idGenParaOverride-1">classifier.score(features_test, label_test)</p>
</li>
<li value="2">After running these lines for all four <strong class="inline _idGenCharOverride-1">n_neighbors</strong>
 values, I got the following results:<p class="snippet _idGenParaOverride-1">K=10: accuracy is 71.5%</p>
<p class="snippet _idGenParaOverride-1">K=15: accuracy is 70.5%</p>
<p class="snippet _idGenParaOverride-1">K=25: accuracy is 72%</p>
<p class="snippet _idGenParaOverride-1">K=50: accuracy is 74%</p>
</li>
<li value="3">Higher K values do not necessarily mean better score. In this example though, <strong class="inline _idGenCharOverride-1">K=50</strong>
 yielded a better result than <strong class="inline _idGenCharOverride-1">K=5</strong>
 .</li>
</ol>
<h3 id="_idParaDest-199"><a id="_idTextAnchor214"></a>
 Activity 9: Support Vector Machine Optimization in scikit-learn</h3>
<p class="_idGenParaOverride-2">This section will discuss how to use the different parameters of a Support Vector Machine classifier. We will be using comparing and contrasting the different support vector regression classifier parameters you learned and find a set of parameters resulting in the highest classification data on the training and testing data loaded and prepared in previous activity. And to ensure that it happens correctly, you will need to have completed the previous activities and exercises.</p>
<div></div>
<p>We will try out a few combinations. You may choose different parameters, that</p>
<ol>
<li class="ParaOverride-1" value="1">Linear kernel<p class="snippet _idGenParaOverride-1">classifier = svm.SVC(kernel="linear")</p>
<p class="snippet _idGenParaOverride-1">classifier.fit(features_train, label_train)</p>
<p class="snippet _idGenParaOverride-1">classifier.score(features_test, label_test)</p>
</li>
<li value="2">Polynomial kernel of degree 4, C=2, gamma=0.05<p class="snippet _idGenParaOverride-1">classifier = svm.SVC(kernel="poly", C=2, degree=4, gamma=0.05)</p>
<p class="snippet _idGenParaOverride-1">classifier.fit(features_train, label_train)</p>
<p class="snippet _idGenParaOverride-1">classifier.score(features_test, label_test)</p>
<p class="_idGenParaOverride-4">The output is as follows: 0.705.</p>
</li>
<li value="3">Polynomial kernel of degree 4, C=2, gamma=0.25<p class="snippet _idGenParaOverride-1">classifier = svm.SVC(kernel="poly", C=2, degree=4, gamma=0.25)</p>
<p class="snippet _idGenParaOverride-1">classifier.fit(features_train, label_train)</p>
<p class="snippet _idGenParaOverride-1">classifier.score(features_test, label_test)</p>
<p class="_idGenParaOverride-4">The output is as follows: 0.76.</p>
</li>
<li value="4">Polynomial kernel of degree 4, C=2, gamma=0.5<p class="snippet _idGenParaOverride-1">classifier = svm.SVC(kernel="poly", C=2, degree=4, gamma=0.5)</p>
<p class="snippet _idGenParaOverride-1">classifier.fit(features_train, label_train)</p>
<p class="snippet _idGenParaOverride-1">classifier.score(features_test, label_test)</p>
<p class="_idGenParaOverride-4">The output is as follows: 0.72.</p>
</li>
<li value="5">Sigmoid kernel<p class="snippet _idGenParaOverride-1">classifier = svm.SVC(kernel="sigmoid")</p>
<p class="snippet _idGenParaOverride-1">classifier.fit(features_train, label_train)</p>
<p class="snippet _idGenParaOverride-1">classifier.score(features_test, label_test)</p>
<p class="_idGenParaOverride-4">The output is as follows: 0.71.</p>
</li>
<li value="6">Default kernel with a gamma of 0.15<p class="snippet _idGenParaOverride-1">classifier = svm.SVC(kernel="rbf", gamma=0.15)</p>
<p class="snippet _idGenParaOverride-1">classifier.fit(features_train, label_train)</p>
<p class="snippet _idGenParaOverride-1">classifier.score(features_test, label_test)</p>
<p class="_idGenParaOverride-5">The output is as follows: 0.76.</p>
<div></div>
</li>
</ol>
<h2 id="_idParaDest-200"><a id="_idTextAnchor215"></a>
 Chapter 5: Using Trees for Predictive Analysis</h2>
<h3 id="_idParaDest-201"><a id="_idTextAnchor216"></a>
 Activity 10: Car Data Classification</h3>
<p>This section will discuss how to build a reliable decision tree model capable of aiding your company in finding cars clients are likely to buy. We will be assuming that you are employed by a car rental agency focusing on building a lasting relationship with its clients. Your task is to build a decision tree model classifying cars into one of four categories: unacceptable, acceptable, good, very good.</p>
<p>The data set can be accessed here: <a href="https://archive.ics.uci.edu/ml/datasets/Car+Evaluation">https://archive.ics.uci.edu/ml/datasets/Car+Evaluation</a>
 . Click the Data Folder link to download the data set. Click the Data Set Description link to access the description of the attributes.</p>
<p>Evaluate the utility of your decision tree model.</p>
<ol>
<li class="ParaOverride-1" value="1">Download the car data file from here: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data">https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data</a>
 . Add a header line to the front of the CSV file to reference it in Python more easily:<p class="_idGenParaOverride-4">
<strong class="inline _idGenCharOverride-1">Buying,Maintenance,Doors,Persons,LuggageBoot,Safety,Class</strong>
</p>
<p class="_idGenParaOverride-4">We simply call the label Class. We named the six features after their descriptions in <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names">https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names</a>
 .</p>
</li>
<li value="2">Load the data set into Python<p class="snippet _idGenParaOverride-1">import pandas</p>
<p class="snippet _idGenParaOverride-1">data_frame = pandas.read_csv('car.data')</p>
<p class="_idGenParaOverride-4">Let's check if the data got loaded correctly:</p>
<p class="snippet _idGenParaOverride-1">data_frame.head()</p>
<p class="snippet _idGenParaOverride-1">Buying Maintenance Doors Persons LuggageBoot Safety Class</p>
<p class="snippet _idGenParaOverride-1">0 vhigh     vhigh     2     2     small    low unacc</p>
<p class="snippet _idGenParaOverride-1">1 vhigh     vhigh     2     2     small    med unacc</p>
<p class="snippet _idGenParaOverride-1">2 vhigh     vhigh     2     2     small high unacc</p>
<p class="snippet _idGenParaOverride-1">3 vhigh     vhigh     2     2         med    low unacc</p>
<p class="snippet _idGenParaOverride-3">4 vhigh     vhigh     2     2         med    med unacc</p>
<div></div>
</li>
<li value="3">As classification works with numeric data, we have to perform label encoding as seen in previous chapter.<p class="snippet _idGenParaOverride-1">labels = {</p>
<p class="snippet _idGenParaOverride-1">    'Buying': ['vhigh', 'high', 'med', 'low'],</p>
<p class="snippet _idGenParaOverride-1">    'Maintenance': ['vhigh', 'high', 'med', 'low'],</p>
<p class="snippet _idGenParaOverride-1">    'Doors': ['2', '3', '4', '5more'],</p>
<p class="snippet _idGenParaOverride-1">    'Persons': ['2', '4', 'more'],</p>
<p class="snippet _idGenParaOverride-1">    'LuggageBoot': ['small', 'med', 'big'],</p>
<p class="snippet _idGenParaOverride-1">    'Safety': ['low', 'med', 'high'],</p>
<p class="snippet _idGenParaOverride-1">    'Class': ['unacc', 'acc', 'good', 'vgood']</p>
<p class="snippet _idGenParaOverride-1">}</p>
<p class="snippet _idGenParaOverride-1">from sklearn import preprocessing</p>
<p class="snippet _idGenParaOverride-1">label_encoders = {}</p>
<p class="snippet _idGenParaOverride-1">data_frame_encoded = pandas.DataFrame()</p>
<p class="snippet _idGenParaOverride-1">for column in data_frame:</p>
<p class="snippet _idGenParaOverride-1">    if column in labels:</p>
<p class="snippet _idGenParaOverride-1">        label_encoders[column] = preprocessing.LabelEncoder()</p>
<p class="snippet _idGenParaOverride-1">        label_encoders[column].fit(labels[column])</p>
<p class="snippet _idGenParaOverride-1">        data_frame_encoded[column] = label_encoders[column].transform(data_frame[column])</p>
<p class="snippet _idGenParaOverride-1">    else:</p>
<p class="snippet _idGenParaOverride-1">data_frame_encoded[column] = data_frame[column]</p>
</li>
<li value="4">Let's separate features from labels:<p class="snippet _idGenParaOverride-1">import numpy as np</p>
<p class="snippet _idGenParaOverride-1">features = np.array(data_frame_encoded.drop(['Class'], 1))</p>
<p class="snippet _idGenParaOverride-1">label = np.array( data_frame_encoded['Class'] )</p>
</li>
<li value="5">It is time to separate training and testing data with the cross-validation (in newer versions model-selection) featue of scikit-learn. We will use 10% test data:<p class="snippet _idGenParaOverride-1">from sklearn import model_selection</p>
<p class="snippet _idGenParaOverride-1">features_train, features_test, label_train, label_test = model_selection.train_test_split(</p>
<p class="snippet _idGenParaOverride-1">    features,</p>
<p class="snippet _idGenParaOverride-1">    label,</p>
<p class="snippet _idGenParaOverride-1">    test_size=0.1</p>
<p class="snippet _idGenParaOverride-3">)</p>
<div></div>
<p class="_idGenParaOverride-4">Note that the train_test_split method will be available in model_selection module, not in the cross_validation module starting in scikit-learn 0.20. In previous versions, model_selection already contains the train_test_split method.</p>
</li>
<li value="6">We have everything to build the decision tree classifier:<p class="snippet _idGenParaOverride-1">from sklearn.tree import DecisionTreeClassifier</p>
<p class="snippet _idGenParaOverride-1">decision_tree = DecisionTreeClassifier()</p>
<p class="snippet _idGenParaOverride-1">decision_tree.fit(features_train, label_train)</p>
<p class="_idGenParaOverride-4">The output of the fit method is as follows:</p>
<p class="snippet _idGenParaOverride-1">DecisionTreeClassifier(</p>
<p class="snippet _idGenParaOverride-1">    class_weight=None,</p>
<p class="snippet _idGenParaOverride-1">    criterion='gini',</p>
<p class="snippet _idGenParaOverride-1">    max_depth=None,</p>
<p class="snippet _idGenParaOverride-1">    max_features=None,</p>
<p class="snippet _idGenParaOverride-1">    max_leaf_nodes=None,</p>
<p class="snippet _idGenParaOverride-1">    min_impurity_decrease=0.0,</p>
<p class="snippet _idGenParaOverride-1">    min_impurity_split=None,</p>
<p class="snippet _idGenParaOverride-1">    min_samples_leaf=1,</p>
<p class="snippet _idGenParaOverride-1">    min_samples_split=2,</p>
<p class="snippet _idGenParaOverride-1">    min_weight_fraction_leaf=0.0,</p>
<p class="snippet _idGenParaOverride-1">    presort=False,</p>
<p class="snippet _idGenParaOverride-1">    random_state=None,</p>
<p class="snippet _idGenParaOverride-1">    splitter='best'</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="_idGenParaOverride-4">You can see the parametrization of the decision tree classifier. There are quite a few options we could set to tweak the performance of the classifier model.</p>
</li>
<li value="7">Let's score our model based on the test data:<p class="snippet _idGenParaOverride-1">decision_tree.score( features_test, label_test )</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-3"> 0.9884393063583815</p>
<div></div>
</li>
<li value="8">This is the point where your knowledge up until chapter 4 would take you on model evaluation. We will now go a bit further and create a deeper evaluation of the model based on the classification_report feature we learned in this topic:<p class="snippet _idGenParaOverride-1">from sklearn.metrics import classification_report</p>
<p class="snippet _idGenParaOverride-1">print(</p>
<p class="snippet _idGenParaOverride-1">    classification_report(</p>
<p class="snippet _idGenParaOverride-1">        label_test,</p>
<p class="snippet _idGenParaOverride-1">        decision_tree.predict(features_test)</p>
<p class="snippet _idGenParaOverride-1">    )</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1">             precision    recall f1-score support</p>
<p class="snippet _idGenParaOverride-1">         0     0.97     0.97     0.97        36</p>
<p class="snippet _idGenParaOverride-1">         1     1.00     1.00     1.00         5</p>
<p class="snippet _idGenParaOverride-1">         2     1.00     0.99     1.00     127</p>
<p class="snippet _idGenParaOverride-1">         3     0.83     1.00     0.91         5</p>
<p class="snippet _idGenParaOverride-1">avg / total     0.99     0.99     0.99     173</p>
<p class="_idGenParaOverride-4">The model has been proven to be quite accurate. In case of such a high accuracy score, suspect the possibility of overfitting.</p>
</li>
</ol>
<h3 id="_idParaDest-202"><a id="_idTextAnchor217"></a>
 Activity 11: Random Forest Classification for your Car Rental Company</h3>
<ol>
<li class="ParaOverride-1" value="1">This section will optimize your classifier to satisfy your clients better when selecting future cars for your car fleet. We will be performing random forest and extreme random forest classification on your car dealership data set you worked on in Activity 1 of this chapter. Suggest further improvements to the model to improve the performance of the classifier.<p class="_idGenParaOverride-4">We can reuse Steps 1  5 of Activity 1. The end of Step 5 looks as follows:</p>
<p class="snippet _idGenParaOverride-1">from sklearn import model_selection</p>
<p class="snippet _idGenParaOverride-1">features_train, features_test, label_train, label_test = model_selection.train_test_split(</p>
<p class="snippet _idGenParaOverride-1">    features,</p>
<p class="snippet _idGenParaOverride-1">    label,</p>
<p class="snippet _idGenParaOverride-1">    test_size=0.1</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="_idGenParaOverride-5">If you are using IPython, your variables may already be accessible in your console.</p>
<div></div>
</li>
<li value="2">Let's create a Random Forest and an Extremely Randomized Trees classifier and train the models.<p class="snippet _idGenParaOverride-1">from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier</p>
<p class="snippet _idGenParaOverride-1">random_forest_classifier = RandomForestClassifier(n_estimators=100, max_depth=6)</p>
<p class="snippet _idGenParaOverride-1">random_forest_classifier.fit(features_train, label_train)</p>
<p class="snippet _idGenParaOverride-1">extra_trees_classifier =ExtraTreesClassifier(</p>
<p class="snippet _idGenParaOverride-1">    n_estimators=100, max_depth=6</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="snippet _idGenParaOverride-1">extra_trees_classifier.fit(features_train, label_train)</p>
</li>
<li value="3">Let's estimate how well the two models perform on the test data:<p class="snippet _idGenParaOverride-1">from sklearn.metrics import classification_report</p>
<p class="snippet _idGenParaOverride-1">print(</p>
<p class="snippet _idGenParaOverride-1">    classification_report(</p>
<p class="snippet _idGenParaOverride-1">        label_test,</p>
<p class="snippet _idGenParaOverride-1">        random_forest_classifier.predict(features_test)</p>
<p class="snippet _idGenParaOverride-1">    )</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="_idGenParaOverride-4">The output for model 1 is as follows:</p>
<p class="snippet _idGenParaOverride-1">                 precision    recall f1-score support</p>
<p class="snippet _idGenParaOverride-1">         0     0.78     0.78     0.78        36</p>
<p class="snippet _idGenParaOverride-1">         1     0.00     0.00     0.00         5</p>
<p class="snippet _idGenParaOverride-1">         2     0.94     0.98     0.96     127</p>
<p class="snippet _idGenParaOverride-1">         3     0.75     0.60     0.67         5</p>
<p class="snippet _idGenParaOverride-1">avg / total     0.87     0.90     0.89     173</p>
<p class="_idGenParaOverride-4">The output for model 1 is as follows:</p>
<p class="snippet _idGenParaOverride-1">print(</p>
<p class="snippet _idGenParaOverride-1">    classification_report(</p>
<p class="snippet _idGenParaOverride-1">        label_test,</p>
<p class="snippet _idGenParaOverride-1">        extra_trees_classifier.predict(features_test)</p>
<p class="snippet _idGenParaOverride-1">    )</p>
<p class="snippet _idGenParaOverride-3">)</p>
<div></div>
<p class="snippet _idGenParaOverride-1">             precision    recall f1-score support</p>
<p class="snippet _idGenParaOverride-1">          0     0.72     0.72     0.72        36</p>
<p class="snippet _idGenParaOverride-1">          1     0.00     0.00     0.00         5</p>
<p class="snippet _idGenParaOverride-1">          2     0.93     1.00     0.96     127</p>
<p class="snippet _idGenParaOverride-1">          3     0.00     0.00     0.00         5</p>
<p class="snippet _idGenParaOverride-1">avg / total     0.83     0.88     0.86     173</p>
</li>
<li value="4">We can also calculate the accuracy scores:<p class="snippet _idGenParaOverride-1">random_forest_classifier.score(features_test, label_test)</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1"> 0.9017341040462428</p>
<p class="_idGenParaOverride-4">The output for <strong class="inline _idGenCharOverride-1">extraTreesClassifier</strong>
 is as follows:</p>
<p class="snippet _idGenParaOverride-1">extra_trees_classifier.score(features_test, label_test)</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1"> 0.884393063583815</p>
<p class="_idGenParaOverride-4">We can see that the random forest classifier is performing slightly better than the extra trees classifier.</p>
</li>
<li value="5">As a first optimization technique, let's see which features are more important and which features are less important. Due to randomization, removing the least important features may reduce the random noise in the model.<p class="snippet _idGenParaOverride-1">random_forest_classifier.feature_importances_</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1">array([0.12656512, 0.09934031, 0.02073233, 0.35550329, 0.05411809, 0.34374086])</p>
<p class="_idGenParaOverride-4">The output for <strong class="inline _idGenCharOverride-1">extra_trees_classifier</strong>
 is as follows:</p>
<p class="snippet _idGenParaOverride-1">extra_trees_classifier.feature_importances_</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-3">array([0.08699494, 0.07557066, 0.01221275, 0.38035005, 0.05879822, 0.38607338])</p>
<div></div>
<p class="_idGenParaOverride-4">Both classifiers treats the third and the fifth attributes quite unimportant. We may not be sure about the fifth attribute, as the importance score is more than 5% in both models. However, we are quite certain that the third attribute is the least significant attribute in the decision. Let's see the feature names once again.</p>
<p class="snippet _idGenParaOverride-1">data_frame_encoded.head()</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1">Buying Maintenance Doors Persons LuggageBoot Safety Class</p>
<p class="snippet _idGenParaOverride-1">0     3            3     0        0            2     1    </p>
<p class="snippet _idGenParaOverride-1">1     3            3     0        0            2     2    </p>
<p class="snippet _idGenParaOverride-1">2     3            3     0        0            2     0    </p>
<p class="snippet _idGenParaOverride-1">3     3            3     0        0            1     1    </p>
<p class="snippet _idGenParaOverride-1">4     3            3     0        0            1     2    </p>
<p class="_idGenParaOverride-4">The least important feature is Doors. It is quite evident in hindsight: the number of doors doesn't have as big of an influence in the car's rating than the safety rating for instance.</p>
</li>
<li value="6">Remove the third feature from the model and retrain the classifier.<p class="snippet _idGenParaOverride-1">features2 = np.array(data_frame_encoded.drop(['Class', 'Doors'], 1))</p>
<p class="snippet _idGenParaOverride-1">label2 = np.array(data_frame_encoded['Class'])</p>
<p class="snippet _idGenParaOverride-1">features_train2,</p>
<p class="snippet _idGenParaOverride-1">features_test2,</p>
<p class="snippet _idGenParaOverride-1">label_train2,</p>
<p class="snippet _idGenParaOverride-1">label_test2 = model_selection.train_test_split(</p>
<p class="snippet _idGenParaOverride-1">    features2,</p>
<p class="snippet _idGenParaOverride-1">    label2,</p>
<p class="snippet _idGenParaOverride-1">    test_size=0.1</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="snippet _idGenParaOverride-1">random_forest_classifier2 = RandomForestClassifier(</p>
<p class="snippet _idGenParaOverride-1">    n_estimators=100, max_depth=6</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="snippet _idGenParaOverride-1">random_forest_classifier2.fit(features_train2, label_train2)</p>
<p class="snippet _idGenParaOverride-1">extra_trees_classifier2 = ExtraTreesClassifier(</p>
<p class="snippet _idGenParaOverride-1">    n_estimators=100, max_depth=6</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="snippet _idGenParaOverride-3">extra_trees_classifier2.fit(features_train2, label_train2)</p>
<div></div>
</li>
<li value="7">Let's compare how well the new models fare compared to the original ones:<p class="snippet _idGenParaOverride-1">print(</p>
<p class="snippet _idGenParaOverride-1">    classification_report(</p>
<p class="snippet _idGenParaOverride-1">        label_test2,</p>
<p class="snippet _idGenParaOverride-1">        random_forest_classifier2.predict(features_test2)</p>
<p class="snippet _idGenParaOverride-1">    )</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1">            precision    recall f1-score support</p>
<p class="snippet _idGenParaOverride-1">         0     0.89     0.85     0.87        40</p>
<p class="snippet _idGenParaOverride-1">         1     0.00     0.00     0.00         3</p>
<p class="snippet _idGenParaOverride-1">         2     0.95     0.98     0.96     125</p>
<p class="snippet _idGenParaOverride-1">         3     1.00     1.00     1.00         5</p>
<p class="snippet _idGenParaOverride-1">avg / total     0.92     0.93     0.93     173</p>
</li>
<li value="8">Second Model:<p class="snippet _idGenParaOverride-1">print(</p>
<p class="snippet _idGenParaOverride-1">    classification_report(</p>
<p class="snippet _idGenParaOverride-1">        label_test2,</p>
<p class="snippet _idGenParaOverride-1">        extra_trees_classifier2.predict(features_test2)</p>
<p class="snippet _idGenParaOverride-1">    )</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1">            precision    recall f1-score support</p>
<p class="snippet _idGenParaOverride-1">         0     0.78     0.78     0.78        40</p>
<p class="snippet _idGenParaOverride-1">         1     0.00     0.00     0.00         3</p>
<p class="snippet _idGenParaOverride-1">         2     0.93     0.98     0.95     125</p>
<p class="snippet _idGenParaOverride-1">         3     1.00     0.40     0.57         5</p>
<p class="snippet _idGenParaOverride-3">avg / total     0.88     0.90     0.88     173</p>
<div></div>
<p class="_idGenParaOverride-4">Although we did improve a few percentage points, note that a direct comparison is not possible, because of following reasons. First, the train-test split selects different data for training and testing. A few badly selected data points may easily cause a few percentage point increase or decrease in the scores. Second, the way how we train the classifiers also has random elements. This randomization may also shift the performance of the classifiers a bit. Always use best judgement when interpreting results and measure your results multiple times on different train-test splits if needed.</p>
</li>
<li value="9">Let's tweak the parametrization of the classifiers a bit more. The following set of parameters increase the F1 Score of the Random Forest Classifier to 97%:<p class="snippet _idGenParaOverride-1">random_forest_classifier2 = RandomForestClassifier(</p>
<p class="snippet _idGenParaOverride-1">    n_estimators=150,</p>
<p class="snippet _idGenParaOverride-1">    max_ depth=8,</p>
<p class="snippet _idGenParaOverride-1">    criterion='entropy',</p>
<p class="snippet _idGenParaOverride-1">    max_features=5</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="snippet _idGenParaOverride-1">random_forest_classifier2.fit(features_train2, label_train2)</p>
<p class="snippet _idGenParaOverride-1">print(</p>
<p class="snippet _idGenParaOverride-1">    classification_report(</p>
<p class="snippet _idGenParaOverride-1">        label_test2,</p>
<p class="snippet _idGenParaOverride-1">        random_forest_classifier2.predict(features_test2)</p>
<p class="snippet _idGenParaOverride-1">    )</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1">           precision    recall f1-score support</p>
<p class="snippet _idGenParaOverride-1">          0     0.95     0.95     0.95        40</p>
<p class="snippet _idGenParaOverride-1">          1     0.50     1.00     0.67         3</p>
<p class="snippet _idGenParaOverride-1">          2     1.00     0.97     0.98     125</p>
<p class="snippet _idGenParaOverride-1">          3     0.83     1.00     0.91         5</p>
<p class="snippet _idGenParaOverride-3">avg / total     0.97     0.97     0.97     173</p>
<div></div>
</li>
<li value="10">Using the same parameters on the Extra Trees Classifier, we also get surprisingly good results:<p class="snippet _idGenParaOverride-1">extra_trees_classifier2 = ExtraTreesClassifier(</p>
<p class="snippet _idGenParaOverride-1">    n_estimators=150,</p>
<p class="snippet _idGenParaOverride-1">    max_depth=8,</p>
<p class="snippet _idGenParaOverride-1">    criterion='entropy',</p>
<p class="snippet _idGenParaOverride-1">    max_features=5</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="snippet _idGenParaOverride-1">extra_trees_classifier2.fit(features_train2, label_train2)</p>
<p class="snippet _idGenParaOverride-1">print(</p>
<p class="snippet _idGenParaOverride-1">    classification_report(</p>
<p class="snippet _idGenParaOverride-1">        label_test2,</p>
<p class="snippet _idGenParaOverride-1">        extra_trees_classifier2.predict(features_test2)</p>
<p class="snippet _idGenParaOverride-1">    )</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1">            precision    recall f1-score support</p>
<p class="snippet _idGenParaOverride-1">          0     0.92     0.88     0.90        40</p>
<p class="snippet _idGenParaOverride-1">          1     0.40     0.67     0.50         3</p>
<p class="snippet _idGenParaOverride-1">          2     0.98     0.97     0.97     125</p>
<p class="snippet _idGenParaOverride-1">          3     0.83     1.00     0.91         5</p>
<p class="snippet _idGenParaOverride-1">avg / total     0.95     0.94     0.94     173</p>
</li>
</ol>
<h2 id="_idParaDest-203"><a id="_idTextAnchor218"></a>
 Chapter 6: Clustering</h2>
<h3 id="_idParaDest-204"><a id="_idTextAnchor219"></a>
 Activity 12: k-means Clustering of Sales Data</h3>
<p>This section will detect product sales that perform similarly in nature to recognize trends in product sales.</p>
<p>We will be using the Sales Transactions Weekly Dataset from this URL:</p>
<p class="_idGenParaOverride-2">
<a href="https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly">https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly</a>
 Perform clustering on the dataset using the k-means Algorithm. Make sure you prepare your data for clustering based on what you have learned in the previous chapters.</p>
<div></div>
<p>Use the default settings for the k-means algorithm.</p>
<ol>
<li class="ParaOverride-1" value="1">Load the dataset using pandas.<p class="snippet _idGenParaOverride-1">import pandas</p>
<p class="snippet _idGenParaOverride-1">pandas.read_csv('Sales_Transactions_Dataset_Weekly.csv')</p>
</li>
<li value="2">If you examine the data in the CSV file, you can realize that the first column contains product id strings. These values just add noise to the clustering process. Also notice that for weeks 0 to 51, there is a W-prefixed label and a Normalized label. Using the normalized label makes more sense, so we can drop the regular weekly labels from the data set.<p class="snippet _idGenParaOverride-1">import numpy as np</p>
<p class="snippet _idGenParaOverride-1">drop_columns = ['Product_Code']</p>
<p class="snippet _idGenParaOverride-1">for w in range(0, 52):</p>
<p class="snippet _idGenParaOverride-1">    drop_columns.append('W' + str(w))</p>
<p class="snippet _idGenParaOverride-1">features = data_frame.drop(dropColumns, 1)</p>
</li>
<li value="3">Our data points are normalized except for the min and max<p class="snippet _idGenParaOverride-1">from sklearn.preprocessing import MinMaxScaler</p>
<p class="snippet _idGenParaOverride-1">scaler = MinMaxScaler()</p>
<p class="snippet _idGenParaOverride-1">scaled_features = scaler.fit_transform(features)</p>
</li>
<li value="4">Create a k-means clustering model and fit the data points into 8 clusters.<p class="snippet _idGenParaOverride-1">from sklearn.cluster import KMeans</p>
<p class="snippet _idGenParaOverride-1">k_means_model = KMeans()</p>
<p class="snippet _idGenParaOverride-1">k_means_model.fit(scaled_features)</p>
</li>
<li value="5">The labels belonging to each data point can be retrieved using the labels_ property. These labels determine the clustering of the rows of the original data frame.<p class="snippet _idGenParaOverride-1">k_means_model.labels_</p>
</li>
<li value="6">Retrieve the center points and the labels from the clustering algorithm:<p class="snippet _idGenParaOverride-3">k_means_model.cluster_centers_</p>
<div></div>
<p class="_idGenParaOverride-4">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-1"> array([5, 5, 4, 5, 5, 3, 4, 5, 5, 5, 5, 5, 4, 5, 0, 0, 0, 0, 0, 4, 4, 4,</p>
<p class="snippet _idGenParaOverride-1">       4, 0, 0, 5, 0, 0, 5, 0, 4, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</p>
<p class="snippet _idGenParaOverride-1">       0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 4, 0, 0, 5, 0, 0, 5, 0,</p>
<p class="snippet _idGenParaOverride-1">       ...</p>
<p class="snippet _idGenParaOverride-1">       1, 7, 3, 2, 6, 7, 6, 2, 2, 6, 2, 7, 2, 7, 2, 6, 1, 3, 2, 2, 6, 6,</p>
<p class="snippet _idGenParaOverride-1">       7, 7, 7, 1, 1, 2, 1, 2, 7, 7, 6, 2, 7, 6, 6, 6, 1, 6, 1, 6, 7, 7,</p>
<p class="snippet _idGenParaOverride-1">       1, 1, 3, 5, 3, 3, 3, 5, 7, 2, 2, 2, 3, 2, 2, 7, 7, 3, 3, 3, 3, 2,</p>
<p class="snippet _idGenParaOverride-1">       2, 6, 3, 3, 5, 3, 2, 2, 6, 7, 5, 2, 2, 2, 6, 2, 7, 6, 1])</p>
</li>
</ol>
<p>How are these labels beneficial?</p>
<p>Suppose that in the original data frame, the product names are given. You can easily recognize that similar types of products sell similarly. There are also products that fluctuate a lot, and products that are seasonal in nature. For instance, if some products promoted fat loss and getting into shape, they tend to sell during the first half of the year, before the beach season.</p>
<h3 id="_idParaDest-205"><a id="_idTextAnchor220"></a>
 Activity 13: Shape Recognition with the Mean Shift algorithm</h3>
<p>This section will learn how images can be clustered. We will be assuming that you are working for a company detecting human emotions from photos. Your task is to extract pixels making up a face in an avatar photo.</p>
<p>Create a clustering algorithm with Mean Shift to cluster pixels of images. Examine the results of the Mean Shift algorithm and check if any of the clusters contains a face when used on avatar images.</p>
<p>Then apply the k-means algorithm with a fixed default number of clusters: 8. Compare your results with the Mean Shift clustering algorithm.</p>
<ol>
<li class="ParaOverride-1" value="1">Select an image you would like to cluster and load the image.</li>
<li value="2">We chose this image from the Author's Youtube channel:<div id="_idContainer118" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00076.jpg" alt="" />
</div>
<h6 class="_idGenParaOverride-7">Fig 7.13: An image with the Author's picture</h6>
<div></div>
</li>
<li value="3">The image size has been significantly reduced so that our algorithm would terminate more quickly.<p class="snippet _idGenParaOverride-1">image = Image.open('destructuring.jpg')</p>
<p class="snippet _idGenParaOverride-1">pixels = image.load()</p>
</li>
<li value="4">Transform the pixels into a data frame to perform clustering<p class="snippet _idGenParaOverride-1">import pandas</p>
<p class="snippet _idGenParaOverride-1">data_frame = pandas.DataFrame(</p>
<p class="snippet _idGenParaOverride-1">    [[x,y,pixels[x,y][0], pixels[x,y][1], pixels[x,y][2]]</p>
<p class="snippet _idGenParaOverride-1">        for x in range(image.size[0])</p>
<p class="snippet _idGenParaOverride-1">        for y in range(image.size[1])</p>
<p class="snippet _idGenParaOverride-1">    ],</p>
<p class="snippet _idGenParaOverride-1">    columns=['x', 'y', 'r', 'g', 'b']</p>
<p class="snippet _idGenParaOverride-1">)</p>
</li>
<li value="5">Perform Mean Shift clustering on the image using scikit-learn. Note that this time we will skip normalization of the features, because proximity of the pixels and proximity of color components are represented in close to equal weight. The largest difference in pixels distance is 750, while the largest difference in a color component is 256.<p class="snippet _idGenParaOverride-1">from sklearn.cluster import MeanShift</p>
<p class="snippet _idGenParaOverride-1">mean_shift_model = MeanShift()</p>
<p class="snippet _idGenParaOverride-1">mean_shift_model.fit(data_frame)</p>
<p class="snippet _idGenParaOverride-1">for i in range(len(mean_shift_model.cluster_centers_)):</p>
<p class="snippet _idGenParaOverride-1">    image = Image.open('destructuring.jpg')</p>
<p class="snippet _idGenParaOverride-1">    pixels = image.load()</p>
<p class="snippet _idGenParaOverride-1">    for j in range(len(data_frame)):</p>
<p class="snippet _idGenParaOverride-1">        if (mean_shift_model.labels_[j] != i ):</p>
<p class="snippet _idGenParaOverride-1">           pixels[ int(data_frame['x'][j]),</p>
<p class="snippet _idGenParaOverride-1">       int(data_frame['y'][j]) ] = (255, 255, 255)</p>
<p class="snippet _idGenParaOverride-3">    image.save( 'cluster' + str(i) + '.jpg' )</p>
<div></div>
</li>
<li value="6">The algorithm found the following two clusters:<div id="_idContainer119" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00077.jpg" alt="" />
</div>
<h6 class="_idGenParaOverride-6"></h6>
<h6 class="_idGenParaOverride-6">Fig 7.14: Images after performing k-means Clustering</h6>
</li>
<li value="7">The Mean Shift algorithm treated my skin and the yellow JavaScript and Destructuring text close enough to each other to form the same cluster.</li>
<li value="8">Let's use the k-means algorithm to formulate eight clusters on the same data.<p class="snippet _idGenParaOverride-1">k_means_model = KMeans(n_clusters=8)</p>
<p class="snippet _idGenParaOverride-1">k_means_model.fit(data_frame)</p>
<p class="snippet _idGenParaOverride-1">for i in range(len(k_means_model.cluster_centers_)):</p>
<p class="snippet _idGenParaOverride-1">    image = Image.open('destructuring.jpg')</p>
<p class="snippet _idGenParaOverride-1">    pixels = image.load()</p>
<p class="snippet _idGenParaOverride-1">    for j in range(len(data_frame)):</p>
<p class="snippet _idGenParaOverride-1">        if (k_means_model.labels_[j] != i):</p>
<p class="snippet _idGenParaOverride-1">            pixels[int(data_frame['x'][j]), int(data_frame['y'][j])] = (255, 255, 255)</p>
<p class="snippet _idGenParaOverride-1">    image.save('kmeanscluster' + str(i) + '.jpg')</p>
</li>
<li value="9">The 8 clusters are the following:<p class="_idGenParaOverride-4">The output for the first is as follows:</p>
</li>
</ol>
<div class="_idGenObjectLayout-1">
<div id="_idContainer120" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00078.jpg" alt="Images after performing K-means Clustering" />
</div>
</div>
<h6 class="_idGenParaOverride-2">Fig 7.15: Images after performing k-means Clustering</h6>
<div></div>
<p>The output for the second is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer121" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00079.jpg" alt="Fig 7.16: Images after performing K-means Clustering" />
</div>
</div>
<h6>Fig 7.16: Images after performing k-means Clustering</h6>
<p>The output for the third is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer122" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00080.jpg" alt="Fig 7.17: Images after performing K-means Clustering" />
</div>
</div>
<h6>Fig 7.17: Images after performing k-means Clustering</h6>
<p>The output for the fourth is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer123" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00081.jpg" alt="Fig 7.18: Images after performing K-means Clustering" />
</div>
</div>
<h6 class="_idGenParaOverride-2">Fig 7.18: Images after performing k-means Clustering</h6>
<div></div>
<p>The output for the fifth is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer124" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00082.jpg" alt="Fig 7.19: Images after performing K-means Clustering" />
</div>
</div>
<h6>Fig 7.19: Images after performing k-means Clustering</h6>
<p>The output for the sixth is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer125" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00083.jpg" alt="Fig 7.20: Images after performing K-means Clustering" />
</div>
</div>
<h6>Fig 7.20: Images after performing k-means Clustering</h6>
<p>The output for the seventh is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer126" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00084.jpg" alt="Fig 7.21: Images after performing K-means Clustering" />
</div>
</div>
<h6 class="_idGenParaOverride-2">Fig 7.21: Images after performing k-means Clustering</h6>
<div></div>
<p>The output for the eighth is as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer127" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00085.jpg" alt="Fig 7.22: Images after performing K-means Clustering" />
</div>
</div>
<h6>Fig 7.22: Images after performing k-means Clustering</h6>
<p>As you can see, the fifth cluster recognized my face quite well. The clustering algorithm indeed located data points that are close and contain similar colors.</p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor221"></a>
 Chapter 7: Deep Learning with Neural Networks</h2>
<h3 id="_idParaDest-207"><a id="_idTextAnchor222"></a>
 Activity 14: Written digit detection</h3>
<ol>
<li class="ParaOverride-1" value="1">This section will discuss how to provide more security for the cryptocurrency traders via the detection of hand-written digits. We will be using assuming that you are a software developer at a new Cryptocurrency trader platform. The latest security measure you are implementing requires the recognition of hand-written digits. Use the MNIST library to train a neural network to recognize digits. You can read more about this dataset on <a href="https://www.tensorflow.org/tutorials/">https://www.tensorflow.org/tutorials/</a>
 .</li>
<li value="2">Improve the accuracy of the model as much as possible. And to ensure that it happens correctly, you will need to complete the previous topic.</li>
<li value="3">Load the dataset and format the input<p class="snippet _idGenParaOverride-1">import tensorflow.keras.datasets.mnist as mnist</p>
<p class="snippet _idGenParaOverride-1">(features_train, label_train),</p>
<p class="snippet _idGenParaOverride-1">(features_test, label_test) = mnist.load_data()</p>
<p class="snippet _idGenParaOverride-1">features_train = features_train / 255.0</p>
<p class="snippet _idGenParaOverride-1">features_test = features_test / 255.0</p>
<p class="snippet _idGenParaOverride-1">def flatten(matrix):</p>
<p class="snippet _idGenParaOverride-1">    return [elem for row in matrix for elem in row]</p>
<p class="snippet _idGenParaOverride-1">features_train_vector = [</p>
<p class="snippet _idGenParaOverride-1">    flatten(image) for image in features_train</p>
<p class="snippet _idGenParaOverride-1">]</p>
<p class="snippet _idGenParaOverride-1">features_test_vector = [</p>
<p class="snippet _idGenParaOverride-1">    flatten(image) for image in features_test</p>
<p class="snippet _idGenParaOverride-1">]</p>
<p class="snippet _idGenParaOverride-1">import numpy as np</p>
<p class="snippet _idGenParaOverride-1">label_train_vector = np.zeros((label_train.size, 10))</p>
<p class="snippet _idGenParaOverride-1">for i, label in enumerate(label_train_vector):</p>
<p class="snippet _idGenParaOverride-1">    label[label_train[i]] = 1</p>
<p class="snippet _idGenParaOverride-1">label_test_vector = np.zeros((label_test.size, 10))</p>
<p class="snippet _idGenParaOverride-1">for i, label in enumerate(label_test_vector):</p>
<p class="snippet _idGenParaOverride-1">    label[label_test[i]] = 1</p>
</li>
<li value="4">Set up the Tensorflow graph. Instead of the <strong class="inline _idGenCharOverride-1">sigmoid</strong>
 function, we will now use the <strong class="inline _idGenCharOverride-1">relu</strong>
 function.<p class="snippet _idGenParaOverride-1">import tensorflow as tf</p>
<p class="snippet _idGenParaOverride-1">f = tf.nn.softmax</p>
<p class="snippet _idGenParaOverride-1">x = tf.placeholder(tf.float32, [None, 28 * 28 ])</p>
<p class="snippet _idGenParaOverride-1">W = tf.Variable( tf.random_normal([784, 10]))</p>
<p class="snippet _idGenParaOverride-1">b = tf.Variable( tf.random_normal([10]))</p>
<p class="snippet _idGenParaOverride-1">y = f(tf.add(tf.matmul( x, W ), b ))</p>
</li>
<li value="5">Train the model.<p class="snippet _idGenParaOverride-1">import random</p>
<p class="snippet _idGenParaOverride-1">y_true = tf.placeholder(tf.float32, [None, 10])</p>
<p class="snippet _idGenParaOverride-1">cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(</p>
<p class="snippet _idGenParaOverride-1">    logits=y,</p>
<p class="snippet _idGenParaOverride-1">    labels=y_true</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="snippet _idGenParaOverride-1">cost = tf.reduce_mean(cross_entropy)</p>
<p class="snippet _idGenParaOverride-1">optimizer = tf.train.GradientDescentOptimizer(</p>
<p class="snippet _idGenParaOverride-1">    learning_rate = 0.5</p>
<p class="snippet _idGenParaOverride-1">).minimize(cost)</p>
<p class="snippet _idGenParaOverride-1">session = tf.Session()</p>
<p class="snippet _idGenParaOverride-1">session.run(tf.global_variables_initializer())</p>
<p class="snippet _idGenParaOverride-1">iterations = 600</p>
<p class="snippet _idGenParaOverride-1">batch_size = 200</p>
<p class="snippet _idGenParaOverride-1">sample_size = len(features_train_vector)</p>
<p class="snippet _idGenParaOverride-1">for _ in range(iterations):</p>
<p class="snippet _idGenParaOverride-1">    indices = random.sample(range(sample_size), batchSize)</p>
<p class="snippet _idGenParaOverride-1">    batch_features = [</p>
<p class="snippet _idGenParaOverride-1">        features_train_vector[i] for i in indices</p>
<p class="snippet _idGenParaOverride-1">    ]</p>
<p class="snippet _idGenParaOverride-1">    batch_labels = [</p>
<p class="snippet _idGenParaOverride-1">        label_train_vector[i] for i in indices</p>
<p class="snippet _idGenParaOverride-1">    ]</p>
<p class="snippet _idGenParaOverride-1">    min = i * batch_size</p>
<p class="snippet _idGenParaOverride-1">    max = (i+1) * batch_size</p>
<p class="snippet _idGenParaOverride-1">    dictionary = {</p>
<p class="snippet _idGenParaOverride-1">        x: batch_features,</p>
<p class="snippet _idGenParaOverride-1">        y_true: batch_labels</p>
<p class="snippet _idGenParaOverride-1">    }</p>
<p class="snippet _idGenParaOverride-1">    session.run(optimizer, feed_dict=dictionary)</p>
</li>
<li value="6">Test the model<p class="snippet _idGenParaOverride-1">label_predicted = session.run(classify( x ), feed_dict={</p>
<p class="snippet _idGenParaOverride-1">    x: features_test_vector</p>
<p class="snippet _idGenParaOverride-1">})</p>
<p class="snippet _idGenParaOverride-1">label_predicted = [</p>
<p class="snippet _idGenParaOverride-1">    np.argmax(label) for label in label_predicted</p>
<p class="snippet _idGenParaOverride-1">]</p>
<p class="snippet _idGenParaOverride-1">confusion_matrix(label_test, label_predicted)</p>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1">array([[ 0, 0, 223, 80, 29, 275, 372, 0, 0, 1],</p>
<p class="snippet _idGenParaOverride-1">   [ 0, 915, 4, 10, 1, 13, 192, 0, 0, 0],</p>
<p class="snippet _idGenParaOverride-1">   [ 0, 39, 789, 75, 63, 30, 35, 0, 1, 0],</p>
<p class="snippet _idGenParaOverride-1">   [ 0, 6, 82, 750, 13, 128, 29, 0, 0, 2],</p>
<p class="snippet _idGenParaOverride-1">   [ 0, 43, 16, 16, 793, 63, 49, 0, 2, 0],</p>
<p class="snippet _idGenParaOverride-1">   [ 0, 22, 34, 121, 40, 593, 76, 5, 0, 1],</p>
<p class="snippet _idGenParaOverride-1">   [ 0, 29, 34, 6, 44, 56, 788, 0, 0, 1],</p>
<p class="snippet _idGenParaOverride-1">   [ 1, 54, 44, 123, 715, 66, 24, 1, 0, 0],</p>
<p class="snippet _idGenParaOverride-1">   [ 0, 99, 167, 143, 80, 419, 61, 0, 4, 1],</p>
<p class="snippet _idGenParaOverride-1">   [ 0, 30, 13, 29, 637, 238, 58, 3, 1, 0]], dtype=int64)</p>
</li>
<li value="7">Calculate the accuracy score:<p class="snippet _idGenParaOverride-3">accuracy_score(label_test, label_predicted)</p>
<div></div>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1"> 0.4633</p>
</li>
<li value="8">By re-running the code segment responsible for training the data set, we can improve the accuracy:<p class="snippet _idGenParaOverride-1">for _ in range(iterations):</p>
<p class="snippet _idGenParaOverride-1">    indices = random.sample(range(sample_size), batch_size)</p>
<p class="snippet _idGenParaOverride-1">    batch_features = [</p>
<p class="snippet _idGenParaOverride-1">        features_train_vector[i] for i in indices</p>
<p class="snippet _idGenParaOverride-1">    ]</p>
<p class="snippet _idGenParaOverride-1">    batch_labels = [</p>
<p class="snippet _idGenParaOverride-1">        label_train_vector[i] for i in indices</p>
<p class="snippet _idGenParaOverride-1">    ]</p>
<p class="snippet _idGenParaOverride-1">    min = i * batch_size</p>
<p class="snippet _idGenParaOverride-1">    max = (i+1) * batch_size</p>
<p class="snippet _idGenParaOverride-1">    dictionary = {</p>
<p class="snippet _idGenParaOverride-1">        x: batch_features,</p>
<p class="snippet _idGenParaOverride-1">        y_true: batch_labels</p>
<p class="snippet _idGenParaOverride-1">    }</p>
<p class="snippet _idGenParaOverride-1">    session.run(optimizer, feed_dict=dictionary)</p>
<p class="_idGenParaOverride-4">Second run: 0.5107</p>
<p class="_idGenParaOverride-4">Third run: 0.5276</p>
<p class="_idGenParaOverride-4">Fourth run: 0.5683</p>
<p class="_idGenParaOverride-4">Fifth run: 0.6002</p>
<p class="_idGenParaOverride-4">Sixth run: 0.6803</p>
<p class="_idGenParaOverride-4">Seventh run: 0.6989</p>
<p class="_idGenParaOverride-4">Eighth run: 0.7074</p>
<p class="_idGenParaOverride-4">Ninth run: 0.713</p>
<p class="_idGenParaOverride-4">Tenth run: 0.7163</p>
<p class="_idGenParaOverride-4">Twentieth run: 0.7308</p>
<p class="_idGenParaOverride-4">Thirtieth run: 0.8188</p>
<p class="_idGenParaOverride-4">Fortieth run: 0.8256</p>
<p class="_idGenParaOverride-5">Fiftieth run: 0.8273</p>
<div></div>
<p class="_idGenParaOverride-4">At the end of the fiftieth run, the improved confusion matrix looks as follows:</p>
<p class="snippet _idGenParaOverride-1">array([</p>
<p class="snippet _idGenParaOverride-1"> [946, 0,    6,    3,    0,    1, 15,    2,    7,    0],</p>
<p class="snippet _idGenParaOverride-1"> [ 0,1097,    3,    7,    1,    0,    4,    0, 23,    0],</p>
<p class="snippet _idGenParaOverride-1"> [11, 3, 918, 11, 18,    0, 13,    8, 50,    0],</p>
<p class="snippet _idGenParaOverride-1"> [3,    0, 23, 925,    2, 10,    4,    9, 34,    0],</p>
<p class="snippet _idGenParaOverride-1"> [2,    2,    6,    1, 929,    0, 14,    2, 26,    0],</p>
<p class="snippet _idGenParaOverride-1"> [16, 4,    7, 62,    8, 673, 22,    3, 97,    0],</p>
<p class="snippet _idGenParaOverride-1"> [8,    2,    4,    3,    8,    8, 912,    2, 11,    0],</p>
<p class="snippet _idGenParaOverride-1"> [5,    9, 33,    6,    9,    1,    0, 949, 16,    0],</p>
<p class="snippet _idGenParaOverride-1"> [3,    4,    5, 12,    7,    4, 12,    3, 924,    0],</p>
<p class="snippet _idGenParaOverride-1"> [8,    5,    7, 40, 470, 11,    5, 212, 251,    0]</p>
<p class="snippet _idGenParaOverride-1">],</p>
<p class="snippet _idGenParaOverride-1">     dtype=int64)</p>
</li>
</ol>
<p>Not a bad result. More than 8 out of 10 digits are accurately recognized.</p>
<h3 id="_idParaDest-208"><a id="_idTextAnchor223"></a>
 Activity 15 : Written Digit Detection with Deep Learning</h3>
<p>This section will discuss how deep learning improves the performance of your model. We will be assuming that your boss is not satisfied with the results you presented in previous activity and asks you to consider adding two hidden layers to your original model and determine whether new layers improve the accuracy of the model. And to ensure that it happens correctly, you will need to have knowledge of Deep Learning.</p>
<ol>
<li class="ParaOverride-1" value="1">Execute the code of previous Activity and measure the accuracy of the model.</li>
<li value="2">Change the neural network by adding new layers. We will combine the <strong class="inline _idGenCharOverride-1">relu</strong>
 and <strong class="inline _idGenCharOverride-1">softmax</strong>
 activator functions:<p class="snippet _idGenParaOverride-1">x = tf.placeholder(tf.float32, [None, 28 * 28 ])</p>
<p class="snippet _idGenParaOverride-1">f1 = tf.nn.relu</p>
<p class="snippet _idGenParaOverride-1">W1 = tf.Variable(tf.random_normal([784, 200]))</p>
<p class="snippet _idGenParaOverride-1">b1 = tf.Variable(tf.random_normal([200]))</p>
<p class="snippet _idGenParaOverride-1">layer1_out = f1(tf.add(tf.matmul(x, W1), b1))</p>
<p class="snippet _idGenParaOverride-1">f2 = tf.nn.softmax</p>
<p class="snippet _idGenParaOverride-1">W2 = tf.Variable(tf.random_normal([200, 100]))</p>
<p class="snippet _idGenParaOverride-1">b2 = tf.Variable(tf.random_normal([100]))</p>
<p class="snippet _idGenParaOverride-1">layer2_out = f2(tf.add(tf.matmul(layer1_out, W2), b2))</p>
<p class="snippet _idGenParaOverride-1">f3 = tf.nn.softmax</p>
<p class="snippet _idGenParaOverride-1">W3 = tf.Variable(tf.random_normal([100, 10]))</p>
<p class="snippet _idGenParaOverride-1">b3 = tf.Variable( tf.random_normal([10]))</p>
<p class="snippet _idGenParaOverride-3">y = f3(tf.add(tf.matmul(layer2_out, W3), b3))</p>
<div></div>
</li>
<li value="3">Retrain the model<p class="snippet _idGenParaOverride-1">y_true = tf.placeholder(tf.float32, [None, 10])</p>
<p class="snippet _idGenParaOverride-1">cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(</p>
<p class="snippet _idGenParaOverride-1">    logits=y,</p>
<p class="snippet _idGenParaOverride-1">    labels=y_true</p>
<p class="snippet _idGenParaOverride-1">)</p>
<p class="snippet _idGenParaOverride-1">cost = tf.reduce_mean(cross_entropy)</p>
<p class="snippet _idGenParaOverride-1">optimizer = tf.train.GradientDescentOptimizer(</p>
<p class="snippet _idGenParaOverride-1">learning_rate=0.5).minimize(cost)</p>
<p class="snippet _idGenParaOverride-1">session = tf.Session()</p>
<p class="snippet _idGenParaOverride-1">session.run(tf.global_variables_initializer())</p>
<p class="snippet _idGenParaOverride-1">iterations = 600</p>
<p class="snippet _idGenParaOverride-1">batch_size = 200</p>
<p class="snippet _idGenParaOverride-1">sample_size = len(features_train_vector)</p>
<p class="snippet _idGenParaOverride-1">for _ in range(iterations):</p>
<p class="snippet _idGenParaOverride-1">    indices = random.sample(range(sample_size), batchSize)</p>
<p class="snippet _idGenParaOverride-1">    batch_features = [</p>
<p class="snippet _idGenParaOverride-1">        features_train_vector[i] for i in indices</p>
<p class="snippet _idGenParaOverride-1">    ]</p>
<p class="snippet _idGenParaOverride-1">    batch_labels = [</p>
<p class="snippet _idGenParaOverride-1">        label_train_vector[i] for i in indices</p>
<p class="snippet _idGenParaOverride-1">    ]</p>
<p class="snippet _idGenParaOverride-1">    min = i * batch_size</p>
<p class="snippet _idGenParaOverride-1">    max = (i+1) * batch_size</p>
<p class="snippet _idGenParaOverride-1">    dictionary = {</p>
<p class="snippet _idGenParaOverride-1">        x: batch_features,</p>
<p class="snippet _idGenParaOverride-1">        y_true: batch_labels</p>
<p class="snippet _idGenParaOverride-1">    }</p>
<p class="snippet _idGenParaOverride-1">    session.run(optimizer, feed_dict=dictionary)</p>
</li>
<li value="4">Evaluate the model<p class="snippet _idGenParaOverride-1">label_predicted = session.run(y, feed_dict={</p>
<p class="snippet _idGenParaOverride-1">    x: features_test_vector</p>
<p class="snippet _idGenParaOverride-1">})</p>
<p class="snippet _idGenParaOverride-1">label_predicted = [</p>
<p class="snippet _idGenParaOverride-1">    np.argmax(label) for label in label_predicted</p>
<p class="snippet _idGenParaOverride-1">]</p>
<p class="snippet _idGenParaOverride-3">confusion_matrix(label_test, label_predicted)</p>
<div></div>
<p class="_idGenParaOverride-4">The output is as follows:</p>
<p class="snippet _idGenParaOverride-1">array([[ 801, 11,    0, 14,    0,    0, 56,    0, 61, 37],</p>
<p class="snippet _idGenParaOverride-1">     [ 2, 1069,    0, 22,    0,    0, 18,    0,    9, 15],</p>
<p class="snippet _idGenParaOverride-1">     [ 276, 138,    0, 225,    0,    2, 233,    0, 105, 53],</p>
<p class="snippet _idGenParaOverride-1">     [ 32, 32,    0, 794,    0,    0, 57,    0, 28, 67],</p>
<p class="snippet _idGenParaOverride-1">     [ 52, 31,    0, 24,    0,    3, 301,    0, 90, 481],</p>
<p class="snippet _idGenParaOverride-1">     [ 82, 50,    0, 228,    0,    3, 165,    0, 179, 185],</p>
<p class="snippet _idGenParaOverride-1">     [ 71, 23,    0, 14,    0,    0, 712,    0, 67, 71],</p>
<p class="snippet _idGenParaOverride-1">     [ 43, 85,    0, 32,    0,    3, 31,    0, 432, 402],</p>
<p class="snippet _idGenParaOverride-1">     [ 48, 59,    0, 192,    0,    2, 45,    0, 425, 203],</p>
<p class="snippet _idGenParaOverride-1">     [ 45, 15,    0, 34,    0,    2, 39,    0, 162, 712]],</p>
<p class="snippet _idGenParaOverride-1">     dtype=int64)</p>
</li>
<li value="5">Calculating the accuracy score.<p class="snippet _idGenParaOverride-1">accuracy_score(label_test, label_predicted)</p>
</li>
</ol>
<p>The output is <strong class="inline _idGenCharOverride-1">0.4516</strong>
 .</p>
<p>The accuracy did not improve.</p>
<p>Let's see if further runs improve the accuracy of the model.</p>
<p>Second run: 0.5216</p>
<p>Third run: 0.5418</p>
<p>Fourth run: 0.5567</p>
<p>Fifth run: 0.564</p>
<p>Sixth run: 0.572</p>
<p>Seventh run: 0.5723</p>
<p>Eighth run: 0.6001</p>
<p>Ninth run: 0.6076</p>
<p>Tenth run: 0.6834</p>
<p>Twentieth run: 0.7439</p>
<p>Thirtieth run: 0.7496</p>
<p>Fortieth run: 0.7518</p>
<p>Fiftieth run: 0.7536</p>
<p class="_idGenParaOverride-2">Afterwards, we got the following results: 0.755, 0.7605, 0.7598, 0.7653</p>
<div></div>
<p>The final confusion matrix:</p>
<p class="snippet">array([[ 954,    0,    2,    1,    0,    6,    8,    0,    5,    4],</p>
<p class="snippet">     [ 0, 1092,    5,    3,    0,    0,    6,    0, 27,    2],</p>
<p class="snippet">     [ 8,    3, 941, 16,    0,    2, 13,    0, 35, 14],</p>
<p class="snippet">     [ 1,    1, 15, 953,    0, 14,    2,    0, 13, 11],</p>
<p class="snippet">     [ 4,    3,    8,    0,    0,    1, 52,    0, 28, 886],</p>
<p class="snippet">     [ 8,    1,    5, 36,    0, 777, 16,    0, 31, 18],</p>
<p class="snippet">     [ 8,    1,    6,    1,    0,    6, 924,    0,    9,    3],</p>
<p class="snippet">     [ 3, 10, 126, 80,    0,    4,    0,    0, 35, 770],</p>
<p class="snippet">     [ 4,    0,    6, 10,    0,    6,    4,    0, 926, 18],</p>
<p class="snippet">     [ 4,    5,    1,    8,    0,    2,    2,    0, 18, 969]],</p>
<p class="snippet">     dtype=int64)</p>
<p>This deep neural network behaves even more chaotically than the single layer one. It took 600 iterations of 200 samples to get from an accuracy of 0.572 to 0.5723. Not long after this iteration, we jumped from 0.6076 to 0.6834 in that number of iterations.</p>
</div>
</body></html>