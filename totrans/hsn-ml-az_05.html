<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Machine Learning Studio</h1>
                </header>
            
            <article>
                
<p><strong>Azure Machine Learning <span>S</span>tudio</strong> is an ML-as-a-Service platform for creating custom <strong>machine learning</strong> (<strong>ML</strong>) models. Azure ML Studio is a great tool for beginners who perhaps have some experience of consuming machine learning models and who would like to gain a deeper understanding of the training process. It offers more flexibility than the Cognitive Services APIs and an easy-to-learn development environment. The GUI does not require any programming and allows the user to concentrate on building ML models as efficiently as possible. Azure ML Studio is also a useful tool for more experienced AI developers who have a fairly simple problem at hand and need to get results quickly.</p>
<p>Azure ML Studio consists of two separate services: a Studio Workspace and Studio Web Services. Both of these services also include the backend computational resources needed for processing, so the user does not have to worry about the maintenance of the underlying operating system or hardware. The difference between the two services is clear: the Studio Workspace is used to train ML models and experiment with different configurations, while Studio Web Services provide a REST API interface for scoring examples, using the models published from the Workspace.</p>
<p>Azure ML Studio is designed for collaborative development. It integrates with Azure Active Directory, so users from the same organization can be added to the Workspace with a few clicks. All ML models in the ML Studio Workspace are visible to all members of the Workspace. Workspace members can also edit models created by others, so developers can try to improve each other's results iteratively. Therefore, it is a great tool when developers are following the <strong>Team Data Science Process</strong> (<strong>TDSP</strong>), for example.</p>
<p>ML models are developed inside <em>experiments</em>. An experiment contains all the steps required to produce an ML model, beginning with the input dataset. Experiments can be used to compare different ML models and parameter configurations. ML Studio provides a wide range of modules that can be added to an experiment to perform different tasks, such as preprocessing data or evaluating training results. By combining these modules, the experiment is built step by step, resulting in a training pipeline that can be run to produce a trained ML model. ML Studio also includes a wide experiment template collection, with ready-to-run examples from many different areas. In the next section, we will show how to deploy these templates to an ML Studio Workspace.</p>
<p>The pricing of Workspaces and web services is based on the use of computational resources. Workspace resources are consumed when new ML models are trained in ML Studio. Workspace billing is also based on the number of users, but using the ML Studio UI and building experiments does not incur any extra costs; only the experiment runtime is calculated.</p>
<p>When creating a new ML Studio Workspace, a new web service plan resource is created automatically. Web service resources are consumed when external applications call the ML Studio Web Services API. A web services pricing tier must be chosen when creating a web service resource in the Azure portal. The pricing tier determines the maximum amount of requests that can be handled in a month. If this limit is exceeded, each request is billed on top of the flat monthly price.</p>
<p>To use Azure ML Studio, you need a Workspace account. There are two types of Workspace accounts: <em>Free Tier</em> and <em>Standard</em> <em>T</em>ier. The Free Tier is an independent account that is not connected to an Azure subscription. It has more limitations in terms of use and does not include a production-scale web API, like the Standard Tier does. The Standard Tier requires an Azure subscription and the costs of the ML Studio resources are added to the subscription bill. The Workspace and web services appear as independent items in the resource group, and they can be managed in the Azure portal just like any other Azure resource.</p>
<p class="mce-root">To access the ML Studio UI, go to the <span class="packt_screen">ML Studio Workspace</span> blade in the Azure portal and click on <span class="packt_screen">Launch Machine Learning Studio</span>. You can also enter the portal address in the browser directly: <a href="https://studio.azureml.net/">https://studio.azureml.net/</a>.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Deploying an Azure AI Gallery template</li>
<li>Building an experiment in Azure ML Studio</li>
<li>Deploying a model as a web service in ML Studio</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying an Azure AI Gallery template</h1>
                </header>
            
            <article>
                
<p>Developing models with ML Studio does not have to be done from scratch. Azure AI Gallery contains a wide selection of templates for many different scenarios. These scenarios include many common use cases for ML, such as credit risk prediction, demand estimation, and text sentiment analysis. Templates can be imported to an Azure ML Studio Workspace with a few clicks and they contain all the steps needed to produce a working ML model. Studying templates is a great way to learn about different use cases and the steps required to produce an ML model. Some templates are prepared by Microsoft, but users can also submit their own experiments to the gallery.</p>
<p>The template gallery can be accessed directly from ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building an experiment</h1>
                </header>
            
            <article>
                
<p>In this section, we'll show how to build an experiment from scratch using a custom dataset. With the GUI, creating new experiments is very fast and results can be viewed immediately. Azure ML Studio contains modules for all common ML and data-processing tasks, so it is a great tool for testing ideas quickly and iteratively. If the built-in modules are not sufficient for the task at hand, the script modules can be used for improved extensibility, explained as follows.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Importing and preprocessing data</h1>
                </header>
            
            <article>
                
<p>As already discussed, Azure ML Studio is a complete ML tool that takes care of every step in the ML model development process. The only input needed is a raw dataset in a format understood by ML Studio; if the original data format is not recognized, then a file conversion is required, using either an external tool or the custom script modules in ML Studio. For raw files, the data formats currently recognized by ML Studio are CSV, TSV, ARFF, SvmLight, and R objects. Datasets can also be saved in zipped format to save storage space and bandwidth.</p>
<p>Datasets can be imported to ML Studio in two ways: by uploading a local file from the user's computer, or using cloud storage in Azure. To import data from your local ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Choosing and configuring algorithms</h1>
                </header>
            
            <article>
                
<p>Choosing the right models and tuning the parameters for the model are at the core of AI application development. In most cases, there are several algorithms that are applicable to the task, and it may not be clear from the beginning which algorithm will perform best. For example, some algorithms might perform better for small datasets, while others excel on big data. Usually, there are also other constraints to think about, such as runtime or the amount of computational resources available. The best model is the one that achieves a sufficient level of accuracy with the minimum amount of computational resources.</p>
<p>The first step when solving an ML problem is to identify which family of algorithms should be used. The algorithm family depends mostly on the type of the <em>predicted</em> value, such as if predicting a number, the possible algorithms are different than when predicting a <em>categorical</em> value. A categorical value is one where the number of possible outcomes is finite. The simplest categorical value is a Boolean variable, which can take two values (true/false). The number of possible outcomes can also restrict which types of algorithms can be used for the problem, since not all algorithms handle very high-dimensional data well. One example of high-dimensional categorical data is encountered in text analysis, where each word might represent a different category and the number of categories is equal to the size of the dictionary. For such high-dimensional data, it is often best to use neural network models, which can handle a large number of output values.</p>
<p>Azure ML Studio contains a selection of the most commonly used ML algorithms that can be dragged and dropped to an experiment canvas. The algorithms are listed in the <span class="packt_screen">Initialize model</span> section, under the <span class="packt_screen">Machine Learning</span> menu. The algorithms are grouped into four categories. The anomaly detection modules are meant for detecting outliers in datasets where most of the values are similar to each other, but there are some exceptions that we want to identify. These models are widely used in predictive fault detection, for example, in the manufacturing and processing industries, where machines usually operate normally, but may sometimes produce anomalous values, indicating that the machine is about to break. The classification modules are used for training supervised algorithms that classify inputs to exclusive categories. The clustering modules provide unsupervised algorithms to find similar items in a dataset. Regression algorithms predict numerical values (but inputs can also be categorical variables). In addition to these ML algorithms, ML Studio provides modules for other common ML-related tasks, such as <strong>P</strong><strong>rincipal Component Analysis</strong> (<strong>PCA</strong>) and text tokenization.</p>
<p>In this section, we'll show an example of how to train a regression model. The input dataset is a record of flight delay information from multiple airports, available as a built-in dataset in ML Studio. The dataset includes information on the time of the flight, its origin, and its destination airports, and the airline that operates the flight. The label column, the value that we want to predict, is the departure delay in minutes (column <kbd>DepDelay</kbd>). A positive value for <kbd>DepDelay</kbd> means that the flight has been delayed, and a negative value means that it has departed ahead of schedule. This is a fairly large dataset, containing over 2.7 million rows and 14 columns. To get more detailed information about the dataset, see the full description in the documentation available at <a href="https://docs.microsoft.com/en-us/azure/machine-learning/studio/use-sample-datasets">https://docs.microsoft.com/en-us/azure/machine-learning/studio/use-sample-datasets</a>.</p>
<p>The training process consists of the following steps:</p>
<ol>
<li>Import data</li>
<li>Preprocess data (choose which columns are used)</li>
<li>Split data into training and test datasets</li>
<li>Pick an ML algorithm and train it using the training dataset</li>
<li>Use the trained model to create predictions for the test dataset</li>
<li>Compare the predicted values to the actual values in the test dataset</li>
</ol>
<p class="mce-root"/>
<p>The input dataset contains mostly numerical data that does not require much preprocessing. The <kbd>Carrier</kbd> column contains categorical values in text format, but ML Studio converts these values into numeric values automatically. The only preprocessing step is choosing the columns to be used to train the model. In this example, the following columns are chosen as the features of the model:</p>
<table style="border-collapse: collapse;width: 80%;border-color: #000000" border="1">
<tbody>
<tr style="height: 10px">
<td style="height: 10px"><strong>Column</strong></td>
<td style="height: 10px"><strong>Description</strong></td>
</tr>
<tr>
<td><kbd>Month</kbd></td>
<td>Month (categorical numeric, 1-12)</td>
</tr>
<tr style="height: 10px">
<td style="height: 10px"><kbd>DayOfWeek</kbd></td>
<td style="height: 10px">Day of week (categorical numeric, 1-7)</td>
</tr>
<tr style="height: 10px">
<td style="height: 10px"><kbd>OriginAirportID</kbd></td>
<td style="height: 10px">Airport—departure (categorical numeric, 70 unique values)</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px"><kbd>Carrier</kbd></td>
<td style="height: 64px">
<p class="mce-root">Airline (categorical string, 16 unique values)</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px"><kbd>CRSDepTime</kbd></td>
<td style="height: 64px">Time of day (categorical numeric, 1-2359, 1440 possible values)</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Note that the time of departure is given in numeric format, where the number 101 corresponds to 01:01, for example. The <span class="packt_screen">Select Columns in Dataset</span> module can be used to pick these columns and the label column (<kbd>DepDelay</kbd>) is to be used in the training process.</p>
<p>Before the model is trained, the data must be separated into training and test datasets. This is a crucial step in the training process: it is important that the accuracy of the model is measured with examples that the model has not seen during the training process. The <span class="packt_screen">Split Data</span> module is meant for this purpose. By setting the <span><span class="packt_screen">Fraction of rows in the first output dataset</span> property of the module to <kbd>0.75</kbd>,</span> for example, the first output port of the module will contain 75% of the rows and the second output will contain 25%, selected randomly from the input dataset. We will use this splitting to divide the data into the training and test datasets, respectively.</p>
<p>Once the data has been processed and divided into training and test datasets, the model is trained with the training dataset. The <span class="packt_screen">Train Model</span> module in ML Studio requires two inputs: an uninitialized ML model and the training dataset. The output of the module is the trained ML model that can be used to make predictions. In this example, the aim is to predict a numerical variable (the flight delay in minutes). As discussed previously, this type of problem requires a regression model. For simple testing, it is usually best to start with linear regression. This model does not often produce the best results, but it runs fast and gives a baseline for accuracy when evaluating more advanced models. Here is an example of a full training pipeline:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/4e14730c-4ed6-4ab7-903d-c6384a3368b3.png" style="width:45.42em;height:43.00em;" width="1444" height="1366"/></p>
<p>In this case, the <span class="packt_screen">Decision Forest Regression</span> model was trained with 75% of the input data. This model can produce accurate results, but the training time can be long if the dataset contains many rows. Training this model with a little over 2 million rows took about 1 hour, while the linear regression model was trained in less than 1 minute. Although some algorithms may perform better in certain situations, it is usually difficult to predict beforehand which algorithm will produce the best results for a given problem. The best approach is to experiment with different algorithms and parameters to find the best model. We will show how this can be done in an organized manner using the modules in ML Studio, as follows.</p>
<p>To get more information about each module, click on the module so that the configuration panel appears on the right-hand side of the canvas. Follow the link at the bottom of the panel under <span class="packt_screen">Quick Help</span>. The module documentation includes detailed information about each module and how to configure it.</p>
<p>After the <span class="packt_screen">Train Model</span> module has been connected with an ML module and an input dataset, its output can be taken to the <span class="packt_screen">Score Model</span> module to make predictions. To understand how well our model is performing, the test dataset is scored and the predictions for the model are compared to the true values (labels). The <span class="packt_screen">Score Model</span> module will add a new column to the output data, containing the predicted values. The predicted values are then plotted against the actual values as a scatter plot to see how well they match up against each other.</p>
<p>Although ML Studio does not provide any native visualization modules, the <span class="packt_screen">Execute R Script</span> module can produce R graphics as output. The <kbd>plot</kbd> command is suitable for simple figures and is supported natively in the R module. The <kbd>ggplot2</kbd> library is also available in the R module. This library is widely used and produces high quality pictures. Here is an example of how to plot the <kbd>DepDelay</kbd> (actual values) on the <em>x</em> axis and the <kbd>Scored Label Mean</kbd> (predicted value) on the <em>y</em> axis:</p>
<pre># Import ggplot2 library<br/>library(ggplot2)<br/><br/># Map the first input port to data frame<br/>input1 &lt;- maml.mapInputPort(1)<br/><br/># Create a graph with ggplot<br/>graph &lt;- ggplot(input1, aes(x=DepDelay, y=`Scored Label Mean`))<br/>graph &lt;- graph + labs(x="Departure delay (actual)",<br/>                      y="Departure delay (predicted)")<br/>graph + geom_point(color="blue");<br/><br/># Pass the data frame to the output port<br/>maml.mapOutputPort("input1");</pre>
<p class="mce-root"/>
<p>Note that if the linear regression model was used, the predicted values would be in a different column (<kbd>Scored Labels</kbd>).</p>
<p>These are all the steps needed to train the model and analyze the results in a single pipeline. The model can now be trained by clicking the <span class="packt_screen">Run</span> button in the toolbar at the bottom in ML Studio. After the run has finished, the results can be viewed by right-clicking the second output (<span class="packt_screen">R Device</span>) and choosing <span class="packt_screen">Visualize</span>. The plotting output can be viewed under the <span class="packt_screen">Graphics</span> section, as demonstrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/38837ea6-5f12-4b49-9df3-b355c609c855.png" style="width:37.67em;height:45.00em;" width="702" height="840"/></p>
<p>In an ideal scenario, each predicted value would be equal to the actual value and there would be a straight line from the bottom-left corner to the upper-right corner. The preceding screenshot is therefore far from ideal. It seems that the model predicts some high values when the actual values are low, and vice versa; it is not able to predict high values when it should. We can already see from this screenshot that the <em>variance</em> of the results is high. The conclusion is that the features that were given to the model are not conclusive enough to predict delays accurately. The selected features (see the preceding list) do not contain sufficient information to explain the variation in the label variable.</p>
<p>Since the initial results do not seem convincing, what should the next step be? Instead of continuing to experiment with different configurations, it might be more productive to take a step back and consider whether there is some additional data that gives more information about when delays might occur. For example, weather conditions might correlate well with flight delays, so it could be a good idea to include weather data in the model. To see an example of this, navigate to Azure AI Gallery and search for the <kbd>Binary Classification: Flight delay prediction</kbd> template.</p>
<p>In the following subsections, we'll examine how different feature variables contribute to the variation in the label variable. We'll also show how to use ML Studio modules to evaluate different models and explore different parameter configurations.</p>
<p>To a beginner, the wide selection of different ML algorithms may feel overwhelming. Which algorithm gives quick results, and which is good for large datasets? To help answer these and other questions, Microsoft has published an Azure ML algorithm cheat sheet: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-cheat-sheet">https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-cheat-sheet</a>. The cheat sheet shows the pros and cons of each algorithm at a quick glance.</p>
<p>More advanced users will know that the algorithms in ML Studio are just a small portion of all of the available models. The algorithm selection can be extended with R libraries by using the <span class="packt_screen">Create R Model</span> module. This is a code-based alternative for developing ML models in ML Studio.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feature selection</h1>
                </header>
            
            <article>
                
<p>A common problem when developing ML models is deciding which features should be used when training a model. For a supervised learning algorithm, the best features are those that are highly correlated with the label variable. This means, broadly speaking, that changing one of the variables induces a change in the other variable as well. An example of highly correlated variables could be the time of day and the amount of road traffic: traffic jams usually occur during the rush hour, while the amount of traffic during the night is particularly low.</p>
<p>The general aim of feature selection is to discover the variables that have the largest impact on the target variable. If the input dataset contains a large amount of columns, it ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Comparing models and parameters</h1>
                </header>
            
            <article>
                
<p>One of the core tasks in developing ML models is choosing the ML algorithm and configuring the parameters of the algorithm. ML Studio provides modules for both of these tasks, allowing you to compare multiple models, or parameter values in a single run.</p>
<p>To train multiple models in one experiment, the training and test datasets can be reused by directing the datasets to multiple training branches as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/c1dc845b-3c5f-4b61-9d28-35925b2117b5.png" style="width:51.50em;height:47.08em;" width="1416" height="1294"/></p>
<p>The preceding experiment is similar to the earlier training experiment, except that there are two <span class="packt_screen">Train Model</span> modules with different algorithms as inputs. The results of the <span class="packt_screen">Score Model</span> module are also directed to the <span class="packt_screen">Evaluate Model</span> module, instead of visualizing the raw prediction results, as was done earlier. The evaluation module takes two datasets as inputs, each dataset containing the original labels and the predicted values from the scoring module.</p>
<p class="mce-root"/>
<p>The output of the evaluation module contains the summary statistics of the predicted values (the first input dataset is shown on the first row):</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f2c2b8df-2081-4cda-9d8c-19a428d066d0.png" style="width:58.25em;height:19.00em;" width="1984" height="648"/></p>
<p>The columns of the evaluation results depend on the nature of the algorithms. The accuracy metrics that are used to evaluate regression models are different than those used for the evaluation of classification models, for example. As seen in the preceding screenshot, the evaluation module calculates several different metrics for a regression model. The different accuracy metrics capture different aspects of the errors. The <span class="packt_screen">Root Mean Squared Error</span> is probably the most widely used metric for regression models. This metric indicates the confidence interval containing 95% of the examples in the test dataset.</p>
<p>Sometimes, different metrics can give contradicting results. In the preceding example, the second model has a lower <span class="packt_screen">Root Mean Squared Error</span>, while other metrics, such as the <span class="packt_screen">Mean Absolute Error</span>, are better in the first model. Therefore, it is important to choose the accuracy metric carefully when comparing results for different models. Different metrics emphasize different aspects of the error distribution, so the best metric depends on the problem and the input data. The properties of each metric is beyond the scope of this book, and we refer the reader to the ML Studio documentation and general statistics literature for details about the accuracy metrics.</p>
<p class="mce-root"/>
<p>As already mentioned, ML algorithms include parameters that affect how the model is trained. These parameters are often called <strong>hyperparameters</strong>. While the default values of the hyperparameters are chosen to work well in most cases, sometimes the accuracy of a model can be improved by choosing different values for the parameters. The <span class="packt_screen">Tune Model Hyperparameters</span> module enables you to train a model multiple times in a single run using different parameter values. This module can be used instead of the <span class="packt_screen">Train Model</span> module to create a trained model, as in the following experiment:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/557405e5-7dfe-4a29-8d7d-3854c6cb26da.png" style="width:37.42em;height:29.25em;" width="552" height="431"/></p>
<p>The inputs to <span class="packt_screen">Tune Model Hyperparameters</span> are the same as for <span class="packt_screen">Train Model</span>, except that the former accepts a validation dataset as a third optional input. Using the validation dataset means that the accuracy between different parameter values is evaluated with examples that were not used for training the model.</p>
<p class="mce-root">Each algorithm has its own set of parameters and the parameters used for training must be specified in the algorithm module. To enable multiple values for the parameters, switch the <span class="packt_screen">Create trainer mode</span> option of the algorithm module to <span class="packt_screen">Parameter Range</span>. Note that some algorithms, such as linear regression, do not allow hyperparameter tuning. The possible values of the parameters can be given as a comma-separated list or by specifying a range using the range builder, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/9c76f184-025b-4631-a584-acf16fc80471.png" style="width:23.42em;height:38.50em;" width="355" height="584"/></p>
<p>The <span class="packt_screen">Tune Model Hyperparameters</span> module does not necessarily try out all of the possible combinations of the parameters. The module supports the following parameter sweep modes: entire grid, random grid, and random sweep. If entire grid mode is selected, all combinations will be tried. The random grid mode uses only a subset of all possible combinations, selected randomly. The total amount of runs in a random grid sweep can be controlled by setting the maximum number of runs on random sweep to a suitable value. This is particularly useful if there is a large number of combinations and it would take a very long time to sweep over the whole parameter space. Similarly, the random sweep mode can be used to run a subset of all possible combinations. The difference between the random grid and the random sweep modes is that the latter chooses the parameter values randomly within the specified range, while the form<span>er uses only the exact values defined in the algorithm module.<br/></span></p>
<p><span>Before running the example, set the label column similarly, as for <span class="packt_screen">Train Model</span>. The accuracy metric must also be defined, depending on the nature of the model (classification or regression). The accuracy metric determines which measure is used when selecting the best model. As already discussed, different metrics can disagree about the best model, so it is important to decide which metric is used to evaluate the performance of the model.<br/></span></p>
<p>After the experiment has finished running, the results of the evaluation can be viewed in the first output of the <span class="packt_screen">Tune Model Hyperparameters</span> module, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/99ff6a2d-8c95-45f8-8345-2042309d77d4.png" style="width:66.42em;height:25.33em;" width="1057" height="399"/></p>
<p class="mce-root"/>
<p>Each row in the results corresponds to an independent model, trained with different parameters. The columns show which parameter values were used in each case and the corresponding values of the accuracy metrics. The results are organized in decreasing order of accuracy, as defined by the metric chosen (here: <span class="packt_screen">Mean Absolute Error</span>). The second output of <span class="packt_screen">Tune Model Hyperparameters</span> contains the best trained model, also defined by the metric. This model can be used for scoring, similar to the output of the <span class="packt_screen">Train Model</span> module.</p>
<p>In conclusion, Azure ML Studio includes many built-in modules to evaluate the accuracy of ML models, and to test different models and configurations. It must, however, be borne in mind that even the best models cannot perform well on poor data. If the values to be predicted are not correlated with the feature variables, the algorithm will not be able to make good predictions. Moreover, the algorithm that produces the best accuracy is not always the best model in practice. Particularly with large datasets, it is often necessary to consider the runtime of the training process. If the complexity of the training process grows uncontrollably as the amount of data grows, training the algorithm can become impossible in practice. These aspects must also be taken into account when choosing ML algorithms for any given problem.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying a model as a web service</h1>
                </header>
            
            <article>
                
<p>One of the biggest strengths of Azure ML Studio is the ease with which you can deploy models to the cloud, to be consumed by other applications. Once an ML model is trained, as demonstrated in the previous section, it can be exported to ML Studio Web Services with just a few clicks. Deployment creates a web API for the model, which can be called from any internet-connected application. The model takes the features as input data and produces a predicted value as output. By deploying models to the ML Studio Web Service, there is no need to worry about the underlying server infrastructure. The computing resources and maintenance are handled entirely by Azure.</p>
<p>The following subsections show how to deploy an ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating a predictive experiment</h1>
                </header>
            
            <article>
                
<p class="mce-root">Before a trained model can be exported to the web service, the training experiment that created the model must be converted into a predictive experiment. A predictive experiment defines the scoring pipeline for creating predictions based on web service input. It does not contain any training modules, since the model is already trained. Instead, the model is just loaded from the list of trained models and imported as input to the <span class="packt_screen">Score Model</span> module.</p>
<p class="mce-root"/>
<p>To create a predictive experiment, open a training experiment that has been previously run, or run the training experiment once to create a trained model. The experiment does not need to include a <span class="packt_screen">Score Model</span> module—this will be added to the predictive experiment automatically. Click on <span class="packt_screen">Set up Web Service | Create Predictive experiment</span>. This leads to a new view that shows the new predictive experiment, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f117bdc4-05ce-44cf-858c-91d6f02c2387.png" style="width:53.17em;height:35.92em;" width="812" height="549"/></p>
<p>This example uses the same flight delay prediction experiment that was trained earlier. Note that the <span class="packt_screen">Split Data</span> module has been removed from the predictive experiment and the training modules have now been replaced with the trained model. The <span class="packt_screen">Split Data</span> module was only used to divide the data into training and test datasets, so ML Studio has inferred that it can be removed from the predictive experiment along with the training modules.</p>
<p>Run the experiment once before deploying it. The predictive experiment cannot be deployed as a web service before it has been run at least once. For instructions on how to deploy the model, skip to the next subsection.</p>
<p>In some cases, it might be useful to provide some of the module parameters as input from the web service. For example, the data preprocessing steps might depend on the input data. For this purpose, most of the module configurations can be parameterized so that the configuration values are supplied in the request as input. To parameterize a certain module parameter, go back to the training experiment and click on the module to be configured. In the configuration panel on the right-hand side of the canvas, click on the menu next to the field to be configured and select <span class="packt_screen">Set as web service parameter</span>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/e801e72b-8904-4b16-b1c3-4a661b0dc417.png" style="width:22.67em;height:24.42em;" width="370" height="399"/></p>
<p>This will add the parameter to the list of <span class="packt_screen">Web Services</span> parameters for the module. It is also possible to set a default value for the parameter, in case there is no value supplied in the request. Not all module parameters can be set as <span class="packt_screen">Web Service</span> parameters, however. Those parameters that cannot be set do not have the context menu next to the field (see, for example, <span class="packt_screen">Cleaning mode</span> in the preceding screenshot).</p>
<p>Sometimes, the deployed models might give poor predictions and we may wish to revert back to a previous version of the model. The previous runs can always be viewed from the <span class="packt_screen">Run History</span>, which is found in the bottom panel of the experiment. The <span class="packt_screen">Run History</span> shows every execution of the experiment, and clicking a version of the experiment will open it with the respective configuration and results. This version of the experiment will be locked and cannot be edited or deployed any more, but it can be saved as a new training experiment and retrained to create a new predictive experiment. This is useful if the model has been retrained several times and the optimal parameters for the model have been forgotten.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying and testing a web service</h1>
                </header>
            
            <article>
                
<p>The final step after creating the predictive experiment is to deploy the model to the cloud. Open the predictive experiment and make sure that it has been run successfully at least once, as discussed previously. From the bottom panel, choose <span class="packt_screen">Deploy Web Service | Deploy Web Service [New]</span>. This brings up the deployment configuration view. Choose a name and the price plan for the web service. The price plan determines how many requests the service can handle in a month. The price plan is created at the same time as when a new ML Studio Web Service is created in the Azure portal. If there is no existing price plan, a new one can also be created by choosing <span class="packt_screen">Create new...</span> from the menu.</p>
<p>When the configuration ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Azure Machine Learning Studio is a fully managed platform for developing machine learning models, enabling the user to concentrate on the essential tasks and problems in machine learning development. The graphical user interface is easy to learn and its usage requires no programming skills. Even users who have no prior experience in programming or machine learning can learn to use it, and the Experiment template collection contains many real-world examples of ML models to start with. ML Studio is a great way to start learning to develop ML models, and the sample datasets in ML Studio make it possible to develop your own models, even if you don't have your own data to start with.</p>
<p>Machine Learning Studio contains modules for all the most common ML-related tasks, such as data preprocessing, tuning hyperparameters, and evaluating the performance of ML algorithms. If the module collection is not sufficient for the task at hand, the R and Python script modules can be used to customize tasks and visualize the results, for example.</p>
<p>Machine Learning Studio is a complete environment for developing ML models, including all the steps from data ingestion to serving models in the cloud. Once the models have been trained in ML Studio Workspace, they can be converted to Predictive Experiments and deployed to the ML Studio Web Services for serving with a few clicks. The user does not need to worry about managing the underlying infrastructure behind the web services API, as this is managed entirely by the service. The web services portal contains comprehensive documentation and examples for integrating with the service, making it very easy to begin consuming the ML models from external applications. In the next chapter we will see <span>how to use Azure in data science.</span></p>


            </article>

            
        </section>
    </div>



  </body></html>