- en: Chapter 14. New generation data architectures for Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is our last chapter, and we will take a detour from our usual learning
    topics to cover some of the solution aspects of Machine learning. This is in an
    attempt to complete a practitioner's view on the implementation aspects of Machine
    learning solutions, covering more on the choice of platform for different business
    cases. Let's look beyond Hadoop, NoSQL, and other related solutions. The new paradigm
    is definitely a unified platform architecture that takes care of all the aspects
    of Machine learning, starting from data collection and preparation until the visualizations,
    with focus on all the key architecture drivers such as volume, sources, throughput,
    latency, extensibility, data quality, reliability, security, self-service, and
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following flowchart depicts different data architecture paradigms that
    will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![New generation data architectures for Machine learning](img/B03980_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The topics listed here are covered in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief history of how traditional data architectures were implemented and why
    they are found desirable in the current context of big data and analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of the new-age data architecture requirements in the context of
    Machine learning that includes **Extract, Transform, and Load** (**ETL**), storage,
    processing and reporting, distribution, and the presentation of the insights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to Lambda architectures that unifies strategies for batch and
    real-time processing with some examples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to Polyglot Persistence and Polymorphic databases that unify
    data storage strategies that include structured, unstructured, and semi-structured
    data stores, and centralize the querying approach across data stores. An example
    of how the Greenplum database supports the same and how it integrates with Hadoop
    seamlessly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic Data Architectures include Ontologies Evolution, purpose, use cases,
    and technologies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution of data architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start with understanding how data architectures traditionally have been
    followed by detailing the demands of modern machine learning or analytics platforms
    in the context of big data.
  prefs: []
  type: TYPE_NORMAL
- en: Observation 1—Data stores were always for a purpose
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditionally, data architectures had a clear segregation of purpose, **OLTP**
    (**Online Transaction Processing**), typically known to be used for transactional
    needs, and **OLAP** (**Online Analytic Processing**) data stores that typically
    used for reporting and analytical needs. The following table elaborates the general
    differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | OLTP databases | OLAP databases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Definition** | This involves many small online transactions (INSERT, UPDATE,
    and DELETE). The fast query processing is the core requirement; maintaining data
    integrity, concurrency, and effectiveness is measured by the number of transactions
    per second. It''s usually characterized by a high-level of normalization. | This
    involves a relatively small volume of transactions. Complex Queries involves slicing
    and dicing of data. The data stored is usually aggregated, historical in nature,
    and mostly stored in multi-dimensional schemas (usually star schema). |'
  prefs: []
  type: TYPE_TB
- en: '| **Data type** | Operational data | Integrated/consolidated/aggregated data
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Source** | OLTP databases usually are the actual sources of data | OLAP
    databases consolidate data from various OLTP databases |'
  prefs: []
  type: TYPE_TB
- en: '| **Primary purpose** | This deals with the execution of day-to-day business
    processes/tasks | This serves decision-support |'
  prefs: []
  type: TYPE_TB
- en: '| **CUD** | This is short, fast inserts and updates initiated by users | Periodic
    long-running jobs are refreshing the data |'
  prefs: []
  type: TYPE_TB
- en: '| **Queries** | This usually works on smaller volumes of data and executes
    simpler queries | This often includes complex queries involving aggregations and
    slicing and dicing in the multi-dimensional structure |'
  prefs: []
  type: TYPE_TB
- en: '| **Throughput** | This is usually very fast due to relatively smaller data
    volumes and quicker running queries | This usually run in batches and in higher
    volumes, may take several hours depending on volumes |'
  prefs: []
  type: TYPE_TB
- en: '| **Storage Capacity** | Relatively small as historical data is archived |
    This requires larger storage space due to the volumes that are involved |'
  prefs: []
  type: TYPE_TB
- en: '| **Schema Design** | Highly normalized with many tables | This is typically
    de-normalized with fewer tables and the use of star and/or snowflake schemas |'
  prefs: []
  type: TYPE_TB
- en: '| **Backup and Recovery** | This requires proper backup religiously; operational
    data is critical to run the business. Data loss is likely to entail significant
    monetary loss and legal liability | Instead of regular backups, some environments
    may consider simply reloading the OLTP data as a recovery method |'
  prefs: []
  type: TYPE_TB
- en: Observation 2—Data architectures were shared disk
  prefs: []
  type: TYPE_NORMAL
- en: Shared disk data architecture refers to an architecture where there is a data
    disk that holds all the data, and each node in the cluster has access to this
    data for processing. All the data operations can be performed by any node at a
    given point in time, and in case two nodes attempt at persisting/writing a tuple
    at the same time, to ensure consistency, a disk-based lock or intended lock communication
    is passed on, thus affecting the performance. Further with an increase in the
    number of nodes, contention at the database level increases. These architectures
    are *write* limited as there is a need to handle the locks across the nodes in
    the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Even in the case of the reads, partitioning should be implemented effectively
    to avoid complete table scans. All the traditional RDBMS databases are the shared
    disk data architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![Evolution of data architectures](img/B03980_14_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Observation 3—Traditional ETL architecture had limitations. The following list
    provides the details of these limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Onboarding and integrating data were slow and expensive. Most of the ETL logic
    that exists today is custom coded and is tightly coupled with the database. This
    tight coupling also resulted in a problem where the existing logic code cannot
    be reused. Analytics and reporting requirements needed a different set of tuning
    techniques to be applied. Optimization for analytics was time-consuming and costly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data provenance was often poorly recorded. The data meaning was *lost in translation*.
    Post-onboarding, maintenance and analysis cost for the on-boarded data was usually
    very high. Recreating data lineage was manual, time-consuming, and error-prone.
    There was no strong auditing or record of the data transformations and were generally
    tracked in spreadsheets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target data was difficult to consume. The optimization favors known analytics
    but was not well-suited to the new requirements. A one-size-fits-all canonical
    view was used rather than fit-for-purpose views, or lacks a conceptual model to
    consume easily the target data. It has been difficult to identify what data was
    available, how to get access, and how to integrate the data to answer a question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observation 4—Data was usually only structured
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, the database was designed to fit the RDBMS models. If the
    incoming data was not actually structured, the ETLs would build a structure around
    for being stored in a standard OLTP or OLAP store.
  prefs: []
  type: TYPE_NORMAL
- en: Observation 5—Performance and scalability
  prefs: []
  type: TYPE_NORMAL
- en: The optimization of a data store or a query was possible to an extent, given
    the infrastructure, and beyond a certain point, there was a need for a redesign.
  prefs: []
  type: TYPE_NORMAL
- en: Emerging perspectives & drivers for new age data architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Driver 1—*BIG* data intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have defined big data and large dataset concepts in [Chapter 2](ch02.html
    "Chapter 2. Machine learning and Large-scale datasets"), *Machine learning and
    Large-scale datasets*. The data that is now being ingested and needs to be processed
    typically has the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source**: Depending upon the nature of the information, the source may be
    a real-time stream of data (for example, trade transactions), or batches of data
    containing updates since the last sync'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content**: The data may represent different types of information. Often,
    this information is related to other pieces of data and is needed to be connected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the types of data and different sources that
    need to be supported:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Emerging perspectives & drivers for new age data architectures](img/B03980_14_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Volume**: Depending upon the nature of the data, the volumes that are being
    processed may vary. For example, master data or the securities definition data
    are relatively fixed, whereas the transaction data is enormous compared to the
    other two.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lifecycle**: Master data has a fixed life and is rarely updated (such as,
    slowly changing dimensions). However, the transactional data has a very short
    life but needs to be available for analysis, audit, and so on for longer periods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structure**: While most of the data is structured, there is an advent of
    unstructured data in the financial industry. It is becoming increasingly critical
    for financial systems to incorporate unstructured data as part of their IT architectures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next chart depicts the complexity, velocity volume, and various aspects
    of each data source:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Emerging perspectives & drivers for new age data architectures](img/B03980_14_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: SoftServe'
  prefs: []
  type: TYPE_NORMAL
- en: Driver 2—Data platform requirements are advanced
  prefs: []
  type: TYPE_NORMAL
- en: The landscape of the new age-data platform requirements is drastically expanding,
    and the unified platforms are the happening ones. The next concept map explains
    it all. The core elements of data architectures include ETL (Extract, Transform,
    and Load), Storage, Reporting, Analytics, Visualization, and data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Emerging perspectives & drivers for new age data architectures](img/B03980_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Driver 3—Machine learning and analytics platforms now have a new purpose and
    definition
  prefs: []
  type: TYPE_NORMAL
- en: 'The evolution of analytics and it repurposing itself is depicted in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: Historically, the focus was merely on reporting. Aggregated or preprocessed
    data is loaded into the warehouse to understand what has happened. This is termed
    as **Descriptive analytics** and was primarily a backward step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the advent of ad-hoc data inclusion, the need was to understand why certain
    behavior happened. This is called **Diagnostic analytics** and is focused on understanding
    the root cause of the behavior, which is again based on the historical data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the demand has shifted, and the need is to understand what will happen.
    This is called **Predictive analytics,** and the focus is to predict the events
    based on the historical behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the advent of real-time data, the focus is now on do we make it happen?
    This goes beyond predictive analytics where remediation is a part of something.
    The ultimate focus is to *Make it happen!* with the advent of real-time event
    access. The following diagram depicts the evolution of analytics w.r.t. the value
    and related complexity:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Emerging perspectives & drivers for new age data architectures](img/B03980_14_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next table differentiates the traditional analytics (BI) and the new age
    analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Area | Traditional analytics (BI) | New age analytics |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Scope** | Descriptive AnalyticsDiagnostic Analytics | Predictive AnalyticsData
    science |'
  prefs: []
  type: TYPE_TB
- en: '| **Data** | Limited/Controlled volumesPreprocessed/ValidatedBasic models |
    Large volumesDiverse formats and heavy on varietyRaw data that are not pre-processedThe
    growing model complexity |'
  prefs: []
  type: TYPE_TB
- en: '| **Result** | Here, the focus is on retrospection and the root-cause analysis
    | Here, the focus is on prediction/insights and the accuracy of analysis |'
  prefs: []
  type: TYPE_TB
- en: Driver 4—It is not all about historical and batch, it is real-time and instantaneous
    insights
  prefs: []
  type: TYPE_NORMAL
- en: The data coming in lower volumes and higher velocity is what defines *real-time*.
    The new age analytics systems are expected to handle real-time, batch, and near
    real-time processing requests (these are scheduled and known as micro batches).
    The following graph depicts the properties of real-time and batch data characteristics
    with respect to volume, velocity, and variety being constant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Emerging perspectives & drivers for new age data architectures](img/B03980_14_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Driver 5—Traditional ETL is unable to cope with *BIG* data
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to be able to lay out an ETL architecture strategy that can address
    the following problematic areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Facilitates standardization in implementation—dealing with the need for one
    standard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports building reusable components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building agnostic functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving performance and scalability using parallel processing techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the total overall **cost of ownership** (**TCO**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a specialized skill pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table provides a comparative analysis of the key data loading
    pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | ETLExtract, Transform, and Load | ELTExtract, Load, and Transform | ETLTExtract,
    Transform, Load, and Transform |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Overview** | This is a traditional technique for moving and transforming
    data in which an ETL engine is either separated from the source or the target
    DBMS performs the data transformations. | This is a technique for moving and transforming
    data from one location and formatting it to another instance and format. In this
    style of integration, the target DBMS becomes the transformation engine. | In
    this technique, transformations are partly done by the ETL engine and partly pushed
    to the destination DBMS. |'
  prefs: []
  type: TYPE_TB
- en: '| **Highlights** | A heavy work of transformation is done in the ETL engine.It
    uses the integrated transformation functions.Transformation logic can be configured
    through the GUI.This is supported by Informatica. | A heavy work of transformation
    is handed over to the DBMS layer.Transformation logic runs closer to the data.It
    is supported by Informatica. | Transformation work is split between the ETL engine
    and the DBMS.It is supported by Informatica. |'
  prefs: []
  type: TYPE_TB
- en: '| **Benefits** | This is an easy, GUI-based configuration.The transformation
    logic is independent, outside the database, and is reusableThis works very well
    for granular, simple, function-oriented transformations that do not require any
    database calls.Can run on SMP or MPP hardware. | This leverages the RDBMS engine
    hardware for scalability.It always keeps all the data in RDBMS.It is parallelized
    according to the dataset, and the disk I/O is usually optimized at the engine
    level for faster throughput.It scales as long as the hardware and the RDBMS engine
    can continue to scale.Can achieve 3x to 4x the throughput rates on the appropriately
    tuned MPP RDBMS platform. | It can balance the workload or share the workload
    with the RDBMS. |'
  prefs: []
  type: TYPE_TB
- en: '| **Risks** | This requires a higher processing power on the ETL side.The costs
    are higher.It consists of the complex transformations that would need reference
    data, which will slow down the process. | Transformation logic is tied to a database.The
    transformations that involve smaller volume and simple in nature will not gain
    many benefits. | This will still have a part of the transformation logic within
    the database. |'
  prefs: []
  type: TYPE_TB
- en: Fact 6—No "one" data model fits advanced or complex data processing requirements;
    there is a need for multi-data model platforms
  prefs: []
  type: TYPE_NORMAL
- en: Different databases are designed to solve different problems. Using a single
    database engine for all of the requirements usually leads to non-performant solutions.
    RDBMSs are known to work well-transactional operations, OLAP databases for reporting,
    NoSQL for high-volume data processing, and storage. Some solutions unify these
    storages and provide an abstraction for querying across these stores.
  prefs: []
  type: TYPE_NORMAL
- en: Modern data architectures for Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From this section onwards, we will cover some of the emergent data architectures,
    challenges that gave rise to architectures of this implementation architecture,
    some relevant technology stacks, and use cases where these architectures apply
    (as relevant) in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic data architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the facts covered in the emerging perspectives in the previous section
    give rise to the following core architecture drivers to build semantic data model
    driven data lakes that seamlessly integrate a larger data scope, which is analytics
    ready. The future of analytics is semantified. The goal here is to create a large-scale,
    flexible, standards-driven ETL architecture framework that models with the help
    of tools and other architecture assets to enable the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Enabling a common data architecture that can be a standard architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dovetailing into the Ontology-driven data architecture and data lakes of the
    future (it is important to tie this architecture strategy with the data aggregation
    reference architecture). This will ensure there is a single data strategy that
    takes care of the data quality and data integration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling product groups to integrate rapidly into the data architecture and
    deliver into and draw from the common data repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling ad-hoc analytics on need basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing time needed to implement the new data aggregation, ingestion, and transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling *any format to any format* model (a format-agnostic approach that involves
    data normalization sometimes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complying with emerging semantic standards. This will bring in the flexibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling the common IT management and reduction of the TCO.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling a consolidated cloud (that can be a proprietary) for the Broadridge
    Master Business Data Repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling all the applications and products to "talk to a common language" and
    building the Broadridge data format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce, and in some cases eliminate, the proliferation of too many licenses,
    databases, implementations, stacks, and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data Semantification: It is important to analyze the underlying schemas in
    order to unlock the meaning from them. The semantification process is always iterative
    and evolves over the period of time. The metadata definitions in this context
    will be elaborated or expanded in this process.![Semantic data architecture](img/B03980_14_08.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up an enterprise-wide aggregate data mart is not the solution to problems
    outlined previously. Even if such a data mart was set up, keeping it updated and
    in line with the rest of the projects would be a major problem. As stated earlier,
    the need is to lay out the common reference architecture of a system that can
    accumulate data from many sources without making any assumptions on how, where,
    or when this data will be used.
  prefs: []
  type: TYPE_NORMAL
- en: There are two different advances in the field that we leverage to address the
    issues at an architecture level. These are the evolution of a data lake as an
    architecture pattern, and the emergence of Semantic Web and its growing relevance
    in e-business.
  prefs: []
  type: TYPE_NORMAL
- en: The business data lake
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The enterprise data lake gives the concept of an enterprise data warehouse a
    whole new dimension. While the approach with a data warehouse has always been
    to design a single schema and aggregate the minimum information needed to fulfill
    the schema, data lake turns both these premises of traditional data warehouse
    architectures on its head. The traditional data warehouse is designed for a specific
    purpose in mind (for example, analytics, reporting, and operational insights).
    The schema is designed accordingly, and the minimum information needed for the
    purpose is aggregated. This means that using this warehouse for any other objective
    is only incidental if at all, but it is not designed for such a use.
  prefs: []
  type: TYPE_NORMAL
- en: The business data lake promotes the concept of an appropriate schema—the warehouse
    is not constrained by a fixed, predetermined schema. This allows the data lake
    to assimilate information as and when it becomes available in the organization.
    The important direct implication of this is that rather than assimilating the
    minimum information—the data lake can assimilate all the information produced
    in the organization. Since there are no assumptions made about what the data is,
    options remain open to use the information for any purpose in the future. This
    enables the data lake to power business agility by being able to serve newer ideas
    with the data that is already available in the data lake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The business data lake addresses the following concerns:'
  prefs: []
  type: TYPE_NORMAL
- en: How to handle unstructured data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to link internal and external data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to adapt to the speed of business change?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to remove the repetitive ETL cycle?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to support different levels of data quality and governance based on differing
    business demands?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to let local business units take the initiative?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to ensure the deliverance of platform and that it will it be adopted?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic Web technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using external data that are most often found on the web, the most important
    requirement is understanding the precise semantics of the data. Without this,
    the results cannot be trusted. Here, Semantic Web technologies come to the rescue,
    as they allow semantics ranging from very simple to very complex to be specified
    for any available resource. Semantic Web technologies do not only support capturing
    the passive semantics, but also support active inference and reasoning on the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Web technologies allow data to be annotated with additional metadata
    (as RDF). One of the most fundamental capabilities that this adds is the **AAA
    principle** of Semantic Computing is—*Anyone can add anything about anything at
    any time*. As the information is made up of metadata, adding more metadata can
    enrich the information at any time.
  prefs: []
  type: TYPE_NORMAL
- en: Querying RDF data is done using SPARQL, which allows navigating complex relationship
    graphs to extract meaningful information from the data store. Reasoner (or an
    inference engine) works with the RDF metadata to deliver inferences at the top
    of the data. This allows the system to extract newer insights, which were originally
    not available in the incoming data.
  prefs: []
  type: TYPE_NORMAL
- en: Today, enormous amounts of information are becoming available over the web and
    over corporate and regulatory networks. However, access to all the available information
    remains limited as long as the information is stored separately without easy means
    to combine them from different sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'This exacerbates the need for suitable methods to combine data from various
    sources. This is termed as the *cooperation of information systems*. This is defined
    as the ability to share, combine, and exchange information between heterogeneous
    sources in a transparent way to the end users. These heterogeneous sources are
    usually known to have always handled data in silos, and thus, they are inaccessible.
    To achieve data interoperability, the issues posed by data heterogeneity needs
    to be eliminated. Data sources can be heterogeneous in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Syntax**: Syntactic heterogeneity is caused by the use of different models
    or languages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schema**: Schematic heterogeneity results from structural differences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantics**: Semantic heterogeneity is caused by different meanings or interpretations
    of data in various contexts![Semantic Web technologies](img/B03980_14_09.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data integration provides the ability to manipulate data transparently across
    multiple data sources. Based on the architecture, there are two systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Central Data Integration**: A central data integration system usually has
    a global schema, which provides the user with a uniform interface to access information
    stored in the data sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Peer-to-peer**: In contrast, in a peer-to-peer data integration system, there
    are no general points of control on the data sources (or peers). Instead, any
    peer can accept user queries for the information distributed in the entire system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cooperation of information systems is the ability to share, combine, and/or
    exchange information from multiple information sources, and the ability to access
    the integrated information by its final receivers transparently. The major problems
    that hinder the cooperation of information systems are the autonomy, the distribution,
    the heterogeneity, and the instability of information sources. In particular,
    we are interested in the heterogeneity problem that can be identified at several
    levels: the system, the syntactic, the structural, and the semantic heterogeneity.
    The cooperation of information systems has been extensively studied, and several
    approaches have been proposed to bridge the gap between heterogeneous information
    systems, such as: database translation, standardization, federation, mediation,
    and web services. These approaches provide appropriate solutions to the heterogeneity
    problem at syntactic and basic levels.'
  prefs: []
  type: TYPE_NORMAL
- en: However, in order to achieve semantic interoperability between heterogeneous
    information systems, the meaning of the information that is interchanged has to
    be understood in the systems. Semantic conflicts occur whenever two contexts do
    not use the same interpretation of the information.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in order to deal with semantic heterogeneity, there is a need for
    more semantic-specialized approaches, such as ontologies. In this chapter, our
    focus is to demonstrate how information systems can cooperate using semantics.
    In the next section, let us look at the constitution of semantic data architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Ontology and data integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The diagram here represents the reference architecture for Semantic data architecture-based
    analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ontology and data integration](img/B03980_14_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The key features of semantic data architecture are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metadata representation**: Each of the sources can be represented as local
    ontologies supported by a meta-data dictionary to interpret the nomenclature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global conceptualization**: There will be a global ontology definition that
    maps the local ontologies and provides a single view or nomenclature for a common
    view.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generic querying**: There will be a support for querying at a local of a
    global ontology levels depending on the need and purpose of the consumer / client.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Materialised view**: A high level querying strategy that masks querying between
    nomenclatures and peer sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mapping**: There will be support for defining the thesaurus based mapping
    between ontology attributes and values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vendors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Type | Product/framework | Vendor |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Open source and commercial versions | MarkLogic 8 is the NoSQL graph store
    that supports storing and processing RDF data formats and can serve as a triple
    store. | MarkLogic |'
  prefs: []
  type: TYPE_TB
- en: '| Open source and commercial versions | Stardog is the easiest and the most
    powerful graph database: search, query, reasoning, and constraints in a lightweight,
    pure Java system. | Stardog |'
  prefs: []
  type: TYPE_TB
- en: '| Open source | 4Store is an efficient, scalable, and stable RDF database.
    | Garlik Ltd. |'
  prefs: []
  type: TYPE_TB
- en: '| Open source | Jena is a free and open source Java framework for building
    Semantic Web and Linked Data applications. | Apache |'
  prefs: []
  type: TYPE_TB
- en: '| Open source | Sesame is a powerful Java framework for processing and handling
    RDF data. This includes creating, parsing, storing, inferencing, and querying
    of such data. It offers an easy-to-use API that can be connected to all the leading
    RDF storage solutions. | GPL v2 |'
  prefs: []
  type: TYPE_TB
- en: '| Open Source | Blazegraph is SYSTAP''s flagship graph database. It is specifically
    designed to support big graphs offering both Semantic Web (RDF/SPARQL) and graph
    database (TinkerPop, blueprints, and vertex-centric) APIs. | GPL v2 |'
  prefs: []
  type: TYPE_TB
- en: Multi-model database architecture / polyglot persistence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We could never have imagined, even five years ago, that relational databases
    would become only a single kind of a database technology and not the database
    technology. Internet-scale data processing changed the way we process data.
  prefs: []
  type: TYPE_NORMAL
- en: The new generation architectures such as Facebook, Wikipedia, SalesForce, and
    so on, are found in principles and paradigms, which are radically different from
    the well-established theoretical foundations on which the current data management
    technologies are developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major architectural challenges of these architectures can be characterized
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Commoditizing information:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apple App Store, SaaS, Ubiquitous Computing, Mobility, and the Cloud-Based Multi-Tenant
    architectures have unleashed, in business terms, an ability to commoditize information
    delivery. This model changes almost all the architecture decision making, as we
    now need to think in terms of what the "units of information" that can be offered
    and billed as services are, instead of thinking in terms of the TCO of the solution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Theoretical limitations of RDBMS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Michael Stonebraker, an influential Database theorist, has been writing
    in recent times at the heart of the Internet Scale Architectures is a new theoretical
    model of data processing and management. The theories of database management are
    now more than three decades old, and when they were designed, they were designed
    for the mainframe-type computing environments and very unreliable electronic components.
    Nature and the capabilities of the systems and applications have since evolved
    significantly. With reliability becoming a quality attribute of the underlying
    environment, systems are composed of parallel processing cores, and the nature
    of data creation and usage has undergone tremendous change. In order to conceptualize
    solutions for these new environments, we need to approach the designing solution
    architectures from a computing perspective, not only from an engineering perspective.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Six major forces are driving the data revolution today. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Massive Parallel Processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commoditized Information Delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ubiquitous Computing and Mobile Devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-RDBMS and Semantic Databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Community Computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop and MapReduce have unleashed massive parallel processing of data on a
    substantial basis and have made complex computing algorithms in a programmatic
    platform. This has changed analytics and BI forever. Similarly, web services and
    API-driven architectures have made information delivery commoditized on a substantial
    basis. Today, it is possible to build extremely large systems in such a way that
    each subsystem or component is a complete platform in itself, hosted and managed
    by a different entity altogether.
  prefs: []
  type: TYPE_NORMAL
- en: The previous innovations have changed the traditional Data Architecture completely.
    Especially, semantic computing and the ontology-driven modeling of information
    have turned data design on its head.
  prefs: []
  type: TYPE_NORMAL
- en: Philosophically, the data architecture is going through a factual underpinning.
    In traditional data models, we first design the *data model*—a fixed, design-time
    understanding of the world and its future. A data model fixes the meaning of data
    forever into a fixed structure.
  prefs: []
  type: TYPE_NORMAL
- en: A table is nothing but a category, a set of something. As a result, data has
    meaning only if we understand the set/category to which it belongs. For example,
    if we design an automobile processing system into some categories such as four-wheelers,
    two-wheelers, commercial vehicles, and so on, this division itself has some significant
    meaning embedded into it. The data that is stored in each of these categories
    does not reveal the *purpose of the design* that is embedded in the way the categories
    are designed. For example, another system might view the world of automobiles
    in terms of its drivetrain—electric, petroleum powered, nuclear powered, and more.
    This categorization itself reveals the purpose of the system in some manner, which
    is impossible to obtain from the attributes of any single record.
  prefs: []
  type: TYPE_NORMAL
- en: The term *polyglot* is typically used to define a person who can speak many
    languages. In the context of big data, this term refers to a set of applications
    that use many database technologies where each database technology solves a particular
    problem. The basic premise of this data architecture is that different database
    technologies solve various problems and since complex applications have many problems,
    picking one option to solve a particular issue is better than trying to solve
    all the problems using one option. When we talk about a data system, it is defined
    as a system that takes care of storage and querying of data, which has a runtime
    of several years and needs to address every possible hardware and maintenance
    complexities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Polyglot persistence data architecture is used when there is a complex problem,
    broken down into smaller problems and solved by applying different database models.
    This is followed by aggregating the results into a hybrid data storage platform
    followed by analysis. Some factors influencing the choice of database are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Factor 1—Data Models:'
  prefs: []
  type: TYPE_NORMAL
- en: What type of data sources do we want to integrate?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How would we want to manipulate/analyze the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the volume, variety, and velocity of data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples—Relational, Key-Value, Column-Oriented, Document-Oriented, Graph, and
    so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Factor 2—Consistency, availability, and partitioning (CAP):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**: Only one value of an object to each client (Atomicity)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: All objects are always available (Low Latency)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition tolerance**: Data is split into multiple network partitions (Clustering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CAP theorem requires us to choose any of the two features depicted here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-model database architecture / polyglot persistence](img/B03980_14_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram is an example of a system that has multiple applications
    with a data model built for its purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Multi-model database architecture / polyglot persistence](img/B03980_14_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: ThoughtWorks'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some important aspects that affect this solution are listed next:'
  prefs: []
  type: TYPE_NORMAL
- en: It is important that the proposed hybrid environment be clearly understood to
    ensure that it facilitates taking the right decision about data integration, analytics,
    data visibility, and others, and thus, how the solution fits into the entire big
    data and analytics implementation umbrella.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there is more than one data model, there will be a need for a unified
    platform that can interface with all the databases identified for solution and
    aggregation. This platform should address some bare minimum big data platform
    expectations like; fault tolerance high-availability, transactional integrity,
    data agility and reliability, scalability and performance are addressed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depending on the specific requirements, it is important for us to know/understand
    what sort of a data model works both: for the particular problem and the overall
    solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion strategies address the real-time and batch data updates and how
    they can be made to work in the context of the multi-model database. Since there
    will be a variety of data stores, what will the **System of Record** (**SOR**)
    be? And how do we ensure that data across all the data sources is in sync or up-to-date?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So overall, this is probably a big data challenge at its best. Multiple sources
    of data with very different structures need to be collected, integrated, and analyzed
    to solve a particular business problem. Then, the key is to identify whether the
    data needs to be pushed to the client on-demand or in real-time. And obviously,
    this type of problem cannot be solved easily or cost-effectively with one type
    of database technology. There could be some cases where a straightforward RDBMS
    could work, but in cases where there is non-relational data, there is a need for
    different persistence engines such as NoSQL. Similarly, for an e-commerce business
    problem, it is important that we have a highly available and a scalable data store
    for shopping cart functionality. However, to find products bought by a particular
    group, the same store cannot help. The need here is to go for a hybrid approach
    and have multiple data stores used in conjunction that is known as polyglot persistence.
  prefs: []
  type: TYPE_NORMAL
- en: Vendors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Type | Product/framework | Vendor |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Commercial | FoundationDB is a rock-solid database that gives NoSQL (Key-Value
    store) and SQL access. | FoundationDB |'
  prefs: []
  type: TYPE_TB
- en: '| Open source | ArangoDB is an open source NoSQL solution with a flexible data
    model for documents, graphs, and key-values. | GPL v2 |'
  prefs: []
  type: TYPE_TB
- en: Lambda Architecture (LA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lambda Architecture addresses one important aspect of Machine learning; that
    is, providing a unified platform for real-time and batch analytics. Most of the
    frameworks that we have seen until now support batch architecture (for example,
    Hadoop), in order to support real-time processing integration with specific frameworks
    (for example, Storm).
  prefs: []
  type: TYPE_NORMAL
- en: Nathan Marz introduced the concept of Lambda Architecture for a generic, scalable,
    and fault-tolerant data processing architecture that addresses a real-time stream-based
    processing and batch processing as a unified offering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lambda Architecture facilitates a data architecture that is highly fault-tolerant,
    both: against hardware failures and human mistakes. At the same time, it serves
    a broad range of uses and workloads, where low-latency reads and updates are required.
    The resulting system should be linearly scalable, and it should scale out rather
    than up.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how it looks from a high-level perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Lambda Architecture (LA)](img/B03980_14_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Data Layer**: All of the data entering the system is dispatched to both the
    batch layer and the speed layer for processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch layer**: This manages master data and is responsible for batch pre-computation.
    It handles heavy volumes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed layer**: Speed layer is responsible for handling recent data and compensates
    for the high latency of updates to the serving layer. On an average, this layer
    does not deal with large volumes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving layer**: Serving layer handles indexing the batch views and facilitates
    ad hoc querying demonstrating low-latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query function**: This combines the results from batch views and real-time
    views.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vendors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Type | Product/framework | Vendor |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Open source and commercial | Spring XD is a unified platform for a fragmented
    Hadoop ecosystem. It''s built at the top of the battle-tested open source projects,
    and dramatically simplifies the orchestration of big data workloads and data pipelines.
    | Pivotal (Spring Source) |'
  prefs: []
  type: TYPE_TB
- en: '| Open source | Apache Spark is a fast and conventional engine for big data
    processing with built-in modules for streaming, SQL, Machine learning, and graph
    processing. | Apache |'
  prefs: []
  type: TYPE_TB
- en: '| Open source | Oryx is a simple, real-time, and large-scale Machine learning
    infrastructure. | Apache (Cloudera) |'
  prefs: []
  type: TYPE_TB
- en: '| Open source | The storm is a system used to process streaming data in the
    real time. | Apache (Hortonworks) |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this concluding chapter, our focus has been on the implementation aspects
    of Machine learning. We have understood what traditional analytics platforms have
    been and how they cannot fit the modern data requirements. You have also learned
    the architecture drivers that are promoting the new data architecture paradigms
    such as Lamda Architectures and polyglot persistence (multi-model database architecture),
    and how Semantic architectures help seamless data integration. With this chapter,
    you can assume that you are ready for implementing a Machine learning solution
    for any domain with an ability to not only identify what algorithms or models
    are to be applied to solve a learning problem, but also what platform solutions
    will address it in the best possible way.
  prefs: []
  type: TYPE_NORMAL
