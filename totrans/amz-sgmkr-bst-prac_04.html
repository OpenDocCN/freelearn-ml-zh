<html><head></head><body>
		<div id="_idContainer046">
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Chapter 3: Data Labeling with Amazon SageMaker Ground Truth</h1>
			<p><a id="_idTextAnchor053"/>One of the biggest barriers to ML projects in most companies is access to labeled training data. At one company we worked with, we were trying to identify consumer-impacting outages. The customer had a lot of data from each layer of their application stack, but they couldn't agree on how to define an outage. Is an outage when a load balancer is down? Probably not – we have redundancy in the infrastructure layer. Is an outage when a customer can't access the service for over 10 minutes? That's probably too granular; a single customer might have problems due to local network connectivity issues. So, what exactly do we mean by an outage? How can we automatically label our training data as <em class="italic">outage</em> or <em class="italic">not an outage</em>?</p>
			<p>In this chapter, we'll review labeling data using SageMaker Ground Truth. We'll cover common challenges associated with large datasets and potentially biased data.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Challenges with labeling data at scale</li>
				<li>Addressing unique labeling requirements with custom labeling workflows</li>
				<li>Using active learning to reduce labeling time</li>
				<li>Security and permissions</li>
			</ul>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor054"/>Technical requirements</h1>
			<p>You will need an AWS account to run the examples included in this chapter. If you have not set up the data science environment yet, please refer to <a href="B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Data Science Environments</em>, which provides a walk-through of the setup process.</p>
			<p>Code examples included in the book are available on GitHub at <a href="https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter03">https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter03</a>. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>).</p>
			<p>The code for this chapter is in the <strong class="source-inline">CH03</strong> folder of the GitHub repository.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Challenges with labeling data at scale</h1>
			<p>Besides the conceptual challenges with agreeing on how to label data, we need to consider the logistics. <strong class="bold">SageMaker Ground Truth</strong> lets <a id="_idIndexMarker113"/>you assign data labeling jobs to a human workforce. But you may face additional challenges such as the following:</p>
			<ul>
				<li><strong class="bold">Unique labeling logic</strong>: If our<a id="_idIndexMarker114"/> labeling case requires a custom workflow, we need to model that in Ground Truth.</li>
				<li><strong class="bold">Annotation quality</strong>: The labels applied by workers may be subject to implicit bias that affects the results.</li>
				<li><strong class="bold">Cost and time</strong>: Labeling data requires people for a period of time. If you have a very large dataset, you'll consume a lot of person-hours.</li>
				<li><strong class="bold">Security</strong>: Given that your<a id="_idIndexMarker115"/> data may be sensitive, you need to make sure that access to the data is restricted to an authorized workforce.<p class="callout-heading">Additional information</p><p class="callout">If you need an introduction to Ground Truth, please review <a href="B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a> of <em class="italic">Learn Amazon SageMaker,</em> written by Julien Simon.</p></li>
			</ul>
			<p>To put these concerns into focus, let's consider our weather data introduced in the previous chapter. Ground Truth doesn't have a built-in workflow that lets us prompt workers to label weather data according to our logic for describing air as <em class="italic">good</em> or <em class="italic">bad</em>. The dataset for the entire time span is approximately 499 GB; labeling each entry by hand as <em class="italic">good</em> or <em class="italic">bad</em> weather quality will take some time. Finally, our workers may have their own implicit or unconscious bias. </p>
			<p>A worker who grew up in a city with severe smog may have a much different perception of air quality than someone who grew up in a rural area with very clean air. In the following sections, we'll discuss how to address these challenge<a id="_idTextAnchor056"/>s.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor057"/>Addressing unique labeling requirements with custom labeling workflows</h1>
			<p>Let's get started with a <a id="_idIndexMarker116"/>labeling job for our weather data. We want to label each weather report as <em class="italic">good</em> or <em class="italic">bad</em>. In order to help our workers do that, we'll make a nice frontend that shows the location of the weather station on a map and displays the reading from the weather station. We need a custom workflow because this scenario doesn't fall neatly into any of the existing Ground Truth templates.</p>
			<p>We will have to set up the following:</p>
			<ul>
				<li>A private workforce backed by a Cognito user pool </li>
				<li>A manifest file that lists the items we want to label</li>
				<li>A custom Ground Truth labeling workflow, consisting of two Lambda functions and a UI template</li>
			</ul>
			<p>The notebook <strong class="source-inline">LabelData.ipynb</strong> in the <strong class="source-inline">CH02</strong> folder of our repository walks through these ste<a id="_idTextAnchor058"/>ps.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor059"/>A private labeling workforce</h2>
			<p>Although you can <a id="_idIndexMarker117"/>use a public workforce, most companies will want to use a private workforce to label their own data. Setting up a private workforce starts by defining a Cognito user pool, which, for real use cases, could link to another identity provider such as Active Directory.</p>
			<p>We'll create a user group in Cognito; you could use groups to create teams for different types of labeling jobs. Finally, we'll define a SageMaker work team linked to the Cognito user group. Note that SageMaker creates a labeling domain that we have to set as the callback URL in the Cognito user pool client. </p>
			<p>Once the work team is set up, the notebook will add an example worker.</p>
			<p>The <strong class="bold">Create a private workforce</strong> part of the notebook executes all of these steps for you:</p>
			<ul>
				<li>Creating a Cognito user pool</li>
				<li>Creating a Cognito client for the user pool</li>
				<li>Creating an identity pool for the client</li>
				<li>Creating a user group</li>
				<li>Assigning a domain to the user pool</li>
				<li>Creating a SageMaker work team that uses the Cognito user pool and group</li>
				<li>Adding a sample user</li>
			</ul>
			<p>Once you <a id="_idIndexMarker118"/>execute the <strong class="bold">Create a private workforce</strong> part of the notebook, you should see a private workforce defined, along with the login URL that the workers would use. If you scroll further down this part of the console, you'll also see information about the work team and any workers assigned to the team, as shown in <em class="italic">Figure 3.1</em>:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B17249_03_01.jpg" alt="Figure 3.1 – Labeling workforce shown in the SageMaker console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Labeling workforce shown in the SageMaker console</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor060"/>Listing the data to label</h2>
			<p>We need to create a <a id="_idIndexMarker119"/>manifest file that tells Ground Truth how to find the data we <a id="_idIndexMarker120"/>want to label. In the manifest, we can list references to files in S3 or we can provide text data directly.</p>
			<p>Recall that our source data is in JSON format. Each source file contains multiple entries that look like this:</p>
			<p class="source-code">{"date":{"utc":"2021-03-20T19:00:00.000Z","local":"2021-03-20T23:00:00+04:00"},"parameter":"pm25","value":32,"unit":"µg/m³","averagingPeriod":{"val</p>
			<p class="source-code">ue":1,"unit":"hours"},"location":"US Diplomatic Post: Dubai","city":"Dubai","country":"AE","coordinates":{"latitude":25.25848,"longitude":55.309166</p>
			<p class="source-code">},"attribution":[{"name":"EPA AirNow DOS","url":"http://airnow.gov/index.cfm?action=airnow.global_summary"}],"sourceName":"StateAir_Dubai","sourceT</p>
			<p class="source-code">ype":"government","mobile":false}</p>
			<p>We cannot pass in links to individual files, as each file contains multiple records to label. Rather, we will summarize each record directly in the manifest file. Each line in the manifest will contain the air quality metric and location:</p>
			<p class="source-code">{"source": "pm25,35.8,µg/m³,40.01,116.333"}</p>
			<p>The <strong class="source-inline">Create a manifest file</strong> notebook section will write out a manifest for a set of records. Since you are the only worker you have, we limit the number of records to 20 by default (more on this in the next sec<a id="_idTextAnchor061"/>tion).</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor062"/>Creating the workflow</h2>
			<p>In order to create <a id="_idIndexMarker121"/>a custom workflow, we need the following:</p>
			<ul>
				<li>A <em class="italic">Lambda function that can take one entry from the manifest and inject variables into the UI</em>. In this case, we will simply map the items in the manifest text entries into a metric label to display along with a geolocation.</li>
				<li>A <em class="italic">UI template that displays the data sensibly for a worker</em>. In this case, we have a simple UI template that presents the metric along with a map showing the location where the metric was collected. <p class="callout-heading">Note</p><p class="callout">For the purposes of this book, we are using map tiles from OpenStreetMap. Do not use these tiles for production use cases. Instead, use a commercial provider such as Google Maps or Here.</p></li>
				<li>A <em class="italic">Lambda function that consolidates annotations from multiple workers</em>. We simply do<a id="_idIndexMarker122"/> a pass-through here since we only have one worker in our sample workforce.</li>
			</ul>
			<p>The notebook section <strong class="source-inline">Create a custom workflow</strong> walks you through these steps:</p>
			<ul>
				<li>Defining IAM roles for the workflow and the Lambda function</li>
				<li>Uploading the user interface template and the Lambda processing code to S3</li>
				<li>Creating the pre- and post-processing Lambda functions</li>
				<li>Defining the labeling job</li>
			</ul>
			<p>Once the labeling job is created, you can log in to the labeling portal URL (see <em class="italic">Figure 1.1</em>), using the username and password you specified in the notebook. Once you open the job, you'll see a UI like <em class="italic">Figure 3.2</em>:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B17249_03_02.jpg" alt="Figure 3.2 – Labeling UI showing the location of a weather station. The locations are shown in the local language"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Labeling UI showing the location of a weather station. The locations are shown in the local language</p>
			<p>You'll see a map<a id="_idIndexMarker123"/> showing the location of the measurement and the actual measurement. You can pick <em class="italic">good</em> or <em class="italic">bad</em> to specify whether you think the measurement represents a good or bad air quality day. After you have labeled all of the metrics, your job will show as complete, and you'll see the label for each data point, as shown in <em class="italic">Figure 3.3</em>:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B17249_03_03.jpg" alt="Figure 3.3 – Completed labeling job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Completed labeling job</p>
			<p>We'll describe how to use the labeling output in the next chapter. You'll see examples of the labeling output in the notebook that goes with this c<a id="_idTextAnchor063"/>hapter.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor064"/>Improving labeling quality using multiple workers</h1>
			<p>Relying on a <a id="_idIndexMarker124"/>single opinion for a <a id="_idIndexMarker125"/>subjective evaluation is risky. In some cases, labeling seems straightforward; telling a car from an airplane when labeling transportation pictures is pretty simple. But let's go back to our weather data. If we're labeling air quality as good or bad based on a measurement that's not intuitive, such as the level of particulate matter (PM25), we may find that a worker's opinion depends greatly on the advice we give them and their preconceptions. If a worker believes that a certain city or country has <em class="italic">dirty air</em>, they are likely to favor a <em class="italic">bad</em> label in ambiguous cases. And these biases have real consequences – some governments are very sensitive to the idea that their air quality is bad!</p>
			<p>One way to combat this problem is to use multiple workers to label each item and somehow combine the scores. In the notebook section called <strong class="source-inline">Add another worker</strong>, we'll add a second worker to our private workforce. Then in the <strong class="source-inline">Launch labeling job for multiple workers</strong> section, we'll create a new labeling job. Once the new job is ready, log in as both workers and label the small set of data we've selected. </p>
			<p>What <a id="_idIndexMarker126"/>happens now? We'll need to adjust <a id="_idIndexMarker127"/>our post-processing Lambda to consolidate the annotations. We could use a variety of strategies for the consolidation. For example, we could use a majority voting scheme, with ties being assigned to a <em class="italic">mixed</em> category. In this chapter, we'll simply use the latest annotation as the winner since we only have t<a id="_idTextAnchor065"/>wo workers.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor066"/>Using active learning to reduce labeling time </h1>
			<p>Now that we've set up a<a id="_idIndexMarker128"/> labeling workflow, we<a id="_idIndexMarker129"/> need to think about scale. If our dataset has more than 5,000 records, it's likely that Ground Truth can learn how to label for us. (You need at least 1,250 labeled records for automatic labeling, but at least 5,000 is a good rule of thumb.) This happens in an iterative process, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B17249_03_04.jpg" alt="Figure 3.4 – Auto-labeling workflow&#13;ytt&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Auto-labeling workflow</p>
			<p>When you create a labeling job using automatic labeling, Ground Truth will select a random sample of input data for manual labeling. If at least 90% of these items are labeled <a id="_idIndexMarker130"/>without error, Ground Truth will<a id="_idIndexMarker131"/> split the labeled data into a training and validation set. It will train a model and compute a confidence score, then attempt to label the remaining data. If the automatically generated labels are beneath the confidence threshold, it will refer them to workers for human review. This process repeats until the entire dataset is labeled. While this process is difficult to simulate, it provides an iterative method to improve automatic labeling with human input.</p>
			<p>As a concluding note to this section, you may wonder what the difference is between a model that can automatically label data and a more general-purpose ML model. There's a fine line here. Keep in mind that the data we use for Ground Truth may not be completely representative of the data we see in production. Our goal for a generic ML model is a model that can produce accurate inferences without an<a id="_idTextAnchor067"/>y human input.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor068"/>Security and permissions</h1>
			<p>While some data is not sensitive, most companies would not want to expose their data to the public during the labeling process. In this section, we'll cover data access control, encryption, and workforce management for data labeling.</p>
			<p>You should follow the <a id="_idIndexMarker132"/>principle of least-privileged access when using Ground Truth (or any other cloud service). Restrict the users who are allowed to create labeling jobs, and restrict users allowed to create labeling jobs using non-private workforces. In a custom labeling job, explicitly provide invoke permissions to your<a id="_idIndexMarker133"/> Lambda functions. Restrict labeling job access to only the appropriate S3 buckets and prefixes. </p>
			<p>When you run a labeling job, Ground Truth will always encrypt the output in S3. You can use the S3-managed key or provide your own KMS key. For non-sensitive data, the default S3 managed key is adequate. If you have sensitive data, consider using separate KMS keys for different datasets, as that provides another layer of security. You can also use a KMS key to encrypt the storage volumes on instances used for automatic labeling.</p>
			<p>When managing your workforce, you should restrict access to a known-good IP address range (CIDR block). You should also use the worker tracking features to log which workers are accessing data. When using Cognito for authentication, make use of strong password policies and multi-factor authentication. In most cases, large companies will prefer to use their own identity provider for workforce management.</p>
			<p>Finally, note that you'll need to <a id="_idIndexMarker134"/>add <strong class="bold">CORS</strong> (<strong class="bold">cross-origin resource sharing</strong>) configuration to your S3 buckets involved in labeling jobs, as described in the documentation (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sms-cors-update.html">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-cors-update.html</a>).</p>
			<p>Before we head toward the summary, do have a look at the following table as it summarizes some of the best practices<a id="_idIndexMarker135"/> for<a id="_idTextAnchor069"/> data labeling:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B17249_03_Table_1.jpg" alt="Figure 3.5 – Summary of data labeling best practices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Summary of data labeling best practices</p>
			<p>With this, we now come to the end of the chapter.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor070"/>Summary</h1>
			<p>In this chapter, we started digging into our weather dataset, focusing on the problem of data labeling. We learned how to use SageMaker Ground Truth to label large datasets using a combination of human review and automation, how to use custom workflows to aid the labeling process, and how to fight labeling bias by using multiple opinions. We ended with some advice on making sure that the labeling process is secure. </p>
			<p>In the next chapter, we'll explore data preparation. We'll run a feature engineering processing job on the full dataset.</p>
		</div>
	</body></html>