- en: Chapter 8. Feature Selection and Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In software engineering, there is an old saying: *make it work first, then
    make it fast*. In this book, we have adopted the strategy to *make it run, then
    make it better*. Many of the models that we covered in the initial chapters were
    correct in a very limited sense and could stand some optimization to make them
    more correct. This chapter is all about *making it better*.'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [Chapter 5](part0036_split_000.html#12AK81-a18db0be6c20485ba81f22e43ca13055
    "Chapter 5. Time Out – Obtaining Data"), *Time Out – Obtaining Data*, obtaining
    and shaping the data (which is often the largest problem in many projects) is
    a snap using F# type providers. However, once our data is local and shaped, our
    work in preparing the data for machine learning is not complete. There might still
    be abnormalities in each frame. Things like null values, empty values, and values
    outside a reasonable range need to be addressed. If you come from an R background,
    you will be familiar with `null.omit` and `na.omit`, which remove all of the rows
    from a data frame. We can achieve functional equivalence in F# by applying a filter
    function to the data. In the filter, you can search for null if it is a reference
    type, or `.isNone` if the column is an option type. While this is effective, it
    is a bit of a blunt hammer because you are throwing out a row that might have
    valid values in the other fields when only one field has an inappropriate value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to handle missing data is to replace it with a value that will
    not skew an analysis. Like most things in data science, there are plenty of opinions
    on the different techniques, and I won''t go into too much detail here. Rather,
    I want to make you aware of the issue and show you a common way to remediate it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go into Visual Studio and create a Visual F# Windows Library project called
    `FeatureCleaning`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning data](img/00104.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Locate `Script1.fsx` in the **Solution Explorer** and rename it `CleanData.fsx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning data](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Open that script file, and replace the existing code with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the FSI gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`User` is a record type that represents the users of an application while `users`
    is an array of three users. It looks pretty vanilla except user 3, Sally Price,
    has an age of `1000.0`. What we want to do is take that age out but still keep
    Sally''s record. To do that, let''s remove 1,000 and replace it with the average
    of the ages of all of remaining users. Go back to the script file and enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the FSI should give you the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we create a subarray of the valid users and then get their average
    ages. We then create a subarray of invalid users and map in the average age. Since
    F# does not like mutability, we create a new record for each of the invalid users
    and use the `with` syntax effectively, creating a new record that has all the
    same values as the original record, except the age. We then wrap up by concatenating
    the valid users and the updated user back into a single array. Although this is
    a fairly rudimentary technique, it can be surprisingly effective. As you get further
    into machine learning, you will develop and refine your own techniques for dealing
    with invalid data—and you have to keep in mind that the model that you are using
    will dictate how you clean that data. In some models, taking the average might
    throw things off.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we are confronted with a large number of independent variables, we often
    run into the problem of which values to select. In addition, the variable might
    be binned, combined with other variables, or altered—all of which might make or
    break a particular model.
  prefs: []
  type: TYPE_NORMAL
- en: Collinearity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Collinearity is when we have multiple x variables that are highly related to
    each other; they have a high degree of correlation. When using regressions, you
    always have to be on the watch for collinearity as you can''t be sure which individual
    variable really affects the outcome variable. Here is a classic example. Suppose
    you wanted to measure the happiness of a college student. You have the following
    input variables: age, sex, money available for beer, money available for textbooks.
    In this case, there is a direct relationship between money available for beer
    and money available for textbooks. The more money spent on textbooks, the less
    there is available for beer. To solve for collinearity, you can do a couple of
    things:'
  prefs: []
  type: TYPE_NORMAL
- en: Drop one of the highly-correlated variables. In this case, perhaps drop money
    available for text books.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine correlated variables into a single variable. In this case, perhaps just
    have a category of money in checking account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A common way to test for collinearity is to run your multiple regressions several
    times, each time removing one `x` variable. If there is not a dramatic change
    when two different variables are removed, they are good candidates for collinearity.
    In addition, you can always do a visual scan of the correlation matrix of the
    `x` variables, which you can do using Accord.Net with the `Tools.Corrlelation`
    method. Let''s take a look at this. Go back into Visual Studio and add a new script
    file called `Accord.fsx`. Open the NuGet Package Manager Console and add in Accord:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This represents three students who we interviewed. We asked each their age,
    their gender, how much money they had for textbooks, and how much money they had
    for beer. The first student is a 19-year-old, female, had $50.00 for text books,
    and $10.00 for beer.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you send this to the FSI, you get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a bit hard to read, so I reformatted it:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Age | Gender | $ Books | $ Beer |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Age** | 1.0 | 0.76 | -0.84 | 0.88 |'
  prefs: []
  type: TYPE_TB
- en: '| **Sex** | 0.76 | 1.0 | -0.28 | 0.35 |'
  prefs: []
  type: TYPE_TB
- en: '| **$ Books** | -0.84 | -0.28 | 1.0 | -0.99 |'
  prefs: []
  type: TYPE_TB
- en: '| **$ Beer** | 0.88 | 0.35 | -0.99 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Notice the diagonal values in matrix, 1.0, which means that age is perfectly
    correlated with age, sex is perfectly correlated with sex, and so on. The key
    thing from this example is that there is an almost perfect negative correlation
    between the amount of money for books and the amount of money for beer: it is
    `-0.99`. What this means is that, if you have more money for books, you have less
    for beer, which makes sense. By reading the correlation matrix, you can get a
    quick understanding of what variables are correlated and can possibly be removed.'
  prefs: []
  type: TYPE_NORMAL
- en: A related topic to collinearity is to always keep your `y` variable as independent
    as possible from the `x` variable. For example, if you made a regression where
    you were trying to pick the amount of money available for beer for our student,
    you would not pick any independent variable that related to the amount of money
    the student has. Why? Because they are measuring the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A related topic to collinearity is feature selection. If you have a whole mess
    of `x` variables, how do you decide which ones will be the best ones for your
    analysis? You can start picking and choosing, but that is time-consuming and can
    possibly lead to errors. Instead of guessing, there are some modeling techniques
    that run simulations across all your data to determine the best combination of
    `x` variables to use. One of the most common techniques is called forward-selection
    step-wise regression. Consider a data frame that has five independent variables
    and one dependent variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature selection](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using forward-selection step-wise regression, the technique starts out with
    a single variable, runs a regression, and calculates (in this case) a rmse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature selection](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, the technique goes back and adds in another variable and calculates the
    rmse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature selection](img/00108.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, the technique goes back and further adds in another variable and calculates
    the rmse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature selection](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: By now, you probably have the idea. Depending on the implementation, the stepwise
    might be re-run with different combinations of independent variables and/or different
    test and training sets. When the step-wise is done, you can have a good idea about
    what features are important and what can be discarded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a step-wise regression example in Accord. Go back to
    your script and enter this code (note that this is verbatim from the Accord help
    file on stepwise regression):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Send this to the FSI to get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can tell from the comments in the code, the inputs are 20 fictional people
    that have been recently screened for cancer. The features are their ages and whether
    or not they smoke. The output is whether the person actually did have cancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go back to the script and add this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When you send this to the FSI, you will see something very interesting. The
    `full.Coefficients` returns all of the variables but the `best.Coefficients` returns
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can now see that `Smoking` is the most important variable when predicting
    cancer. If two or more variables were considered important, Accord would have
    told you the number 1 variable, then the next one, and so on. Stepwise regressions
    are a bit on the outs these days as the community has moved to Lasso and some
    other techniques. However, it is still an important tool in your toolkit and is
    something that you should know about.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes our models can be improved by adjusting the data. I am not talking
    about "adjusting numbers" in the Enron accounting or US politician sense. I am
    talking about adjusting the data using some standard scientific techniques that
    might improve the model's accuracy. The general term for this is *normalization*.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different ways to normalize data. I want to show you two common
    ones that work well with regressions. First, if your data is clustered together,
    you can take the log of the values to help tease out relationships that might
    otherwise be hidden. For example, look at our scatterplot of product reviews from
    the beginning of [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*. Notice that
    most of the order quantity centered around 250 to 1,000.
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalization](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'By applying the log to the order quantity and doing the same kind of scatterplot,
    you can see the relationship much more clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalization](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that taking the log typically does not change the relationship among the
    dependent and independent variables, so you can use it safely in replacement of
    the natural values in regressions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you go back to the solution in [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*, you can
    open up the regression project and add a new file called `Accord.Net4.fsx`. Copy
    and paste in the contents from `Accord.Net2.fsx`. Next, replace the data reader
    lines of code with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Notice the change. We are taking the `log()` of our `x` variables. Also, notice
    that our `r2` slightly decreases. The reason for this is that although the log
    does not change the relationship among `AvgReviews`, it does impact how it relates
    to the other `x` variables and potentially the `y` variable. You can see, in this
    case, that it didn't do much.
  prefs: []
  type: TYPE_NORMAL
- en: Besides using log, we can trim outliers. Going back to our graph, do you notice
    that lonely dot at 2.2 average order quantity/3.90 average review?
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalization](img/00112.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at all of the other data points, we would expect that a 3.90 average
    review should have a 2.75 average order quantity at least. Although we might want
    to dive into the details to figure out what is going on, we''ll save that exercise
    for another day. Right now, what it is really doing is messing up our model. Indeed,
    the biggest criticism of regressions is that they are overly sensitive to outliers.
    Let''s look at a simple example. Go to [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*, regression
    project and create a new script, called `Accord5.fsx`. Copy the first part of
    the code from `Accord1.fsx` into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s add a child prodigy who is bored with school so he has a low GPA.
    Add in a student with an age of 10, an IQ of 150, and a GPA of 1.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending the entire script to the REPL gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Notice what happens to our model. Our `r2` moves from 0.79 to 0.66 and our rmse
    climbs from 0.18 to 0.56! Holy cow, that's dramatic! As you can guess, how you
    deal with outliers will have a large impact on your model. If the intention of
    the model is to predict a majority of students' GPAs, we can safely remove the
    outlier because it's not typical. Another way of handling outliers is to use a
    model that does a better job of dealing with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that under our belts, let''s try it with real data. Add a new script file
    and call it `AccordDotNet6.fsx`. Copy and paste all of `AccordDotNet2.fsx` into
    it. Next, locate these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `r2` moves up from 0.35 to 0.37 and our rmse drops from 2.65 to 2.59\. Quite
    an improvement for removing one data point! Feel free to move this change over
    to the AdventureWorks project if you want. I am not going to walk you through
    it, but you now have the skills to do it independently. Dropping outliers is a
    very powerful way to make regressions more accurate, but there's a cost. Before
    we start dropping data elements that don't work from our model, we have to use
    some judgement. In fact, there are textbooks devoted to the science of what to
    do with outliers and missing data. We are not going to get into that in this book,
    other than acknowledge that the issue exists and to advise you to use some common
    sense when dropping elements.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I want to acknowledge a common misperception about normalization and units of
    measure. You might notice that the different `x` variables have significantly
    different units of measure in [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*, and [Chapter
    3](part0029_split_000.html#RL0A2-a18db0be6c20485ba81f22e43ca13055 "Chapter 3. More
    AdventureWorks Regression"), *More AdventureWorks Regression*. In our examples,
    the Units of Customer Review is a 1-5 rating and the Price of Bikes is 0-10,000
    US dollars. You might think that comparing such a large range of numbers would
    adversely affect the model. Without going into details, you can be rest assured
    that regressions are immune to different units of measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, other models (especially classification and clustering models like
    k-NN, k-means, and PCA) are impacted. When we created these kinds of models in
    [Chapter 6](part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055 "Chapter 6. AdventureWorks
    Redux – k-NN and Naïve Bayes Classifiers"), *AdventureWorks Redux – k-NN and Naïve
    Bayes Classifiers*, and [Chapter 7](part0045_split_000.html#1AT9A2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 7. Traffic Stops and Crash Locations – When Two Datasets Are Better Than
    One"), *Traffic Stops and Crash Locations – When Two Datasets Are Better Than
    One*, we ran a risk that we were getting erroneous results because the data was
    not scaled. Fortunately, the features we selected, and the libraries we used (Numl.net
    and Accord), bailed us out. Numl.NET automatically scales input variables in all
    of the classification models. Depending on the type of model, Accord might scale
    for you. For example, in the PCA we wrote in [Chapter 7](part0045_split_000.html#1AT9A2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 7. Traffic Stops and Crash Locations – When Two Datasets Are Better Than
    One"), *Traffic Stops and Crash Locations – When Two Datasets Are Better Than
    One*, we passed in an input parameter called `AnalysisMethod.Center` on this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This scales the input variables to the mean, which is good enough for our analysis.
    When we did the k-NN in [Chapter 6](part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055
    "Chapter 6. AdventureWorks Redux – k-NN and Naïve Bayes Classifiers"), *AdventureWorks
    Redux – k-NN and Naïve Bayes Classifiers*, using Accord, we did not scale the
    data because our two input variables were categorical (`MartialStatus` and `Gender`)
    with only two possibilities (married or not, male or female) and you only need
    to scale continuous variables or categorical variables with more than two values.
    If we had used a continuous variable or a three-factor categorical variable in
    the k-NN, we would have had to scale it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s walk through a quick example of scaling using Accord. Open up the `FeatureCleaning`
    solution from this chapter and add a new script file called `AccordKNN`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Go into the NuGet Package Manager Console and enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Go into the `AccordKNN.fsx` file and add the code we used in [Chapter 6](part0040_split_000.html#164MG1-a18db0be6c20485ba81f22e43ca13055
    "Chapter 6. AdventureWorks Redux – k-NN and Naïve Bayes Classifiers"), *AdventureWorks
    Redux* *–**k-NN and Naïve Bayes Classifiers*, for students who study and drink
    beer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s scale the data so that studying and drinking beer are equivalent.
    We are going to take the simplest methodology of scaling called *mean scaling*.
    Go back to the script and enter this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When you send this to the REPL, you will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the inputs are now relative to their means. The person who studied
    five hours and drank one beer now studied 73% more than the average and drank
    41% less than the average. This k-NN model is now scaled and will give a better
    "apples to apples" comparison when used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and cross validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you remember from Chapters 2, 3, and 4, one of the problems with our methodology
    when building models was that we were guilty of overfitting. Overfitting, the
    bane of predictive analytics, is what happens when we build a model that does
    a great job with past data but then falls apart when new data is introduced. This
    phenomenon is not just for data science; it happens a lot in our society: Professional
    athletes get lucrative contracts and then fail to live up to their prior performances;
    fund managers get hefty salary bumps because of last year''s performance, and
    the list goes on.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation – train versus test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike the Yankees, who never seem to learn, our profession has learned from
    its mistakes and has a great, if imperfect, tool to combat overfitting. We use
    the methodology of train/test/eval to build several models and then select the
    best one not based on how well it did against an existing dataset, but how it
    does against data it has never seen before. To accomplish that, we take our source
    data, import it, clean it, and split it into two subsets: training and testing.
    We then build our model on the training set and, if it seems viable, apply our
    test data to the model. If the model is still valid, we can think about pushing
    it to production. This is represented graphically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross validation – train versus test](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'But there is one more step we can add. We can split our data several times
    and build new models to be validated. The actual splitting of the dataset is its
    own science, but typically each time the base dataset is split into **Training**
    and **Testing** subsets, the records are selected randomly. That means if you
    split your base data five times, you will have five completely different training
    and test subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross validation – train versus test](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This kind of technique can be more important than the actual model selection.
    Both Accord and Numl do some kind of splitting under the hoods and in this book,
    we will trust that they are doing a good job. However, once you start working
    on models in the wild, you will want to dedicate a certain amount of time on every
    project for cross validation.
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation – the random and mean test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Going back to our k-NN example of students that studied and drank beer, how
    do we know if we are predicting accurately? If we want to guess whether a student
    passed or not, we could just flip a coin: heads they pass, tails they fail. The
    assumption in our analysis is that the number of hours studying and the number
    of beers consumed have some kind of causality on the exam outcome. If our model
    does no better than a coin flip, then it is not a model worth using. Open up Visual
    Studio and go back to the `AccordKNN.fsx` file. At the bottom, enter in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the FSI, we get the following (your results will be different):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s enter in some information about each student: the number of hours
    they studied and the number of beers they drank and run the unscaled k-NN on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the REPL gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s see how they actually did on the exam. Add this to the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Sending this to the FSI gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Combining these arrays together in a chart, will give us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross validation – the random and mean test](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we then scored how well the random test and k-NN did predicting the actual
    results, we can see that the random test correctly predicted the result 66% of
    the time and k-NN correctly predicted the result 100% of the time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross validation – the random and mean test](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Because our k-NN did better than the random coin flip, we can consider the model
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of yes/no random test works well when our model is a logistic regression
    or a classification model like k-NN, but what about when the dependent (*Y*) variable
    is a continuous value like in a linear regression? In that case, instead of using
    a random coin flip, we can plug in the mean of the known values. If the outcome
    predicts better than the mean, we probably have a good model. If it does worse
    than the mean, we need to rethink our model. For example, consider predicting
    average bike reviews from AdventureWorks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross validation – the random and mean test](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'When you compare the predicted to the actual (taking the absolute value to
    account for being both higher and lower) and then aggregate the results, you can
    see that our linear regression did a better job in predicting the rating than
    the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross validation – the random and mean test](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you are thinking we have already done something like this in Chapters 2 and
    3, you are right—this is the same concept as the RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation – the confusion matrix and AUC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Going back to our k-NN example, imagine that we ran our k-NN against many students.
    Sometimes the k-NN guessed correctly, sometimes the k-NN did not. There are actually
    four possible outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: k-NN predicted that the student would pass and they did pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-NN predicted that the student would fail and they did fail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-NN predicted that the student would pass and they failed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-NN predicted that the student would fail and they passed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these outcomes has a special name:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predict Pass and Did Pass**: True Positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predict Fail and Did Fail**: True Negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predict Pass and Failed**: False Positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predict Fail and Passed**: False Negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And in a chart format, it would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross validation – the confusion matrix and AUC](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Sometimes the False Positive is called a Type I error and the False Negative
    is called a Type II error.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we ran the k-NN against 100 students, we could add values to that chart
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross validation – the confusion matrix and AUC](img/00121.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Reading this chart, 52 students passed the exam. Of that, we correctly predicted
    50 of them would pass, but we incorrectly predicted two of the passing students
    would fail. Similarly, 43 failed the exam (must have been a tough exam!), 40 of
    which we correctly predicted would fail, and three we incorrectly predicted would
    pass. This matrix is often called a *confusion matrix*.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this confusion matrix, we can then do some basic statistics like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy = True Positives + True Negatives / Total Population = (50 + 40)
    / 100 = 90%*'
  prefs: []
  type: TYPE_NORMAL
- en: '*True Positive Rate (TPR) = True Positives / Total Positives = 50 / 52 = 96%*'
  prefs: []
  type: TYPE_NORMAL
- en: '*False Negative Rate (FNR) = False Negatives / Total Positives = 2 / 52 = 4%*'
  prefs: []
  type: TYPE_NORMAL
- en: '*False Positive Rate (FPR) = False Positives / Total Negatives = 3 / 43 = 7%*'
  prefs: []
  type: TYPE_NORMAL
- en: '*True Negative Rate (TNR) = True Negatives / Total Negatives = 40 / 43 = 93%*'
  prefs: []
  type: TYPE_NORMAL
- en: (Note that TPR is sometimes called Sensitivity, the FNR is sometimes called
    Miss Rate, the False Positive Rate is sometimes called Fall-Out and the TNR is
    sometimes called Specificity.)
  prefs: []
  type: TYPE_NORMAL
- en: '*Positive Likelihood Ratio (LR+) = TPR / FPR = 96 % / 1 – 93% = 13.8*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Negative Likelihood Ratio (LR-) = FNR / TNR = 4% / 93% = .04*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Diagnostic Odds Ratio (DOR) = LR+ / LR- = 33.3*'
  prefs: []
  type: TYPE_NORMAL
- en: Since the DOR is greater than 1, we know that the model is working well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting this into code, we could handwrite these formulas, but Accord.Net has
    already taken care of this for us. Go back into Visual Studio and open `AccordKNN.fsx`.
    At the bottom, enter in this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'On the next line, type `confusionMatrix` and hit dot to see all of the properties
    that are available to you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cross validation – the confusion matrix and AUC](img/00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a very useful class indeed. Let''s select the odds ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And then send the entire code block to the FSI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Since our k-NN is was 100% accurate, we got an odds ratio of infinity (and beyond).
    In a real-world model, the odds ratio would obviously be much lower.
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation – unrelated variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is one more technique that I want to cover to for cross-validation—adding
    in unrelated variables and seeing the impact on the model. If your model is truly
    useful, it should be able to handle extraneous "noise" variables without significantly
    impacting the model's result. As we saw in [Chapter 2](part0024_split_000.html#MSDG2-a18db0be6c20485ba81f22e43ca13055
    "Chapter 2. AdventureWorks Regression"), *AdventureWorks Regression*, any additional
    variable will have a positive impact on most models, so this is a measure of degree.
    If adding an unrelated variable makes the model seem much more accurate, then
    the model itself is suspect. However, if the extra variable only has a marginal
    impact, then our model can be considered solid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see this in action. Go back into `AccordKNN.fsx` and add the following
    code at the bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: I added a third variable that represents each student's zodiac symbol (1.0 =
    Aquarius, 2.0 = Pisces, and so on). When I passed in the same test input (also
    with random zodiac symbols), the predictions were the same as the original k-NN.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We can conclude that the extra variable, although it had an impact at some point
    in the modeling process, was not important enough to alter our original model.
    We can then use this model with a higher degree of confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is a bit different than other machine learning books that you might
    have read because it did not introduce any new models, but instead concentrated
    on the dirty job on gathering, cleaning, and selecting your data. Although not
    as glamorous, it is absolutely essential that you have a firm grasp on these concepts
    because they will often make or break a project. In fact, many projects spend
    over 90% of their time acquiring data, cleaning the data, selecting the correct
    features, and building the appropriate cross-validation methodology. In this chapter,
    we looked at cleaning data and how to account for missing and incomplete data.
    Next, we looked at collinearity and normalization. Finally, we wrapped up with
    some common cross-validation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to apply all of these techniques in the coming chapters. Up next,
    let's go back to the AdventureWorks company and see if we can help them improve
    their production process using a machine learning model based on how the human
    brain works.
  prefs: []
  type: TYPE_NORMAL
