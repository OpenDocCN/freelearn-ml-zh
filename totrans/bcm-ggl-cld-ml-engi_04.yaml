- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Developing and Deploying ML Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发和部署机器学习模型
- en: 'In the previous chapter, we discussed the preparation stage for the ML process,
    including problem framing and data preparation. After we have framed the problem
    and have a clean dataset, it’s time to develop and deploy the ML model. In this
    chapter, we will discuss the model development process. We will start from model
    data input and hardware/software platform setup, then focus on the model development
    pipeline, including model training, validation, testing, and finally deploying
    to production. Our emphasis is on understanding the basic concepts and the thought
    processes behind them and strengthening the knowledge and skills by practicing.
    The following topics are covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了机器学习过程的准备阶段，包括问题界定和数据准备。在界定问题并拥有干净的数据集后，就到了开发并部署机器学习模型的时候了。在本章中，我们将讨论模型开发过程。我们将从模型数据输入和硬件/软件平台设置开始，然后专注于模型开发流程，包括模型训练、验证、测试，最后部署到生产环境。我们的重点是理解基本概念及其背后的思维过程，并通过实践加强知识和技能。本章涵盖了以下主题：
- en: Splitting the dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割数据集
- en: Building the platform
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建平台
- en: Training the model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Validating the model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证模型
- en: Tuning the model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型
- en: Testing and deploying the model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试和部署模型
- en: Practicing with scikit-learn
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行实践
- en: In [*Appendix 3*](B18333_13.xhtml#_idTextAnchor209), we provide practice examples
    of ML model development using the Python data science package scikit-learn.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*附录3*](B18333_13.xhtml#_idTextAnchor209)中，我们提供了使用Python数据科学包scikit-learn进行机器学习模型开发实践示例。
- en: Splitting the dataset
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割数据集
- en: 'Through the data preparation process, we have gained a dataset that is ready
    to be used for model development. To avoid model underfitting and overfitting,
    it is a best practice to split the dataset randomly yet proportionally, into independent
    subsets based on the model development process: a training dataset, a validation
    dataset, and a testing dataset:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数据准备过程，我们已经获得了一个可用于模型开发的数据集。为了避免模型欠拟合和过拟合，最佳实践是随机但按比例地将数据集分割成基于模型开发过程的独立子集：训练数据集、验证数据集和测试数据集：
- en: '**Training dataset**: The subset of data used to train the model. The model
    will learn from the training dataset.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据集**：用于训练模型的子集。模型将从训练数据集中学习。'
- en: '**Validation dataset**: The subset of data used to validate the trained model.
    Model hyperparameters will be tuned for optimization based on validation.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证数据集**：用于验证训练模型的子集。模型超参数将基于验证结果进行优化。'
- en: '**Testing dataset**: The subset of data used to evaluate a final model before
    its deployment to production.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试数据集**：用于在生产部署前评估最终模型的子集。'
- en: A common practice is to use 80 percent of the data for the training subset,
    10 percent for validation, and 10 percent for testing. When you have a large amount
    of data, you can split it into 70 percent training, 15 percent validation, and
    15 percent testing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 常规做法是使用80%的数据作为训练子集，10%用于验证，10%用于测试。当你拥有大量数据时，你可以将其分为70%的训练，15%的验证和15%的测试。
- en: Preparing the platform
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备平台
- en: While data input has a big impact on model quality, the hardware/software platform
    where we train/validate/test the model will also impact the model and the development
    process. Choosing the right platform is very important to the ML process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据输入对模型质量有重大影响，但我们训练/验证/测试模型的硬件/软件平台也会影响模型和开发过程。选择正确的平台对于机器学习过程非常重要。
- en: 'While certainly, you can choose to use a desktop or laptop for ML model development,
    it is a recommended practice to use cloud platforms, thanks to the great advantages
    that cloud computing provides: self-provisioning, on-demand, resilience, and scalability,
    at a global scale. Many tools are provided in cloud computing to assist data scientists
    in data preparation and model development.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然当然，你可以选择使用台式机或笔记本电脑进行机器学习模型开发，但鉴于云计算提供的巨大优势（自助配置、按需、弹性、可扩展性，全球规模），使用云平台是一种推荐的做法。云计算提供了许多工具，以协助数据科学家进行数据准备和模型开发。
- en: 'Among the cloud service providers, Google Cloud Platform provides great ML
    platforms to data scientists: flexible, resilient, and performant, from end to
    end. We will discuss more details of the Google Cloud ML platform in the third
    part of the book.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在云服务提供商中，Google Cloud Platform为数据科学家提供了出色的机器学习平台：灵活、弹性、性能强大，从端到端。本书的第三部分将讨论更多关于Google
    Cloud ML平台的具体细节。
- en: Now that we have prepared the datasets and ML platform, let’s dive right into
    the ML model development process, starting with model training.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据集和机器学习平台，让我们直接进入机器学习模型开发过程，从模型训练开始。
- en: Training the model
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Using the training data subset, on the platform, we train ML models to learn
    the relationships between the target and the features. ML model training is an
    iterative process: it starts from an assumed model with initial parameters and
    continues the learning process until it fits the training dataset. *Figure 4.1*
    shows a sample ML model training process, where we have selected a linear regression
    model (*z=wx+b*) and chosen the initial parameters (*w* and *b*). We calculate
    the model predict-error – the gap between the model output and the actual data
    label – this step is called forward propagation. If the error is not optimized
    (the accuracy is not within the specified range), we will need to move back and
    adjust the model’s parameters (*w* and *b*) – this step is called backward propagation.
    We will then go forward to recalculate the error again. This model training process
    repeats the steps of *forward propagation*, *backward propagation*, and *forward
    propagation* until we get a model that yields the predict-error within the expected
    range, that is, meeting the accuracy defined by the business objectives. The model
    training process is then complete.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据子集，在平台上，我们训练机器学习模型来学习目标变量和特征之间的关系。机器学习模型训练是一个迭代过程：它从一个具有初始参数的假设模型开始，并继续学习过程，直到模型与训练数据集拟合。*图4.1*展示了机器学习模型训练的一个示例过程，其中我们选择了线性回归模型（*z=wx+b*）并选择了初始参数（*w*和*b*）。我们计算模型的预测误差——模型输出与实际数据标签之间的差距——这一步称为前向传播。如果误差没有优化（准确率不在指定的范围内），我们需要回退并调整模型的参数（*w*和*b*）——这一步称为反向传播。然后我们将向前重新计算误差。这个模型训练过程会重复*前向传播*、*反向传播*和*前向传播*的步骤，直到我们得到一个在预期范围内产生预测误差的模型，即满足业务目标定义的准确率。然后模型训练过程就完成了。
- en: '![Figure 4.1 – ML model training process ](img/Figure_4.1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 机器学习模型训练过程](img/Figure_4.1.jpg)'
- en: Figure 4.1 – ML model training process
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 机器学习模型训练过程
- en: As you may have noticed, we chose a linear model (*z=wx+b*) in the previous
    example. In real life, we often use domain knowledge and certain assumptions when
    selecting an ML model. It could be linear, polynomial, or even something that
    can only be expressed by neural networks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已注意到，在先前的例子中，我们选择了线性模型（*z=wx+b*）。在现实生活中，我们在选择机器学习模型时，通常会利用领域知识和某些假设。这可能是一个线性模型、多项式模型，甚至只能用神经网络表达的东西。
- en: Next, we will look at the sample ML problems framed in the previous chapter
    (*examples 1*, *2*, and *3*), discuss linear regression and binary classification,
    and then extend them to advanced models and algorithms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将回顾上一章（*示例1*、*2*和*3*）中提出的样本机器学习问题，讨论线性回归和二元分类，并将它们扩展到高级模型和算法。
- en: Linear regression
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'In [*Chapter 3*](B18333_03.xhtml#_idTextAnchor072)*, Preparing for ML Development*,
    we talked about *example 1*: Zeellow needs to accurately predict house prices
    from their historical dataset. Since the inputs for the problem are labeled, it
    is a supervised learning problem, and since the output is a continuous value,
    it is a regression problem.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B18333_03.xhtml#_idTextAnchor072)*，准备机器学习开发*中，我们讨论了*示例1*：Zeellow需要从他们的历史数据集中准确预测房价。由于问题的输入是有标签的，这是一个监督学习问题，而且输出是一个连续值，因此这是一个回归问题。
- en: 'From a mathematical point of view, this is a typical problem of finding the
    function (relationship) between the target and the features, and the only things
    we have are the sample datasets. So, how do we figure out the function? Let’s
    inspect a simple dataset of *example 1*: the sale prices for 10 houses for a certain
    time period, at a certain location. The sample dataset is shown in *Table 4.2*.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，这是一个典型的寻找目标变量和特征之间函数（关系）的问题，而我们唯一拥有的就是样本数据集。那么，我们如何找出这个函数呢？让我们检查一个简单的*示例1*数据集：在一定时间段内、某一地点的10套房屋的销售价格。样本数据集显示在*表4.2*中。
- en: '![Table 4.2 – Example 1 housing dataset ](img/Figure_4.2.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![表4.2 – 示例1住房数据集](img/Figure_4.2.jpg)'
- en: Table 4.2 – Example 1 housing dataset
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.2 – 示例1住房数据集
- en: There are multiple features related to the target (house price). We will start
    by examining the relationship between the house price and one feature of the house.
    Let us look at the house sale price (target, denoted by *y*) and the house square
    footage (feature, denoted by *x*), leading to the topic of one variable regression.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与目标（房价）相关的特征有多个。我们将首先检查房价与房屋的一个特征之间的关系。让我们看看房屋销售价格（目标，用 *y* 表示）和房屋面积（特征，用 *x*
    表示），从而引出一元回归的主题。
- en: One variable regression
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单变量回归
- en: 'As you can see from *Table 4.2*, there are 10 rows from the sample training
    dataset, and we need to find a function *y=f(x)* that will best describe the relationship
    between target *y* and feature *x*. Naturally, we will use a diagram to visualize
    their relationship. If we put the 10 items in the dataset into a coordinate system,
    we get *Figure 4.3*. As you can see, there are 10 points distributed in the first
    quadrant and we need to make an assumption about the relationship between the
    house price (*y*) and the house square footage (*x*). Is it a linear function
    (as shown by lines l1 and l2) or a quadratic function (curve shown as d1) that
    represents the relationship of *y* and *x*? Evidently, d1 does not work since
    intuitively we know that *y* will increase when *x* increases. Now, among line
    l1 and l2, which one do we choose? This becomes a one-variable linear regression
    problem: find the best line *y=w*x+b** (with parameters *w** and *b**) that best
    fits the training dataset.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从 *Table 4.2* 中所见，样本训练数据集有 10 行，我们需要找到一个函数 *y=f(x)*，它能最好地描述目标 *y* 与特征 *x*
    之间的关系。自然，我们将使用图表来可视化它们的关系。如果我们把数据集中的 10 个项目放入坐标系中，我们得到 *Figure 4.3*。如您所见，有 10
    个点分布在第一象限，我们需要对房价（*y*）与房屋面积（*x*）之间的关系做出假设。它是线性函数（如图线 l1 和 l2 所示）还是二次函数（曲线 d1 所示）来表示
    *y* 与 *x* 的关系？显然，d1 不适用，因为我们直观地知道当 *x* 增加时，*y* 也会增加。现在，在 l1 和 l2 中，我们选择哪一个？这变成了一元线性回归问题：找到最佳拟合线
    *y=w*x+b**（参数为 *w* 和 *b**）以最好地拟合训练数据集。
- en: '![Figure 4.3 – Linear regression ](img/Figure_4.3.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 4.3 – Linear regression](img/Figure_4.3.jpg)'
- en: Figure 4.3 – Linear regression
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 4.3 – Linear regression
- en: 'Our objective here is to find the function that best fits the existing data
    and will predict the best target value (closest to the actual value) for new data.
    How do you define a *best fit*? To answer this question, we came up with the mathematical
    concept of a *cost function* to measure a model’s performance: the difference
    or distance between the predicted values and the actual values.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到最适合现有数据的函数，并预测新数据的最优目标值（最接近实际值）。你如何定义“最佳拟合”？为了回答这个问题，我们提出了数学概念“成本函数”来衡量模型的表现：预测值与实际值之间的差异或距离。
- en: 'There are several ways to measure the difference between predicted values and
    actual values. Denote (*x*i, *y*i) using the coordinate of the *i*th data point;
    that is, *y*i is the actual value for *x*i, and ![](img/B18333_04_001.png) is
    the predicted value for *x*i. The cost function can then be defined as one of
    the following (here *N* is the number of samples, *N=10* for our *example 1*):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以衡量预测值与实际值之间的差异。用坐标表示第 *i* 个数据点的 (*x*i, *y*i)；也就是说，*y*i 是 *x*i 的实际值，而
    ![img/B18333_04_001.png](img/B18333_04_001.png) 是 *x*i 的预测值。然后可以将成本函数定义为以下之一（这里
    *N* 是样本数量，对于我们的 *example 1*，*N=10*）：
- en: 'The **Mean Absolute Error** (**MAE**) is the sum of the absolute differences
    between the prediction and true values:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）是预测值与真实值之间绝对差分的总和：'
- en: '![](img/B18333_04_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![img/B18333_04_002.jpg](img/B18333_04_002.jpg)'
- en: The **Mean Squared Error** (**MSE**) is the sum of the squared differences between
    the prediction and true values:![](img/B18333_04_003.jpg)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）是预测值与真实值之间平方差分的总和：![img/B18333_04_003.jpg](img/B18333_04_003.jpg)'
- en: 'While the MSE and MAE can be both used as cost functions, there are some differences
    between them: minimizing the MAE tends to decrease the gap at each point and can
    lead some to zeros (thus removing some features and making the model scarce).
    Minimizing the MSE will avoid big gaps but will not lead to zero gaps.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 MSE 和 MAE 都可以用作成本函数，但它们之间有一些区别：最小化 MAE 倾向于在每个点上减少差距，可能导致某些值变为零（从而删除一些特征并使模型变得稀疏）。最小化
    MSE 将避免大的差距，但不会导致零差距。
- en: 'Now the problem becomes: how do we choose the right parameter (*w, b*) such
    that the MSE or MAE is minimized? We will use the MSE as the cost function, and
    our focus is to find the right parameters (*w, b*) so that the cost function MSE
    is minimized for the training dataset. To that end, we introduce the concept of
    **gradient descent**.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问题变成了：我们如何选择合适的参数 (*w, b*) 以使 MSE 或 MAE 最小化？我们将使用 MSE 作为成本函数，我们的重点是找到合适的参数
    (*w, b*)，以便在训练数据集上使成本函数 MSE 最小化。为此，我们引入了 **梯度下降** 的概念。
- en: Gradient descent
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: From a mathematical point of view, if we simplify the MSE function with just
    one variable *w*, then the diagram will be something similar to *Figure 4.4.*
    Mathematically we can find the value *w** that minimizes *f(w)* by using derivatives,
    since the derivative of *f(w)* is zero at *w**.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，如果我们只用一个变量 *w* 简化 MSE 函数，那么图表将类似于 *图 4.4.* 从数学上，我们可以通过使用导数找到最小化 *f(w)*
    的值 *w**，因为 *f(w)* 在 *w** 处的导数为零。
- en: From a computer programming point of view, we will need to use an algorithm
    called gradient descent to find the best point *w**. With one variable, a gradient
    is the derivative of the cost function. Starting from an initial point (*w*0,
    *f*0), we want to find the next point at (*w*1, *f*1) where *f*1 *= f(w*1*)* is
    smaller than *f*0*=f (w*0*)*. Since the derivative of *f(w)* at *w*0 is negative
    and we want to minimize *f*, the moving direction from *w*0 to *w*1 will be increasing,
    and the moving magnitude (also called step-size or learning rate) needs to be
    tweaked, such that it will neither be too small to cause many steps to reach *w**
    nor be too big and cause divergence away from *w**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算机编程的角度来看，我们需要使用一个称为梯度下降的算法来找到最佳点 *w**。对于单一变量，梯度是成本函数的导数。从初始点 (*w*0, *f*0)
    开始，我们希望找到下一个点 (*w*1, *f*1)，其中 *f*1 *= f(w*1*)* 小于 *f*0*=f (w*0*)*。由于 *f(w)* 在
    *w*0 处的导数是负的，而我们想最小化 *f*，因此从 *w*0 到 *w*1 的移动方向将是增加的，移动幅度（也称为步长或学习率）需要调整，以确保它既不会太小导致许多步骤才能达到
    *w**，也不会太大导致偏离 *w**。
- en: '![Figure 4.4 – Gradient descent ](img/Figure_4.4.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – 梯度下降](img/Figure_4.4.jpg)'
- en: Figure 4.4 – Gradient descent
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 梯度下降
- en: '*Figure 4.4* also shows the steps for the gradient descent algorithm: moving
    from the initial point *(w*0*, f*0*)* to *(w*1*, f*1*)*, to *(w*2*, f*2*)*, till
    it reaches the optimized point *(w*3*, f*3*) = (w*, f*)*. Note that the starting
    point is important for non-convex cost functions where multiple minimum values
    existed for *w*, as shown in *Figure 4.5*.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.4* 还显示了梯度下降算法的步骤：从初始点 *(w*0*, f*0*)* 移动到 *(w*1*, f*1*)*，到 *(w*2*, f*2*)*，直到达到优化点
    *(w*3*, f*3*) = (w*, f*)*。请注意，对于非凸成本函数，起始点很重要，因为对于 *w* 存在多个最小值，如图 4.5 所示。'
- en: '![Figure 4.5 – Non-convex cost functions ](img/Figure_4.5.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5 – 非凸成本函数](img/Figure_4.5.jpg)'
- en: Figure 4.5 – Non-convex cost functions
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 非凸成本函数
- en: 'The gradient descendent algorithm lets us find the minimum *f* value by repeatedly
    moving from *(w*i*, f*i*)* to *(w*i*+1**, f*i*+1**)*, in the direction as depicted
    by the gradient at the point *(w*i*, f*i*)*: if gradient *(w*i*)* is negative,
    move toward the direction of increasing *w*; otherwise, move toward decreasing
    *w*. After certain moves, if we can find the minimum *f** at point *(w* f*)*,
    we call the model converges at weight *w**, and we have found the parameter *w**.
    Otherwise, the model is non-convergent. After we find the converged *w**, finding
    parameter *b** is relatively easy, and we have found the best fit line: *f(x)
    = w*x + b**, which can be used to predict the house price for new values of *x*.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法允许我们通过反复从 *(w*i*, f*i*)* 移动到 *(w*i*+1**, f*i*+1**)* 来找到最小 *f* 值，移动方向由点
    *(w*i*, f*i*)* 处的梯度表示：如果梯度 *(w*i*)* 是负的，则向增加 *w* 的方向移动；否则，向减少 *w* 的方向移动。经过一定的移动后，如果我们能在点
    *(w* f*)* 处找到最小 *f**，我们称模型在权重 *w** 处收敛，并且我们已经找到了参数 *w**。否则，模型是非收敛的。在我们找到收敛的 *w**
    后，找到参数 *b** 相对容易，我们找到了最佳拟合线：*f(x) = w*x + b**，它可以用来预测新值 *x* 的房价。
- en: Extending to multiple features (variables)
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展到多个特征（变量）
- en: With the gradient descendent algorithm, we are able to find the best model that
    fits the sample dataset. We will use it to predict the sale price (target *y*)
    from new data. In real life, there are many features affecting a house’s sale
    price, such as its age, the number of bedrooms and bathrooms, and of course the
    house’s location. So, we need to extend our model to multiple features (variables).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过梯度下降算法，我们能够找到最适合样本数据集的最佳模型。我们将用它来预测新数据中的销售价格（目标 *y*）。在现实生活中，有许多特征会影响房屋的销售价格，例如其年龄、卧室和浴室的数量，当然还有房屋的位置。因此，我们需要将我们的模型扩展到多个特征（变量）。
- en: 'When extending to multiple features, we use a vector *X=(x*1*, x*2*, x*3*,
    …, x*n*)*T to represent the multiple feature values *x*1*, x*2*, x*3*, …, x*n,
    and the linear function will become this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当扩展到多个特征时，我们使用向量 *X=(x*1*, x*2*, x*3*, …, x*n*)*T 来表示多个特征值 *x*1*, x*2*, x*3*,
    …, x*n，线性函数将变为如下：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where *W* is a matrix and *B* is a vector.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *W* 是一个矩阵，*B* 是一个向量。
- en: 'Then, the cost function can be written as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代价函数可以写成如下形式：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: where *A* is a matrix constructed from the dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *A* 是从数据集中构建的矩阵。
- en: Mathematically, we can find the value *W* that minimizes *F(W)* by using partial
    derivatives with multiple variables. Accordingly, we can also extend the gradient
    descendent algorithm from one dimension to multiple dimensions, to find a matrix
    *W* that best fits the datasets by minimizing the cost function *F(W)*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以通过使用多个变量的偏导数找到使 *F(W)* 最小的 *W* 值。相应地，我们也可以将梯度下降算法从一维扩展到多维度，通过最小化代价函数
    *F(W)* 来找到最佳拟合数据集的矩阵 *W*。
- en: Extending to non-linear regression
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展到非线性回归
- en: 'In previous discussions, we assumed the relationship of the sale price (target
    *y*) and the house square footage *x* is linear. In many real-life ML models,
    there always exist non-linear relationships, for example, a polynomial relationship
    (with one feature/variable *x* here):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的讨论中，我们假设销售价格（目标 *y*）和房屋面积 *x* 之间的关系是线性的。在许多现实生活中的机器学习模型中，总是存在非线性关系，例如，一个多项式关系（这里有一个特征/变量
    *x*）：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: From linear to non-linear, the mathematical logic is the same – we need to find
    the model parameters *(w*1*, w*2*,... w*n*)* by minimizing the cost function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从线性到非线性，数学逻辑是相同的——我们需要通过最小化代价函数来找到模型参数 *(w*1*, w*2*,... w*n*)*。
- en: By extending from the linear one-variable solution to non-linear and multi-variables,
    the regression model solves the type of ML problems that predict continuous values.
    Due to the mathematical complexity of the problem, we will not discuss it further
    here. In the next section, we will look at another type of ML problem, **classification**,
    and we will start with the simplest case of binary classification.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从线性单变量解决方案扩展到非线性和多变量，回归模型解决了预测连续值的机器学习问题类型。由于问题的数学复杂性，我们在此不再进一步讨论。在下一节中，我们将探讨另一种类型的机器学习问题，**分类**，并且我们将从二元分类的最简单情况开始。
- en: Binary classification
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二元分类
- en: In the previous chapter, we also talked about another sample ML problem (*example
    2*) – Zeellow Lender is trying to automate the decision process of approval or
    denial for new loan applications. Since the model output is *yes* or *no*, this
    is a binary classification problem. Another type of classification problem is
    multi-category classification. For example, given an image, we need to tell whether
    it is a dog, a cat, or a cow.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们也讨论了另一个样本机器学习问题（*示例 2*）—— Zeellow Lender 正在尝试自动化对新贷款申请的批准或拒绝决策过程。由于模型输出是
    *是* 或 *否*，这是一个二元分类问题。另一种分类问题是多类别分类。例如，给定一个图像，我们需要判断它是狗、猫还是牛。
- en: 'For classification problems, we always use the concept of *probability*. For
    *example 2*, the ML model will output *yes* or *no*, based on the customer’s loan
    default probabilities: if the default probability is higher than a threshold value,
    we will deny the application. For the image classification example, we will say
    how probable it is that the image is of a cat, a dog, or a cow. Let us start with
    the binary problem of *example 2*.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，我们总是使用 *概率* 的概念。对于 *示例 2*，机器学习模型将根据客户的贷款违约概率输出 *是* 或 *否*：如果违约概率高于阈值，我们将拒绝申请。对于图像分类示例，我们将说明图像是猫、狗还是牛的可能性。让我们从
    *示例 2* 的二元问题开始。
- en: Logistic regression
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'From a mathematical point of view, *example 2* is a problem of finding the
    function (relationship) between the target (*yes*/*no*) and the application features,
    and the only thing we have are the sample datasets. So, how do we figure out the
    function? Let’s inspect a simple dataset we have with *example 2*: the loan application
    decisions for 10 applications for a certain time period at a certain location.
    The sample dataset is shown in *Table 4.6.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学的角度来看，*示例 2* 是一个寻找目标 (*是*/*否*) 与应用特征之间函数（关系）的问题，而我们唯一拥有的就是样本数据集。那么，我们如何找出这个函数呢？让我们检查一下我们拥有的一个简单的与
    *示例 2* 相关的数据集：在一定地点一定时间段内10个贷款申请的决定。样本数据集显示在 *表 4.6* 中。
- en: '![Table 4.6 – Example 2 loan dataset ](img/Figure_4.6.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![表 4.6 – 示例 2 贷款数据集](img/Figure_4.6.jpg)'
- en: Table 4.6 – Example 2 loan dataset
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.6 – 示例 2 贷款数据集
- en: 'From *Table 4.6*, we can see that there are many features of an applicant that
    affect their loan application’s approval or denial. To simplify, we choose the
    applicant’s income *x* as the single feature/variable and use *z* as a linear
    function of *x*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *表 4.6* 中，我们可以看到有许多申请人的特征会影响他们的贷款申请的批准或拒绝。为了简化，我们选择申请人的收入 *x* 作为单一特征/变量，并使用
    *z* 作为 *x* 的线性函数：
- en: '*z = wx + b*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*z = wx + b*'
- en: 'Since the final output target is a yes (*1*) or no (*0*), we will need a function
    that maps the above *z* value to a probability *p* that has a value between *0*
    and *1*: the probability of approving the loan (target variable *y=1*).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最终输出目标是一个 *是* (*1*) 或 *否* (*0*)，我们需要一个将上述 *z* 值映射到介于 *0* 和 *1* 之间的概率 *p* 的函数：批准贷款（目标变量
    *y=1*）的概率。
- en: 'From statistics, we have the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学中，我们有以下内容：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: And this is the so-called **sigmoid function** *(Figure 4.7)*, which maps the
    probability of loan approval *p* with the value of *z*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的 **sigmoid 函数** *(图 4.7)*，它将贷款批准的概率 *p* 与 *z* 的值映射。
- en: '![Figure 4.7 – Sigmoid function](img/Figure_4.7.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – Sigmoid 函数](img/Figure_4.7.jpg)'
- en: Figure 4.7 – Sigmoid function
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – Sigmoid 函数
- en: With the sigmoid function, the output transforms a real value *z* to a probability
    value *p*, which is between *0* and *1*. This brings in the concept of *logistic
    regression*, a classification algorithm used to predict the probability of a target
    value of yes (*1*). Simply put, logistic regression can be thought of as a linear
    regression with the output as the probability of target is *1*, ranging in (*0,1*).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Sigmoid 函数，输出将实值 *z* 转换为介于 *0* 和 *1* 之间的概率值 *p*。这引入了 *逻辑回归* 的概念，这是一种用于预测目标值为
    *是* (*1*) 的概率的分类算法。简单来说，逻辑回归可以被视为一个输出为目标概率为 *1* 的概率范围在 (*0,1*) 之间的线性回归。
- en: The threshold for binary classification
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二元分类的阈值
- en: As we see from the preceding binary classification model, logistic regression
    returns a probability value between *0* and *1*. To convert a probability value
    to a classification, we need to determine the threshold value – when the probability
    is above the threshold, the class is *1* (yes); otherwise, it is *0* (no). For
    *Example 2*, if you set the threshold as *0.9*, then you will approve the loan
    application when the probability is higher than 90%; otherwise, you will reject
    the application. But how do we define the threshold? The answer is related to
    the business case and a model measurement metric called a **confusion matrix**.
    We will discuss them more in the model validation section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从先前的二元分类模型中看到的那样，逻辑回归返回一个介于 *0* 和 *1* 之间的概率值。要将概率值转换为分类，我们需要确定阈值值——当概率高于阈值时，类别为
    *1*（是）；否则，为 *0*（否）。对于 *示例 2*，如果你将阈值设置为 *0.9*，那么当概率高于 90% 时，你将批准贷款申请；否则，你将拒绝申请。但我们是如何定义阈值的呢？答案是相关的业务案例和一个称为
    **混淆矩阵** 的模型度量指标。我们将在模型验证部分进一步讨论它们。
- en: Extending to multi-class classification
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展到多类分类
- en: 'We can extend binary classification problems to multi-class classification
    problems. There are different ways to go from binary to multi-class. Given a multi-classification
    model, we can decompose it into multiple binary classification problems. Please
    refer to the following link for more details:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将二元分类问题扩展到多类分类问题。从二元到多类的转换有多种方法。给定一个多分类模型，我们可以将其分解为多个二元分类问题。更多详情请参阅以下链接：
- en: '[https://svivek.com/teaching/lectures/slides/multiclass/multiclass-full.pdf](https://svivek.com/teaching/lectures/slides/multiclass/multiclass-full.pdf)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://svivek.com/teaching/lectures/slides/multiclass/multiclass-full.pdf](https://svivek.com/teaching/lectures/slides/multiclass/multiclass-full.pdf)'
- en: So far, we have discussed regression and classification problems and introduced
    the gradient descent algorithm. Now let’s look at some advanced algorithms.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了回归和分类问题，并介绍了梯度下降算法。现在让我们看看一些高级算法。
- en: Support vector machine
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机
- en: 'One popular advanced ML algorithm is called **support vector machine**, or
    **SVM**. It is a model commonly used for classification problems. The idea of
    SVM is simple: the algorithm finds a line (two dimensions) or a hyperplane (three
    or more dimensions) that separates the data into different classes.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的先进机器学习算法被称为**支持向量机**，或**SVM**。它是一个常用于分类问题的模型。SVM的想法很简单：算法找到一个线（二维）或超平面（三维或更多维度），将数据分成不同的类别。
- en: 'Let’s use a two-dimension separation problem to illustrate SVM. As shown in
    *Figure 4.8*, we are trying to find a line that separates the points into two
    groups: a group of circles and a group of squares. There are three lines separating
    the points. Out of the three lines, which one is the best choice?'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个二维分离问题来说明SVM。如图*图4.8*所示，我们试图找到一条线，将点分成两组：一组圆和一组正方形。有三条线在分隔点。在这三条线中，哪一条是最好的选择？
- en: '![Figure 4.8 – SVM illustration ](img/Figure_4.8.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8 – SVM示意图](img/Figure_4.8.jpg)'
- en: Figure 4.8 – SVM illustration
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – SVM示意图
- en: Let’s take a closer look at the diagram. For each line, there are points that
    are closest to the line from both classes – we call these points **support vectors**,
    and we call the distance between the line and its support vectors the **margin**.
    The objective of SVM is to maximize the margin. Out of the three lines in *Figure
    4.8*, you can see that line 1 is our choice since it has the greatest margin out
    of the three.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这张图。对于每条线，都有来自两个类别的最近点——我们称这些点为**支持向量**，我们称线与其支持向量之间的距离为**间隔**。SVM的目标是最大化间隔。在*图4.8*中的三条线中，你可以看到线1是我们的选择，因为它是三条线中间隔最大的。
- en: '![Figure 4.9 – Non-linear curve separating the data ](img/Figure_4.9.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图4.9 – 分隔数据的非线性曲线](img/Figure_4.9.jpg)'
- en: Figure 4.9 – Non-linear curve separating the data
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 分隔数据的非线性曲线
- en: If we extend this two-dimensional problem to three dimensions, then a hyperplane
    for which the margin is maximum is the optimal hyperplane. Nevertheless, both
    are still linear models. However, in real life, the separation is often not linear.
    *Figure 4.9* shows an example where the separation is a circle (non-linear).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个二维问题扩展到三维，那么具有最大间隔的超平面是最佳超平面。然而，两者仍然是线性模型。然而，在现实生活中，分离往往是线性的。*图4.9*显示了一个分离是圆（非线性）的例子。
- en: Decision tree and random forest
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树和随机森林
- en: For a classification ML problem with multiple features, the natural thing to
    think of doing would be to classify the feature values – do a binary classification
    at each feature. Another popular ML algorithm, the decision tree model, uses this
    logic to construct a decision tree, in which each internal node represents a test
    on a feature, each leaf node represents a class label, and the branches represent
    feature combinations that lead to the leaf nodes – the class labels. From root
    to leaf, the paths represent classification rules. Decision trees are constructed
    to split a dataset based on different conditions for each feature.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有多个特征的分类机器学习问题，自然而然的想法是对特征值进行分类——在每个特征上进行二进制分类。另一种流行的机器学习算法，决策树模型，使用这种逻辑来构建决策树，其中每个内部节点代表对特征的测试，每个叶节点代表一个类标签，分支代表导致叶节点的特征组合——即类标签。从根到叶，路径代表分类规则。决策树是根据每个特征的不同条件来分割数据集的。
- en: 'Decision tree is one of the most widely used methods for supervised learning.
    *Figure 4.10* illustrates a logic flow of a decision tree for *example 2* discussed
    earlier: the loan application decision process. The decision tree starts from
    the credit history:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是监督学习中应用最广泛的方法之一。*图4.10*展示了之前讨论的*示例2*中决策树的逻辑流程：贷款申请决策过程。决策树从信用历史开始：
- en: If the credit history (score) is good, it will check the applicant’s income
    and the loan amount – if the income is low and the loan amount is big, it will
    reject the application. Otherwise, the loan application will be approved.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果信用历史（评分）良好，它将检查申请人的收入和贷款金额——如果收入低且贷款金额大，它将拒绝申请。否则，贷款申请将被批准。
- en: 'If the credit history is bad, it will check the applicant’s income and loan
    amount – if the income is high and the loan amount is small, it will approve the
    application. Otherwise, the loan application will be rejected:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果信用历史不良，它将检查申请人的收入和贷款金额——如果收入高且贷款金额小，它将批准申请。否则，贷款申请将被拒绝：
- en: '![Figure 4.10 – Decision tree for example 2 ](img/Figure_4.10.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图4.10 – 示例2的决策树](img/Figure_4.10.jpg)'
- en: Figure 4.10 – Decision tree for example 2
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – 示例2的决策树
- en: With the decision tree model, you can predict results based on new feature data,
    such as a new application’s feature values. However, in a situation where the
    dataset size is large, then a decision tree can be complex and may lead to overfitting.
    To tackle the problem, we often use random forest, which consists of many decision
    trees. Random forest gets the predictions from these individual decision trees
    and does a final optimization by combining the decision tree prediction values
    with a voting or averaging process. Random forest is usually better than a single
    decision tree because it avoids overfitting using the averaging or voting mechanism.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树模型，你可以根据新的特征数据预测结果，例如新应用的特性值。然而，在数据集大小很大的情况下，决策树可能会变得复杂，并可能导致过拟合。为了解决这个问题，我们通常使用随机森林，它由许多决策树组成。随机森林从这些单个决策树中获得预测，并通过结合决策树预测值以及投票或平均过程进行最终优化。随机森林通常比单个决策树更好，因为它通过平均或投票机制避免了过拟合。
- en: 'In this section, we discussed various ML model training algorithms: from one-variable
    linear regressions to multi-variable non-linear regressions; from binary classifications
    to multi-class classifications; from support vector machine to decision trees
    and random forest. The result of ML training is a model that fits the training
    dataset well. Will such a model make good predictions on new production data?
    The answer is that we need to validate the model using the validation dataset
    before deploying the model to test and predict production data.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了各种机器学习模型训练算法：从单变量线性回归到多变量非线性回归；从二元分类到多类分类；从支持向量机到决策树和随机森林。机器学习训练的结果是一个很好地拟合训练数据集的模型。这样的模型在新生产数据上做出良好预测的可能性如何？答案是，在将模型部署到测试和预测生产数据之前，我们需要使用验证数据集来验证模型。
- en: Validating the model
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证模型
- en: 'After you train your model, you will need to determine whether it will perform
    well in predicting the target on future new data, and that is the validation process:
    you must validate the model performance on a labeled dataset that was not used
    in training – the validation dataset that was built during the dataset splitting
    phase.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在你训练模型之后，你需要确定模型在预测未来新数据的目标时是否会表现良好，这就是验证过程：你必须在一个未用于训练的标记数据集上验证模型性能，这个验证数据集是在数据集分割阶段构建的。
- en: Model validation
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型验证
- en: 'Recall what we have discussed in [*Chapter 3*](B18333_03.xhtml#_idTextAnchor072):
    in the ML problem framing phase, you define the business problem and craft a business
    metric to measure model success. Now, in this model validation phase, the model
    validation metric needs to be linked to that business metric as closely as possible.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们在[*第3章*](B18333_03.xhtml#_idTextAnchor072)中讨论的内容：在机器学习问题定义阶段，你定义业务问题并制定一个业务指标来衡量模型的成功。现在，在这个模型验证阶段，模型验证指标需要尽可能紧密地与那个业务指标相联系。
- en: Earlier in this chapter, we have defined the cost function, which is used to
    find the optimized model. The cost function is also used for ML model validation.
    For regression problems, the cost function (the gap between the model value and
    the actual value) is usually the MSE, which was discussed in the previous section.
    For binary classification, the cost function is usually expressed in a metric
    called a confusion matrix. Let’s take a closer look at the confusion matrix, as
    well as the impact of changing the classification threshold on the confusion matrix.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早期，我们已经定义了成本函数，该函数用于找到最优模型。成本函数也用于机器学习模型验证。对于回归问题，成本函数（模型值与实际值之间的差距）通常是均方误差（MSE），这在上一节中已经讨论过。对于二元分类，成本函数通常用混淆矩阵来表示。让我们更详细地看看混淆矩阵，以及改变分类阈值对混淆矩阵的影响。
- en: Confusion matrix
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A confusion matrix is used to measure a binary classification model’s predictions,
    as shown in *Figure 4.11.*
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵用于衡量二元分类模型的预测，如图4.11所示。
- en: '![Figure 4.11 – Confusion matrix ](img/Figure_4.11.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图4.11 – 混淆矩阵](img/Figure_4.11.jpg)'
- en: Figure 4.11 – Confusion matrix
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 混淆矩阵
- en: 'Based on whether the model predicts the actual classes, there are four situations:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型是否预测了实际类别，有四种情况：
- en: '**True Positive** (**TP**) is where the model *correctly* predicts the *positive*
    class.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例**（**TP**）是指模型**正确地**预测了**正类**。'
- en: '**True Negative** (**TN**) is where the model *correctly* predicts the *negative*
    class.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负例**（**TN**）是指模型**正确地**预测了**负类**。'
- en: '**False Positive** (**FP**) is where the model *incorrectly* predicts the *positive*
    class.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假正例**（**FP**）是指模型**错误地**预测了**正类**。'
- en: '**False Negative** (**FN**) is where the model *incorrectly* predicts the *negative*
    class.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假负例**（**FN**）是指模型**错误地**预测了**负类**。'
- en: 'Let’s look at a computer vision ML problem: you trained two models for image
    recognition – to classify an image as being of a cat or not. You have run the
    two models on the validation dataset and compared the results with the labels,
    and *Figure 4.12* shows the confusion matrices for the two ML models. How do we
    measure which model performs better?'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个计算机视觉机器学习问题：你训练了两个模型进行图像识别 – 将图像分类为猫或非猫。你已经在这两个模型上运行了验证数据集，并将结果与标签进行了比较，*图
    4.12* 显示了两个机器学习模型的混淆矩阵。我们如何衡量哪个模型表现更好？
- en: '![Figure 4.12 – Confusion matrices for two ML models ](img/Figure_4.12.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – 两个机器学习模型的混淆矩阵](img/Figure_4.12.jpg)'
- en: Figure 4.12 – Confusion matrices for two ML models
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 两个机器学习模型的混淆矩阵
- en: 'To help us compare the classification model performances, we need to define
    more metrics:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们比较分类模型的性能，我们需要定义更多的指标：
- en: '`Recall` (`sensitivity`) measures the proportion of actual positives that were
    identified correctly:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`召回率`（`灵敏度`）衡量的是实际正例中被正确识别的比例：'
- en: '[PRE4]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Specificity` measures the proportion of actual negatives that were identified
    correctly:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`特异性`衡量的是实际负例中被正确识别的比例：'
- en: '[PRE5]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Applying the preceding metrics to the two models, we have come up with the
    following table:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述指标应用于两个模型，我们得到了以下表格：
- en: '![Table 4.13 – Recall and specificity for two ML models ](img/Figure_4.13.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![表 4.13 – 两个机器学习模型的召回率和特异性](img/Figure_4.13.jpg)'
- en: Table 4.13 – Recall and specificity for two ML models
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.13 – 两个机器学习模型的召回率和特异性
- en: Depending on the business goal, these two models’ performance can be measured
    and interpreted from different points of view. If the goal is to identify as many
    cats as possible and the amount of false positives does not matter, then model
    two is better performed since it has a high recall metric. However, if your goal
    is to identify the not-cats, then model one may be a better choice since it has
    a high specificity metric.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 根据业务目标，可以从不同的角度衡量和解释这两个模型的性能。如果目标是尽可能多地识别猫，并且假正例的数量并不重要，那么第二个模型表现更好，因为它有较高的召回率指标。然而，如果你的目标是识别非猫，那么第一个模型可能是一个更好的选择，因为它有较高的特异性指标。
- en: You can use more metrics to help with your decision making. Next, we introduce
    the concepts of the receiver operating characteristic curve and the area under
    the curve.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用更多的指标来帮助你做出决策。接下来，我们将介绍接收者操作特征曲线和曲线下面积的概念。
- en: ROC curve and AUC
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROC 曲线和 AUC
- en: In previous chapters, we discussed the cutoff for converting a probability into
    a class. The threshold will impact the confusion matrix.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了将概率转换为类别的截止值。阈值将影响混淆矩阵。
- en: An **Receiver Operating Characteristic** (**ROC**) curve is a graph showing
    the **True Positive Rate** (**TPR**) and **False Positive Rate** (**FPR**), as
    two dimensions, under all threshold values.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）曲线是一个图表，显示了在所有阈值值下，**真正例率**（**TPR**）和**假正例率**（**FPR**），作为两个维度。'
- en: 'The TPR is a synonym for recall and is therefore defined as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 真正例率（TPR）是召回率的同义词，因此定义为如下：
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The FPR is defined as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 假正例率（FPR）的定义如下：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 4.14 – ROC curves ](img/Figure_4.14.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14 – ROC 曲线](img/Figure_4.14.jpg)'
- en: Figure 4.14 – ROC curves
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – ROC 曲线
- en: 'To build an ROC curve, you calculate the TPR (or recall) against the FPR for
    each threshold and plot it on a graph. *Figure 4.14* shows a sample ROC. If we
    take a close look at the graph, we will see that the point at (*0,0*) represents
    zero true positives and zero false positives. The point at (*1,1*) means that
    all the positives are correctly identified but all the negatives also incorrectly
    identified. The dotted line from (*0,0*) to (*1,1*), called a **random classifier**,
    represents *TPR=FPR*. In the diagram, the ideal line is the *perfect classifier*,
    which represents *TPR=1* with no *false positives*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建ROC曲线，您需要计算每个阈值下的TPR（或召回率）与FPR的对应值，并在图上绘制。*图4.14*展示了样本ROC。如果我们仔细观察图表，我们会看到(*0,0*)点代表零真正例和零假正例。(*1,1*)点意味着所有正例都被正确识别，但所有负例也被错误地识别。从(*0,0*)到(*1,1*)的虚线，称为**随机分类器**，代表*TPR=FPR*。在图中，理想线是*完美分类器*，代表*TPR=1*且没有*假正例*：
- en: '![Figure 4.15 – AUC: the area under the curve](img/Figure_4.15.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图4.15 – AUC：曲线下的面积](img/Figure_4.15.jpg)'
- en: 'Figure 4.15 – AUC: the area under the curve'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 – AUC：曲线下的面积
- en: 'Since the goal for a classification problem is a model that has a high TPR
    and a low FPR – as close as possible to the *perfect classifier* – we often use
    the **Area Under the Curve** (**AUC**)-ROC as a measurement of classification
    model performance: the greater the AUC-ROC, the better. A sample AUC-ROC is shown
    in *Figure 4.15*.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，目标是模型具有高TPR（真正例率）和低FPR（假正例率）——尽可能接近*完美分类器*——因此我们经常使用**曲线下面积**（**AUC**）-ROC作为分类模型性能的衡量标准：AUC-ROC越大，越好。*图4.15*展示了样本AUC-ROC。
- en: More classification metrics
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多分类指标
- en: 'As you can see, with the four numbers from the confusion matrix, you can calculate
    a model’s recall and specificity, and here we introduce accuracy, precision, and
    F1-score:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，从混淆矩阵的四个数字中，您可以计算出模型的召回率和特异性，在此我们介绍准确率、精确率和F1-score：
- en: '`Accuracy` measures the proportion of correct predictions among the total number
    of cases:'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Accuracy` 衡量在所有案例中正确预测的比例：'
- en: '[PRE8]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Precision` measures the proportion of positive identifications that are actually
    correct:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Precision` 衡量实际正确的正识别比例：'
- en: '[PRE9]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`F1-score` combines precision and sensitivity and measures the overall performance:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F1-score` 结合了精确度和灵敏度，衡量整体性能：'
- en: '[PRE10]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So far, we have introduced many classification metrics – which one should you
    choose? It really depends on the business context and goals. For a classification
    model that identifies emails as spam or not spam, while precision is good to identify
    the spam, you also want to avoid labeling a legitimate email as spam. For a classification
    model that identifies whether a patient has a terminal illness or not, it is vitally
    important to identify the illness for a patient who actually has that illness.
    In this situation, sensitivity is a better metric than precision to use.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了许多分类指标——你应该选择哪一个？这实际上取决于业务背景和目标。对于一个将电子邮件识别为垃圾邮件或非垃圾邮件的分类模型，虽然精确度有助于识别垃圾邮件，但你也不想将合法电子邮件误标为垃圾邮件。对于一个识别患者是否患有绝症的分类模型，识别实际患有该疾病的患者至关重要。在这种情况下，灵敏度比精确度是一个更好的指标。
- en: The F1 score combines precision and recall to give you one number that quantifies
    the overall performance. You might want to use the F1 score when you have a class
    imbalance but you want to preserve the equality between precision and sensitivity.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数结合精确度和召回率，给出一个量化整体性能的单一数字。当您有类别不平衡但希望保持精确度和灵敏度之间的相等性时，您可能想使用F1分数。
- en: Tuning the model
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整模型
- en: During the model validation process, we evaluate the model performances, and
    there are situations where the model does not fit the validation dataset. Let’s
    examine the different cases.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型验证过程中，我们评估模型性能，有时会出现模型不适应当验数据集的情况。让我们检查不同的情况。
- en: Overfitting and underfitting
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: While underfitting describes the situation where prediction error is not minimized,
    overfitting is the case where the model fits the training dataset very well but
    does not fit the validation dataset. An overfitting model gets a very low cost
    function value during training but poorly predicts on new data. *Figure 4.16*
    depicts the situations for underfitting, robust, and overfitting.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合描述的是预测误差没有最小化的情况，而过拟合是指模型非常适应训练数据集但不适应当验数据集的情况。一个过拟合的模型在训练过程中会得到非常低的代价函数值，但在新数据上的预测表现很差。*图4.16*描绘了欠拟合、稳健和过拟合的情况。
- en: '![Figure 4.16 – Model fittings ](img/Figure_4.16.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图4.16 – 模型拟合](img/Figure_4.16.jpg)'
- en: Figure 4.16 – Model fittings
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 – 模型拟合
- en: When we try to minimize the cost function and avoid underfitting, we need to
    make sure our model is generalized and not prone to overfitting. From our ML practice,
    we know that overfitting is often caused by making a model more complex than necessary.
    As you can see in *Figure 4.16*, overfitting makes a training model memorize data.
    The ML’s fundamental principle is making the model fit well on the data without
    losing generality. To avoid overfitting, we introduce regularization to decrease
    model complexity.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们试图最小化损失函数并避免欠拟合时，我们需要确保我们的模型是泛化的，而不是容易过拟合。从我们的机器学习实践中，我们知道过拟合通常是由于使模型比必要的更复杂而引起的。正如你在*图4.16*中看到的那样，过拟合使训练模型记住数据。机器学习的根本原则是在不失去泛化的情况下使模型很好地拟合数据。为了避免过拟合，我们引入正则化来降低模型复杂性。
- en: Regularization
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: 'To avoid overfitting, we need to reduce model complexity. Model complexity
    can be thought of in two ways:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合，我们需要降低模型复杂性。模型复杂性可以从两个方面来考虑：
- en: Model complexity as a function of the *total number of features* with nonzero
    weights
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型复杂性作为具有非零权重的*特征总数*的函数
- en: Model complexity as a function of the *weights* of all the features in the model
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型复杂性作为模型中所有特征*权重*的函数
- en: 'The idea of regularization is introduced to add a factor to penalize the model
    complexity and enhance model generalization. Corresponding to the preceding two
    complexities, we have two kinds of regularization/generalization:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的想法是为了添加一个因子来惩罚模型复杂性并增强模型泛化。对应于前面的两种复杂性，我们有两种正则化/泛化类型：
- en: 'Quantify complexity with the *L2 regularization* formula, which defines the
    regularization term as the sum of the squares of all the feature weights – weights
    close to zero have little effect on model complexity, while outlier weights can
    have a huge impact. *Ridge regression* uses L2 regularization: the cost function
    is altered by adding a penalty equivalent to the square of the weights. Let *p*
    be the number of features, and the coefficient (weight) of the *i*th feature is
    *w*i; then, the cost function is written as follows:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*L2正则化*公式量化复杂性，该公式将正则化项定义为所有特征权重平方的和——接近零的权重对模型复杂性的影响很小，而异常值权重可以产生巨大影响。*岭回归*使用L2正则化：通过添加一个相当于权重平方的惩罚来改变损失函数。设*p*为特征的数量，第*i*个特征的系数（权重）为*w*i；则，损失函数可以表示如下：
- en: '![](img/B18333_04_004.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18333_04_004.jpg)'
- en: 'Quantify complexity with the *L1 regularization* formula, which defines the
    regularization term as the sum of the absolute of all the feature weights – weights
    close to zero have a large effect on model complexity, while outlier weights have
    less impact. Lasso regression uses L1 regularization: the cost function is altered
    by adding a penalty equivalent to the absolute of the weights:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*L1正则化*公式量化复杂性，该公式将正则化项定义为所有特征权重的绝对值之和——接近零的权重对模型复杂性的影响很大，而异常值权重的影响较小。Lasso回归使用L1正则化：通过添加一个相当于权重绝对值的惩罚来改变损失函数：
- en: '![](img/B18333_04_005.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18333_04_005.jpg)'
- en: How should you choose the parameter lambda for the preceding formulas? If the
    lambda value is too high, the model will be simple and carry a risk of underfitting
    the data. If the lambda value is too low, the model will be more complex, with
    the risk of overfitting your data and leading to generalization issues with new
    data. The ideal value of lambda produces a model that fits the training data and
    generalizes well to new data. One objective of model tuning is to balance the
    model complexity and generalization.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该如何选择前面公式的参数lambda？如果lambda值过高，模型将变得简单，存在欠拟合数据的危险。如果lambda值过低，模型将更加复杂，存在过拟合数据和导致新数据泛化问题的风险。lambda的理想值产生一个既适合训练数据又对新数据泛化良好的模型。模型调整的一个目标是在模型复杂性和泛化之间取得平衡。
- en: Other than regularization, we can use early stopping to avoid overfitting. Early
    stopping is a form of regularization used to avoid overfitting when training a
    learner with an iterative method, such as gradient descent. This means ending
    the training when the training results are good enough, and before the model fully
    reaches convergence.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 除了正则化之外，我们还可以使用提前停止来避免过拟合。提前停止是一种正则化形式，用于在训练使用迭代方法（如梯度下降）的学习者时避免过拟合。这意味着在训练结果足够好时结束训练，在模型完全收敛之前。
- en: Hyperparameter tuning
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整
- en: Hyperparameter tuning is the process of finding the best version of a model
    by running many training jobs on your dataset. It uses the algorithm and ranges
    of hyperparameters that you specify, then chooses the hyperparameter values that
    result in a model that performs the best, as measured by a metric that you choose.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整是通过在数据集上运行多个训练作业来寻找模型最佳版本的过程。它使用你指定的算法和超参数范围，然后选择那些能产生最佳性能的模型超参数值，这些性能是通过你选择的指标来衡量的。
- en: There are two basic types of hyperparameters. The first kind is model hyperparameters.
    They are directly linked to the model that is selected and thus have a direct
    impact on the performance of that model. They help define the model itself, for
    example, the number of layers in a neural network model and the activation functions
    that are used.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数有两种基本类型。第一种是模型超参数。它们直接与所选模型相关联，因此对模型性能有直接影响。它们有助于定义模型本身，例如，神经网络模型中的层数和所使用的激活函数。
- en: The second kind is algorithm hyperparameters. They do not affect the performance
    of the algorithm directly but affect the efficiency and the rate of model training.
    For example, the learning rate for a gradient descent algorithm may affect how
    quickly an ML model converges.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种是算法超参数。它们不会直接影响到算法的性能，但会影响模型训练的效率和速度。例如，梯度下降算法的学习率可能会影响机器学习模型收敛的速度。
- en: 'The process of tuning hyperparameters involves changing the hyperparameter
    values and attempting to find those that yield the best results. The common hyperparameters
    that are often tuned include the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 调整超参数的过程涉及改变超参数的值，并尝试找到那些能产生最佳结果值的超参数。常见的常调超参数包括以下内容：
- en: '**The batch size**: The number of samples that are processed during training,
    before the model is updated'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：在模型更新之前，训练过程中处理的样本数量'
- en: '**The number of training epochs**: The number of times that we run through
    the full set of training data during model training'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练轮数**：在模型训练过程中，我们遍历完整训练数据集的次数'
- en: '**The learning rate**: The distance we travel when trying to find the optimal
    value for a parameter'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：在尝试找到参数最优值时我们所走过的距离'
- en: Through the ML model training, validation, and hyperparameter tuning, we have
    come up with a model that can be deployed for testing.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过机器学习模型的训练、验证和超参数调整，我们已开发出一个可以用于测试的模型。
- en: Testing and deploying the model
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和部署模型
- en: 'To test and get performance metrics from your model, you must make inferences
    or predictions from the model—which typically requires deployment. The goal of
    the deployment phase is to provide a managed environment to host models for inference
    both securely and with low latency. You can deploy your model in one of two ways:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试并从你的模型中获得性能指标，你必须从模型中进行推理或预测——这通常需要部署。部署阶段的目标是提供一个受管理的环境，以安全且低延迟的方式托管模型进行推理。你可以通过以下两种方式之一部署你的模型：
- en: '**Single predictions**: Deploy your model online with a permanent endpoint.
    For example, we can deploy the housing model (price prediction) with an online
    endpoint.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单次预测**：通过一个永久端点在线部署你的模型。例如，我们可以通过在线端点部署房价预测模型。'
- en: '`.csv` file or multiple sets of records to be sent at a time, the model will
    return a batch of predictions.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.csv` 文件或一次发送多组记录，模型将返回一批预测结果。'
- en: After deploying a model into testing, you evaluate the model to see whether
    it meets the performance requirements and the business requirements, which is
    the ultimate goal for any ML problem. All the stakeholders will need to evaluate
    the ML solution’s benefits and approve the model’s deployment to production. Keep
    in mind that the most accurate model may not be the best solution to an ML problem.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在将模型部署到测试环境中后，你需要评估模型是否满足性能要求和业务需求，这是任何机器学习问题的最终目标。所有利益相关者都需要评估机器学习解决方案的益处，并批准模型在生产环境中的部署。请记住，最精确的模型可能并不是解决机器学习问题的最佳方案。
- en: After all the stakeholders approve the model, we then deploy the model to production.
    Otherwise, we need to go back to the process of model training, validation and
    tuning, re-testing, and re-evaluation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有利益相关者批准模型后，我们随后将模型部署到生产环境。否则，我们需要回到模型训练、验证和调整、重新测试和重新评估的过程。
- en: After deploying a model to production, you still need to monitor the production
    data, since new data accumulates over time, and alternative or new outcomes can
    potentially be identified. Therefore, deploying a model is a continuous process,
    not a one-time exercise.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在将模型部署到生产环境中后，您仍然需要监控生产数据，因为随着时间的推移，新的数据会不断积累，并且可能识别出替代或新的结果。因此，部署模型是一个持续的过程，而不仅仅是一次性练习。
- en: Practicing model development with scikit-learn
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行模型开发实践
- en: Scikit-learn is one of the most useful libraries for ML in Python. The scikit-learn
    library contains a lot of tools for ML, including ones for classification and
    regression.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 是 Python 中最有用的机器学习库之一。Scikit-learn 库包含许多机器学习工具，包括分类和回归工具。
- en: In [*Appendix 3*](B18333_13.xhtml#_idTextAnchor209) of the book, we have provided
    a step-by-step practice exercise for using scikit-learn to develop ML models.
    Practicing these steps is essential to master scikit-learn skills. Please refer
    to [*Appendix 3*](B18333_13.xhtml#_idTextAnchor209), *Practicing with Scikit-Learn*,
    to learn and practice with examples of ML model training, validation, and testing
    using scikit-learn.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的 [*附录 3*](B18333_13.xhtml#_idTextAnchor209) 中，我们提供了一系列使用 scikit-learn 开发机器学习模型的逐步实践练习。掌握这些步骤对于精通
    scikit-learn 技能至关重要。请参阅 [*附录 3*](B18333_13.xhtml#_idTextAnchor209)，*使用 Scikit-Learn
    进行实践*，通过机器学习模型训练、验证和测试的示例来学习和实践。
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we have discussed the basic concepts of the ML model development
    process: data splitting, platform setup, ML model training, validation, testing,
    and deployment.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了机器学习模型开发过程的基本概念：数据拆分、平台设置、机器学习模型训练、验证、测试和部署。
- en: Since the concept of AI emerged in the 1950s, there were no big breakthroughs
    until 2012, when **deep learning** (**DL**) was invented using neural networks.
    DL has greatly improved ML model performance and opened up a huge avenue for applying
    ML to many business use cases. In the next chapter, we will discuss neural networks
    and DL.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人工智能的概念在 1950 年代出现，直到 2012 年深度学习（**DL**）通过神经网络被发明，在此之前没有出现重大突破。深度学习极大地提高了机器学习模型的表现，并为将机器学习应用于许多商业用例开辟了广阔的道路。在下一章中，我们将讨论神经网络和深度学习。
- en: Further reading
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For further insights into the topics of the chapter, you can refer to the following:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解本章的主题，您可以参考以下内容：
- en: '[https://scikit-learn.org/stable/tutorial/basic/tutorial.xhtml](https://scikit-learn.org/stable/tutorial/basic/tutorial.xhtml)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://scikit-learn.org/stable/tutorial/basic/tutorial.xhtml](https://scikit-learn.org/stable/tutorial/basic/tutorial.xhtml)'
- en: '[https://scikit-learn.org/stable/tutorial/index.xhtml](https://scikit-learn.org/stable/tutorial/index.xhtml)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://scikit-learn.org/stable/tutorial/index.xhtml](https://scikit-learn.org/stable/tutorial/index.xhtml)'
- en: '[*Appendix 3*](B18333_13.xhtml#_idTextAnchor209)*, Practicing with ScikitLearn*'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*附录 3*](B18333_13.xhtml#_idTextAnchor209)*, 使用 ScikitLearn 进行实践*'
