- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing and Deploying ML Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we discussed the preparation stage for the ML process,
    including problem framing and data preparation. After we have framed the problem
    and have a clean dataset, it’s time to develop and deploy the ML model. In this
    chapter, we will discuss the model development process. We will start from model
    data input and hardware/software platform setup, then focus on the model development
    pipeline, including model training, validation, testing, and finally deploying
    to production. Our emphasis is on understanding the basic concepts and the thought
    processes behind them and strengthening the knowledge and skills by practicing.
    The following topics are covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing and deploying the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practicing with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [*Appendix 3*](B18333_13.xhtml#_idTextAnchor209), we provide practice examples
    of ML model development using the Python data science package scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Through the data preparation process, we have gained a dataset that is ready
    to be used for model development. To avoid model underfitting and overfitting,
    it is a best practice to split the dataset randomly yet proportionally, into independent
    subsets based on the model development process: a training dataset, a validation
    dataset, and a testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training dataset**: The subset of data used to train the model. The model
    will learn from the training dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation dataset**: The subset of data used to validate the trained model.
    Model hyperparameters will be tuned for optimization based on validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing dataset**: The subset of data used to evaluate a final model before
    its deployment to production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A common practice is to use 80 percent of the data for the training subset,
    10 percent for validation, and 10 percent for testing. When you have a large amount
    of data, you can split it into 70 percent training, 15 percent validation, and
    15 percent testing.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While data input has a big impact on model quality, the hardware/software platform
    where we train/validate/test the model will also impact the model and the development
    process. Choosing the right platform is very important to the ML process.
  prefs: []
  type: TYPE_NORMAL
- en: 'While certainly, you can choose to use a desktop or laptop for ML model development,
    it is a recommended practice to use cloud platforms, thanks to the great advantages
    that cloud computing provides: self-provisioning, on-demand, resilience, and scalability,
    at a global scale. Many tools are provided in cloud computing to assist data scientists
    in data preparation and model development.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the cloud service providers, Google Cloud Platform provides great ML
    platforms to data scientists: flexible, resilient, and performant, from end to
    end. We will discuss more details of the Google Cloud ML platform in the third
    part of the book.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have prepared the datasets and ML platform, let’s dive right into
    the ML model development process, starting with model training.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the training data subset, on the platform, we train ML models to learn
    the relationships between the target and the features. ML model training is an
    iterative process: it starts from an assumed model with initial parameters and
    continues the learning process until it fits the training dataset. *Figure 4.1*
    shows a sample ML model training process, where we have selected a linear regression
    model (*z=wx+b*) and chosen the initial parameters (*w* and *b*). We calculate
    the model predict-error – the gap between the model output and the actual data
    label – this step is called forward propagation. If the error is not optimized
    (the accuracy is not within the specified range), we will need to move back and
    adjust the model’s parameters (*w* and *b*) – this step is called backward propagation.
    We will then go forward to recalculate the error again. This model training process
    repeats the steps of *forward propagation*, *backward propagation*, and *forward
    propagation* until we get a model that yields the predict-error within the expected
    range, that is, meeting the accuracy defined by the business objectives. The model
    training process is then complete.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – ML model training process ](img/Figure_4.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – ML model training process
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noticed, we chose a linear model (*z=wx+b*) in the previous
    example. In real life, we often use domain knowledge and certain assumptions when
    selecting an ML model. It could be linear, polynomial, or even something that
    can only be expressed by neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the sample ML problems framed in the previous chapter
    (*examples 1*, *2*, and *3*), discuss linear regression and binary classification,
    and then extend them to advanced models and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 3*](B18333_03.xhtml#_idTextAnchor072)*, Preparing for ML Development*,
    we talked about *example 1*: Zeellow needs to accurately predict house prices
    from their historical dataset. Since the inputs for the problem are labeled, it
    is a supervised learning problem, and since the output is a continuous value,
    it is a regression problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From a mathematical point of view, this is a typical problem of finding the
    function (relationship) between the target and the features, and the only things
    we have are the sample datasets. So, how do we figure out the function? Let’s
    inspect a simple dataset of *example 1*: the sale prices for 10 houses for a certain
    time period, at a certain location. The sample dataset is shown in *Table 4.2*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.2 – Example 1 housing dataset ](img/Figure_4.2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 4.2 – Example 1 housing dataset
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple features related to the target (house price). We will start
    by examining the relationship between the house price and one feature of the house.
    Let us look at the house sale price (target, denoted by *y*) and the house square
    footage (feature, denoted by *x*), leading to the topic of one variable regression.
  prefs: []
  type: TYPE_NORMAL
- en: One variable regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you can see from *Table 4.2*, there are 10 rows from the sample training
    dataset, and we need to find a function *y=f(x)* that will best describe the relationship
    between target *y* and feature *x*. Naturally, we will use a diagram to visualize
    their relationship. If we put the 10 items in the dataset into a coordinate system,
    we get *Figure 4.3*. As you can see, there are 10 points distributed in the first
    quadrant and we need to make an assumption about the relationship between the
    house price (*y*) and the house square footage (*x*). Is it a linear function
    (as shown by lines l1 and l2) or a quadratic function (curve shown as d1) that
    represents the relationship of *y* and *x*? Evidently, d1 does not work since
    intuitively we know that *y* will increase when *x* increases. Now, among line
    l1 and l2, which one do we choose? This becomes a one-variable linear regression
    problem: find the best line *y=w*x+b** (with parameters *w** and *b**) that best
    fits the training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Linear regression ](img/Figure_4.3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Linear regression
  prefs: []
  type: TYPE_NORMAL
- en: 'Our objective here is to find the function that best fits the existing data
    and will predict the best target value (closest to the actual value) for new data.
    How do you define a *best fit*? To answer this question, we came up with the mathematical
    concept of a *cost function* to measure a model’s performance: the difference
    or distance between the predicted values and the actual values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to measure the difference between predicted values and
    actual values. Denote (*x*i, *y*i) using the coordinate of the *i*th data point;
    that is, *y*i is the actual value for *x*i, and ![](img/B18333_04_001.png) is
    the predicted value for *x*i. The cost function can then be defined as one of
    the following (here *N* is the number of samples, *N=10* for our *example 1*):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Mean Absolute Error** (**MAE**) is the sum of the absolute differences
    between the prediction and true values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18333_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The **Mean Squared Error** (**MSE**) is the sum of the squared differences between
    the prediction and true values:![](img/B18333_04_003.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While the MSE and MAE can be both used as cost functions, there are some differences
    between them: minimizing the MAE tends to decrease the gap at each point and can
    lead some to zeros (thus removing some features and making the model scarce).
    Minimizing the MSE will avoid big gaps but will not lead to zero gaps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the problem becomes: how do we choose the right parameter (*w, b*) such
    that the MSE or MAE is minimized? We will use the MSE as the cost function, and
    our focus is to find the right parameters (*w, b*) so that the cost function MSE
    is minimized for the training dataset. To that end, we introduce the concept of
    **gradient descent**.'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From a mathematical point of view, if we simplify the MSE function with just
    one variable *w*, then the diagram will be something similar to *Figure 4.4.*
    Mathematically we can find the value *w** that minimizes *f(w)* by using derivatives,
    since the derivative of *f(w)* is zero at *w**.
  prefs: []
  type: TYPE_NORMAL
- en: From a computer programming point of view, we will need to use an algorithm
    called gradient descent to find the best point *w**. With one variable, a gradient
    is the derivative of the cost function. Starting from an initial point (*w*0,
    *f*0), we want to find the next point at (*w*1, *f*1) where *f*1 *= f(w*1*)* is
    smaller than *f*0*=f (w*0*)*. Since the derivative of *f(w)* at *w*0 is negative
    and we want to minimize *f*, the moving direction from *w*0 to *w*1 will be increasing,
    and the moving magnitude (also called step-size or learning rate) needs to be
    tweaked, such that it will neither be too small to cause many steps to reach *w**
    nor be too big and cause divergence away from *w**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Gradient descent ](img/Figure_4.4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Gradient descent
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.4* also shows the steps for the gradient descent algorithm: moving
    from the initial point *(w*0*, f*0*)* to *(w*1*, f*1*)*, to *(w*2*, f*2*)*, till
    it reaches the optimized point *(w*3*, f*3*) = (w*, f*)*. Note that the starting
    point is important for non-convex cost functions where multiple minimum values
    existed for *w*, as shown in *Figure 4.5*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Non-convex cost functions ](img/Figure_4.5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Non-convex cost functions
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient descendent algorithm lets us find the minimum *f* value by repeatedly
    moving from *(w*i*, f*i*)* to *(w*i*+1**, f*i*+1**)*, in the direction as depicted
    by the gradient at the point *(w*i*, f*i*)*: if gradient *(w*i*)* is negative,
    move toward the direction of increasing *w*; otherwise, move toward decreasing
    *w*. After certain moves, if we can find the minimum *f** at point *(w* f*)*,
    we call the model converges at weight *w**, and we have found the parameter *w**.
    Otherwise, the model is non-convergent. After we find the converged *w**, finding
    parameter *b** is relatively easy, and we have found the best fit line: *f(x)
    = w*x + b**, which can be used to predict the house price for new values of *x*.'
  prefs: []
  type: TYPE_NORMAL
- en: Extending to multiple features (variables)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the gradient descendent algorithm, we are able to find the best model that
    fits the sample dataset. We will use it to predict the sale price (target *y*)
    from new data. In real life, there are many features affecting a house’s sale
    price, such as its age, the number of bedrooms and bathrooms, and of course the
    house’s location. So, we need to extend our model to multiple features (variables).
  prefs: []
  type: TYPE_NORMAL
- en: 'When extending to multiple features, we use a vector *X=(x*1*, x*2*, x*3*,
    …, x*n*)*T to represent the multiple feature values *x*1*, x*2*, x*3*, …, x*n,
    and the linear function will become this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: where *W* is a matrix and *B* is a vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the cost function can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: where *A* is a matrix constructed from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, we can find the value *W* that minimizes *F(W)* by using partial
    derivatives with multiple variables. Accordingly, we can also extend the gradient
    descendent algorithm from one dimension to multiple dimensions, to find a matrix
    *W* that best fits the datasets by minimizing the cost function *F(W)*.
  prefs: []
  type: TYPE_NORMAL
- en: Extending to non-linear regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In previous discussions, we assumed the relationship of the sale price (target
    *y*) and the house square footage *x* is linear. In many real-life ML models,
    there always exist non-linear relationships, for example, a polynomial relationship
    (with one feature/variable *x* here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: From linear to non-linear, the mathematical logic is the same – we need to find
    the model parameters *(w*1*, w*2*,... w*n*)* by minimizing the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: By extending from the linear one-variable solution to non-linear and multi-variables,
    the regression model solves the type of ML problems that predict continuous values.
    Due to the mathematical complexity of the problem, we will not discuss it further
    here. In the next section, we will look at another type of ML problem, **classification**,
    and we will start with the simplest case of binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we also talked about another sample ML problem (*example
    2*) – Zeellow Lender is trying to automate the decision process of approval or
    denial for new loan applications. Since the model output is *yes* or *no*, this
    is a binary classification problem. Another type of classification problem is
    multi-category classification. For example, given an image, we need to tell whether
    it is a dog, a cat, or a cow.
  prefs: []
  type: TYPE_NORMAL
- en: 'For classification problems, we always use the concept of *probability*. For
    *example 2*, the ML model will output *yes* or *no*, based on the customer’s loan
    default probabilities: if the default probability is higher than a threshold value,
    we will deny the application. For the image classification example, we will say
    how probable it is that the image is of a cat, a dog, or a cow. Let us start with
    the binary problem of *example 2*.'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From a mathematical point of view, *example 2* is a problem of finding the
    function (relationship) between the target (*yes*/*no*) and the application features,
    and the only thing we have are the sample datasets. So, how do we figure out the
    function? Let’s inspect a simple dataset we have with *example 2*: the loan application
    decisions for 10 applications for a certain time period at a certain location.
    The sample dataset is shown in *Table 4.6.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.6 – Example 2 loan dataset ](img/Figure_4.6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 4.6 – Example 2 loan dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'From *Table 4.6*, we can see that there are many features of an applicant that
    affect their loan application’s approval or denial. To simplify, we choose the
    applicant’s income *x* as the single feature/variable and use *z* as a linear
    function of *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*z = wx + b*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the final output target is a yes (*1*) or no (*0*), we will need a function
    that maps the above *z* value to a probability *p* that has a value between *0*
    and *1*: the probability of approving the loan (target variable *y=1*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'From statistics, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And this is the so-called **sigmoid function** *(Figure 4.7)*, which maps the
    probability of loan approval *p* with the value of *z*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Sigmoid function](img/Figure_4.7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Sigmoid function
  prefs: []
  type: TYPE_NORMAL
- en: With the sigmoid function, the output transforms a real value *z* to a probability
    value *p*, which is between *0* and *1*. This brings in the concept of *logistic
    regression*, a classification algorithm used to predict the probability of a target
    value of yes (*1*). Simply put, logistic regression can be thought of as a linear
    regression with the output as the probability of target is *1*, ranging in (*0,1*).
  prefs: []
  type: TYPE_NORMAL
- en: The threshold for binary classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we see from the preceding binary classification model, logistic regression
    returns a probability value between *0* and *1*. To convert a probability value
    to a classification, we need to determine the threshold value – when the probability
    is above the threshold, the class is *1* (yes); otherwise, it is *0* (no). For
    *Example 2*, if you set the threshold as *0.9*, then you will approve the loan
    application when the probability is higher than 90%; otherwise, you will reject
    the application. But how do we define the threshold? The answer is related to
    the business case and a model measurement metric called a **confusion matrix**.
    We will discuss them more in the model validation section.
  prefs: []
  type: TYPE_NORMAL
- en: Extending to multi-class classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can extend binary classification problems to multi-class classification
    problems. There are different ways to go from binary to multi-class. Given a multi-classification
    model, we can decompose it into multiple binary classification problems. Please
    refer to the following link for more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://svivek.com/teaching/lectures/slides/multiclass/multiclass-full.pdf](https://svivek.com/teaching/lectures/slides/multiclass/multiclass-full.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed regression and classification problems and introduced
    the gradient descent algorithm. Now let’s look at some advanced algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One popular advanced ML algorithm is called **support vector machine**, or
    **SVM**. It is a model commonly used for classification problems. The idea of
    SVM is simple: the algorithm finds a line (two dimensions) or a hyperplane (three
    or more dimensions) that separates the data into different classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use a two-dimension separation problem to illustrate SVM. As shown in
    *Figure 4.8*, we are trying to find a line that separates the points into two
    groups: a group of circles and a group of squares. There are three lines separating
    the points. Out of the three lines, which one is the best choice?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – SVM illustration ](img/Figure_4.8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – SVM illustration
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a closer look at the diagram. For each line, there are points that
    are closest to the line from both classes – we call these points **support vectors**,
    and we call the distance between the line and its support vectors the **margin**.
    The objective of SVM is to maximize the margin. Out of the three lines in *Figure
    4.8*, you can see that line 1 is our choice since it has the greatest margin out
    of the three.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Non-linear curve separating the data ](img/Figure_4.9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Non-linear curve separating the data
  prefs: []
  type: TYPE_NORMAL
- en: If we extend this two-dimensional problem to three dimensions, then a hyperplane
    for which the margin is maximum is the optimal hyperplane. Nevertheless, both
    are still linear models. However, in real life, the separation is often not linear.
    *Figure 4.9* shows an example where the separation is a circle (non-linear).
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree and random forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a classification ML problem with multiple features, the natural thing to
    think of doing would be to classify the feature values – do a binary classification
    at each feature. Another popular ML algorithm, the decision tree model, uses this
    logic to construct a decision tree, in which each internal node represents a test
    on a feature, each leaf node represents a class label, and the branches represent
    feature combinations that lead to the leaf nodes – the class labels. From root
    to leaf, the paths represent classification rules. Decision trees are constructed
    to split a dataset based on different conditions for each feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision tree is one of the most widely used methods for supervised learning.
    *Figure 4.10* illustrates a logic flow of a decision tree for *example 2* discussed
    earlier: the loan application decision process. The decision tree starts from
    the credit history:'
  prefs: []
  type: TYPE_NORMAL
- en: If the credit history (score) is good, it will check the applicant’s income
    and the loan amount – if the income is low and the loan amount is big, it will
    reject the application. Otherwise, the loan application will be approved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the credit history is bad, it will check the applicant’s income and loan
    amount – if the income is high and the loan amount is small, it will approve the
    application. Otherwise, the loan application will be rejected:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Decision tree for example 2 ](img/Figure_4.10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Decision tree for example 2
  prefs: []
  type: TYPE_NORMAL
- en: With the decision tree model, you can predict results based on new feature data,
    such as a new application’s feature values. However, in a situation where the
    dataset size is large, then a decision tree can be complex and may lead to overfitting.
    To tackle the problem, we often use random forest, which consists of many decision
    trees. Random forest gets the predictions from these individual decision trees
    and does a final optimization by combining the decision tree prediction values
    with a voting or averaging process. Random forest is usually better than a single
    decision tree because it avoids overfitting using the averaging or voting mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we discussed various ML model training algorithms: from one-variable
    linear regressions to multi-variable non-linear regressions; from binary classifications
    to multi-class classifications; from support vector machine to decision trees
    and random forest. The result of ML training is a model that fits the training
    dataset well. Will such a model make good predictions on new production data?
    The answer is that we need to validate the model using the validation dataset
    before deploying the model to test and predict production data.'
  prefs: []
  type: TYPE_NORMAL
- en: Validating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After you train your model, you will need to determine whether it will perform
    well in predicting the target on future new data, and that is the validation process:
    you must validate the model performance on a labeled dataset that was not used
    in training – the validation dataset that was built during the dataset splitting
    phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Model validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall what we have discussed in [*Chapter 3*](B18333_03.xhtml#_idTextAnchor072):
    in the ML problem framing phase, you define the business problem and craft a business
    metric to measure model success. Now, in this model validation phase, the model
    validation metric needs to be linked to that business metric as closely as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, we have defined the cost function, which is used to
    find the optimized model. The cost function is also used for ML model validation.
    For regression problems, the cost function (the gap between the model value and
    the actual value) is usually the MSE, which was discussed in the previous section.
    For binary classification, the cost function is usually expressed in a metric
    called a confusion matrix. Let’s take a closer look at the confusion matrix, as
    well as the impact of changing the classification threshold on the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A confusion matrix is used to measure a binary classification model’s predictions,
    as shown in *Figure 4.11.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Confusion matrix ](img/Figure_4.11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on whether the model predicts the actual classes, there are four situations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive** (**TP**) is where the model *correctly* predicts the *positive*
    class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative** (**TN**) is where the model *correctly* predicts the *negative*
    class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive** (**FP**) is where the model *incorrectly* predicts the *positive*
    class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative** (**FN**) is where the model *incorrectly* predicts the *negative*
    class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at a computer vision ML problem: you trained two models for image
    recognition – to classify an image as being of a cat or not. You have run the
    two models on the validation dataset and compared the results with the labels,
    and *Figure 4.12* shows the confusion matrices for the two ML models. How do we
    measure which model performs better?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Confusion matrices for two ML models ](img/Figure_4.12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Confusion matrices for two ML models
  prefs: []
  type: TYPE_NORMAL
- en: 'To help us compare the classification model performances, we need to define
    more metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Recall` (`sensitivity`) measures the proportion of actual positives that were
    identified correctly:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Specificity` measures the proportion of actual negatives that were identified
    correctly:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Applying the preceding metrics to the two models, we have come up with the
    following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.13 – Recall and specificity for two ML models ](img/Figure_4.13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 4.13 – Recall and specificity for two ML models
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the business goal, these two models’ performance can be measured
    and interpreted from different points of view. If the goal is to identify as many
    cats as possible and the amount of false positives does not matter, then model
    two is better performed since it has a high recall metric. However, if your goal
    is to identify the not-cats, then model one may be a better choice since it has
    a high specificity metric.
  prefs: []
  type: TYPE_NORMAL
- en: You can use more metrics to help with your decision making. Next, we introduce
    the concepts of the receiver operating characteristic curve and the area under
    the curve.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curve and AUC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous chapters, we discussed the cutoff for converting a probability into
    a class. The threshold will impact the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: An **Receiver Operating Characteristic** (**ROC**) curve is a graph showing
    the **True Positive Rate** (**TPR**) and **False Positive Rate** (**FPR**), as
    two dimensions, under all threshold values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TPR is a synonym for recall and is therefore defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The FPR is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.14 – ROC curves ](img/Figure_4.14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – ROC curves
  prefs: []
  type: TYPE_NORMAL
- en: 'To build an ROC curve, you calculate the TPR (or recall) against the FPR for
    each threshold and plot it on a graph. *Figure 4.14* shows a sample ROC. If we
    take a close look at the graph, we will see that the point at (*0,0*) represents
    zero true positives and zero false positives. The point at (*1,1*) means that
    all the positives are correctly identified but all the negatives also incorrectly
    identified. The dotted line from (*0,0*) to (*1,1*), called a **random classifier**,
    represents *TPR=FPR*. In the diagram, the ideal line is the *perfect classifier*,
    which represents *TPR=1* with no *false positives*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – AUC: the area under the curve](img/Figure_4.15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15 – AUC: the area under the curve'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the goal for a classification problem is a model that has a high TPR
    and a low FPR – as close as possible to the *perfect classifier* – we often use
    the **Area Under the Curve** (**AUC**)-ROC as a measurement of classification
    model performance: the greater the AUC-ROC, the better. A sample AUC-ROC is shown
    in *Figure 4.15*.'
  prefs: []
  type: TYPE_NORMAL
- en: More classification metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you can see, with the four numbers from the confusion matrix, you can calculate
    a model’s recall and specificity, and here we introduce accuracy, precision, and
    F1-score:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Accuracy` measures the proportion of correct predictions among the total number
    of cases:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Precision` measures the proportion of positive identifications that are actually
    correct:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`F1-score` combines precision and sensitivity and measures the overall performance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So far, we have introduced many classification metrics – which one should you
    choose? It really depends on the business context and goals. For a classification
    model that identifies emails as spam or not spam, while precision is good to identify
    the spam, you also want to avoid labeling a legitimate email as spam. For a classification
    model that identifies whether a patient has a terminal illness or not, it is vitally
    important to identify the illness for a patient who actually has that illness.
    In this situation, sensitivity is a better metric than precision to use.
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score combines precision and recall to give you one number that quantifies
    the overall performance. You might want to use the F1 score when you have a class
    imbalance but you want to preserve the equality between precision and sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During the model validation process, we evaluate the model performances, and
    there are situations where the model does not fit the validation dataset. Let’s
    examine the different cases.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and underfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While underfitting describes the situation where prediction error is not minimized,
    overfitting is the case where the model fits the training dataset very well but
    does not fit the validation dataset. An overfitting model gets a very low cost
    function value during training but poorly predicts on new data. *Figure 4.16*
    depicts the situations for underfitting, robust, and overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Model fittings ](img/Figure_4.16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Model fittings
  prefs: []
  type: TYPE_NORMAL
- en: When we try to minimize the cost function and avoid underfitting, we need to
    make sure our model is generalized and not prone to overfitting. From our ML practice,
    we know that overfitting is often caused by making a model more complex than necessary.
    As you can see in *Figure 4.16*, overfitting makes a training model memorize data.
    The ML’s fundamental principle is making the model fit well on the data without
    losing generality. To avoid overfitting, we introduce regularization to decrease
    model complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To avoid overfitting, we need to reduce model complexity. Model complexity
    can be thought of in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Model complexity as a function of the *total number of features* with nonzero
    weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model complexity as a function of the *weights* of all the features in the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The idea of regularization is introduced to add a factor to penalize the model
    complexity and enhance model generalization. Corresponding to the preceding two
    complexities, we have two kinds of regularization/generalization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantify complexity with the *L2 regularization* formula, which defines the
    regularization term as the sum of the squares of all the feature weights – weights
    close to zero have little effect on model complexity, while outlier weights can
    have a huge impact. *Ridge regression* uses L2 regularization: the cost function
    is altered by adding a penalty equivalent to the square of the weights. Let *p*
    be the number of features, and the coefficient (weight) of the *i*th feature is
    *w*i; then, the cost function is written as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18333_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Quantify complexity with the *L1 regularization* formula, which defines the
    regularization term as the sum of the absolute of all the feature weights – weights
    close to zero have a large effect on model complexity, while outlier weights have
    less impact. Lasso regression uses L1 regularization: the cost function is altered
    by adding a penalty equivalent to the absolute of the weights:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18333_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How should you choose the parameter lambda for the preceding formulas? If the
    lambda value is too high, the model will be simple and carry a risk of underfitting
    the data. If the lambda value is too low, the model will be more complex, with
    the risk of overfitting your data and leading to generalization issues with new
    data. The ideal value of lambda produces a model that fits the training data and
    generalizes well to new data. One objective of model tuning is to balance the
    model complexity and generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Other than regularization, we can use early stopping to avoid overfitting. Early
    stopping is a form of regularization used to avoid overfitting when training a
    learner with an iterative method, such as gradient descent. This means ending
    the training when the training results are good enough, and before the model fully
    reaches convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hyperparameter tuning is the process of finding the best version of a model
    by running many training jobs on your dataset. It uses the algorithm and ranges
    of hyperparameters that you specify, then chooses the hyperparameter values that
    result in a model that performs the best, as measured by a metric that you choose.
  prefs: []
  type: TYPE_NORMAL
- en: There are two basic types of hyperparameters. The first kind is model hyperparameters.
    They are directly linked to the model that is selected and thus have a direct
    impact on the performance of that model. They help define the model itself, for
    example, the number of layers in a neural network model and the activation functions
    that are used.
  prefs: []
  type: TYPE_NORMAL
- en: The second kind is algorithm hyperparameters. They do not affect the performance
    of the algorithm directly but affect the efficiency and the rate of model training.
    For example, the learning rate for a gradient descent algorithm may affect how
    quickly an ML model converges.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of tuning hyperparameters involves changing the hyperparameter
    values and attempting to find those that yield the best results. The common hyperparameters
    that are often tuned include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The batch size**: The number of samples that are processed during training,
    before the model is updated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The number of training epochs**: The number of times that we run through
    the full set of training data during model training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The learning rate**: The distance we travel when trying to find the optimal
    value for a parameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through the ML model training, validation, and hyperparameter tuning, we have
    come up with a model that can be deployed for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and deploying the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test and get performance metrics from your model, you must make inferences
    or predictions from the model—which typically requires deployment. The goal of
    the deployment phase is to provide a managed environment to host models for inference
    both securely and with low latency. You can deploy your model in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single predictions**: Deploy your model online with a permanent endpoint.
    For example, we can deploy the housing model (price prediction) with an online
    endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.csv` file or multiple sets of records to be sent at a time, the model will
    return a batch of predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After deploying a model into testing, you evaluate the model to see whether
    it meets the performance requirements and the business requirements, which is
    the ultimate goal for any ML problem. All the stakeholders will need to evaluate
    the ML solution’s benefits and approve the model’s deployment to production. Keep
    in mind that the most accurate model may not be the best solution to an ML problem.
  prefs: []
  type: TYPE_NORMAL
- en: After all the stakeholders approve the model, we then deploy the model to production.
    Otherwise, we need to go back to the process of model training, validation and
    tuning, re-testing, and re-evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: After deploying a model to production, you still need to monitor the production
    data, since new data accumulates over time, and alternative or new outcomes can
    potentially be identified. Therefore, deploying a model is a continuous process,
    not a one-time exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Practicing model development with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scikit-learn is one of the most useful libraries for ML in Python. The scikit-learn
    library contains a lot of tools for ML, including ones for classification and
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Appendix 3*](B18333_13.xhtml#_idTextAnchor209) of the book, we have provided
    a step-by-step practice exercise for using scikit-learn to develop ML models.
    Practicing these steps is essential to master scikit-learn skills. Please refer
    to [*Appendix 3*](B18333_13.xhtml#_idTextAnchor209), *Practicing with Scikit-Learn*,
    to learn and practice with examples of ML model training, validation, and testing
    using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have discussed the basic concepts of the ML model development
    process: data splitting, platform setup, ML model training, validation, testing,
    and deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the concept of AI emerged in the 1950s, there were no big breakthroughs
    until 2012, when **deep learning** (**DL**) was invented using neural networks.
    DL has greatly improved ML model performance and opened up a huge avenue for applying
    ML to many business use cases. In the next chapter, we will discuss neural networks
    and DL.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For further insights into the topics of the chapter, you can refer to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://scikit-learn.org/stable/tutorial/basic/tutorial.xhtml](https://scikit-learn.org/stable/tutorial/basic/tutorial.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://scikit-learn.org/stable/tutorial/index.xhtml](https://scikit-learn.org/stable/tutorial/index.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Appendix 3*](B18333_13.xhtml#_idTextAnchor209)*, Practicing with ScikitLearn*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
