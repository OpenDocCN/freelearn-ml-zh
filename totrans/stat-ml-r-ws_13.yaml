- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probability Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Probability distribution** is an essential concept in statistics and machine
    learning. It describes the underlying distribution that governs the generation
    of potential outcomes or events in an experiment or random process. There are
    different types of probability distributions, depending on the specific domain
    and characteristics of the data. A proper probability distribution is a useful
    tool in understanding and modeling the behavior of random processes and events,
    providing convenient tools for decision-making and predictions when developing
    data-driven predictive and optimization models.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the common probability distributions
    and their parameters. You will also be able to use these probability distributions
    to perform usual tasks such as sampling and probability calculations in R, as
    well as common sampling distribution and order statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing probability distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring common discrete distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering common continuous distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding common sampling distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding order statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the code in this chapter, you will need to have the latest versions
    of the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ggplot2`, 3.4.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dplyr`, 1.0.10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the versions of the packages mentioned in the preceding list
    are the latest ones at the time of writing this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_10/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_10/working.R).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing probability distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probability distribution provides a framework for understanding and predicting
    the behavior of random variables. Once we know the underlying data-generating
    probability distribution, we can make more informed decisions about how things
    are likely to appear, either in a predictive or optimization context. In other
    words, if the selected probability distribution can model the observed data very
    well, we have a powerful tool to predict potential future values, as well as the
    uncertainty of such occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: Here, a random variable is a variable whose value is not fixed and may assume
    multiple or infinitely many possible values, representing the outcomes (or realizations)
    of a random event. Probability distributions allow us to represent and analyze
    the probability of these outcomes, offering a comprehensive view of the underlying
    uncertainties in various scenarios. A probability distribution takes the random
    variable, denoted as x, and converts it into a probability, P(x), a floating number
    valued between 0 and 1\. The probability distribution can be a **probability density
    function** (**PDF**) or **probability mass function** (**PMF**) that specifies
    the probability of observing an outcome for a continuous variable (or discrete
    variable), or a **cumulative distribution function** (**CDF**) that provides the
    total probability that a random variable is less than or equal to a given fixed
    quantity.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we use f(x) to represent the probability density function
    of x, and F(x) to represent the CDF.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main categories of probability distributions: discrete probability
    distribution and continuous probability distribution. A discrete probability distribution
    deals with discrete variables, which are random variables that can assume a limited
    or countable number of possible values. For example, if we have a probability
    distribution that specifies the probability of experiencing a rainy day in a week,
    the underlying random variable is the day of the week and can only take an integer
    value between 1 and 7\. Letâ€™s assume there is a total of C possible values for
    the discrete random variable. For a given possible value, xÂ i, with i âˆˆ {1, â€¦
    , C}, the corresponding PMF is f(xÂ i) âˆˆ [0,1], and all probabilities should sum
    to 1, giving âˆ‘Â i=1Â CÂ Â f( xÂ i) = 1\. We can think of PMF f(x) as a bar chart that
    specifies the probability output for each discrete input.'
  prefs: []
  type: TYPE_NORMAL
- en: We will cover a few common discrete distributions. For example, the binomial
    distribution models the number of successes in a fixed number of **Bernoulli**
    trials with the same probability of success, the **Poisson** distribution models
    the number of events within a fixed interval of time or space, and the geometric
    distribution models the number of trials required for the first success in a sequence
    of Bernoulli trials. These will be covered later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous probability distributions, on the other hand, involve continuous
    variables, which can take an infinite number of values within a specified range,
    denoted as ð’³. The random variable, x âˆˆ ð’³, is now continuous, and the corresponding
    PDF f(x) âˆˆ [0,1] satisfies âˆ« f(x)dx = 1 for x âˆˆ ð’³, where we have switched the
    summation sign to an integral to account for an infinite amount of possible values
    of the continuous variable, x. We can think of PDF f(x) as a line plot that specifies
    the probability output for each continuous input.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover a few widely used continuous distributions, starting with the
    normal (or Gaussian) distribution, which describes the distribution of many natural
    quantities. Other examples of continuous distributions include the exponential
    distribution, which models the time between independent events in a Poisson process,
    and the uniform distribution, which assigns equal probability to all outcomes
    within a specified range.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.1* summarizes these two types of probability distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 â€“ Summarizing the two categories of probability distributions.
    Both distributions sum to 1](img/B18680_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 â€“ Summarizing the two categories of probability distributions. Both
    distributions sum to 1
  prefs: []
  type: TYPE_NORMAL
- en: Note that each probability distribution has an associated closed-form expression
    with a corresponding set of parameters. We will highlight the expression and parameters
    for each distribution next, starting with the discrete distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring common discrete probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discrete probability distributions are characterized by their corresponding
    PMFs, which assign a probability to each possible outcome of the input random
    variable. The sum of the probabilities for all possible outcomes in a discrete
    distribution equals 1, leading to âˆ‘Â i=1Â CÂ Â f( xÂ i) = 1\. This also means that
    one of the outcomes *must* occur, giving f(xÂ i) > 0, âˆ€ i = 1, â€¦ , C.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete probability distributions are vital in various fields, such as finance.
    They are commonly used for statistical analyses, including hypothesis testing,
    parameter estimation, and predictive modeling. We can use discrete probability
    distributions to quantify uncertainties, make predictions, and gain insights into
    the underlying data-generating process of the observed outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s start with the most fundamental discrete distribution: the Bernoulli
    distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The Bernoulli distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Bernoulli distribution** is a fundamental discrete probability distribution
    that specifies the behavior of a binary random variable in a single Bernoulli
    trial. Here, a **Bernoulli trial** is a single experiment with only two possible
    outcomes, which can also be labeled as â€œsuccessâ€ and â€œfailure.â€ It is the simplest
    discrete probability distribution and serves as the basis for more complex distributions,
    such as binomial and geometric distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The Bernoulli distribution is widely used in modeling scenarios with binary
    outcomes, such as coin tosses, yes/no survey questions, or the presence/absence
    of a specific feature in a dataset. For example, in statistical hypothesis testing,
    the Bernoulli distribution is often used in scenarios such as comparing the success
    rates of two treatments for a medical condition. In finance, the Bernoulli distribution
    can be used to model binary outcomes, such as a stockâ€™s price going up or down.
  prefs: []
  type: TYPE_NORMAL
- en: As a convention, the two outcomes in a Bernoulli distribution are often encoded
    as `1` for success and `0` for failure. The Bernoulli distribution is characterized
    by a single parameter, p, which represents the probability of success. In other
    words, we have f(x = 1) = p. Similarly, since total probabilities sum to `1`,
    the probability of failure is given by 1 âˆ’ p, giving f(x = 0) = 1 âˆ’ p.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can express the PMF of a Bernoulli distribution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = {p, if x = 1Â Â 1 âˆ’ p, if x = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we can express f(x) in a more compact form, as follows. It is
    easy to verify that these two representations are equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x = i) = pÂ i (1 âˆ’ p)Â 1âˆ’i for i âˆˆ {0,1}
  prefs: []
  type: TYPE_NORMAL
- en: Note that p âˆˆ [0,1].
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the mean, Î¼ (first moment), and variance, ÏƒÂ 2 (second moment), which
    characterize the Bernoulli distribution, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Î¼ = p
  prefs: []
  type: TYPE_NORMAL
- en: ÏƒÂ 2 = p(1 âˆ’ p)
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ll simulate and analyze Bernoulli-distributed random variables using R in
    the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.1 â€“ simulating and analyzing Bernoulli-distributed random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will simulate and analyze Bernoulli-distributed random
    variables using the `rbinom()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simulate a single Bernoulli trial with a success probability of `0.6`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, the outcome will be displayed as either `0` or `1`. We can control the
    random seed to ensure the reproducibility of the results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate five random Bernoulli outcomes with the same probability of success:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the mean and variance of the Bernoulli distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `cat()` function to concatenate and print out the results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Analyze the results of multiple Bernoulli trials in terms of the observed/empirical
    probability of success:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since we only sampled 5 times, the resulting empirical probability of success
    (`0.4`) is very far from the true probability (`0.6`). We can enlarge the size
    of random trials to get a more reliable estimate:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using a total of `1000` trials, we can now reproduce the exact true probability
    of success.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next section reviews the binomial distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The binomial distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `1` or `0`. Specifically, it is a discrete probability distribution that
    specifies the count of successes in a given number of Bernoulli trials. These
    trials are independent and share the same probability of success.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two parameters for the binomial distribution PMF:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of trials, n
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of success, p, in each trial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'We still assume only two possible outcomes: success (`1`) or failure (`0`).
    The probability of failure can also be represented as q , where q = 1 âˆ’ p.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The PMF of the binomial distribution with a total of k successes is given in
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: P(x = k) = C(n, k) pÂ k (1 âˆ’ p)Â nâˆ’1
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, C(n, k) represents the number of combinations of choosing k successes
    from n trials, which can be calculated using the binomial coefficient formula:'
  prefs: []
  type: TYPE_NORMAL
- en: C(n, k) = Â n !Â _Â k !(n âˆ’ k) !
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two moments are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Î¼ = np
  prefs: []
  type: TYPE_NORMAL
- en: ÏƒÂ 2 = np(1 âˆ’ p)
  prefs: []
  type: TYPE_NORMAL
- en: Using the PMF of the binomial distribution, we can compute the probability of
    observing a specific number of successes in a specific number of trials.
  prefs: []
  type: TYPE_NORMAL
- en: The binomial distribution has some important relationships with other probability
    distributions. For instance, as n approaches infinity and p remains constant,
    the binomial distribution converges to the normal distribution, a continuous probability
    distribution to be introduced later.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s go through an exercise to get familiar with the functions related to binomial
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.2 â€“ simulating and analyzing binomial random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will simulate and analyze binomial-distributed random
    variables using the `dbinom()` and `pbinom()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `dbinom()` function to calculate the probability of observing 0 to
    10 successes based on a binomial distribution with a success probability of `0.5`
    and a total of 10 trials:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use `0:n` to create a list of integers from 0 to 10, each of which
    will then get passed to the `dbinom()` function to evaluate the corresponding
    probability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a bar plot of the binomial probabilities using the `barplot()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 10**.2*, which shows that the middle occurrence
    of 5 has the highest probability:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.2 â€“ Visualizing the binomial distribution with n=10 and p=0.5](img/B18680_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 â€“ Visualizing the binomial distribution with n=10 and p=0.5
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the cumulative binomial probabilities using the `pbinom()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that the cumulative binomial probabilities from the CDF are
    calculated as the cumulative sum of the previous element-wise probabilities in
    the PMF.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can also use the CDF to compute the probability of observing a specific value
    or higher.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the probability of obtaining at least seven successes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the probability of obtaining at least 7 successes is calculated by taking
    the complement of observing 6 or fewer successes â€“ that is, we have P(X â‰¥ 7) =
    1 âˆ’ P(X â‰¤ 6).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Letâ€™s go through another application-related exercise to put these calculations
    into perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.3 â€“ calculating winning probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will calculate the winning probabilities of a sports team:'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the sports team has a probability of 80% of winning a match. If there
    are a total of five matches, what is the probability of winning at least four
    matches?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the probability of winning, at most, three matches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that this probability is a complement to the previous probability of winning
    at least four matches. We can verify this relationship as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the following section, weâ€™ll pause to discuss the normal approximation to
    the binomial distribution. This relationship is widely used in statistical analysis
    and has its roots in the central limit theorem.
  prefs: []
  type: TYPE_NORMAL
- en: The normal approximation to the binomial distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The normal (or **Gaussian**) approximation to the binomial distribution says
    that the binomial distribution can be approximated by a normal distribution that
    shares the same mean value (Î¼ = np) and variance value (ÏƒÂ 2 = np(1 âˆ’ p)). Such
    normal approximation to the binomial distribution becomes more accurate as the
    number of experiments (n) increases, and the success probability (p) does not
    approach 0 or 1\. As a rule of thumb, we often use the normal approximation when
    both np â‰¥ 10 and nq â‰¥ 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the normal approximation, we need to standardize the binomial random
    variable, x, by converting it into the form of a standard normal variable, z,
    based on the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: z = Â x âˆ’ Î¼Â _Â Ïƒ
  prefs: []
  type: TYPE_NORMAL
- en: Here, Î¼ = np and Ïƒ = âˆšÂ _Â np(1 âˆ’ p)Â . This is also called the **z-score**. Going
    through such standardization is a common practice when trying to compare different
    quantities on the same scale. We can make use of the standard normal distribution
    (to be introduced later) to work on the corresponding PDF or CDF, depending on
    the specific task. In this case, we can use the standard normal distribution (that
    is, N(0,1)) to approximate the probabilities associated with the binomial distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s look at a concrete example. Suppose we toss a coin 100 times (assuming
    a fair coin with an equal probability of landing with a head or a tail) and would
    like to compute the probability of obtaining between 40 and 60 heads (both inclusive).
    Let x be the random variable that denotes the number of heads. We know that x
    assumes a binomial distribution with n = 100 and p = 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check whether the normal approximation is appropriate, we calculate np =
    100 * 0.5 = 50 > 10 and np(1 âˆ’ p) = 100 * 0.5 * 0.5 = 25 > 10\. We also verify
    the same using R, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Both conditions evaluate `TRUE`. Now, we can standardize the upper and lower
    limits (`60` and `40`, respectively) to convert them into a standardized score.
    To do this, we need to obtain the parameters of the binomial distribution, followed
    by applying the standardization formula, z = x âˆ’ Î¼Â _Â ÏƒÂ , to get the z-score. The
    following code snippet completes the standardization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With the standardized z-score, we can now calculate the original probability
    based on the standard normal distribution alone. In other words, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: P(40 â‰¤ x â‰¤ 60) = P(Â 40 âˆ’ 50Â _Â 5Â  â‰¤ Â x âˆ’ 50Â _Â 5Â  â‰¤ Â 60 âˆ’ 50Â _Â 5Â ) = P( âˆ’ 2 â‰¤
    z â‰¤ 2)
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following code snippet, we can now call the `pnorm()` function
    to calculate the CDF at points z = âˆ’ 2 and z = 2, whose difference gives the final
    probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Letâ€™s also calculate the corresponding probability using the binomial distribution
    to see how close the normal approximation is. The following code snippet uses
    the `pbino``m()` function to obtain the CDF of the binomial at both limits and
    then takes the difference to give the total probability of observing an outcome
    in this range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The result shows that the approximation is accurate up to the second decimal
    place. Therefore, the normalization approximation provides an alternative approach
    to calculating the probabilities if directly using the binomial distribution is
    inconvenient. However, although the normal approximation is a powerful tool, we
    still need to check the required conditions to ensure a good approximation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.3* summarizes the two approaches to calculating the total probability
    of observing a specific range of values. We can calculate it by taking the difference
    in the CDF between the two boundaries of the range. Alternatively, we can rely
    on the normal distribution to approximate the binomial distribution and calculate
    the total probability after obtaining the standardized z-score based on these
    parameters, assuming the conditions are satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 â€“ Summarizing the normal approximation to the binomial distribution
    when calculating the total probability of observing a specific range of values](img/B18680_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 â€“ Summarizing the normal approximation to the binomial distribution
    when calculating the total probability of observing a specific range of values
  prefs: []
  type: TYPE_NORMAL
- en: We will review the Poisson distribution in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The Poisson distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another popular discrete probability distribution is the Poisson distribution,
    which describes the number of events within a fixed interval of time or space.
    It has a single constant parameter that specifies the average rate of occurrence.
    Specifically, we must denote Î» as the average rate of occurrence of events in
    the given interval. We can express the PMF of the Poisson distribution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(x = k) = Â Î»Â k eÂ âˆ’Î»Â _Â k !
  prefs: []
  type: TYPE_NORMAL
- en: Here, P(x = k) denotes the probability of experiencing a total of k occurrences
    in the fixed interval, e is Eulerâ€™s number (approximately 2.71), and k ! is the
    factorial of k (the product of all positive integers up to k).
  prefs: []
  type: TYPE_NORMAL
- en: 'With this equation, we can calculate the probability of observing any (integer)
    number of events within the given fixed interval. We can further calculate the
    mean and variance of the Poisson distribution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Î¼ = Î»
  prefs: []
  type: TYPE_NORMAL
- en: ÏƒÂ 2 = Î»
  prefs: []
  type: TYPE_NORMAL
- en: The Poisson distribution is often used to model rare events that occur independently
    and at a constant average rate. Some real-world applications of Poisson processes
    include the number of hotel bookings received at the front desk per hour and the
    number of emails arriving in an inbox within a day.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s go through an exercise to get familiar with common probability calculations
    related to the Poisson distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.4 â€“ simulating and analyzing Poisson-distributed random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use R to work with the Poisson distribution, including
    calculating the probabilities (using the PMF), plotting the distribution, and
    generating random samples. Specifically, we will calculate the Poisson probabilities
    for an average rate of occurrence of 5 events per interval (Î» = 5), plot the PMF,
    and calculate the cumulative probabilities. We will also generate a random sample
    of 100 observations from this Poisson distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the probabilities of observing 0 to 15 occurrences/events per interval
    based on a Poisson distribution parameterized by Î» = 5 using the `dpois()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a bar plot of the Poisson probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 10**.4*. As expected, the peak probabilities
    appear around five times:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.4 â€“ Visualizing the PMF of the Poisson distribution as a bar plot](img/B18680_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 â€“ Visualizing the PMF of the Poisson distribution as a bar plot
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate cumulative Poisson probabilities for each number of events (0 to
    15) using the `ppois()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 10**.5*. Note that the CDF curve increases
    sharply around the mean occurrence of five times and gradually saturates as we
    move toward the right end of the graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.5 â€“ Visualizing the CDF of the Poisson distribution as a bar plot](img/B18680_10_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 â€“ Visualizing the CDF of the Poisson distribution as a bar plot
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate `100` random samples from this Poisson distribution using the `rpois()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As expected, the majority of the occurrences are around five times.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Poisson approximation to binomial distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As it turns out, we can also use the Poisson distribution to approximate the
    binomial distribution under specific conditions. For instance, when the number
    of trials (n) in a binomial distribution is large and the success probability
    (p) is small, the binomial distribution can be approximated by a Poisson distribution
    with Î» = np.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s use an example to demonstrate how to apply the Poisson approximation
    to the binomial distribution. Suppose we have a binomial distribution with n =
    1000 and p = 0.01\. We want to find the probability of observing exactly 15 successes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start with the binomial probability and calculate the corresponding
    probability using the `dbinom()` function after specifying the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we must calculate the approximate Poisson parameter, Î» = np:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can calculate the Poisson probability of observing 15 successes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The result suggests that the approximation is quite accurate to the third decimal
    point.
  prefs: []
  type: TYPE_NORMAL
- en: One more interesting property is that adding up several independent Poisson-distributed
    random variables also produces a Poisson distribution, which is parameterized
    by the sum of the corresponding individual Î» values. For example, if xÂ 1 and xÂ 2
    are independent Poisson random variables with Î»Â 1 and Î»Â 2, respectively, their
    sum (y = xÂ 1 + xÂ 2) also follows a Poisson distribution with Î» = Î»Â 1 + Î»Â 2\. This
    gives a convenient property when working with the sum of multiple Poisson-distributed
    random variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, weâ€™ll cover another widely used discrete distribution:
    the geometric distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The geometric distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The geometric distribution is a discrete probability distribution that describes
    the number of trials required for the first success in a sequence of independent
    Bernoulli trials, each with the same probability of success. Similar to the binomial
    distribution, the geometric distribution is a collection of multiple independent
    Bernoulli trials, although the subject of interest is the first occurrence of
    success in the sequence of trials. Here, the first occurrence of success means
    that all previous trials need to be non-success, and the current trial is the
    first success among multiple trials performed so far.
  prefs: []
  type: TYPE_NORMAL
- en: It is commonly used to model the waiting time until an event occurs or the number
    of attempts needed to achieve a desired outcome. Examples include the number of
    attempts to pass a driving test until success, the number of times we observe
    continuous sunny days, and the number of coin flips needed to obtain the first
    head. The geometric distribution is quite useful when modeling the waiting time
    or the number of attempts needed to achieve the first success in a sequence of
    independent Bernoulli trials.
  prefs: []
  type: TYPE_NORMAL
- en: 'The geometric distribution is defined by a single parameter, p, which represents
    the probability of success on each Bernoulli trial. The PMF of the geometric distribution
    is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: P(x = k) = (1 âˆ’ p)Â kâˆ’1 p
  prefs: []
  type: TYPE_NORMAL
- en: This formula specifies the probability of observing the first success on the
    kÂ th trial. Therefore, the probability is calculated as a joint probability of
    observing k individual events, where the first k âˆ’ 1 events are non-success with
    a joint probability of (1 âˆ’ p)Â kâˆ’1, and the last event is a success with a probability
    of p.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean and variance parameters of a geometric distribution can be expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Î¼ = Â 1Â _Â p
  prefs: []
  type: TYPE_NORMAL
- en: ÏƒÂ 2 = Â 1 âˆ’ pÂ _Â pÂ 2
  prefs: []
  type: TYPE_NORMAL
- en: Note that the geometric distribution is **memoryless**, meaning that the probability
    of success in the next trial does not depend on the past trials. In other words,
    the waiting time until the first success does not change, regardless of the number
    of past trials that have already been conducted.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s go through an exercise to simulate and analyze a random variable while
    following the geometric distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.5 â€“ simulating and analyzing geometrically-distributed random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use R to work with the geometric distribution, including
    calculating the PMF and CDF probabilities, plotting the distribution, and generating
    random samples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a geometric distribution with a success probability of p = 0.25, calculate
    geometric probabilities for each number of trials (from 1 to 10) using the `dgeom()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the `0:9` argument represents the number of failures in the `dgeom()`
    function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a bar plot for these geometric probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 10**.6*. As expected, the probability of
    obtaining a longer sequence of continuous failures decreases as the number of
    trials increases:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.6 â€“ Visualizing the PMF of the geometric distribution as a bar
    plot](img/B18680_10_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 â€“ Visualizing the PMF of the geometric distribution as a bar plot
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate cumulative geometric probabilities for the previous trials using
    the `pgeom()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate 100 random samples from this geometric distribution using the `rgeom()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows a decreasing frequency as the numbers grow large, which matches
    the PMF of the geometric distribution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Letâ€™s go through another application-related exercise on the probability of
    finding bugs in a computer program.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.6 â€“ simulating and analyzing geometrically-distributed random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will visit a real-world example that involves a software
    tester trying to find bugs in a computer program. In this example, the software
    tester finds bugs in a program with a probability of `0.1` on each attempt, and
    the attempts are independent. The company wants to know the probability of finding
    the first bug within the first five attempts, as well as the expected number of
    attempts needed to find the first bug:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the probability of finding the first bug within the first five attempts
    using the `pgeom()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, note that we use `4` since the `dgeom()` function uses zero-based indexing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As the `pgeom()` function returns the CDF of a specific input, we can equivalently
    calculate this probability by summing up all previous probabilities up to the
    current input, as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the expected number of attempts needed to find the first bug using
    the mean (expected value) of the geometric distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the probabilities of finding the first bug within different numbers
    of attempts (from `1` to `20`) in a bar chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 10**.7*. This figure suggests that rare
    events (those requiring a continuous stream of failures) assume lower probabilities
    as we move toward the right:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.7 â€“ Visualizing the probabilities of finding the first bug within
    different numbers of attempts](img/B18680_10_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 â€“ Visualizing the probabilities of finding the first bug within
    different numbers of attempts
  prefs: []
  type: TYPE_NORMAL
- en: As this figure suggests, the probability of observing the first bug decreases
    as the number of attempts increases.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing different discrete probability distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The discrete probability distributions introduced so far in this chapter are
    essential tools to model scenarios where the outcome variable takes on discrete
    values. Each discrete distribution has specific assumptions, properties, and applications.
    *Figure 10**.8* provides a summary takeaway and analysis by comparing the main
    characteristics of the binomial, Poisson, and geometric distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 â€“ Summarizing and comparing different discrete distributions](img/B18680_10_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 â€“ Summarizing and comparing different discrete distributions
  prefs: []
  type: TYPE_NORMAL
- en: With different discrete distributions at hand, it is important to understand
    the specific assumptions and requirements of each distribution to select the appropriate
    one for a given problem. For example, the binomial distribution is suitable for
    modeling the number of successes in a given number of experiments, the Poisson
    distribution is suitable for modeling the number of events occurring in a fixed
    period, and the geometric distribution is often used to model the number of trials
    required to achieve the first success.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, these discrete probability distributions can be used to analyze
    various real-world scenarios, make predictions, and optimize processes. Getting
    a good understanding of the characteristics and applications of each distribution
    allows you to choose the right distribution for the specific problem and perform
    relevant analyses using R.
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces continuous distributions, including normal distribution,
    exponential distribution, and uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering common continuous probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous probability distributions model the probability of random variables
    that assume any value within a specific continuous range. In other words, the
    underlying random variable is continuous instead of discrete. These distributions
    describe the probabilities of observing values that fall within a continuous interval,
    rather than equal to individual discrete outcomes in a discrete probability distribution.
    Specifically, in a continuous probability distribution, the probability of the
    random variable equal to any specific value is typically zero, since the possible
    outcomes are uncountable. Instead, probabilities for continuous distributions
    are calculated for intervals or ranges of values.
  prefs: []
  type: TYPE_NORMAL
- en: We can use a PDF to describe a continuous distribution. This corresponds to
    the PMF of a discrete probability distribution. The PDF defines the probability
    of observing a value within an infinitesimally small interval around a given point.
    The area under the PDF curve over a specific range represents the probability
    of the random variable falling within that range â€“ that is, the probabilities
    are calculated for intervals or ranges of values by integrating the PDF over the
    desired range. In contrast, probabilities are assigned to individual points in
    a PMF, and the probabilities are calculated for a set of discrete values by summing
    up their individual probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the visualization for continuous probability distributions is also
    different. Compared to the bar chart used for the PMF of discrete probabilities
    distributions, the PDF of continuous distributions is plotted as smooth curves
    called density plots. The area under the curve over a specific range represents
    the probability of the random variable falling within that range.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.9* summarizes the main differences between discrete and continuous
    probability distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 â€“ Summarizing the differences between discrete and continuous
    probability distributions](img/B18680_10_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 â€“ Summarizing the differences between discrete and continuous probability
    distributions
  prefs: []
  type: TYPE_NORMAL
- en: In summary, discrete and continuous probability distributions model different
    types of random variables. Discrete probability distributions represent countable
    outcomes, while continuous distributions represent uncountable possibilities within
    a continuous range. These differences between these two types of distributions
    determine how we select the appropriate distribution for a given problem and perform
    relevant analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, weâ€™ll introduce the most widely used continuous probability
    distribution: the normal probability distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The normal distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **normal probability distribution**, also called the **Gaussian distribution**,
    is a continuous probability distribution that models scenarios where the continuous
    outcomes are symmetrically distributed around the mean. It is the most widely
    used probability distribution in practice as many natural and social phenomena
    tend to follow a normal distribution as a result of the central limit theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two parameters are used to characterize the normal distribution: the mean (Î¼)
    and the standard deviation (Ïƒ). The mean represents the central tendency or the
    average value of the distribution. The outcomes around the center of the distribution
    get the highest probability. The standard deviation describes the dispersion or
    spread of the data from the mean, serving as a measure of variability in the distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The PDF of the normal distribution is specified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = Â 1Â _Â âˆšÂ _Â 2Ï€Â  ÏƒÂ  eÂ âˆ’(xâˆ’Î¼)Â 2Â _Â 2ÏƒÂ 2
  prefs: []
  type: TYPE_NORMAL
- en: We can also write x âˆ¼ N(Î¼, ÏƒÂ 2), which reads as the random variable, x, follows
    a normal distribution parameterized by Î¼ and ÏƒÂ 2.
  prefs: []
  type: TYPE_NORMAL
- en: Graphically, the normal distribution looks like a bell-shaped curve, with most
    values concentrated near the mean and fewer values toward the extremes at both
    ends. An empirical rule, called the 68-95-99.7 rule, says that approximately 68%
    of the values lie within 1 standard deviation of the mean, 95% within 2 standard
    deviations, and 99.7% within 3 standard deviations.
  prefs: []
  type: TYPE_NORMAL
- en: 'A particular normal distribution that is commonly used is the standard normal
    distribution, which is written as z âˆ¼ N(0, 1) â€“ that is, a standard normal distribution
    has Î¼ = 0 and Ïƒ = 0\. A special property is that we can transform any normally
    distributed random variable into a standard normal variable using the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: z = Â x âˆ’ Î¼Â _Â Ïƒ
  prefs: []
  type: TYPE_NORMAL
- en: We can then use the standard normal table (called a Z-table) to find probabilities
    and percentiles of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s go through an exercise to practice the calculations related to the normal
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.7 â€“ simulating and analyzing normal random variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will simulate and analyze normal-distributed random variables:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the probability density of the standard normal distribution using
    the `dnorm()` function from `-4` to `4` with a step size of `0.1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `seq()` function to create a vector of equally spaced values
    and extract the corresponding probability for each of the input values using the
    `dnorm()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the normal distribution as a continuous curve using the `plot()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 10**.10*, which shows that the PDF is centered
    around the mean, `0`, and has a standard deviation of `1` as the spread:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.10 â€“ Visualizing the density plot of the standard normal distribution](img/B18680_10_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 â€“ Visualizing the density plot of the standard normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the cumulative probabilities of the normal distribution using the
    `pnorm()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 10**.11*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.11 â€“ Visualizing the cumulative density function of the standard
    normal distribution](img/B18680_10_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 â€“ Visualizing the cumulative density function of the standard normal
    distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate random samples from a normal distribution using the `rnorm()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the 90th quantile (inverse cumulative probability) for a given probability
    using the `qnorm()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Letâ€™s look at another exercise for solving practical problems using the normal
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.8 â€“ calculating probabilities with the normal distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Letâ€™s assume a company manufactures batteries with an average life span of
    100 hours and a standard deviation of 10 hours. Letâ€™s also assume that the lifespan
    of the batteries follows a normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simulate a dataset of 1,000 batteries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `rnomr()` function to sample from the given normal distribution
    randomly. We also specify the random seed for reproducibility.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the probability that a randomly chosen battery will last more than
    120 hours:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `pnorm()` function to calculate the total probability of being
    smaller than `120`, then take the complement to get the probability of being larger
    than `120`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As expected, the probability of deviating from the mean by two standard deviations
    is quite small.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the PDF of the lifespans with the area under the curve above `120` hours
    shaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we build a DataFrame, `df`, to store the sample value and the corresponding
    density that was obtained using the `density()` function. We then subset to get
    the corresponding DataFrame, `df_shaded`, for the area to be shaded. In `ggplot`,
    we use the `geom_density()` function to plot the density curve, `geom_vline()`
    to add a vertical line indicating the threshold, and `geom_area()` to shade the
    area toward the right.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Running this code generates *Figure 10**.12*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.12 â€“ Visualizing the PDF of the empirical normal distribution with
    the area above the threshold shaded](img/B18680_10_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 â€“ Visualizing the PDF of the empirical normal distribution with
    the area above the threshold shaded
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section covers another continuous probability distribution: the exponential
    distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: The exponential distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **exponential distribution** is a continuous probability distribution thatâ€™s
    commonly used to model the time or space between events in a Poisson process.
    As covered in the previous section, a Poisson process models events that occur
    independently and at a constant average rate. The exponential distribution is
    often employed to describe the waiting time between rare events, such as the time
    between phone calls at a call center.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PDF of the exponential distribution is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = Î» eÂ âˆ’Î»x, x â‰¥ 0
  prefs: []
  type: TYPE_NORMAL
- en: Here, Î» is the rate parameter, which represents the average number of events
    that occur per unit of time or space, and e is the base of the natural logarithm.
  prefs: []
  type: TYPE_NORMAL
- en: A defining characteristic of an exponential distribution is the memoryless property,
    which says that the probability of an event occurring in the future is independent
    of the time that has already elapsed since the last event. This makes the exponential
    distribution a suitable choice for modeling waiting times between independent
    and rare events.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean and standard deviation of the exponential distribution are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Î¼ = Â 1Â _Â Î»
  prefs: []
  type: TYPE_NORMAL
- en: ÏƒÂ 2 = Â 1Â _Â Î»Â 2
  prefs: []
  type: TYPE_NORMAL
- en: The exponential distribution contains a single parameter, Î», and models the
    average number of events occurring per unit of time or space for the waiting time
    between events. The same parameter, Î», in the context of modeling a Poisson process,
    refers to the average number of events occurring in a fixed interval, in terms
    of either time or space. Both distributions are used to model different aspects
    of the same Poisson process.
  prefs: []
  type: TYPE_NORMAL
- en: Now, letâ€™s go through an exercise to review the probability calculations related
    to the exponential distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.9 â€“ calculating probabilities with the exponential distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will look at generating random samples while following
    an exponential distribution and calculate and visualize the total probability
    above a certain threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a random sample of 1,000 data points from an exponential distribution
    with a rate parameter of `0.01` using the `rexp()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the probability that the waiting time between events is more than
    150 units using the `pexp()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the PDF and shade the area under the curve for waiting times greater than
    the threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we create a DataFrame for different waiting times generated in the random
    samples, obtain the corresponding densities using the `dexp()` function, and build
    the `df` and `df_shaded` DataFrames to be used in `ggplot`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Running this code generates *Figure 10**.13*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.13 â€“ Visualizing the empirical PDF of the exponential distribution
    with the area above the threshold shaded](img/B18680_10_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 â€“ Visualizing the empirical PDF of the exponential distribution
    with the area above the threshold shaded
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Uniform distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, uniform distribution is a continuous probability distribution
    where all outcomes within a given range are equally likely. The PDF, which appears
    as a straight line (illustrated next), is characterized by two parameters, the
    lower bound (a) and the upper bound (b), which define the range of possible values.
    All values within this range have the same probability of occurrence, and values
    outside the range have a probability of zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PDF of the uniform distribution is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) = {Â 1Â _Â b âˆ’ aÂ , if x âˆˆ [a, b]Â Â 0, otherwise
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters of a uniform distribution are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Î¼ = Â a + bÂ _Â 2
  prefs: []
  type: TYPE_NORMAL
- en: ÏƒÂ 2 = Â (b âˆ’ a)Â 2Â _Â 12
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s go through a similar exercise to analyze uniformly distributed random
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.10 â€“ calculating probabilities with uniform distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will look at generating random samples, calculating probabilities,
    and plotting the PDF of a uniform distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a random sample of 10,000 data points from a uniform distribution
    with a lower bound (`a`) of `2` and an upper bound (`b`) of `10` using the `runif()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the probability that a value selected from the uniform distribution
    is greater than `7` using the `punif()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we use the `punif()` function to calculate the CDF between `a` and `threshold`.
    We can also approximate this probability using the empirical samples:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the approximation is quite close, and it will be even closer
    when the sample size gets larger.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot its PDF using `ggplot()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we build a sequence of placeholders on the *x* axis. Then, we obtain
    the corresponding densities and combine them into a DataFrame for plotting. Running
    this code generates *Figure 10**.14*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.14 â€“ Visualizing the PDF of a uniform distribution](img/B18680_10_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 â€“ Visualizing the PDF of a uniform distribution
  prefs: []
  type: TYPE_NORMAL
- en: Uniform distribution can be used to generate normally distributed random samples.
    Letâ€™s look at how this can be done.
  prefs: []
  type: TYPE_NORMAL
- en: Generating normally distributed random samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have learned how to sample from a Gaussian distribution using the
    `rnorm()` function. It helps to look at how these random samples are generated
    under the hood. This specific technique is known as the inverse transform method,
    which relies on the inverse CDF or quantile function of the target distribution
    (in this case, the normal distribution).
  prefs: []
  type: TYPE_NORMAL
- en: The process involves three steps. First, we will generate random samples from
    a uniform distribution, usually U(0,1), where all values between `0` and `1` are
    equally likely. Next, for each uniform random sample, we will locate the corresponding
    value in the target distribution using the inverse CDF (quantile function) of
    the standard normal distribution. Lastly, we will apply the scale-location transformation
    to convert the standard normal random sample into a random sample from the target
    normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: This method relies on the property that the CDF of a continuous random variable
    is a function that maps the sample space (the domain of the PDF) to the range
    of [`0`, `1`]. The inverse CDF is the inverse of the CDF and maps the interval,
    [0, 1], back to the sample space. By passing the uniform random samples into the
    inverse CDF, we reverse the mapping and obtain random samples that follow the
    target distribution, subject to additional transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s look at this process in greater detail. Suppose we want to sample from
    a normal distribution, N(Î¼, ÏƒÂ 2). How do we generate random samples from this
    particular distribution? One approach is to generate a random sample, x, from
    a standard normal distribution N(0,1), and then apply the scale-location transformation
    to obtain the final sample, Ïƒx + Î¼. By scaling the sample based on Ïƒ, followed
    by adding Î¼, the resulting sample will follow a normal distribution with mean,
    Î¼, and variance, ÏƒÂ 2.
  prefs: []
  type: TYPE_NORMAL
- en: The first step of sampling from a standard normal distribution is the key, whereas
    the second step is a simple and deterministic transformation. The approach we
    introduced earlier is to transform a uniformly distributed variable using the
    inverse CDF of a standard normal distribution. For example, if U is uniformly
    distributed on [0,1], then Î¦Â âˆ’1(U) follows N(0,1), where Î¦Â âˆ’1 is the inverse of
    the cumulative function of a standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'An illustrative example is shown in *Figure 10**.15*. First, we sample a random
    point from the uniform distribution on [0,1]. Next, we use the inverse CDF of
    the standard normal distribution to obtain the corresponding sample in the CDF
    space, given the fact that the CDF monotonically maps an arbitrary input value
    to an output on [0,1]. Mathematically, the random sample of the standard normal
    is given by x = Î¦Â âˆ’1(U). Considering the one-to-one mapping relationship between
    the PDF and the CDF of the standard normal, we could obtain the same input, x,
    in the PDF space as well. We would also expect most of the samples to be centered
    around the mean. Finally, we apply the scale-location transformation to convert
    to the random sample of the normal distribution with the desired mean and variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 â€“ Obtaining a random sample from the desired univariate Gaussian
    distribution](img/B18680_10_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 â€“ Obtaining a random sample from the desired univariate Gaussian
    distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s look at a concrete example of generating normally distributed random
    samples using the inverse transform method. In the following code snippet, we
    first set the seed for reproducibility and define the parameters of the target
    normal distribution. We then generate 5 samples while following the uniform distribution
    between 0 and 1\. Finally, we calculate the corresponding quantiles for the uniform
    sample using the inverse CDF (the quantile function, `qnorm()`) of the normal
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also apply the scale-location transformation to obtain the same random
    samples, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The next section covers sampling distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding common sampling distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A sampling distribution is a probability distribution of a sample statistic
    based on many samples drawn from a population. In other words, it is the distribution
    of a particular statistic (such as the mean, median, or proportion) calculated
    from many sets of samples from the same population, where each set has the same
    size. There are two things to take note of here. First, the sampling distribution
    is not about the random samples drawn from the PDF. Instead, it is a distribution
    thatâ€™s made from an aggregate statistic, which comes from another distribution
    drawn from the PDF. Second, we would need to sample from the PDF in multiple rounds
    to create the sampling distribution, where each round consists of multiple samples
    from the PDF.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s look at an exercise in R to illustrate the concept of the sampling distribution
    using the sample mean as the statistic of interest. We will generate samples from
    a population whose distribution is given and calculate the sample means. Then,
    we will create a histogram of the sample means to visualize the sampling distribution
    of the sample mean.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.11 â€“ generating a sampling distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will first generate a population of samples from a normal
    distribution. We will then sample from this population in multiple rounds, with
    each round consisting of multiple samples. Finally, we will extract the mean of
    each round of samples and plot them together in a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate 100,000 samples from N(50,10). Check the summary of the samples using
    `summary()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to sample 50 numbers from the previous population and return
    the mean of the samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we use the `sample()` function to sample 50 numbers from the population
    of samples created earlier without replacement. We can test out this function
    and observe a different return in each run, which is also close to the population
    mean of `50`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat this function for a total of 1,000 times to obtain a corresponding set
    of 1,000 sample means:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `replicate()` function to apply the `get_sample_mean()` function
    repeatedly for a specified number of times. It is often used in simulations, resampling
    methods, or any situation where you need to perform the same operation multiple
    times and collect the results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The result shows a sample mean from the sampling distribution that is very close
    to the population mean.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the sampling distribution of the sample mean using a histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this code generates *Figure 10**.16*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.16 â€“ Visualizing the sampling distribution of the sample mean in
    a histogram](img/B18680_10_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 â€“ Visualizing the sampling distribution of the sample mean in a
    histogram
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, weâ€™ll introduce common sampling distributions that are
    used in statistical estimation and hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: Common sampling distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Several common sampling distributions are widely used in statistical inference.
    These distributions arise from the properties of different statistics calculated
    from random samples drawn from a population. Some of the most important sampling
    distributions include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n > 30`), the sampling distribution of the sample mean approaches a normal
    distribution, even if the original population distribution is not necessarily
    normal. This property allows us to use the normal distribution to make inferences
    for the sample mean, such as hypothesis testing and confidence interval construction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling distribution of the sample proportion**: The distribution of the
    sample proportion (the proportion of successes in a group of samples) in a large
    number of independent Bernoulli trials (binary outcomes) follows a normal distribution,
    given a sufficiently large sample size and a success probability not too close
    to 0 or 1\. In this case, the mean stays the same and the standard deviation is
    a transformed population proportion, adjusted by the sample size. In particular,
    under certain conditions (np > 10 and n(1 âˆ’ p) > 10 based on our previous introduction
    on approximating the binomial distribution via normal distribution), the sampling
    distribution of the sample proportion can be approximated by a normal distribution
    with the same mean and a standard deviation given by Ïƒ = âˆšÂ _Â p(1 âˆ’ p) / nÂ .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**t-distribution**: The **t-distribution** is also called the **Studentâ€™s t-distribution**.
    It is used when estimating the population mean from a small sample (typically
    n<30) drawn from a normal population, whose standard deviation is not provided.
    The t-distribution is similar in shape to the normal distribution but has a thicker
    tail, with the exact shape determined by the degree of freedom (*df*), which is
    typically equal to n âˆ’ 1\. The t-distribution can be used to calculate the t-scores,
    which are used in hypothesis testing and confidence interval construction when
    the population standard deviation is not provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chi-square distribution**: The **chi-square distribution** is used in hypothesis
    testing and the construction of confidence intervals for the population variance.
    It can also be used to test independence in contingency tables. It is a family
    of right-skewed distributions defined by a single parameter, the degree of freedom.
    In the context of hypothesis testing for the population variance, the test statistic
    assumes a chi-square distribution with n âˆ’ 1 degrees of freedom. In the context
    of contingency tables, the test statistic follows a chi-square distribution with
    (r âˆ’ 1)(c âˆ’ 1) degrees of freedom, where r denotes the number of rows and c denotes
    the number of columns in the table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-distribution**: The **F-distribution** is used in the **analysis of variance**
    (**ANOVA**) to test the null hypothesis that the means of multiple groups are
    equal. It is a family of right-skewed distributions that are defined by two parameters,
    the *df* for the numerator (*df1*) and the denominator (*df2*). For example, in
    the context of one-way ANOVA, the test statistic follows an F-distribution with
    *df1=k - 1* and *df2=N -* *k*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, these sampling distributions are important tools in various inferential
    procedures, such as making statistical inferences about the parameters of the
    population distribution based on sample data via hypothesis testing or constructing
    confidence intervals. The four types of sampling distributions are also a subset
    of a bigger list in the playbook of statistical inferences, each looking at different
    assumptions, sample statistics, and inference procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s look at an exercise on constructing the confidence interval for the population
    mean using the t-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.12 â€“ estimating the population mean using the t-distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use a small sample to estimate the population mean
    and construct the confidence interval of the estimate using the t-distribution
    in R:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate 10 samples from a normal distribution, N(50,10), using the `rnorm()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the sample mean and standard deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will now use the sample mean and standard deviation to estimate the population
    mean.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the 95% confidence interval using the t-distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `qt()` function to find the critical t-value corresponding
    to a two-tailed test with a significance level of `0.05` and degrees of freedom,
    n âˆ’ 1\. Then, we calculate the margin of error by multiplying the critical t-value
    by the standard error of the sample mean (note that we need to divide by the square
    root of sample size) and construct the confidence interval by adding and subtracting
    the margin of error from the sample mean.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will introduce constructing the confidence interval around a sample estimate
    in more detail in the next chapter. For now, it suffices to understand how a sampling
    distribution could be derived and used to produce estimates about the profile
    of the population.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section covers another interesting and important topic: order statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding order statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Order statistics** are the values of a collection of samples when arranged
    in ascending or descending order. These ordered samples provide useful information
    about the distribution and characteristics of the sampled data. Usually, the kÂ th
    order statistic is the kÂ th smallest value in the sorted sample.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, for a collection of samples of size n, the order statistics are
    denoted as XÂ 1, XÂ 2, â€¦ , XÂ n, where XÂ 1 is the smallest value (the minimum), XÂ n
    is the largest value (the maximum), and XÂ k represents the kÂ th smallest value
    in the sorted sample.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s look at how to extract order statistics in R.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting order statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extracting the order statistics of a collection of samples could involve two
    types of tasks. We may be interested in collecting samples in an ordered fashion,
    which can be achieved using the `sort()` function. Alternatively, we may be interested
    in extracting a specific order statistic from the ordered collection of samples,
    such as finding the third-largest sample in the collection or calculating a particular
    quantile of the collection.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s look at an example of such extraction in R.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.13 â€“ extracting order statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will generate a collection of normally distributed random
    samples and look at sorting the samples and extracting particular order statistics
    from them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate 10 random samples from a normal distribution with a mean of `50` and
    a standard deviation of `10`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sort them in ascending order using the `sort()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that these samples are now ordered in ascending order. We can switch
    to descending order by setting `decreasing = T` in the `sort()` function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the minimum value (first order statistic):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the maximum value (last order statistic):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we obtain the index of the last entry using the length of the collection
    of samples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Find the third order statistic (that is, `k =` `3`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the median (50th percentile):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the median (or any other order statistics) stays the same in the
    raw samples or the sorted samples:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the 25th and 75th percentiles (first and third quartiles) using the
    `quantile()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Again, applying the same function to the ordered samples gives the same results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the next section, weâ€™ll cover a very important use of order statistics in
    finance: the value at risk.'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the value at risk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **value at risk** (**VaR**) is a widely used risk management metric in finance
    that estimates the potential loss in a portfolio or investment over a specified
    period for a given confidence level, such as 95% or 99%. It is used to quantify
    the downside risk and allocate capital accordingly. Here, the confidence level
    represents the probability that the potential loss will not exceed the calculated
    VaR. A higher confidence level indicates a more conservative estimate of the risk.
  prefs: []
  type: TYPE_NORMAL
- en: There are different approaches to calculating the VaR. We are going to focus
    on the simplest approach â€“ that is, using **historical simulation**. This method
    uses historical data to simulate potential losses. First, we must sort the historical
    returns in ascending order. The VaR is then calculated as the percentile of return
    corresponding to the specified confidence level. This is a simple and intuitive
    approach, although it assumes that the historical behavior of the returns is representative
    of the future.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s go through an exercise to illustrate how to calculate the VaR.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 10.14 â€“ calculating the VaR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will discuss how to calculate the VaR, a measure of the
    potential loss in the value of a portfolio over a specified period for a given
    confidence level:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate daily returns in a year (252 trading days) from a normal distribution
    with a mean of `0.08` and a standard deviation of `0.05`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can check the summary of the daily returns as follows, which shows that
    the daily return could be as high as 20% and as low as -7%. This means that although
    the underlying asset is profitable on average (with an expected return of 8%),
    there is still a significant risk in terms of daily fluctuations. The VaR gives
    us a measure of how large the risk is in extreme situations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the VaR at the 95% confidence level for a total portfolio value of
    1 million USD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we sort the daily returns in ascending order and store them in `sorted_returns`.
    Then, we obtain the index of the bottom 5% quantile in `VaR_index`, which is then
    used to retrieve the corresponding VaR of daily returns. Finally, we convert the
    percentage return into the loss in portfolio value in `VaR_amount`. Note that
    although the result is a negative number, we often report it as a positive number
    to indicate the potential loss (or even more) that could occur in an extreme situation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the daily returns as a density plot and shade the area of the VaR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we first convert the daily returns into a DataFrame, which is used by
    `ggplot()` to plot a density curve using `geom_density()`. We also add a vertical
    line representing the VaR using `geom_vline()`. To shade the area below the VaR,
    we use `ggplot_build()` to filter the data and use `geom_ribbon()` to color the
    region that satisfies the filtering condition.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Running this code generates *Figure 10**.17*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.17 â€“ Visualizing the daily returns and the VaR area](img/B18680_10_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 â€“ Visualizing the daily returns and the VaR area
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can quantify the VaR based on the empirical distribution of the observed
    daily returns.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered common probability distributions. We started by
    introducing discrete probability distributions, including the Bernoulli distribution,
    the binomial distribution, the Poisson distribution, and the geometric distribution.
    We followed by covering common continuous probability distributions, including
    the normal distribution, the exponential distribution, and the uniform distribution.
    Next, we introduced common sampling distributions and their use in statistical
    inferences for population statistics. Finally, we covered order statistics and
    their use in calculating the VaR in the context of daily stock returns.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover statistical estimation procedures, including
    point estimation, the central limit theorem, and the confidence interval.
  prefs: []
  type: TYPE_NORMAL
