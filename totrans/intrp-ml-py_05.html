<html><head></head><body>
  <div id="_idContainer151" class="Basic-Text-Frame">
    <h1 class="chapterNumber">5</h1>
    <h1 id="_idParaDest-131" class="chapterTitle">Local Model-Agnostic Interpretation Methods</h1>
    <p class="normal">In the previous two chapters, we dealt exclusively with global interpretation methods. This chapter will foray into<a id="_idIndexMarker498"/> local interpretation methods, which are there to explain why a single prediction or a group of predictions was made. It will cover how to leverage <strong class="keyWord">SHapley Additive exPlanations</strong> (<strong class="keyWord">SHAP’s</strong>) <code class="inlineCode">KernelExplainer</code> and also another method called <strong class="keyWord">Local Interpretable Model-agnostic Explanations</strong> (<strong class="keyWord">LIME</strong>) for local interpretations. We will also explore how to use these methods with both tabular and text data.</p>
    <p class="normal">These are the main topics we are going to cover in this chapter:</p>
    <ul>
      <li class="bulletList">Leveraging SHAP’s <code class="inlineCode">KernelExplainer</code> for local interpretations with SHAP values</li>
      <li class="bulletList">Employing LIME</li>
      <li class="bulletList">Using LIME for <strong class="keyWord">Natural Language Processing</strong> (<strong class="keyWord">NLP</strong>)</li>
      <li class="bulletList">Trying SHAP for NLP</li>
      <li class="bulletList">Comparing SHAP with LIME</li>
    </ul>
    <h1 id="_idParaDest-132" class="heading-1">Technical requirements</h1>
    <p class="normal">This chapter’s example uses the <code class="inlineCode">mldatasets</code>, <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">nltk</code>, <code class="inlineCode">lightgbm</code>, <code class="inlineCode">rulefit</code>, <code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">shap</code>, and <code class="inlineCode">lime</code> libraries. Instructions on how to install all of these libraries are in the <em class="italic">Preface</em> of the book. </p>
    <div class="note">
      <p class="normal">The code for this chapter is located here: <a href="https://packt.link/SRqJp"><span class="url">https://packt.link/SRqJp</span></a>.</p>
    </div>
    <h1 id="_idParaDest-133" class="heading-1">The mission</h1>
    <p class="normal">Who doesn’t love chocolate?! It’s a global favorite, with around nine out of ten people loving it and about a billion people eating it every day. One popular form in which it is consumed is a chocolate bar. However, even universally beloved ingredients can be used in ways that aren’t universally <a id="_idIndexMarker499"/>appealing—so, chocolate bars can range from the sublime to the mediocre to the downright unpleasant. </p>
    <p class="normal">Often, this is solely determined by the quality of the cocoa or additional ingredients, and sometimes it becomes an acquired taste once it’s combined with exotic flavors.</p>
    <p class="normal">A French chocolate manufacturer obsessed with excellence has reached out to you. They have a problem. All of their bars have been highly rated by critics, yet critics have very particular taste buds. And some bars they love have inexplicably mediocre sales, but non-critics seem to like them in focus groups and tastings, so they are puzzled why sales don’t coincide with their market research. They have found a dataset of chocolate bars rated by knowledgeable lovers of chocolate, and these ratings happen to coincide with their sales. To get an unbiased opinion, they have sought your expertise.</p>
    <p class="normal">As for the dataset, members of the <em class="italic">Manhattan Chocolate Society</em> have been meeting since 2007 for the sole purpose of tasting and judging fine chocolate to educate consumers and inspire chocolate makers to produce higher-quality chocolate. Since then, they have compiled a dataset of over 2,200 chocolate bars, rated by their members with the following scale:</p>
    <ul>
      <li class="bulletList">4.0–5.00 = <em class="italic">Outstanding</em></li>
      <li class="bulletList">3.5–3.99 = <em class="italic">Highly Recommended</em></li>
      <li class="bulletList">3.0–3.49 = <em class="italic">Recommended</em></li>
      <li class="bulletList">2.0–2.99 = <em class="italic">Disappointing</em></li>
      <li class="bulletList">1.0–1.90 = <em class="italic">Unpleasant</em></li>
    </ul>
    <p class="normal">These ratings are derived from a rubric that factors in aroma, appearance, texture, flavor, aftertaste, and overall opinion, and the bars rated are mostly darker chocolate bars since the aim is to appreciate the flavors of cacao. In addition to the ratings, the <em class="italic">Manhattan Chocolate Society</em> dataset includes many characteristics, such as the country where the cocoa bean was farmed, how many ingredients the bar has, whether it includes salt, and the words used to describe it.</p>
    <p class="normal">The goal is to understand why one of the chocolate manufacturers’ bars is rated <em class="italic">Outstanding</em> yet sells poorly, while<a id="_idIndexMarker500"/> another one, whose sales are impressive, is rated as <em class="italic">Disappointing</em>.</p>
    <h1 id="_idParaDest-134" class="heading-1">The approach</h1>
    <p class="normal">You have decided to use local model interpretation to explain why each bar is rated as it is. To that end, you will prepare the<a id="_idIndexMarker501"/> dataset and then train classification models to predict if chocolate bar ratings are above or equal to <em class="italic">Highly Recommended</em>, because the client would like all their bars to fall above this threshold. You will need to train two models: one for tabular data, and another NLP<a id="_idIndexMarker502"/> one for the words used to describe the chocolate bars. We will employ <strong class="keyWord">Support Vector Machines</strong> (<strong class="keyWord">SVMs</strong>) and <strong class="keyWord">Light Gradient Boosting Machine</strong> (<strong class="keyWord">LightGBM</strong>), respectively, for these tasks. If you haven’t used these <a id="_idIndexMarker503"/>black-box models, no worries—we will briefly explain them. Once you have trained the models, then comes the fun part: leveraging two local model-agnostic interpretation methods to understand what makes a specific chocolate bar <em class="italic">Highly Recommended</em> or not.</p>
    <p class="normal">These explanation methods are SHAP and LIME, which when combined will provide a richer explanation to convey back to your client. Then, we will compare both methods to understand their strengths and limitations.</p>
    <h1 id="_idParaDest-135" class="heading-1">The preparations</h1>
    <ul>
      <li class="bulletList">You will find the code for this example here: <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/05/ChocoRatings.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/05/ChocoRatings.ipynb</span></a> </li>
    </ul>
    <h2 id="_idParaDest-136" class="heading-2">Loading the libraries</h2>
    <p class="normal">To run this example, you <a id="_idIndexMarker504"/>need to install the following libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">mldatasets</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, and <code class="inlineCode">nltk</code> to manipulate it</li>
      <li class="bulletList"><code class="inlineCode">sklearn</code> (scikit-learn) and <code class="inlineCode">lightgbm</code> to split the data and fit the models</li>
      <li class="bulletList"><code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">shap</code>, and <code class="inlineCode">lime</code> to visualize the interpretations</li>
    </ul>
    <p class="normal">You should load all of them first, as <a id="_idIndexMarker505"/>follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">import</span> nltk
<span class="hljs-keyword">from</span> nltk.probability <span class="hljs-keyword">import</span> FreqDist
<span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> word_tokenize
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> sklearn.pipeline <span class="hljs-keyword">import</span> make_pipeline
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics, svm
<span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> TfidfVectorizer
<span class="hljs-keyword">import</span> lightgbm <span class="hljs-keyword">as</span> lgb
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">import</span> shap
<span class="hljs-keyword">import</span> lime
<span class="hljs-keyword">import</span> lime.lime_tabular
<span class="hljs-keyword">from</span> lime.lime_text <span class="hljs-keyword">import</span> LimeTextExplainer
</code></pre>
    <h2 id="_idParaDest-137" class="heading-2">Understanding and preparing the data</h2>
    <p class="normal">We load the data into a<a id="_idIndexMarker506"/> DataFrame we call <code class="inlineCode">chocolateratings_df</code>, like this:</p>
    <pre class="programlisting code"><code class="hljs-code">chocolateratings_df = mldatasets.load(<span class="hljs-string">"chocolate-bar-ratings_v2"</span>)
</code></pre>
    <p class="normal">There should be over 2,200 records and 18 columns. We can verify this was the case simply by inspecting the contents of the DataFrame, like this:</p>
    <pre class="programlisting code"><code class="hljs-code">chocolateratings_df
</code></pre>
    <p class="normal">The output shown here in <em class="italic">Figure 5.1</em> corresponds to what we were expecting:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_01.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.1: Contents of the chocolate bar dataset</p>
    <h3 id="_idParaDest-138" class="heading-3">The data dictionary</h3>
    <p class="normal">The data dictionary<a id="_idIndexMarker507"/> comprises the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">company</code>: Categorical; the manufacturer of the chocolate bar (out of over 500 different ones)</li>
      <li class="bulletList"><code class="inlineCode">company_location</code>: Categorical; the country of the manufacturer (66 different countries)</li>
      <li class="bulletList"><code class="inlineCode">review_date</code>: Continuous; the year in which the bar was reviewed (from 2006 to 2020)</li>
      <li class="bulletList"><code class="inlineCode">country_of_bean_origin</code>: Categorical; the country where the cocoa beans were harvested (62 different countries)</li>
      <li class="bulletList"><code class="inlineCode">cocoa_percent</code>: Categorical; what percentage of the bar is cocoa</li>
      <li class="bulletList"><code class="inlineCode">rating</code>: Continuous; the rating given by the <em class="italic">Manhattan Chocolate Society</em> (possible values: 1–5)</li>
      <li class="bulletList"><code class="inlineCode">counts_of_ingredients</code>: Continuous; the amount of ingredients in the bar</li>
      <li class="bulletList"><code class="inlineCode">cocoa_butter</code>: Binary; was it made with cocoa butter? </li>
      <li class="bulletList"><code class="inlineCode">vanilla</code>: Binary; was it made with vanilla? </li>
      <li class="bulletList"><code class="inlineCode">lecithin</code>: Binary; was it made with lecithin? </li>
      <li class="bulletList"><code class="inlineCode">salt</code>: Binary; was it made with salt? </li>
      <li class="bulletList"><code class="inlineCode">sugar</code>: Binary; was it made with sugar?</li>
      <li class="bulletList"><code class="inlineCode">sweetener_without_sugar</code>: Binary; was it made with sweetener without sugar? </li>
      <li class="bulletList"><code class="inlineCode">first_taste</code>: Text; word(s) used to describe the first taste</li>
      <li class="bulletList"><code class="inlineCode">second_taste</code>: Text; word(s) used to describe the second taste</li>
      <li class="bulletList"><code class="inlineCode">third_taste</code>: Text; word(s) used to describe the third taste</li>
      <li class="bulletList"><code class="inlineCode">fourth_taste</code>: Text; word(s) used to describe the fourth taste</li>
    </ul>
    <p class="normal">Now that we have taken a peek <a id="_idIndexMarker508"/>at the data, we can quickly prepare it and then work on the modeling and interpretation!</p>
    <h3 id="_idParaDest-139" class="heading-3">Data preparation</h3>
    <p class="normal">The first thing we ought to do is set aside the<a id="_idIndexMarker509"/> text features so that we can process them separately. We can start by creating a DataFrame named <code class="inlineCode">tastes_df</code> containing text features and then drop them from <code class="inlineCode">chocolateratings_df</code>. We can then explore <code class="inlineCode">tastes_df</code> using <code class="inlineCode">head</code> and <code class="inlineCode">tail</code>, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">tastes_df = chocolateratings_df[
    [<span class="hljs-string">'first_taste'</span>, <span class="hljs-string">'second_taste'</span>, <span class="hljs-string">'third_taste'</span>, <span class="hljs-string">'fourth_taste'</span>]
]
chocolateratings_df = chocolateratings_df.drop(
    [<span class="hljs-string">'first_taste'</span>, <span class="hljs-string">'second_taste'</span>,
     <span class="hljs-string">'third_taste'</span>, <span class="hljs-string">'fourth_taste'</span>],axis=<span class="hljs-number">1</span>
)
tastes_df.head(90).tail(90)
</code></pre>
    <p class="normal">The preceding code produces the DataFrame shown here in <em class="italic">Figure 5.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_02.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.2: The taste columns have quite a few null values</p>
    <p class="normal">Now, let’s categorically encode the categorical features. There are too many countries in <code class="inlineCode">company_location</code> and <code class="inlineCode">country_of_bean_origin</code>, so let’s establish a threshold. If, say, there are fewer than 3.333% (or 74 rows) for any country, let’s bucket them into an <code class="inlineCode">Other</code> category and then <a id="_idIndexMarker510"/>encode the categories. We can easily do this with the <code class="inlineCode">make_dummies_with_limits</code> function and the process is shown again in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">chocolateratings_df = mldatasets.make_dummies_with_limits(
    chocolateratings_df, <span class="hljs-string">'company_location'</span>, <span class="hljs-number">0.03333</span>
)
chocolateratings_df = mldatasets.make_dummies_with_limits(
    chocolateratings_df, <span class="hljs-string">'</span><span class="hljs-string">country_of_bean_origin'</span>, <span class="hljs-number">0.03333</span>
)
</code></pre>
    <p class="normal">Now, to process the content of <code class="inlineCode">tastes_df</code>, the following code replaces all the null values with empty strings, then joins all the columns in <code class="inlineCode">tastes_df</code> together, forming a single series. Then, it strips leading and trailing whitespace. The code is illustrated in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">tastes_s = tastes_df.replace(
    np.nan, <span class="hljs-string">''</span>, regex=<span class="hljs-literal">True</span>).agg(<span class="hljs-string">' '</span>.join, axis=<span class="hljs-number">1</span>).<span class="hljs-built_in">str</span>.strip()
</code></pre>
    <p class="normal">And voilà! You can verify that the result is a <code class="inlineCode">pandas</code> series (<code class="inlineCode">tastes_s</code>) with (mostly) taste-related adjectives <a id="_idIndexMarker511"/>by printing it. As expected, this series is the same length as the <code class="inlineCode">chocolateratings_df</code> DataFrame, as illustrated in the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">0          cocoa blackberry robust
1             cocoa vegetal savory
2                rich fatty bready
3              fruity melon roasty
4                    vegetal nutty
                   ...            
2221       muted roasty accessible
2222    fatty mild nuts mild fruit
2223            fatty earthy cocoa
Length: 2224, dtype: object
</code></pre>
    <p class="normal">But let’s find out how many of its phrases are unique, with <code class="inlineCode">print(np.unique(tastes_s).shape)</code>. Since the output is (<code class="inlineCode">2178</code>,), that means fewer than 50 phrases are duplicated, so tokenizing by phrases would be a bad idea since so few of them are repeated. After all, when tokenizing, we want the elements to repeat enough times to make it worthwhile. </p>
    <p class="normal">There are many approaches you could take here, such as tokenizing by bi-grams (sequences of two words) or even subwords (dividing words into logical parts). However, even though order matters slightly (because the first words are related to the first taste, and so on), our dataset is too small and had too many nulls (especially in <code class="inlineCode">third taste</code> and <code class="inlineCode">fourth taste</code>) to derive meaning from the order. This is why it was a good choice to concatenate all the “tastes” together, thus removing their discernible division.</p>
    <p class="normal">Another thing to note is that our words are (mostly) adjectives such as “fruity” and “nutty”. We made a small effort to remove adverbs such as “sweetly”, but there are still some nouns present, such as “fruit” and “nuts”, versus adjectives such as “fruity” and “nutty”. We can’t be sure if the chocolate connoisseurs who judged the bars meant something different by using “fruit” rather than “fruity”. However, if we were sure of this, we could have performed <strong class="keyWord">stemming</strong> or <strong class="keyWord">lemmatization</strong> to turn all instances of “fruit”, “fruity”, and “fruitiness” into a consistent “fru” (<em class="italic">stem</em>) or “fruiti” (<em class="italic">lemma</em>). We won’t concern ourselves with this because many of our adjectives’ variations are not as common in the phrases anyway.</p>
    <p class="normal">Let’s find out the most <a id="_idIndexMarker512"/>common words by first tokenizing them with <code class="inlineCode">word_tokenize</code> and using <code class="inlineCode">FreqDist</code> to count their frequency. We can then place the resulting <code class="inlineCode">tastewords_fdist</code> dictionary into a DataFrame (<code class="inlineCode">tastewords_df</code>). We can save only those words with more than 74 instances as a list (<code class="inlineCode">commontastes_l</code>). The code is illustrated in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">tastewords_fdist = FreqDist(
    word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(tastes_s.<span class="hljs-built_in">str</span>.cat(sep=<span class="hljs-string">' '</span>))
)
tastewords_df = pd.DataFrame.from_dict(
    tastewords_fdist, orient=<span class="hljs-string">'index'</span>).rename(columns={<span class="hljs-number">0</span>:<span class="hljs-string">'freq'</span>}
)
commontastes_l = tastewords_df[
    tastewords_df.freq &gt; <span class="hljs-number">74</span>].index.to_list()
<span class="hljs-built_in">print</span>(commontastes_l)
</code></pre>
    <p class="normal">As you can tell from the following output for <code class="inlineCode">commontastes_l</code>, the most common words are mostly different (except for <code class="inlineCode">spice</code> and <code class="inlineCode">spicy</code>):</p>
    <pre class="programlisting con"><code class="hljs-con">['cocoa', 'rich', 'fatty', 'roasty', 'nutty', 'sweet', 'sandy', 'sour', 'intense', 'mild', 'fruit', 'sticky', 'earthy', 'spice', 'molasses', 'floral', 'spicy', 'woody', 'coffee', 'berry', 'vanilla', 'creamy']
</code></pre>
    <p class="normal">Something we can do with this list to enhance our tabular dataset is turn these common words into binary features. In other words, there will be a column for each one of these “common tastes” (<code class="inlineCode">commontastes_l</code>), and if the “tastes” for the chocolate bar include it, the column will contain a 1, otherwise a 0. Fortunately, we can easily do this with two lines of code. First, we create a new column with our text-tastes series (<code class="inlineCode">tastes_s</code>). Then, we use the <code class="inlineCode">make_dummies_from_dict</code> function we used in the last chapter to generate the dummy features by looking for each “common taste” in the contents of our new column, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">chocolateratings_df[<span class="hljs-string">'tastes'</span>] = tastes_s
chocolateratings_df = mldatasets.make_dummies_from_dict(
    chocolateratings_df, <span class="hljs-string">'tastes'</span>, commontastes_l)
</code></pre>
    <p class="normal">Now that we are done with our feature engineering, we can use <code class="inlineCode">info()</code> to examine our DataFrame. The output has all numeric non-null features except for <code class="inlineCode">company</code>. There are over 500 companies, so <strong class="keyWord">categorical encoding</strong> of this <a id="_idIndexMarker513"/>feature would be complicated and, because it would be advisable to bucket most companies as <code class="inlineCode">Other</code>, it would likely introduce bias toward the few companies that are most represented. Therefore, it’s better to remove this column<a id="_idIndexMarker514"/> altogether. The output is shown here:</p>
    <pre class="programlisting con"><code class="hljs-con">RangeIndex: 2224 entries, 0 to 2223
Data columns (total 46 columns):
#   Column                      Non-Null Count  Dtype  
---  ------                      --------------  -----  
0   company                     2224 non-null   object
1   review_date                 2224 non-null   int64  
2   cocoa_percent               2224 non-null   float64
:        :                         :     :        :
43  tastes_berry                2224 non-null   int64  
44  tastes_vanilla              2224 non-null   int64  
45  tastes_creamy               2224 non-null   int64  
dtypes: float64(2), int64(30), object(1), uint8(13)
</code></pre>
    <p class="normal">Our last step to prepare the data for modeling starts with initializing <code class="inlineCode">rand</code>, a constant to serve as our “random state” throughout this exercise. Then, we define <code class="inlineCode">y</code> as the <code class="inlineCode">rating</code> column converted to 1 if greater than or equal to <code class="inlineCode">3.5</code>, and <code class="inlineCode">0</code> otherwise. <code class="inlineCode">X</code> is everything else (excluding <code class="inlineCode">company</code>). Then, we split <code class="inlineCode">X</code> and <code class="inlineCode">y</code> into train and test datasets with <code class="inlineCode">train_test_split</code>, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">rand = <span class="hljs-number">9</span>
y = chocolateratings_df[<span class="hljs-string">'rating'</span>].\
apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x &gt;= <span class="hljs-number">3.5</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
X = chocolateratings_df.drop([<span class="hljs-string">'rating'</span>,<span class="hljs-string">'company'</span>], axis=<span class="hljs-number">1</span>).copy()
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=<span class="hljs-number">0.33</span>, random_state=rand)
</code></pre>
    <p class="normal">In addition to the tabular test and train datasets, for our NLP models, we will need text-only feature datasets that are consistent with our <code class="inlineCode">train_test_split</code> so that we can use the same <code class="inlineCode">y</code> labels. To this end, we can do this by subsetting our tastes series (<code class="inlineCode">tastes_s</code>) using the <code class="inlineCode">index</code> of our <code class="inlineCode">X_train</code> and <code class="inlineCode">X_test</code> sets to yield NLP-specific versions of the series, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train_nlp = tastes_s[X_train.index]
X_test_nlp = tastes_s[X_test.index]
</code></pre>
    <p class="normal">OK! We are all set now. Let’s<a id="_idIndexMarker515"/> start modeling and interpreting our models!</p>
    <h1 id="_idParaDest-140" class="heading-1">Leveraging SHAP’s KernelExplainer for local interpretations with SHAP values</h1>
    <p class="normal">For this section, and<a id="_idIndexMarker516"/> for subsequent use, we will<a id="_idIndexMarker517"/> train a <strong class="keyWord">Support Vector Classifier</strong> (<strong class="keyWord">SVC</strong>) model first.</p>
    <h2 id="_idParaDest-141" class="heading-2">Training a C-SVC model</h2>
    <p class="normal">SVM is a family of model <a id="_idIndexMarker518"/>classes that operate in high-dimensional space to find an optimal hyperplane, where they attempt to separate the classes with the maximum margin between them. Support vectors are the points closest to the decision boundary (the dividing hyperplane) that would change it if were removed. To find the best hyperplane, they use a <a id="_idIndexMarker519"/>cost function called <strong class="keyWord">hinge loss</strong> and a computationally cheap method to operate in high-dimensional space, called the <strong class="keyWord">kernel trick</strong>, and <a id="_idIndexMarker520"/>even though a hyperplane suggests linear separability, it’s not always limited to a linear kernel.</p>
    <p class="normal">The scikit-learn implementation we will use is called C-SVC. SVC uses an L2 regularization parameter called <em class="italic">C</em> and, by default, uses <a id="_idIndexMarker521"/>a kernel called the <strong class="keyWord">Radial Basis Function </strong>(<strong class="keyWord">RBF</strong>), which is decidedly nonlinear. For an RBF, a <strong class="keyWord">gamma</strong> hyperparameter<a id="_idIndexMarker522"/> defines the radius of influence of each training example in the kernel, but in an inversely proportional fashion. Hence, a low value increases the radius, while a high value decreases it.</p>
    <p class="normal">The SVM family includes several variations for classification and even regression classes through <strong class="keyWord">Support Vector Regression</strong> (<strong class="keyWord">SVR</strong>). The<a id="_idIndexMarker523"/> most significant advantage of SVM models is that they tend to work effectively and efficiently when there are many features compared to the observations, and even when the features exceed the observations! It also tends to find latent nonlinear relationships in the data, without overfitting or becoming unstable. However, SVMs are not as scalable to larger datasets, and it’s hard to tune their hyperparameters.</p>
    <p class="normal">Since we will use <code class="inlineCode">seaborn</code> plot styling, which is activated with <code class="inlineCode">set()</code>, for some of this chapter’s plots, we will first save the original <code class="inlineCode">matplotlib</code> settings (<code class="inlineCode">rcParams</code>) so that we can restore them later. One thing to note about <code class="inlineCode">SVC</code> is that it doesn’t natively produce probabilities since it’s linear algebra. However, if <code class="inlineCode">probability=True</code>, the scikit-learn implementation uses<a id="_idIndexMarker524"/> cross-validation and then fits a logistic regression model to the SVC’s scores to produce the probabilities. We are also using <code class="inlineCode">gamma=auto</code>, which means it is set to 1/# features—so, 1/44. As always, it is recommended to set your <code class="inlineCode">random_state</code> parameter for reproducibility. Once we fit the model to the training data, we can use <code class="inlineCode">evaluate_class_mdl</code> to <a id="_idIndexMarker525"/>evaluate our model’s predictive performance, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">svm_mdl = svm.SVC(probability=<span class="hljs-literal">True</span>, gamma=<span class="hljs-string">'auto'</span>, random_state=rand)
fitted_svm_mdl = svm_mdl.fit(X_train, y_train)
y_train_svc_pred, y_test_svc_prob, y_test_svc_pred =\
    mldatasets.evaluate_class_mdl(
    fitted_svm_mdl, X_train, X_test, y_train, y_test
)
</code></pre>
    <p class="normal">The preceding code produces the output shown here in <em class="italic">Figure 5.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_03.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.3: Predictive performance of our SVC model</p>
    <p class="normal"><em class="italic">Figure 5.3</em> shows the<a id="_idIndexMarker526"/> performance achieved is not bad, considering this is a small imbalanced dataset in an already challenging<a id="_idIndexMarker527"/> domain for machine learning models’ user ratings. In any<a id="_idIndexMarker528"/> case, the <strong class="keyWord">Area Under the Curve</strong> (<strong class="keyWord">AUC</strong>) curve is above the dotted coin toss line, and the <strong class="keyWord">Matthews correlation coefficient</strong> (<strong class="keyWord">MCC</strong>) is safely <a id="_idIndexMarker529"/>above 0. More importantly, precision is substantially higher than recall, and this is very good given the hypothetical cost of misclassifying a lousy chocolate bar as <em class="italic">Highly Recommended</em>. We favor precision over recall because we would prefer to have fewer false positives than false negatives.</p>
    <h2 id="_idParaDest-142" class="heading-2">Computing SHAP values using KernelExplainer</h2>
    <p class="normal">Given how computationally intensive calculating SHAP values by brute force can be, the SHAP library takes many <a id="_idIndexMarker530"/>statistically valid shortcuts. As we learned in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>, these shortcuts range from leveraging a decision tree’s structure (<code class="inlineCode">TreeExplainer</code>) to the difference<a id="_idIndexMarker531"/> in a neural network’s activations, a baseline (<code class="inlineCode">DeepExplainer</code>) to a neural network’s gradient (<code class="inlineCode">GradientExplainer</code>). These shortcuts make the explainers model-specific since they are limited to<a id="_idIndexMarker532"/> a family of model classes. However, there is a model-agnostic explainer in SHAP, called the <code class="inlineCode">KernelExplainer</code>.</p>
    <p class="normal"><code class="inlineCode">KernelExplainer</code> has two shortcuts; it samples a subset of all feature permutations for coalitions and uses a weighting scheme according to the size of the coalition to compute SHAP values. The first shortcut is a recommended technique to reduce computation time. The second one is drawn from LIME’s weighting scheme, which we will cover next in this chapter, and the authors of SHAP did this so that it remains compliant with Shapley. However, for “missing” features in the coalition, it randomly samples from the features’ values in a background training dataset, which violates the <strong class="keyWord">dummy</strong> property of Shapley values. More importantly, as with <strong class="keyWord">permutation feature importance</strong>, if there’s multicollinearity, it puts too much weight on unlikely<a id="_idIndexMarker533"/> instances. Despite this near-fatal flaw, <code class="inlineCode">KernelExplainer</code> has all the other benefits of Shapley values and at least one of LIME’s main advantages.</p>
    <p class="normal">Before we engage with the <code class="inlineCode">KernelExplainer</code>, it’s important to note that for classification models, it yields a list of multiple SHAP values. We can access the list of values for each class with an index. Confusion may arise if this index is not in the order we expect because it’s in the order provided by the model. So, it is essential to make sure of the order of the classes in our model by running <code class="inlineCode">print(svm_mdl.classes_)</code>.</p>
    <p class="normal">The output <code class="inlineCode">array([0, 1])</code> tells us that <em class="italic">Not Highly Recommended</em> has an index of 0, as we would expect, and <em class="italic">Highly Recommended</em> has an index of 1. We are interested in the SHAP values for the latter because this is what we are trying to predict.</p>
    <p class="normal"><code class="inlineCode">KernelExplainer</code> takes a <code class="inlineCode">predict</code> function for a model (<code class="inlineCode">fitted_svm_mdl.predict_proba</code>) and some background training data (<code class="inlineCode">X_train_summary</code>). <code class="inlineCode">KernelExplainer</code> leverages additional measures to minimize computation. One of these<a id="_idIndexMarker534"/> is using <strong class="keyWord">k-means</strong> to summarize the background training data instead of using it whole. Another method could be using a sample of the training data. In this case, we opted for k-means clustering into 10 <a id="_idIndexMarker535"/>centroids. Once we have <a id="_idIndexMarker536"/>initialized our explainer, we can use samples of our test dataset (<code class="inlineCode">nsamples=200</code>) to come up with the SHAP values. It uses L1 regularization (<code class="inlineCode">l1_reg</code>) during the fitting process. What we are telling it here is to regularize to a point where it only has 20 relevant features. Lastly, we can use a <code class="inlineCode">summary_plot</code> to plot<a id="_idIndexMarker537"/> our SHAP values for class 1. The code is illustrated in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">np.random.seed(rand)
X_train_summary = shap.kmeans(X_train, <span class="hljs-number">10</span>)
shap_svm_explainer = shap.KernelExplainer(
    fitted_svm_mdl.predict_proba, X_train_summary
)
shap_svm_values_test = shap_svm_explainer.shap_values(
    X_test, nsamples=<span class="hljs-number">200</span>, l1_reg=<span class="hljs-string">"num_features(20)"</span>
)
shap.summary_plot(shap_svm_values_test[<span class="hljs-number">1</span>], X_test, plot_type=<span class="hljs-string">"dot"</span>)
</code></pre>
    <p class="normal">The preceding code produces the output shown in <em class="italic">Figure 5.4</em>. Even though the point of this chapter is local model interpretation, it’s important to start with the global form of this to make sure outcomes are intuitive. If they aren’t, perhaps something is amiss:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_04.png" alt="A picture containing graphical user interface  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.4: Global model interpretation with SHAP using a summary plot</p>
    <p class="normal">In <em class="italic">Figure 5.4</em>, we can tell that the highest (red) cocoa percentages (<code class="inlineCode">cocoa_percent</code>) tend to correlate with a decrease in<a id="_idIndexMarker538"/> the likelihood of <em class="italic">Highly Recommended</em>, but the middle values (purple) tend to<a id="_idIndexMarker539"/> increase it. This finding makes intuitive sense because the darkest chocolates are more of <a id="_idIndexMarker540"/>an acquired taste than less-dark chocolates. The low values (blue) are scattered throughout so they show no trend, but this could be because there aren’t many. </p>
    <p class="normal">On the other hand, <code class="inlineCode">review_date</code> suggests that it was likely to be <em class="italic">Highly Recommended</em> in earlier years. There are significant shades of red and purple on both sides of 0, so it’s hard to identify a trend here. A dependence plot, such as those used in <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>, would be better for this purpose. However, it’s very easy for binary features to visualize how high and low values, ones and zeros, impact the model. For instance, we can tell that the presence of cocoa, creamy, rich, and berry tastes increases the likelihood of <a id="_idIndexMarker541"/>the chocolate being recommended, while <a id="_idIndexMarker542"/>sweet, earthy, sour, and fatty tastes do the<a id="_idIndexMarker543"/> opposite. Likewise, the odds for <em class="italic">Highly</em> <em class="italic">Recommended</em> decrease if the chocolate was manufactured in the US! Sorry, the US.</p>
    <h2 id="_idParaDest-143" class="heading-2">Local interpretation for a group of predictions using decision plots</h2>
    <p class="normal">For local interpretation, you don’t have to visualize one point at a time—you can instead interpret several at a time. The key is <a id="_idIndexMarker544"/>providing some context to compare the points adequately, and there can’t be so many that you can’t distinguish them. Usually, you would find outliers or only those that meet specific criteria. For this exercise, we will select only those bars that were produced by your client, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">sample_test_idx = X_test.index.get_indexer_for(
    [<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">18</span>,<span class="hljs-number">19</span>,<span class="hljs-number">21</span>,<span class="hljs-number">24</span>,<span class="hljs-number">25</span>,<span class="hljs-number">27</span>]
)
</code></pre>
    <p class="normal">One great thing about Shapley is its additivity property, which can be easily demonstrated. If you add all the SHAP values to the expected value used to compute them, you get a prediction. Of course, this is a classification problem, so the prediction is a probability; so, to get a Boolean array instead, we have to check if the probability is greater than 0.5. We can check if this Boolean array matches our model’s test dataset predictions <code class="inlineCode">(y_test_svc_pred</code>) by running the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">expected_value = shap_svm_explainer.expected_value[<span class="hljs-number">1</span>]
y_test_shap_pred =\
    (shap_svm_values_test[<span class="hljs-number">1</span>].<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>) + expected_value) &gt; <span class="hljs-number">0.5</span>
<span class="hljs-built_in">print</span>(np.array_equal(y_test_shap_pred, y_test_svc_pred))
</code></pre>
    <p class="normal">It should, and it does! You can see it confirmed with a <code class="inlineCode">True</code> value.</p>
    <p class="normal">SHAP’s decision plot comes with a highlight feature that we can use to make false negatives (<code class="inlineCode">FN</code>) stand out. Now, let’s figure out which of our sample observations are <code class="inlineCode">FN</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">FN = (~y_test_shap_pred[sample_test_idx]) &amp;
    (y_test.iloc[sample_test_idx] == <span class="hljs-number">1</span>).to_numpy()
</code></pre>
    <p class="normal">We can now quickly reset and plot a <code class="inlineCode">decision_plot</code>. It takes the <code class="inlineCode">expected_value</code>, the SHAP values, and the actual values <a id="_idIndexMarker545"/>of those items we wish to plot. Optionally, we can provide a Boolean array of the items we want to highlight, with dotted lines—in this case, the false<a id="_idIndexMarker546"/> negatives (<code class="inlineCode">FN</code>), as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.decision_plot(
    expected_value, shap_svm_values_test[<span class="hljs-number">1</span>][sample_test_idx],\
    X_test.iloc[sample_test_idx], highlight=FN)
</code></pre>
    <p class="normal">The plot produced in <em class="italic">Figure 5.5</em> has a single color-coded line for each observation.</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_05.png" alt="Chart, radar chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.5: Local model interpretation with SHAP for a sample of predictions, highlighting false negatives</p>
    <p class="normal">The color of each line represents not <a id="_idIndexMarker547"/>the value of any feature, but the model output. Since we used <code class="inlineCode">predict_proba</code> in <code class="inlineCode">KernelExplainer</code>, this is a probability, but otherwise, it would have displayed SHAP values, and the value they have when they strike the top <em class="italic">x</em>-axis is the predicted value. The features are sorted in terms of importance but only among the observations plotted, and you can tell that the lines increase and decrease horizontally depending on each feature. How much they vary and in which direction depends on the feature’s contribution to the outcome. The gray line represents the<a id="_idIndexMarker548"/> class’s expected value, which is like the intercept in a linear model. In fact, similarly, all lines start at this value, making it best to read the plot from bottom to top.</p>
    <p class="normal">You can tell that there are three false negatives plotted in <em class="italic">Figure 5.5</em> because they have dotted lines. Using this plot, we can easily visualize which features made them veer toward the left the most because this is what made them negative predictions. </p>
    <p class="normal">For instance, we know that the leftmost false negative was to the right of the expected value line until <code class="inlineCode">lecithin</code> and then continued decreasing until <code class="inlineCode">company_location_France</code>, and <code class="inlineCode">review_date</code> increased its <a id="_idIndexMarker549"/>likelihood of <em class="italic">Highly Recommended</em>, but it wasn’t enough. You can tell that <code class="inlineCode">county_of_bean_origin_Other</code> decreased the likelihood of two of the misclassifications. This decision could be unfair because the country could be one of over 50 countries that didn’t get their own feature. Quite possibly, there’s a lot of variation between the beans of these countries grouped together.</p>
    <p class="normal">Decision plots can also isolate a single observation. When it does this, it prints the value of each feature next to the dotted line. Let’s plot one for a decision plot of the same company (true-positive observation #696), as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.decision_plot(
    expected_value, shap_svm_values_test[<span class="hljs-number">1</span>][<span class="hljs-number">696</span>],
    X_test.iloc[<span class="hljs-number">696</span>],highlight=<span class="hljs-number">0</span>
)
</code></pre>
    <p class="normal"><em class="italic">Figure 5.6</em> here was outputted by the preceding code:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_06.png" alt="Graphical user interface, table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.6: The SHAP decision plot for a single true positive in the sample of predictions</p>
    <p class="normal">In <em class="italic">Figure 5.6</em>, you can see that <code class="inlineCode">lecithin</code> and <code class="inlineCode">counts_of_ingredients</code> decreased the <em class="italic">Highly Recommended</em> likelihood to <a id="_idIndexMarker550"/>a point where they could have jeopardized it. Fortunately, all features<a id="_idIndexMarker551"/> above those veered the line decidedly rightward because <code class="inlineCode">company_location_France=1</code>, <code class="inlineCode">cocoa_percent=70</code>, and <code class="inlineCode">tastes_berry=1</code> are all favorable.</p>
    <h2 id="_idParaDest-144" class="heading-2">Local interpretation for a single prediction at a time using a force plot</h2>
    <p class="normal">Your client, the chocolate manufacturer, has two bars they<a id="_idIndexMarker552"/> want you to compare. Bar #5 is <em class="italic">Outstanding</em> and #24 is <em class="italic">Disappointing</em>. They are both in your test dataset. One way of comparing them is to place their values side by side in a DataFrame to understand how exactly they differ. We<a id="_idIndexMarker553"/> will concatenate the rating, the actual label <code class="inlineCode">y</code>, and the <code class="inlineCode">y_pred</code> predicted label to these observations’ values, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">eval_idxs = (X_test.index==<span class="hljs-number">5</span>) | (X_test.index==<span class="hljs-number">24</span>)
X_test_eval = X_test[eval_idxs]
eval_compare_df = pd.concat([
    chocolateratings_df.iloc[X_test[eval_idxs].index].rating,
    pd.DataFrame({<span class="hljs-string">'y'</span>:y_test[eval_idxs]}, index=[<span class="hljs-number">5</span>,<span class="hljs-number">24</span>]),
    pd.DataFrame({<span class="hljs-string">'y_pred'</span>:y_test_svc_pred[eval_idxs]},
    index=[<span class="hljs-number">24</span>,<span class="hljs-number">5</span>]), X_test_eval], axis=<span class="hljs-number">1</span>).transpose()
eval_compare_df
</code></pre>
    <p class="normal">The preceding code produces the DataFrame shown in <em class="italic">Figure 5.7</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_07.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.7: Observations #5 and #24 side by side, with feature differences highlighted in yellow</p>
    <p class="normal">With this DataFrame, you can confirm <a id="_idIndexMarker554"/>that they aren’t misclassifications because <code class="inlineCode">y=y_pred</code>. A misclassification could make model interpretations unreliable for understanding why <a id="_idIndexMarker555"/>people tend to like one chocolate bar more than another. Then, you can examine the features to spot the differences—for instance, you can tell that the <code class="inlineCode">review_date</code> is 2 years apart. Also, the beans for the <em class="italic">Outstanding</em> bar were from Venezuela, and the <em class="italic">Disappointing</em> beans came from another, lesser-represented country. The <em class="italic">Outstanding</em> one had a berry taste, and the <em class="italic">Disappointing</em> one was earthy.</p>
    <p class="normal">The force plot can tell us a complete story of what weighed in the model’s decisions (and, presumably, the reviewers’), and gives us clues as to what consumers might prefer. Plotting a <code class="inlineCode">force_plot</code> requires the expected value for the class of your interest (<code class="inlineCode">expected_value</code>), the SHAP values for the observation of your interest, and this observation’s actual values. We will start<a id="_idIndexMarker556"/> with observation #5, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.force_plot(
    expected_value,
    shap_svm_values_test[<span class="hljs-number">1</span>][X_test.index==<span class="hljs-number">5</span>],\
    X_test[X_test.index==<span class="hljs-number">5</span>],
    matplotlib=<span class="hljs-literal">True</span>
)
</code></pre>
    <p class="normal">The preceding code <a id="_idIndexMarker557"/>produces the plot shown in <em class="italic">Figure 5.8</em>. This force plot depicts how much <code class="inlineCode">review_date</code>, <code class="inlineCode">cocoa_percent</code>, and <code class="inlineCode">tastes_berry</code> weigh in the prediction, while the only feature that seems to be weighing in the opposite direction is <code class="inlineCode">counts_of_ingredients</code>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_08.png" alt="Timeline  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.8: The force plot for observation #5 (Outstanding)</p>
    <p class="normal">Let’s compare it with a force plot of observation #24, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.force_plot(expected_value,\  
                shap_svm_values_test[1][X_test.index==24],\
                X_test[X_test.index==24], matplotlib=True)
</code></pre>
    <p class="normal">The preceding code produces the plot shown in <em class="italic">Figure 5.9</em>. We can easily tell that <code class="inlineCode">tastes_earthy</code> and <code class="inlineCode">country_of_bean_origin_Other</code> are considered highly negative attributes by our model. The outcome can be mostly explained by the difference in the chocolate tasting of “berry” versus “earthy.” Despite our findings, the beans’ origin country needs further investigation. After all, it is possible that the actual country of origin doesn’t correlate with poor ratings.</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_09.png" alt="Timeline  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.9: The force plot for observation #24 (Disappointing)</p>
    <p class="normal">In this section, we<a id="_idIndexMarker558"/> covered the <code class="inlineCode">KernelExplainer</code>, which uses some tricks it learned from LIME. But what is LIME? We will find that out next!</p>
    <h1 id="_idParaDest-145" class="heading-1">Employing LIME</h1>
    <p class="normal">Until now, the model-agnostic interpretation methods we’ve covered attempt to reconcile the totality of outputs of a model with its inputs. For these methods to get a good idea of how and why <code class="inlineCode">X</code> becomes <code class="inlineCode">y_pred</code>, we need some data first. Then, we perform simulations with this data, pushing<a id="_idIndexMarker559"/> variations of it into a model and evaluating what comes out of the model. Sometimes, they even leverage a global surrogate to connect the dots. By using what we learned in this process, we yield feature importance values that quantify a feature’s impact, interactions, or decisions on a global level. For many methods such as SHAP, these can be observed locally too. However, even when they can be observed locally, what was quantified globally may not apply locally. For this reason, there should be another approach that quantifies the local effects of features solely for local interpretation—one such as LIME!</p>
    <h2 id="_idParaDest-146" class="heading-2">What is LIME?</h2>
    <p class="normal"><strong class="keyWord">LIME</strong> trains local surrogates to explain a single prediction. To this end, it starts by asking us which <em class="italic">data point</em> we want to interpret. You also provide it with your black-box model and a sample dataset. It then makes predictions on a <em class="italic">perturbed</em> version of the dataset with the model, creating a scheme whereby it samples and <em class="italic">weighs</em> points higher if they are <em class="italic">closer</em> to your chosen data point. This area around your point is called a neighborhood. Then, using the<a id="_idIndexMarker560"/> sampled points and black-box predictions in this neighborhood, it trains a weighted <em class="italic">intrinsically interpretable surrogate model</em>. Lastly, it interprets the surrogate model.</p>
    <p class="normal">There are lots of keywords to unpack here so let’s define them, as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Chosen data point</strong>: LIME<a id="_idIndexMarker561"/> calls the data point, row, or observation you want to interpret an <em class="italic">instance</em>. It’s just another word for this concept.</li>
      <li class="bulletList"><strong class="keyWord">Perturbation</strong>: LIME <a id="_idIndexMarker562"/>simulates new samples by adding noise to each sample. In other words, it creates<a id="_idIndexMarker563"/> random samples close to each instance.</li>
      <li class="bulletList"><strong class="keyWord">Weighting scheme</strong>: LIME <a id="_idIndexMarker564"/>uses an exponential smoothing kernel to both define the local instance neighborhood radius and determine how to weigh the points farthest versus those closest to the instance.</li>
      <li class="bulletList"><strong class="keyWord">Closer</strong>: LIME uses Euclidean distance for tabular and image data, and cosine similarity for text. This is hard to imagine in high-dimensional feature spaces, but you can calculate the distance between points for any number of dimensions and find which points are closest to the one of interest.</li>
      <li class="bulletList"><strong class="keyWord">Intrinsically interpretable surrogate model</strong>: LIME uses a sparse linear model with <a id="_idIndexMarker565"/>weighted ridge regularization. However, it could use any intrinsically interpretable model as long as the data points can be weighted. The idea behind this is twofold. It needs a model that can yield reliable intrinsically interpretable parameters, such as coefficients that indicate the influence of the feature on the prediction. It also needs to consider data points closest to the chosen point more because these are more relevant.</li>
    </ul>
    <p class="normal">Much like with <strong class="keyWord">k-Nearest Neighbors</strong> (<strong class="keyWord">k-NN</strong>), the intuition behind LIME is that points in a neighborhood have commonality<a id="_idIndexMarker566"/> because we expect points close to each other to have similar, if not the same, labels. There are decision boundaries for classifiers, so this could be a very naive assumption to make when close points are divided by one.</p>
    <p class="normal">Similar to another model class in the Nearest Neighbors family, <strong class="keyWord">Radius Nearest Neighbors</strong>, LIME factors in the distance <a id="_idIndexMarker567"/>along a radius and weighs points accordingly, although it does this exponentially. However, LIME is not a model class but an interpretation method, so the similarities stop there. Instead of “voting” for predictions among neighbors, it fits a weighted surrogate sparse linear model because it assumes that every complex model is linear locally, and because it’s not a model class, the predictions the surrogate model makes don’t matter. In fact, the surrogate model doesn’t even have to fit the data like a glove because all you need from it is the coefficients. Of course, that being said, it is best if it fits well so that there is higher fidelity in the interpretation.</p>
    <p class="normal">LIME works for tabular, image, and text data and generally has high local fidelity, meaning that it can approximate the model predictions quite well on a local level. However, this is contingent on the neighborhood being defined correctly, which stems from choosing the right kernel width and the assumption of local linearity holding true.</p>
    <h2 id="_idParaDest-147" class="heading-2">Local interpretation for a single prediction at a time using LimeTabularExplainer</h2>
    <p class="normal">To explain a<a id="_idIndexMarker568"/> single prediction, you first instantiate a <code class="inlineCode">LimeTabularExplainer</code> by providing it with your sample dataset in a <code class="inlineCode">numpy</code> 2D array (<code class="inlineCode">X_test.values</code>), a list with the names of the features (<code class="inlineCode">X_test.columns</code>), a list with the indices of the categorical features (only the first three features aren’t categorical), and the class names. Even though only the sample dataset is required, it is recommended that you provide names for your features and classes so that the interpretation makes sense. For tabular data, telling LIME which features are categorical (<code class="inlineCode">categorical_features</code>) is important because it treats categorical features differently from continuous ones, and not specifying this could potentially make for a poor-fitting local surrogate. Another parameter that can greatly impact the local surrogate is <code class="inlineCode">kernel_width</code>. This defines the diameter of the neighborhood, thus answering the question of what is considered local. It has a default value, which may or may not yield interpretations that make sense for your instance. You could tune this parameter on an instance-by-instance basis to ensure that each time you generate an explanation, it is consistent. Please note that the random nature of perturbation, when applied to a large neighborhood, might lead to inconsistent results. So as you make this neighborhood smaller, you will reduce the variability between runs. The code can be seen in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">lime_svm_explainer = lime.lime_tabular.LimeTabularExplainer(
    X_test.values,
    feature_names=X_test.columns,
    categorical_features=<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>,<span class="hljs-number">44</span>)),
    class_names=[<span class="hljs-string">'Not Highly Recomm.'</span>, <span class="hljs-string">'Highly Recomm.'</span>]
)
</code></pre>
    <p class="normal">With the instantiated explainer, you can now use <code class="inlineCode">explain_instance</code> to fit a local surrogate model to observation #5. We will also use our model’s classifier function (<code class="inlineCode">predict_proba</code>) and limit <a id="_idIndexMarker569"/>our number of features to eight (<code class="inlineCode">num_features=8</code>). We can take the “explanation” returned and immediately visualize it with <code class="inlineCode">show_in_notebook</code>. At the same time, the <code class="inlineCode">predict_proba</code> parameter makes sure it also includes a plot to show which class is the most probable, according to the local surrogate model. The code is illustrated in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">lime_svm_explainer.explain_instance(
    X_test[X_test.index==5].values[0],
    fitted_svm_mdl.predict_proba,
    num_features=8
).show_in_notebook(predict_proba=True)
</code></pre>
    <p class="normal">The preceding code provides the output shown in <em class="italic">Figure 5.10</em>. This figure can be read as other feature importance plots with the most influential features having the highest coefficients – and vice versa. However, it has features that weigh in each direction. According to the local surrogate, a <code class="inlineCode">cocoa_percent</code> value smaller or equal to 70 is a favorable attribute, as is the berry taste. A lack of sour, sweet, and molasses tastes also weighs favorably in this model. However, a lack of rich, creamy, and cocoa tastes does the opposite, but not enough to push the scales toward <em class="italic">Not Highly Recommended</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_10.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.10: LIME’s tabular explanation for observation #5 (Outstanding)</p>
    <p class="normal">With a small adjustment to the code that produced <em class="italic">Figure 5.10</em>, we can produce the same plot but for observation #24, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">lime_svm_explainer.explain_instance(
    X_test[X_test.index==<span class="hljs-number">24</span>].values[<span class="hljs-number">0</span>],
    fitted_svm_mdl.predict_proba,
    num_features=<span class="hljs-number">8</span>
).show_in_notebook(predict_proba=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Here, in <em class="italic">Figure 5.11</em>, we <a id="_idIndexMarker570"/>can clearly see why the local surrogate believes that observation #24 is <em class="italic">Not Highly Recommended</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_11.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.11: LIME’s tabular explanation for observation #24 (Disappointing)</p>
    <p class="normal">Once you compare the explanation of #24 (<em class="italic">Figure 5.11</em>) with that of #5 (<em class="italic">Figure 5.10</em>), the problems become evident. A single feature, <code class="inlineCode">tastes_berry</code>, is what differentiates both explanations. Of course, we have limited it to the top eight features, so there’s probably much more to it. However, you would expect the top eight features to include the ones that make the most difference.</p>
    <p class="normal">According to SHAP, knowing that <code class="inlineCode">tastes_earthy=1</code> is what globally explains the disappointing nature of the #24 chocolate bar, but this appears to be counterintuitive. So, what happened? It turns out that observations #5 and #24 are relatively similar and, thus, in the same neighborhood. This neighborhood also includes many chocolate bars with berry tastes and very few with earthy ones. However, there are not enough earthy ones to consider it a salient feature, so it attributes the difference between <em class="italic">Highly Recommended</em> and <em class="italic">Not Highly Recommended</em> to other features that seem to differentiate more often, at least locally. The reason for this is twofold: the local neighborhood could be too small, and linear models, given their simplicity, are on the bias end of a <em class="italic">bias-variance trade-off</em>. This bias is only exacerbated by the fact that some features such as <code class="inlineCode">tastes_berry</code> can <a id="_idIndexMarker571"/>appear relatively more often than <code class="inlineCode">tastes_earthy</code>. There’s an approach we can use to fix this, and we’ll cover this in the next section.</p>
    <h1 id="_idParaDest-148" class="heading-1">Using LIME for NLP</h1>
    <p class="normal">At the beginning of the chapter, we set<a id="_idIndexMarker572"/> aside training and test datasets with the cleaned-up contents of all the “tastes” columns for NLP. We<a id="_idIndexMarker573"/> can take a peek at the test dataset for NLP, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(X_test_nlp)
</code></pre>
    <p class="normal">This outputs the following:</p>
    <pre class="programlisting con"><code class="hljs-con">1194                 roasty nutty rich
77      roasty oddly sweet marshmallow
121              balanced cherry choco
411                sweet floral yogurt
1259           creamy burnt nuts woody
                     ...              
327          sweet mild molasses bland
1832          intense fruity mild sour
464              roasty sour milk note
2013           nutty fruit sour floral
1190           rich roasty nutty smoke
Length: 734, dtype: object
</code></pre>
    <p class="normal">No machine learning model can ingest the data as text, so we need to turn it into a numerical format—in other words, vectorize it. There are many techniques we can use to do this. In our case, we are not interested in the position of words in each phrase, nor the semantics. However, we are interested in their relative occurrence—after all, that was an issue for us in the last section.</p>
    <p class="normal">For these reasons, <strong class="keyWord">Term Frequency-Inverse Document Frequency</strong> (<strong class="keyWord">TF-IDF</strong>) is the ideal method because it’s meant to evaluate<a id="_idIndexMarker574"/> how often a term (each word) appears in a document (each phrase). However, it’s weighted according to its frequency in the entire corpus (all phrases). We can easily vectorize our datasets using the TF-IDF method with <code class="inlineCode">TfidfVectorizer</code> from scikit-learn. However, when you have to make TF-IDF scores, these are fitted to the training dataset only because that way, the transformed train and test datasets have consistent scoring for each term. Have a look at the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">vectorizer = TfidfVectorizer(lowercase=<span class="hljs-literal">False</span>)
X_train_nlp_fit = vectorizer.fit_transform(X_train_nlp)
X_test_nlp_fit = vectorizer.transform(X_test_nlp)
</code></pre>
    <p class="normal">To get an idea of what the TF-IDF score looks like, we can place all the feature names in one column of a DataFrame, and<a id="_idIndexMarker575"/> their respective scores for a single observation in another. Note that since<a id="_idIndexMarker576"/> the vectorizer produces a <code class="inlineCode">scipy</code> sparse matrix, we have to convert it into a <code class="inlineCode">numpy</code> matrix with <code class="inlineCode">todense()</code> and then a <code class="inlineCode">numpy</code> array with <code class="inlineCode">asarray()</code>. We can sort this DataFrame in descending order by TD-IDF scores. The code is shown in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">pd.DataFrame(
    {
        <span class="hljs-string">'taste'</span>:vectorizer.get_feature_names_out(),
        <span class="hljs-string">'tf-idf'</span>: np.asarray(
            X_test_nlp_fit[X_test_nlp.index==<span class="hljs-number">5</span>].todense())[<span class="hljs-number">0</span>]
    }
).sort_values(by=<span class="hljs-string">'tf-idf'</span>, ascending=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">The preceding code produces the output shown here in <em class="italic">Figure 5.12</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_12.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.12: The TF-IDF scores for words present in observation #5</p>
    <p class="normal">And as you can tell from <em class="italic">Figure 5.12</em>, the TD-IDF scores are normalized values between 0 and 1, and those most common in the corpus have a lower value. Interestingly enough, we realize that observation #5 in our tabular dataset had <code class="inlineCode">berry=1</code> because of <strong class="keyWord">raspberry</strong>. The categorical encoding<a id="_idIndexMarker577"/> method <a id="_idIndexMarker578"/>we used searched occurrences of <code class="inlineCode">berry</code> regardless of whether they matched an entire word or not. This isn’t a problem because raspberry is a kind of berry, and raspberry wasn’t one of our common tastes with its own binary column.</p>
    <p class="normal">Now that we have vectorized our NLP datasets, we can proceed with the modeling.</p>
    <h2 id="_idParaDest-149" class="heading-2">Training a LightGBM model</h2>
    <p class="normal"><strong class="keyWord">LightGBM</strong>, like <strong class="keyWord">XGBoost</strong>, is another<a id="_idIndexMarker579"/> very popular and performant gradient-boosting framework that leverages boosted-tree ensembles and histogram-based split finding. The main differences lie in the split method’s algorithms, which for LightGBM uses sampling <a id="_idIndexMarker580"/>with <strong class="keyWord">Gradient-Based One-Side Sampling</strong> (<strong class="keyWord">GOSS</strong>) and bundling sparse features with <strong class="keyWord">Exclusive Feature Bundling</strong> (<strong class="keyWord">EFB</strong>) versus XGBoost’s<a id="_idIndexMarker581"/> more rigorous <strong class="keyWord">Weighted Quantile Sketch</strong> and <strong class="keyWord">Sparsity-aware Split Finding</strong>. Another difference lies in how<a id="_idIndexMarker582"/> the trees <a id="_idIndexMarker583"/>are built, which is <strong class="keyWord">depth-first</strong> (top-down) for XGBoost and <strong class="keyWord">breadth-first</strong> (across a tree’s leaves) for LightGBM. We won’t get into the details of how these algorithms work because that would derail the topic at hand. However, it’s important to note that thanks to GOSS, LightGBM is usually even faster than XGBoost, and though it can lose predictive performance due to GOSS split approximations, it gains some of it back with its best-first approach. On the<a id="_idIndexMarker584"/> other hand, <strong class="keyWord">Explainable Boosting Machine</strong> (<strong class="keyWord">EBM</strong>) makes LightGBM ideal for training on sparse features efficiently and effectively, such as those in our <code class="inlineCode">X_train_nlp_fit</code> sparse matrix! That pretty much sums up why we are using LightGBM for this exercise.</p>
    <p class="normal">To train the LightGBM model, we first initialize the model by setting the maximum tree depth (<code class="inlineCode">max_depth</code>), the learning rate (<code class="inlineCode">learning_rate</code>), the number of boosted trees to fit (<code class="inlineCode">n_estimators</code>), the <code class="inlineCode">objective</code>, which is binary classification, and—last but not least—the <code class="inlineCode">random_state</code> for reproducibility. With <code class="inlineCode">fit</code>, we train the model using our vectorized NLP training dataset (<code class="inlineCode">X_train_nlp_fit</code>) and the same labels used for the SVM model (<code class="inlineCode">y_train</code>). Once trained, we can evaluate using the <code class="inlineCode">evaluate_class_mdl</code> we used with the SVM. The code is illustrated in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">lgb_mdl = lgb.LGBMClassifier(
    max_depth=<span class="hljs-number">13</span>,
    learning_rate=<span class="hljs-number">0.05</span>,
    n_estimators=<span class="hljs-number">100</span>,
    objective=<span class="hljs-string">'binary'</span>,
    random_state=rand
)
fitted_lgb_mdl = lgb_mdl.fit(X_train_nlp_fit, y_train)
y_train_lgb_pred, y_test_lgb_prob, y_test_lgb_pred =\
    mldatasets.evaluate_class_mdl(
        fitted_lgb_mdl, X_train_nlp_fit, X_test_nlp_fit, y_train, y_test
    )
</code></pre>
    <p class="normal">The preceding code <a id="_idIndexMarker585"/>produces <em class="italic">Figure 5.13</em>, shown here:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_13.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.13: The predictive performance of our LightGBM model</p>
    <p class="normal"><em class="italic">Figure 5.13</em> shows the performance achieved by LightGBM is slightly lower than for the SVM (<em class="italic">Figure 5.3</em>) but it’s still pretty <a id="_idIndexMarker586"/>good, safely above the coin-toss line. The comments for the SVM about favoring precision over recall for this model also apply here.</p>
    <h2 id="_idParaDest-150" class="heading-2">Local interpretation for a single prediction at a time using LimeTextExplainer</h2>
    <p class="normal">To interpret any black-box<a id="_idIndexMarker587"/> model prediction with LIME, we need to specify a classifier function such as <code class="inlineCode">predict_proba</code> for your model, and it will use this function to make predictions with perturbed data in the neighborhood of your instance and then train a linear model with it. The instance must be in its numerical form—in other words, vectorized. However, it would be easier if you could provide any arbitrary text, and it could then vectorize it on the fly. This is precisely what a pipeline can do for us. With the <code class="inlineCode">make_pipeline</code> function from scikit-learn, you can define a sequence of estimators that transform the data, followed by one that can fit it. In this case, we just need <code class="inlineCode">vectorizer</code> to transform our data, followed by our LightGBM model (<code class="inlineCode">lgb_mdl</code>) that takes the transformed data, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">lgb_pipeline = make_pipeline(vectorizer, lgb_mdl)
</code></pre>
    <p class="normal">Initializing a <code class="inlineCode">LimeTextExplainer</code> is pretty simple. All parameters are optional, but it’s recommended to specify names for your classes. Just as with <code class="inlineCode">LimeTabularExplainer</code>, a <code class="inlineCode">kernel_width</code> optional parameter can be critical because it defines the neighborhood’s size, and there’s a default that may not be optimal but can be tuned on an instance-by-instance basis. The code is illustrated here:</p>
    <pre class="programlisting code"><code class="hljs-code">lime_lgb_explainer = LimeTextExplainer(
    class_names=[<span class="hljs-string">'Not Highly Recomm.'</span>, <span class="hljs-string">'Highly Recomm.'</span>]
)
</code></pre>
    <p class="normal">Explaining an instance with <code class="inlineCode">LimeTextExplainer</code> is similar to doing it for <code class="inlineCode">LimeTabularExplainer</code>. The difference is that we are using a pipeline (<code class="inlineCode">lgb_pipeline</code>), and the data we are providing (first parameter) is text since the pipeline can transform it for us. The code is illustrated in the following snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">lime_lgb_explainer.explain_instance(
    X_test_nlp[X_test_nlp.index==<span class="hljs-number">5</span>].values[<span class="hljs-number">0</span>],
    lgb_pipeline.predict_proba, num_features=<span class="hljs-number">4</span>
).show_in_notebook(text=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">According to the LIME text explainer (see <em class="italic">Figure 5.14</em>), the model predicts <em class="italic">Highly Recommended</em> for observation #5 because of the word <strong class="keyWord">caramel</strong>. At least according to the local neighborhood, <strong class="keyWord">raspberry</strong> is not a factor. In this case, the local surrogate is making a different prediction from the LightGBM model. In some cases, the LIME prediction disagrees with the model prediction. If the disagreement rate is too high, we would refer to this as “low fidelity.”</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_14.png" alt="Chart, waterfall chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.14: LIME’s text explanation for observation #5 (Outstanding)</p>
    <p class="normal">Now, let’s contrast the <a id="_idIndexMarker588"/>interpretation for observation #5 with that of #24, as we’ve done before. We can use the same code but simply replace <code class="inlineCode">5</code> with <code class="inlineCode">24</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">lime_lgb_explainer.explain_instance(
    X_test_nlp[X_test_nlp.index==<span class="hljs-number">24</span>].values[<span class="hljs-number">0</span>],
    lgb_pipeline.predict_proba, num_features=<span class="hljs-number">4</span>
).show_in_notebook(text=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">According to <em class="italic">Figure 5.15</em>, you can tell that observation #24, described as tasting like <strong class="keyWord">burnt wood earthy choco</strong>, is <em class="italic">Not Highly Recommended</em> because of the words <strong class="keyWord">earthy</strong> and <strong class="keyWord">burnt</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_15.png" alt="Chart, waterfall chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.15: LIME’s tabular explanation for observation #24 (Disappointing)</p>
    <p class="normal">Given that we are using a pipeline that can vectorize any arbitrary text, let’s have some fun with that! We will first try a phrase made out of adjectives we suspect that our model favors, then try one with unfavorable adjectives, and lastly, try using words that our model shouldn’t be familiar with, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">lime_lgb_explainer.explain_instance(
    <span class="hljs-string">'</span><span class="hljs-string">creamy rich complex fruity'</span>,
    lgb_pipeline.predict_proba, num_features=<span class="hljs-number">4</span>
).show_in_notebook(text=<span class="hljs-literal">True</span>)
lime_lgb_explainer.explain_instance(
    <span class="hljs-string">'sour bitter roasty molasses'</span>,
    lgb_pipeline.predict_proba, num_features=<span class="hljs-number">4</span>
).show_in_notebook(text=<span class="hljs-literal">True</span>)
lime_lgb_explainer.explain_instance(
    <span class="hljs-string">'nasty disgusting gross stuff'</span>,
    lgb_pipeline.predict_proba, num_features=<span class="hljs-number">4</span>
).show_in_notebook(text=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">In <em class="italic">Figure 5.16</em>, the explanations are spot-on for <strong class="keyWord">creamy rich complex fruity</strong> and <strong class="keyWord">sour bitter roasty molasses</strong> since<a id="_idIndexMarker589"/> the model knows these words to be either very favorable or unfavorable. These words are also common enough to be appreciated on a local level.</p>
    <p class="normal">You can see the output here:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_16.png" alt="Graphical user interface  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.16: Arbitrary phrases not in the training or test dataset can be effortlessly explained with LIME, as long as words are in the corpus</p>
    <p class="normal">However, you’d be mistaken to think that the prediction of <em class="italic">Not Highly Recommended</em> for <strong class="keyWord">nasty disgusting gross stuff</strong> has anything to do with the words. The LightGBM model hasn’t seen these words before, so the prediction has more to do with <em class="italic">Not Highly Recommended</em> being the majority class, which <a id="_idIndexMarker590"/>is a good guess, and the sparse matrix for this phrase is all zeros. Therefore, LIME likely found few distant points—if any at all—in its neighborhood, so the zero coefficients of LIME’s local surrogate model reflect this.</p>
    <h1 id="_idParaDest-151" class="heading-1">Trying SHAP for NLP</h1>
    <p class="normal">Most of SHAP’s explainers<a id="_idIndexMarker591"/> will work with tabular data. <code class="inlineCode">DeepExplainer</code> can do text but is restricted to deep learning models, and, as we will cover in <em class="chapterRef">Chapter 7</em>, <em class="italic">Visualizing Convolutional Neural Networks</em>, three of them do images, including <code class="inlineCode">KernelExplainer</code>. In fact, SHAP’s <code class="inlineCode">KernelExplainer</code> was <a id="_idIndexMarker592"/>designed to be a general-purpose, truly model-agnostic method, but it’s not promoted as an option for NLP. It is easy to understand why: it’s slow, and NLP models tend to be very complex and with hundreds—if not thousands—of features to boot. In cases such as this one, where word order is not a factor and you have a few hundred features, but the top 100 are present in most of your observations, <code class="inlineCode">KernelExplainer</code> could work.</p>
    <p class="normal">In addition to overcoming the high computation cost, there are a couple of technical hurdles you would need to overcome. One of them is that <code class="inlineCode">KernelExplainer</code> is compatible with a pipeline, but it expects a single set of predictions back. But LightGBM returns two sets, one for each class: <em class="italic">Not Highly Recommended</em> and <em class="italic">Highly Recommended</em>. To overcome this problem, we can create a <code class="inlineCode">lambda</code> function (<code class="inlineCode">predict_fn</code>) that includes a <code class="inlineCode">predict_proba</code> function, which returns only those predictions for <em class="italic">Highly Recommended</em>. This is illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">predict_fn = <span class="hljs-keyword">lambda</span> X: lgb_mdl.predict_proba(X)[:,<span class="hljs-number">1</span>]
</code></pre>
    <p class="normal">The second technical hurdle is to do with SHAP’s incompatibility with SciPy’s sparse matrices, and for our explainer, we will need sample vectorized test data, which is in this format. To overcome this issue, we <a id="_idIndexMarker593"/>can convert our data in<a id="_idIndexMarker594"/> the SciPy sparse-matrix format in to a <code class="inlineCode">numpy</code> matrix and then in to a <code class="inlineCode">pandas</code> DataFrame (<code class="inlineCode">X_test_nlp_samp_df</code>). To overcome any slowness, we can use the same <code class="inlineCode">kmeans</code> trick we used last time. Other than the adjustments made to overcome obstacles, the following code is exactly the same as SHAP performed with the SVM model:</p>
    <pre class="programlisting code"><code class="hljs-code">X_test_nlp_samp_df = pd.DataFrame(
    shap.sample(X_test_nlp_fit, <span class="hljs-number">50</span>).todense()
)
shap_lgb_explainer = shap.KernelExplainer(
    predict_fn, shap.kmeans(X_train_nlp_fit.todense(), <span class="hljs-number">10</span>)
)
shap_lgb_values_test = shap_lgb_explainer.shap_values(
    X_test_nlp_samp_df, l1_reg=<span class="hljs-string">"num_features(20)"</span>
)
shap.summary_plot(
    shap_lgb_values_test,
    X_test_nlp_samp_df,
    plot_type=<span class="hljs-string">"dot"</span>,
    feature_names=vectorizer.get_feature_names_out()
)
</code></pre>
    <p class="normal">By using SHAP’s summary plot in <em class="italic">Figure 5.17</em>, you can tell that globally the words <strong class="keyWord">creamy</strong>, <strong class="keyWord">rich</strong>, <strong class="keyWord">cocoa</strong>, <strong class="keyWord">fruit</strong>, <strong class="keyWord">spicy</strong>, <strong class="keyWord">nutty</strong>, and <strong class="keyWord">berry</strong> have a positive impact on the model toward predicting <em class="italic">Highly Recommended</em>. On the other hand, <strong class="keyWord">sweet</strong>, <strong class="keyWord">sour</strong>, <strong class="keyWord">earthy</strong>, <strong class="keyWord">hammy</strong>, <strong class="keyWord">sandy</strong>, and <strong class="keyWord">fatty</strong> have the<a id="_idIndexMarker595"/> opposite effect. These results shouldn’t be entirely unexpected given what we learned with our prior SVM model, with the tabular data and local LIME interpretations. That<a id="_idIndexMarker596"/> being said, the SHAP values were derived from samples of a sparse matrix, and they could be missing details and perhaps even be partially incorrect, especially for underrepresented features. Therefore, we should take the conclusions with a grain of salt, especially toward the bottom half of the plot. To increase the interpretation fidelity, it’s best to increase sample size, but given the slowness of <code class="inlineCode">KernelExplainer</code>, there’s a trade-off to consider.</p>
    <p class="normal">You can view the output here:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_17.png" alt="A screenshot of a computer  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 5.17: The SHAP summary plot for the LightGBM NLP model</p>
    <p class="normal">Now that we have validated our SHAP values globally, we can use them for local interpretation with a force plot. Unlike LIME, we cannot use arbitrary data for this. With<a id="_idIndexMarker597"/> SHAP, we are limited to those data points we have previously<a id="_idIndexMarker598"/> generated SHAP values for. For instance, let’s take the 18th observation from our test dataset sample, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(shap.sample(X_test_nlp, <span class="hljs-number">50</span>).to_list()[<span class="hljs-number">18</span>])
</code></pre>
    <p class="normal">The preceding code outputs this phrase:</p>
    <pre class="programlisting con"><code class="hljs-con">woody earthy medicinal
</code></pre>
    <p class="normal">It’s important to note which words are represented in the 18th observation because the <code class="inlineCode">X_test_nlp_samp_df</code> DataFrame contains the vectorized representation. The 18th observation’s row in this DataFrame is what you use to generate the force plot, along with the SHAP values for this observation and the expected value for the class, as illustrated in the following code snippet:</p>
    <pre class="programlisting code"><code class="hljs-code">shap.force_plot(
    shap_lgb_explainer.expected_value,
    shap_lgb_values_test[<span class="hljs-number">18</span>,:],
    X_test_nlp_samp_df.iloc[<span class="hljs-number">18</span>,:], 
    feature_names=vectorizer.get_feature_names_out()
)
</code></pre>
    <p class="normal"><em class="italic">Figure 5.18</em> is the force plot for <strong class="keyWord">woody earthy medicinal</strong>. As you can tell, <strong class="keyWord">earthy</strong> and <strong class="keyWord">woody</strong> weigh heavily in a <a id="_idIndexMarker599"/>prediction against <em class="italic">Highly Recommended</em>. The word <strong class="keyWord">medicinal</strong> is not featured in the force plot and instead you get a lack of <strong class="keyWord">creamy</strong> and <strong class="keyWord">cocoa</strong> as negative factors. As you<a id="_idIndexMarker600"/> can imagine, <em class="italic">medicinal</em> is not a word used often to describe chocolate bars, so there was only one observation in the sampled dataset that included it. Therefore, its average marginal contribution across possible coalitions would be greatly diminished:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_18.png" alt="Graphical user interface, timeline  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.18: The SHAP force plot for the 18th observation of the sampled test dataset</p>
    <p class="normal">Let’s try another one, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(shap.sample(X_test_nlp, <span class="hljs-number">50</span>).to_list()[<span class="hljs-number">9</span>])
</code></pre>
    <p class="normal">The 9th observation is the following phrase:</p>
    <pre class="programlisting con"><code class="hljs-con">intense spicy floral
</code></pre>
    <p class="normal">Generating a <code class="inlineCode">force_plot</code> for this observation is the same as before, except you replace <code class="inlineCode">18</code> with <code class="inlineCode">9</code>. If you run this code, you produce the output shown here in <em class="italic">Figure 5.19</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_05_19.png" alt="Timeline  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 5.19: The SHAP force plot for the 9th observation of the sampled test dataset</p>
    <p class="normal">As you can<a id="_idIndexMarker601"/> appreciate in <em class="italic">Figure 5.19</em>, all the words in the phrase are featured in the force<a id="_idIndexMarker602"/> plot: <strong class="keyWord">floral</strong> and <strong class="keyWord">spicy</strong> pushing toward <em class="italic">Highly Recommended</em>, and <strong class="keyWord">intense</strong> toward <em class="italic">Not Highly Recommended</em>. So, now you know how to perform both tabular and NLP interpretations with SHAP, how does it compare with LIME?</p>
    <h1 id="_idParaDest-152" class="heading-1">Comparing SHAP with LIME</h1>
    <p class="normal">As you will have <a id="_idIndexMarker603"/>noticed by now, both SHAP and LIME have limitations, but they also have strengths. SHAP is grounded in <a id="_idIndexMarker604"/>game theory and approximate Shapley values, so its SHAP values are supported by theory. These have great properties such as additivity, efficiency, and substitutability that make them consistent but <a id="_idIndexMarker605"/>violate the dummy property. It always adds up and doesn’t need parameter tuning to accomplish this. However, it’s more suited for global interpretations, and one of its most model-agnostic explainers, <code class="inlineCode">KernelExplainer</code>, is painfully slow. <code class="inlineCode">KernelExplainer</code> also deals with missing values by using random ones, which can put too much weight on unlikely observations.</p>
    <p class="normal">LIME is speedy, very model-agnostic, and adaptable to all kinds of data. However, it’s not grounded on strict and consistent principles but has the intuition that neighbors are alike. Because of this, it can require tricky parameter tuning to define the neighborhood size optimally, and even then, it’s only suitable for local interpretations.</p>
    <h1 id="_idParaDest-153" class="heading-1">Mission accomplished</h1>
    <p class="normal">The mission was to understand why one of your client’s bars is <em class="italic">Outstanding</em> while another one is <em class="italic">Disappointing</em>. Your approach employed the interpretation of machine learning models to arrive at the following<a id="_idIndexMarker606"/> conclusions:</p>
    <ul>
      <li class="bulletList">According to SHAP on the tabular model, the <em class="italic">Outstanding</em> bar owes that rating to its berry taste and its cocoa percentage of 70%. On the other hand, the unfavorable rating for the <em class="italic">Disappointing</em> bar is due mostly to its earthy flavor and bean country of origin (<code class="inlineCode">Other</code>). Review date plays a smaller role, but it seems that chocolate bars reviewed in that period (2013–15) were at an advantage.</li>
      <li class="bulletList">LIME confirms that <code class="inlineCode">cocoa_percent&lt;=70</code> is a desirable property, and that, in addition to <strong class="keyWord">berry</strong>, <strong class="keyWord">creamy</strong>, <strong class="keyWord">cocoa</strong>, and <strong class="keyWord">rich</strong> are favorable tastes, while <strong class="keyWord">sweet</strong>, <strong class="keyWord">sour</strong>, and <strong class="keyWord">molasses</strong> are unfavorable.</li>
      <li class="bulletList">The commonality between both methods using the tabular model is that despite the many non-taste-related attributes, taste features are among the most salient. Therefore, it’s only fitting to interpret the words used to describe each chocolate bar via an NLP model.</li>
      <li class="bulletList">The <em class="italic">Outstanding</em> bar was represented by the phrase <strong class="keyWord">oily nut caramel raspberry</strong>, of which, according to <code class="inlineCode">LIMETextExplainer</code>, <strong class="keyWord">caramel</strong> is positive and <strong class="keyWord">oily</strong> is negative. The other two words are neutral. On the other hand, the <em class="italic">Disappointing</em> bar was represented by <strong class="keyWord">burnt wood earthy choco</strong>, of which <strong class="keyWord">burnt</strong> and <strong class="keyWord">earthy</strong> are unfavorable and the other two are favorable.</li>
      <li class="bulletList">The inconsistencies between the tastes in tabular and NLP interpretations are due to the presence of lesser-represented tastes, including <strong class="keyWord">raspberry</strong>, which is not as common as <strong class="keyWord">berry</strong>.</li>
      <li class="bulletList">According to SHAP’s global explanation of the NLP model, <strong class="keyWord">creamy</strong>, <strong class="keyWord">rich</strong>, <strong class="keyWord">cocoa</strong>, <strong class="keyWord">fruit</strong>, <strong class="keyWord">spicy</strong>, <strong class="keyWord">nutty</strong>, and <strong class="keyWord">berry</strong> have a positive impact on the model toward predicting <em class="italic">Highly Recommended</em>. On the other hand, <strong class="keyWord">sweet</strong>, <strong class="keyWord">sour</strong>, <strong class="keyWord">earthy</strong>, <strong class="keyWord">hammy</strong>, <strong class="keyWord">sandy</strong>, and <strong class="keyWord">fatty</strong> have the opposite effect.</li>
    </ul>
    <p class="normal">With these notions of which <a id="_idIndexMarker607"/>chocolate bar characteristics and tastes are considered less attractive by <em class="italic">Manhattan Chocolate Society</em> members, a client can apply changes to their chocolate bar formulas to appeal to a broader audience—that is, if the assumption is correct about that group being representative of their target audience.</p>
    <p class="normal">It could be argued that it is pretty apparent that words such as <strong class="keyWord">earthy</strong> and <strong class="keyWord">burnt</strong> are not favorable words to associate with chocolate bars, while <strong class="keyWord">caramel</strong> is. Therefore, we could have reached this conclusion without machine learning! But first of all, a conclusion not informed by data would have been an opinion, and, secondly, context is everything. Furthermore, humans can’t always be relied upon to place one point objectively in its context—especially considering it’s among thousands of records!</p>
    <p class="normal">Also, local model interpretation is <em class="italic">not only about the explanation for one prediction</em> because it’s connected to how a model makes all predictions but, more importantly, how it makes predictions for similar points—in other words, in the local neighborhood! In the next chapter, we will expand on what it means to be in the local neighborhood by looking at the commonalities (<em class="italic">anchors</em>) and inconsistencies (<em class="italic">counterfactuals</em>) we can find there.</p>
    <h1 id="_idParaDest-154" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we learned how to use SHAP’s <code class="inlineCode">KernelExplainer</code>, as well as its decision and force plot to conduct local interpretations. We carried out a similar analysis using LIME’s instance explainer for both tabular and text data. Lastly, we looked at the strengths and weaknesses of SHAP’s <code class="inlineCode">KernelExplainer</code> and LIME. In the next chapter, we will learn how to create even more human-interpretable explanations of a model’s decisions, such as <em class="italic">if X conditions are met, then Y is the outcome</em>.</p>
    <h1 id="_idParaDest-155" class="heading-1">Dataset sources</h1>
    <ul>
      <li class="bulletList">Brelinski, Brady (2020). <em class="italic">Manhattan Chocolate Society</em>: <a href="http://flavorsofcacao.com/mcs_index.html"><span class="url">http://flavorsofcacao.com/mcs_index.html</span></a></li>
    </ul>
    <h1 id="_idParaDest-156" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Platt, J. C., 1999, <em class="italic">Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods</em>. Advances in Large Margin Classifiers, MIT Press: <a href="https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf"><span class="url">https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf</span></a></li>
      <li class="bulletList">Lundberg, S. and Lee, S., 2017, <em class="italic">A Unified Approach to Interpreting Model Predictions</em>. <a href="https://arxiv.org/abs/1705.07874"><span class="url">https://arxiv.org/abs/1705.07874</span></a> (documentation for SHAP: <a href="https://github.com/slundberg/shap"><span class="url">https://github.com/slundberg/shap</span></a>)</li>
      <li class="bulletList">Ribeiro, M. T., Singh, S., and Guestrin, C., 2016, <em class="italic">“Why Should I Trust You?”: Explaining the Predictions of Any Classifier</em>. Proceedings of the 22<sup class="superscript">nd</sup> ACM SIGKDD International Conference on Knowledge Discovery and Data Mining: <a href="http://arxiv.org/abs/1602.04938"><span class="url">http://arxiv.org/abs/1602.04938</span></a></li>
      <li class="bulletList">Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu, T., 2017, <em class="italic">LightGBM: A Highly Efficient Gradient Boosting Decision Tree</em>. Advances in Neural Information Processing Systems vol. 30, pp. 3149–3157: <a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree"><span class="url">https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_5.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>