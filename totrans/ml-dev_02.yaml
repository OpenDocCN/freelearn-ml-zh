- en: The Learning Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first chapter, we saw a general overview of the mathematical concepts,
    history, and areas of the field of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: As this book intends to provide a practical but formally correct way of learning,
    now it's time to explore the general thought process for any machine learning
    process. These concepts will be pervasive throughout the chapters and will help
    us to define a common framework of the best practices of the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we will cover in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem and definitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset retrieval, preprocessing, and feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model definition, training, and evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding results and metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every machine learning problem tends to have its own particularities. Nevertheless,
    as the discipline advances through time, there are emerging patterns of what kind
    of steps a machine learning process should include, and the best practices for
    them. The following sections will be a list of these steps, including code examples
    for the cases that apply.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When solving machine learning problems, it's important to take time to analyze
    both the data and the possible amount of work beforehand. This preliminary step
    is flexible and less formal than all the subsequent ones on this list.
  prefs: []
  type: TYPE_NORMAL
- en: From the definition of machine learning, we know that our final goal is to make
    the computer learn or generalize a certain behavior or model from a sample set
    of data. So, the first thing we should do is understand the new capabilities we
    want to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the enterprise field, this is the time to have more practical discussions
    and brainstorms. The main questions we could ask ourselves during this phase could
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the real problem we are trying to solve?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the current information pipeline?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I streamline data acquisition?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the incoming data complete, or does it have gaps?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What additional data sources could we merge in order to have more variables
    to hand?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the data release periodical, or can it be acquired in real time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What should be the minimal representative unit of time for this particular problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the behavior I try to characterize change in nature, or are its fundamentals
    more or less stable through time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the problem involves getting on the business knowledge side and
    looking at all the valuable sources of information that could influence the model.
    Once identified, the following task will generate an organized and structured
    set of values, which will be the input to our model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's proceed to see an example of an initial problem definition, and the thought
    process of the initial analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say firm A is a retail chain that wants to be able to predict a certain
    product's demand on certain dates. This could be a challenging task because it
    involves human behavior, which has some non-deterministic components.
  prefs: []
  type: TYPE_NORMAL
- en: What kind of data input would be needed to build such a model? Of course, we
    would want the transaction listings for that kind of item. But what if the item
    is a commodity? If the item depends on the price of soybean or flour, the current
    and past harvest quantities could enrich the model. If the product is a medium-class
    item, current inflation and salary changes could also correlate with the current
    earnings.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem involves some business knowledge and looking to gather
    all the valuable sources of information that could influence the model. In some
    sense, it is more of an art form, and this doesn't change its importance a little
    bit.
  prefs: []
  type: TYPE_NORMAL
- en: Let's then assume that the basics of the problem have been analyzed, and the
    behavior and characteristics of the incoming data and desired output are clearer.
    The following task will generate an organized and structured set of values that
    will be the input to our model. This group of data, after a process of cleaning
    and adapting, will be called our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset definition and retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have identified the data sources, the next task is to gather all the
    tuples or records as a homogeneous set. The format can be a tabular arrangement,
    a series of real values (such as audio or weather variables), and N-dimensional
    matrices (a set of images or cloud points), among other types.
  prefs: []
  type: TYPE_NORMAL
- en: The ETL process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous stages in the big data processing field evolved over several decades
    under the name of data mining, and then adopted the popular name of **big data**.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best outcomes of these disciplines is the specification of the **Extraction**,
    **Transform**, **Load** (**ETL**) process.
  prefs: []
  type: TYPE_NORMAL
- en: This process starts with a mix of many data sources from business systems, then
    moves to a system that transforms the data into a readable state, and then finishes
    by generating a data mart with very structured and documented data types.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of applying this concept, we will mix the elements of this process
    with the final outcome of a structured dataset, which includes in its final form
    an additional label column (in the case of supervised learning problems).
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69cc866b-1119-49a8-b79c-930ee2bc7247.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the ETL process, from raw data to a useful dataset
  prefs: []
  type: TYPE_NORMAL
- en: The diagram illustrates the first stages of the data pipeline, starting with
    all the organization's data, whether it is commercial transactions, IoT device
    raw values, or other valuable data sources' information elements, which are commonly
    in very different types and compositions. The ETL process is in charge of gathering
    the raw information from them using different software filters, applying the necessary
    transforms to arrange the data in a useful manner, and finally, presenting the
    data in tabular format (we can think of this as a single database table with a
    last feature or result column, or a big CSV file with consolidated data). The
    final result can be conveniently used by the following processes without practically
    thinking of the many quirks of data formatting, because they have been standardized
    into a very clear table structure.
  prefs: []
  type: TYPE_NORMAL
- en: Loading datasets and doing exploratory analysis with SciPy and pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to get a practical overview of some types of dataset formats, we will
    use the previously presented Python libraries (SciPy and pandas) for this example, given
    their almost universal use.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by importing and performing a simple statistical analysis of several
    dataset input formats.
  prefs: []
  type: TYPE_NORMAL
- en: The sample data files will be in the data directory inside each chapter's code
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Working interactively with IPython
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce **Python interactive console**, or **IPython**,
    a command-line shell that allows us to explore concepts and methods in an interactive
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run IPython, you call it from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68caabb5-7103-4b92-a604-9deb5c1b1dc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we see IPython executing, and then the initial quick help. The most interesting
    part is the last line - it will allow you to import libraries and execute commands and
    will show the resulting objects. An additional and convenient feature of IPython
    is that you can redefine variables on the fly to see how the results differ with
    different inputs.
  prefs: []
  type: TYPE_NORMAL
- en: In the current examples, we are using the standard Python version for the most
    supported Linux distribution at the time of writing (Ubuntu 16.04). The examples
    should be equivalent for Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let''s import pandas and load a sample `.csv` file (a very common
    format with one row per line, and registers). It contains a very famous dataset
    for classification problems with the dimensions of the attributes of 150 instances
    of iris plants, with a numerical column indicating the class (1, 2, or 3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In this line, we import pandas in the usual way, making its method available
    for use with the `import` statement. The `as` modifier allows us to use a succinct
    name for all objects and methods in the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this line, we use the `read_csv` method, allowing pandas to guess the possible
    item separator for the `.csv` file, and storing it in a `dataframe` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform some simple exploration of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We are now able to see the column names of the dataset and explore the first
    *n* instances of it. Looking at the first registers, you can see the varying measures
    for the `setosa` iris class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s access a particular subset of columns and display the first three
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Pandas includes many related methods for importing tabulated data formats, such
    as HDF5 (`read_hdf`), JSON (`read_json`), and Excel (`read_excel`). For a complete
    list of formats, visit [http://pandas.pydata.org/pandas-docs/stable/io.html](http://pandas.pydata.org/pandas-docs/stable/io.html)[.](http://pandas.pydata.org/pandas-docs/stable/io.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to these simple exploration methods, we will now use pandas to
    get all the descriptive statistics concepts we''ve seen in order to characterize
    the distribution of the `Sepal.Length` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And here are the main metrics of this distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will graphically evaluate the accuracy of these metrics by looking at
    the histogram of this distribution, this time using the built-in `plot.hist` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8a004d2f-7e5c-456d-bd00-e237fbd9a10d.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram of the Iris Sepal Length
  prefs: []
  type: TYPE_NORMAL
- en: As the metrics show, the distribution is right skewed, because the skewness
    is positive, and it is of the plainly distributed type (has a spread much greater
    than 1), as the kurtosis metrics indicate.
  prefs: []
  type: TYPE_NORMAL
- en: Working on 2D data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's stop here for tabular data, and go for 2D data structures. As images are
    the most commonly used type of data in popular machine learning problems, we will
    show you some useful methods included in the SciPy stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is optimized to run on the Jupyter notebook with inline
    graphics. You will find the source code in the source file, `Dataset_IO.pynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Importing a single image basically consists of importing the corresponding
    modules, using the `imread` method to read the indicated image into a matrix,
    and showing it using matplotlib. The `%` starting line corresponds to a parameter
    modification and indicates that the following `matplotlib` graphics should be
    shown inline on the notebook, with the following results (the axes correspond
    to pixel numbers):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/860f0d8c-37b9-4c19-9172-aa151c1fe3b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial RGB image loaded
  prefs: []
  type: TYPE_NORMAL
- en: 'The testing variable will contain a height * width * channel number array,
    with all the red, green, and blue values for each image pixel. Let''s get this
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The interpreter will display the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also try to separate the channels and represent them separately, with
    red, green, and blue scales, to get an idea of the color patterns in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the previous example, we create three subplots indicating the structure and
    position with a three-digit code. The first indicates the row number, the second
    indicates the column number, and the last, the plot position on that structure.
    The `cmap` parameter indicates the colormap assigned to each graphic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0630ea6-bf26-47b6-b7a0-cad85fc6193e.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the separated channels of the sample image.
  prefs: []
  type: TYPE_NORMAL
- en: Note that red and green channels share a similar pattern, while the blue tones
    are predominant in this bird figure. This channel separation could be an extremely
    rudimentary preliminary way to detect this kind of bird in its habitat.
  prefs: []
  type: TYPE_NORMAL
- en: This section is a simplified introduction to the different methods of loading
    datasets. In the following chapters, we will see different advanced ways to get
    the datasets, including  loading and training the different batches of sample
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering is in some ways one of the most underrated parts of the
    machine learning process, even though it is considered the cornerstone of the
    learning process by many prominent figures of the community.
  prefs: []
  type: TYPE_NORMAL
- en: What's the purpose of this process? In short, it takes the raw data from databases,
    sensors, archives, and so on, and transforms it in a way that makes it easy for
    the model to generalize. This discipline takes criteria from many sources, including
    common sense. It's indeed more like an art than a rigid science. It is a manual
    process, even when some parts of it can be automatized via a group of techniques
    grouped in the feature extraction field.
  prefs: []
  type: TYPE_NORMAL
- en: As part of this process we also have many powerful mathematical tools and dimensionality
    reduction techniques, such as **Principal Component Analysis** (**PCA**) and **Autoencoders**,
    that allow data scientists to skip features that don't enrich the representation
    of the data in useful ways.
  prefs: []
  type: TYPE_NORMAL
- en: Imputation of missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with not-so-perfect or incomplete datasets, a missing register
    may not add value to the model in itself, but all the other elements of the row
    could be useful to the model. This is especially true when the model has a high
    percentage of incomplete values, so no row can be discarded.
  prefs: []
  type: TYPE_NORMAL
- en: The main question in this process is *"how do you interpret a missing value?"*
    There are many ways, and they usually depend on the problem itself.
  prefs: []
  type: TYPE_NORMAL
- en: A very naive approach could be set the value to zero, supposing that the mean
    of the data distribution is 0\. An improved step could be to relate the missing
    data with the surrounding content, assigning the average of the whole column,
    or an interval of *n* elements of the same columns. Another option is to use the
    column's median or most frequent value.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there are more advanced techniques, such as robust methods and
    even k-nearest neighbors, that we won't cover in this book.
  prefs: []
  type: TYPE_NORMAL
- en: One hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numerical or categorical information can easily be normally represented by integers,
    one for each option or discrete result. But there are situations where `bins`
    indicating the current option are preferred. This form of data representation
    is called **one hot encoding**. This encoding simply transforms a certain input
    into a binary array containing only zeros, except for the value indicated by the
    value of a variable, which will be one.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the simple case of an integer, this will be the representation of the list
    [1, 3, 2, 4] in one hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s perform a simple implementation of a one hot integer encoder for integer
    arrays, in order to better understand the concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we first define the `get_one_hot` function, which takes an
    array as input and returns an array.
  prefs: []
  type: TYPE_NORMAL
- en: What we do is take the elements of the arrays one by one, and for each element
    in it, we generate a zero array with length equal to the maximum value of the
    array, in order to have space for all possible values. Then we insert `1` on the
    index position indicated by the current value (we subtract `1` because we go from
    1-based indexes to 0-based indexes).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try the function we just wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Dataset preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we first dive into data science, a common mistake is expecting all the
    data to be very polished and with good characteristics from the very beginning.
    Alas, that is not the case for a very considerable percentage of cases, for many
    reasons such as null data, sensor errors that cause outliers and NAN, faulty registers,
    instrument-induced bias, and all kinds of defects that lead to poor model fitting
    and that must be eradicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two key processes in this stage are data normalization and feature scaling. This
    process consists of applying simple transformations called **affine** that map
    the current unbalanced data into a more manageable shape, maintaining its integrity
    but providing better stochastic properties and improving the future applied model.
    The common goal of the standardization techniques is to bring the data distribution
    closer to a normal distribution, with the following techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Normalization and feature scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One very important step in dataset preprocessing is normalization and feature
    scaling. Data normalization allows our optimization techniques, specially the
    iterative ones, to converge better, and makes the data more manageable.
  prefs: []
  type: TYPE_NORMAL
- en: Normalization or standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique aims to give the dataset the properties of a normal distribution,
    that is, a mean of 0 and a standard deviation of 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way to obtain these properties is by calculating the so-called *z* scores,
    based on the dataset samples, with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72fb7b7b-e99f-4fdc-98bb-f6c65f00ab3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s visualize and practice this new concept with the help of scikit-learn,
    reading a file from the `MPG` dataset, which contains city-cycle fuel consumption
    in miles per gallon, based on the following features: `mpg`, `cylinders`, `displacement`, 
    `horsepower`, `weight`, `acceleration`, `model year`, `origin`, and `car name`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following picture allows us to compare the non normalized and normalized
    data representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb067f14-b133-4671-8471-8c49d0dd910c.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the original dataset, and its normalized counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: It's very important to have an account of the denormalizing of the resulting
    data at the time of evaluation so that you do not lose the representative of the
    data, especially if the model is applied to regression, when the regressed data
    won't be useful if not scaled.
  prefs: []
  type: TYPE_NORMAL
- en: Model definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we wanted to summarize the machine learning process using just one word,
    it would certainly be models. This is because what we build with machine learning
    are abstractions or models representing and simplifying reality, allowing us to
    solve real-life problems based on a model that we have trained on.
  prefs: []
  type: TYPE_NORMAL
- en: The task of choosing which model to use is becoming increasingly difficult,
    given the increasing number of models appearing almost every day, but you can
    make general approximations by grouping methods by the type of task you want to
    perform and also the type of input data, so that the problem is simplified to
    a smaller set of options.
  prefs: []
  type: TYPE_NORMAL
- en: Asking ourselves the right questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the risk of generalizing too much, let''s try to summarize a sample decision
    problem for a model:'
  prefs: []
  type: TYPE_NORMAL
- en: Are we trying to characterize data by simply grouping information based on its
    characteristics, without any or a few previous hints? This is the domain of clustering
    techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first and most basic question: are we trying to predict the instant outcome
    of a variable, or to tag or classify data into groups? If the former, we are tackling
    a regression problem. If the latter, this is the realm of classification problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Having the former questions resolved, and opting for any of the options of
    point 2, we should ask ourselves: is the data sequential, or rather, should we
    take the sequence in account? Recurrent neural networks should be one of the first
    options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Continuing with non-clustering techniques: is the data or pattern to discover
    spatially located? Convolutional neural networks are a common starting point for
    this kind of problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the most common cases (data without a particular arrangement), if the function
    can be represented by a single univariate or multivariate function, we can apply
    linear, polynomial, or logistic regression, and if we want to upgrade the model,
    a multilayer neural network will provide support for more complex non-linear solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many dimensions and variables are we working on? Do we just want to extract
    the most useful features (and thus data dimensions), excluding the less interesting
    ones? This is the realm of dimensionality reduction techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we want to learn a set of strategies with a finite set of steps aiming to
    reach a goal? This belongs to the field of reinforcement learning. If none of
    these classical methods are fit for our research, a very high number of niche
    techniques appear and should be subject to additional analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following chapters, you will find additional information about how to
    base your decision on stronger criteria, and finally apply a model to your data.
    Also, if you see your answers don't relate well with the simple criteria explained
    in this section, you can check *Chapter 8*, *Recent Models and Developments*,
    for more advanced models.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This machine learning process step is also very important because it provides
    a distinctive measure of the quality of your model, and if wrongly chosen, it
    could either ruin the accuracy of the model or its efficiency in the speed of
    convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Expressed in a simple way, the loss function is a function that measures the
    distance from the model's estimated value to the real expected value.
  prefs: []
  type: TYPE_NORMAL
- en: An important fact that we have to take into account is that the objective of
    almost all of the models is to minimize the error function, and for this, we need
    it to be differentiable, and the derivative of the error function should be as
    simple as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Another fact is that when the model gets increasingly complex, the derivative
    of the error will also get more complex, so we will need to approximate solutions
    for the derivatives with iterative methods, one of them being the well-known gradient
    descent.
  prefs: []
  type: TYPE_NORMAL
- en: Model fitting and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part of the machine learning process, we have the model and data ready,
    and we proceed to train and validate our model.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset partitioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of training the models, we usually partition all the provided data
    into three sets: the training set, which will actually be used to adjust the parameters
    of the models; the validation set, which will be used to compare alternative models
    applied to that data (it can be ignored if we have just one model and architecture
    in mind); and the test set, which will be used to measure the accuracy of the
    chosen model. The proportions of these partitions are normally 70/20/10.'
  prefs: []
  type: TYPE_NORMAL
- en: Common training terms –  iteration, batch, and epoch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When training the model, there are some common terms that indicate the different
    parts of the iterative optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: An **iteration **defines one instance of calculating the error gradient and
    adjusting the model parameters. When the data is fed into groups of samples, each
    one of these groups is called a **batch**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batches can include the whole dataset (traditional batching), or include just
    a tiny subset until the whole dataset is fed forward, called mini-batching. The
    number of samples per batch is called the **batch size**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each pass of the whole dataset is called an **epoch**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of training – online and batch processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The training process provides many ways of iterating over the datasets and adjusting
    the parameters of the models according to the input data and error minimization
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the dataset can and will be evaluated many times and in a variety
    of ways during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to assure a good fitting start, the model weights have to be initialized
    to the most effective values. Neural networks, which normally have a *tanh* activation
    function, are mainly sensitive to the range [-1,1], or [0,1]; for this reason,
    it's important to have the data normalized, and the parameters should also be
    within that range.
  prefs: []
  type: TYPE_NORMAL
- en: The model parameters should have useful initial values for the model to converge.
    One important decision at the start of training is the initialization values for
    the model parameters (commonly called **weights**). A canonical initial rule is
    not initializing variables at 0 because it prevents the models from optimizing,
    as they do not have a suitable function slope multiplier to adjust. A common sensible
    standard is to use a normal random distribution for all the values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using NumPy, you would normally initialize a coefficient vector with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: One particular source of problems at this stage is setting all of the model's
    parameters to zero. As many optimization techniques normally multiply the weights
    by a determinate coefficient to the approximate minimum, multiplying by zero will
    prevent any change in the model, excepting the bias terms.
  prefs: []
  type: TYPE_NORMAL
- en: Model implementation and results interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No model is practical if it can't be used outside the training and test sets.
    This is when the model is deployed into production.
  prefs: []
  type: TYPE_NORMAL
- en: In this stage, we normally load all the model's operation and trained weights,
    wait for new unknown data, and when it arrives, we feed it through all the chained
    functions of the model, informing the outcomes of the output layer or operation
    via a web service, printing to standard output, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will have a final task - to interpret the results of the model in the
    real world to constantly check whether it works in the current conditions. In
    the case of generative models, the suitability of the predictions is easier to
    understand because the goal is normally the representation of a previously known
    entity.
  prefs: []
  type: TYPE_NORMAL
- en: Regression metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For regression metrics, a number of indicators are calculated to give a succinct
    idea of the fitness of the regressed model. Here is a list of the main metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `mean_absolute_error` function computes mean absolute error, a risk metric
    corresponding to the expected value of the absolute error loss, or *l1-norm* loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *ŷ[i]* is the predicted value of the *i*th sample, and *y[i]* is the corresponding
    true value, then the **mean absolute error** (**MAE**) estimated over *n* samples
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/addea822-7795-4645-8773-df4c2196e5b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Median absolute error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The median absolute error is particularly interesting because it is robust to
    outliers. The loss is calculated by taking the median of all absolute differences
    between the target and the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *ŷ* is the predicted value of the *i*th sample and *y[i]* is the corresponding
    true value, then the median absolute error estimated over *n* samples is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01cce5da-4454-43a3-88e3-30ae07e8d931.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean squared error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **mean squared error** (**MSE**) is a risk metric equal to the expected
    value of the squared (quadratic) error loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *ŷ[i]* is the predicted value of the *i*th sample and *y[i]* is the corresponding
    true value, then the MSE estimated over *n* samples is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19b41214-110d-4bd7-87a0-6ca71a855c5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The task of classification implies different rules for the estimation of the
    error. The advantage we have is that the number of outputs is discrete, so it
    can be determined exactly whether a prediction has failed or not in a binary way.
    That leads us to the main indicators.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The accuracy calculates either the fraction or the count of correct predictions
    for a model.
  prefs: []
  type: TYPE_NORMAL
- en: In multi-label classification, the function returns the subset's accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: If the entire set of predicted labels for a sample strictly matches the true
    set of labels, then the subset's accuracy is *1.0*; otherwise, it is *0.0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *ŷ[i]* is the predicted value of the *i*th sample and *y[i]* is the corresponding
    true value, then the fraction of correct predictions over *n* samples is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d11bb0e9-44c1-4ed6-aa2e-2908f6076d06.png)'
  prefs: []
  type: TYPE_IMG
- en: Precision score, recall, and F-measure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Precision** score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d21665f9-726e-4904-bd22-7ff7a183d4ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *t[p]* is the number of true positives and *f[p]* is the number of false
    positives. The precision is the ability of the classifier to not label as positive
    a sample that is negative. The best value is 1 and the worst value is *0*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/320e3e5d-3413-4616-a921-375e4ca896cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *t[p]* is the number of true positives and *f[n]* is the number of false
    negatives. The recall can be described as the ability of the classifier to find
    all the positive samples. Its values range from 1 (optimum) to zero.
  prefs: []
  type: TYPE_NORMAL
- en: '**F measure** (*F[β]* and F[1] measures) can be interpreted as a special kind
    of mean (weighted harmonic mean) of the precision and recall. A  *F[β]* measure''s
    best value is 1 and its worst score is 0. With *β = 1*, *F[β]* and *F[1]* are
    equivalent, and the recall and the precision are equally important:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca5234a8-9e44-4d0e-a76a-230ae5b8c02b.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every classification task aims to predict a label or tag for new unknown data.
    A very efficient way of showing the classification's accuracy is through a confusion
    matrix, where we show [classified sample, ground truth] pairs and a detailed view
    of how the predictions are doing.
  prefs: []
  type: TYPE_NORMAL
- en: The expected output should be the main diagonal of the matrix with a 1.0 score;
    that is, all the expected values should match the real ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code example, we will do a synthetic sample of predictions
    and real values, and generate a confusion matrix of the final data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And the final confusion matrix graphic representation for these values will
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af7d43a7-d90f-4114-b495-f48a31597747.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: In the image, we see the high accuracy value in the (*5,5*) diagonal value with
    three correct predictions, and the (*8,8*) value with two. As we can see, the
    distribution of the accuracy by value can be intuitively extracted just by analyzing
    the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering quality measurements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning techniques, understood as the labeling of data without
    a ground truth, makes it a bit difficult to implement significant metrics for
    the models. Nevertheless, there are a number of measures implemented for this
    kind of technique. In this section, we have a list of the most well-known ones.
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette coefficient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **silhouette** **coefficient** is a metric that doesn't need to know the
    labeling of the dataset. It gives an idea of the separation between clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is composed of two different elements:'
  prefs: []
  type: TYPE_NORMAL
- en: The mean distance between a sample and all other points in the same class (*a*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean distance between a sample and all other points in the nearest cluster
    (*b*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The formula for this coefficient *s* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fed0e65c-0325-4353-bcdb-012e59c3a141.png)'
  prefs: []
  type: TYPE_IMG
- en: The silhouette coefficient is only defined if the number of classes is at least
    two, and the coefficient for a whole sample set is the mean of the coefficient
    for all samples.
  prefs: []
  type: TYPE_NORMAL
- en: Homogeneity, completeness, and V-measure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Homogeneity, completeness, and V-measure are three key related indicators of
    the quality of a clustering operation. In the following formulas, we will use
    *K* for the number of clusters, *C* for the number of classes, *N* for the total
    number of samples, and *a[ck]* for the number of elements of class *c* in cluster
    *k*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Homogeneity** is a measure of the ratio of samples of a single class pertaining
    to a single cluster. The fewer different classes included in one cluster, the
    better. The lower bound should be 0.0 and the upper bound should be 1.0 (higher
    is better), and the formulation for it is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/631fb8c1-f23f-4fdd-90c6-9121e272c095.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Completeness **measures the ratio of the member of a given class that is
    assigned to the same cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7398ef4a-b2d6-4845-a62f-bc7f07b530d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**V-measure** is the harmonic mean of homogeneity and completeness, expressed
    by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7598e979-d41d-4e76-ae8a-61ed1fc8c857.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed all the main steps involved in a machine learning
    process. We will be, indirectly, using them throughout the book, and we hope they
    help you structure your future work too.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will review the programming languages and frameworks
    that we will be using to solve all our machine learning problems and become proficient
    with them before starting with the projects.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lichman, M. (2013). UCI Machine Learning Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).
    Irvine, CA: University of California, School of Information and Computer Science.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In *Proceedings
    on the Tenth International Conference of Machine Learning*, 236-243, University
    of Massachusetts, Amherst. Morgan Kaufmann.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Townsend, James T. *Theoretical analysis of an alphabetic confusion matrix.*
    Attention, Perception, & Psychophysics 9.1 (1971): 40-50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peter J. Rousseeuw (1987). *Silhouettes: a Graphical Aid to the Interpretation
    and Validation of Cluster Analysis*. Computational and Applied Mathematics 20:
    53-65.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kent, Allen, et al, Machine literature searching VIII. *Operational criteria
    for designing information retrieval systems.* Journal of the Association for Information
    Science and Technology 6.2 (1955): 93-101.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosenberg, Andrew, and Julia Hirschberg, V-Measure: *A Conditional Entropy-Based
    External Cluster Evaluation Measure*. EMNLP-CoNLL. Vol. 7\. 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
