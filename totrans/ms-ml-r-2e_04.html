<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Advanced Feature Selection in Linear Models</h1>
            </header>

            <article>
                
<div class="packt_quote">"I found that math got to be too abstract for my liking and computer science seemed concerned with little details--trying to save a microsecond or a kilobyte in a computation. In statistics I found a subject that combined the beauty of both math and computer science, using them to solve real-world problems."</div>
<p>This was quoted by <em>Rob Tibshirani</em>, <em>Professor</em>, <em>Stanford University</em> at:</p>
<p><a href="http://statweb.stanford.edu/~tibs/research_page.html">https://statweb.stanford.edu/~tibs/research_page.html</a>.</p>
<p>So far, we've examined the usage of linear models for both quantitative and qualitative outcomes with an emphasis on the techniques of feature selection, that is, the methods and techniques to exclude useless or unwanted predictor variables. We saw that the linear models can be quite effective in machine learning problems. However, newer techniques that have been developed and refined in the last couple of decades or so can improve predictive ability and interpretability above and beyond the linear models that we discussed in the preceding chapters. In this day and age, many datasets have numerous features in relation to the number of observations or, as it is called, high-dimensionality. If you've ever worked on a genomics problem, this will quickly become self-evident. Additionally, with the size of the data that we are being asked to work with, a technique like best subsets or stepwise feature selection can take inordinate amounts of time to converge even on high-speed computers. I'm not talking about minutes: in many cases, hours of system time are required to get a best subsets solution.</p>
<p>There is a better way in these cases. In this chapter, we will look at the concept of regularization where the coefficients are constrained or shrunk towards zero. There are a number of methods and permutations to these methods of regularization but we will focus on Ridge regression, <strong>Least Absolute Shrinkage and Selection Operator</strong> (<strong>LASSO</strong>), and finally, elastic net, which combines the benefit of both techniques into one.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Regularization in a nutshell</h1>
            </header>

            <article>
                
<p>You may recall that our linear model follows the form, <em>Y = B0 + B<sub>1</sub>x<sub>1</sub> +...B<sub>n</sub>x<sub>n</sub> + e</em>, and also that the best fit tries to minimize the RSS, which is the sum of the squared errors of the actual minus the estimate, or <em>e<sub>1</sub><sup>2</sup> + e<sub>2</sub><sup>2</sup> + ... e<sub>n</sub><sup>2</sup></em>.</p>
<p>With regularization, we will apply what is known as <strong>shrinkage penalty</strong> in conjunction with the minimization RSS. This penalty consists of a lambda (symbol <em>λ</em>), along with the normalization of the beta coefficients and weights. How these weights are normalized differs in the techniques, and we will discuss them accordingly. Quite simply, in our model, we are minimizing <em>(RSS + λ(normalized coefficients))</em>. We will select <em>λ</em>, which is known as the tuning parameter, in our model building process. Please note that if lambda is equal to 0, then our model is equivalent to OLS, as it cancels out the normalization term.</p>
<p>So what does this do for us and why does it work? First of all, regularization methods are very computationally efficient. In best subsets, we are searching <strong>2<sup>p</sup> models</strong>, and in large datasets, it may not be feasible to attempt. In R, we are only fitting one model to each value of lambda and this is far more efficient. Another reason goes back to our bias-variance trade-off, which was discussed in the preface. In the linear model, where the relationship between the response and the predictors is close to linear, the least squares estimates will have low bias but may have high variance. This means that a small change in the training data can cause a large change in the least squares coefficient estimates (James, 2013). Regularization through the proper selection of lambda and normalization may help you improve the model fit by optimizing the bias-variance trade-off. Finally, regularization of coefficients works to solve multi collinearity problems.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Ridge regression</h1>
            </header>

            <article>
                
<p>Let's begin by exploring what ridge regression is and what it can and cannot do for you. With ridge regression, the normalization term is the sum of the squared weights, referred to as an <strong>L2-norm</strong>. Our model is trying to minimize <em>RSS + λ(sum Bj<sup>2</sup>)</em>. As lambda increases, the coefficients shrink toward zero but never become zero. The benefit may be an improved predictive accuracy, but as it does not zero out the weights for any of your features, it could lead to issues in the model's interpretation and communication. To help with this problem, we will turn to LASSO.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">LASSO</h1>
            </header>

            <article>
                
<p>LASSO applies the <strong>L1-norm</strong> instead of the L2-norm as in ridge regression, which is the sum of the absolute value of the feature weights and thus minimizes <em>RSS + λ(sum |Bj|)</em>. This shrinkage penalty will indeed force a feature weight to zero. This is a clear advantage over ridge regression, as it may greatly improve the model interpretability.</p>
<p>The mathematics behind the reason that the L1-norm allows the weights/coefficients to become zero, is out of the scope of this book (refer to <em>Tibsharini</em>, <em>1996</em> for further details).</p>
<p>If LASSO is so great, then ridge regression must be clearly obsolete. Not so fast! In a situation of high collinearity or high pairwise correlations, LASSO may force a predictive feature to zero and thus you can lose the predictive ability; that is, say if both feature A and B should be in your model, LASSO may shrink one of their coefficients to zero. The following quote sums up this issue nicely:</p>
<div class="packt_quote">"One might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size."<br/>
                                                                                                                     -(James, 2013)</div>
<p>There is the possibility of achieving the best of both the worlds and that leads us to the next topic, elastic net.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Elastic net</h1>
            </header>

            <article>
                
<p>The power of elastic net is that, it performs the feature extraction that ridge regression does not and it will group the features that LASSO fails to do. Again, LASSO will tend to select one feature from a group of correlated ones and ignore the rest. Elastic net does this by including a mixing parameter, alpha, in conjunction with lambda. Alpha will be between <kbd>0</kbd> and <kbd>1</kbd> and as before, lambda will regulate the size of the penalty. Please note that an alpha of zero is equal to ridge regression and an alpha of one is equivalent to LASSO. Essentially, we are blending the L1 and L2 penalties by including a second tuning parameter with a quadratic (squared) term of the beta coefficients. We will end up with the goal of minimizing <em>(RSS + λ[(1-alpha) (sum|Bj|<sup>2</sup>)/2 + alpha (sum |Bj|)])/N)</em>.</p>
<p>Let's put these techniques to test. We will primarily utilize the <kbd>leaps</kbd>, <kbd>glmnet</kbd>, and <kbd>caret</kbd> packages to select the appropriate features and thus the appropriate model in our business case.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business case</h1>
            </header>

            <article>
                
<p>For this chapter, we will stick to cancer--prostate cancer in this case. It is a small dataset of 97 observations and nine variables but allows you to fully grasp what is going on with regularization techniques by allowing a comparison with traditional techniques. We will start by performing best subsets regression to identify the features and use this as a baseline for our comparison.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p><em>The Stanford University Medical Center</em> has provided preoperative <strong>Prostate Specific Antigen</strong> (<strong>PSA</strong>) data on 97 patients who are about to undergo radical prostatectomy (complete prostate removal) for the treatment of prostate cancer. The <strong>American Cancer Society</strong> (<strong>ACS</strong>) estimates that nearly 30,000 American men died of prostate cancer in 2014 (<a href="http://www.cancer.org/"><span class="URLPACKT">http://www.cancer.org/</span></a>). PSA is a protein that is produced by the prostate gland and is found in the bloodstream. The goal is to develop a predictive model of PSA among the provided set of clinical measures. PSA can be an effective prognostic indicator, among others, of how well a patient can and should do after surgery. The patient's PSA levels are measured at various intervals after the surgery and used in various formulas to determine if a patient is cancer-free. A preoperative predictive model in conjunction with the postoperative data (not provided here) can possibly improve cancer care for thousands of men each year.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>The data set for the 97 men is in a data frame with 10 variables, as follows:</p>
<ul>
<li><kbd>lcavol</kbd>: This is the log of the cancer volume</li>
<li><kbd>lweight</kbd>: This is the log of the prostate weight</li>
<li><kbd>age</kbd>: This is the age of the patient in years</li>
<li><kbd>lbph</kbd>: This is the log of the amount of <strong>Benign Prostatic Hyperplasia</strong> (<strong>BPH</strong>), which is the non-cancerous enlargement of the prostate</li>
<li><kbd>svi</kbd>: This is the seminal vesicle invasion and an indicator variable of whether or not the cancer cells have invaded the seminal vesicles outside the prostate wall (<kbd>1</kbd> = yes, <kbd>0</kbd> = no)</li>
<li><kbd>lcp</kbd>: This is the log of capsular penetration and a measure of how much the cancer cells have extended in the covering of the prostate</li>
<li><kbd>gleason</kbd>: This is the patient's Gleason score; a score (2-10) provided by a pathologist after a biopsy about how abnormal the cancer cells appear--the higher the score, the more aggressive the cancer is assumed to be</li>
<li><kbd>pgg4</kbd>: This is the percent of Gleason patterns-four or five (high-grade cancer)</li>
<li><kbd>lpsa</kbd>: This is the log of the PSA; it is the response/outcome</li>
<li><kbd>train</kbd>: This is a logical vector (true or false) that signifies the training or test set</li>
</ul>
<p>The dataset is contained in the R package <kbd>ElemStatLearn</kbd>. After loading the required packages and data frame, we can begin to explore the variables and any possible relationships, as follows:</p>
<pre>
    <strong>&gt; library(ElemStatLearn) #contains the data</strong><br/>    <strong>&gt; library(car) #package to calculate Variance Inflation Factor</strong><br/>    <strong>&gt; library(corrplot) #correlation plots</strong><br/>    <strong>&gt; library(leaps) #best subsets regression</strong><br/>    <strong>&gt; library(glmnet) #allows ridge regression, LASSO and elastic net</strong><br/>    <strong>&gt; library(caret) #parameter tuning</strong>
</pre>
<p>With the packages loaded, bring up the <kbd>prostate</kbd> dataset and explore its structure:</p>
<pre>
    <strong>&gt; data(prostate)</strong><br/>    <strong>&gt; str(prostate)</strong><br/>    <strong>'data.frame':97 obs. of  10 variables:</strong><br/>    <strong> $ lcavol : num  -0.58 -0.994 -0.511 -1.204 0.751 ...</strong><br/>    <strong> $ lweight: num  2.77 3.32 2.69 3.28 3.43 ...</strong><br/>    <strong> $ age    : int  50 58 74 58 62 50 64 58 47 63 ...</strong><br/>    <strong> $ lbph   : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...</strong><br/>    <strong> $ svi    : int  0 0 0 0 0 0 0 0 0 0 ...</strong><br/>    <strong> $ lcp    : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...</strong><br/>    <strong> $ gleason: int  6 6 7 6 6 6 6 6 6 6 ...</strong><br/>    <strong> $ pgg45  : int  0 0 20 0 0 0 0 0 0 0 ...</strong><br/>    <strong> $ lpsa   : num  -0.431 -0.163 -0.163 -0.163 0.372 ...</strong><br/>    <strong> $ train  : logi  TRUE TRUE TRUE TRUE TRUE TRUE ...</strong>
</pre>
<p>The examination of the structure should raise a couple of issues that we will need to double-check. If you look at the features, <kbd>svi</kbd>, <kbd>lcp</kbd>, <kbd>gleason</kbd>, and <kbd>pgg45</kbd> have the same number in the first 10 observations, with the exception of one--the seventh observation in <kbd>gleason</kbd>. In order to make sure that these are viable as input features, we can use plots and tables so as to understand them. To begin with, use the following <kbd>plot()</kbd> command and input the entire data frame, which will create a scatterplot matrix:</p>
<pre>
    <strong>&gt; plot(prostate)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/image_04_01.png"/></div>
<p>With these many variables on one plot, it can get a bit difficult to understand what is going on, so we will drill down further. It does look like there is a clear linear relationship between our outcomes, <kbd>lpsa</kbd>, and <kbd>lcavol</kbd>. It also appears that the features mentioned previously have an adequate dispersion and are well-balanced across what will become our <kbd>train</kbd> and <kbd>test</kbd> sets with the possible exception of the <kbd>gleason</kbd> score. Note that the <kbd>gleason</kbd> scores captured in this dataset are of four values only. If you look at the plot where <kbd>train</kbd> and <kbd>gleason</kbd> intersect, one of these values is not in either <kbd>test</kbd> or <kbd>train</kbd>. This could lead to potential problems in our analysis and may require transformation. So, let's create a plot specifically for that feature, as follows:</p>
<pre>
    <strong>&gt; plot(prostate$gleason)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="304" width="463" class="image-border" src="assets/image_04_02.png"/></div>
<p>We have a problem here. Each dot represents an observation and the <span class="packt_screen"><em>x</em> axis</span> is the observation number in the data frame. There is only one <span class="packt_screen">Gleason Score</span> of <em><span class="packt_screen">8.0</span></em> and only five of score <em><span class="packt_screen">9.0</span></em>. You can look at the exact counts by producing a table of the features:</p>
<pre>
    <strong>&gt; table(prostate$gleason)</strong><br/>    <strong> 6  7  8  9 </strong><br/>    <strong>35 56  1  5 </strong>
</pre>
<p>What are our options? We could do any of the following:</p>
<ul>
<li>Exclude the feature altogether</li>
<li>Remove only the scores of <strong><span class="packt_screen">8.0</span></strong> and <strong><span class="packt_screen">9.0</span></strong></li>
<li>Recode this feature, creating an indicator variable</li>
</ul>
<p>I think it may help if we create a <kbd>boxplot</kbd> of <kbd>Gleason Score</kbd> versus <kbd>Log of PSA</kbd>. We used the <kbd>ggplot2</kbd> package to create boxplots in a prior chapter, but one can also create it with base R, as follows:</p>
<pre>
    <strong>&gt; boxplot(prostate$lpsa ~ prostate$gleason, xlab = "Gleason Score", <br/>      ylab = "Log of PSA")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="320" width="487" class="image-border" src="assets/image_04_03.png"/></div>
<p>Looking at the preceding plot, I think the best option will be to turn this into an indicator variable with <strong><span class="packt_screen">0</span></strong> being a <strong><span class="packt_screen">6</span></strong> score and <strong><span class="packt_screen">1</span></strong> being a <strong><span class="packt_screen">7</span></strong> or a higher score. Removing the feature may cause a loss of predictive ability. The missing values will also not work with the <kbd>glmnet</kbd> package that we will use.</p>
<p>You can code an indicator variable with one simple line of code using the <kbd>ifelse()</kbd> command by specifying the column in the data frame that you want to change. Then follow the logic that, if the observation is number <em>x</em>, then code it <em>y</em>, or else code it <em>z</em>:</p>
<pre>
    <strong>&gt; prostate$gleason &lt;- ifelse(prostate$gleason == 6, 0, 1)</strong>
</pre>
<p>As always, let's verify that the transformation worked as intended by creating a table in the following way:</p>
<pre>
    <strong>&gt; table(prostate$gleason)</strong><br/>    <strong> 0  1 </strong><br/>    <strong>35 62</strong>
</pre>
<p>That worked to perfection! As the scatterplot matrix was hard to read, let's move on to a correlation plot, which indicates if a relationship/dependency exists between the features. We will create a correlation object using the <kbd>cor()</kbd> function and then take advantage of the <kbd>corrplot</kbd> library with <kbd>corrplot.mixed()</kbd>, as follows:</p>
<pre>
    <strong>&gt; p.cor = cor(prostate)</strong><br/>    <strong>&gt; corrplot.mixed(p.cor)</strong>
</pre>
<p>The output of the preceding command is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="271" width="472" class="image-border" src="assets/image_04_04.png"/></div>
<p>A couple of things jump out here. First, PSA is highly correlated with the log of cancer volume (<kbd>lcavol</kbd>); you may recall that in the scatterplot matrix, it appeared to have a highly linear relationship. Second, multicollinearity may become an issue; for example, cancer volume is also correlated with capsular penetration and this is correlated with the seminal vesicle invasion. This should be an interesting learning exercise!</p>
<p>Before the learning can begin, the training and testing sets must be created. As the observations are already coded as being in the <kbd>train</kbd> set or not, we can use the <kbd>subset()</kbd> command and set the observations where <kbd>train</kbd> is coded to <kbd>TRUE</kbd> as our training set and <kbd>FALSE</kbd> for our testing set. It is also important to drop <kbd>train</kbd> as we do not want that as a feature:</p>
<pre>
    <strong>&gt; train &lt;- subset(prostate, train == TRUE)[, 1:9]</strong><br/>    <strong>&gt; str(train)</strong><br/>    <strong>'data.frame':67 obs. of  9 variables:</strong><br/>    <strong> $ lcavol : num  -0.58 -0.994 -0.511 -1.204 0.751 ...</strong><br/>    <strong> $ lweight: num  2.77 3.32 2.69 3.28 3.43 ...</strong><br/>    <strong> $ age    : int  50 58 74 58 62 50 58 65 63 63 ...</strong><br/>    <strong> $ lbph   : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...</strong><br/>    <strong> $ svi    : int  0 0 0 0 0 0 0 0 0 0 ...</strong><br/>    <strong> $ lcp    : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...</strong><br/>    <strong> $ gleason: num  0 0 1 0 0 0 0 0 0 1 ...</strong><br/>    <strong> $ pgg45  : int  0 0 20 0 0 0 0 0 0 30 ...</strong><br/>    <strong> $ lpsa   : num  -0.431 -0.163 -0.163 -0.163 0.372 ...</strong><br/>    <strong>&gt; test &lt;- subset(prostate, train == FALSE)[, 1:9]</strong><br/>    <strong>&gt; str(test)</strong><br/>    <strong>'data.frame':30 obs. of  9 variables:</strong><br/>    <strong> $ lcavol : num  0.737 -0.777 0.223 1.206 2.059 ...</strong><br/>    <strong> $ lweight: num  3.47 3.54 3.24 3.44 3.5 ...</strong><br/>    <strong> $ age    : int  64 47 63 57 60 69 68 67 65 54 ...</strong><br/>    <strong> $ lbph   : num  0.615 -1.386 -1.386 -1.386 1.475 ...</strong><br/>    <strong> $ svi    : int  0 0 0 0 0 0 0 0 0 0 ...</strong><br/>    <strong> $ lcp    : num  -1.386 -1.386 -1.386 -0.431 1.348 ...</strong><br/>    <strong> $ gleason: num  0 0 0 1 1 0 0 1 0 0 ...</strong><br/>    <strong> $ pgg45  : int  0 0 0 5 20 0 0 20 0 0 ...</strong><br/>    <strong> $ lpsa   : num  0.765 1.047 1.047 1.399 1.658 ...</strong>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>With the data prepared, we will begin the modeling process. For comparison purposes, we will create a model using best subsets regression like the previous two chapters and then utilize the regularization techniques.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Best subsets</h1>
            </header>

            <article>
                
<p>The following code is, for the most part, a rehash of what we developed in <a href="e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml" target="_blank">Chapter 2</a>, <em>Linear Regression - The Blocking and Tackling of Machine Learning</em>. We will create the best subset object using the <kbd>regsubsets()</kbd> command and specify the <kbd>train</kbd> portion of <kbd>data</kbd>. The variables that are selected will then be used in a model on the <kbd>test</kbd> set, which we will evaluate with a mean squared error calculation.</p>
<p>The model that we are building is written out as <kbd>lpsa ~ .</kbd> with the tilde and period stating that we want to use all the remaining variables in our data frame, with the exception of the response:</p>
<pre>
    <strong>&gt; subfit &lt;- regsubsets(lpsa ~ ., data = train)</strong>
</pre>
<p>With the model built, you can produce the best subset with two lines of code. The first one turns the <kbd>summary</kbd> model into an object where we can extract the various subsets and determine the best one with the <kbd>which.min()</kbd> command. In this instance, I will use BIC, which was discussed in <a href="e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml" target="_blank">Chapter 2</a>, <em>Linear Regression - The Blocking and Tackling of Machine Learning</em>, which is as follows:</p>
<pre>
    <strong>&gt; b.sum &lt;- summary(subfit)</strong><br/>    <strong>&gt; which.min(b.sum$bic)</strong><br/>    <strong>  [1] 3</strong>
</pre>
<p>The output is telling us that the model with the <kbd>3</kbd> features has the lowest <kbd>bic</kbd> value. A plot can be produced to examine the performance across the subset combinations, as follows:</p>
<pre>
    <strong>&gt; plot(b.sum$bic, type = "l", xlab = "# of Features", ylab = "BIC", <br/>      main = "BIC score by Feature Inclusion")</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="261" width="428" class="image-border" src="assets/image_04_05.png"/></div>
<p>A more detailed examination is possible by plotting the actual model object, as follows:</p>
<pre>
    <strong>&gt; plot(subfit, scale = "bic", main = "Best Subset Features")</strong>
</pre>
<p>The output of the preceding command is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="248" width="407" class="image-border" src="assets/image_04_06.png"/></div>
<p>So, the previous plot shows us that the three features included in the lowest <kbd>BIC</kbd> are <kbd>lcavol</kbd>, <kbd>lweight</kbd>, and <kbd>gleason</kbd>. It is noteworthy that <kbd>lcavol</kbd> is included in every combination of the models. This is consistent with our earlier exploration of the data. We are now ready to try this model on the <kbd>test</kbd> portion of the data, but first, we will produce a plot of the fitted values versus the actual values looking for linearity in the solution, and as a check on the constancy of the variance. A linear model will need to be created with just the three features of interest. Let's put this in an object called <kbd>ols</kbd> for the OLS. Then the fits from <kbd>ols</kbd> will be compared to the actual in the training set, as follows:</p>
<pre>
    <strong>&gt; ols &lt;- lm(lpsa ~ lcavol + lweight + gleason, data = train)</strong><br/>    <strong>&gt; plot(ols$fitted.values, train$lpsa, xlab = "Predicted", ylab = <br/>      "Actual", main = "Predicted vs Actual")</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="334" width="522" class="image-border" src="assets/image_04_07.png"/></div>
<p>An inspection of the plot shows that a linear fit should perform well on this data and that the non-constant variance is not a problem. With that, we can see how this performs on the test set data by utilizing the <kbd>predict()</kbd> function and specifying <kbd>newdata=test</kbd>, as follows:</p>
<pre>
    <strong>&gt; pred.subfit &lt;- predict(ols, newdata = test)</strong><br/>    <strong>&gt; plot(pred.subfit, test$lpsa , xlab = "Predicted", ylab = <br/>      "Actual", main = "Predicted vs Actual")</strong>
</pre>
<p>The values in the object can then be used to create a plot of the <kbd>Predicted vs Actual</kbd> values, as shown in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="294" width="458" class="image-border" src="assets/image_04_08.png"/></div>
<p>The plot doesn't seem to be too terrible. For the most part, it is a linear fit with the exception of what looks to be two outliers on the high end of the PSA score. Before concluding this section, we will need to calculate <strong>Mean Squared Error</strong> (<strong>MSE</strong>) to facilitate comparison across the various modeling techniques. This is easy enough where we will just create the residuals and then take the mean of their squared values, as follows:</p>
<pre>
    <strong>&gt; resid.subfit &lt;- test$lpsa - pred.subfit</strong><br/>    <strong>&gt; mean(resid.subfit^2)</strong><br/>    <strong>[1] 0.5084126</strong>
</pre>
<p>So, MSE of <kbd>0.508</kbd> is our benchmark for going forward.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Ridge regression</h1>
            </header>

            <article>
                
<p>With ridge regression, we will have all the eight features in the model, so this will be an intriguing comparison with the best subsets model. The package that we will use and is in fact already loaded, is <kbd>glmnet</kbd>. The package requires that the input features are in a matrix instead of a data frame and for ridge regression, we can follow the command sequence of <kbd>glmnet(x = our input matrix, y = our response, family = the distribution, alpha=0)</kbd>. The syntax for alpha relates to <kbd>0</kbd> for ridge regression and <kbd>1</kbd> for doing LASSO.</p>
<p>To get the <kbd>train</kbd> set ready for use in <kbd>glmnet</kbd> is actually quite easy by using <kbd>as.matrix()</kbd> for the inputs and creating a vector for the response, as follows:</p>
<pre>
    <strong>&gt; x &lt;- as.matrix(train[, 1:8])</strong><br/>    <strong>&gt; y &lt;- train[, 9]</strong>
</pre>
<p>Now, run the ridge regression by placing it in an object called, appropriately I might add, <kbd>ridge</kbd>. It is important to note here that the <kbd>glmnet</kbd> package will first standardize the inputs before computing the lambda values and then will unstandardize the coefficients. You will need to specify the distribution of the response variable as <kbd>gaussian</kbd> as it is continuous and <kbd>alpha=0</kbd> for ridge regression, as follows:</p>
<pre>
    <strong>&gt; ridge &lt;- glmnet(x, y, family = "gaussian", alpha = 0)</strong>
</pre>
<p>The object has all the information that we need in order to evaluate the technique. The first thing to try is the <kbd>print()</kbd> command, which will show us the number of nonzero coefficients, percent deviance explained, and correspondent value of <kbd>Lambda</kbd>. The default number in the package of steps in the algorithm is <kbd>100</kbd>. However, the algorithm will stop prior to <kbd>100</kbd> steps if the percent deviation does not dramatically improve from one lambda to another; that is, the algorithm converges to an optimal solution. For the purpose of saving space, I will present only the following first five and last ten lambda results:</p>
<pre>
    <strong>&gt; print(ridge)</strong><br/>    <strong>Call:  glmnet(x = x, y = y, family = "gaussian", alpha = 0) </strong><br/>    <strong>       Df      %Dev    Lambda</strong><br/>    <strong>  [1,]  8 3.801e-36 878.90000</strong><br/>    <strong>  [2,]  8 5.591e-03 800.80000</strong><br/>    <strong>  [3,]  8 6.132e-03 729.70000</strong><br/>    <strong>  [4,]  8 6.725e-03 664.80000</strong><br/>    <strong>  [5,]  8 7.374e-03 605.80000</strong><br/>    <strong>  ...........................</strong><br/>    <strong> [91,]  8 6.859e-01   0.20300</strong><br/>    <strong> [92,]  8 6.877e-01   0.18500</strong><br/>    <strong> [93,]  8 6.894e-01   0.16860</strong><br/>    <strong> [94,]  8 6.909e-01   0.15360</strong><br/>    <strong> [95,]  8 6.923e-01   0.13990</strong><br/>    <strong> [96,]  8 6.935e-01   0.12750</strong><br/>    <strong> [97,]  8 6.946e-01   0.11620</strong><br/>    <strong> [98,]  8 6.955e-01   0.10590</strong><br/>    <strong> [99,]  8 6.964e-01   0.09646</strong><br/>    <strong>[100,]  8 6.971e-01   0.08789</strong>
</pre>
<p>Look at row <kbd>100</kbd> for example. It shows us that the number of nonzero coefficients or-said another way-the number of features included, is <span class="packt_screen">eight</span>; please recall that it will always be the same for ridge regression. We also see that the percent of deviance explained is <kbd>.6971</kbd> and the <kbd>Lambda</kbd> tuning parameter for this row is <kbd>0.08789</kbd>. Here is where we can decide on which lambda to select for the <kbd>test</kbd> set. The lambda of <kbd>0.08789</kbd> can be used, but let's make it a little simpler, and for the <kbd>test</kbd> set, try <kbd>0.10</kbd>. A couple of plots might help here so let's start with the package's default, adding annotations to the curve by adding <kbd>label=TRUE</kbd> in the following syntax:</p>
<pre>
    <strong>&gt; plot(ridge, label = TRUE)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="293" width="457" class="image-border" src="assets/image_04_09.png"/></div>
<p>In the default plot, the <em>y</em> axis is the value of <span class="packt_screen">Coefficients</span> and the <em>x</em> axis is <span class="packt_screen">L1 Norm</span>. The plot tells us the coefficient values versus the <span class="packt_screen">L1 Norm</span>. The top of the plot contains a second <em>x</em> axis, which equates to the number of features in the model. Perhaps a better way to view this is by looking at the coefficient values changing as <kbd>lambda</kbd> changes. We just need to tweak the code in the following <kbd>plot()</kbd> command by adding <kbd>xvar="lambda"</kbd>. The other option is the percent of deviance explained by substituting <kbd>lambda</kbd> with <kbd>dev</kbd>:</p>
<pre>
    <strong>&gt; plot(ridge, xvar = "lambda", label = TRUE)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="302" width="472" class="image-border" src="assets/image_04_10.png"/></div>
<p>This is a worthwhile plot as it shows that as <kbd>lambda</kbd> decreases, the shrinkage parameter decreases and the absolute values of the coefficients increase. To see the coefficients at a specific <kbd>lambda</kbd> value, use the <kbd>coef()</kbd> command. Here, we will specify the <kbd>lambda</kbd> value that we want to use by specifying s=0.1. We will also state that we want <kbd>exact=TRUE</kbd>, which tells <kbd>glmnet</kbd> to fit a model with that specific <kbd>lambda</kbd> value versus interpolating from the values on either side of our <kbd>lambda</kbd>, as follows:</p>
<pre>
    <strong>&gt; ridge.coef &lt;- coef(ridge, s = 0.1, exact = TRUE)</strong><br/>    <strong>&gt; ridge.coef</strong><br/>    <strong>9 x 1 sparse Matrix of class "dgCMatrix"</strong><br/>    <strong>                      1</strong><br/>    <strong>(Intercept)  0.13062197</strong><br/>    <strong>lcavol       0.45721270</strong><br/>    <strong>lweight      0.64579061</strong><br/>    <strong>age         -0.01735672</strong><br/>    <strong>lbph         0.12249920</strong><br/>    <strong>svi          0.63664815</strong><br/>    <strong>lcp         -0.10463486</strong><br/>    <strong>gleason      0.34612690</strong><br/>    <strong>pgg45        0.00428580</strong>
</pre>
<p>It is important to note that <kbd>age</kbd>, <kbd>lcp</kbd>, and <kbd>pgg45</kbd> are close to, but not quite, zero. Let's not forget to plot deviance versus coefficients as well:</p>
<pre>
    <strong>&gt; plot(ridge, xvar = "dev", label = TRUE)</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="260" width="405" class="image-border" src="assets/image_04_11.png"/></div>
<p>Comparing the two previous plots, we can see that as <kbd>lambda</kbd> decreases, the coefficients increase and the percent/fraction of the deviance explained increases. If we were to set <kbd>lambda</kbd> equal to zero, we would have no shrinkage penalty and our model would equate the <kbd>OLS</kbd>.</p>
<p>To prove this on the <kbd>test</kbd> set, we will have to transform the features as we did for the training data:</p>
<pre>
    <strong>&gt; newx &lt;- as.matrix(test[, 1:8])</strong>
</pre>
<p>Then, we use the <kbd>predict</kbd> function to create an object that we will call <kbd>ridge.y</kbd> with <kbd>type = "response"</kbd> and our <kbd>lambda</kbd> equal to <kbd>0.10</kbd> and plot the <kbd>Predicted</kbd> values versus the <kbd>Actual</kbd> values, as follows:</p>
<pre>
    <strong>&gt; ridge.y &lt;- predict(ridge, newx = newx, type = "response", s = <br/>      0.1)</strong><br/>    <strong>&gt; plot(ridge.y, test$lpsa, xlab = "Predicted", ylab = "Actual",main <br/>      = "Ridge Regression")</strong>
</pre>
<p>The output of the following command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="275" width="428" class="image-border" src="assets/image_04_12.png"/></div>
<p>The plot of <kbd>Predicted</kbd> versus <kbd>Actual</kbd> of <kbd>Ridge Regression</kbd> seems to be quite similar to best subsets, complete with two interesting outliers at the high end of the PSA measurements. In the real world, it would be advisable to explore these outliers further so as to understand whether they are truly unusual or we are missing something. This is where domain expertise would be invaluable. The MSE comparison to the benchmark may tell a different story. We first calculate the residuals, and then take the mean of those residuals squared:</p>
<pre>
    <strong>&gt; ridge.resid &lt;- ridge.y - test$lpsa</strong><br/>    <strong>&gt; mean(ridge.resid^2)</strong><br/>    <strong>[1] 0.4789913</strong>
</pre>
<p>Ridge regression has given us a slightly better MSE. It is now time to put LASSO to the test to see if we can decrease our errors even further.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">LASSO</h1>
            </header>

            <article>
                
<p>To run LASSO next is quite simple and we only have to change one number from our ridge regression model: that is, change <kbd>alpha=0</kbd> to <kbd>alpha=1</kbd> in the <kbd>glmnet()</kbd> syntax. Let's run this code and also see the output of the model, looking at the first five and last 10 results:</p>
<pre>
    <strong>&gt; lasso &lt;- glmnet(x, y, family = "gaussian", alpha = 1)</strong><br/>    <strong>&gt; print(lasso)</strong><br/>    <strong>Call: glmnet(x = x, y = y, family = "gaussian", alpha = 1) </strong><br/>    <strong>Df %Dev Lambda</strong><br/>    <strong>[1,] 0 0.00000 0.878900</strong><br/>    <strong>[2,] 1 0.09126 0.800800</strong><br/>    <strong>[3,] 1 0.16700 0.729700</strong><br/>    <strong>[4,] 1 0.22990 0.664800</strong><br/>    <strong>[5,] 1 0.28220 0.605800</strong><br/>    <strong>........................</strong><br/>    <strong>[60,] 8 0.70170 0.003632</strong><br/>    <strong>[61,] 8 0.70170 0.003309</strong><br/>    <strong>[62,] 8 0.70170 0.003015</strong><br/>    <strong>[63,] 8 0.70170 0.002747</strong><br/>    <strong>[64,] 8 0.70180 0.002503</strong><br/>    <strong>[65,] 8 0.70180 0.002281</strong><br/>    <strong>[66,] 8 0.70180 0.002078</strong><br/>    <strong>[67,] 8 0.70180 0.001893</strong><br/>    <strong>[68,] 8 0.70180 0.001725</strong><br/>    <strong>[69,] 8 0.70180 0.001572</strong>
</pre>
<p>Note that the model building process stopped at step <kbd>69</kbd> as the deviance explained no longer improved as <kbd>lambda</kbd> decreased. Also, note that the <kbd>Df</kbd> column now changes along with <kbd>lambda</kbd>. At first glance, here it seems that all the eight features should be in the model with a <kbd>lambda</kbd> of <kbd>0.001572</kbd>. However, let's try and find and test a model with fewer features, around seven, for argument's sake. Looking at the rows, we see that around a <kbd>lambda</kbd> of <kbd>0.045</kbd>, we end up with <kbd>7</kbd> features versus <kbd>8</kbd>. Thus, we will plug this <kbd>lambda</kbd> in for our <kbd>test</kbd> set evaluation, as follows:</p>
<pre>
    <strong>[31,] 7 0.67240 0.053930</strong><br/>    <strong>[32,] 7 0.67460 0.049140</strong><br/>    <strong>[33,] 7 0.67650 0.044770</strong><br/>    <strong>[34,] 8 0.67970 0.040790</strong><br/>    <strong>[35,] 8 0.68340 0.037170</strong>
</pre>
<p>Just as with ridge regression, we can plot the results:</p>
<pre>
    <strong>&gt; plot(lasso, xvar = "lambda", label = TRUE)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="289" width="450" class="image-border" src="assets/image_04_13.png"/></div>
<p>This is an interesting plot and really shows how LASSO works. Notice how the lines labeled <strong>8</strong>, <strong>3</strong>, and <strong>6</strong> behave, which corresponds to the <kbd>pgg45</kbd>, <kbd>age</kbd>, and <kbd>lcp</kbd> features respectively. It looks as if <kbd>lcp</kbd> is at or near zero until it is the last feature that is added. We can see the coefficient values of the seven feature model just as we did with ridge regression by plugging it into <kbd>coef()</kbd>, as follows:</p>
<pre>
    <strong>&gt; lasso.coef &lt;- coef(lasso, s = 0.045, exact = TRUE)</strong><br/>    <strong>&gt; lasso.coef</strong><br/>    <strong>9 x 1 sparse Matrix of class "dgCMatrix"</strong><br/>    <strong>                        1</strong><br/>    <strong>(Intercept) -0.1305852115</strong><br/>    <strong>lcavol       0.4479676523</strong><br/>    <strong>lweight      0.5910362316</strong><br/>    <strong>age         -0.0073156274</strong><br/>    <strong>lbph         0.0974129976</strong><br/>    <strong>svi          0.4746795823</strong><br/>    <strong>lcp          .           </strong><br/>    <strong>gleason      0.2968395802</strong><br/>    <strong>pgg45        0.0009790322</strong>
</pre>
<p>The LASSO algorithm zeroed out the coefficient for <kbd>lcp</kbd> at a <kbd>lambda</kbd> of <kbd>0.045</kbd>. Here is how it performs on the <kbd>test</kbd> data:</p>
<pre>
    <strong>&gt; lasso.y &lt;- predict(lasso, newx = newx, type = "response", s =  <br/>      0.045)</strong><br/>    <strong>&gt; plot(lasso.y, test$lpsa, xlab = "Predicted", ylab = "Actual", <br/>      main = "LASSO")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="284" width="443" class="image-border" src="assets/image_04_14.png"/></div>
<p>We calculate MSE as we did before:</p>
<pre>
    <strong>&gt; lasso.resid &lt;- lasso.y - test$lpsa</strong><br/>    <strong>&gt; mean(lasso.resid^2)</strong><br/>    <strong>[1] 0.4437209</strong>
</pre>
<p>It looks like we have similar plots as before, with only the slightest improvement in MSE. Our last best hope for dramatic improvement is with elastic net. To this end, we will still use the <kbd>glmnet</kbd> package. The twist will be that, we will solve for <kbd>lambda</kbd> and for the elastic net parameter known as <kbd>alpha</kbd>. Recall that <kbd>alpha = 0</kbd> is the ridge regression penalty and <kbd>alpha = 1</kbd> is the LASSO penalty. The elastic net parameter will be <kbd>0 ≤ alpha ≤ 1</kbd>. Solving for two different parameters simultaneously can be complicated and frustrating, but we can use our friend in R, the <kbd>caret</kbd> package, for assistance.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Elastic net</h1>
            </header>

            <article>
                
<p>The <kbd>caret</kbd> package stands for classification and regression training. It has an excellent companion website to help in understanding all of its capabilities: <a href="http://topepo.github.io/caret/index.html."><span class="URLPACKT">http://topepo.github.io/caret/index.html</span>.</a> The package has many different functions that you can use and we will revisit some of them in the later chapters. For our purpose here, we want to focus on finding the optimal mix of lambda and our elastic net mixing parameter, <kbd>alpha</kbd>. This is done using the following simple three-step process:</p>
<ol>
<li>Use the <kbd>expand.grid()</kbd> function in base R to create a vector of all the possible combinations of <kbd>alpha</kbd> and <kbd>lambda</kbd> that we want to investigate.</li>
<li>Use the <kbd>trainControl()</kbd> function from the <kbd>caret</kbd> package to determine the resampling method; we will use LOOCV as we did in <a href="e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Linear Regression - The Blocking and Tackling of Machine Learning</em>.</li>
<li>Train a model to select our <kbd>alpha</kbd> and <kbd>lambda</kbd> parameters using <kbd>glmnet()</kbd> in caret's <kbd>train()</kbd> function.</li>
</ol>
<p>Once we've selected our parameters, we will apply them to the <kbd>test</kbd> data in the same way as we did with ridge regression and LASSO.</p>
<div class="packt_tip">Our grid of combinations should be large enough to capture the best model but not too large that it becomes computationally unfeasible. That won't be a problem with this size dataset, but keep this in mind for future references.</div>
<p>Here are the values of hyperparameters we can try:</p>
<ul>
<li>Alpha from <kbd>0</kbd> to <kbd>1</kbd> by <kbd>0.2</kbd> increments; remember that this is bound by <kbd>0</kbd> and <kbd>1</kbd></li>
<li>Lambda from <kbd>0.00</kbd> to <kbd>0.2</kbd> in steps of <kbd>0.02</kbd>; the <kbd>0.2</kbd> lambda should provide a cushion from what we found in ridge regression (lambda=<kbd>0.1</kbd>) and LASSO (lambda=<kbd>0.045</kbd>)</li>
</ul>
<p>You can create this vector by using the <kbd>expand.grid()</kbd> function and building a sequence of numbers for what the <kbd>caret</kbd> package will automatically use. The <kbd>caret</kbd> package will take the values for <kbd>alpha</kbd> and <kbd>lambda</kbd> with the following code:</p>
<pre>
    <strong>&gt; grid &lt;- expand.grid(.alpha = seq(0, 1, by = .2), .lambda = <br/>      seq(0.00, 0.2,  by = 0.02))</strong>
</pre>
<p>The <kbd>table()</kbd> function will show us the complete set of 66 combinations:</p>
<pre>
    <strong>&gt; table(grid)</strong><br/>    <strong>      .lambda</strong><br/>    <strong>.alpha 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2</strong><br/>    <strong>   0   1    1    1    1    1   1    1    1    1    1   1</strong><br/>    <strong>   0.2 1    1    1    1    1   1    1    1    1    1   1</strong><br/>    <strong>   0.4 1    1    1    1    1   1    1    1    1    1   1</strong><br/>    <strong>   0.6 1    1    1    1    1   1    1    1    1    1   1</strong><br/>    <strong>   0.8 1    1    1    1    1   1    1    1    1    1   1</strong><br/>    <strong>   1   1    1    1    1    1   1    1    1    1    1   1</strong>
</pre>
<p>We can confirm that this is what we wanted--<kbd>alpha</kbd> from <kbd>0</kbd> to <kbd>1</kbd> and <kbd>lambda</kbd> from <kbd>0</kbd> to <kbd>0.2</kbd>.</p>
<div class="packt_infobox">For the resampling method, we will put in the code for <kbd>LOOCV</kbd> for the method. There are also other resampling alternatives such as bootstrapping or k-fold cross-validation and numerous options that you can use with <kbd>trainControl()</kbd>, but we will explore these options in future chapters.</div>
<p>You can tell the model selection criteria with <kbd>selectionFunction()</kbd> in <kbd>trainControl()</kbd>. For quantitative responses, the algorithm will select based on its default of <strong>Root Mean Square Error</strong> (<strong>RMSE</strong>), which is perfect for our purposes:</p>
<pre>
    <strong>&gt; control &lt;- trainControl(method = "LOOCV")</strong>
</pre>
<p>It is now time to use <kbd>train()</kbd> to determine the optimal elastic net parameters. The function is similar to <kbd>lm()</kbd>. We will just add the syntax: <kbd>method="glmnet"</kbd>, <kbd>trControl=control</kbd> and <kbd>tuneGrid=grid</kbd>. Let's put this in an object called <kbd>enet.train</kbd>:</p>
<pre>
    <strong>&gt; enet.train &lt;- train(lpsa ~ ., data = train, method = "glmnet", <br/>      trControl = control, tuneGrid = grid)</strong>
</pre>
<p>Calling the object will tell us the parameters that lead to the lowest <kbd>RMSE</kbd>, as follows:</p>
<pre>
    <strong>&gt; enet.train</strong><br/>    <strong>glmnet </strong><br/>    <strong>67 samples</strong><br/>    <strong> 8 predictor</strong><br/>    <strong>No pre-processing</strong><br/>    <strong>Resampling: </strong><br/>    <strong>Summary of sample sizes: 66, 66, 66, 66, 66, 66, ... </strong><br/>    <strong>Resampling results across tuning parameters:</strong><br/>    <strong>  alpha  lambda  RMSE   Rsquared</strong><br/>    <strong>  0.0    0.00    0.750  0.609   </strong><br/>    <strong>  0.0    0.02    0.750  0.609   </strong><br/>    <strong>  0.0    0.04    0.750  0.609   </strong><br/>    <strong>  0.0    0.06    0.750  0.609   </strong><br/>    <strong>  0.0    0.08    0.750  0.609   </strong><br/>    <strong>  0.0    0.10    0.751  0.608   </strong><br/>    <strong>   .........................</strong><br/>    <strong>  1.0    0.14    0.800  0.564   </strong><br/>    <strong>  1.0    0.16    0.809  0.558   </strong><br/>    <strong>  1.0    0.18    0.819  0.552   </strong><br/>    <strong>  1.0    0.20    0.826  0.549   </strong>
</pre>
<p><kbd>RMSE</kbd> was used to select the optimal model using the smallest value. The final values used for the model were <kbd>alpha = 0</kbd> and <kbd>lambda = 0.08</kbd>.</p>
<p>This experimental design has led to the optimal tuning parameters of <kbd>alpha = 0</kbd> and <kbd>lambda = 0.08</kbd>, which is a ridge regression with <kbd>s = 0.08</kbd> in <kbd>glmnet</kbd>, recall that we used <kbd>0.10</kbd>. The <kbd>R-squared</kbd> is 61 percent, which is nothing to write home about.</p>
<p>The process for the <kbd>test</kbd> set validation is just as before:</p>
<pre>
    <strong>&gt; enet &lt;- glmnet(x, y, family = "gaussian", alpha = 0, lambda = <br/>      .08)</strong><br/>    <strong>&gt; enet.coef &lt;- coef(enet, s = .08, exact = TRUE)</strong><br/>    <strong>&gt; enet.coef</strong><br/>    <strong>9 x 1 sparse Matrix of class "dgCMatrix"</strong><br/>    <strong>                       1</strong><br/>    <strong>(Intercept)  0.137811097</strong><br/>    <strong>lcavol       0.470960525</strong><br/>    <strong>lweight      0.652088157</strong><br/>    <strong>age         -0.018257308</strong><br/>    <strong>lbph         0.123608113</strong><br/>    <strong>svi          0.648209192</strong><br/>    <strong>lcp         -0.118214386</strong><br/>    <strong>gleason      0.345480799</strong><br/>    <strong>pgg45        0.004478267</strong><br/>    <strong>&gt; enet.y &lt;- predict(enet, newx=newx, type="response", s=.08)</strong><br/>    <strong>&gt; plot(enet.y, test$lpsa, xlab="Predicted", ylab="Actual", <br/>      main="Elastic Net")</strong>
</pre>
<p>The output of the preceding command is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="283" width="441" class="image-border" src="assets/image_04_15.png"/></div>
<p>Calculate MSE as we did before:</p>
<pre>
    <strong>&gt; enet.resid &lt;- enet.y - test$lpsa</strong><br/>    <strong>&gt; mean(enet.resid^2)</strong><br/>    <strong>[1] 0.4795019</strong>
</pre>
<p>This model error is similar to the ridge penalty. On the <kbd>test</kbd> set, our LASSO model did the best in terms of errors. We may be over-fitting! Our best subset model with three features is the easiest to explain, and in terms of errors, is acceptable to the other techniques. We can use cross-validation in the <kbd>glmnet</kbd> package to possibly identify a better solution.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Cross-validation with glmnet</h1>
            </header>

            <article>
                
<p>We have used <kbd>LOOCV</kbd> with the <kbd>caret</kbd> package; now we will try k-fold cross-validation. The <kbd>glmnet</kbd> package defaults to ten folds when estimating lambda in <kbd>cv.glmnet()</kbd>. In k-fold CV, the data is partitioned into an equal number of subsets (folds) and a separate model is built on each k-1 set and then tested on the corresponding holdout set with the results combined (averaged) to determine the final parameters.</p>
<p>In this method, each fold is used as a <kbd>test</kbd> set only once. The <kbd>glmnet</kbd> package makes it very easy to try this and will provide you with an output of the lambda values and the corresponding MSE. It defaults to <kbd>alpha = 1</kbd>, so if you want to try ridge regression or an elastic net mix, you will need to specify it. As we will be trying for as few input features as possible, we will stick to the defaults, but given the size of the training data, use only three folds:</p>
<pre>
    <strong>&gt; set.seed(317)</strong>  <br/>    <strong>&gt; lasso.cv = cv.glmnet(x, y, nfolds = 3)</strong><br/>    <strong>&gt; plot(lasso.cv)</strong>
</pre>
<p>The output of the preceding code is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="276" width="431" class="image-border" src="assets/image_04_16.png"/></div>
<p>The plot for CV is quite different than the other <kbd>glmnet</kbd> plots, showing <span class="packt_screen">log(Lambda)</span> versus <span class="packt_screen">Mean-Squared Error</span> along with the number of features. The two dotted vertical lines signify the minimum of MSE (left line) and one standard error from the minimum (right line). One standard error away from the minimum is a good place to start if you have an over-fitting problem. You can also call the exact values of these two lambdas, as follows:</p>
<pre>
    <strong>&gt; lasso.cv$lambda.min #minimum<br/>    [1] 0.0133582<br/>    &gt; lasso.cv$lambda.1se #one standard error away<br/>    [1] 0.124579</strong>
</pre>
<p>Using <kbd>lambda.1se</kbd>, we can go through the following process of viewing the coefficients and validating the model on the test data:</p>
<pre>
    <strong>&gt; coef(lasso.cv, s = "lambda.1se")<br/>    9 x 1 sparse Matrix of class "dgCMatrix"<br/>     1<br/>    (Intercept) -0.13543760<br/>    lcavol 0.43892533<br/>    lweight 0.49550944<br/>    age . <br/>    lbph 0.04343678<br/>    svi 0.34985691<br/>    lcp . <br/>    gleason 0.21225934<br/>    pgg45 . </strong><br/>    <br/>    <strong>&gt; lasso.y.cv = predict(lasso.cv, newx=newx, type = "response",<br/>    s = "lambda.1se")<br/><br/>    &gt; lasso.cv.resid = lasso.y.cv - test$lpsa<br/><br/>    &gt; mean(lasso.cv.resid^2)<br/>    [1] 0.4465453</strong>
</pre>
<p>This model achieves an error of <kbd>0.45</kbd> with just five features, zeroing out <kbd>age</kbd>, <kbd>lcp</kbd>, and <kbd>pgg45</kbd>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Model selection</h1>
            </header>

            <article>
                
<p>We looked at five different models in examining this dataset. The following points were the <kbd>test</kbd> set error of these models:</p>
<ul>
<li>Best subsets is 0.51</li>
<li>Ridge regression is 0.48</li>
<li>LASSO is 0.44</li>
<li>Elastic net is 0.48</li>
<li>LASSO with CV is 0.45</li>
</ul>
<p>On a pure error, LASSO with seven features performed the best. However, does this best address the question that we are trying to answer? Perhaps the more parsimonious model that we found using CV with a lambda of <kbd>~0.125</kbd> is more appropriate. My inclination is to put forth the latter as it is more interpretable.</p>
<p>Having said all this, there is clearly a need for domain-specific knowledge from oncologists, urologists, and pathologists in order to understand what would make the most sense. There is that, but there is also the need for more data. With this sample size, the results can vary greatly just by changing the randomization seeds or creating different <kbd>train</kbd> and <kbd>test</kbd> sets (try it and see for yourself.) At the end of the day, these results may likely raise more questions than provide you with answers. But is this bad? I would say no, unless you made the critical mistake of over-promising at the start of the project about what you will be able to provide. This is a fair warning to prudently apply the tools put forth in <a href="b971f400-c64b-4e11-b9e0-9039038f4536.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>A Process for Success</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Regularization and classification</h1>
            </header>

            <article>
                
<p>The regularization techniques applied above will also work for classification problems, both binomial and multinomial.  Therefore, let's not conclude this chapter until we apply some sample code on a logistic regression problem, specifically the breast cancer data from the prior chapter.  As in regression with a quantitative response, this can be an important technique to utilize data sets with high dimensionality.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Logistic regression example </h1>
            </header>

            <article>
                
<p>Recall that, in the breast cancer data we analyzed, the probability of a tumor being malignant can be denoted as follows in a logistic function:</p>
<p><em>P(malignant) = 1 / 1 + e<sup>-(B0 + B1X1 + BnXn)</sup></em><sup> </sup></p>
<p>Since we have a linear component in the function, L1 and L2 regularization can be applied. To demonstrate this, let's load and prepare the breast cancer data like we did in the previous chapter:</p>
<pre>
<strong>    &gt; library(MASS)</strong><br/><strong>    &gt; biopsy$ID = NULL</strong><br/><strong>    &gt; names(biopsy) = c("thick", "u.size", "u.shape", "adhsn",</strong><br/><strong>    "s.size", "nucl", "chrom", "n.nuc", "mit", "class")</strong><br/><strong>    &gt; biopsy.v2 &lt;- na.omit(biopsy)</strong><strong><br/>    &gt; set.seed(123) </strong><br/><strong>    &gt; ind &lt;- sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, <br/>      0.3))</strong><br/><strong>    &gt; train &lt;- biopsy.v2[ind==1, ] </strong><br/><strong>    &gt; test &lt;- biopsy.v2[ind==2, ] </strong>
</pre>
<p>Transform the data to an input matrix and the labels:</p>
<pre>
<strong>    &gt; x &lt;- as.matrix(train[, 1:9])</strong><br/><strong>    &gt; y &lt;- train[, 10]</strong>
</pre>
<p>In the function <kbd>cv.glmnet</kbd>, we will change the family to binomial and the measure to area under the curve, along with five folds:</p>
<pre>
<strong>    &gt; set.seed(3)</strong><br/><strong>    &gt; fitCV &lt;- cv.glmnet(x, y, family = "binomial",</strong><br/><strong>         type.measure = "auc",</strong><br/><strong>         nfolds = 5)</strong>
</pre>
<p>Plotting <kbd>fitCV</kbd> gives us the <kbd>AUC</kbd> by lambda:</p>
<pre>
<strong>    &gt; plot(fitCV)</strong>
</pre>
<p>The output from the plot command is as shown:</p>
<div class="CDPAlignCenter CDPAlign"><img height="305" width="475" class="image-border" src="assets/image_04_17.png"/></div>
<p>Interesting! Notice the immediate improvement to <kbd>AUC</kbd> by adding just one feature. Let's just have a look at the coefficients for one standard error:</p>
<pre>
<strong>    &gt; fitCV$lambda.1se<br/>    [1] 0.1876892<br/>    &gt; coef(fitCV, s = "lambda.1se")<br/>    10 x 1 sparse Matrix of class "dgCMatrix"<br/>     1<br/>    (Intercept) -1.84478214<br/>    thick 0.01892397<br/>    u.size 0.10102690<br/>    u.shape 0.08264828<br/>    adhsn . <br/>    s.size . <br/>    nucl 0.13891750<br/>    chrom . <br/>    n.nuc . <br/>    mit .</strong>
</pre>
<p>Here, we see that the four features selected are <kbd>thickness</kbd>, <kbd>u.size</kbd>, <kbd>u.shape</kbd>, and <kbd>nucl</kbd>.  Like we did in the prior chapter, let's look at how it performs on the test set in terms of error and <kbd>auc</kbd>:</p>
<pre>
<strong>    &gt; library(InformationValue)<br/>    &gt; predCV &lt;- predict(fitCV, newx = as.matrix(test[, 1:9]),<br/>        s = "lambda.1se",<br/>        type = "response")<br/>    actuals &lt;- ifelse(test$class == "malignant", 1, 0)<br/>    misClassError(actuals, predCV)<br/>    [1] 0.0622<br/>    &gt; plotROC(actuals, predCV)</strong>
</pre>
<p>Output from the previous code:</p>
<div class="CDPAlignCenter CDPAlign"><img height="194" width="302" class="image-border" src="assets/image_04_18.png"/></div>
<p>The results show that it performed comparable to the logistic regression done previously.  It doesn't look like using <kbd>lambda.1se</kbd> was optimal and we should see if we can improve the output of the sample prediction using <kbd>lambda.min</kbd>:</p>
<pre>
<strong>    &gt; predCV.min &lt;- predict(fitCV, newx = as.matrix(test[, 1:9]),</strong><br/><strong>        s = "lambda.min",</strong><br/><strong>        type = "response")</strong><strong><br/>    &gt; misClassError(actuals, predCV.min)</strong><br/><strong>    [1] 0.0239</strong>
</pre>
<p>There you have it!  An error rate as good as what we did in <a href="d5d39222-b2f8-4c80-9348-34e075893e47.xhtml">Chapter 3</a>, <em>Logistic Regression and Discriminant Analysis.</em></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, the goal was to use a small dataset to provide an introduction to practically apply an advanced feature selection for linear models. The outcome for our data was quantitative, but the <kbd>glmnet</kbd> package we used also supports qualitative outcomes (binomial and multinomial classifications). An introduction to regularization and the three techniques that incorporate it were provided and utilized to build and compare models. Regularization is a powerful technique to improve computational efficiency and to possibly extract more meaningful features when compared to the other modeling techniques. Additionally, we started to use the <kbd>caret</kbd> package to optimize multiple parameters when training a model. Up to this point, we've been purely talking about linear models. In the next couple of chapters, we will begin to use nonlinear models for both classification and regression problems.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>