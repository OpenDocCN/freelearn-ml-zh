<html><head></head><body>
		<div id="_idContainer214" class="IMG---Figure">
			<h1 id="_idParaDest-216" class="chapter-number"><a id="_idTextAnchor215"/>13</h1>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor216"/>Autonomous Vehicles</h1>
			<p>Today, almost every car company is advancing technology in their cars using <strong class="bold">Autonomous Vehicle</strong> (<strong class="bold">AV</strong>) systems and <strong class="bold">Advanced Driver Assistance Systems</strong> (<strong class="bold">ADAS</strong>). This covers everything from cruise control to several safety features and fully autonomous driving that you are all probably familiar with. If you are not familiar with these concepts, we encourage you to take the following crash course – test drive a car with fully autonomous capabilities to appreciate the technology and sophistication involved in building these kinds of systems. Companies that are currently heavily investing in AV and ADAS systems require heavy computational resources to test, simulate, and develop related technologies before deploying them in their cars. Many companies are turning to the cloud when there is a need for on-demand, elastic compute for these large-scale applications. The previous chapters have covered storage, network, and computing, and introduced ML <span class="No-Break">in general.</span></p>
			<p>In this chapter, we will broadly cover what AV systems and ADAS are, and how AWS compute and ML services help with the design and deployment of AV/ADAS architectures. Specifically, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Introducing <span class="No-Break">AV systems</span></li>
				<li>AWS services supporting <span class="No-Break">AV systems</span></li>
				<li>Designing an architecture for <span class="No-Break">AV systems</span></li>
				<li>ML applied to <span class="No-Break">AV systems</span></li>
			</ul>
			<p>By the end of this chapter, you will have learned about <span class="No-Break">the following:</span></p>
			<ul>
				<li>The technology used in AVs at a <span class="No-Break">high level</span></li>
				<li>AWS services that can be used to create and test software related <span class="No-Break">to AVs</span></li>
				<li>How machine learning is used in the development <span class="No-Break">of AVs</span></li>
			</ul>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor217"/>Technical requirements</h1>
			<p>You should have the following prerequisites before getting started with <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Familiarity with AWS and its <span class="No-Break">basic usage.</span></li>
				<li>A web browser. (For the best experience, it is recommended that you use a Chrome or <span class="No-Break">Firefox browser.)</span></li>
				<li>An AWS account. (If you are unfamiliar with how to get started with an AWS account, you can go to this <span class="No-Break">link: </span><a href="https://aws.amazon.com/getting-started/"><span class="No-Break">https://aws.amazon.com/getting-started/</span></a><span class="No-Break">.)</span></li>
			</ul>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor218"/>Introducing AV systems</h1>
			<p>As we mentioned earlier, several automotive companies are already implementing ADAS and AV systems in their vehicles. As such, there is a large amount of research and development happening in the space, but<a id="_idIndexMarker883"/> this section will introduce you to key terms and concepts so we can proceed further and explore how machine learning is <span class="No-Break">involved here.</span></p>
			<p>First, let us discuss ADAS and AV at a high level. There are several threads of questions from people being introduced to this field leading to confusion, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Are AV systems and ADAS one and the <span class="No-Break">same thing?</span></li>
				<li>Are ADAS contained within <span class="No-Break">AV systems?</span></li>
				<li>Does a company usually develop ADAS first and then <span class="No-Break">AV systems?</span></li>
				<li>Are there different levels of automation within <span class="No-Break">AV systems?</span></li>
			</ul>
			<p>Before we can answer these questions, let’s drill down even further. In order to automate any part of the driving experience, whether it is for a car or a container truck, innovation in the following components <span class="No-Break">becomes necessary:</span></p>
			<ul>
				<li>Driver assistance or <span class="No-Break">self-driving hardware</span></li>
				<li>Driver assistance or <span class="No-Break">self-driving software</span></li>
				<li>Data and <span class="No-Break">compute services</span></li>
			</ul>
			<p>The first step in adding these technologies is adding the right hardware. Usually, this comprises a<a id="_idIndexMarker884"/> combination of RADAR, LiDAR, and <span class="No-Break"><strong class="bold">Camera sensors</strong></span><span class="No-Break">.</span></p>
			<ul>
				<li><strong class="bold">Radio Detection and Ranging</strong>, or <strong class="bold">RADAR</strong>, uses radio waves to estimate the distance and<a id="_idIndexMarker885"/> velocity of objects<a id="_idIndexMarker886"/> around it. RADARs are classified based on their range as short-range, mid-range, and long-range. Shorter-range RADARs are used for functions such as parking distance assistance and blind spot monitoring. Longer-range RADARs are used for lane following, automatic braking, and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">Light Detection and Ranging</strong>, or <strong class="bold">LiDAR</strong>, is similar to RADAR but uses the reflection of light <a id="_idIndexMarker887"/>from surfaces to determine the distance to objects. High-resolution LiDAR can also be used to <a id="_idIndexMarker888"/>determine the shape of objects along with <strong class="bold">Deep Learning</strong> (<strong class="bold">DL</strong>) algorithms, as<a id="_idIndexMarker889"/> we will learn later in <span class="No-Break">this chapter.</span></li>
				<li>Lastly, cameras are placed around the vehicles for low-level tasks such as parking, as well as high-level ones such as fully autonomous driving (when the right DL algorithms are used along with a full camera array). Elon Musk, the CEO of Tesla, a self-driving car company with a large number of cars currently sold with self-driving capabilities, famously preferred Tesla cars to be designed with camera systems only – no RADAR or LiDAR – since he said, humans only depend on vision to drive. The following figure shows an older systems architecture for Tesla, which now heavily depends on computer vision-based systems rather <span class="No-Break">than LiDAR.</span></li>
			</ul>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B18493_13_001.jpg" alt="Figure 13.1 – Camera, RADAR, and LiDAR sensors mounted on a self-driving vehicle. ﻿"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Camera, RADAR, and LiDAR sensors mounted on a self-driving vehicle. </p>
			<p>On a typical car chassis, you can imagine these sensors mounted, as seen in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.1</em>. As we can see, there are several cameras, RADAR, and LiDAR sensors that are used to achieve various levels of autonomous driving for vehicles. <em class="italic">Now, what are these levels of </em><span class="No-Break"><em class="italic">autonomous driving?</em></span></p>
			<p>Along with some of the <a id="_idIndexMarker890"/>hardware (sensors) defined previously, a complex software stack is required to build and maintain the features required for AV development. These features are categorized<a id="_idIndexMarker891"/> by the <strong class="bold">Society of Automotive Engineers</strong> (<strong class="bold">SAE</strong>) and are widely adopted. The SAE defined (in 2014 and later revised) five levels of autonomous driving (see <span class="No-Break">here: </span><a href="https://www.sae.org/blog/sae-j3016-update"><span class="No-Break">https://www.sae.org/blog/sae-j3016-update</span></a><span class="No-Break">):</span></p>
			<ul>
				<li><strong class="bold">Level 0</strong>: Features that <a id="_idIndexMarker892"/>provide warnings and limited assistance, for example, automatic emergency braking, blind spot warnings, and lane <span class="No-Break">departure warnings.</span></li>
				<li><strong class="bold">Level 1 – Basic driver assistance</strong>: The driver can take their feet off the pedals but needs to keep their hands on the wheel to take over, for example, lane centering or cruise control. Note that cruise control can be <em class="italic">adaptive</em> as well, maintaining a safe <a id="_idIndexMarker893"/>distance from the vehicle in front <span class="No-Break">of you.</span></li>
				<li><strong class="bold">Level 2 – Limited automation</strong>: The system controls steering, braking, and driving, but in limited scenarios. However, the driver must be ready to take over as needed to <span class="No-Break">maintain safety.</span></li>
				<li><strong class="bold">Level 3 – Low-level automation</strong>: The system can navigate in a greater number of circumstances, such as driving in traffic in addition to highway driving. Drivers are still required to take over driving in <span class="No-Break">certain situations.</span></li>
				<li><strong class="bold">Level 4 – High-level automation</strong>: The system controls the car in all situations, and drivers are only required to take over rarely when unknown situations are encountered. The technology is aimed to be used in driverless taxis and trucks, with the driver still present, to reduce workload <span class="No-Break">and fatigue.</span></li>
				<li><strong class="bold">Level 5 – Full automation</strong>: The system can handle all situations of driving, and a driver is not<a id="_idIndexMarker894"/> required to <span class="No-Break">be present.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">To clarify one of the questions posed at the beginning of this chapter, ADAS may be part of a larger AV system or used standalone. They are primarily focused on lower-level automation tasks and driver aids such as adaptive cruise control or driver alertness <span class="No-Break">warning systems.</span></p>
			<p>The following figure shows the SAE definitions for levels <span class="No-Break">of automation:</span></p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/B18493_13_002.jpg" alt="Figure 13.2 – SAE levels of automation. Source: https://www.sae.org/blog/sae-j3016-update"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – SAE levels of automation. Source: <a href="https://www.sae.org/blog/sae-j3016-update">https://www.sae.org/blog/sae-j3016-update</a></p>
			<p>In this section, we <a id="_idIndexMarker895"/>have introduced basic concepts around AV systems at a high level, including hardware and software components required in the development of these systems. In the next section, we will take a look at AWS services that support the development of <span class="No-Break">AV systems.</span></p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor219"/>AWS services supporting AV systems</h1>
			<p>The development <a id="_idIndexMarker896"/>and testing of AV systems and ADAS require a cloud platform with highly scalable compute, storage, and networking. Being HPC applications, these components were covered in detail in previous chapters. As a recap, we have covered the following topics that are still relevant in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li><a href="B18493_03.xhtml#_idTextAnchor050"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Compute and Networking</em> (see topics on architectural patterns and <span class="No-Break">compute instances)</span></li>
				<li><a href="B18493_04.xhtml#_idTextAnchor074"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Data Storage</em> (see topics on Amazon S3 and FSx <span class="No-Break">for Lustre)</span></li>
				<li><a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Data Analysis and Preprocessing</em> (see topics on large-scale <span class="No-Break">data processing)</span></li>
				<li><em class="italic">Chapters 6</em> to <em class="italic">9</em> (covering distributed training and deployment on the cloud and at <span class="No-Break">the edge)</span></li>
			</ul>
			<p>In this section, we will <a id="_idIndexMarker897"/>highlight some more services, including the ones that we discussed in the context of AV and ADAS development. A single autonomous vehicle can generate several TB of data per day. This data is used across the AV development workflow that is discussed at the following link (<a href="https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/">https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/</a>) and includes <span class="No-Break">the following:</span></p>
			<ul>
				<li>Data acquisition <span class="No-Break">and ingestion</span></li>
				<li>Data processing <span class="No-Break">and analytics</span></li>
				<li><span class="No-Break">Labeling</span></li>
				<li><span class="No-Break">Map development</span></li>
				<li>Model and <span class="No-Break">algorithm development</span></li>
				<li><span class="No-Break">Simulation</span></li>
				<li>Verification <span class="No-Break">and validation</span></li>
				<li>Deployment <span class="No-Break">and orchestration</span></li>
			</ul>
			<p>With ever-expanding data sizes, customers look for scalable, virtually unlimited-capacity data stores at the center of all the preceding activities. For example, Lyft Level 5 manages <strong class="bold">Petabytes</strong> (<strong class="bold">PB</strong>) of storage<a id="_idIndexMarker898"/> data from car sensors in <strong class="bold">Amazon S3</strong>. Amazon S3 is also central to the concept <a id="_idIndexMarker899"/>of building a <strong class="bold">data lake</strong> for applications in the AV space, which will be discussed in the <span class="No-Break">next section.</span></p>
			<p><em class="italic">But how do customers get this data into Amazon S3 in the </em><span class="No-Break"><em class="italic">first place?</em></span></p>
			<p>Customers have several options for this. In the AV space, the customer uses the <strong class="bold">Snow</strong> family of devices (<strong class="bold">SnowBall</strong>, <strong class="bold">SnowCone</strong>, and <strong class="bold">SnowMobile</strong>) to transfer up to PB of data to Amazon S3. Customers with on-prem systems can also use <strong class="bold">Amazon Outposts</strong> to temporarily host and process this data with APIs similar to the cloud and also use <strong class="bold">Amazon Direct Connect</strong> to securely<a id="_idIndexMarker900"/> transfer data to Amazon S3 using a dedicated network connection. Several use cases highlighted here are from public references of companies using AWS for actual AV development (see the <em class="italic">References</em> section for <span class="No-Break">more information).</span></p>
			<p>Customers can also use <strong class="bold">AWS IoT FleetWise</strong>, a service<a id="_idIndexMarker901"/> that, at the time of writing, is still under preview, to easily collect and transfer data from vehicles to the cloud in near real time. With IoT<a id="_idIndexMarker902"/> FleetWise, customers first model the vehicle sensors using the FleetWise designer, then<a id="_idIndexMarker903"/> they install the <strong class="bold">IoT FleetWise Edge Agent</strong> onto the compatible edge devices that are running on the vehicle, define data schemas and conditions to collect the data, and stream this data to <strong class="bold">Amazon Timestream</strong> or <a id="_idIndexMarker904"/><span class="No-Break">Amazon S3.</span></p>
			<p>The data collected can be raw or processed sensor data, image, audio, video, RADAR, or LiDAR data. Once this data is on Amazon S3, it can be processed, analyzed, and labeled before using it in downstream tasks. Several customers, such as TuSimple, use Amazon <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) to do various<a id="_idIndexMarker905"/> AV-related processing tasks. Customers trying to optimize the cost of processing with this scale of data use EC2 Spot Instances extensively. In 2020, Lyft Level 5 reported that more than 75% of their computing fleet was on EC2 Spot Instances to reduce the cost of operation (see the <em class="italic">References</em> section for a link to the <span class="No-Break">use case).</span></p>
			<p>For image- and video-based preprocessing workloads, several pre-trained ML models can be used but require access to GPU-based instances. Toyota Research Institute, for example, extensively uses P3 and P4 instances for highly scalable and performant cloud applications. Companies such as Momenta also use Amazon EMR to create analytics services such as safe driving assistance <span class="No-Break">decision services.</span></p>
			<p>For labeling data (primarily image, video, and 3D LiDAR data), customers use <strong class="bold">Amazon SageMaker Ground Truth</strong> on AWS, which <a id="_idIndexMarker906"/>provides specialized templates for these labeling use cases, access to private, vendor, or public labeling worker pools, and several assistive labeling capabilities to speed up these time-intensive tasks and <span class="No-Break">reduce costs.</span></p>
			<p>Customers also use pipelines to orchestrate end-to-end preprocessing, training, or post-processing workflows. A service that can help create, manage, and run these pipelines at scale is Amazon <strong class="bold">Managed Workflows for Apache Airflow</strong> or <strong class="bold">MWAA</strong>. MWAA is a managed <a id="_idIndexMarker907"/>orchestration service for Apache Airflow that makes it very simple to set up and operate end-to-end data pipelines in the cloud at scale. Alternatives to using MWAA include AWS services such as <strong class="bold">Step Functions</strong> and <strong class="bold">Amazon </strong><span class="No-Break"><strong class="bold">SageMaker Pipelines</strong></span><span class="No-Break">.</span></p>
			<p>Model training, simulation, model compilation, verification, and validation workflows can make use of <strong class="bold">Amazon EC2</strong>, or one of the following managed services – <strong class="bold">Amazon SageMaker</strong>, <strong class="bold">Amazon Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>), <strong class="bold">Amazon Elastic Container Service</strong> (<strong class="bold">ECS</strong>), and/or <span class="No-Break"><strong class="bold">AWS Batch</strong></span><span class="No-Break">.</span></p>
			<p>In the next section, we<a id="_idIndexMarker908"/> will discuss a reference architecture for AV development on AWS that brings together many of <span class="No-Break">these services.</span></p>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor220"/>Designing an architecture for AV systems</h1>
			<p>In this section, we will be<a id="_idIndexMarker909"/> discussing a reference architecture published by AWS called the <em class="italic">Autonomous Driving Data Lake Reference Architecture</em>, a link to which can be found in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section.</span></p>
			<p>The complete architecture is replicated in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/B18493_13_003.jpg" alt="Figure 13.3 – Autonomous Driving Data Lake Reference Architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Autonomous Driving Data Lake Reference Architecture</p>
			<p>In this section, we will zoom into parts of this architecture to discuss it in further detail. Let’s start with <span class="No-Break">data ingestion:</span></p>
			<ul>
				<li><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.4</em> shows how<a id="_idIndexMarker910"/> cars may be installed with a data logger or some removable storage media that stores data from sensors. Custom hardware or AWS Outposts can be used to process data<a id="_idIndexMarker911"/> that is stored from one or more trips. For near real time, <strong class="bold">AWS IoT core</strong> can be used along with <strong class="bold">Amazon Kinesis Firehose</strong> to deliver data to<a id="_idIndexMarker912"/> Amazon S3. Customers can also use Amazon Direct Connect, as mentioned earlier in this chapter, for secure and fast data transfer to <span class="No-Break">Amazon S3.</span></li>
			</ul>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/B18493_13_004.jpg" alt="Figure 13.4 – Data ingestion – steps 1 and 2"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Data ingestion – steps 1 and 2</p>
			<ul>
				<li><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.5</em> shows how <strong class="bold">Amazon EMR</strong> can be<a id="_idIndexMarker913"/> used to filter incoming raw data. For example, the<a id="_idIndexMarker914"/> quality of data can be assessed using custom PySpark code and dropped into <span class="No-Break">different buckets.</span></li>
			</ul>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/B18493_13_005.jpg" alt="Figure 13.5 – Initial data processing – step 3"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Initial data processing – step 3</p>
			<ul>
				<li><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.6</em> shows <em class="italic">step 5</em> and <em class="italic">step 6</em>, where data is further cleaned and enriched (for example, with location-specific or weather data). Given the large amount of data being<a id="_idIndexMarker915"/> collected, another processing step in Amazon EMR can be used to identify interesting scenes for downstream steps such <span class="No-Break">as ML.</span></li>
			</ul>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B18493_13_006.jpg" alt="Figure 13.6 – Data enrichment – steps 5 and 6"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – Data enrichment – steps 5 and 6</p>
			<ul>
				<li><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.7</em> shows how Amazon MWAA can be used to orchestrate these end-to-end data processing workflows (<em class="italic">step 4</em>). Data generated in many intermediate steps can be<a id="_idIndexMarker916"/> cataloged in <strong class="bold">AWS Glue Data Catalog</strong>, and the lineage of this<a id="_idIndexMarker917"/> data can be stored as a graph in <strong class="bold">Amazon Neptune</strong> (<span class="No-Break"><em class="italic">step 7</em></span><span class="No-Break">).</span></li>
				<li>Finally, data can be preprocessed for visualization tools in Fargate tasks, with <strong class="bold">AWS AppSync</strong> and <strong class="bold">Amazon QuickSight</strong> providing visualization and KPI <span class="No-Break">reporting capabilities.</span></li>
			</ul>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B18493_13_007.jpg" alt="Figure 13.7 – Workflow Orchestration, Data Catalog and Visualization Tools – steps 4, 7, and 10, respectively"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7 – Workflow Orchestration, Data Catalog and Visualization Tools – steps 4, 7, and 10, respectively</p>
			<ul>
				<li><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.8</em> shows how <strong class="bold">Amazon Fargate</strong> tasks can<a id="_idIndexMarker918"/> be used to extract images or sequences of images from videos, with <strong class="bold">AWS Lambda</strong> functions used with <a id="_idIndexMarker919"/>pre-trained models or with the help of <strong class="bold">Amazon Rekognition</strong> to blur and anonymize parts of images such as faces or license plates. For AV customers, further pre-labeling can be done where pre-trained models available in open source can be used<a id="_idIndexMarker920"/> to identify pedestrians, cars, trucks, road signs, and <span class="No-Break">so on.</span></li>
			</ul>
			<p>This helps the labeling process go faster since most of the labeling effort is only to adjust existing labels, compared to creating all labels (such as bounding boxes) from scratch. This high-quality labeled data is the most important step when creating ML-powered ADAS and AV systems, which can include multiple models. More details on ML for AV systems will be discussed in the <span class="No-Break">next section.</span></p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B18493_13_008.jpg" alt="Figure 13.8 – Labeling and ML on labeled data – steps 8 and 9"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – Labeling and ML on labeled data – steps 8 and 9</p>
			<p>In the next section, we will <a id="_idIndexMarker921"/>discuss how ML is applied to AV systems and <span class="No-Break">use cases.</span></p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor221"/>ML applied to AV systems</h1>
			<p>Developing highly <a id="_idIndexMarker922"/>sophisticated <strong class="bold">Deep Neural Networks</strong> (<strong class="bold">DNNs</strong>) with the ability to safely operate an AV is a<a id="_idIndexMarker923"/> highly complex technical challenge. Practitioners require PB of real-world sensor data, hundreds <a id="_idIndexMarker924"/>of thousands, if not millions, of <strong class="bold">virtual Central Processing Unit</strong> (<strong class="bold">vCPU</strong>) hours, and thousands of accelerator <a id="_idIndexMarker925"/>chips or <strong class="bold">Graphics Processing Unit</strong> (<strong class="bold">GPU</strong>) hours to train these DNNs (also called <em class="italic">models</em> or <em class="italic">algorithms</em>). The end goal is to ensure these models can operate a vehicle autonomously safer than a <span class="No-Break">human driver.</span></p>
			<p>In this section, we’ll talk about what is involved in developing models relevant to end-to-end AV/ADAS development workflows <span class="No-Break">on AWS.</span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor222"/>Model development</h2>
			<p>AVs typically<a id="_idIndexMarker926"/> operate through five key processes, each of which may involve ML to <span class="No-Break">various degrees:</span></p>
			<ul>
				<li>Localization <span class="No-Break">and mapping</span></li>
				<li><span class="No-Break">Perception</span></li>
				<li><span class="No-Break">Prediction</span></li>
				<li><span class="No-Break">Planning</span></li>
				<li><span class="No-Break">Control</span></li>
			</ul>
			<p>Each of the steps also requires different supporting data and infrastructure to efficiently produce a functional model or models. For example, while the perception stack is built on top of large computer vision models requiring distributed compute infrastructure to support DL training, the <a id="_idIndexMarker927"/>control step consumes a mix of general-purpose GPU and large memory GPU cards optimized for DL, in an online or offline <strong class="bold">Reinforcement Learning</strong> (<span class="No-Break"><strong class="bold">RL</strong></span><span class="No-Break">) workflow.</span></p>
			<p>In the next section, we will <a id="_idIndexMarker928"/>explore some of the challenges that a cloud-based ML environment can overcome for successful AV development by <span class="No-Break">leveraging AWS.</span></p>
			<h3>Challenges</h3>
			<p>There are three main challenges in<a id="_idIndexMarker929"/> building DL-based <span class="No-Break">AV models:</span></p>
			<ul>
				<li>Feeding TB or more of training data to ML frameworks running on large-scale, high-performance <span class="No-Break">computing infrastructure</span></li>
				<li>Elasticity to linearly scale compute infrastructure to thousands of accelerators leveraging high <span class="No-Break">bandwidth networking</span></li>
				<li>Orchestration of training the <span class="No-Break">ML frameworks</span></li>
			</ul>
			<p>Large amounts of data also means a large number of resources needed for labeling, so let us discuss this <span class="No-Break">challenge next.</span></p>
			<h4>Labeling large amounts of data</h4>
			<p>Vehicle sensor and simulation data contain streams of images and videos, point clouds from RADAR and LiDAR, and time series data from inertia measuring sensors, GPS, and the vehicle <strong class="bold">Controller Area Network</strong> (<strong class="bold">CAN</strong>) bus. In a given test drive of 8 hours, an AV may collect more than 40 TB of data across these sensors. Depending on the company, a test fleet can<a id="_idIndexMarker930"/> range anywhere from a handful of vehicles to nearly 1,000. At such a scale, AV data lakes grow at PB annually. Altogether, this data is indexed by time and stored as scenes or scenarios leveraged for model training. For the purposes of building models and agents that ultimately drive the vehicle, the data needs to get processed <span class="No-Break">and labeled.</span></p>
			<p>The first challenge is to make these labeled datasets available to the ML framework during training as fast as it can process a data batch. The amount of data required to train one model in one <a id="_idIndexMarker931"/>specific task alone can be in excess of hundreds of TB, making pre-fetching and loading data into memory unfeasible. The combination of ML framework and compute hardware accelerator dictates the speed at which a batch of such data gets read from the source for a specific <span class="No-Break">model task.</span></p>
			<p>Today, you can use several built-in labeling templates on Amazon SageMaker Ground Truth for labeling images, video, and LiDAR data. For a description of AWS services that can be used to preprocess and label high-resolution video files recorded, visit the linked blog in the <em class="italic">References</em> section. Specifically for LiDAR use cases, you can use data that has both LiDAR as well as image data captured in sync with the LiDAR using multiple onboard cameras. Amazon SageMaker Ground Truth can synchronize a frame containing 3D point cloud data with up to eight camera sources. Once the raw data manifest is ready, you can use SageMaker Ground Truth for 3D object detection, object tracking, and semantic segmentation of 3D point clouds. As with standard SageMaker Ground Truth labeling jobs, you can use a fully private workforce or a trusted vendor to complete your labeling tasks. <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.9</em> is an example of a car being labeled using a 3D bounding box along with three projected side views and images from cameras corresponding to <span class="No-Break">that timestamp:</span></p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B18493_13_009.jpg" alt="Figure 13.9 – Amazon SageMaker Ground Truth labeling jobs with LiDAR and camera data for AV workloads"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.9 – Amazon SageMaker Ground Truth labeling jobs with LiDAR and camera data for AV workloads</p>
			<p>For more information on<a id="_idIndexMarker932"/> using Ground Truth to label 3D point cloud data, see the link in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section.</span></p>
			<h4>Training with large amounts of data</h4>
			<p>Typically, DL tasks in the context of AV can be related to <em class="italic">perception</em> (finding information about the environment or obstacles) and <em class="italic">localization</em> (finding your position in the world with high accuracy). Several state-of-the-art DL models are being developed for detecting other vehicles, roads, pedestrians, signs, and objects and also to describe the position of these 2D objects around the vehicle in the 3D world. The <em class="italic">KITTI</em> <em class="italic">Benchmark</em> is often used to test new algorithms and approaches to looking at use cases related to autonomous vehicles, such as semantic segmentation and object detection. Access to the KITTI dataset (and other similar datasets such as the <em class="italic">Audi A2D2 Autonomous driving dataset</em>) can be found on the Registry of Open Data <span class="No-Break">on AWS.</span></p>
			<p>Training large semantic segmentation <a id="_idIndexMarker933"/>and object detection <a id="_idIndexMarker934"/>models on large open source datasets, such as <strong class="bold">Mask R-CNN</strong> on the <strong class="bold">Common Objects in Context</strong> (<strong class="bold">COCO</strong>) dataset, can achieve throughputs of 60 images per second – approximately 35 MB/s – on a single multi-GPU instance. For simpler architectures, training throughput can reach thousands of images per second due to the smaller scale of the network being trained on more straightforward tasks. That is true in the case of image classification using <em class="italic">ResNet</em> models that make up the backbone of larger models such as <em class="italic">Mask R-CNN</em>. More recently, models such as <em class="italic">DeepManta</em> have been used to obtain high scores on other related tasks such as vehicle pose estimation – links to these methods and papers can be found in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section.</span></p>
			<p>At lower rates of data transfer, the training job can retrieve data objects directly from Amazon S3. Amazon S3 is a foundational data service for AV development on AWS, as discussed in this blog post on <em class="italic">Building an Autonomous Driving Data Lake</em> (see the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section).</span></p>
			<p>Some data loaders provide <a id="_idIndexMarker935"/>connectivity directly to Amazon S3, such as TensorFlow, for TFRecord-style datasets. However, it requires optimizing the size of each data file as well as the number of worker processes to maximize Amazon S3 throughput, which can make the data loading pipeline complex and less scalable. It is possible to achieve hundreds of GB/s total throughputs reading directly from S3 when horizontally scaling the data reader, but there is a compromise on CPU utilization for the training process itself. This blog (<a href="https://aws.amazon.com/blogs/machine-learning/optimizing-i-o-for-gpu-performance-tuning-of-deep-learning-training-in-amazon-sagemaker/">https://aws.amazon.com/blogs/machine-learning/optimizing-i-o-for-gpu-performance-tuning-of-deep-learning-training-in-amazon-sagemaker/</a>) explains how to optimize I/O for GPU performance. Mobileye explained their use of TFRecord datasets and Pipe mode on Amazon SageMaker for training their large DNNs resulting in faster training and a 10x improvement in development time in their reinvent video , which is included in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section.</span></p>
			<p>For a more straightforward architecture, which leverages the ML framework’s native support of <em class="italic">POSIX-compliant</em> file system interface, AWS offers <strong class="bold">FSx for Lustre</strong> (more information about FSx and POSIX compliance is linked to in the <em class="italic">References</em> section). FSx for Lustre is a high-throughput, low-latency distributed file system that can be provisioned from existing S3 data, making the whole dataset available to the DNN training workers as files. These files can be iterated over using any of the major ML framework data readers, such as PyTorch dataset <span class="No-Break">or DataLoader.</span></p>
			<p>FSx for Lustre can scale its baseline aggregate bandwidth to 200 GB/s for a 1 PB training dataset, with burst speeds of 1.3 GB/s per TiB of training data. The larger the provisioned FSx for Lustre deployment, the higher the aggregate bandwidth, enabling a PB-scale network fabric. FSx for Lustre is hydrated with a subset of the data from the Autonomous Driving data lake and synchronized back using data repository tasks in case model artifacts or data transformations are generated and recorded during training. For a real-world example of how the Amazon ML Solutions Lab helped Hyundai train a model using SageMaker’s distributed training library and FSx for Lustre 10x faster with only 5x the number of instances, take a look at the link in the <em class="italic">References</em> section to this <span class="No-Break">use case.</span></p>
			<p>The need for a PB-scale data repository also comes from the need to scale the number of compute workers processing this data. At the 60 images per second rate, a single worker would take more than 6.5 hours to train over the 118,000 images in the <em class="italic">COCO</em> dataset, considering a<a id="_idIndexMarker936"/> dozen epochs to achieve reasonable accuracy. Scaling the number of images per training iteration is key for achieving reasonable training times. Even more so given the experimental and iterative nature of building DL based models, requiring multiple training runs for a single model to be built. Large-scale training generally translates to high costs of running training experiments. Amazon SageMaker provides cost-saving features for both training as well <span class="No-Break">as deployment.</span></p>
			<p>Alex Bain, Lead for ML Systems at Lyft Level 5, said: “<em class="italic">By using Amazon SageMaker distributed training, we reduced our model training time from days to a couple of hours. By running our ML workloads on AWS, we streamlined our development cycles and reduced costs, ultimately accelerating our mission to deliver self-driving capabilities to </em><span class="No-Break"><em class="italic">our customers.</em></span><span class="No-Break">”</span></p>
			<p>Visit the following blog post for more examples of use cases around cost <span class="No-Break">savings: </span><a href="https://aws.amazon.com/blogs/aws/amazon-sagemaker-leads-way-in-machine-learning/"><span class="No-Break">https://aws.amazon.com/blogs/aws/amazon-sagemaker-leads-way-in-machine-learning/</span></a><span class="No-Break">.</span></p>
			<p>Amazon SageMaker provides connections to common file systems that store data, such as Amazon S3, EFS, and FSx for Lustre. When running long training jobs, the choice of an appropriate storage service can speed up training times overall. If training data is already in EFS, it is common to continue preprocessing data on EFS and training the model by pointing SageMaker to EFS. When data is in Amazon S3, customers can decide to use this data directly from S3 to make use of features on SageMaker such as Fast File mode, Pipe mode, data shuffling, or sharding by S3 key for distributed training (more information about these modes is included in the <em class="italic">References</em> section). Customers can also use FSx for Lustre since it automatically makes data available to SageMaker training instances and avoids any repetitive copying of data. When multiple epochs use slightly different subsets of data that fit into instance memory, or in the case of distributed training, FSx for Lustre provides extremely fast and consistent access to datasets by mounting a volume with the data accessible to your <span class="No-Break">training code.</span></p>
			<h4>Scaling</h4>
			<p>With distributed training strategies, many compute nodes within a cluster read batches of data, train over them, and<a id="_idIndexMarker937"/> synchronize the model parameters as the training goes on. The unit of compute for these clusters is not the individual compute<a id="_idIndexMarker938"/> instance, sometimes called a <em class="italic">node</em>, but the individual GPUs. This is because the DNNs require hardware acceleration for training. So, distribution occurs within and across multi-GPU <span class="No-Break">compute instances.</span></p>
			<p>Amazon’s EC2 service provides the broadest compute platform in the cloud with 17 distinct compute instance families. Each family is designed for a few specific workloads and consists of a given ratio of vCPU, GPU (for certain instances), memory, storage, and networking. For full, end-to-end AV development, companies largely rely on the C, M, R, G, and P <span class="No-Break">instance families.</span></p>
			<p>For ML model training, companies<a id="_idIndexMarker939"/> leverage the <strong class="bold">Deep Learning Amazon Machine Images</strong> (<strong class="bold">DLAMI</strong>) to launch NVIDIA GPU-based EC2 instances in the <em class="italic">P family</em>. Each EC2 P family instance generation integrates the latest NVIDIA technology, including the p2 instances (Tesla K80) and the p3 instances (Volta V100), and the recently released p4d (with Ampere <span class="No-Break">A100 GPUs).</span></p>
			<p>AWS and NVIDIA continue to collaborate on achieving state-of-the-art model training times for data-parallel and model-parallel training (see the blog link in the <em class="italic">References</em> section). SageMaker distributed training includes libraries for distributed data-parallel and model-parallel modes of training. These libraries for data and model-parallel training extend SageMaker’s training capabilities, so you can train large-scale models with small code changes to your training scripts. For readers interested in this topic, the following video from the AWS Deep Engines team is on an AWS library that is useful for both data- and model-parallel <span class="No-Break">training: </span><a href="https://youtu.be/nz1EwsS5OiA"><span class="No-Break">https://youtu.be/nz1EwsS5OiA</span></a><span class="No-Break">.</span></p>
			<p>At the single GPU level, optimizing the memory consumption helps increase the throughput. For model training on data that can be batched, this means increasing the number of images per iteration before running out of GPU memory. Therefore, the higher the GPU memory, the greater the achievable training throughput, which favors large memory <span class="No-Break">GPU nodes.</span></p>
			<p>Across GPUs, fast communication within and between instances enables faster synchronization of gradients during training. Networking is, therefore, a key aspect of scalability in enhancing the speed of each iteration step. This type of infrastructure is analogous to a tightly coupled <span class="No-Break">HPC infrastructure.</span></p>
			<p>AWS offers EC2 instances<a id="_idIndexMarker940"/> that support HPC and accelerated computing on the cloud. AWS has demonstrated the fastest training times for models such as Mask R-CNN and near linear scalability of large-scale GPU instances using the EC2 <strong class="bold">p3dn.24xlarge</strong> instance. This instance has 8 NVIDIA V100 GPUs, with 32 GB of memory each, and <a id="_idIndexMarker941"/>can make use of the <strong class="bold">AWS Elastic Fabric Adapter</strong> (<strong class="bold">EFA</strong>) network interface. EFA is a custom-built OS bypass hardware interface that enhances the performance of inter-instance communication, achieving 100 gigabits per second bandwidth per card and natively integrating with communication<a id="_idIndexMarker942"/> libraries such as MPI and <strong class="bold">NVIDIA Collective Communication Library</strong> (<strong class="bold">NCCL</strong>) used on <span class="No-Break">ML applications.</span></p>
			<p>AWS introduced the latest generation (in 2020) of NVIDIA GPU hardware <strong class="bold">General Availability</strong> (<strong class="bold">GA</strong>) with EC2 p4d instances. This<a id="_idIndexMarker943"/> instance takes ML training in the cloud to the next level and includes 8 NVIDIA A100 GPUs with 40 GB of memory per GPU and an improved networking stack. Instead of a single network interface card, the p4d instance has 4 EFA cards for a total of 400 gigabits per second bandwidth. Within the instance, the p4d family also increases GPU-to-GPU communication bandwidth with an NVlink mesh topology for ML frameworks using NCCL. The new p4d instance design provides up to 3.8x the training throughput compared to the <strong class="bold">p3dn.24xlarge</strong> on backbone models for the major computer vision tasks, such as semantic segmentation, used in the data labeling phase of the AV development process. For more information on the p4d design and benchmark results, refer to this deep <span class="No-Break">dive blog.</span></p>
			<p>During the December 2020 AWS re:Invent conference, AWS announced plans to make Intel Habana Gaudi accelerators and an in-house built training chip, which will offer more <strong class="bold">Terraflops</strong> (<strong class="bold">TFLOPS</strong>) than any <a id="_idIndexMarker944"/>compute instance in the cloud, available. In 2020, AWS collaborated with NVIDIA to bring down training times of Mask R-CNN on the cloud to 6 minutes and 45 seconds on PyTorch and 6 minutes and 12 seconds with TensorFlow (see the link in the <em class="italic">References</em> section). For more information on data-parallel and model-parallel training with minimal code changes on Amazon SageMaker, see the documentation on SageMaker distributed training linked to in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section.</span></p>
			<h4>Orchestration</h4>
			<p>The final challenge in training AV DNNs is managing and orchestrating the tightly coupled HPC infrastructure at scale. AWS provides a suite of services and solutions for HPC that you can leverage to build and manage a large-scale DNN training cluster for AV including tools such as Amazon EKS, Amazon ECS, AWS Batch, and AWS Parallel Cluster. These topics have been discussed in the preceding chapters in detail and will not be <span class="No-Break">repeated here.</span></p>
			<p>The infrastructure management <a id="_idIndexMarker945"/>challenge also includes the ability to integrate upstream and downstream tasks from the AV stack throughout the development of the models. As an example, the validation of a perception model stack may include online driving simulations. When integrating a simulation environment into a model-building deployment, the requirements for distributed compute change from tightly coupled high-performance computing to highly parallelized, embarrassingly parallel batch simulations and client-server architectures. Efficiently integrating services becomes critically important to bringing AV systems development from a research exercise to a scalable, <span class="No-Break">production-ready pipeline.</span></p>
			<p>These options give you the flexibility to build a scalable and diverse ML platform for your data scientist team with the quickest velocity and most robust infrastructure in the world. Regardless of whether you use a managed platform for ML such as Amazon SageMaker, or manage your own platform on Kubernetes, ML deployment and orchestration specifically for AV needs to have <span class="No-Break">the following:</span></p>
			<ul>
				<li>Continuous training and <span class="No-Break">re-training functionality</span></li>
				<li><span class="No-Break">Continuous deployment</span></li>
				<li><span class="No-Break">Continuous monitoring</span></li>
			</ul>
			<p>More information<a id="_idIndexMarker946"/> about MLOps on AWS can be found <span class="No-Break">at </span><a href="https://aws.amazon.com/sagemaker/mlops/"><span class="No-Break">https://aws.amazon.com/sagemaker/mlops/</span></a><span class="No-Break">.</span></p>
			<p>For AV specifically, here is what a typical ML workflow spanning a few weeks may <span class="No-Break">look like:</span></p>
			<ol>
				<li>New or <span class="No-Break">updated datasets.</span></li>
				<li>Dataset curation and <span class="No-Break">scene selection.</span></li>
				<li>Pre-labeling and data curation, where pre-trained models are used to provide coarse quality labeled data to human labelers, and in some cases where sensitive information is obfuscated from image and <span class="No-Break">video datasets.</span></li>
				<li>Labeling and active learning (more information <span class="No-Break">at </span><a href="https://aws.amazon.com/sagemaker/data-labeling/"><span class="No-Break">https://aws.amazon.com/sagemaker/data-labeling/</span></a><span class="No-Break">).</span></li>
				<li>Distributed training for various tasks (see <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span><span class="No-Break">).</span></li>
				<li><span class="No-Break">Model testing:</span><ul><li><span class="No-Break">Software-in-the-loop testing</span></li><li><span class="No-Break">Hardware-in-the-loop testing</span></li><li><span class="No-Break">On-road testing</span></li></ul></li>
				<li>Dataset collection, and go back to <span class="No-Break"><em class="italic">step 1.</em></span></li>
			</ol>
			<p>For more information <a id="_idIndexMarker947"/>about these steps, you may be interested in reading about how an actual customer of AWS, Aurora, achieves self-driving capabilities using their Aurora Driver platform, at <a href="https://www.cnet.com/roadshow/news/aurora-drive-aws-autonomous-vehicle-development/">https://www.cnet.com/roadshow/news/aurora-drive-aws-autonomous-vehicle-development/</a> <span class="No-Break">and </span><a href="https://www.youtube.com/watch?v=WAELZY_TJ04"><span class="No-Break">https://www.youtube.com/watch?v=WAELZY_TJ04</span></a><span class="No-Break">.</span></p>
			<p><a href="B18493_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Distributed Training of Machine Learning Models</em>, discussed distributed training, and <a href="B18493_08.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Optimizing and Managing Machine Learning Models for Edge Deployment</em>, discussed model deployment at the edge; these are both very relevant to the topics listed in the previous steps (<em class="italic">steps 5–9</em>). Within model testing, software-in-the-loop testing can be done using tools on AWS, and this will be discussed in the next section using a hands-on example that you can follow along on your <span class="No-Break">AWS account.</span></p>
			<h4>Software-in-the-loop (SITL) simulation</h4>
			<p>In this section, we will be discussing one very specific type of <strong class="bold">Software-in-the-Loop </strong>(<strong class="bold">SITL</strong>) testing that is useful for AV customers. Note that this is not the only type of simulation that is being run by<a id="_idIndexMarker948"/> AV customers around the world today. Some may involve perception tasks, planning or mapping tasks, and also end-to-end <a id="_idIndexMarker949"/>software tasks before moving on to <strong class="bold">Hardware-in-the-Loop</strong> (<strong class="bold">HITL</strong>) or <span class="No-Break">on-road testing.</span></p>
			<p>In this section, we will walk through how you can set up a high-fidelity simulation of a driving environment and even test out some DL models for AV within the simulation environment! To do this, you have to follow two <span class="No-Break">high-level steps:</span></p>
			<ol>
				<li value="1">Create a container with your <span class="No-Break">simulation environment.</span></li>
				<li>Use RoboMaker to run <span class="No-Break">your simulation.</span></li>
			</ol>
			<p>Once you set this up, you<a id="_idIndexMarker950"/> can interactively work with your simulation, use the environment as part of a RL loop, or even generate synthetic data for your future <span class="No-Break">ML experiments.</span></p>
			<p>Before we walk through the steps, here are <span class="No-Break">some basics:</span></p>
			<ul>
				<li><em class="italic">What is AWS RoboMaker?</em> <strong class="bold">RoboMaker</strong> is a cloud-based simulation service where you can run <a id="_idIndexMarker951"/>your simulations without managing any infrastructure. More information can be found at <a href="https://aws.amazon.com/robomaker/">https://aws.amazon.com/robomaker/</a>. RoboMaker provides GPU-based compute for high-fidelity simulators discussed in the <span class="No-Break">following point.</span></li>
				<li>Specific to AV and manufacturing-related simulations, RoboMaker lets you use simulators such as CARLA (which we will be using in this section), AirSim, Aslan, Summit, DeepDrive or <em class="italic">Ignition</em>, Drake or NVIDIA Isaac Sim, and even custom simulations that you create using high-fidelity rendering engines such as Unity or Unreal Engine. Links to these simulators and tools can be found in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section.</span></li>
				<li><strong class="bold">CARLA</strong> is an open source <a id="_idIndexMarker952"/>project that is commonly used in AV studies for simulating vehicles in environments and testing out DL or RL models for AV. CARLA exposes an API that lets users control all aspects of the simulation, such as driving, environment, traffic, and pedestrians, and also lets users configure sensors on vehicles, such as LiDARs, cameras, <span class="No-Break">and GPS.</span></li>
			</ul>
			<p>Great, let us now get started with the steps required to run CARLA <span class="No-Break">on RoboMaker!</span></p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor223"/>Step 1 – build and push the CARLA container to Amazon ECR</h2>
			<p>To build our <a id="_idIndexMarker953"/>custom simulation container, we need <span class="No-Break">two files:</span></p>
			<ul>
				<li><span class="No-Break">A Dockerfile</span></li>
				<li>A shell script to build and push the customer’s Docker container to ECR (you can use your own pipeline to do this step <span class="No-Break">as well)</span></li>
			</ul>
			<p>Here, we will be building the container on a SageMaker notebook instance, but you may use the same script from your local laptop or an EC2 instance provided you have the right permissions set up. To demonstrate this step, we will assume that you’re aware of <span class="No-Break">SageMaker notebooks.</span></p>
			<p>Here is what the Dockerfile looks like for CARLA when viewed from a notebook instance. The <strong class="source-inline">writefile</strong> command writes the following code into a new file <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">Dockerfile</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
%%writefile Dockerfile
FROM carlasim/carla:0.9.11
USER root
# see https://github.com/NVIDIA/nvidia-docker/issues/1632
RUN rm /etc/apt/sources.list.d/cuda.list
RUN rm /etc/apt/sources.list.d/nvidia-ml.list
# install dependencies
RUN apt update &amp;&amp; \
      apt install -y python3-pip \
      libjpeg-dev \
      libtiff5-dev \
      libomp-dev \
      fontconfig
# fix ALSA errors
RUN echo pcm.!default { type plug slave.pcm "null" } &gt;&gt; /etc/asound.conf
# install NICE DCV (for RoboMaker)
RUN apt update -y &amp;&amp; apt upgrade -y &amp;&amp; apt install -y wget pgp
RUN wget https://d1uj6qtbmh3dt5.cloudfront.net/NICE-GPG-KEY
RUN gpg --import NICE-GPG-KEY
RUN wget https://d1uj6qtbmh3dt5.cloudfront.net/2021.1/Servers/nice-dcv-2021.1-10598-ubuntu1804-x86_64.tgz
RUN tar -xvzf nice-dcv-2021.1-10598-ubuntu1804-x86_64.tgz
RUN apt update &amp;&amp; apt install -y ./nice-dcv-2021.1-10598-ubuntu1804-x86_64/nice-dcv-gl_2021.1.937-1_amd64.ubuntu1804.deb \
                                 ./nice-dcv-2021.1-10598-ubuntu1804-x86_64/nice-dcv-gltest_2021.1.275-1_amd64.ubuntu1804.deb
# install opengl
RUN apt update &amp;&amp; apt install -y libglfw3 libglfw3-dev
# install xterm
RUN apt update &amp;&amp; apt install -y xterm
# run as user carla
USER carla
# install example dependencies
RUN python3 -m pip install -U pip
RUN cd ~/PythonAPI/examples &amp;&amp; python3 -m pip install -r requirements.txt
# set path to carla python API
ENV PYTHONPATH=/home/carla/PythonAPI/carla/dist/carla-0.9.11-py3.7-linux-x86_64.egg
ENTRYPOINT ["/bin/bash", "-c"]</pre>
			<p>As you can see, we <a id="_idIndexMarker954"/>start off with the CARLA base image and install some <span class="No-Break">additional dependencies:</span></p>
			<pre class="source-code">
FROM carlasim/carla:0.9.11</pre>
			<p>The second file that is required is a script to build and push the container. First, we define the arguments (the name of the container) and some inputs to the script, such as region, account, and the full name of the Docker container to <span class="No-Break">be built:</span></p>
			<pre class="source-code">
%%sh
# The name of our algorithm
algorithm_name=$1
account=$(aws sts get-caller-identity --query Account --output text)
# Get the region defined in the current configuration (default to us-west-2 if none defined)
region=$(aws configure get region)
fullname="${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest"</pre>
			<p>Then, we create <a id="_idIndexMarker955"/>the repository and log <span class="No-Break">into it:</span></p>
			<pre class="source-code">
# If the repository doesn't exist in ECR, create it.
aws ecr describe-repositories --repository-names "${algorithm_name}" &gt; /dev/null 2&gt;&amp;1
if [ $? -ne 0 ]
then
    aws ecr create-repository --repository-name "${algorithm_name}" &gt; /dev/null
fi
# Get the login command from ECR and execute it directly
aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}</pre>
			<p>Finally, we build and <a id="_idIndexMarker956"/>push the container <span class="No-Break">to ECR:</span></p>
			<pre class="source-code">
# Build the docker image locally with the image name and then push it to ECR
# with the full name.
docker build  -t ${algorithm_name} .
docker tag ${algorithm_name} ${fullname}
docker push ${fullname}
echo ${fullname}</pre>
			<p>This shell script takes the name of your container as an argument, builds the container based on a local Dockerfile, and pushes the container to a repository in ECR. Once you have these two scripts on a notebook instance or anywhere with Docker installed, you can run <span class="No-Break">the following:</span></p>
			<pre class="source-code">
./build_and_push.sh carlasim</pre>
			<p>This will output the location of the container you just built, similar to <span class="No-Break">the following:</span></p>
			<pre class="source-code">
&lt;account_number&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com/carlasim:latest</pre>
			<p>Copy this output, as you will need it in <em class="italic">Step 2 – configure and run CARLA </em><span class="No-Break"><em class="italic">on RoboMaker</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor224"/>Step 2 – configure and run CARLA on RoboMaker</h2>
			<p>To configure and <a id="_idIndexMarker957"/>run CARLA simulations on RoboMaker, go<a id="_idIndexMarker958"/> through the <span class="No-Break">following steps:</span></p>
			<ol>
				<li value="1">Navigate to the <strong class="bold">AWS RoboMaker</strong> console on AWS – <a href="https://aws.amazon.com/robomaker/">https://aws.amazon.com/robomaker/</a> (sign in to your AWS account <span class="No-Break">if asked).</span></li>
				<li>Next, look for <strong class="bold">Simulation Jobs</strong> on the left sidebar menu and click <strong class="bold">Create simulation job</strong>. You should now see a screen similar to <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.10</em>. Leave the default simulation<a id="_idIndexMarker959"/> job duration as is (<strong class="bold">8 hours</strong>), and create a new role (see arrow) <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">carsimrole</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/B18493_13_010.jpg" alt="Figure 13.10 – Create a simulation job on AWS RoboMaker"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.10 – Create a simulation job on AWS RoboMaker</p>
			<ol>
				<li value="3">On the same screen, scroll down to the compute options and make sure you select CPU and GPU, with sliders corresponding to <strong class="bold">Simulation Unit (SU) limit</strong> and <strong class="bold">GPU Unit (GPU) limit</strong> all the way to the right (maximum), as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.11</em>. Also, browse <a id="_idIndexMarker960"/>to a known location to save the outputs of your simulation job on Amazon S3. In <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.11</em>, the<a id="_idIndexMarker961"/> location selected shows <strong class="source-inline">s3://carlasim-bucket/output</strong>; make sure you browse to a bucket that you have access to and not the one shown in <span class="No-Break">this example:</span></li>
			</ol>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/B18493_13_011.jpg" alt="Figure 13.11 – Configure compute options and output location for your simulation job"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.11 – Configure compute options and output location for your simulation job</p>
			<ol>
				<li value="4">On the <strong class="bold">Specify robot application</strong> page, select <strong class="bold">None</strong> for<a id="_idIndexMarker962"/> robot application (see <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">):</span></li>
			</ol>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/B18493_13_012.jpg" alt="Figure 13.12 – Select None for robot application"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.12 – Select None for robot application</p>
			<ol>
				<li value="5">Move on<a id="_idIndexMarker963"/> to the <strong class="bold">Specify Simulation application</strong> page, as<a id="_idIndexMarker964"/> shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/B18493_13_013.jpg" alt="Figure 13.13 – Create a new simulation application"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.13 – Create a new simulation application</p>
			<ol>
				<li value="6">Select <strong class="bold">Create new application</strong>, and <a id="_idIndexMarker965"/>add the ECR<a id="_idIndexMarker966"/> repository link you copied from <em class="italic">step 1</em>. If you already have a simulation application, you can choose it from the drop-down menu, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/B18493_13_014.jpg" alt="Figure 13.14 – Configure Simulation application by selecting it from the drop-down menu"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.14 – Configure Simulation application by selecting it from the drop-down menu</p>
			<ol>
				<li value="7">Scroll down to the <strong class="bold">Simulation application configuration</strong> section, enter the following as <a id="_idIndexMarker967"/>your launch command, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.15</em>, and remember to check the option for running with a <a id="_idIndexMarker968"/><span class="No-Break">streaming session:</span><pre class="source-code">
./CarlaUE4.sh -opengl</pre></li>
			</ol>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/B18493_13_015.jpg" alt="Figure 13.15 – Enter the launch command in the Simulation application configuration section"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.15 – Enter the launch command in the Simulation application configuration section</p>
			<ol>
				<li value="8">In the <strong class="bold">Simulation application tools</strong> section, create<a id="_idIndexMarker969"/> two terminals with the options<a id="_idIndexMarker970"/> highlighted in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<img src="image/B18493_13_016.jpg" alt="Figure 13.16 – Add two custom tools for terminal access"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.16 – Add two custom tools for terminal access</p>
			<ol>
				<li value="9">Finally, click <strong class="bold">Next</strong> to go to the summary screen, and then click <strong class="bold">Create</strong>. This process of creating your<a id="_idIndexMarker971"/> simulation environment will take<a id="_idIndexMarker972"/> a few minutes. Once it is created, you should see your simulation job was created, as seen in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.17</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="image/B18493_13_017.jpg" alt="Figure 13.17 – Wait for the simulation job to be created"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.17 – Wait for the simulation job to be created</p>
			<ol>
				<li value="10">Once the simulation is<a id="_idIndexMarker973"/> created and the <strong class="bold">Status</strong> field says <strong class="bold">Running</strong>, click the <strong class="bold">Connect</strong> button in the <strong class="bold">Simulation application</strong> section, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.18</em>. You can also <a id="_idIndexMarker974"/>access the terminal for running scripts or monitoring the <span class="No-Break">simulation environment.</span></li>
			</ol>
			<div>
				<div id="_idContainer209" class="IMG---Figure">
					<img src="image/B18493_13_018.jpg" alt="Figure 13.18 – Connect to the main simulation application or the terminals created"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.18 – Connect to the main simulation application or the terminals created</p>
			<ol>
				<li value="11">Click <strong class="bold">Connect</strong> on one of the <a id="_idIndexMarker975"/><strong class="bold">xterm</strong> applications, <strong class="bold">xterm1</strong> or <strong class="bold">xterm2</strong>, and explore the file system; you should see Python examples inside the <strong class="source-inline">PythonAPI</strong> folder, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.19</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer210" class="IMG---Figure">
					<img src="image/B18493_13_019.jpg" alt="Figure 13.19 – CARLA Python examples inside the simulation job"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.19 – CARLA Python examples inside the simulation job</p>
			<p>Visit the getting started <a id="_idIndexMarker976"/>guide to understand<a id="_idIndexMarker977"/> the Python API as well as included examples (<a href="https://carla.readthedocs.io/en/0.9.2/getting_started/">https://carla.readthedocs.io/en/0.9.2/getting_started/</a>). Some examples are provided in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.20</em> to <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.22</em></span><span class="No-Break">.</span></p>
			<ul>
				<li><em class="italic">Figure 13.20</em> shows a sample application where you can manually drive a Tesla Cybertruck in the CARLA simulation with 264 <span class="No-Break">other vehicles:</span></li>
			</ul>
			<div>
				<div id="_idContainer211" class="IMG---Figure">
					<img src="image/B18493_13_020.jpg" alt="Figure 13.20 – Driving a Tesla Cybertruck manually in a CARLA simulation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.20 – Driving a Tesla Cybertruck manually in a CARLA simulation</p>
			<ul>
				<li><em class="italic">Figure 13.21</em> shows a sample <a id="_idIndexMarker978"/>application that <a id="_idIndexMarker979"/>uses a Python program through one of the terminal applications we created to <span class="No-Break">spawn traffic:</span></li>
			</ul>
			<div>
				<div id="_idContainer212" class="IMG---Figure">
					<img src="image/B18493_13_021.jpg" alt="Figure 13.21 – Spawning traffic in the CARLA simulation world using Python code"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.21 – Spawning traffic in the CARLA simulation world using Python code</p>
			<ul>
				<li><em class="italic">Figure 13.22</em> shows an <a id="_idIndexMarker980"/>application that <a id="_idIndexMarker981"/>simulates LiDAR data around a vehicle in <span class="No-Break">the simulation:</span></li>
			</ul>
			<div>
				<div id="_idContainer213" class="IMG---Figure">
					<img src="image/B18493_13_022.jpg" alt="Figure 13.22 – Simulated LiDAR data around the car"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.22 – Simulated LiDAR data around the car</p>
			<p>As a next step, read<a id="_idIndexMarker982"/> how you can use RL models to control your<a id="_idIndexMarker983"/> car for self-driving use cases in this <span class="No-Break">tutorial: </span><a href="https://carla.readthedocs.io/en/latest/tuto_G_rllib_integration/"><span class="No-Break">https://carla.readthedocs.io/en/latest/tuto_G_rllib_integration/</span></a></p>
			<p>Let’s summarize all that we’ve learned so far in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor225"/>Summary</h1>
			<p>In this chapter, we discussed AV and ADAS systems at a high level, along with a reference architecture to build some of these systems on AWS. We also discussed the three main challenges practitioners face when training AV-related ML models in the cloud: feeding TB or more of training data to ML frameworks running on a large-scale, high-performance computing infrastructure, elasticity to linearly scale compute infrastructure to thousands of accelerators leveraging high bandwidth networking, and orchestrating the ML <span class="No-Break">framework training.</span></p>
			<p>Lastly, we walked you through examples of how you can make use of tools on AWS to run SITL simulations for testing your <span class="No-Break">ML models.</span></p>
			<p>In the next chapter, we will focus on solving numerical optimization problems <span class="No-Break">on AWS.</span></p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor226"/>References</h1>
			<p>For more information about topics discussed in this chapter, visit the <span class="No-Break">following links:</span></p>
			<ul>
				<li><em class="italic">Autonomous Vehicle and ADAS development on AWS Part 1: Achieving </em><span class="No-Break"><em class="italic">Scale</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/industries/autonomous-vehicle-and-adas-development-on-aws-part-1-achieving-scale/"><span class="No-Break">https://aws.amazon.com/blogs/industries/autonomous-vehicle-and-adas-development-on-aws-part-1-achieving-scale/</span></a></li>
				<li><em class="italic">Building an Autonomous Driving and ADAS Data Lake on </em><span class="No-Break"><em class="italic">AWS</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/"><span class="No-Break">https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/</span></a></li>
				<li><em class="italic">Implementing Hardware-in-the-Loop for Autonomous Driving Development on </em><span class="No-Break"><em class="italic">AWS</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/architecture/field-notes-implementing-hardware-in-the-loop-for-autonomous-driving-development-on-aws/"><span class="No-Break">https://aws.amazon.com/blogs/architecture/field-notes-implementing-hardware-in-the-loop-for-autonomous-driving-development-on-aws/</span></a></li>
				<li><em class="italic">Advanced Driver Assistance Systems (</em><span class="No-Break"><em class="italic">ADAS)</em></span><span class="No-Break">: </span><a href="https://www.gartner.com/en/information-technology/glossary/advanced-driver-assistance-systems-adass"><span class="No-Break">https://www.gartner.com/en/information-technology/glossary/advanced-driver-assistance-systems-adass</span></a></li>
				<li><em class="italic">CARLA </em><span class="No-Break"><em class="italic">Documentation</em></span><span class="No-Break">: </span><a href="https://carla.readthedocs.io/en/0.9.2/getting_started/"><span class="No-Break">https://carla.readthedocs.io/en/0.9.2/getting_started/</span></a></li>
				<li><em class="italic">Highly Automated and Autonomous Vehicle Development with Amazon Web </em><span class="No-Break"><em class="italic">Services</em></span><span class="No-Break">: </span><a href="https://pages.awscloud.com/rs/112-TZM-766/images/Autonomous_Vehicle_Development_with_AWS.pdf"><span class="No-Break">https://pages.awscloud.com/rs/112-TZM-766/images/Autonomous_Vehicle_Development_with_AWS.pdf</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">AWS IoT </em><span class="No-Break"><em class="italic">FleetWise</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/iot-fleetwise/"><span class="No-Break">https://aws.amazon.com/iot-fleetwise/</span></a></li>
				<li><em class="italic">Lyft Increases Simulation Capacity, Lowers Costs Using Amazon EC2 Spot </em><span class="No-Break"><em class="italic">Instances</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/solutions/case-studies/Lyft-level-5-spot/"><span class="No-Break">https://aws.amazon.com/solutions/case-studies/Lyft-level-5-spot/</span></a></li>
				<li><em class="italic">Autonomous Driving Data Lake Reference </em><span class="No-Break"><em class="italic">Architecture</em></span><span class="No-Break">: </span><a href="https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/autonomous-driving-data-lake-ra.pdf?did=wp_card&amp;trk=wp_card"><span class="No-Break">https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/autonomous-driving-data-lake-ra.pdf?did=wp_card&amp;trk=wp_card</span></a></li>
				<li><em class="italic">Automating Data Ingestion and Labeling for Autonomous Vehicle </em><span class="No-Break"><em class="italic">Development</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/architecture/field-notes-automating-data-ingestion-and-labeling-for-autonomous-vehicle-development/"><span class="No-Break">https://aws.amazon.com/blogs/architecture/field-notes-automating-data-ingestion-and-labeling-for-autonomous-vehicle-development/</span></a></li>
				<li><em class="italic">Use Ground Truth to Label 3D Point </em><span class="No-Break"><em class="italic">Clouds</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-point-cloud.html</span></a></li>
				<li><em class="italic">Mask RCNN </em><span class="No-Break"><em class="italic">paper</em></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/1703.06870"><span class="No-Break">https://arxiv.org/abs/1703.06870</span></a></li>
				<li><em class="italic">COCO </em><span class="No-Break"><em class="italic">dataset</em></span><span class="No-Break">: </span><a href="https://cocodataset.org"><span class="No-Break">https://cocodataset.org</span></a></li>
				<li><em class="italic">KITTI dataset</em>: <a href="https://registry.opendata.aws/kitti/">https://registry.opendata.aws/kitti/</a> <span class="No-Break">and </span><a href="http://www.cvlibs.net/datasets/kitti/"><span class="No-Break">http://www.cvlibs.net/datasets/kitti/</span></a></li>
				<li><em class="italic">A2D2 </em><span class="No-Break"><em class="italic">dataset</em></span><span class="No-Break">: </span><a href="https://registry.opendata.aws/aev-a2d2/"><span class="No-Break">https://registry.opendata.aws/aev-a2d2/</span></a></li>
				<li><span class="No-Break"><em class="italic">ResNet</em></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/1512.03385"><span class="No-Break">https://arxiv.org/abs/1512.03385</span></a></li>
				<li><span class="No-Break"><em class="italic">DeepManta</em></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/1703.07570"><span class="No-Break">https://arxiv.org/abs/1703.07570</span></a></li>
				<li><em class="italic">Vehicle Pose Estimation on KITTI Cars </em><span class="No-Break"><em class="italic">Hard</em></span><span class="No-Break">: </span><a href="https://paperswithcode.com/sota/vehicle-pose-estimation-on-kitti-cars-hard"><span class="No-Break">https://paperswithcode.com/sota/vehicle-pose-estimation-on-kitti-cars-hard</span></a><span class="No-Break"><span class="hidden">.</span></span></li>
				<li><em class="italic">Building an Autonomous Driving and ADAS Data Lake on </em><span class="No-Break"><em class="italic">AWS</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/"><span class="No-Break">https://aws.amazon.com/blogs/architecture/field-notes-building-an-autonomous-driving-and-adas-data-lake-on-aws/</span></a></li>
				<li><em class="italic">TFRecord </em><span class="No-Break"><em class="italic">dataset</em></span><span class="No-Break">: </span><a href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset"><span class="No-Break">https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset</span></a></li>
				<li><em class="italic">Performance Design Patterns for Amazon </em><span class="No-Break"><em class="italic">S3</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html"><span class="No-Break">https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html</span></a></li>
				<li><em class="italic">Moving our Machine Learning to the Cloud Inspired </em><span class="No-Break"><em class="italic">Innovation</em></span><span class="No-Break">: </span><a href="https://www.mobileye.com/blog/moving-our-machine-learning-to-the-cloud-inspired-innovation/"><span class="No-Break">https://www.mobileye.com/blog/moving-our-machine-learning-to-the-cloud-inspired-innovation/</span></a></li>
				<li><em class="italic">Lustre User </em><span class="No-Break"><em class="italic">Guide</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html"><span class="No-Break">https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html</span></a></li>
				<li><em class="italic">Pytorch </em><span class="No-Break"><em class="italic">DataLoader</em></span><span class="No-Break">: </span><a href="https://pytorch.org/docs/stable/data.html"><span class="No-Break">https://pytorch.org/docs/stable/data.html</span></a></li>
				<li><em class="italic">Exporting changes to the data </em><span class="No-Break"><em class="italic">repository</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/export-changed-data-meta-dra.html"><span class="No-Break">https://docs.aws.amazon.com/fsx/latest/LustreGuide/export-changed-data-meta-dra.html</span></a></li>
				<li><em class="italic">Hyundai reduces ML model training time for autonomous driving models using Amazon </em><span class="No-Break"><em class="italic">SageMaker</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/de/blogs/machine-learning/hyundai-reduces-training-time-for-autonomous-driving-models-using-amazon-sagemaker/"><span class="No-Break">https://aws.amazon.com/de/blogs/machine-learning/hyundai-reduces-training-time-for-autonomous-driving-models-using-amazon-sagemaker/</span></a></li>
				<li><em class="italic">Access Training </em><span class="No-Break"><em class="italic">Data</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/model-access-training-data.html</span></a></li>
				<li><em class="italic">Amazon EC2 P4 </em><span class="No-Break"><em class="italic">Instances</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/ec2/instance-types/p4/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/p4/</span></a></li>
				<li><em class="italic">AWS and NVIDIA achieve the fastest training times for Mask R-CNN and </em><span class="No-Break"><em class="italic">T5-3B</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/machine-learning/aws-and-nvidia-achieve-the-fastest-training-times-for-mask-r-cnn-and-t5-3b/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/aws-and-nvidia-achieve-the-fastest-training-times-for-mask-r-cnn-and-t5-3b/</span></a></li>
				<li><em class="italic">NVLink and </em><span class="No-Break"><em class="italic">NVSwitch</em></span><span class="No-Break">: </span><a href="https://www.nvidia.com/en-us/data-center/nvlink/"><span class="No-Break">https://www.nvidia.com/en-us/data-center/nvlink/</span></a></li>
				<li><em class="italic">NVIDIA Collective Communications Library (</em><span class="No-Break"><em class="italic">NCCL)</em></span><span class="No-Break">: </span><a href="https://developer.nvidia.com/nccl"><span class="No-Break">https://developer.nvidia.com/nccl</span></a></li>
				<li><em class="italic">AWS </em><span class="No-Break"><em class="italic">re:Invent</em></span><span class="No-Break">: </span><a href="https://reinvent.awsevents.com/"><span class="No-Break">https://reinvent.awsevents.com/</span></a></li>
				<li><em class="italic">Amazon EC2 DL1 </em><span class="No-Break"><em class="italic">Instances</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/ec2/instance-types/habana-gaudi/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/habana-gaudi/</span></a></li>
				<li><em class="italic">Distributed training </em><span class="No-Break"><em class="italic">libraries</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/sagemaker/distributed-training/"><span class="No-Break">https://aws.amazon.com/sagemaker/distributed-training/</span></a></li>
				<li><span class="No-Break"><em class="italic">CARLA</em></span><span class="No-Break">: </span><a href="http://carla.org/"><span class="No-Break">http://carla.org/</span></a></li>
				<li><span class="No-Break"><em class="italic">AirSim</em></span><span class="No-Break">: </span><a href="https://github.com/microsoft/AirSim"><span class="No-Break">https://github.com/microsoft/AirSim</span></a></li>
				<li><em class="italic">Project </em><span class="No-Break"><em class="italic">Aslan</em></span><span class="No-Break">: </span><a href="https://github.com/project-aslan/Aslan"><span class="No-Break">https://github.com/project-aslan/Aslan</span></a></li>
				<li><em class="italic">SUMMIT </em><span class="No-Break"><em class="italic">Simulator</em></span><span class="No-Break">: </span><a href="https://github.com/AdaCompNUS/summit"><span class="No-Break">https://github.com/AdaCompNUS/summit</span></a></li>
				<li><span class="No-Break"><em class="italic">Deepdrive</em></span><span class="No-Break">: </span><a href="https://github.com/deepdrive/deepdrive"><span class="No-Break">https://github.com/deepdrive/deepdrive</span></a></li>
				<li><span class="No-Break"><em class="italic">Gazebo</em></span><span class="No-Break">: </span><a href="https://gazebosim.org/home"><span class="No-Break">https://gazebosim.org/home</span></a></li>
				<li><span class="No-Break"><em class="italic">Drake</em></span><span class="No-Break">: </span><a href="https://drake.mit.edu/"><span class="No-Break">https://drake.mit.edu/</span></a></li>
				<li><em class="italic">NVIDIA Isaac </em><span class="No-Break"><em class="italic">Sim</em></span><span class="No-Break">: </span><a href="https://developer.nvidia.com/isaac-sim"><span class="No-Break">https://developer.nvidia.com/isaac-sim</span></a></li>
				<li><span class="No-Break"><em class="italic">Unity</em></span><span class="No-Break">: </span><a href="https://unity.com/"><span class="No-Break">https://unity.com/</span></a></li>
				<li><em class="italic">Unreal </em><span class="No-Break"><em class="italic">Engine</em></span><span class="No-Break">: </span><a href="https://www.unrealengine.com/"><span class="No-Break">https://www.unrealengine.com/</span></a></li>
			</ul>
		</div>
	</body></html>