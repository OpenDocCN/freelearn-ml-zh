- en: Text Mining with Mallet - Topic Modeling and Spam Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll first discuss what **text mining** is, what kind of
    analysis it is able to offer, and why you might want to use it in your application.
    We''ll then discuss how to work with **Mallet**, a Java library for natural-language
    processing, covering data import and text pre-processing. Afterward, we will look
    into two text-mining applications: **topic modeling**, where we will discuss how
    text mining can be used to identify topics found in  text documents without reading
    them individually, and **spam detection**, where we will discuss how to automatically
    classify text documents into categories.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Introducing text mining
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and working with Mallet
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam detection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing text mining
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text mining, or text analytics, refers to the process of automatically extracting
    high-quality information from text documents, most often written in natural language,
    where high-quality information is considered to be relevant, novel, and interesting.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: While a typical text analytics application is used to scan a set of documents
    to generate a search index, text mining can be used in many other applications,
    including text categorization into specific domains; text clustering to automatically
    organize a set of documents; sentiment analysis to identify and extract subjective
    information in documents; concept or entity extraction that is capable of identifying
    people, places, organizations, and other entities from documents; document summarization
    to automatically provide the most important points in the original document; and
    learning relations between named entities.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'The process based on statistical pattern mining usually involves the following
    steps:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Information retrieval and extraction
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transforming unstructured text data into structured data; for example, parsing,
    removing noisy words, lexical analysis, calculating word frequencies, and deriving
    linguistic features
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discovery of patterns from structured data and tagging or annotation
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluation and interpretation of the results
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Later in this chapter, we will look at two application areas: topic modeling
    and **text categorization**. Let''s examine what they bring to the table.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic modeling is an unsupervised technique and might be useful if you need
    to analyze a large archive of text documents and wish to understand what the archive
    contains, without necessarily reading every single document by yourself. A text
    document can be a blog post, an email, a tweet, a document, a book chapter, a
    diary entry, and so on. Topic modeling looks for patterns in a corpus of text;
    more precisely, it identifies topics as lists of words that appear in a statistically
    meaningful way. The most well-known algorithm is **Latent Dirichlet Allocation**
    (**LDA**), which assumes that the author composed a piece of text by selecting
    words from possible baskets of words, where each basket corresponds to a topic.
    Using this assumption, it becomes possible to mathematically decompose text into
    the most likely baskets from where the words first came. The algorithm then iterates
    over this process until it converges to the most likely distribution of words
    into baskets, which we call *topics*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是一种无监督技术，如果你需要分析大量的文本文档档案并希望了解档案包含的内容，这可能很有用，而不必亲自阅读每一份文档。文本文档可以是博客文章、电子邮件、推文、文档、书籍章节、日记条目等。主题建模在文本语料库中寻找模式；更确切地说，它通过统计上有意义的单词列表来识别主题。最著名的算法是**潜在狄利克雷分配**（**LDA**），它假设作者通过从可能的单词篮子中选择单词来创作文本，每个篮子对应一个主题。利用这个假设，可以将文本从最可能的单词来源篮子中数学分解出来。然后算法重复这个过程，直到收敛到最可能的单词分布到篮子中的分布，我们称之为*主题*。
- en: 'For example, if we use topic modeling on a series of news articles, the algorithm
    would return a list of topics and keywords that most likely comprise of these
    topics. Using the example of news articles, the list might look similar to the
    following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们对一系列新闻文章进行主题建模，算法将返回一个包含这些主题的可能关键词列表。以新闻文章为例，列表可能看起来像以下这样：
- en: Winner, goal, football, score, first place
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胜利者、进球、足球、得分、第一名
- en: Company, stocks, bank, credit, business
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司、股票、银行、信贷、商业
- en: Election, opponent, president, debate, upcoming
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选举、对手、总统、辩论、即将到来
- en: By looking at the keywords, we can recognize that the news articles were concerned
    with sports, business, upcoming election, and so on. Later in this chapter, we
    will learn how to implement topic modeling using the news article example.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看关键词，我们可以识别出新闻文章涉及体育、商业、即将到来的选举等内容。在本章的后面部分，我们将学习如何使用新闻文章的例子来实现主题建模。
- en: Text classification
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类
- en: 'In text classification, or text categorization, the goal is to assign a text
    document according to its content to one or more classes or categories, which
    tend to be a more general subject area, such as vehicles or pets. Such general
    classes are referred to as topics, and the classification task is then called
    **text classification**, **text categorization**, **topic classification**, or
    **topic spotting**. While documents can be categorized according to other attributes
    such as document type, author, and publication year, the focus in this chapter
    will be on the document content only. Examples of text classification include
    the following components:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分类，或文本分类中，目标是根据其内容将文本文档分配到一类或多类，这些类别通常是一个更广泛的主题领域，如车辆或宠物。这样的通用类别被称为主题，此时的分类任务被称为**文本分类**、**文本分类**、**主题分类**或**主题检测**。虽然文档可以根据其他属性进行分类，如文档类型、作者和出版年份，但本章的重点将仅限于文档内容。以下是一些文本分类的例子：
- en: Spam detection in email messages, user comments, web pages, and so on
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件中的垃圾邮件检测、用户评论、网页等
- en: Detection of sexually explicit content
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性内容检测
- en: Sentiment detection, which automatically classifies a product or service review
    as positive or negative
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感检测，它自动将产品或服务评论分类为正面或负面
- en: Email sorting according to content
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据内容对电子邮件进行排序
- en: Topic-specific search, where search engines restrict searches to a particular
    topic or genre, hence providing more accurate results
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题特定搜索，其中搜索引擎将搜索限制在特定主题或类型，从而提供更准确的结果
- en: These examples show how important text classification is in information retrieval
    systems; hence, most modern information retrieval systems use some kind of text
    classifier. The classification task that we will use as an example in this book
    is text classification for detecting email spam.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子显示了文本分类在信息检索系统中的重要性；因此，大多数现代信息检索系统都使用某种形式的文本分类器。本书中将用作例子的分类任务是检测电子邮件垃圾邮件的文本分类。
- en: We will continue this chapter with an introduction to Mallet, a Java-based package
    for statistical natural-language processing, document classification, clustering,
    topic modeling, information extraction, and other machine-learning applications
    to text. We will then cover two text analytics applications, namely, topics modeling
    and spam detection as text classification.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Installing Mallet
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mallet is available for download at the UMass Amherst University website at [http://mallet.cs.umass.edu/download.php](http://mallet.cs.umass.edu/download.php).
    Navigate to the Download section as shown in the following screenshot and select
    the latest stable release (**2.0.8**, at the time of writing this book):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d8e3fb3-83a6-48cc-b297-3650b7e8140d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: 'Download the ZIP file and extract the content. In the extracted directory,
    you should find a folder named `dist` with two JAR files: `mallet.jar` and `mallet-deps.jar`.
    The first one contains all of the packaged Mallet classes, while the second one
    packs all of the dependencies. We will include both JARs files in your project
    as referenced libraries, as shown in the following screenshot:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4ea3672-2a16-4e49-a313-bdb4ae51066c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: 'If you are using Eclipse, right-click on Project, select Properties, and pick
    Java Build Path. Select the Libraries tab and click Add External JARs. Now, select
    the two JARs files and confirm, as shown in the following screenshot:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d0141f0-1ce7-4a70-a288-9857e639ec1f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Now we are ready to start using Mallet.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Working with text data
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the main challenges in text mining is transforming unstructured written
    natural language into structured attribute-based instances. The process involves
    many steps, as shown here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76c1e7b0-4d6a-4ed6-bff4-981855149687.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: First, we extract some text from the internet, existing documents, or databases.
    At the end of the first step, the text could still be present in the XML format
    or some other proprietary format. The next step is to extract the actual text
    and segment it into parts of the document, for example, title, headline, abstract,
    and body. The third step is involved with normalizing text encoding to ensure
    the characters are presented in the same way; for example, documents encoded in
    formats such as ASCII, ISO 8859-1 and Windows-1250 are transformed into Unicode
    encoding. Next, tokenization splits the document into particular words, while
    the next step removes frequent words that usually have low predictive power, for
    example, the, a, I, and we.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The **Part-Of-Speech** (**POS**) tagging and lemmatization step could be included
    to transform each token to its basic form, which is known as **lemma**, by removing
    word endings and modifiers. For example, running becomes run, and better becomes
    good. A simplified approach is stemming, which operates on a single word without
    any context of how the particular word is used, and therefore cannot distinguish
    between words having different meaning, depending on the part of speech, for example,
    axes as a plural of axe as well as axis.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**词性标注**（**POS**）和词形还原步骤可以包括将每个标记转换为它的基本形式，这被称为**词元**，通过去除词尾和修饰词来实现。例如，running变为run，而better变为good。一种简化的方法是词干提取，它在一个单词上操作，没有任何上下文，因此无法区分具有不同意义的单词，例如，axes作为axe的复数形式以及axis。'
- en: The last step transforms tokens into a feature space. Most often, feature space
    is a **Bag-Of-Words** (**BoW**) presentation. In this presentation, a set of all
    words appearing in the dataset is created. Each document is then presented as
    a vector that counts how many times a particular word appears in the document.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步将标记转换为特征空间。通常，特征空间是**词袋模型**（**BoW**）的表示。在这个表示中，创建了一个包含数据集中所有单词的集合。然后，每个文档都表示为一个向量，该向量统计特定单词在文档中出现的次数。
- en: 'Consider the following example with two sentences:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下两个句子的例子：
- en: Jacob likes table tennis. Emma likes table tennis too
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob likes table tennis. Emma likes table tennis too
- en: Jacob also likes basketball
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob also likes basketball
- en: 'The BoW in this case consists of {Jacob, likes, table, tennis, Emma, too, also,
    basketball}, which has eight distinct words. The two sentences could be now presented
    as vectors using the indexes of the list, indicating how many times a word at
    a particular index appears in the document, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，BoW（词袋模型）由以下单词组成：{Jacob, likes, table, tennis, Emma, too, also, basketball}，共有八个不同的单词。现在可以将这两个句子表示为向量，使用列表的索引表示特定索引处的单词在文档中出现的次数，如下所示：
- en: '[1, 2, 2, 2, 1, 0, 0, 0]'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 2, 2, 2, 1, 0, 0, 0]'
- en: '[1, 1, 0, 0, 0, 0, 1, 1]'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 1, 0, 0, 0, 0, 1, 1]'
- en: Such vectors finally become instances for further learning.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的向量最终成为进一步学习的实例。
- en: Another very powerful presentation based on the BoW model is **word2vec**. Word2vec
    was introduced in 2013 by a team of researchers led by Tomas Mikolov at Google.
    Word2vec is a neural network that learns distributed representations for words.
    An interesting property of this presentation is that words appear in clusters,
    so that some word relationships, such as analogies, can be reproduced using vector
    math. A famous example shows that king−man+woman returns queen. Further details
    and implementation are available at the following link: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于BoW模型的一个非常强大的展示方式是**word2vec**。Word2vec是由Google的研究团队，以Tomas Mikolov为首，在2013年引入的。Word2vec是一个神经网络，它学习单词的分布式表示。这种展示的一个有趣特性是单词出现在簇中，因此可以使用向量数学重现一些单词关系，例如类比。一个著名的例子表明，king−man+woman返回queen。更多细节和实现可以在以下链接中找到：[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).
- en: Importing data
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入数据
- en: In this chapter, we will not look into how to scrap a set of documents from
    a website or extract them from database. Instead, we will assume that we have
    already collected them as set of documents and stored them in the `.txt` file
    format. Now let's look at two options for loading them. The first option addresses
    the situation where each document is stored in its own `.txt` file. The second
    option addresses the situation where all of the documents are stored in a single
    file by taking one per line.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不会探讨如何从网站抓取一组文档或从数据库中提取它们。相反，我们将假设我们已经将它们收集为文档集，并以`.txt`文件格式存储。现在让我们看看加载它们的两种选项。第一种选项解决的是每个文档都存储在其自己的`.txt`文件中的情况。第二种选项解决的是所有文档都存储在单个文件中，每行一个文档的情况。
- en: Importing from directory
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从目录导入
- en: 'Mallet supports reading from directory with the `cc.mallet.pipe.iterator.FileIterator`
    class. A file iterator is constructed with the following three parameters:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Mallet支持使用`cc.mallet.pipe.iterator.FileIterator`类从目录中读取。文件迭代器是通过以下三个参数构建的：
- en: A list of `File[]` directories with text files
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含文本文件的`File[]`目录列表
- en: A file filter that specifies which files to select within a directory
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文件过滤器，用于指定在目录中要选择哪些文件
- en: A pattern that is applied to a filename to produce a class label
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用到文件名上以产生类标签的模式
- en: 'Consider the data structured into folders as shown in the following screenshot.
    We have documents organized in five topics by folders (`tech`, `entertainment`,
    `politics`, `sport`, and `business`). Each folder contains documents on particular
    topics, as shown in the following screenshot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下截图所示的数据结构。我们按照五个主题（`tech`、`entertainment`、`politics`、`sport`和`business`）将文档组织在文件夹中。每个文件夹包含特定主题的文档，如下截图所示：
- en: '![](img/8c8de3e3-7eb8-4a67-adef-fc6b9320aecf.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8c8de3e3-7eb8-4a67-adef-fc6b9320aecf.png)'
- en: 'In this case, we initialize `iterator` as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们初始化`iterator`如下：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first parameter specifies the path to our root folder, the second parameter
    limits the iterator to the `.txt` files only, while the last parameter asks the
    method to use the last directory name in the path as class label.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数指定了我们的根文件夹的路径，第二个参数将迭代器限制为仅`.txt`文件，而最后一个参数要求方法使用路径中的最后一个目录名作为类标签。
- en: Importing from file
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文件导入
- en: 'Another option to load the documents is through `cc.mallet.pipe.iterator.CsvIterator.CsvIterator(Reader,
    Pattern, int, int, int)`, which assumes all of the documents are in a single file
    and returns one instance per line extracted by a regular expression. The class
    is initialized by the following components:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 加载文档的另一种选项是通过`cc.mallet.pipe.iterator.CsvIterator.CsvIterator(Reader, Pattern,
    int, int, int)`，它假设所有文档都在一个文件中，并且通过正则表达式提取每一行返回一个实例。该类通过以下组件初始化：
- en: '`Reader`: This is the object that specifies how to read from a file'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Reader`：这是一个对象，指定如何从文件中读取'
- en: '`Pattern`: This is a regular expression, extracting three groups: data, target
    label, and document name'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pattern`：这是一个正则表达式，提取三个组：数据、目标标签和文档名'
- en: '`int, int, int`: These are the indexes of data, target, and name groups as
    they appear in a regular expression'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`int, int, int`：这些是正则表达式中数据、目标和名称组的索引'
- en: 'Consider a text document in the following format, specifying the document name,
    category, and content:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下格式的文本文档，指定文档名称、类别和内容：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To parse a line into three groups, we can use the following regular expression:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要将一行解析为三个组，我们可以使用以下正则表达式：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are three groups that appear in parenthesies `()`, where the third group
    contains the data, the second group contains the target class, and the first group
    contains the document ID. `iterator` is initialized as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在括号`()`中出现了三个组，其中第三个组包含数据，第二个组包含目标类，第一个组包含文档ID。`iterator`的初始化如下：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, the regular expression extracts the three groups separated by an empty
    space and their order is `3, 2, 1`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，正则表达式提取了由空格分隔的三个组，其顺序为`3, 2, 1`。
- en: Now let's move to the data-preprocessing pipeline.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向数据预处理管道。
- en: Pre-processing text data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理文本数据
- en: 'Once we initialize an iterator that will go through the data, we need to pass
    the data through a sequence of transformations as described at the beginning of
    this section. Mallet supports this process through a pipeline and a wide variety
    of steps that could be included in a pipeline, which are collected in the `cc.mallet.pipe`
    package. Some examples are as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化了一个将遍历数据的迭代器，我们需要将数据通过本节开头所述的一系列转换传递。Mallet通过管道和一系列可以包含在管道中的步骤来支持此过程，这些步骤收集在`cc.mallet.pipe`包中。以下是一些示例：
- en: '`Input2CharSequence`: This is a pipe that can read from various kinds of text
    sources (either URL, file, or reader) into `CharSequence`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Input2CharSequence`：这是一个管道，可以从各种文本源（无论是URL、文件还是读取器）读取到`CharSequence`'
- en: '`CharSequenceRemoveHTML`: This pipe removes HTML from `CharSequence`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CharSequenceRemoveHTML`：这个管道从`CharSequence`中移除HTML'
- en: '`MakeAmpersandXMLFriendly`: This converts `&` into `&amp` in tokens of a token
    sequence'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MakeAmpersandXMLFriendly`：这会将标记序列中的`&`转换为`&amp`'
- en: '`TokenSequenceLowercase`: This converts the text in each token in the token
    sequence in the data field into lowercase'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TokenSequenceLowercase`：这会将数据字段中每个标记序列中的文本转换为小写'
- en: '`TokenSequence2FeatureSequence`: This converts the token sequence in the data
    field of each instance into a feature sequence'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TokenSequence2FeatureSequence`：这会将每个实例的数据字段中的标记序列转换为特征序列'
- en: '`TokenSequenceNGrams`: This converts the token sequence in the data field into
    a token sequence of ngrams, that is, a combination of two or more words'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TokenSequenceNGrams`：这会将数据字段中的标记序列转换为ngrams的标记序列，即两个或更多单词的组合'
- en: The full list of processing steps is available in the following Mallet documentation: [http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html](http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to build a class that will import our data. We will do that
    using the following steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a pipeline, where each processing step is denoted as a pipeline
    in Mallet. Pipelines can be wired together in a serial fashion with a list of
    `ArrayList<Pipe>` objects:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s begin by reading data from a file object and converting all of the characters
    into lowercase:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will tokenize raw strings with a regular expression. The following pattern
    includes unicode letters and numbers and the underscore character:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will now remove stop words, that is, frequent words with no predictive power,
    using a standard English stop list. Two additional parameters indicate whether
    stop-word removal should be case-sensitive and mark deletions instead of just
    deleting the words. We''ll set both of them to `false`:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Instead of storing the actual words, we can convert them into integers, indicating
    a word index in the BoW:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We''ll do the same for the class label; instead of the label string, we''ll
    use an integer, indicating a position of the label in our bag of words:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We could also print the features and the labels by invoking the `PrintInputAndTarget`
    pipe:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We store the list of pipelines in a `SerialPipes` class that will covert an
    instance through a sequence of pipes:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now let's take a look at how apply this in a text-mining application!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling for BBC News
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, the goal of topic modeling is to identify patterns in
    a text corpus that correspond to document topics. In this example, we will use
    a dataset originating from BBC News. This dataset is one of the standard benchmarks
    in machine-learning research, and is available for non-commercial and research
    purposes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to build a classifier that is able to assign a topic to an uncategorized
    document.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: BBC dataset
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2006, Greene and Cunningham collected the BBC dataset to study a particular
    document—*Clustering challenge using support vector machines*. The dataset consists
    of 2,225 documents from the BBC News website from 2004 to 2005, corresponding
    to the stories collected from five topical areas: business, entertainment, politics,
    sport, and technology. The dataset can be seen at the following website: [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'We can download the raw text files under the Dataset: BBC section. You will
    also notice that the website contains an already processed dataset, but, for this
    example, we want to process the dataset by ourselves. The ZIP contains five folders,
    one per topic. The actual documents are placed in the corresponding topic folder,
    as shown in the following screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7869097-c8f5-4e07-a09c-1c7995546906.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Now, let's build a topic classifier.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin the modeling phase using the following steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the dataset and processing the text using the following
    lines of code:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will then create a default `pipeline`  object as previously described:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will initialize the `folderIterator` object:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will now construct a new instance list with the `pipeline` that we want
    to use to process the text:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We process each instance provided by  `iterator`:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now let's create a model with five topics using the `cc.mallet.topics.ParallelTopicModel.ParallelTopicModel`
    class that implements a simple threaded LDA model. LDA is a common method for
    topic modeling that uses Dirichlet distribution to estimate the probability that
    a selected topic generates a particular document. We will not dive deep into the
    details in this chapter; the reader is referred to the original paper by D. Blei
    et al. (2003).
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note: There is another classification algorithm in machine learning with the
    same initialism that refers to **Linear Discriminant Analysis** (**LDA**). Beside
    the common acronym, it has nothing in common with the LDA model.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'The class is instantiated with parameters alpha and beta, which can be broadly
    interpreted as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: High alpha value means that each document is likely to contain a mixture of
    most of the topics, and not any single topic specifically. A low alpha value puts
    less of such constraints on documents, and this means that it is more likely that
    a document may contain mixture of just a few, or even only one, of the topics.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high beta value means that each topic is likely to contain a mixture of most
    of the words, and not any word specifically; while a low value means that a topic
    may contain a mixture of just a few of the words.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, we initially keep both parameters low (alpha_t = `0.01`, beta_w
    = `0.01`) as we assume topics in our dataset are not mixed much and there are
    many words for each of the topics:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will add `instances` to the model, and since we are using parallel implementation,
    we will specify the number of threads that will run in parallel, as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will now run the model for a selected number of iterations. Each iteration
    is used for better estimation of internal LDA parameters. For testing, we can
    use a small number of iterations, for example, 50; while in real applications,
    use `1000` or `2000` iterations. Finally, we will call the `void estimate() `method
    that will actually build an LDA model:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The model outputs the following result:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`LL/token` indicates the model''s log-likelihood, divided by the total number
    of tokens, indicating how likely the data is given the model. Increasing values
    mean the model is improving.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'The output also shows the top words describing each topic. The words correspond
    to initial topics really well:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic 0**: `game`, `england`, `year`, `time`, `win`, `world`, `6` ⇒ sport'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic 1**: `year`, `1`, `company`, `market`, `growth`, `economy`, `firm` ⇒
    finance'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic 2**: `people`, `technology`, `mobile`, `mr`, `games`, `users`, `music`
    ⇒ tech'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic 3**: `film`, `year`, `music`, `show`, `awards`, `award`, `won` ⇒ entertainment'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic 4**: `mr`, `government`, `people`, `labor`, `election`, `party`, `blair`
    ⇒ politics'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are still some words that don't make much sense, for instance, `mr`, `1`,
    and `6`. We could include them in the stop word list. Also, some words appear
    twice, for example, `award` and `awards`. This happened because we didn't apply
    any stemmer or lemmatization pipe.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll take a look to check whether the model is any good.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a model
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As statistical topic modeling has an unsupervised nature, it makes model selection
    difficult. For some applications, there may be some extrinsic tasks at hand, such
    as information retrieval or document classification, for which performance can
    be evaluated. However, in general, we want to estimate the model's ability to
    generalize topics regardless of the task.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: In 2009, Wallach et al. introduced an approach that measures the quality of
    a model by computing the log probability of held-out documents under the model.
    The likelihood of unseen documents can be used to compare models—higher likelihood
    implies a better model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'We will evaluate the model using the following steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s split the documents into training and test sets (that is, held-out documents),
    where we use 90% for training and 10% for testing:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let''s rebuild our model using only `90%` of our documents:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will initialize an `estimator` object that implements Wallach''s log probability
    of held-out documents, `MarginalProbEstimator`:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: An intuitive description of LDA is summarized by Annalyn Ng in her blog: [https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/](https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/). To
    get deeper insight into the LDA algorithm, its components, and its working, take
    a look at the original paper LDA by David Blei et al. (2003) at [http://jmlr.csail.mit.edu/papers/v3/blei03a.html](http://jmlr.csail.mit.edu/papers/v3/blei03a.html),
    or take a look at the summarized presentation by D. Santhanam of Brown University
    at [http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf](http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'The class implements many estimators that require quite deep theoretical knowledge
    of how the LDA method works. We''ll pick the left-to-right evaluator, which is
    appropriate for a wide range of applications, including text mining, and speech
    recognition. The left-to-right evaluator is implemented as the `double evaluateLeftToRight`
    method, accepting the following components:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '`Instances heldOutDocuments`: This tests the instances.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int numParticles`: This algorithm parameter indicates the number of left-to-right
    tokens, where the default value is 10.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boolean useResampling`: This states whether to resample topics in left-to-right
    evaluation; resampling is more accurate, but leads to quadratic scaling in the
    length of documents.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PrintStream docProbabilityStream`: This is the file or `stdout` in which we
    write the inferred log probabilities per document.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s run  `estimator`, as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In our particular case, the `estimator` outputs the following `log likelihood`,
    which makes sense when it is compared to other models that are either constructed
    with different parameters, pipelines, or data—the higher the log likelihood, the
    better the model is:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now let's take a look at how to make use of this model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Reusing a model
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are usually not building models on the fly, it often makes sense to train
    a model once and use it repeatedly to classify new data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Note that, if you'd like to classify new documents, they need go through the
    same pipeline as other documents—the pipe needs to be the same for both training
    and classification. During training, the pipe's data alphabet is updated with
    each training instance. If you create a new pipe with the same steps, you don't
    produce the same pipeline as its data alphabet is empty. Therefore, to use the
    model on new data, we have to save or load the pipe along with the model and use
    this pipe to add new instances.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Saving a model
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mallet supports a standard method for saving and restoring objects based on
    serialization.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'We simply create a new instance of the `ObjectOutputStream` class and write
    the object into a file, as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Restoring a model
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Restoring a model saved through serialization is simply an inverse operation
    using the `ObjectInputStream` class:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We discussed how to build an LDA model to automatically classify documents into
    topics. In the next example, we'll look into another text mining problem—text
    classification.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Detecting email spam
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spam or electronic spam refers to unsolicited messages, typically carrying
    advertising content, infected attachments, links to phishing or malware sites,
    and so on. While the most widely recognized form of spam is email spam, spam abuses
    appear in other media as well: website comments, instant messaging, internet forums,
    blogs, online ads, and so on.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how to build Naive Bayesian spam filtering,
    using BoW representation to identify spam emails. Naive Bayes spam filtering is
    one of the basic techniques that was implemented in the first commercial spam
    filters; for instance, Mozilla Thunderbird mail client uses native implementation
    of such filtering. While the example in this chapter will use email spam, the
    underlying methodology can be applied to other type of text-based spam as well.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Email spam dataset
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2000, Androutsopoulos et al. collected one of the first email spam datasets
    to benchmark spam-filtering algorithms. They studied how the Naive Bayes classifier
    can be used to detect spam, if additional pipes such as stop list, stemmer, and
    lemmatization contribute to better performance. The dataset was reorganized by
    Andrew Ng in OpenClassroom's machine-learning class, available for download at
    [http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Select and download the second option, `ex6DataEmails.zip`, as shown in the
    following screenshot:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/089d7c7f-00e3-4f7c-beff-9c635d90401c.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: 'The ZIP contains the following folders:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: The `nonspam-train` and `spam-train` folders contain the pre-processed emails
    that you will use for training. They have 350 emails each.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `nonspam-test` and `spam-test` folders constitute the test set, containing
    130 spam and 130 nonspam emails. These are the documents that you will make predictions
    on. Notice that, even though separate folders tell you the correct labeling, you
    should make your predictions on all of the test documents without this knowledge.
    After you make your predictions, you can use the correct labeling to check whether
    your classifications were correct.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To leverage Mallet''s folder iterator, let''s reorganize the folder structure
    as follows. We will create two folders, `train` and `test`, and put the `spam/nospam`
    folders under the corresponding folders. The initial folder structure is as shown
    in the following screenshot:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d70d02fe-2552-4cdf-a0d7-5a65f00d0f72.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'The final folder structure will be as shown in the following screenshot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fc63051-3265-49f9-915c-0efac24c48a2.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: The next step is to transform email messages to feature vectors.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Feature generation
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform feature generation using the following steps:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a default pipeline, as described previously:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that we added an additional `FeatureSequence2FeatureVector` pipe that transforms
    a feature sequence into a feature vector. When we have data in a feature vector,
    we can use any classification algorithm, as we saw in the previous chapters. We'll
    continue our example in Mallet to demonstrate how to build a classification model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize a folder iterator to load our examples in the `train` folder
    comprising email examples in the `spam` and `nonspam` subfolders, which will be
    used as example labels:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will construct a new instance list with the `pipeline` object that we want
    to use to process the text:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will process each instance provided by the iterator:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We have now loaded the data and transformed it into feature vectors. Let's train
    our model on the training set and predict the `spam/nonspam` classification on
    the `test` set.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Training and testing
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mallet implements a set of classifiers in the `cc.mallet.classify` package,
    including decision trees, Naive Bayes, AdaBoost, bagging, boosting, and many others.
    We''ll start with a basic classifier, that is, a Naive Bayes classifier. A classifier
    is initialized by the `ClassifierTrainer` class, which returns a classifier when
    we invoke its `train(Instances)` method:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now let's see how this classier works and evaluate its performance on a separate
    dataset.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Model performance
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To evaluate the classifier on a separate dataset, we will use the following
    steps:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the emails located in our `test` folder:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will pass the data through the same pipeline that we initialized during
    training:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To evaluate classifier performance, we''ll use the `cc.mallet.classify.Trial`
    class, which is initialized with a classifier and set of test instances:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The evaluation is performed immediately at initialization. We can then simply
    take out the measures that we care about. In our example, we''d like to check
    the precision and recall on classifying spam email messages, or F-measure, which
    returns a harmonic mean of both values, as follows:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The evaluation object outputs the following results:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The results show that the model correctly discovers 97.69% of spam messages
    (recall), and when it marks an email as spam, it is correct in 96.94% cases. In
    other words, it misses approximately 2 per 100 spam messages and marks 3 per 100
    valid messages as spam. So, it's not really perfect, but it's more than a good
    start!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how text mining is different than traditional
    attribute-based learning, requiring a lot of pre-processing steps to transform
    written natural language into feature vectors. Further, we discussed how to leverage
    Mallet, a Java-based library for NLP by applying it to two real-life problems.
    First, we modeled topics in a news corpus using the LDA model to build a model
    that is able to assign a topic to new document. We also discussed how to build
    a Naive Bayesian spam-filtering classifier using the BoW representation.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes the technical demonstrations of how to apply various
    libraries to solve machine-learning tasks. As we weren't able to cover more interesting
    applications and give further details at many points, the next chapter gives some
    further pointers on how to continue learning and dive deeper into particular topics.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
