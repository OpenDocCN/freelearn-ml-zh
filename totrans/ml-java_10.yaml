- en: Text Mining with Mallet - Topic Modeling and Spam Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll first discuss what **text mining** is, what kind of
    analysis it is able to offer, and why you might want to use it in your application.
    We''ll then discuss how to work with **Mallet**, a Java library for natural-language
    processing, covering data import and text pre-processing. Afterward, we will look
    into two text-mining applications: **topic modeling**, where we will discuss how
    text mining can be used to identify topics found in  text documents without reading
    them individually, and **spam detection**, where we will discuss how to automatically
    classify text documents into categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing text mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and working with Mallet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing text mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text mining, or text analytics, refers to the process of automatically extracting
    high-quality information from text documents, most often written in natural language,
    where high-quality information is considered to be relevant, novel, and interesting.
  prefs: []
  type: TYPE_NORMAL
- en: While a typical text analytics application is used to scan a set of documents
    to generate a search index, text mining can be used in many other applications,
    including text categorization into specific domains; text clustering to automatically
    organize a set of documents; sentiment analysis to identify and extract subjective
    information in documents; concept or entity extraction that is capable of identifying
    people, places, organizations, and other entities from documents; document summarization
    to automatically provide the most important points in the original document; and
    learning relations between named entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process based on statistical pattern mining usually involves the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Information retrieval and extraction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transforming unstructured text data into structured data; for example, parsing,
    removing noisy words, lexical analysis, calculating word frequencies, and deriving
    linguistic features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discovery of patterns from structured data and tagging or annotation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluation and interpretation of the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Later in this chapter, we will look at two application areas: topic modeling
    and **text categorization**. Let''s examine what they bring to the table.'
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic modeling is an unsupervised technique and might be useful if you need
    to analyze a large archive of text documents and wish to understand what the archive
    contains, without necessarily reading every single document by yourself. A text
    document can be a blog post, an email, a tweet, a document, a book chapter, a
    diary entry, and so on. Topic modeling looks for patterns in a corpus of text;
    more precisely, it identifies topics as lists of words that appear in a statistically
    meaningful way. The most well-known algorithm is **Latent Dirichlet Allocation**
    (**LDA**), which assumes that the author composed a piece of text by selecting
    words from possible baskets of words, where each basket corresponds to a topic.
    Using this assumption, it becomes possible to mathematically decompose text into
    the most likely baskets from where the words first came. The algorithm then iterates
    over this process until it converges to the most likely distribution of words
    into baskets, which we call *topics*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we use topic modeling on a series of news articles, the algorithm
    would return a list of topics and keywords that most likely comprise of these
    topics. Using the example of news articles, the list might look similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Winner, goal, football, score, first place
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Company, stocks, bank, credit, business
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Election, opponent, president, debate, upcoming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By looking at the keywords, we can recognize that the news articles were concerned
    with sports, business, upcoming election, and so on. Later in this chapter, we
    will learn how to implement topic modeling using the news article example.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In text classification, or text categorization, the goal is to assign a text
    document according to its content to one or more classes or categories, which
    tend to be a more general subject area, such as vehicles or pets. Such general
    classes are referred to as topics, and the classification task is then called
    **text classification**, **text categorization**, **topic classification**, or
    **topic spotting**. While documents can be categorized according to other attributes
    such as document type, author, and publication year, the focus in this chapter
    will be on the document content only. Examples of text classification include
    the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Spam detection in email messages, user comments, web pages, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detection of sexually explicit content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment detection, which automatically classifies a product or service review
    as positive or negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Email sorting according to content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic-specific search, where search engines restrict searches to a particular
    topic or genre, hence providing more accurate results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These examples show how important text classification is in information retrieval
    systems; hence, most modern information retrieval systems use some kind of text
    classifier. The classification task that we will use as an example in this book
    is text classification for detecting email spam.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue this chapter with an introduction to Mallet, a Java-based package
    for statistical natural-language processing, document classification, clustering,
    topic modeling, information extraction, and other machine-learning applications
    to text. We will then cover two text analytics applications, namely, topics modeling
    and spam detection as text classification.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Mallet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mallet is available for download at the UMass Amherst University website at [http://mallet.cs.umass.edu/download.php](http://mallet.cs.umass.edu/download.php).
    Navigate to the Download section as shown in the following screenshot and select
    the latest stable release (**2.0.8**, at the time of writing this book):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d8e3fb3-83a6-48cc-b297-3650b7e8140d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Download the ZIP file and extract the content. In the extracted directory,
    you should find a folder named `dist` with two JAR files: `mallet.jar` and `mallet-deps.jar`.
    The first one contains all of the packaged Mallet classes, while the second one
    packs all of the dependencies. We will include both JARs files in your project
    as referenced libraries, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4ea3672-2a16-4e49-a313-bdb4ae51066c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you are using Eclipse, right-click on Project, select Properties, and pick
    Java Build Path. Select the Libraries tab and click Add External JARs. Now, select
    the two JARs files and confirm, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d0141f0-1ce7-4a70-a288-9857e639ec1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we are ready to start using Mallet.
  prefs: []
  type: TYPE_NORMAL
- en: Working with text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the main challenges in text mining is transforming unstructured written
    natural language into structured attribute-based instances. The process involves
    many steps, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76c1e7b0-4d6a-4ed6-bff4-981855149687.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we extract some text from the internet, existing documents, or databases.
    At the end of the first step, the text could still be present in the XML format
    or some other proprietary format. The next step is to extract the actual text
    and segment it into parts of the document, for example, title, headline, abstract,
    and body. The third step is involved with normalizing text encoding to ensure
    the characters are presented in the same way; for example, documents encoded in
    formats such as ASCII, ISO 8859-1 and Windows-1250 are transformed into Unicode
    encoding. Next, tokenization splits the document into particular words, while
    the next step removes frequent words that usually have low predictive power, for
    example, the, a, I, and we.
  prefs: []
  type: TYPE_NORMAL
- en: The **Part-Of-Speech** (**POS**) tagging and lemmatization step could be included
    to transform each token to its basic form, which is known as **lemma**, by removing
    word endings and modifiers. For example, running becomes run, and better becomes
    good. A simplified approach is stemming, which operates on a single word without
    any context of how the particular word is used, and therefore cannot distinguish
    between words having different meaning, depending on the part of speech, for example,
    axes as a plural of axe as well as axis.
  prefs: []
  type: TYPE_NORMAL
- en: The last step transforms tokens into a feature space. Most often, feature space
    is a **Bag-Of-Words** (**BoW**) presentation. In this presentation, a set of all
    words appearing in the dataset is created. Each document is then presented as
    a vector that counts how many times a particular word appears in the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example with two sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: Jacob likes table tennis. Emma likes table tennis too
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacob also likes basketball
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The BoW in this case consists of {Jacob, likes, table, tennis, Emma, too, also,
    basketball}, which has eight distinct words. The two sentences could be now presented
    as vectors using the indexes of the list, indicating how many times a word at
    a particular index appears in the document, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1, 2, 2, 2, 1, 0, 0, 0]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[1, 1, 0, 0, 0, 0, 1, 1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such vectors finally become instances for further learning.
  prefs: []
  type: TYPE_NORMAL
- en: Another very powerful presentation based on the BoW model is **word2vec**. Word2vec
    was introduced in 2013 by a team of researchers led by Tomas Mikolov at Google.
    Word2vec is a neural network that learns distributed representations for words.
    An interesting property of this presentation is that words appear in clusters,
    so that some word relationships, such as analogies, can be reproduced using vector
    math. A famous example shows that king−man+woman returns queen. Further details
    and implementation are available at the following link: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).
  prefs: []
  type: TYPE_NORMAL
- en: Importing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will not look into how to scrap a set of documents from
    a website or extract them from database. Instead, we will assume that we have
    already collected them as set of documents and stored them in the `.txt` file
    format. Now let's look at two options for loading them. The first option addresses
    the situation where each document is stored in its own `.txt` file. The second
    option addresses the situation where all of the documents are stored in a single
    file by taking one per line.
  prefs: []
  type: TYPE_NORMAL
- en: Importing from directory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mallet supports reading from directory with the `cc.mallet.pipe.iterator.FileIterator`
    class. A file iterator is constructed with the following three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of `File[]` directories with text files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A file filter that specifies which files to select within a directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pattern that is applied to a filename to produce a class label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the data structured into folders as shown in the following screenshot.
    We have documents organized in five topics by folders (`tech`, `entertainment`,
    `politics`, `sport`, and `business`). Each folder contains documents on particular
    topics, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c8de3e3-7eb8-4a67-adef-fc6b9320aecf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, we initialize `iterator` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter specifies the path to our root folder, the second parameter
    limits the iterator to the `.txt` files only, while the last parameter asks the
    method to use the last directory name in the path as class label.
  prefs: []
  type: TYPE_NORMAL
- en: Importing from file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another option to load the documents is through `cc.mallet.pipe.iterator.CsvIterator.CsvIterator(Reader,
    Pattern, int, int, int)`, which assumes all of the documents are in a single file
    and returns one instance per line extracted by a regular expression. The class
    is initialized by the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Reader`: This is the object that specifies how to read from a file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pattern`: This is a regular expression, extracting three groups: data, target
    label, and document name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int, int, int`: These are the indexes of data, target, and name groups as
    they appear in a regular expression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider a text document in the following format, specifying the document name,
    category, and content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To parse a line into three groups, we can use the following regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three groups that appear in parenthesies `()`, where the third group
    contains the data, the second group contains the target class, and the first group
    contains the document ID. `iterator` is initialized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, the regular expression extracts the three groups separated by an empty
    space and their order is `3, 2, 1`.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move to the data-preprocessing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we initialize an iterator that will go through the data, we need to pass
    the data through a sequence of transformations as described at the beginning of
    this section. Mallet supports this process through a pipeline and a wide variety
    of steps that could be included in a pipeline, which are collected in the `cc.mallet.pipe`
    package. Some examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Input2CharSequence`: This is a pipe that can read from various kinds of text
    sources (either URL, file, or reader) into `CharSequence`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CharSequenceRemoveHTML`: This pipe removes HTML from `CharSequence`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MakeAmpersandXMLFriendly`: This converts `&` into `&amp` in tokens of a token
    sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TokenSequenceLowercase`: This converts the text in each token in the token
    sequence in the data field into lowercase'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TokenSequence2FeatureSequence`: This converts the token sequence in the data
    field of each instance into a feature sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TokenSequenceNGrams`: This converts the token sequence in the data field into
    a token sequence of ngrams, that is, a combination of two or more words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full list of processing steps is available in the following Mallet documentation: [http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html](http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to build a class that will import our data. We will do that
    using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a pipeline, where each processing step is denoted as a pipeline
    in Mallet. Pipelines can be wired together in a serial fashion with a list of
    `ArrayList<Pipe>` objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s begin by reading data from a file object and converting all of the characters
    into lowercase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will tokenize raw strings with a regular expression. The following pattern
    includes unicode letters and numbers and the underscore character:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now remove stop words, that is, frequent words with no predictive power,
    using a standard English stop list. Two additional parameters indicate whether
    stop-word removal should be case-sensitive and mark deletions instead of just
    deleting the words. We''ll set both of them to `false`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of storing the actual words, we can convert them into integers, indicating
    a word index in the BoW:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll do the same for the class label; instead of the label string, we''ll
    use an integer, indicating a position of the label in our bag of words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also print the features and the labels by invoking the `PrintInputAndTarget`
    pipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We store the list of pipelines in a `SerialPipes` class that will covert an
    instance through a sequence of pipes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now let's take a look at how apply this in a text-mining application!
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling for BBC News
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, the goal of topic modeling is to identify patterns in
    a text corpus that correspond to document topics. In this example, we will use
    a dataset originating from BBC News. This dataset is one of the standard benchmarks
    in machine-learning research, and is available for non-commercial and research
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to build a classifier that is able to assign a topic to an uncategorized
    document.
  prefs: []
  type: TYPE_NORMAL
- en: BBC dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2006, Greene and Cunningham collected the BBC dataset to study a particular
    document—*Clustering challenge using support vector machines*. The dataset consists
    of 2,225 documents from the BBC News website from 2004 to 2005, corresponding
    to the stories collected from five topical areas: business, entertainment, politics,
    sport, and technology. The dataset can be seen at the following website: [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can download the raw text files under the Dataset: BBC section. You will
    also notice that the website contains an already processed dataset, but, for this
    example, we want to process the dataset by ourselves. The ZIP contains five folders,
    one per topic. The actual documents are placed in the corresponding topic folder,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7869097-c8f5-4e07-a09c-1c7995546906.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's build a topic classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin the modeling phase using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the dataset and processing the text using the following
    lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then create a default `pipeline`  object as previously described:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will initialize the `folderIterator` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now construct a new instance list with the `pipeline` that we want
    to use to process the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We process each instance provided by  `iterator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now let's create a model with five topics using the `cc.mallet.topics.ParallelTopicModel.ParallelTopicModel`
    class that implements a simple threaded LDA model. LDA is a common method for
    topic modeling that uses Dirichlet distribution to estimate the probability that
    a selected topic generates a particular document. We will not dive deep into the
    details in this chapter; the reader is referred to the original paper by D. Blei
    et al. (2003).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note: There is another classification algorithm in machine learning with the
    same initialism that refers to **Linear Discriminant Analysis** (**LDA**). Beside
    the common acronym, it has nothing in common with the LDA model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The class is instantiated with parameters alpha and beta, which can be broadly
    interpreted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: High alpha value means that each document is likely to contain a mixture of
    most of the topics, and not any single topic specifically. A low alpha value puts
    less of such constraints on documents, and this means that it is more likely that
    a document may contain mixture of just a few, or even only one, of the topics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A high beta value means that each topic is likely to contain a mixture of most
    of the words, and not any word specifically; while a low value means that a topic
    may contain a mixture of just a few of the words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, we initially keep both parameters low (alpha_t = `0.01`, beta_w
    = `0.01`) as we assume topics in our dataset are not mixed much and there are
    many words for each of the topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will add `instances` to the model, and since we are using parallel implementation,
    we will specify the number of threads that will run in parallel, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now run the model for a selected number of iterations. Each iteration
    is used for better estimation of internal LDA parameters. For testing, we can
    use a small number of iterations, for example, 50; while in real applications,
    use `1000` or `2000` iterations. Finally, we will call the `void estimate() `method
    that will actually build an LDA model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The model outputs the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`LL/token` indicates the model''s log-likelihood, divided by the total number
    of tokens, indicating how likely the data is given the model. Increasing values
    mean the model is improving.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output also shows the top words describing each topic. The words correspond
    to initial topics really well:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic 0**: `game`, `england`, `year`, `time`, `win`, `world`, `6` ⇒ sport'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic 1**: `year`, `1`, `company`, `market`, `growth`, `economy`, `firm` ⇒
    finance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic 2**: `people`, `technology`, `mobile`, `mr`, `games`, `users`, `music`
    ⇒ tech'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic 3**: `film`, `year`, `music`, `show`, `awards`, `award`, `won` ⇒ entertainment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic 4**: `mr`, `government`, `people`, `labor`, `election`, `party`, `blair`
    ⇒ politics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are still some words that don't make much sense, for instance, `mr`, `1`,
    and `6`. We could include them in the stop word list. Also, some words appear
    twice, for example, `award` and `awards`. This happened because we didn't apply
    any stemmer or lemmatization pipe.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll take a look to check whether the model is any good.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As statistical topic modeling has an unsupervised nature, it makes model selection
    difficult. For some applications, there may be some extrinsic tasks at hand, such
    as information retrieval or document classification, for which performance can
    be evaluated. However, in general, we want to estimate the model's ability to
    generalize topics regardless of the task.
  prefs: []
  type: TYPE_NORMAL
- en: In 2009, Wallach et al. introduced an approach that measures the quality of
    a model by computing the log probability of held-out documents under the model.
    The likelihood of unseen documents can be used to compare models—higher likelihood
    implies a better model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will evaluate the model using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s split the documents into training and test sets (that is, held-out documents),
    where we use 90% for training and 10% for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s rebuild our model using only `90%` of our documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will initialize an `estimator` object that implements Wallach''s log probability
    of held-out documents, `MarginalProbEstimator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: An intuitive description of LDA is summarized by Annalyn Ng in her blog: [https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/](https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/). To
    get deeper insight into the LDA algorithm, its components, and its working, take
    a look at the original paper LDA by David Blei et al. (2003) at [http://jmlr.csail.mit.edu/papers/v3/blei03a.html](http://jmlr.csail.mit.edu/papers/v3/blei03a.html),
    or take a look at the summarized presentation by D. Santhanam of Brown University
    at [http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf](http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'The class implements many estimators that require quite deep theoretical knowledge
    of how the LDA method works. We''ll pick the left-to-right evaluator, which is
    appropriate for a wide range of applications, including text mining, and speech
    recognition. The left-to-right evaluator is implemented as the `double evaluateLeftToRight`
    method, accepting the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Instances heldOutDocuments`: This tests the instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`int numParticles`: This algorithm parameter indicates the number of left-to-right
    tokens, where the default value is 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boolean useResampling`: This states whether to resample topics in left-to-right
    evaluation; resampling is more accurate, but leads to quadratic scaling in the
    length of documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PrintStream docProbabilityStream`: This is the file or `stdout` in which we
    write the inferred log probabilities per document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s run  `estimator`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In our particular case, the `estimator` outputs the following `log likelihood`,
    which makes sense when it is compared to other models that are either constructed
    with different parameters, pipelines, or data—the higher the log likelihood, the
    better the model is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now let's take a look at how to make use of this model.
  prefs: []
  type: TYPE_NORMAL
- en: Reusing a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we are usually not building models on the fly, it often makes sense to train
    a model once and use it repeatedly to classify new data.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, if you'd like to classify new documents, they need go through the
    same pipeline as other documents—the pipe needs to be the same for both training
    and classification. During training, the pipe's data alphabet is updated with
    each training instance. If you create a new pipe with the same steps, you don't
    produce the same pipeline as its data alphabet is empty. Therefore, to use the
    model on new data, we have to save or load the pipe along with the model and use
    this pipe to add new instances.
  prefs: []
  type: TYPE_NORMAL
- en: Saving a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mallet supports a standard method for saving and restoring objects based on
    serialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'We simply create a new instance of the `ObjectOutputStream` class and write
    the object into a file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Restoring a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Restoring a model saved through serialization is simply an inverse operation
    using the `ObjectInputStream` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We discussed how to build an LDA model to automatically classify documents into
    topics. In the next example, we'll look into another text mining problem—text
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting email spam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spam or electronic spam refers to unsolicited messages, typically carrying
    advertising content, infected attachments, links to phishing or malware sites,
    and so on. While the most widely recognized form of spam is email spam, spam abuses
    appear in other media as well: website comments, instant messaging, internet forums,
    blogs, online ads, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how to build Naive Bayesian spam filtering,
    using BoW representation to identify spam emails. Naive Bayes spam filtering is
    one of the basic techniques that was implemented in the first commercial spam
    filters; for instance, Mozilla Thunderbird mail client uses native implementation
    of such filtering. While the example in this chapter will use email spam, the
    underlying methodology can be applied to other type of text-based spam as well.
  prefs: []
  type: TYPE_NORMAL
- en: Email spam dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2000, Androutsopoulos et al. collected one of the first email spam datasets
    to benchmark spam-filtering algorithms. They studied how the Naive Bayes classifier
    can be used to detect spam, if additional pipes such as stop list, stemmer, and
    lemmatization contribute to better performance. The dataset was reorganized by
    Andrew Ng in OpenClassroom's machine-learning class, available for download at
    [http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Select and download the second option, `ex6DataEmails.zip`, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/089d7c7f-00e3-4f7c-beff-9c635d90401c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The ZIP contains the following folders:'
  prefs: []
  type: TYPE_NORMAL
- en: The `nonspam-train` and `spam-train` folders contain the pre-processed emails
    that you will use for training. They have 350 emails each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `nonspam-test` and `spam-test` folders constitute the test set, containing
    130 spam and 130 nonspam emails. These are the documents that you will make predictions
    on. Notice that, even though separate folders tell you the correct labeling, you
    should make your predictions on all of the test documents without this knowledge.
    After you make your predictions, you can use the correct labeling to check whether
    your classifications were correct.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To leverage Mallet''s folder iterator, let''s reorganize the folder structure
    as follows. We will create two folders, `train` and `test`, and put the `spam/nospam`
    folders under the corresponding folders. The initial folder structure is as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d70d02fe-2552-4cdf-a0d7-5a65f00d0f72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final folder structure will be as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fc63051-3265-49f9-915c-0efac24c48a2.png)'
  prefs: []
  type: TYPE_IMG
- en: The next step is to transform email messages to feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Feature generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will perform feature generation using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a default pipeline, as described previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note that we added an additional `FeatureSequence2FeatureVector` pipe that transforms
    a feature sequence into a feature vector. When we have data in a feature vector,
    we can use any classification algorithm, as we saw in the previous chapters. We'll
    continue our example in Mallet to demonstrate how to build a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize a folder iterator to load our examples in the `train` folder
    comprising email examples in the `spam` and `nonspam` subfolders, which will be
    used as example labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We will construct a new instance list with the `pipeline` object that we want
    to use to process the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will process each instance provided by the iterator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We have now loaded the data and transformed it into feature vectors. Let's train
    our model on the training set and predict the `spam/nonspam` classification on
    the `test` set.
  prefs: []
  type: TYPE_NORMAL
- en: Training and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mallet implements a set of classifiers in the `cc.mallet.classify` package,
    including decision trees, Naive Bayes, AdaBoost, bagging, boosting, and many others.
    We''ll start with a basic classifier, that is, a Naive Bayes classifier. A classifier
    is initialized by the `ClassifierTrainer` class, which returns a classifier when
    we invoke its `train(Instances)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now let's see how this classier works and evaluate its performance on a separate
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To evaluate the classifier on a separate dataset, we will use the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the emails located in our `test` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We will pass the data through the same pipeline that we initialized during
    training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To evaluate classifier performance, we''ll use the `cc.mallet.classify.Trial`
    class, which is initialized with a classifier and set of test instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The evaluation is performed immediately at initialization. We can then simply
    take out the measures that we care about. In our example, we''d like to check
    the precision and recall on classifying spam email messages, or F-measure, which
    returns a harmonic mean of both values, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The evaluation object outputs the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The results show that the model correctly discovers 97.69% of spam messages
    (recall), and when it marks an email as spam, it is correct in 96.94% cases. In
    other words, it misses approximately 2 per 100 spam messages and marks 3 per 100
    valid messages as spam. So, it's not really perfect, but it's more than a good
    start!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how text mining is different than traditional
    attribute-based learning, requiring a lot of pre-processing steps to transform
    written natural language into feature vectors. Further, we discussed how to leverage
    Mallet, a Java-based library for NLP by applying it to two real-life problems.
    First, we modeled topics in a news corpus using the LDA model to build a model
    that is able to assign a topic to new document. We also discussed how to build
    a Naive Bayesian spam-filtering classifier using the BoW representation.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter concludes the technical demonstrations of how to apply various
    libraries to solve machine-learning tasks. As we weren't able to cover more interesting
    applications and give further details at many points, the next chapter gives some
    further pointers on how to continue learning and dive deeper into particular topics.
  prefs: []
  type: TYPE_NORMAL
