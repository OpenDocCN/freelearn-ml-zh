- en: Text Mining with Mallet - Topic Modeling and Spam Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Mallet进行文本挖掘 - 主题建模和垃圾邮件检测
- en: 'In this chapter, we''ll first discuss what **text mining** is, what kind of
    analysis it is able to offer, and why you might want to use it in your application.
    We''ll then discuss how to work with **Mallet**, a Java library for natural-language
    processing, covering data import and text pre-processing. Afterward, we will look
    into two text-mining applications: **topic modeling**, where we will discuss how
    text mining can be used to identify topics found in  text documents without reading
    them individually, and **spam detection**, where we will discuss how to automatically
    classify text documents into categories.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先讨论什么是**文本挖掘**，它能提供什么样的分析，以及为什么你可能想在你的应用中使用它。然后我们将讨论如何使用**Mallet**，这是一个用于自然语言处理的Java库，涵盖数据导入和文本预处理。之后，我们将探讨两个文本挖掘应用：**主题建模**，我们将讨论如何使用文本挖掘来识别文本文档中的主题，而无需逐个阅读它们，以及**垃圾邮件检测**，我们将讨论如何自动将文本文档分类到类别中。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing text mining
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍文本挖掘
- en: Installing and working with Mallet
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和使用Mallet
- en: Topic modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模
- en: Spam detection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾邮件检测
- en: Introducing text mining
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍文本挖掘
- en: Text mining, or text analytics, refers to the process of automatically extracting
    high-quality information from text documents, most often written in natural language,
    where high-quality information is considered to be relevant, novel, and interesting.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 文本挖掘，或文本分析，是指从文本文档中自动提取高质量信息的过程，这些文档通常是用自然语言编写的，其中高质量信息被认为是相关的、新颖的和有趣的。
- en: While a typical text analytics application is used to scan a set of documents
    to generate a search index, text mining can be used in many other applications,
    including text categorization into specific domains; text clustering to automatically
    organize a set of documents; sentiment analysis to identify and extract subjective
    information in documents; concept or entity extraction that is capable of identifying
    people, places, organizations, and other entities from documents; document summarization
    to automatically provide the most important points in the original document; and
    learning relations between named entities.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个典型的文本分析应用被用来扫描一组文档以生成搜索索引时，文本挖掘可以应用于许多其他应用，包括将文本分类到特定领域；通过文本聚类自动组织一组文档；情感分析以识别和提取文档中的主观信息；概念或实体提取，能够从文档中识别人、地点、组织和其他实体；文档摘要以自动提供原始文档中的最重要观点；以及学习命名实体之间的关系。
- en: 'The process based on statistical pattern mining usually involves the following
    steps:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基于统计模式挖掘的过程通常涉及以下步骤：
- en: Information retrieval and extraction
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 信息检索和提取
- en: Transforming unstructured text data into structured data; for example, parsing,
    removing noisy words, lexical analysis, calculating word frequencies, and deriving
    linguistic features
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将非结构化文本数据转换为结构化数据；例如，解析、去除噪声词、词汇分析、计算词频和推导语言特征
- en: Discovery of patterns from structured data and tagging or annotation
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从结构化数据中发现模式并进行标记或注释
- en: Evaluation and interpretation of the results
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果的评估和解释
- en: 'Later in this chapter, we will look at two application areas: topic modeling
    and **text categorization**. Let''s examine what they bring to the table.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将探讨两个应用领域：主题建模和**文本分类**。让我们来看看它们能带来什么。
- en: Topic modeling
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: Topic modeling is an unsupervised technique and might be useful if you need
    to analyze a large archive of text documents and wish to understand what the archive
    contains, without necessarily reading every single document by yourself. A text
    document can be a blog post, an email, a tweet, a document, a book chapter, a
    diary entry, and so on. Topic modeling looks for patterns in a corpus of text;
    more precisely, it identifies topics as lists of words that appear in a statistically
    meaningful way. The most well-known algorithm is **Latent Dirichlet Allocation**
    (**LDA**), which assumes that the author composed a piece of text by selecting
    words from possible baskets of words, where each basket corresponds to a topic.
    Using this assumption, it becomes possible to mathematically decompose text into
    the most likely baskets from where the words first came. The algorithm then iterates
    over this process until it converges to the most likely distribution of words
    into baskets, which we call *topics*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是一种无监督技术，如果你需要分析大量的文本文档档案并希望了解档案包含的内容，这可能很有用，而不必亲自阅读每一份文档。文本文档可以是博客文章、电子邮件、推文、文档、书籍章节、日记条目等。主题建模在文本语料库中寻找模式；更确切地说，它通过统计上有意义的单词列表来识别主题。最著名的算法是**潜在狄利克雷分配**（**LDA**），它假设作者通过从可能的单词篮子中选择单词来创作文本，每个篮子对应一个主题。利用这个假设，可以将文本从最可能的单词来源篮子中数学分解出来。然后算法重复这个过程，直到收敛到最可能的单词分布到篮子中的分布，我们称之为*主题*。
- en: 'For example, if we use topic modeling on a series of news articles, the algorithm
    would return a list of topics and keywords that most likely comprise of these
    topics. Using the example of news articles, the list might look similar to the
    following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们对一系列新闻文章进行主题建模，算法将返回一个包含这些主题的可能关键词列表。以新闻文章为例，列表可能看起来像以下这样：
- en: Winner, goal, football, score, first place
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胜利者、进球、足球、得分、第一名
- en: Company, stocks, bank, credit, business
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司、股票、银行、信贷、商业
- en: Election, opponent, president, debate, upcoming
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选举、对手、总统、辩论、即将到来
- en: By looking at the keywords, we can recognize that the news articles were concerned
    with sports, business, upcoming election, and so on. Later in this chapter, we
    will learn how to implement topic modeling using the news article example.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看关键词，我们可以识别出新闻文章涉及体育、商业、即将到来的选举等内容。在本章的后面部分，我们将学习如何使用新闻文章的例子来实现主题建模。
- en: Text classification
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类
- en: 'In text classification, or text categorization, the goal is to assign a text
    document according to its content to one or more classes or categories, which
    tend to be a more general subject area, such as vehicles or pets. Such general
    classes are referred to as topics, and the classification task is then called
    **text classification**, **text categorization**, **topic classification**, or
    **topic spotting**. While documents can be categorized according to other attributes
    such as document type, author, and publication year, the focus in this chapter
    will be on the document content only. Examples of text classification include
    the following components:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分类，或文本分类中，目标是根据其内容将文本文档分配到一类或多类，这些类别通常是一个更广泛的主题领域，如车辆或宠物。这样的通用类别被称为主题，此时的分类任务被称为**文本分类**、**文本分类**、**主题分类**或**主题检测**。虽然文档可以根据其他属性进行分类，如文档类型、作者和出版年份，但本章的重点将仅限于文档内容。以下是一些文本分类的例子：
- en: Spam detection in email messages, user comments, web pages, and so on
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件中的垃圾邮件检测、用户评论、网页等
- en: Detection of sexually explicit content
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性内容检测
- en: Sentiment detection, which automatically classifies a product or service review
    as positive or negative
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感检测，它自动将产品或服务评论分类为正面或负面
- en: Email sorting according to content
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据内容对电子邮件进行排序
- en: Topic-specific search, where search engines restrict searches to a particular
    topic or genre, hence providing more accurate results
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题特定搜索，其中搜索引擎将搜索限制在特定主题或类型，从而提供更准确的结果
- en: These examples show how important text classification is in information retrieval
    systems; hence, most modern information retrieval systems use some kind of text
    classifier. The classification task that we will use as an example in this book
    is text classification for detecting email spam.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子显示了文本分类在信息检索系统中的重要性；因此，大多数现代信息检索系统都使用某种形式的文本分类器。本书中将用作例子的分类任务是检测电子邮件垃圾邮件的文本分类。
- en: We will continue this chapter with an introduction to Mallet, a Java-based package
    for statistical natural-language processing, document classification, clustering,
    topic modeling, information extraction, and other machine-learning applications
    to text. We will then cover two text analytics applications, namely, topics modeling
    and spam detection as text classification.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续本章，介绍Mallet，这是一个基于Java的用于统计自然语言处理、文档分类、聚类、主题建模、信息提取和其他机器学习应用文本的软件包。然后我们将介绍两个文本分析应用，即主题建模和作为文本分类的垃圾邮件检测。
- en: Installing Mallet
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Mallet
- en: 'Mallet is available for download at the UMass Amherst University website at [http://mallet.cs.umass.edu/download.php](http://mallet.cs.umass.edu/download.php).
    Navigate to the Download section as shown in the following screenshot and select
    the latest stable release (**2.0.8**, at the time of writing this book):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Mallet可在马萨诸塞大学阿默斯特分校网站[http://mallet.cs.umass.edu/download.php](http://mallet.cs.umass.edu/download.php)下载。导航到下载部分，如以下截图所示，并选择最新的稳定版本（**2.0.8**，本书编写时）：
- en: '![](img/8d8e3fb3-83a6-48cc-b297-3650b7e8140d.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d8e3fb3-83a6-48cc-b297-3650b7e8140d.png)'
- en: 'Download the ZIP file and extract the content. In the extracted directory,
    you should find a folder named `dist` with two JAR files: `mallet.jar` and `mallet-deps.jar`.
    The first one contains all of the packaged Mallet classes, while the second one
    packs all of the dependencies. We will include both JARs files in your project
    as referenced libraries, as shown in the following screenshot:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下载ZIP文件并提取内容。在提取的目录中，你应该找到一个名为`dist`的文件夹，其中包含两个JAR文件：`mallet.jar`和`mallet-deps.jar`。第一个包含所有打包的Mallet类，而第二个包含所有依赖项。我们将把这两个JAR文件作为引用库包含到你的项目中，如以下截图所示：
- en: '![](img/f4ea3672-2a16-4e49-a313-bdb4ae51066c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4ea3672-2a16-4e49-a313-bdb4ae51066c.png)'
- en: 'If you are using Eclipse, right-click on Project, select Properties, and pick
    Java Build Path. Select the Libraries tab and click Add External JARs. Now, select
    the two JARs files and confirm, as shown in the following screenshot:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Eclipse，右键单击项目，选择属性，然后选择Java构建路径。选择库选项卡，点击添加外部JAR文件。现在，选择两个JAR文件并确认，如以下截图所示：
- en: '![](img/5d0141f0-1ce7-4a70-a288-9857e639ec1f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d0141f0-1ce7-4a70-a288-9857e639ec1f.png)'
- en: Now we are ready to start using Mallet.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好开始使用Mallet了。
- en: Working with text data
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: 'One of the main challenges in text mining is transforming unstructured written
    natural language into structured attribute-based instances. The process involves
    many steps, as shown here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 文本挖掘的主要挑战之一是将非结构化的自然语言文本转换为基于属性的实例。这个过程涉及许多步骤，如下所示：
- en: '![](img/76c1e7b0-4d6a-4ed6-bff4-981855149687.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76c1e7b0-4d6a-4ed6-bff4-981855149687.png)'
- en: First, we extract some text from the internet, existing documents, or databases.
    At the end of the first step, the text could still be present in the XML format
    or some other proprietary format. The next step is to extract the actual text
    and segment it into parts of the document, for example, title, headline, abstract,
    and body. The third step is involved with normalizing text encoding to ensure
    the characters are presented in the same way; for example, documents encoded in
    formats such as ASCII, ISO 8859-1 and Windows-1250 are transformed into Unicode
    encoding. Next, tokenization splits the document into particular words, while
    the next step removes frequent words that usually have low predictive power, for
    example, the, a, I, and we.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从互联网、现有文档或数据库中提取一些文本。在第一步结束时，文本可能仍然以XML格式或其他专有格式存在。下一步是从实际文本中提取文本并将其分割成文档的部分，例如标题、标题、摘要和正文。第三步涉及对文本编码进行标准化，以确保字符以相同的方式呈现；例如，以ASCII、ISO
    8859-1和Windows-1250等格式编码的文档被转换为Unicode编码。接下来，分词将文档分割成特定的单词，而下一步则移除通常具有低预测能力的频繁单词，例如the、a、I和we。
- en: The **Part-Of-Speech** (**POS**) tagging and lemmatization step could be included
    to transform each token to its basic form, which is known as **lemma**, by removing
    word endings and modifiers. For example, running becomes run, and better becomes
    good. A simplified approach is stemming, which operates on a single word without
    any context of how the particular word is used, and therefore cannot distinguish
    between words having different meaning, depending on the part of speech, for example,
    axes as a plural of axe as well as axis.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**词性标注**（**POS**）和词形还原步骤可以包括将每个标记转换为它的基本形式，这被称为**词元**，通过去除词尾和修饰词来实现。例如，running变为run，而better变为good。一种简化的方法是词干提取，它在一个单词上操作，没有任何上下文，因此无法区分具有不同意义的单词，例如，axes作为axe的复数形式以及axis。'
- en: The last step transforms tokens into a feature space. Most often, feature space
    is a **Bag-Of-Words** (**BoW**) presentation. In this presentation, a set of all
    words appearing in the dataset is created. Each document is then presented as
    a vector that counts how many times a particular word appears in the document.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步将标记转换为特征空间。通常，特征空间是**词袋模型**（**BoW**）的表示。在这个表示中，创建了一个包含数据集中所有单词的集合。然后，每个文档都表示为一个向量，该向量统计特定单词在文档中出现的次数。
- en: 'Consider the following example with two sentences:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下两个句子的例子：
- en: Jacob likes table tennis. Emma likes table tennis too
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob likes table tennis. Emma likes table tennis too
- en: Jacob also likes basketball
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob also likes basketball
- en: 'The BoW in this case consists of {Jacob, likes, table, tennis, Emma, too, also,
    basketball}, which has eight distinct words. The two sentences could be now presented
    as vectors using the indexes of the list, indicating how many times a word at
    a particular index appears in the document, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，BoW（词袋模型）由以下单词组成：{Jacob, likes, table, tennis, Emma, too, also, basketball}，共有八个不同的单词。现在可以将这两个句子表示为向量，使用列表的索引表示特定索引处的单词在文档中出现的次数，如下所示：
- en: '[1, 2, 2, 2, 1, 0, 0, 0]'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 2, 2, 2, 1, 0, 0, 0]'
- en: '[1, 1, 0, 0, 0, 0, 1, 1]'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 1, 0, 0, 0, 0, 1, 1]'
- en: Such vectors finally become instances for further learning.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的向量最终成为进一步学习的实例。
- en: Another very powerful presentation based on the BoW model is **word2vec**. Word2vec
    was introduced in 2013 by a team of researchers led by Tomas Mikolov at Google.
    Word2vec is a neural network that learns distributed representations for words.
    An interesting property of this presentation is that words appear in clusters,
    so that some word relationships, such as analogies, can be reproduced using vector
    math. A famous example shows that king−man+woman returns queen. Further details
    and implementation are available at the following link: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于BoW模型的一个非常强大的展示方式是**word2vec**。Word2vec是由Google的研究团队，以Tomas Mikolov为首，在2013年引入的。Word2vec是一个神经网络，它学习单词的分布式表示。这种展示的一个有趣特性是单词出现在簇中，因此可以使用向量数学重现一些单词关系，例如类比。一个著名的例子表明，king−man+woman返回queen。更多细节和实现可以在以下链接中找到：[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/).
- en: Importing data
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入数据
- en: In this chapter, we will not look into how to scrap a set of documents from
    a website or extract them from database. Instead, we will assume that we have
    already collected them as set of documents and stored them in the `.txt` file
    format. Now let's look at two options for loading them. The first option addresses
    the situation where each document is stored in its own `.txt` file. The second
    option addresses the situation where all of the documents are stored in a single
    file by taking one per line.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不会探讨如何从网站抓取一组文档或从数据库中提取它们。相反，我们将假设我们已经将它们收集为文档集，并以`.txt`文件格式存储。现在让我们看看加载它们的两种选项。第一种选项解决的是每个文档都存储在其自己的`.txt`文件中的情况。第二种选项解决的是所有文档都存储在单个文件中，每行一个文档的情况。
- en: Importing from directory
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从目录导入
- en: 'Mallet supports reading from directory with the `cc.mallet.pipe.iterator.FileIterator`
    class. A file iterator is constructed with the following three parameters:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Mallet支持使用`cc.mallet.pipe.iterator.FileIterator`类从目录中读取。文件迭代器是通过以下三个参数构建的：
- en: A list of `File[]` directories with text files
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含文本文件的`File[]`目录列表
- en: A file filter that specifies which files to select within a directory
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文件过滤器，用于指定在目录中要选择哪些文件
- en: A pattern that is applied to a filename to produce a class label
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用到文件名上以产生类标签的模式
- en: 'Consider the data structured into folders as shown in the following screenshot.
    We have documents organized in five topics by folders (`tech`, `entertainment`,
    `politics`, `sport`, and `business`). Each folder contains documents on particular
    topics, as shown in the following screenshot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下截图所示的数据结构。我们按照五个主题（`tech`、`entertainment`、`politics`、`sport`和`business`）将文档组织在文件夹中。每个文件夹包含特定主题的文档，如下截图所示：
- en: '![](img/8c8de3e3-7eb8-4a67-adef-fc6b9320aecf.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8c8de3e3-7eb8-4a67-adef-fc6b9320aecf.png)'
- en: 'In this case, we initialize `iterator` as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们初始化`iterator`如下：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first parameter specifies the path to our root folder, the second parameter
    limits the iterator to the `.txt` files only, while the last parameter asks the
    method to use the last directory name in the path as class label.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数指定了我们的根文件夹的路径，第二个参数将迭代器限制为仅`.txt`文件，而最后一个参数要求方法使用路径中的最后一个目录名作为类标签。
- en: Importing from file
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文件导入
- en: 'Another option to load the documents is through `cc.mallet.pipe.iterator.CsvIterator.CsvIterator(Reader,
    Pattern, int, int, int)`, which assumes all of the documents are in a single file
    and returns one instance per line extracted by a regular expression. The class
    is initialized by the following components:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 加载文档的另一种选项是通过`cc.mallet.pipe.iterator.CsvIterator.CsvIterator(Reader, Pattern,
    int, int, int)`，它假设所有文档都在一个文件中，并且通过正则表达式提取每一行返回一个实例。该类通过以下组件初始化：
- en: '`Reader`: This is the object that specifies how to read from a file'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Reader`：这是一个对象，指定如何从文件中读取'
- en: '`Pattern`: This is a regular expression, extracting three groups: data, target
    label, and document name'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pattern`：这是一个正则表达式，提取三个组：数据、目标标签和文档名'
- en: '`int, int, int`: These are the indexes of data, target, and name groups as
    they appear in a regular expression'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`int, int, int`：这些是正则表达式中数据、目标和名称组的索引'
- en: 'Consider a text document in the following format, specifying the document name,
    category, and content:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下格式的文本文档，指定文档名称、类别和内容：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To parse a line into three groups, we can use the following regular expression:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要将一行解析为三个组，我们可以使用以下正则表达式：
- en: '[PRE2]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are three groups that appear in parenthesies `()`, where the third group
    contains the data, the second group contains the target class, and the first group
    contains the document ID. `iterator` is initialized as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在括号`()`中出现了三个组，其中第三个组包含数据，第二个组包含目标类，第一个组包含文档ID。`iterator`的初始化如下：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, the regular expression extracts the three groups separated by an empty
    space and their order is `3, 2, 1`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，正则表达式提取了由空格分隔的三个组，其顺序为`3, 2, 1`。
- en: Now let's move to the data-preprocessing pipeline.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向数据预处理管道。
- en: Pre-processing text data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理文本数据
- en: 'Once we initialize an iterator that will go through the data, we need to pass
    the data through a sequence of transformations as described at the beginning of
    this section. Mallet supports this process through a pipeline and a wide variety
    of steps that could be included in a pipeline, which are collected in the `cc.mallet.pipe`
    package. Some examples are as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化了一个将遍历数据的迭代器，我们需要将数据通过本节开头所述的一系列转换传递。Mallet通过管道和一系列可以包含在管道中的步骤来支持此过程，这些步骤收集在`cc.mallet.pipe`包中。以下是一些示例：
- en: '`Input2CharSequence`: This is a pipe that can read from various kinds of text
    sources (either URL, file, or reader) into `CharSequence`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Input2CharSequence`：这是一个管道，可以从各种文本源（无论是URL、文件还是读取器）读取到`CharSequence`'
- en: '`CharSequenceRemoveHTML`: This pipe removes HTML from `CharSequence`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CharSequenceRemoveHTML`：这个管道从`CharSequence`中移除HTML'
- en: '`MakeAmpersandXMLFriendly`: This converts `&` into `&amp` in tokens of a token
    sequence'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MakeAmpersandXMLFriendly`：这会将标记序列中的`&`转换为`&amp`'
- en: '`TokenSequenceLowercase`: This converts the text in each token in the token
    sequence in the data field into lowercase'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TokenSequenceLowercase`：这会将数据字段中每个标记序列中的文本转换为小写'
- en: '`TokenSequence2FeatureSequence`: This converts the token sequence in the data
    field of each instance into a feature sequence'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TokenSequence2FeatureSequence`：这会将每个实例的数据字段中的标记序列转换为特征序列'
- en: '`TokenSequenceNGrams`: This converts the token sequence in the data field into
    a token sequence of ngrams, that is, a combination of two or more words'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TokenSequenceNGrams`：这会将数据字段中的标记序列转换为ngrams的标记序列，即两个或更多单词的组合'
- en: The full list of processing steps is available in the following Mallet documentation: [http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html](http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的处理步骤列表可以在以下Mallet文档中找到：[http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html](http://mallet.cs.umass.edu/api/index.html?cc/mallet/pipe/iterator/package-tree.html)。
- en: 'Now we are ready to build a class that will import our data. We will do that
    using the following steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好构建一个将导入我们的数据的类。我们将使用以下步骤来完成：
- en: 'Let''s build a pipeline, where each processing step is denoted as a pipeline
    in Mallet. Pipelines can be wired together in a serial fashion with a list of
    `ArrayList<Pipe>` objects:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们构建一个管道，其中每个处理步骤在Mallet中用一个管道表示。管道可以通过`ArrayList<Pipe>`对象列表以串行方式连接起来：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s begin by reading data from a file object and converting all of the characters
    into lowercase:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从读取文件对象中的数据并转换所有字符为小写开始：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will tokenize raw strings with a regular expression. The following pattern
    includes unicode letters and numbers and the underscore character:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用正则表达式对原始字符串进行分词。以下模式包括Unicode字母、数字和下划线字符：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will now remove stop words, that is, frequent words with no predictive power,
    using a standard English stop list. Two additional parameters indicate whether
    stop-word removal should be case-sensitive and mark deletions instead of just
    deleting the words. We''ll set both of them to `false`:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将移除停用词，即没有预测能力的频繁单词，使用标准的英语停用词表。两个额外的参数指示是否应区分大小写以及是否标记删除而不是简单地删除单词。我们将它们都设置为`false`：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Instead of storing the actual words, we can convert them into integers, indicating
    a word index in the BoW:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将实际的单词转换为整数，表示在BoW（词袋模型）中的单词索引，而不是存储实际的单词：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We''ll do the same for the class label; instead of the label string, we''ll
    use an integer, indicating a position of the label in our bag of words:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将对类别标签做同样的处理；而不是使用标签字符串，我们将使用一个整数，表示标签在词袋中的位置：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We could also print the features and the labels by invoking the `PrintInputAndTarget`
    pipe:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也可以通过调用`PrintInputAndTarget`管道来打印特征和标签：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We store the list of pipelines in a `SerialPipes` class that will covert an
    instance through a sequence of pipes:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将管道列表存储在`SerialPipes`类中，该类将通过一系列管道转换实例：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now let's take a look at how apply this in a text-mining application!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在文本挖掘应用中应用这个方法！
- en: Topic modeling for BBC News
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BBC新闻的主题建模
- en: As discussed earlier, the goal of topic modeling is to identify patterns in
    a text corpus that correspond to document topics. In this example, we will use
    a dataset originating from BBC News. This dataset is one of the standard benchmarks
    in machine-learning research, and is available for non-commercial and research
    purposes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，主题建模的目标是识别文本语料库中与文档主题相对应的模式。在这个例子中，我们将使用来自BBC新闻的数据集。这个数据集是机器学习研究中的标准基准之一，可用于非商业和研究目的。
- en: The goal is to build a classifier that is able to assign a topic to an uncategorized
    document.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是构建一个能够将一个未分类的文档分配到某个主题的分类器。
- en: BBC dataset
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BBC数据集
- en: 'In 2006, Greene and Cunningham collected the BBC dataset to study a particular
    document—*Clustering challenge using support vector machines*. The dataset consists
    of 2,225 documents from the BBC News website from 2004 to 2005, corresponding
    to the stories collected from five topical areas: business, entertainment, politics,
    sport, and technology. The dataset can be seen at the following website: [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年，格林和坎宁安收集了BBC数据集来研究一个特定的文档——*使用支持向量机的聚类挑战*。该数据集包含来自2004年至2005年BBC新闻网站上的2,225个文档，对应于从五个主题领域收集的故事：商业、娱乐、政治、体育和技术。该数据集可以在以下网站上查看：[http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html)。
- en: 'We can download the raw text files under the Dataset: BBC section. You will
    also notice that the website contains an already processed dataset, but, for this
    example, we want to process the dataset by ourselves. The ZIP contains five folders,
    one per topic. The actual documents are placed in the corresponding topic folder,
    as shown in the following screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在“数据集：BBC”部分下下载原始文本文件。你还会注意到该网站包含一个已经处理过的数据集，但在这个例子中，我们想要自己处理数据集。ZIP文件包含五个文件夹，每个文件夹对应一个主题。实际的文档放置在相应的主题文件夹中，如下面的截图所示：
- en: '![](img/c7869097-c8f5-4e07-a09c-1c7995546906.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c7869097-c8f5-4e07-a09c-1c7995546906.png)'
- en: Now, let's build a topic classifier.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个主题分类器。
- en: Modeling
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模
- en: 'We will begin the modeling phase using the following steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下步骤开始建模阶段：
- en: 'We will start by importing the dataset and processing the text using the following
    lines of code:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入数据集并使用以下代码行处理文本：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will then create a default `pipeline`  object as previously described:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个默认的`pipeline`对象，如之前所述：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will initialize the `folderIterator` object:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化`folderIterator`对象：
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will now construct a new instance list with the `pipeline` that we want
    to use to process the text:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将使用我们想要用于处理文本的`pipeline`构建一个新的实例列表：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We process each instance provided by  `iterator`:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们处理`iterator`提供的每个实例：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now let's create a model with five topics using the `cc.mallet.topics.ParallelTopicModel.ParallelTopicModel`
    class that implements a simple threaded LDA model. LDA is a common method for
    topic modeling that uses Dirichlet distribution to estimate the probability that
    a selected topic generates a particular document. We will not dive deep into the
    details in this chapter; the reader is referred to the original paper by D. Blei
    et al. (2003).
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们使用`cc.mallet.topics.ParallelTopicModel.ParallelTopicModel`类创建一个具有五个主题的模型，该类实现了一个简单的线程化LDA模型。LDA是主题建模中常用的方法，它使用狄利克雷分布来估计所选主题生成特定文档的概率。我们不会在本章深入探讨细节；读者可参考D.
    Blei等人（2003年）的原始论文。
- en: 'Note: There is another classification algorithm in machine learning with the
    same initialism that refers to **Linear Discriminant Analysis** (**LDA**). Beside
    the common acronym, it has nothing in common with the LDA model.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在机器学习中，还有一个具有相同缩写的分类算法，它指的是**线性判别分析**（**LDA**）。除了常见的缩写外，它与LDA模型没有共同之处。
- en: 'The class is instantiated with parameters alpha and beta, which can be broadly
    interpreted as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 该类使用alpha和beta参数实例化，可以广泛解释如下：
- en: High alpha value means that each document is likely to contain a mixture of
    most of the topics, and not any single topic specifically. A low alpha value puts
    less of such constraints on documents, and this means that it is more likely that
    a document may contain mixture of just a few, or even only one, of the topics.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高alpha值意味着每个文档很可能包含大多数主题的混合，而不是任何单个主题。低alpha值对文档的这种约束较少，这意味着文档可能只包含少数，甚至只有一个主题的混合。
- en: A high beta value means that each topic is likely to contain a mixture of most
    of the words, and not any word specifically; while a low value means that a topic
    may contain a mixture of just a few of the words.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高beta值意味着每个主题很可能包含大多数单词的混合，而不是任何特定的单词；而低值则意味着一个主题可能只包含少数单词的混合。
- en: 'In our case, we initially keep both parameters low (alpha_t = `0.01`, beta_w
    = `0.01`) as we assume topics in our dataset are not mixed much and there are
    many words for each of the topics:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们最初将两个参数都保持得很低（alpha_t = `0.01`, beta_w = `0.01`），因为我们假设我们的数据集中的主题混合不多，每个主题都有许多单词：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will add `instances` to the model, and since we are using parallel implementation,
    we will specify the number of threads that will run in parallel, as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将向模型添加`instances`，由于我们使用并行实现，我们将指定并行运行的线程数，如下所示：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will now run the model for a selected number of iterations. Each iteration
    is used for better estimation of internal LDA parameters. For testing, we can
    use a small number of iterations, for example, 50; while in real applications,
    use `1000` or `2000` iterations. Finally, we will call the `void estimate() `method
    that will actually build an LDA model:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将运行模型进行选定次数的迭代。每次迭代都用于更好地估计内部LDA参数。对于测试，我们可以使用少量迭代，例如，50次；而在实际应用中，使用`1000`或`2000`次迭代。最后，我们将调用`void
    estimate()`方法，实际上将构建一个LDA模型：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The model outputs the following result:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出以下结果：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`LL/token` indicates the model''s log-likelihood, divided by the total number
    of tokens, indicating how likely the data is given the model. Increasing values
    mean the model is improving.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`LL/token`表示模型的对数似然，除以总标记数，表示数据在模型下的可能性。值的增加意味着模型正在改进。'
- en: 'The output also shows the top words describing each topic. The words correspond
    to initial topics really well:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 输出还显示了描述每个主题的前几个单词。这些单词与初始主题非常吻合：
- en: '**Topic 0**: `game`, `england`, `year`, `time`, `win`, `world`, `6` ⇒ sport'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题0**：`game`、`england`、`year`、`time`、`win`、`world`、`6` ⇒ 体育'
- en: '**Topic 1**: `year`, `1`, `company`, `market`, `growth`, `economy`, `firm` ⇒
    finance'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题1**：`year`、`1`、`company`、`market`、`growth`、`economy`、`firm` ⇒ 金融'
- en: '**Topic 2**: `people`, `technology`, `mobile`, `mr`, `games`, `users`, `music`
    ⇒ tech'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题2**：`people`、`technology`、`mobile`、`mr`、`games`、`users`、`music`⇒技术'
- en: '**Topic 3**: `film`, `year`, `music`, `show`, `awards`, `award`, `won` ⇒ entertainment'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题3**：`film`、`year`、`music`、`show`、`awards`、`award`、`won`⇒娱乐'
- en: '**Topic 4**: `mr`, `government`, `people`, `labor`, `election`, `party`, `blair`
    ⇒ politics'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题4**：`mr`、`government`、`people`、`labor`、`election`、`party`、`blair`⇒政治'
- en: There are still some words that don't make much sense, for instance, `mr`, `1`,
    and `6`. We could include them in the stop word list. Also, some words appear
    twice, for example, `award` and `awards`. This happened because we didn't apply
    any stemmer or lemmatization pipe.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有一些词没有太多意义，例如`mr`、`1`和`6`。我们可以将它们包含在停用词列表中。此外，一些词出现了两次，例如`award`和`awards`。这是因为我们没有应用任何词干提取或词形还原管道。
- en: In the next section, we'll take a look to check whether the model is any good.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将检查模型是否表现良好。
- en: Evaluating a model
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: As statistical topic modeling has an unsupervised nature, it makes model selection
    difficult. For some applications, there may be some extrinsic tasks at hand, such
    as information retrieval or document classification, for which performance can
    be evaluated. However, in general, we want to estimate the model's ability to
    generalize topics regardless of the task.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于统计主题建模具有无监督的性质，这使得模型选择变得困难。对于某些应用，可能有一些外在任务在手，例如信息检索或文档分类，其性能可以评估。然而，一般来说，我们想要估计模型泛化主题的能力，无论任务如何。
- en: In 2009, Wallach et al. introduced an approach that measures the quality of
    a model by computing the log probability of held-out documents under the model.
    The likelihood of unseen documents can be used to compare models—higher likelihood
    implies a better model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 2009年，Wallach等人提出了一种通过在模型下计算保留文档的对数概率来衡量模型质量的方法。未观察到的文档的似然性可以用来比较模型——更高的似然性意味着更好的模型。
- en: 'We will evaluate the model using the following steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下步骤评估模型：
- en: 'Let''s split the documents into training and test sets (that is, held-out documents),
    where we use 90% for training and 10% for testing:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将文档分为训练集和测试集（即保留的文档），其中我们使用90%进行训练，10%进行测试：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let''s rebuild our model using only `90%` of our documents:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们仅使用我们文档的`90%`重建我们的模型：
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will initialize an `estimator` object that implements Wallach''s log probability
    of held-out documents, `MarginalProbEstimator`:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将初始化一个实现Wallach的保留文档对数概率的`estimator`对象，即`MarginalProbEstimator`：
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: An intuitive description of LDA is summarized by Annalyn Ng in her blog: [https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/](https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/). To
    get deeper insight into the LDA algorithm, its components, and its working, take
    a look at the original paper LDA by David Blei et al. (2003) at [http://jmlr.csail.mit.edu/papers/v3/blei03a.html](http://jmlr.csail.mit.edu/papers/v3/blei03a.html),
    or take a look at the summarized presentation by D. Santhanam of Brown University
    at [http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf](http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的直观描述由Annalyn Ng在她的博客中总结：[https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/](https://annalyzin.wordpress.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2/)。要深入了解LDA算法、其组件和工作原理，请查看David
    Blei等人于2003年发表的原始论文LDA，网址为[http://jmlr.csail.mit.edu/papers/v3/blei03a.html](http://jmlr.csail.mit.edu/papers/v3/blei03a.html)，或者查看布朗大学D.
    Santhanam的总结演示，网址为[http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf](http://www.cs.brown.edu/courses/csci2950-p/spring2010/lectures/2010-03-03_santhanam.pdf)。
- en: 'The class implements many estimators that require quite deep theoretical knowledge
    of how the LDA method works. We''ll pick the left-to-right evaluator, which is
    appropriate for a wide range of applications, including text mining, and speech
    recognition. The left-to-right evaluator is implemented as the `double evaluateLeftToRight`
    method, accepting the following components:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该类实现了许多需要相当深入的理论知识来了解LDA方法如何工作的估计器。我们将选择从左到右的评估器，它适用于广泛的用途，包括文本挖掘和语音识别。从左到右的评估器作为`double
    evaluateLeftToRight`方法实现，接受以下组件：
- en: '`Instances heldOutDocuments`: This tests the instances.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Instances heldOutDocuments`：这测试实例。'
- en: '`int numParticles`: This algorithm parameter indicates the number of left-to-right
    tokens, where the default value is 10.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`int numParticles`：此算法参数表示从左到右的标记数量，默认值为10。'
- en: '`boolean useResampling`: This states whether to resample topics in left-to-right
    evaluation; resampling is more accurate, but leads to quadratic scaling in the
    length of documents.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boolean useResampling`：这表示是否在从左到右评估时重新采样主题；重新采样更准确，但会导致文档长度的平方级缩放。'
- en: '`PrintStream docProbabilityStream`: This is the file or `stdout` in which we
    write the inferred log probabilities per document.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PrintStream docProbabilityStream`：这是写入每个文档推断的对数概率的文件或`stdout`。'
- en: 'Let''s run  `estimator`, as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们按照以下方式运行`estimator`：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In our particular case, the `estimator` outputs the following `log likelihood`,
    which makes sense when it is compared to other models that are either constructed
    with different parameters, pipelines, or data—the higher the log likelihood, the
    better the model is:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的特定情况下，`estimator`输出了以下`log likelihood`，当与其他使用不同参数、管道或数据构建的模型进行比较时是有意义的——对数似然越高，模型越好：
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now let's take a look at how to make use of this model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何利用这个模型。
- en: Reusing a model
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重复使用模型
- en: As we are usually not building models on the fly, it often makes sense to train
    a model once and use it repeatedly to classify new data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通常不会即时构建模型，因此一次性训练一个模型并重复使用它来分类新数据通常是有意义的。
- en: Note that, if you'd like to classify new documents, they need go through the
    same pipeline as other documents—the pipe needs to be the same for both training
    and classification. During training, the pipe's data alphabet is updated with
    each training instance. If you create a new pipe with the same steps, you don't
    produce the same pipeline as its data alphabet is empty. Therefore, to use the
    model on new data, we have to save or load the pipe along with the model and use
    this pipe to add new instances.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果您想对新的文档进行分类，它们需要经过与其他文档相同的管道——管道对于训练和分类都应该是相同的。在训练过程中，管道的数据字母表会随着每个训练实例的更新而更新。如果您创建了一个具有相同步骤的新管道，您不会产生相同的管道，因为其数据字母表是空的。因此，为了在新数据上使用模型，我们必须保存或加载管道以及模型，并使用此管道添加新实例。
- en: Saving a model
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存模型
- en: Mallet supports a standard method for saving and restoring objects based on
    serialization.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Mallet支持基于序列化的保存和恢复对象的标准方法。
- en: 'We simply create a new instance of the `ObjectOutputStream` class and write
    the object into a file, as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需创建一个`ObjectOutputStream`类的新实例，并将对象写入文件，如下所示：
- en: '[PRE26]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Restoring a model
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恢复模型
- en: 'Restoring a model saved through serialization is simply an inverse operation
    using the `ObjectInputStream` class:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通过序列化保存的模型恢复是一个使用`ObjectInputStream`类的逆操作：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We discussed how to build an LDA model to automatically classify documents into
    topics. In the next example, we'll look into another text mining problem—text
    classification.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了如何构建一个LDA模型来自动将文档分类到主题。在下一个示例中，我们将探讨另一个文本挖掘问题——文本分类。
- en: Detecting email spam
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测邮件垃圾邮件
- en: 'Spam or electronic spam refers to unsolicited messages, typically carrying
    advertising content, infected attachments, links to phishing or malware sites,
    and so on. While the most widely recognized form of spam is email spam, spam abuses
    appear in other media as well: website comments, instant messaging, internet forums,
    blogs, online ads, and so on.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件或电子垃圾邮件是指未经请求的消息，通常携带广告内容、感染附件、钓鱼网站或恶意软件链接等。虽然最广为人知的垃圾邮件形式是电子邮件垃圾邮件，但垃圾邮件滥用也出现在其他媒体中：网站评论、即时消息、互联网论坛、博客、在线广告等。
- en: In this chapter, we will discuss how to build Naive Bayesian spam filtering,
    using BoW representation to identify spam emails. Naive Bayes spam filtering is
    one of the basic techniques that was implemented in the first commercial spam
    filters; for instance, Mozilla Thunderbird mail client uses native implementation
    of such filtering. While the example in this chapter will use email spam, the
    underlying methodology can be applied to other type of text-based spam as well.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何使用BoW表示法构建朴素贝叶斯垃圾邮件过滤，这是一种在第一个商业垃圾邮件过滤器中实现的基本技术；例如，Mozilla Thunderbird邮件客户端使用此类过滤的原生实现。虽然本章的示例将使用邮件垃圾邮件，但底层方法也可以应用于其他类型的基于文本的垃圾邮件。
- en: Email spam dataset
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 邮件垃圾邮件数据集
- en: In 2000, Androutsopoulos et al. collected one of the first email spam datasets
    to benchmark spam-filtering algorithms. They studied how the Naive Bayes classifier
    can be used to detect spam, if additional pipes such as stop list, stemmer, and
    lemmatization contribute to better performance. The dataset was reorganized by
    Andrew Ng in OpenClassroom's machine-learning class, available for download at
    [http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年，安德鲁特斯波洛斯等人收集了第一个电子邮件垃圾邮件数据集，用于评估垃圾邮件过滤算法。他们研究了如何使用朴素贝叶斯分类器来检测垃圾邮件，以及额外的管道，如停用词列表、词干提取和词形还原是否有助于提高性能。该数据集由Andrew
    Ng在OpenClassroom的机器学习课程中重新组织，可在[http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html](http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex6/ex6.html)下载。
- en: 'Select and download the second option, `ex6DataEmails.zip`, as shown in the
    following screenshot:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 选择并下载第二个选项，`ex6DataEmails.zip`，如图所示：
- en: '![](img/089d7c7f-00e3-4f7c-beff-9c635d90401c.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/089d7c7f-00e3-4f7c-beff-9c635d90401c.png)'
- en: 'The ZIP contains the following folders:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ZIP文件包含以下文件夹：
- en: The `nonspam-train` and `spam-train` folders contain the pre-processed emails
    that you will use for training. They have 350 emails each.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nonspam-train`和`spam-train`文件夹包含用于训练的预处理电子邮件，每个文件夹有350封电子邮件。'
- en: The `nonspam-test` and `spam-test` folders constitute the test set, containing
    130 spam and 130 nonspam emails. These are the documents that you will make predictions
    on. Notice that, even though separate folders tell you the correct labeling, you
    should make your predictions on all of the test documents without this knowledge.
    After you make your predictions, you can use the correct labeling to check whether
    your classifications were correct.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nonspam-test`和`spam-test`文件夹构成了测试集，包含130封垃圾邮件和130封非垃圾邮件。这些是你将进行预测的文档。请注意，尽管单独的文件夹告诉你正确的标签，但你应该在没有这种知识的情况下对所有测试文档进行预测。在你做出预测后，你可以使用正确的标签来检查你的分类是否正确。'
- en: 'To leverage Mallet''s folder iterator, let''s reorganize the folder structure
    as follows. We will create two folders, `train` and `test`, and put the `spam/nospam`
    folders under the corresponding folders. The initial folder structure is as shown
    in the following screenshot:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用Mallet的文件夹迭代器，让我们重新组织文件夹结构如下。我们将创建两个文件夹，`train`和`test`，并将`spam/nospam`文件夹放在相应的文件夹下。初始文件夹结构如图所示：
- en: '![](img/d70d02fe-2552-4cdf-a0d7-5a65f00d0f72.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d70d02fe-2552-4cdf-a0d7-5a65f00d0f72.png)'
- en: 'The final folder structure will be as shown in the following screenshot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最终文件夹结构将如图所示：
- en: '![](img/8fc63051-3265-49f9-915c-0efac24c48a2.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8fc63051-3265-49f9-915c-0efac24c48a2.png)'
- en: The next step is to transform email messages to feature vectors.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将电子邮件消息转换为特征向量。
- en: Feature generation
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征生成
- en: 'We will perform feature generation using the following steps:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下步骤进行特征生成：
- en: 'We will create a default pipeline, as described previously:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个默认的管道，如前所述：
- en: '[PRE28]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that we added an additional `FeatureSequence2FeatureVector` pipe that transforms
    a feature sequence into a feature vector. When we have data in a feature vector,
    we can use any classification algorithm, as we saw in the previous chapters. We'll
    continue our example in Mallet to demonstrate how to build a classification model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们添加了一个额外的`FeatureSequence2FeatureVector`管道，它将特征序列转换为特征向量。当我们有特征向量中的数据时，我们可以使用任何分类算法，就像我们在前面的章节中看到的那样。我们将在Mallet中继续我们的例子，以展示如何构建分类模型。
- en: 'We initialize a folder iterator to load our examples in the `train` folder
    comprising email examples in the `spam` and `nonspam` subfolders, which will be
    used as example labels:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化一个文件夹迭代器，以加载`train`文件夹中的示例，该文件夹包含`spam`和`nonspam`子文件夹中的电子邮件示例，这些将用作示例标签：
- en: '[PRE29]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will construct a new instance list with the `pipeline` object that we want
    to use to process the text:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用我们想要用于处理文本的`pipeline`对象构建一个新的实例列表：
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will process each instance provided by the iterator:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将处理迭代器提供的每个实例：
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We have now loaded the data and transformed it into feature vectors. Let's train
    our model on the training set and predict the `spam/nonspam` classification on
    the `test` set.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经加载数据并将其转换为特征向量。现在让我们在训练集上训练我们的模型，并在测试集上预测`spam/nonspam`分类。
- en: Training and testing
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和测试
- en: 'Mallet implements a set of classifiers in the `cc.mallet.classify` package,
    including decision trees, Naive Bayes, AdaBoost, bagging, boosting, and many others.
    We''ll start with a basic classifier, that is, a Naive Bayes classifier. A classifier
    is initialized by the `ClassifierTrainer` class, which returns a classifier when
    we invoke its `train(Instances)` method:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Mallet在`cc.mallet.classify`包中实现了一系列分类器，包括决策树、朴素贝叶斯、AdaBoost、bagging、boosting等。我们将从一个基本的分类器开始，即朴素贝叶斯分类器。分类器通过`ClassifierTrainer`类初始化，当我们调用其`train(Instances)`方法时返回一个分类器：
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now let's see how this classier works and evaluate its performance on a separate
    dataset.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看这个更高级的类是如何工作的，并评估它在独立数据集上的性能。
- en: Model performance
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能
- en: 'To evaluate the classifier on a separate dataset, we will use the following
    steps:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在独立数据集上评估分类器，我们将使用以下步骤：
- en: 'Let''s start by importing the emails located in our `test` folder:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入位于我们的`test`文件夹中的电子邮件：
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will pass the data through the same pipeline that we initialized during
    training:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用与训练期间初始化相同的管道传递数据：
- en: '[PRE34]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To evaluate classifier performance, we''ll use the `cc.mallet.classify.Trial`
    class, which is initialized with a classifier and set of test instances:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估分类器的性能，我们将使用`cc.mallet.classify.Trial`类，该类使用分类器和一组测试实例进行初始化：
- en: '[PRE35]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The evaluation is performed immediately at initialization. We can then simply
    take out the measures that we care about. In our example, we''d like to check
    the precision and recall on classifying spam email messages, or F-measure, which
    returns a harmonic mean of both values, as follows:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估在初始化时立即执行。然后我们可以简单地取出我们关心的度量。在我们的例子中，我们想检查分类垃圾邮件消息的精确率和召回率，或者F度量，它返回这两个值的调和平均值，如下所示：
- en: '[PRE36]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The evaluation object outputs the following results:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 评估对象输出以下结果：
- en: '[PRE37]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The results show that the model correctly discovers 97.69% of spam messages
    (recall), and when it marks an email as spam, it is correct in 96.94% cases. In
    other words, it misses approximately 2 per 100 spam messages and marks 3 per 100
    valid messages as spam. So, it's not really perfect, but it's more than a good
    start!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，该模型正确地发现了97.69%的垃圾邮件（召回率），并且当它标记一封电子邮件为垃圾邮件时，它在96.94%的情况下是正确的。换句话说，它大约每100封垃圾邮件中遗漏2封，并将每100封有效邮件中的3封标记为垃圾邮件。所以，它并不完美，但已经是一个很好的开始了！
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how text mining is different than traditional
    attribute-based learning, requiring a lot of pre-processing steps to transform
    written natural language into feature vectors. Further, we discussed how to leverage
    Mallet, a Java-based library for NLP by applying it to two real-life problems.
    First, we modeled topics in a news corpus using the LDA model to build a model
    that is able to assign a topic to new document. We also discussed how to build
    a Naive Bayesian spam-filtering classifier using the BoW representation.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了文本挖掘与传统的基于属性的学习的不同之处，需要大量的预处理步骤将书面自然语言转换为特征向量。此外，我们还讨论了如何利用Mallet，一个基于Java的NLP库，通过将其应用于两个现实生活中的问题来应用它。首先，我们使用LDA模型在新闻语料库中建模主题，以构建一个能够为新文档分配主题的模型。我们还讨论了如何使用BoW表示法构建基于朴素贝叶斯的垃圾邮件过滤分类器。
- en: This chapter concludes the technical demonstrations of how to apply various
    libraries to solve machine-learning tasks. As we weren't able to cover more interesting
    applications and give further details at many points, the next chapter gives some
    further pointers on how to continue learning and dive deeper into particular topics.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了如何应用各种库来解决机器学习任务的演示。由于我们无法在许多地方涵盖更多有趣的应用和提供更多细节，下一章将给出一些进一步的学习指针，如何继续学习并深入了解特定主题。
