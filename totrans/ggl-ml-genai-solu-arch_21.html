<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer225" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-358"><a id="_idTextAnchor420" class="calibre6 pcalibre pcalibre1"/>18</h1>
<h1 id="_idParaDest-359" class="calibre5"><a id="_idTextAnchor421" class="calibre6 pcalibre pcalibre1"/>Bringing It All Together: Building ML Solutions with Google Cloud and Vertex AI</h1>
<p class="calibre3">You’ve finally made it! This is the last chapter in our book. In this chapter, we will bring together the topics we’ve learned in this book by building a couple of example solutions that combine multiple concepts and Google Cloud services we discussed throughout the book. This chapter focuses mainly on building solutions, so it will be short on textual content, and the primary activities will be outlined in the Jupyter Notebook files accompanying <span>the chapter.</span></p>
<p class="calibre3">We will build two solutions in the chapter: the first will be a deeper dive into <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>), and<a id="_idIndexMarker2155" class="calibre6 pcalibre pcalibre1"/> the second will be an end-to-end solution that combines traditional <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), MLOps, and <a id="_idIndexMarker2156" class="calibre6 pcalibre pcalibre1"/><span>generative AI.</span></p>
<p class="calibre3">The topics in this chapter are <span>as follows:</span></p>
<ul class="calibre16">
<li class="calibre8">Building a RAG implementation piece <span>by piece</span></li>
<li class="calibre8">An example business use case and <span>reference architecture</span></li>
<li class="calibre8">Building and implementing the <span>use case</span></li>
<li class="calibre8">Recap and <span>next steps</span></li>
</ul>
<p class="calibre3">Before we dive into the architectural details, we need to perform a quick prerequisite step to set up the required environment settings in our Google <span>Cloud project.</span></p>
<h1 id="_idParaDest-360" class="calibre5"><a id="_idTextAnchor422" class="calibre6 pcalibre pcalibre1"/>Prerequisites</h1>
<p class="calibre3">This section describes the steps we need to perform to set up our environment for the implementation steps later in <span>this chapter.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In <a href="B18143_04.xhtml#_idTextAnchor146" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 4</em></span></a> of this book, we created a local directory in our Cloud Shell environment and cloned our GitHub repository into that directory. If you did not perform those steps, please reference those instructions now, in the section named <em class="italic">Create a directory and clone our GitHub repository</em> in <a href="B18143_04.xhtml#_idTextAnchor146" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 4</em></span></a><span>.</span></p>
<p class="callout">When you have ensured that the GitHub repository for this book has been cloned into the local directory in your Cloud Shell environment, continue with the prerequisite steps in <span>this section.</span></p>
<h2 id="_idParaDest-361" class="calibre9"><a id="_idTextAnchor423" class="calibre6 pcalibre pcalibre1"/>Using the Google Cloud Shell</h2>
<p class="calibre3">We will use the <a id="_idIndexMarker2157" class="calibre6 pcalibre pcalibre1"/>Google Cloud Shell to perform the activities in this section. You can access it by clicking the Cloud Shell symbol in the top-right corner of the Google Cloud console screen, as shown in <span><em class="italic">Figure 18</em></span><span><em class="italic">.1</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer221">
<img alt="Figure 18.1: Activating the Cloud Shell" src="image/B18143_18_1.jpg" class="calibre206"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 18.1: Activating the Cloud Shell</p>
<p class="calibre3">Open the Google Cloud Shell as shown in <span><em class="italic">Figure 18</em></span><em class="italic">.1</em>, and perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Change the directory to the location at which the code for this chapter <span>is stored:</span><pre class="source-code">
cd ~/packt-ml-sa/Google-Machine-Learning-for-Solutions-Architects/Chapter-18/prereqs/</pre></li> <li class="calibre8">Run the <span><strong class="source-inline">chapter-18-prereqs.sh</strong></span><span> script:</span><pre class="source-code">
bash chapter-18-prereqs.sh</pre></li> <li class="calibre8">If you are prompted to authorize the Cloud Shell, <span>click </span><span><strong class="bold">Authorize</strong></span><span>.</span><p class="calibre3">The script performs the following steps on <span>our behalf:</span></p><ol class="calibre77"><li class="upper-roman">Grant the <strong class="bold">Eventarc Event Receiver</strong> IAM role to our project’s default Compute Engine service account (our Jupyter Notebook, Cloud Functions, and other components use this service account). If you want to use a service account other than the default Compute Engine service account, you would need to grant the <strong class="bold">Eventarc Event Receiver</strong> IAM role to that service <span>account instead.</span></li><li class="upper-roman">Grant the <strong class="bold">Service Account Token Creator</strong> IAM role to our project’s Pub/Sub service agent. This is required for when we use Pub/Sub later in <span>this chapter.</span></li></ol></li>
</ol>
<p class="calibre3">Now that the prerequisites have been completed, we can continue with the main content of the <a id="_idIndexMarker2158" class="calibre6 pcalibre pcalibre1"/>chapter. The first solution we will focus on is building a RAG implementation piece <span>by piece.</span></p>
<h1 id="_idParaDest-362" class="calibre5"><a id="_idTextAnchor424" class="calibre6 pcalibre pcalibre1"/>Building a RAG implementation piece by piece</h1>
<p class="calibre3">In <a href="B18143_17.xhtml#_idTextAnchor408" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 17</em></span></a>, we implemented a RAG solution using Vertex AI Search. I explained that Vertex AI Search makes<a id="_idIndexMarker2159" class="calibre6 pcalibre pcalibre1"/> the process very easy for us because it abstracts away the steps in the process, such as chunking and embedding our content, and it performs all of those steps for us behind the scenes. There are also popular frameworks, such as LlamaIndex, that help to simplify RAG implementations, and Vertex AI has also launched a grounding service (Vertex AI Grounding) that you can use to ground responses from a generative model with results from Google Search or with your own data (using the aforementioned Vertex AI Search solution). In this section, we will dive deeper into the process and build a RAG solution, piece by piece. Before we dive into the solution architecture, we’ll cover some of the concepts that had been abstracted away in <a href="B18143_17.xhtml#_idTextAnchor408" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 17</em></span></a>, most notably regarding <strong class="bold">tokens</strong> <span>and </span><span><strong class="bold">chunks</strong></span><span>.</span></p>
<h2 id="_idParaDest-363" class="calibre9"><a id="_idTextAnchor425" class="calibre6 pcalibre pcalibre1"/>Tokens and chunks</h2>
<p class="calibre3">In this section, we’ll discuss the concepts of tokens and chunks, beginning <span>with tokens.</span></p>
<h3 class="calibre11">Tokens</h3>
<p class="calibre3">LLMs<a id="_idIndexMarker2160" class="calibre6 pcalibre pcalibre1"/> generally <a id="_idIndexMarker2161" class="calibre6 pcalibre pcalibre1"/>work with tokens rather than words. For example, when dealing with text, a token often represents subsections of words, and tokenization can be done in different ways, such as breaking text up by characters, or using subword-based tokenization (the word “unbelievable” could be split into subwords such as “un,” “believe,” <span>and “able”).</span></p>
<p class="calibre3">The exact size and definition of a token can vary based on different tokenization methods, models, languages, and other factors, but a general rule of thumb for English text using subword <a id="_idIndexMarker2162" class="calibre6 pcalibre pcalibre1"/>tokenizers is around four characters per <a id="_idIndexMarker2163" class="calibre6 pcalibre pcalibre1"/>token <span>on average.</span></p>
<p class="calibre3">Next, let’s discuss the concept <span>of chunks.</span></p>
<h3 class="calibre11">Chunks</h3>
<p class="calibre3">When creating<a id="_idIndexMarker2164" class="calibre6 pcalibre pcalibre1"/> embeddings, we usually break a document<a id="_idIndexMarker2165" class="calibre6 pcalibre pcalibre1"/> into chunks and then create embeddings of those chunks. Again, this can be done in different ways, using different tools. In the Jupyter Notebook file accompanying this chapter, we use Google Cloud Document AI to break our documents <span>into chunks.</span></p>
<p class="calibre3">For this purpose, one of the parameters we need to specify for our Document AI processor is the chunk size to use, which is measured (in this case) by the number of tokens per chunk. You may need to experiment with the value of this parameter to find the chunk size that works best for your use case (e.g., based on the length and structure of the document sections). We generally want our chunks to capture some level of semantic granularity, but there are trade-offs in terms of this granularity. For example, smaller chunks can capture more granular semantic context and provide more precise search results but can be less efficient (computationally) to process. We also need to ensure chunk sizes are within the input length limits of the embedding model we’re using in order to avoid possible truncation. A good practice is to start with a moderate chunk size and adjust it based on how well it fits <span>our needs.</span></p>
<p class="calibre3">Fortunately, Document AI can automatically handle chunking based on layout, even if you don’t specify a preconfigured chunk size, which can be helpful if you don’t know what chunk size <span>to use.</span></p>
<p class="calibre3">Now that we’ve covered those concepts, let’s dive into the architecture of the RAG implementation we will build in <span>this chapter.</span></p>
<h2 id="_idParaDest-364" class="calibre9"><a id="_idTextAnchor426" class="calibre6 pcalibre pcalibre1"/>Example RAG solution architecture</h2>
<p class="calibre3">In this section, we <a id="_idIndexMarker2166" class="calibre6 pcalibre pcalibre1"/>will review the architecture of our example RAG implementation. The proposed solution architecture is shown in <span><em class="italic">Figure 18</em></span><span><em class="italic">.2</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer222">
<img alt="Figure 18.2: RAG solution piece by piece on Google Cloud" src="image/B18143_18_2.jpg" class="calibre207"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 18.2: RAG solution piece by piece on Google Cloud</p>
<p class="calibre3">You can see that between some of the steps depicted in <span><em class="italic">Figure 18</em></span><em class="italic">.2</em>, <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>) is used <a id="_idIndexMarker2167" class="calibre6 pcalibre pcalibre1"/>to store the inputs and outputs of each step, but those intermediary processes are omitted to make the diagram more readable. Also, when we implement this solution in the Jupyter Notebook that accompanies this chapter, the notebook is the application/user that coordinates each of the steps in the <span>overall process.</span></p>
<p class="calibre3">The steps in the solution depicted in <span><em class="italic">Figure 18</em></span><em class="italic">.2</em> are described <span>as follows:</span></p>
<ol class="calibre7">
<li class="calibre8">Our documents, which are stored in GCS, are sent to Google Cloud Document AI for chunking. As the name suggests, the chunking process breaks the documents into chunks, which are smaller sections of the document. This is required in order to create standard-sized chunks that serve as inputs to the embedding process that comes in the next step. The size of the chunks is configurable in Document AI, and further details on this process are <span>described later.</span></li>
<li class="calibre8">The chunks are then sent to the Google Cloud text embedding LLM to create embeddings for the chunks. The resulting embeddings are stored in GCS, alongside their respective chunks (this step is omitted from <span>the diagram).</span></li>
<li class="calibre8">We create a Vertex AI Vector Search index, and the embeddings are ingested from GCS to the Vertex AI Vector Search index (the GCS intermediary step is omitted from <span>the diagram).</span></li>
<li class="calibre8">Next, the application/user asks a question that relates to the contents of our documents. The question is sent as a query to the Google Cloud text embedding LLM to <span>be embedded/vectorized.</span></li>
<li class="calibre8">The vectorized query is then used as input in a request to Vertex AI Vector Search, which searches our index to find similar embeddings. Remember that the embeddings represent an element of semantic meaning, so similar embeddings have similar meanings. This is how we can perform a semantic search to find embeddings that are similar to <span>our query.</span></li>
<li class="calibre8">Next, we take the embeddings returned from our Vertex AI Vector Search query and find the chunks in GCS that relate to those embeddings (remember that <em class="italic">Step 2</em> in our solution created a stored association of chunks <span>and embeddings).</span></li>
<li class="calibre8">Now, it’s finally time to send a prompt to Gemini. The retrieved document chunks from <em class="italic">Step 6</em> serve as the context for the prompt. This helps Gemini respond to our prompt based on the relevant content from our documents and not just from its <span>pre-trained knowledge.</span><p class="calibre3">Gemini<a id="_idIndexMarker2168" class="calibre6 pcalibre pcalibre1"/> responds to <span>the prompt.</span></p></li>
</ol>
<p class="calibre3">Now that we’ve walked through the steps in the process, let’s go ahead and implement <span>this solution!</span></p>
<h2 id="_idParaDest-365" class="calibre9"><a id="_idTextAnchor427" class="calibre6 pcalibre pcalibre1"/>Building the RAG implementation on Google Cloud</h2>
<p class="calibre3">To build our <a id="_idIndexMarker2169" class="calibre6 pcalibre pcalibre1"/>solution, open the<a id="_idIndexMarker2170" class="calibre6 pcalibre pcalibre1"/> Jupyter Notebook file that accompanies this chapter, and perform the activities described in that file. We can use the same Vertex AI Workbench instance that we created in <a href="B18143_14.xhtml#_idTextAnchor348" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 14</em></span></a> for this purpose. Please open JupyterLab on that notebook instance. In the directory explorer on the left side of the screen, navigate to the <strong class="source-inline">Chapter-18</strong> directory and open the <strong class="source-inline">rag.ipynb</strong> notebook file. You can choose Python (local) as the kernel. Again, you can run each cell in the notebook by selecting the cell and pressing <em class="italic">Shift</em> + <em class="italic">Enter</em> on your keyboard. In addition to the relevant code, the notebook file contains markdown text that describes what the code <span>is doing.</span></p>
<p class="calibre3">When you have completed the steps in the notebook, you will have officially built your own RAG <span>solution—nice work!</span></p>
<p class="calibre3">Note that in this case, we are using documents stored in GCS as our source of truth, but we could also use other data, such as data stored <span>in BigQuery.</span></p>
<p class="callout-heading">Document citation</p>
<p class="callout">For congruity and comparability, we will use one of the same reference documents that we used in our RAG implementation in <a href="B18143_17.xhtml#_idTextAnchor408" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 17</em></span></a>, cited <span>as follows:</span></p>
<p class="callout"><em class="italic">Hila Zelicha, Jieping Yang, Susanne M Henning, Jianjun Huang, Ru-Po Lee, Gail Thames, Edward H Livingston, David Heber, and Zhaoping Li, 2024. Effect of cinnamon spice on continuously monitored glycemic response in adults with prediabetes: a 4-week randomized controlled crossover </em><span><em class="italic">trial. DOI:https://doi.org/10.1016/j.ajcnut.2024.01.008</em></span></p>
<p class="calibre3">Next, we will build a broader solution that brings together many topics from throughout this entire book. I’ll begin by explaining the <span>use case.</span></p>
<h1 id="_idParaDest-366" class="calibre5"><a id="_idTextAnchor428" class="calibre6 pcalibre pcalibre1"/>An example business use case and reference architecture</h1>
<p class="calibre3">We will focus on a <strong class="bold">computer vision</strong> (<strong class="bold">CV</strong>) use case to identify objects in images. Extending the CV use<a id="_idIndexMarker2171" class="calibre6 pcalibre pcalibre1"/> case we implemented in <a href="B18143_15.xhtml#_idTextAnchor371" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 15</em></span></a>, we will build our CV model via an MLOps pipeline that incorporates the majority of the main topics from the earlier chapters in this book, such as <span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8">Data preparation, cleaning, <span>and transformation</span></li>
<li class="calibre8">Model training, deployment, inference, <span>and evaluation</span></li>
</ul>
<p class="calibre3">The novel approach we are using here is to utilize generative AI to generate the data that will be used to test and evaluate our model. Before we start building, I’ll present additional context on why I chose to include this use case as <span>an example.</span></p>
<h2 id="_idParaDest-367" class="calibre9"><a id="_idTextAnchor429" class="calibre6 pcalibre pcalibre1"/>Additional background on our use case</h2>
<p class="calibre3">Throughout <a id="_idIndexMarker2172" class="calibre6 pcalibre pcalibre1"/>this book, one topic has come up more frequently than any other: data is often the most important factor in training high-quality ML models. Without adequate data, your ML project will not succeed. We’ve discussed that getting access to sufficient data often proves difficult, and this challenge continues to become ever more prevalent as the average size of models increases (today’s models with trillions of parameters require enormous amounts of data). We also discussed that there’s a limited amount of data in the world, but one mechanism that could help address this challenge is the creation of synthetic data and the use of generative AI models to generate new data. The use case I outline in this section combines all of the steps in the traditional ML model development life cycle and extends the process to include generating data that can be used to test our model. In the next subsection, I’ll describe the architecture for <span>this solution.</span></p>
<h2 id="_idParaDest-368" class="calibre9"><a id="_idTextAnchor430" class="calibre6 pcalibre pcalibre1"/>The reference architecture</h2>
<p class="calibre3">This section <a id="_idIndexMarker2173" class="calibre6 pcalibre pcalibre1"/>outlines the components of the solution architecture we will build. In addition to Vertex AI components, the reference architecture brings other Google Cloud services into scope to build the overall solution, such as Google Cloud Functions, Pub/Sub, Eventarc, and Imagen. We originally introduced and described all of those Google Cloud services in <a href="B18143_03.xhtml#_idTextAnchor059" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 3</em></span></a> of this book. If you are not familiar with them, I recommend refreshing your knowledge from <span>that chapter.</span></p>
<p class="calibre3">Our solution begins with an MLOps pipeline that will train and deploy our CV model, which I will describe in detail shortly. The resulting model is then used in a broader architecture <a id="_idIndexMarker2174" class="calibre6 pcalibre pcalibre1"/>to implement the overall solution. I’ll begin by outlining the <span>MLOps pipeline.</span></p>
<p class="callout-heading">A note on the dataset (CIFAR-10)</p>
<p class="callout">In our model training and evaluation code, we’re using something called the <strong class="bold">CIFAR-10</strong> (<strong class="bold">Canadian Institute For Advanced Research, 10 classes</strong>) dataset, which is a commonly used<a id="_idIndexMarker2175" class="calibre6 pcalibre pcalibre1"/> benchmark dataset for CV and image classification. It contains 60,000 color images belonging to ten different classes: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, and Truck. Each image has a fixed size of 32x32 pixels, and the images are stored in the RGB color space, meaning each pixel is represented by three values corresponding to the red, green, and blue color channels. It’s important to keep these details in mind when we want to send generated data to our model at <span>inference time.</span></p>
<p class="callout">The CIFAR-10 dataset is so commonly used for benchmarking CV models that it is included in the built-in datasets module in Tensorflow/Keras. This means that we can use it in our code by simply importing that module rather than needing to download the data from an <span>external source.</span></p>
<h3 class="calibre11">Our MLOps pipeline</h3>
<p class="calibre3">This section <a id="_idIndexMarker2176" class="calibre6 pcalibre pcalibre1"/>will <a id="_idIndexMarker2177" class="calibre6 pcalibre pcalibre1"/>walk through the steps implemented in our MLOps pipeline, as depicted in <span><em class="italic">Figure 18</em></span><span><em class="italic">.3</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer223">
<img alt="Figure 18.3: MLOps pipeline for CV model" src="image/B18143_18_3.jpg" class="calibre208"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 18.3: MLOps pipeline for CV model</p>
<p class="calibre3">In <span><em class="italic">Figure 18</em></span><em class="italic">.3</em>, the process works <span>as follows:</span></p>
<ol class="calibre7">
<li class="calibre8">The first step <a id="_idIndexMarker2178" class="calibre6 pcalibre pcalibre1"/>in our pipeline—the <a id="_idIndexMarker2179" class="calibre6 pcalibre pcalibre1"/>model training step—is invoked. While the MLOps pipeline we built for our tabular Titanic dataset in <a href="B18143_11.xhtml#_idTextAnchor288" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 11</em></span></a> started with distinct data preprocessing steps using Serverless Spark in Dataproc, in our pipeline in this chapter, the data ingestion and preparation steps are handled directly in the code of our model training job. Also, as noted, in this case, we are using the built-in CIFAR-10 image dataset in Tensorflow/Keras rather than fetching a dataset from an external source. Vertex AI Pipelines starts the model training process by submitting a model training job to the Vertex AI <span>training service.</span></li>
<li class="calibre8">In order to execute our custom training job, the Vertex AI training service fetches our custom Docker container from Google <span>Artifact Registry.</span><p class="calibre3">When our model has been trained, the trained model artifacts are saved <span>in GCS.</span></p><p class="calibre3">The model training job status <span>is complete.</span></p></li>
<li class="calibre8">The next step in our pipeline—the model import step—is invoked. This is an intermediate step that prepares the model metadata to be referenced in later components of our pipeline. The relevant metadata in this case consists of the location of the model artifacts in GCS and the specification of the Docker container image in Google Artifact Registry that will be used to serve <span>our model.</span></li>
<li class="calibre8">The next step in our pipeline—the model upload step—is invoked. This step references the metadata from the model <span>import step.</span></li>
<li class="calibre8">The model metadata is used to register the model in the Vertex AI Model Registry. This makes it easy to deploy our model for serving traffic in <span>Vertex AI.</span><p class="calibre3">The model upload job status <span>is complete.</span></p></li>
<li class="calibre8">The next <a id="_idIndexMarker2180" class="calibre6 pcalibre pcalibre1"/>step in our pipeline—the <a id="_idIndexMarker2181" class="calibre6 pcalibre pcalibre1"/>endpoint creation <span>step—is invoked.</span></li>
<li class="calibre8">An endpoint is created in the Vertex AI prediction service. This endpoint will be used to host <span>our model.</span><p class="calibre3">The endpoint creation job status <span>is complete.</span></p></li>
<li class="calibre8">The next step in our pipeline—the model deployment <span>step—is invoked.</span></li>
<li class="calibre8">Our model is deployed to our endpoint in the Vertex AI prediction service. This step references the metadata of the endpoint that has just been created by our pipeline, as well as the metadata of our model in the Vertex AI <span>Model Registry.</span><p class="calibre3">The model deployment job status <span>is complete.</span></p></li>
</ol>
<p class="calibre3">Now that we’ve walked through the steps in our model training and deployment pipeline, I will begin to outline the broader solution architecture we will build in this chapter, of which <a id="_idIndexMarker2182" class="calibre6 pcalibre pcalibre1"/>our MLOps<a id="_idIndexMarker2183" class="calibre6 pcalibre pcalibre1"/> pipeline is just <span>a subset.</span></p>
<h3 class="calibre11">The end-to-end solution</h3>
<p class="calibre3">We will build an <a id="_idIndexMarker2184" class="calibre6 pcalibre pcalibre1"/>event-driven<a id="_idIndexMarker2185" class="calibre6 pcalibre pcalibre1"/> architecture, which is a popular pattern for developing serverless solutions intended to be implemented automatically in response to an event or a set of events. This is the end-to-end solution that incorporates the majority of the topics we’ve covered throughout this book, as depicted in <span><em class="italic">Figure 18</em></span><span><em class="italic">.4</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer224">
<img alt="Figure 18.4: The end-to-end solution" src="image/B18143_18_4.jpg" class="calibre209"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 18.4: The end-to-end solution</p>
<p class="calibre3">In <span><em class="italic">Figure 18</em></span><em class="italic">.4</em>, our MLOps pipeline is simplified in the top-left corner of the diagram. It still implements all of the same steps we discussed in the previous section, but the diagram is simplified so we can focus our discussion on the broader, end-to-end solution. In this context, the MLOps pipeline is represented as a single step in the <span>overall process.</span></p>
<p class="calibre3">Notice that Eventarc features prominently in our solution, as it is the primary mechanism for orchestrating the steps in our event-based architecture. In the Jupyter Notebook file that accompanies this chapter, I will explain in more detail exactly how Eventarc is being <a id="_idIndexMarker2186" class="calibre6 pcalibre pcalibre1"/>configured <a id="_idIndexMarker2187" class="calibre6 pcalibre pcalibre1"/>behind <span>the scenes.</span></p>
<p class="calibre3">The following set of steps describes the architecture implemented in <span><em class="italic">Figure 18</em></span><span><em class="italic">.4</em></span><span>:</span></p>
<ol class="calibre7">
<li class="calibre8">Our MLOps pipeline trains and deploys our <span>CV model.</span></li>
<li class="calibre8">When the MLOps pipeline completes, it publishes a message to a Pub/Sub topic we created for <span>that purpose.</span></li>
<li class="calibre8">Eventarc detects that a message has been published to the <span>Pub/Sub topic.</span></li>
<li class="calibre8">Eventarc triggers the Cloud Function we’ve created to generate <span>an image.</span></li>
<li class="calibre8">The code in our image generation function makes a call to the Imagen API with a prompt to generate an image containing one of the types of objects our model was trained to recognize (a type of object supported by the <span>CIFAR-10 dataset).</span></li>
<li class="calibre8">Imagen generates an image and returns it to <span>our function.</span></li>
<li class="calibre8">Our function stores the new image <span>in GCS.</span></li>
<li class="calibre8">GCS emits an event indicating that a new object has been uploaded to our bucket. Eventarc detects <span>this event.</span></li>
<li class="calibre8">Eventarc invokes our next Cloud Function and passes the GCS event metadata to our function. This metadata includes details such as the identifiers of the bucket and the object <span>in question.</span></li>
<li class="calibre8">Our prediction function takes the details regarding the bucket and the object in question from the event metadata and uses those details to fetch the newly created object (i.e., the newly generated image <span>from Imagen).</span></li>
<li class="calibre8">Our prediction function then performs some preprocessing on the image to transform it into a format that is expected by our model (i.e., similar to the format of the CIFAR-10 data the model was trained on). Our function then sends the transformed data as a prediction request to the Vertex AI endpoint that hosts <span>our model.</span></li>
<li class="calibre8">Our model predicts what type of object is in the image, and sends a prediction response to our <span>Cloud Function.</span></li>
<li class="calibre8">Our Cloud Function saves the prediction response <span>in GCS.</span></li>
</ol>
<p class="calibre3">When the process has been completed, you can view the generated image and the resulting prediction from our model <span>in GCS.</span></p>
<p class="calibre3">Notice that all of the steps in the solution are implemented automatically and without the need to provision any servers. This is a fully serverless, event-driven <span>solution architecture.</span></p>
<p class="calibre3">An interesting side effect of this solution is that, although the primary intention is to test our newly trained model on generated data, this solution could also be applied to the inverse use case. That is, if we are confident that our model has been trained effectively and provides consistently accurate results, we could use it to evaluate the quality of the generated data. For example, if our model predicts that the generated data contains a particular type of <a id="_idIndexMarker2188" class="calibre6 pcalibre pcalibre1"/>object with a <a id="_idIndexMarker2189" class="calibre6 pcalibre pcalibre1"/>probability of 99.8%, we can interpret this as a reflection of the quality of the <span>generated data.</span></p>
<p class="calibre3">Now that we’ve discussed the various steps in the process, let’s start <span>building it!</span></p>
<h1 id="_idParaDest-369" class="calibre5"><a id="_idTextAnchor431" class="calibre6 pcalibre pcalibre1"/>Building and implementing the use case on Google Cloud</h1>
<p class="calibre3">To build our<a id="_idIndexMarker2190" class="calibre6 pcalibre pcalibre1"/> solution, open the Jupyter Notebook file that accompanies this chapter, and perform the activities described in that file. We can use the same Vertex AI Workbench instance that we created in <a href="B18143_14.xhtml#_idTextAnchor348" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 14</em></span></a> for this purpose. Please open JupyterLab on that notebook instance. In the directory explorer on the left side of the screen, navigate to the <strong class="source-inline">Chapter-18</strong> directory and open the <strong class="source-inline">end-to-end-mlops-genai.ipynb</strong> notebook file. You can choose Python (local) as the kernel. Again, you can run each cell in the notebook by selecting the cell and pressing <em class="italic">Shift</em> + <em class="italic">Enter</em> on your keyboard. In addition to the relevant code, the notebook file contains markdown text that describes what the code <span>is doing.</span></p>
<p class="calibre3">If you have followed all of the practical steps and successfully built the solution, then it’s time to give yourself a good pat on the back. Let’s take a moment to reflect on what you have done here. You have successfully built and executed an MLOps pipeline that trains and deploys a Keras convolutional neural network implementing a CV use case. You have then built a completely serverless, event-driven solution architecture using a combination of many Google Cloud services. That is absolutely something to be very <span>proud of!</span></p>
<p class="calibre3">As we approach<a id="_idIndexMarker2191" class="calibre6 pcalibre pcalibre1"/> the end of this chapter and book, let’s summarize what we’ve learned and discuss how to keep learning in <span>this space.</span></p>
<h1 id="_idParaDest-370" class="calibre5"><a id="_idTextAnchor432" class="calibre6 pcalibre pcalibre1"/>Recap and next steps</h1>
<p class="calibre3">In this chapter, we combined the majority of the important concepts that we’ve covered throughout this book and explained how they all tie together. We did this by building solutions that incorporated many of the topics and Google Cloud products <span>we’ve discussed.</span></p>
<p class="calibre3">First, you built a RAG implementation, piece by piece, focusing on the combination of various generative AI concepts in that process, such as using Google Cloud Document AI to break a document into chunks in a manner that preserves the hierarchical structure of the original document. This solution also included using the Google Cloud text embedding LLM to create embeddings for the document chunks and using Vertex AI Vector Search to store and index those embeddings. You used the resulting solution to implement a question-answering use case with Gemini, grounding the answers in the contents of the document. To do this, you used the Google Cloud text embedding LLM to create embeddings of the prompted question and used those embeddings to perform a semantic similarity search in Vertex AI Vector Search to find similar embeddings relating to specific chunks from the document. Those chunks were then provided as context when sending the request <span>to Gemini.</span></p>
<p class="calibre3">Next, you built a solution that combined many topics from this entire book, including all of the steps in a traditional model development life cycle, as well as generative AI and broader solution architecture concepts. This resulted in a fully automated, serverless, event-driven solution that incorporated many Google Cloud services, such as Google Cloud Functions, Pub/Sub, Eventarc, <span>and Imagen.</span></p>
<p class="calibre3">We’ve certainly covered a lot of topics in this book, and the learning journey never ends. Let’s wrap up by discussing how you can continue your learning journey in the fields of Google Cloud solutions architecture, ML, and <span>generative AI.</span></p>
<h2 id="_idParaDest-371" class="calibre9"><a id="_idTextAnchor433" class="calibre6 pcalibre pcalibre1"/>Next steps</h2>
<p class="calibre3">The technology fields we’ve discussed in this book continue to evolve at an accelerating pace, with new frameworks and patterns emerging daily, and generative AI is arguably the fastest-evolving field of them all. For example, in addition to the popular LangChain framework we’ve discussed in this book, which can also be implemented as a managed experience in the Vertex AI Reasoning Engine, extensions of those frameworks, such as LangGraph, LangSmith, and many others continue to emerge, providing additional functionality and flexibility for building generative AI solutions and applications. Due to the accelerating pace of development and overall excitement in this space, new learning resources will continue to emerge. In addition to the GitHub repository associated with this book, many other example repositories provide valuable tutorials on how to implement various patterns. Perhaps the most valuable of these is, of course, the official Vertex AI example repository, which can be accessed at the <span>following link:</span></p>
<p class="calibre3"><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples" class="calibre6 pcalibre pcalibre1"><span>https://github.com/GoogleCloudPlatform/vertex-ai-samples</span></a></p>
<p class="calibre3">I also find the information available at <a href="http://langchain.com" class="calibre6 pcalibre pcalibre1">langchain.com</a> to be a useful learning resource, and I highly recommend referencing the example architecture use cases in the Google Cloud Solutions Center, accessible at the <span>following link:</span></p>
<p class="calibre3"><a href="https://solutions.cloud.google.com/" class="calibre6 pcalibre pcalibre1"><span>https://solutions.cloud.google.com/</span></a></p>
<p class="calibre3">As we wrap up, remember that the practical exercises and notebooks we used throughout this book create resources on Google Cloud, which can incur costs. For that reason, I recommend reviewing each of the practical steps we performed in this book and ensuring that all resources are deleted if you no longer plan to use them. The GitHub repository associated with this book contains steps for deleting resources such as models, endpoints, and data stores. For other types of resources, please refer to the Google Cloud documentation to ensure that the relevant resources <span>are deleted.</span></p>
<p class="calibre3">I write this final section with a sense of sadness that our journey together is coming to an end, and I want to thank you for embarking on this journey with me. If you’ve made it this far in the book, then you have gained expert knowledge in AI/ML, generative AI, Google Cloud, and solutions architecture, and you now know a lot more about all of those topics than most people in the world! Congratulations, and well done! I wish you the best of luck in your future AI adventures, and I hope you will use your knowledge and skills to achieve <span>wonderful things!</span></p>
</div>
</div></body></html>