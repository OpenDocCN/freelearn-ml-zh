<html><head></head><body>
		<div id="_idContainer304">
			<h1 id="_idParaDest-49"><em class="italic"><a id="_idTextAnchor054"/>Chapter 6</em>: Exploring Multi-Fidelity Optimization</h1>
			<p><strong class="bold">Multi-Fidelity Optimization </strong>(<strong class="bold">MFO</strong>) is<a id="_idIndexMarker281"/> the fourth of four groups of hyperparameter tuning methods. The main characteristic of this group is that all methods belonging to this group utilize the cheap approximation of the whole hyperparameter tuning pipeline so we can have similar performance results with a much lower computational cost and faster experiment time. This group is suitable when you have a very large model or a very large number of samples, for example, when you are developing a neural-network-based model.</p>
			<p>In this chapter, we will discuss several methods in the MFO group, including coarse-to-fine search, successive halving, hyper band, and <strong class="bold">Bayesian Optimization and Hyperband</strong> (<strong class="bold">BOHB</strong>). As in <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a><em class="italic">, Exploring Heuristic Search</em> we will discuss the definition of each method, the differences<a id="_idIndexMarker282"/> between them, how they work, and the pros and cons of each.</p>
			<p>By the end of this chapter, you will be confident in explaining MFO and its variations, and also how they work at a high level and in a technical way. You will also be able to tell the differences between them, along with the pros and cons of each. You will also experience the crucial benefit of understanding each of the methods in practice: being able to configure the method to match your own problem and knowing what to do when there are errors or unexpected outputs from the method.</p>
			<p>In this chapter, we’ll be covering the following main topics:</p>
			<ul>
				<li>Introducing MFO</li>
				<li>Understanding coarse-to-fine search</li>
				<li>Understanding successive halving</li>
				<li>Understanding hyper band</li>
				<li>Understanding BOHB</li>
			</ul>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor055"/>Introducing MFO</h1>
			<p>MFO is a <a id="_idIndexMarker283"/>group of hyperparameter tuning methods that work by creating a cheap approximation of the whole hyperparameter tuning pipeline so that we can get similar performance results with much<em class="italic"> lower computational cost</em> and <em class="italic">faster experiment time</em>. There are many ways to create a cheap approximation. For example, we can work only on the subsets of the full data in the first several steps rather than directly working on the full data, or we can also try to use fewer epochs when training a neural-network-based model before training our model with full epochs. In other words, MFO methods work by <em class="italic">combining cheap low-fidelity and expensive high-fidelity</em> evaluations, where usually the proportion of cheaper evaluations is much larger than the more expensive evaluations so that we can achieve lower computational cost and thus faster experiment time. However, MFO methods can also be categorized as part of <a id="_idIndexMarker284"/>the <strong class="bold">informed search</strong> category since they utilize knowledge from previous iterations to have a (hopefully) better search space in future.</p>
			<p>All of the methods that we have learned in the previous chapters can be categorized as <strong class="bold">black-box optimization</strong> methods. All black-box optimization methods <a id="_idIndexMarker285"/>try to perform hyperparameter tuning without utilizing any information from what is happening inside the ML model or the data that is used by the model. A black-box optimizer<a id="_idIndexMarker286"/> will only focus on searching the best set of hyperparameters from the defined hyperparameter space and <em class="italic">treat other factors as a black box</em> (see <em class="italic">Figure 6.1</em>). This characteristic has its own good and bad implications. It enables us to utilize a black-box optimizer, which is more flexible for various types of models or data, but it also costs us more since we do not consider other factors that may speed up the process.</p>
			<div>
				<div id="_idContainer270" class="IMG---Figure">
					<img src="image/B18753_06_001.jpg" alt="Figure 6.1 – Illustration of black-box optimizer&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Illustration of black-box optimizer</p>
			<p>The expense of <a id="_idIndexMarker287"/>black-box optimization methods means we can’t utilize them when we are working with a <em class="italic">very large model</em> or <em class="italic">big data</em> that requires a very long time for just one training iteration. That’s where the MFO group of hyperparameter tuning methods comes into the picture! By considering other factors that are treated as black-box by black-box optimizers, we can have a faster process while sacrificing a bit of the generality that black-box optimizers have. </p>
			<p class="callout-heading">Generality</p>
			<p class="callout"><strong class="bold">Generality</strong> means <a id="_idIndexMarker288"/>the model is able to perform on many unseen cases.</p>
			<p>Furthermore, most of the methods categorized in this group can <em class="italic">utilize parallel computational resources</em> very nicely, which can further boost the speed of the hyperparameter tuning process. However, the benefit of faster processes offered by MFO <a id="_idIndexMarker289"/>methods comes with a cost. We may have <em class="italic">worse performing tuning results</em> since there is a chance we have excluded a better subspace during the cheap low-fidelity evaluations step. However, the <em class="italic">speedup is arguably more significant</em> than the estimation error, especially when we are working with a very large model and/or big data.</p>
			<p class="callout-heading">Important Note </p>
			<p class="callout">The MFO group of hyperparameter tuning methods is <em class="italic">not</em> a completely different group compared to black-box optimization methods, including exhaustive search, Bayesian optimization, and heuristic search. In fact, we can also apply a similar procedure done in a multi-fidelity optimization method to a black-box optimizer. In other words, <em class="italic">we can combine black-box-and multi-fidelity</em> models so we can get the best of both worlds.</p>
			<p>For example, we can perform hyperparameter tuning with one of the <strong class="bold">Bayesian Optimization</strong> (<strong class="bold">BO</strong>) methods (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>, <em class="italic">Exploring Bayesian Optimization</em>) and also apply the successive halving method (see the <em class="italic">Understanding successive halving</em> section) on top of it. This <a id="_idIndexMarker290"/>way, we will ensure that we only perform BO on important subspace, rather than letting BO explore the whole hyperparameter space by itself. By doing this, we can have a faster experiment time with lower computational cost.</p>
			<p>Now that you are aware of what MFO is, how it differs from black-box optimization methods, and how it works at a high level, we will dive deeper into several MFO methods in the following sections. </p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor056"/>Understanding coarse-to-fine search</h1>
			<p><strong class="bold">Coarse-to-Fine Search </strong>(<strong class="bold">CFS</strong>) is a<a id="_idIndexMarker291"/> combination of grid and random search hyperparameter tuning methods (see <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a><em class="italic">, Exploring Exhaustive Search</em>). Unlike grid and random search, which are categorized in the <strong class="bold">uninformed search</strong> group of <a id="_idIndexMarker292"/>methods, CFS utilizes knowledge from previous iterations to have a (hopefully) better search space in the future. In other words, CFS is a <em class="italic">combination of sequential and parallel</em> hyperparameter tuning methods. It is indeed a very simple method since it is basically a <em class="italic">combination of two other simple methods: grid and random search</em>.</p>
			<p>CFS can be effectively utilized as a hyperparameter tuning method when you are working with a medium-sized model, for example, a shallow neural network (other types of models can also work) and a moderate amount of training data.</p>
			<p>The main idea of CFS is just to start with a <em class="italic">coarse</em> random search from the whole hyperparameter space, then gradually <em class="italic">refine</em> the search in more detail, either using random or grid search. The following figure summarizes how CFS works as a hyperparameter tuning method.</p>
			<div>
				<div id="_idContainer271" class="IMG---Figure">
					<img src="image/B18753_06_002.jpg" alt="Figure 6.2 – Illustration of CFS&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Illustration of CFS</p>
			<p>As illustrated in <em class="italic">Figure 6.2</em>, CFS<a id="_idIndexMarker293"/> starts by performing a random search in the whole pre-defined hyperparameter space. Then, it looks for a promising subspace based on the first coarse random search evaluation results. The definition of a promising subspace may vary and can be adjusted to your own preference. The following list shows several definitions of a promising subspace that you can adopt:</p>
			<ul>
				<li>Get only the <em class="italic">top N percentiles</em> of the best set of hyperparameters based on the evaluation performed in the previous trial.</li>
				<li>Put a <em class="italic">hard threshold</em> to filter out the bad set of hyperparameters from the previous trial. </li>
				<li>Conduct a <em class="italic">univariate analysis</em> to get the best range of values for each hyperparameter.</li>
			</ul>
			<p>No matter what definition you are using to define the promising subspace, we will always get a list of values for each hyperparameter. Then, we can create a new hyperparameter space based on the minimum and maximum values in each list of hyperparameter values.</p>
			<p>After getting the promising subspace, we can continue the process by performing a grid search or another random search in the smaller area. Note that you can also put a condition on when to keep using random search and when to start using grid search. Again, it is up to you to choose the appropriate condition. However, it is better to perform a random search than a grid search, so that we can have <em class="italic">more evaluations based on the cheap low-fidelity approach</em> compared to the expensive high-fidelity approach. We keep repeating this procedure until we reach the stopping criterion. </p>
			<p>The following procedure explains in more detail how <a id="_idIndexMarker294"/>CFS works as a hyperparameter tuning method:</p>
			<ol>
				<li>Split the original full data into a training set and a test set. (See <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">,</em> <em class="italic">Evaluating Machine Learning Models</em>.)</li>
				<li>Define the hyperparameter space, <strong class="source-inline">H</strong>, with the accompanied distributions, the objective function, <strong class="source-inline">f</strong>, based on the training set, and the stopping criterion.</li>
				<li>Define the grid size for creating the grid search hyperparameter space, <strong class="source-inline">grid_size</strong>, and the random search number of iterations, <strong class="source-inline">random_iters</strong>. </li>
				<li>Define the criterion of a promising subspace by utilizing the objective function, <strong class="source-inline">f</strong>. </li>
				<li>Define the criterion of when to start using grid search.</li>
				<li>Set the initial best set of hyperparameters, <strong class="source-inline">best_set</strong>, with the value <strong class="source-inline">None</strong>. </li>
				<li>Perform a random search on the current hyperparameter space, <strong class="source-inline">H</strong>, for <strong class="source-inline">random_iters</strong> times.</li>
				<li>Select a promising subspace based on the criterion defined in <em class="italic">step 4</em>:<ol><li value="1">If the current best-performing set of hyperparameters is worse than the previous <strong class="source-inline">best_set</strong>, add <strong class="source-inline">best_set</strong> to the promising subspace.</li><li>If the current best-performing set of hyperparameters is better than the previous <strong class="source-inline">best_set</strong>, update <strong class="source-inline">best_set</strong>.</li></ol></li>
				<li>If the criterion in <strong class="source-inline">step 5 </strong>is met, do the following:<ol><li value="1">Update the current hyperparameter space, <strong class="source-inline">H</strong>, with the promising subspace selected in <em class="italic">step 8</em>, using unique <strong class="source-inline">grid_size </strong>values for each of the hyperparameters. </li><li>Perform a grid search on the updated hyperparameter space, <strong class="source-inline">H</strong>.</li></ol></li>
				<li>If the criterion in <em class="italic">step 5</em> is not met, do the following:<ol><li value="1">Update the current hyperparameter space, <strong class="source-inline">H</strong>, with the promising subspace selected in <em class="italic">step 8</em> using the minimum and maximum values for each hyperparameter.</li><li>Perform a random search on the updated hyperparameter space, <strong class="source-inline">H</strong>, for <strong class="source-inline">random_iters</strong> times.</li></ol></li>
				<li>Repeat <em class="italic">steps 8 – 10</em> until the stopping criterion is met.</li>
				<li>Train on the full training set using the best hyperparameter combination.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>In CFS, the <a id="_idIndexMarker295"/>multi-fidelity characteristic is based neither on the amount of data nor the number of training epochs, but on the <em class="italic">granularity of the search</em> performed in the search space during each trial. In other words, we will <em class="italic">keep using all of the data</em> and <em class="italic">all of the training epochs</em> with a <em class="italic">refined hyperparameter space</em> in each trial.</p>
			<p>Let’s see how CFS works as a hyperparameter tuning method on dummy data generated by the <strong class="bold">scikit-learn</strong> package. Scikit-learn has a function called <strong class="source-inline">make_classification</strong> to create dummy classification data with several customizable configurations. In this example, we use the following configurations to generate the dummy data:</p>
			<ul>
				<li><em class="italic">Number of classes</em>. We set the number of target classes in the data to 2 by setting <strong class="source-inline">n_classes=2</strong>.</li>
				<li><em class="italic">Number of samples</em>. We set the number of samples to 500 by setting <strong class="source-inline">n_samples=500</strong>.</li>
				<li><em class="italic">Number of features</em>. We set the number of features or the number of dependent variables in the data to 25 by setting <strong class="source-inline">n_features=25</strong>.</li>
				<li><em class="italic">Number of informative features</em>. We set the number of features that have high importance to distinguish between all of the target classes to 18 by setting <strong class="source-inline">n_informative=18</strong>.</li>
				<li><em class="italic">Number of redundant features</em>. We set the number of features that are basically just a weighted sum from other features to 5 by setting <strong class="source-inline">n_redundant=5</strong>.</li>
				<li><em class="italic">Random seed</em>. To ensure reproducibility, we set <strong class="source-inline">random_state=0</strong>.</li>
			</ul>
			<p>We utilize a <strong class="bold">Multi-Layer Perceptron </strong>(<strong class="bold">MLP</strong>) with <a id="_idIndexMarker296"/>one hidden layer as the classifier model and use the <em class="italic">mean of seven-fold cross-validation accuracy scores</em> as the objective function (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>, <em class="italic">Exploring Bayesian Optimization</em>). In this example, we are not using grid search as part of the CFS procedure, meaning that we only use random search in each of the trials. We set the maximum number of trials to <strong class="source-inline">12</strong>, which acts as the stopping criterion. We set the number of iterations for each random search trial to <strong class="source-inline">20</strong>. Finally, we utilize the <em class="italic">top N percentiles</em> scheme to define the promising subspace in each trial, with <strong class="source-inline">N=50</strong>. We define the hyperparameter space as follows:</p>
			<ul>
				<li>Number of neurons in the hidden layer: <strong class="source-inline">hidden_layer_sizes=range(1,51)</strong></li>
				<li>Initial learning rate: <strong class="source-inline">learning_rate_init=np.linspace(0.001,0.1,50)</strong></li>
			</ul>
			<p>The following figure shows how <a id="_idIndexMarker297"/>CFS works in each iteration or trial. The<em class="italic"> purple dots</em> refer to hyperparameter values tested in the current trial, while the <em class="italic">red rectangles</em> refer to the promising subspace to be searched in the next trial.  </p>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/B18753_06_003.jpg" alt="Figure 6.3 – Illustration of the CFS process&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Illustration of the CFS process</p>
			<p>In <em class="italic">Figure 6.3</em>, we can see clearly how<a id="_idIndexMarker298"/> CFS starts by working at the full hyperparameter space and then gradually searches in the smaller subspaces. It is also worth noting that although we only use random search in this example, we can see that CFS still increases its fidelity over the number of trials until we get a final set of hyperparameters in the last trial. We can also see the performance of each trial in the following figure.</p>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<img src="image/B18753_06_004.jpg" alt="Figure 6.4 – Convergence plot&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Convergence plot</p>
			<p>The blue line in <em class="italic">Figure 6.4</em> reflects the average cross-validation scores from all tested hyperparameters (see the purple dots in<em class="italic"> Figure 6.3</em>) at each trial. The red line reflects the cross-validation score of the best-performing set of hyperparameters at each trial. We can see that the red line has a nice <em class="italic">non-decreasing monotonic</em> characteristic. This happens because we always add back the best set of hyperparameters from all previous trials to the promising subspace definition, as defined in <em class="italic">step 8</em> in the previous procedure. We will learn how to implement CFS with scikit-learn in <em class="italic">Chapter 7, Hyperparameter Tuning via Scikit</em>.</p>
			<p>The following <a id="_idIndexMarker299"/>table summarizes the pros and cons of utilizing CFS as a <a id="_idIndexMarker300"/>hyperparameter tuning method.</p>
			<div>
				<div id="_idContainer274" class="IMG---Figure">
					<img src="image/B18753_06_005.jpg" alt="Figure 6.5 – Pros and cons of CFS&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Pros and cons of CFS</p>
			<p>In this section, we have discussed CFS, looking at what it is, how it works, and the pros and cons. We will discuss another interesting MFO method in the next section.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor057"/>Understanding successive halving</h1>
			<p><strong class="bold">Successive Halving </strong>(<strong class="bold">SH</strong>) is an<a id="_idIndexMarker301"/> MFO method that is not only able to focus on a more promising hyperparameter subspace but can also <em class="italic">allocate computational cost wisely</em> in each trial. Unlike CFS, which utilizes all of the data in each trial, SH can utilize less data for a not-too-promising subspace while utilizing more data for a more promising subspace. It can be said that SH is a variant of CFS with a much clearer algorithm definition and is wiser in spending the computational cost. The most effective way to utilize SH as a hyperparameter tuning method is when you are working with a large model (for example, a deep neural network) and/or working with a large amount of data.</p>
			<p>Similar to CFS, SH also <em class="italic">utilizes grid search or random search</em> to search for the best set of hyperparameters. At the first iteration, SH will perform a grid or random search on the whole hyperparameter space with a small amount of <strong class="bold">budget</strong> or resources, and then it will gradually increase the budget while also removing the worst half of the hyperparameters candidates at each iteration. In other words, SH performs hyperparameter tuning with a lower budget on a bigger search space and a higher budget on a more promising smaller subspace. SH can also be seen as a <strong class="bold">tournament</strong> between hyperparameter candidates, where only the best candidate will survive at the end of the trials.</p>
			<p class="callout-heading">Budget Definition in SH</p>
			<p class="callout">In a default hyperparameter tuning setup, the <a id="_idIndexMarker302"/>budget is defined as the number of samples in the data. However, it is also possible to define the budget in other ways. For example, we can also define the budget as the maximum training time, number of iterations during XGBoost training steps, number of estimators in a random forest, or number of epochs when training a neural network model.</p>
			<p>To have a better understanding of<a id="_idIndexMarker303"/> SH, let’s look at the following example before we discuss how it works in a formal procedure. We utilize the same model and the same hyperparameter space definition used in the example in the <em class="italic">Understanding CFS</em> section. We also utilize a similar procedure to generate a dummy classification dataset a hundred times bigger in size, meaning we have <strong class="source-inline">50000</strong> samples instead of only <strong class="source-inline">500</strong> samples as in the CFS example.</p>
			<p>In this example, we utilize random search instead of grid search to sample the hyperparameter candidates in each trial. The following figure shows the accuracy scores of hyperparameter candidates over trials. Each line refers to the trend of each hyperparameter candidate’s objective function score, which in this case is the <em class="italic">seven-fold cross-validation accuracy score</em>, over the number of trials.  The final objective function score, based on the best set of hyperparameters selected from the SH tuning process, is <strong class="source-inline">0.984</strong>. We will learn how to implement SH in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a><em class="italic">, Hyperparameter Tuning via Scikit</em> and <a href="B18753_09_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 9</em></a><em class="italic">, Hyperparameter Tuning via Optuna</em>.</p>
			<div>
				<div id="_idContainer275" class="IMG---Figure">
					<img src="image/B18753_06_006.jpg" alt="Figure 6.6 – Illustration of the SH process&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Illustration of the SH process</p>
			<p>In <em class="italic">Figure 6.6</em>, we can clearly <a id="_idIndexMarker304"/>see how SH takes only the top hyperparameter candidates (see the orange ovals) from each trial for further evaluation in the next trial. In the first iteration, a random search is performed <strong class="source-inline">240</strong> times with only <strong class="source-inline">600</strong> out of <strong class="source-inline">50000</strong> of the samples available in the data. This means we have <strong class="source-inline">240</strong> hyperparameter candidates, <strong class="source-inline">n_candidates</strong>, in the first iteration. Out of those hyperparameter candidates, SF takes only the top <strong class="source-inline">80</strong> candidates to be evaluated with a larger number of samples in the second iteration, which is <strong class="source-inline">1800</strong> samples. For the third iteration, SF again takes only the top <strong class="source-inline">27</strong> candidates and evaluates them on <strong class="source-inline">5400</strong> samples. </p>
			<p>This process continues until we <em class="italic">can’t use a larger number of samples</em> since it will be greater than the maximum resources, <strong class="source-inline">max_resources</strong>, defined in the first place. In this example, the maximum resources are defined as the number of samples that we have in the data. However, it can also be defined as the total number of epochs or training steps based on the definition of the budget or resources.</p>
			<p>In this example, we stopped at the fourth iteration, where we need to evaluate <strong class="source-inline">3</strong> candidates based on <strong class="source-inline">48600</strong> samples. The final hyperparameter candidate chosen is the one that has the highest seven-fold cross-validation accuracy score evaluated on those <strong class="source-inline">48600</strong> samples.</p>
			<p>As you will notice, the gradual <a id="_idIndexMarker305"/>increment of the number of samples in each trial and the gradual decrement of the number of candidates in each trial follows the same multiplier factor, <strong class="source-inline">factor</strong>, which is <strong class="source-inline">3</strong> in this example. That’s why we have to stop at the fourth iteration, since if we continue to the fifth iteration, we would need <strong class="source-inline">48600*3=145800</strong> samples, while we only have <strong class="source-inline">50000</strong> samples in the data. Note that we have to set the value of the multiplier factor ourselves before running the SH tuning process. In other words, this multiplier factor is the hyperparameter for SH. </p>
			<p class="callout-heading">Multiplier Factor in SH</p>
			<p class="callout">The halving term in <a id="_idIndexMarker306"/>SH refers to setting the multiplier factor value to two. In other words, only the best half of the hyperparameter candidates in each trial are passed to the next trial. However, we can also change this with another value. For example, when we set the multiplier factor as three, it means we take only the top one-third of hyperparameter candidates in each trial. In practice, setting the multiplier factor as three usually works better than setting it as two.</p>
			<p>Besides the multiplier factor and maximum resources, SH also has other hyperparameters, such as the minimum number of resources to be used at the first iteration, <strong class="source-inline">min_resources</strong>, and the initial number of candidates to be evaluated at the first iteration, <strong class="source-inline">n_candidates</strong>. If grid search is utilized in the SH tuning process, <strong class="source-inline">n_candidates</strong> will equal the number of all combinations of hyperparameters in the search space. If a random search is utilized, then we have to set the value of <strong class="source-inline">n_candidates</strong> ourselves. In our example, where random search is utilized, we set <strong class="source-inline">min_resources=600</strong> and <strong class="source-inline">n_candidates=240</strong>. </p>
			<p>While setting <strong class="source-inline">factor</strong> to be equal to three is the common practice, this is not the case for <strong class="source-inline">min_resources</strong> and <strong class="source-inline">n_candidates</strong>. There are many factors to be considered before choosing the right values for both the <strong class="source-inline">min_resources</strong> and <strong class="source-inline">n_candidates</strong> hyperparameters. In other words, there is a trade-off between them, as explained here: </p>
			<ul>
				<li>Choosing a <em class="italic">bigger</em> value for <strong class="source-inline">n_candidates</strong> is useful when the bad and good hyperparameters can be easily distinguished with a smaller number of samples (a smaller value for <strong class="source-inline">min_resources</strong>). </li>
				<li>Choosing a <em class="italic">smaller</em> value for <strong class="source-inline">n_candidates </strong>is useful when we need a larger number of samples (a larger value for <strong class="source-inline">min_resources</strong>) to distinguish between the bad and good hyperparameters.</li>
			</ul>
			<p>Another hyperparameter that<a id="_idIndexMarker307"/> SH has is the minimum early stopping rate, <strong class="source-inline">min_early_stopping</strong>. This integer-type hyperparameter has a default value of zero. If it is set to more than zero, it will reduce the number of iterations while increasing the number of resources to be used at the first iteration. In our previous example, we set <strong class="source-inline">min_early_stopping=0</strong>.</p>
			<p>To summarize, SH as a hyperparameter tuning method works as follows:</p>
			<ol>
				<li value="1">Split the original dataset into train and test sets.</li>
				<li>Define the hyperparameter space, <strong class="source-inline">H</strong>, with the accompanied distributions, and the objective function, <strong class="source-inline">f</strong>, based on the training set.</li>
				<li>Define the budget/resources. Usually, this is defined as the number of samples or training epochs.</li>
				<li>Define the maximum amount of resources, <strong class="source-inline">max_resources</strong>. Usually, this is defined as the total number of samples in data or the total number of epochs.</li>
				<li>Define the multiplier factor, <strong class="source-inline">factor</strong>, the minimum amount of resources to be used at the first iteration, <strong class="source-inline">min_resources</strong>, and the minimum early stopping rate, <strong class="source-inline">min_early_stopping</strong>.</li>
				<li>Define the initial number of hyperparameter candidates to be evaluated at the first iteration, <strong class="source-inline">n_candidates</strong>. If grid search is utilized, this will be automatically parsed from the total number of hyperparameter combinations in the search space.</li>
				<li>Calculate the maximum number of iterations, <em class="italic">n</em><span class="subscript">iter</span>, using the following formula:</li>
			</ol>
			<p>           <img src="image/Formula_B18753_06_001.png" alt=""/></p>
			<ol>
				<li value="8">Assert if <strong class="source-inline">n_candidates</strong> ≥ <img src="image/Formula_B18753_06_002.png" alt=""/> to ensure there is at least one candidate in the last iteration.</li>
				<li>Warm up the first<a id="_idIndexMarker308"/> iteration:<ol><li value="1">Sample <strong class="source-inline">n_candidates </strong>sets of hyperparameters from the hyperparameter space. If grid search is utilized, just return all of the hyperparameter combinations in the space. This set of candidates is referred to as <em class="italic">candidates1</em>.</li><li>Evaluate all <em class="italic">candidates1</em> sets of hyperparameters, using <strong class="source-inline">min_resources</strong>, based on the objective function, <em class="italic">f</em>.</li><li>Calculate the <em class="italic">topK</em> value that will be used to select top candidates for the next iteration: </li></ol></li>
			</ol>
			<p><img src="image/Formula_B18753_06_003.png" alt=""/></p>
			<ol>
				<li value="10">For each iteration, <em class="italic">i</em>, starting from the second iteration until <em class="italic">n</em><span class="subscript">iter</span> iteration, proceed as follows:<ol><li value="1">Update the current set of candidates, <em class="italic">candidatesi</em>, by selecting <em class="italic">topK</em> candidates from <em class="italic">candidatesi-1</em> in terms of the most optimal objective function score.</li><li>Update the current allocated resources, <em class="italic">resourcesi</em>, based on the following formula:</li></ol></li>
			</ol>
			<p><img src="image/Formula_B18753_06_004.png" alt=""/></p>
			<ol>
				<li value="3">Evaluate all <em class="italic">candidatesi</em> sets of hyperparameters, using <em class="italic">resourcesi</em>, based on the objective function, <em class="italic">f</em>.</li>
				<li>Update the <em class="italic">topK</em> value based on the following formula:</li>
			</ol>
			<p><img src="image/Formula_B18753_06_005.png" alt=""/></p>
			<ol>
				<li value="11">Return the best hyperparameter candidate:<ol><li value="1">Evaluate all candidates in the last iteration using the allocated number of resources and the objective function, <strong class="source-inline">f</strong>. Note that it’s possible that the allocated resource in the last iteration is less than <strong class="source-inline">max_resources</strong>.</li><li>Select the candidate with the optimal objective function score.</li></ol></li>
				<li>Train on the<a id="_idIndexMarker309"/> full train set using the best set of hyperparameters from <em class="italic">step 11</em>.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>Based on the previous example and the stated procedure, we can see that SH performs cheap, low-fidelity evaluations on the first several iterations by using a low number of resources and starts to perform more expensive high-fidelity evaluations on the final several iterations by using a high number of resources.</p>
			<p class="callout-heading">Integration with Other Black-Box Methods</p>
			<p class="callout">SH can also be utilized along with other black-box hyperparameter tuning methods apart from grid and random search. For example, in the <strong class="bold">Optuna</strong> (see <a href="B18753_09_ePub.xhtml#_idTextAnchor082"><em class="italic">Chapter 9</em></a><em class="italic">, Hyperparameter Tuning via Optuna</em>) package, we can combine TPE (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>) with SH, where SH acts as a <strong class="bold">pruner</strong>. Note that in Optuna, the budget/resources is defined as the number of training steps or epochs instead of the number of samples.</p>
			<p>The following is a <a id="_idIndexMarker310"/>list of the pros and cons of SH as a hyperparameter<a id="_idIndexMarker311"/> tuning method:</p>
			<div>
				<div id="_idContainer281" class="IMG---Figure">
					<img src="image/B18753_06_007.jpg" alt="Figure 6.7 – Pros and cons of SH&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Pros and cons of SH</p>
			<p>In practice, most of the time, we do not know how to balance the trade-off between the number of resources and the number of candidates since there is no clear definition of how to distinguish bad and good hyperparameters. One thing that can help us to find a sweet spot in this trade-off is leveraging previous similar experiment configurations or by performing <strong class="bold">meta-learning</strong> based on the available meta-data from previous similar experiments.</p>
			<p>Now you are aware of SH, how it works, when to use it, and its pros and cons, in the next section, we will learn about an extension of this method that attempts to overcome the cons of SH.</p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor058"/>Understanding hyper band</h1>
			<p><strong class="bold">Hyper Band </strong>(<strong class="bold">HB</strong>) is an <a id="_idIndexMarker312"/>extension of SH that is specifically designed to overcome issues inherent in SH (see <em class="italic">Figure 6.7</em>). Although we can perform meta-learning to help us balance the trade-off, most of the time we do not have the metadata that’s needed in practice. Furthermore, the possibility of SH removing better sets of hyperparameters in the first several iterations is also worrying and can’t be solved by just finding a sweet spot from the trade-off. HB tries to solve these issues by calling SH several times iteratively. </p>
			<p>Since HB is just an extension of SH, it is suggested that you utilize HB as your hyperparameter tuning method when you are working with a large model (for example, a deep neural network) and/or working with a large amount of data, just like SH. Furthermore, it is even better to utilize HB than SH when you do not have the time or metadata needed to help you configure the trade-off between the amount of resources and the number of candidates, which is the case most of the time.</p>
			<p>The main difference between HB and SH is in their hyperparameters. HB has the same hyperparameters as SH (see the <em class="italic">Understanding SH</em> section) except for <strong class="source-inline">n_candidates</strong>. In HB, we don’t have to choose the best value for <strong class="source-inline">n_candidates</strong> since it is calculated automatically within the HB algorithm. </p>
			<p>Basically, HB <a id="_idIndexMarker313"/>works by running SH iteratively with variations of <strong class="source-inline">n_candidates</strong> and <strong class="source-inline">min_resources</strong> in each of the <strong class="bold">brackets</strong> (each SH run), starting from the combination of the highest possible value for <strong class="source-inline">n_candidates</strong> and the lowest possible value for <strong class="source-inline">min_resources</strong>, and going to the lowest possible value for <strong class="source-inline">n_candidates</strong> and the highest possible value for <em class="italic">resources</em> (see <em class="italic">Figure 6.8</em>). It’s like a brute-force approach to try <em class="italic">almost</em> all of the possible combinations of <strong class="source-inline">n_candidates</strong> and <strong class="source-inline">min_resources</strong>.</p>
			<div>
				<div id="_idContainer282" class="IMG---Figure">
					<img src="image/B18753_06_008.jpg" alt="Figure 6.8 – Illustration of the HB process. Here, nj and rj refer to n_candidates and min_resources for bracket-j, respectively&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Illustration of the HB process. Here, nj and rj refer to n_candidates and min_resources for bracket-j, respectively</p>
			<p>As illustrated in <em class="italic">Figure 6.8</em>, assume that we set <strong class="source-inline">factor=3</strong>, <strong class="source-inline">min_resources=1</strong>, <strong class="source-inline">max_resources=27</strong>, and <strong class="source-inline">min_early_stopping=0</strong>. As you can see, HB allocates the minimum amount of resources with the maximum number of candidates in the first bracket, while it allocates the maximum amount of resources with the minimum number of candidates in the last bracket. Again, each bracket refers to each SH run, meaning we are running SH four times in this illustration, where the last bracket is basically the same as performing random or grid search on a small hyperparameter space.</p>
			<p>By testing <em class="italic">almost</em> all of the possible combinations of <strong class="source-inline">n_candidates</strong> and <strong class="source-inline">min_resources</strong>, HB is able to remove the trade-off in SH while also reducing the possibility of excluding better hyperparameters in the first iterations. However, this groundbreaking characteristic of <a id="_idIndexMarker314"/>HB <em class="italic">doesn’t ensure that it will be always better than SH</em>. Why? Because HB hasn’t actually tried all the possible combinations. We might find a better combination of <strong class="source-inline">n_candidates</strong> and <strong class="source-inline">min_resources</strong> values just by performing a single SH than all the possible combinations HB tried. However, this takes time and luck since we have to manually select the <strong class="source-inline">n_candidates</strong> and <strong class="source-inline">min_resources</strong> values.</p>
			<p class="callout-heading">Integration with Other Black-Box Methods</p>
			<p class="callout">In the original paper on HB, the authors utilize random search for each SH run. However, as with SH, we can also integrate HB with other black-box methods.</p>
			<p>The following procedure further states how <a id="_idIndexMarker315"/>HB works formally as a hyperparameter tuning method:</p>
			<ol>
				<li value="1">Split the full original dataset into train and test sets.</li>
				<li>Define the hyperparameter space, <strong class="source-inline">H</strong>, with the accompanied distributions, and the objective function, <strong class="source-inline">f</strong>, based on the training set.</li>
				<li>Define the <strong class="source-inline">budget</strong> resource. This is usually defined as the number of samples or training epochs.</li>
				<li>Define the maximum resources, <strong class="source-inline">max_resources</strong>. This is usually defined as the total number of samples in the data or the total number of epochs.</li>
				<li>Define the multiplier factor, <strong class="source-inline">factor</strong>, the minimum early stopping rate, <strong class="source-inline">min_early_stopping</strong>, and the minimum number of resources for all brackets, <strong class="source-inline">min_resources</strong>. Usually, <strong class="source-inline">min_resources</strong> is set to one, if the budget is defined as the number of samples.</li>
				<li>Create a dictionary, <strong class="source-inline">top_candidates</strong>, that will be utilized to store the best-performing set of hyperparameters from each SH run.</li>
				<li>Calculate the number of brackets, <em class="italic">nbrackets</em>, using the following formula: </li>
			</ol>
			<p><img src="image/Formula_B18753_06_016.png" alt=""/></p>
			<ol>
				<li value="8">For each bracket-<em class="italic">j</em>, starting from <strong class="source-inline">j=1</strong> until <strong class="source-inline">j=nbrackets</strong>, do the following:<ol><li value="1">Calculate the minimum number of resources to be used at the first iteration of SH for bracket-<em class="italic">j</em>, <img src="image/Formula_B18753_06_017.png" alt=""/>, using the following formula:</li></ol></li>
			</ol>
			<p><img src="image/Formula_B18753_06_018.png" alt=""/></p>
			<ol>
				<li value="2">Calculate the initial number of hyperparameter candidates to be evaluated at the first iteration of SH for bracket-<em class="italic">j</em>, <img src="image/Formula_B18753_06_019.png" alt=""/>, using the following formula:</li>
			</ol>
			<p><img src="image/Formula_B18753_06_020.png" alt=""/></p>
			<ol>
				<li value="3">Do <em class="italic">steps 7 – 11</em> from the SH procedure given in the <em class="italic">Understanding SH</em> section by utilizing <img src="image/Formula_B18753_06_021.png" alt=""/> as the <strong class="source-inline">min_resources</strong> and <img src="image/Formula_B18753_06_022.png" alt=""/> as the <strong class="source-inline">n_candidates</strong> for the current SH run, respectively. Other hyperparameters for SH, such as <strong class="source-inline">max_resources</strong>, <strong class="source-inline">min_early_stopping</strong>, and <strong class="source-inline">factor</strong>, are inherited from HB. </li>
				<li>Store the best set <a id="_idIndexMarker316"/>of hyperparameters output from the current SH run, along with the objective function score, in the <strong class="source-inline">top_candidates</strong> dictionary.</li>
			</ol>
			<ol>
				<li value="9">Select the best candidate that has the most optimal objective function score from the <strong class="source-inline">top_candidates</strong> dictionary.</li>
				<li>Train on the full training set using the best set of hyperparameters from <em class="italic">step 9</em>.</li>
				<li>Evaluate the final trained model on the test set.</li>
			</ol>
			<p>The following table summarizes the <a id="_idIndexMarker317"/>pros and cons of utilizing HB as a<a id="_idIndexMarker318"/> hyperparameter tuning method:</p>
			<div>
				<div id="_idContainer290" class="IMG---Figure">
					<img src="image/B18753_06_009.jpg" alt="Figure 6.9 – Pros and cons of HB&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Pros and cons of HB</p>
			<p>It is worth noting that although HB can help us to deal with the trade-off of SH, it has a higher computational cost, since we have to run several SH rounds iteratively. It is even more costly when we are faced with a case where the bad and good hyperparameters cannot be easily distinguished with a small budget value. Why? The first several brackets of HB that utilize small budgets will result in a noisy estimation, since the relative rankings inside the SH iterations on smaller budgets do not reflect the actual relative rankings on higher budgets. In the most extreme case, the best set of hyperparameters will result from the last bracket (random search). If this is the case, then HB will run <em class="italic">n</em><em class="italic">brackets </em>times slower compared to random search.</p>
			<p>In this section, we have discussed HB, what it is, how it works, and its pros and cons. We will discuss another interesting MFO method in the next section.</p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor059"/>Understanding BOHB</h1>
			<p><strong class="bold">Bayesian Optimization and Hyper Band </strong>(<strong class="bold">BOHB</strong>) is <a id="_idIndexMarker319"/>an extension of HB that is superior to CFS, SH, and HB, in terms of understanding the relationship between the hyperparameter candidates and the objective function. If CFS, SH, and HB are all part of the informed search group based on random search, BOHB is an informed search group that is based on the BO method. This means BOHB is able to decide which subspace needs to be searched based on previous experiences rather than luck.</p>
			<p>As its name implies, BOHB is the combination of the BO and HB methods. While SH and HB can also be utilized with other black-box methods (see the <em class="italic">Understanding SH</em> and <em class="italic">Understanding HB </em>sections), BOHB is specifically designed to utilize a BO method in a way that can support HB. Furthermore, the BO method in BOHB also tracks all the previous evaluations on all budgets, so that it can serve as the base for future evaluations. Note that the BO method used in BOHB is<a id="_idIndexMarker320"/> the <strong class="bold">multivariate TPE</strong>, which is able to take into account the interdependencies among hyperparameters (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>). </p>
			<p>The main selling point of <a id="_idIndexMarker321"/>BOHB is its ability to achieve both a strong initial performance and a strong final performance. This can be easily seen in <em class="italic">Figure 6.10</em>, from the original BOHB paper (see the following note for details). BO (without performing metalearning) will outperform random search if we have more time to let it learn from previous experiences. If we don’t have time, BO will deliver a similar or even worse performance compared to random search. On the other hand, HB performs much better than random search when we have limited time, but will perform similarly to random search if we allow more time for random search to explore the hyperparameter space. By combining the best of both worlds, BOHB is able to not only outperform random search in a limited time but also when given enough time for random search to catch up.</p>
			<div>
				<div id="_idContainer291" class="IMG---Figure">
					<img src="image/B18753_06_010.jpg" alt="Figure 6.10 – Comparison between random search, BO, HB, and BOHB&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Comparison between random search, BO, HB, and BOHB</p>
			<p class="callout-heading">The Original BOHB Paper</p>
			<p class="callout"><em class="italic">BOHB: Robust and Efficient Hyperparameter Optimization at Scale</em> by Stefan Falkner, Aaron Klein, and Frank Hutter, Proceedings of the 35th International Conference on Machine Learning, PMLR 80:1437-1446, 2018 (<a href="http://proceedings.mlr.press/v80/falkner18a.html">http://proceedings.mlr.press/v80/falkner18a.html</a>).</p>
			<p>The following procedure further states how <a id="_idIndexMarker322"/>BOHB works formally as a hyperparameter tuning method. Note that BOHB and HB are very similar except that random search in HB is replaced by the combination of multivariate TPE and random search. Since HB just performs SH several times iteratively, the actual replacement is actually performed in each of the SH runs (each bracket) in HB.</p>
			<p>Let’s pick up from the previous instructions again.</p>
			<p><em class="italic">6.  (The first six steps are the same as those in the Understanding HB section.)</em></p>
			<p>7.  Define the probability of just performing a random search rather than fitting the multivariate TPE, <em class="italic">random_prob</em>.</p>
			<p>8.  Define the percentage of the good set of hyperparameters for the multivariate TPE fitting procedure, <em class="italic">top_n_percent</em>. (See <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>.)</p>
			<p>9.  Define a dictionary, <em class="italic">candidates_dict</em>, that stores the budget/resources used in a particular SH iteration and the pairs of hyperparameter candidates and the objective function score as the key and value, respectively.</p>
			<p>10.  Define the minimum number of sets of hyperparameters that are randomly sampled before starting to fit the multivariate TPE, <strong class="source-inline">n_min</strong>. By default, we set <strong class="source-inline">n_min</strong> to match the number of hyperparameters in the space plus one.</p>
			<p>11.  For each bracket-<em class="italic">j</em>, starting from <strong class="source-inline">j=1</strong> until <strong class="source-inline">j=nbrackets</strong>, do the following:</p>
			<ol>
				<li>Calculate the minimum number of resources to be used on the first iteration of SH for bracket-<em class="italic">j</em>, <img src="image/Formula_B18753_06_023.png" alt=""/>, using the following formula:</li>
			</ol>
			<p> <img src="image/Formula_B18753_06_024.png" alt=""/></p>
			<ol>
				<li value="2">Calculate the initial number of hyperparameters candidates to be evaluated on the first iteration of SH for bracket-j, <img src="image/Formula_B18753_06_025.png" alt=""/>, using the following formula:</li>
			</ol>
			<p> <img src="image/Formula_B18753_06_026.png" alt=""/></p>
			<ol>
				<li value="3">Perform steps 7 – 11 from the SH procedure stated in the Understanding SH section by utilizing <img src="image/Formula_B18753_06_027.png" alt=""/> as min_resources and <img src="image/Formula_B18753_06_028.png" alt=""/> as n_candidates for the current<a id="_idIndexMarker323"/> SH run, respectively, where step 9. I. is replaced with the following procedure:</li>
				<li>Generate a random number between zero and one from a uniform distribution, rnd.</li>
				<li>If <strong class="source-inline">rnd&lt;random_prod</strong> or <em class="italic">models_dict</em> is empty, perform a random search to sample the initial hyperparameter candidates.</li>
				<li>Count the number of sampled hyperparameters in <strong class="source-inline">candidates_dict[</strong><img src="image/Formula_B18753_06_029.png" alt=""/><strong class="source-inline">]</strong>, and store it as <strong class="source-inline">num_curr_candidates</strong>.</li>
				<li>If <strong class="source-inline">num_curr_candidates &lt; n_min</strong>, then perform a random search to sample the initial hyperparameter candidates.</li>
				<li>Alternatively, utilize the multivariate TPE (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>) to sample the initial hyperparameter candidates. Note that we always utilize multivariate TPE on the largest budget available in <strong class="source-inline">candidates_dict</strong>. The number of hyperparameter sets for both good and bad groups is defined based on the following formula:</li>
			</ol>
			<p><img src="image/Formula_B18753_06_030.png" alt=""/></p>
			<p><img src="image/Formula_B18753_06_031.png" alt=""/></p>
			<ol>
				<li value="9">Store the<a id="_idIndexMarker324"/> sampled initial hyperparameter candidates along with the objective function score (either from step ii, iv, or v) in <strong class="source-inline">candidates_dict[</strong><img src="image/Formula_B18753_06_032.png" alt=""/><strong class="source-inline">]</strong>.</li>
				<li>Store the best set of hyperparameters output from the current SH run, along with the objective function score, in the <strong class="source-inline">top_candidates</strong> dictionary.</li>
			</ol>
			<ol>
				<li value="12">Select the best candidate that has the most optimal objective function score from the <strong class="source-inline">top_candidates</strong> dictionary.</li>
				<li>Train on the full training set using the best set of hyperparameters from <em class="italic">step 14</em>.</li>
				<li>Evaluate the<a id="_idIndexMarker325"/> final trained model on the test set.</li>
			</ol>
			<p>Note that to ensure that BOHB tracks all of the evaluations on all budgets, we also need to store the hyperparameter candidates in each of the SH iterations for each HB bracket to <strong class="source-inline">candidates_dict[budget]</strong> along with their objective function score. Here, hyperparameter candidates in each of the SH iterations refer to <em class="italic">candidatesi</em>, while budget refers to <em class="italic">resourcesi</em> in <em class="italic">step 10</em> in the <em class="italic">Understanding SH</em> section, which also can be seen in the following figure:</p>
			<div>
				<div id="_idContainer302" class="IMG---Figure">
					<img src="image/B18753_06_011.jpg" alt="Figure 6.11 – BOHB tracks all the evaluations on all budgets&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – BOHB tracks all the evaluations on all budgets</p>
			<p>You may wonder whether it is possible for BOHB to take advantage of parallel resources since it utilizes a BO method that is notorious for not being able to exploit parallel computing resources. The answer is <em class="italic">yes, it is possible</em>! You can take advantage of parallel resources since in each of the BOHB iterations, specifically in the HB iterations, we can utilize more than one worker to evaluate multiple sets of hyperparameters, in parallel. </p>
			<p>What about the sequential nature of the multivariate TPE utilized in BOHB? Yes, there may be some sequential processes that need to be performed inside the TPE model. However, BOHB limits the number of sets of hyperparameters given to the multivariate TPE so it might not take too much time. Furthermore, the limitation on the number of hyperparameter sets is actually specifically designed by the authors of BOHB. The following is a direct quote from the original paper on BOHB:</p>
			<p class="author-quote">The parallelism in TPE is achieved by limiting the number of samples to optimize EI, purposefully not optimizing it fully to obtain diversity. This ensures that consecutive suggestions by the model are diverse enough to yield near-linear speedups when evaluated in parallel.</p>
			<p>It is also worth noting that we always utilize the multivariate TPE on the largest budget available to ensure that it is fitted on enough budget (high-fidelity) to minimize the chance of a noisy estimation. So, combined with the limitation on the number of hyperparameter sets passed to the TPE, we are trying to ensure that the multivariate TPE is fitted on the right number of hyperparameter sets.</p>
			<p>The following table <a id="_idIndexMarker326"/>summarizes the pros and<a id="_idIndexMarker327"/> cons of utilizing BOHB as a hyperparameter tuning method:</p>
			<div>
				<div id="_idContainer303" class="IMG---Figure">
					<img src="image/B18753_06_012.jpg" alt="Figure 6.12 – Pros and cons of BOHB&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Pros and cons of BOHB</p>
			<p>Just as HB may run <em class="italic">nbrackets</em> times slower compared to random search when we are faced with a situation where the bad and good hyperparameters cannot be easily distinguished with a small budget value, BOHB will also run <em class="italic">nbrackets</em> times slower compared to the vanilla BO, where we are faced with the same condition.</p>
			<p>In this section, we have covered BOHB in detail, including what it is, how it works, and its pros and cons.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor060"/>Summary</h1>
			<p>In this chapter, we have discussed the fourth of the four groups of hyperparameter tuning methods, called the MFO group. We have discussed MFO in general and what makes it different from black-box optimization methods, as well as discussing several variants, including CFS, SH, HB, and BOHB. We have seen the differences between them and the pros and cons of each. From now on, you should be able to explain MFO with confidence when someone asks you about it. You should also be able to debug and set up the most suitable configuration for the chosen method that suits your specific problem definition.</p>
			<p>In the next chapter, we will begin implementing the various hyperparameter tuning methods that we have learned about so far using the scikit-learn package. We will become familiar with the scikit-learn package and learn how to utilize it in various hyperparameter tuning methods.</p>
		</div>
	</body></html>