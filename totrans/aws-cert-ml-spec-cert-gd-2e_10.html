<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer161">
			<h1 class="chapter-number"><a id="_idTextAnchor1355"/>10</h1>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor1356"/>Model Deployment</h1>
			<p><a id="_idTextAnchor1357"/><a id="_idTextAnchor1358"/><a id="_idTextAnchor1359"/><a id="_idTextAnchor1360"/><a id="_idTextAnchor1361"/><a id="_idTextAnchor1362"/><a id="_idTextAnchor1363"/><a id="_idTextAnchor1364"/><a id="_idTextAnchor1365"/><a id="_idTextAnchor1366"/><a id="_idTextAnchor1367"/><a id="_idTextAnchor1368"/><a id="_idTextAnchor1369"/><a id="_idTextAnchor1370"/><a id="_idTextAnchor1371"/><a id="_idTextAnchor1372"/><a id="_idTextAnchor1373"/><a id="_idTextAnchor1374"/><a id="_idTextAnchor1375"/><a id="_idTextAnchor1376"/><a id="_idTextAnchor1377"/><a id="_idTextAnchor1378"/><a id="_idTextAnchor1379"/><a id="_idTextAnchor1380"/>In the previous chapter, you explored various aspects of Amazon SageMaker, including different instances, data preparation in Jupyter Notebook, model training with built-in algorithms, and crafting custom code for training and inference. Now, your focus shifts to diverse model deployment choices using <span class="No-Break">AWS services.</span></p>
			<p>If you are navigating the landscape of model deployment on AWS, understanding the options is crucial. One standout service is Amazon SageMaker – a fully managed solution that streamlines the entire <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) life cycle, especially when it comes to deploying models. There are several factors that influence the model deployment options. As you go ahead in this chapter you will learn different options to deploy models <span class="No-Break">using SageMaker.</span></p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor1381"/>Factors influencing model deployment options</h1>
			<p>Here are the primary factors that play a crucial role in determining a model <span class="No-Break">deployment option:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Scalability requirements</strong></span><ul><li><strong class="bold">High traffic:</strong> Imagine you are developing a recommendation system for a popular e-commerce platform expecting fluctuating traffic throughout the day. If the application anticipates high traffic and varying loads, services such as Amazon SageMaker with autoscaling capabilities or AWS Lambda may be preferable. This is crucial to maintain performance during <span class="No-Break">peak hours.</span></li></ul></li>
				<li><strong class="bold">Real-time versus </strong><span class="No-Break"><strong class="bold">batch inference</strong></span><ul><li><strong class="bold">Real-time inference</strong>: Consider a fraud detection system for a financial institution where immediate decisions are essential for transaction approval or denial. For such real-time predictions, services such as Amazon SageMaker and AWS Lambda are suitable. For fraud detection, these services provide low-latency responses, enabling quick decisions on the legitimacy of transactions. Real-time transactions trigger immediate predictions through SageMaker’s inference endpoints in <span class="No-Break">this case.</span></li><li><strong class="bold">Batch inference:</strong> In a healthcare setting, you may need to process a large volume of patient data periodically to update predictive models for disease diagnosis. This is a batch processing use case, and a combination of SageMaker and services such as Amazon S3 can be employed. You can efficiently handle large datasets, perform periodic model updates, and ensure that predictions align with the latest information, which is crucial for maintaining accuracy in <span class="No-Break">healthcare predictions.</span></li></ul></li>
				<li><strong class="bold">Infrastructure </strong><span class="No-Break"><strong class="bold">management complexity</strong></span><ul><li><strong class="bold">Managed deployment versus custom deployment:</strong> Suppose you are developing a computer vision application for analyzing satellite images, requiring specialized configurations and dependencies. In cases where custom configurations are necessary, opting for custom deployments using EC2 instances allows more control over the infrastructure. SageMaker’s managed services are ideal for scenarios where you prioritize ease of management and do not want to delve into intricate <span class="No-Break">infrastructure details.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Cost considerations</strong></span><ul><li><strong class="bold">Pay-per-use:</strong> Consider a weather forecasting application where the computational demand varies based on weather events. AWS Lambda’s pay-per-use model is advantageous in situations where the workload fluctuates. You pay only for the compute time consumed, making it cost-effective for applications with sporadic, unpredictable usage patterns compared to alternatives with <span class="No-Break">fixed costs.</span></li></ul></li>
			</ul>
			<h1 id="_idParaDest-255"><a id="_idTextAnchor1382"/>SageMaker deployment options</h1>
			<p>Amazon SageMaker offers diverse deployment options to deploy ML models effectively. In this section, you will explore different ways of deploying models using SageMaker, providing technology solutions with scenarios <span class="No-Break">and examples.</span></p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor1383"/>Real-time endpoint deployment</h2>
			<p>In this <strong class="bold">scenario</strong>, you have a trained image classification model, and you want to deploy it to provide real-time predictions for <span class="No-Break">incoming images.</span></p>
			<h3 id="_idParaDest-257"><a id="_idTextAnchor1384"/>Solution</h3>
			<p>Create a SageMaker model and deploy it to a <span class="No-Break">real-time endpoint.</span></p>
			<h3 id="_idParaDest-258"><a id="_idTextAnchor1385"/>Steps</h3>
			<ol>
				<li>Train your model using SageMaker <span class="No-Break">training jobs.</span></li>
				<li>Create a SageMaker model from the trained <span class="No-Break">model artifacts.</span></li>
				<li>Deploy the model to a <span class="No-Break">real-time endpoint.</span></li>
			</ol>
			<h3 id="_idParaDest-259"><a id="_idTextAnchor1386"/>Example code snippet</h3>
			<pre class="source-code"><strong class="source-inline">from sagemaker import get_execution_role</strong></pre>
			<pre class="source-code"><strong class="source-inline">from sagemaker.model import Model</strong></pre>
			<pre class="source-code"><strong class="source-inline">from sagemaker.predictor import RealTimePredictor</strong></pre>
			<pre class="source-code"><strong class="source-inline">role = get_execution_role()</strong></pre>
			<pre class="source-code"><strong class="source-inline">model_artifact='s3://your-s3-bucket/path/to/model.tar.gz'</strong></pre>
			<pre class="source-code"><strong class="source-inline">model = Model(model_data=model_artifact, role=role)</strong></pre>
			<pre class="source-code"><strong class="source-inline">predictor = model.deploy(instance_type='ml.m4.xlarge', endpoint_name='image-classification-endpoint')</strong></pre>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor1387"/>Batch transform job</h2>
			<p>In this <strong class="bold">scenario</strong>, you have a large dataset, and you want to perform batch inference on the entire dataset using a <span class="No-Break">trained model.</span></p>
			<h3 id="_idParaDest-261"><a id="_idTextAnchor1388"/>Solution</h3>
			<p>Use SageMaker batch transform to process the entire dataset in <span class="No-Break">a batch.</span></p>
			<h3 id="_idParaDest-262"><a id="_idTextAnchor1389"/>Steps</h3>
			<ol>
				<li>Create a <span class="No-Break">SageMaker transformer.</span></li>
				<li>Start the batch <span class="No-Break">transform job.</span></li>
			</ol>
			<h3 id="_idParaDest-263"><a id="_idTextAnchor1390"/>Example code snippet</h3>
			<pre class="console"><strong class="source-inline">from sagemaker.transformer import Transformer</strong></pre>
			<pre class="console"><strong class="source-inline">transformer = Transformer(model_name='your-model-name',</strong></pre>
			<pre class="console"><strong class="source-inline">                          instance_count=1,</strong></pre>
			<pre class="console"><strong class="source-inline">                          instance_type='ml.m4.xlarge',</strong></pre>
			<pre class="console"><strong class="source-inline">                          strategy='SingleRecord',</strong></pre>
			<pre class="console"><strong class="source-inline">                          assemble_with='Line',</strong></pre>
			<pre class="console"><strong class="source-inline">                          output_path='s3://your-s3-bucket/output')</strong></pre>
			<pre class="console"><strong class="source-inline">transformer.transform('s3://your-s3-bucket/input/data.csv', content_type='text/csv')</strong></pre>
			<pre class="console"><strong class="source-inline">transformer.wait()</strong></pre>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor1391"/>Multi-model endpoint deployment</h2>
			<p>In this <strong class="bold">scenario</strong>, you have multiple versions of a model, and you want to deploy them on a single endpoint for A/B testing or <span class="No-Break">gradual rollout.</span></p>
			<h3 id="_idParaDest-265"><a id="_idTextAnchor1392"/>Solution</h3>
			<p>Use SageMaker multi-model endpoints to deploy and manage multiple models on a <span class="No-Break">single endpoint.</span></p>
			<h3 id="_idParaDest-266"><a id="_idTextAnchor1393"/>Steps</h3>
			<ol>
				<li>Create and train <span class="No-Break">multiple models.</span></li>
				<li>Create a <span class="No-Break">SageMaker multi-model.</span></li>
				<li>Deploy the multi-model to <span class="No-Break">an endpoint.</span></li>
			</ol>
			<h3 id="_idParaDest-267"><a id="_idTextAnchor1394"/>Example code snippet</h3>
			<pre class="source-code"><strong class="source-inline">from sagemaker.multimodel import MultiModel</strong></pre>
			<pre class="source-code"><strong class="source-inline">multi_model = MultiModel(model_data_prefix='s3://your-s3-bucket/multi-models')</strong></pre>
			<pre class="source-code"><strong class="source-inline">predictor = multi_model.deploy(instance_type='ml.m4.xlarge', endpoint_name='multi-model-endpoint')</strong></pre>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor1395"/>Endpoint autoscaling</h2>
			<p>In this <strong class="bold">scenario</strong>, your application experiences varying workloads, and you want to automatically adjust the number of instances based <span class="No-Break">on traffic.</span></p>
			<h3 id="_idParaDest-269"><a id="_idTextAnchor1396"/>Solution</h3>
			<p>Enable auto-scaling for <span class="No-Break">SageMaker endpoints.</span></p>
			<h3 id="_idParaDest-270"><a id="_idTextAnchor1397"/>Steps</h3>
			<ol>
				<li>Configure the SageMaker endpoint to <span class="No-Break">use autoscaling.</span></li>
				<li>Set up the minimum and maximum instance counts based on the <span class="No-Break">expected workload.</span></li>
			</ol>
			<h3 id="_idParaDest-271"><a id="_idTextAnchor1398"/>Example code snippet</h3>
			<pre class="source-code"><strong class="source-inline">from sagemaker.predictor import Predictor</strong></pre>
			<pre class="source-code"><strong class="source-inline">predictor = Predictor(endpoint_name='your-endpoint-name', sagemaker_session=sagemaker_session)</strong></pre>
			<pre class="source-code"><strong class="source-inline">predictor.predict('input_data')</strong></pre>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor1399"/>Serverless APIs with AWS Lambda and SageMaker</h2>
			<p>In this <strong class="bold">scenario</strong>, you want to create a serverless API using AWS Lambda to interact with your <span class="No-Break">SageMaker model.</span></p>
			<h3 id="_idParaDest-273"><a id="_idTextAnchor1400"/>Solution</h3>
			<p>Use AWS Lambda to invoke the <span class="No-Break">SageMaker endpoint.</span></p>
			<h3 id="_idParaDest-274"><a id="_idTextAnchor1401"/>Steps</h3>
			<ol>
				<li>Create an AWS <span class="No-Break">Lambda function.</span></li>
				<li>Integrate the Lambda function with the <span class="No-Break">SageMaker endpoint.</span></li>
			</ol>
			<h3 id="_idParaDest-275"><a id="_idTextAnchor1402"/>Example code snippet</h3>
			<pre class="console"><strong class="source-inline">import boto3</strong></pre>
			<pre class="console"><strong class="source-inline">import json</strong></pre>
			<pre class="console"><strong class="source-inline">def lambda_handler(event, context):</strong></pre>
			<pre class="console"><strong class="source-inline">    # Perform preprocessing on input data</strong></pre>
			<pre class="console"><strong class="source-inline">    input_data = event['input_data']</strong></pre>
			<pre class="console"><strong class="source-inline">    # Call SageMaker endpoint</strong></pre>
			<pre class="console"><strong class="source-inline">    # ...</strong></pre>
			<pre class="console"><strong class="source-inline">    return {</strong></pre>
			<pre class="console"><strong class="source-inline">        'statusCode': 200,</strong></pre>
			<pre class="console"><strong class="source-inline">        'body': json.dumps('Inference successful!')</strong></pre>
			<pre class="console"><strong class="source-inline">    }</strong></pre>
			<p>In the realm of deploying ML models, Amazon SageMaker emerges as a robust choice. Its managed environment, scalability features, and seamless integration with other AWS services make it an efficient and reliable solution for businesses navigating diverse deployment scenarios. Whether you are anticipating high traffic or need a hassle-free deployment experience, SageMaker is your ally in the journey of model deployment excellence. In the next section, you will learn about creating pipelines with <span class="No-Break">Lambda functions.</span></p>
			<h1 id="_idParaDest-276"><a id="_idTextAnchor1403"/><a id="_idTextAnchor1404"/>Creating alternative pipelines with Lambda Functions</h1>
			<p>Indeed, SageMaker <a id="_idTextAnchor1405"/>is an awesome <a id="_idTextAnchor1406"/>platform that you can use to create training and inference pipelines. However, you can always work with different services to come up with similar solutions. One of these services, which you will learn <a id="_idTextAnchor1407"/>about next, is known as <span class="No-Break"><strong class="bold">Lambda functions</strong></span><span class="No-Break">.</span></p>
			<p>AWS Lambda is a serverless compute service where you can run a function as a service. In other words, you can concentrate your efforts on just writing your function. Then, you just need to tell AWS how to run it (that is, the environment and resource configurations), so all the necessary resources will be provisioned to run your code and then discontinued once it <span class="No-Break">is completed.</span></p>
			<p>Throughout <a href="B21197_03.xhtml#_idTextAnchor337"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">AWS Services for Data Migration and Processing</em>, you explored how Lambda functions integrate with many different services, such as Kinesis and AWS Batch. Indeed, AWS did a very good job of integrating Lambda with 140+ services (and the list is constantly increasing). That means that when you work with a specific AWS service, you will remember that it is likely to integrate <span class="No-Break">with Lambda.</span></p>
			<p>It is important to bear this in mind because Lambda functions can really expand your possibilities to create scalable and integrated architectures. For example, you can trigger a Lambda function when a file is uploaded to S3 in order to preprocess your data before loading it to Redshift. Alternatively, you can create an API that triggers a Lambda function at each endpoint execution. Again, the possibilities are endless with this <span class="No-Break">powerful service.</span></p>
			<p>It is also useful to know that you can write your function in different programming languages, such as Node.js, Python, Go, Java, and more. Your function does not necessarily have to be triggered by another AWS service – that is, you can trigger it manually for your web or mobile application, <span class="No-Break">for example.</span></p>
			<p>When it comes to deployment, you can upload your function as a ZIP file or as a container image. Although this is not ideal for an automated deployment process, coding directly into the AWS Lambda console is <span class="No-Break">also possible.</span></p>
			<p>As with <a id="_idTextAnchor1408"/>any other service, this <a id="_idTextAnchor1409"/>one also has some downsides that you should be <span class="No-Break">aware of:</span></p>
			<ul>
				<li><strong class="bold">Memory allocation for your function</strong>: This ranges from 128 MB to 10,240 MB (AWS has recently increased this limit from 3 GB to 10 GB, as <span class="No-Break">stated previously)</span></li>
				<li><strong class="bold">Function timeout</strong>: This is a maximum of 900 seconds (<span class="No-Break">15 minutes)</span></li>
				<li><strong class="bold">Function layer</strong>: This is a maximum of <span class="No-Break">five layers</span></li>
				<li><strong class="bold">Burst concurrency</strong>: This is from 500 to 3,000, depending on the <span class="No-Break">AWS Region</span></li>
				<li><strong class="bold">Deployment package size</strong>: This is 250 MB unzipped, <span class="No-Break">including layers</span></li>
				<li><strong class="bold">Container image code package size</strong>: This is <span class="No-Break">10 GB</span></li>
				<li><strong class="bold">Available space in the /tmp directory</strong>: This is <span class="No-Break">512 MB</span></li>
			</ul>
			<p>Before opting for Lambda functions, make sure these restrictions fit your use case. By bringing Lambda functions closer to your scope of alternative pipelines for SageMaker, you can leverage one potential use of Lambda, which is to create inference pipelines for <span class="No-Break">the models.</span></p>
			<p>As you know, SageMaker has a very handy <strong class="source-inline">.deploy()</strong> method that will create endpoints for model inference. This is so that you can call to pass the input data to receive predictions back. Here, you can create this inference endpoint by using the API gateway and <span class="No-Break">Lambda functions.</span></p>
			<p>If you do not need an inference endpoint and you just want to make predictions and store results somewhere (in a batch fashion), then all you need is a Lambda function, which is able to fetch the input data, instantiate the model object, make predictions, and store the results in the appropriate location. Of course, it does this by considering all the limitations that you <span class="No-Break">discussed earlier.</span></p>
			<p>Alright, now that you have a good background about Lambda and some use cases, you can take a look at the most important configurations that you should be aware of for <span class="No-Break">the exam.</span></p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor1410"/><a id="_idTextAnchor1411"/>Creating and configuring a Lambda Function</h2>
			<p>First of all, you should know that you can create a Lambda function in different ways, such as via the <a id="_idTextAnchor1412"/>AWS CLI (a Lambda API reference), the AWS Lambda console, or even <a id="_idTextAnchor1413"/>deployment frameworks (for example, <em class="italic">the </em><span class="No-Break"><em class="italic">serverless framework</em></span><span class="No-Break">).</span></p>
			<p>Serverless frameworks are usually provider and programming language-independent. In other words, they usually allow you to choose where you want to deploy a serverless infrastructure from a varied list of cloud providers and <span class="No-Break">programming languages.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The concept <a id="_idTextAnchor1414"/>of serverless architecture is not specific to AWS. In fact, many cloud providers offer other services similar to AWS Lambda functions. That is why these serverless frameworks have been built – to help developers and engineers to deploy their services, wherever they want, including AWS. This is unlikely to come up in your exam, but it is something that you should know so that you are aware of different ways in which to solve your challenges as a data scientist or <span class="No-Break">data engineer.</span></p>
			<p>Since you want to pass the AWS Certified Machine Learning Specialty exam, here, you will walk through the AWS Lambda console. This is so that you become more familiar with its interface and the most important <span class="No-Break">configuration options.</span></p>
			<p>When you navigate to the Lambda console and request a new Lambda function, AWS will provide you with some <span class="No-Break">starting options:</span></p>
			<ul>
				<li><strong class="bold">Author from scratch</strong>: This is if you want to create your function <span class="No-Break">from scratch</span></li>
				<li><strong class="bold">Use a blueprint</strong>: This is if you want to create your function from sample code and a configuration preset for common <span class="No-Break">use cases</span></li>
				<li><strong class="bold">Container image</strong>: This is if you want to select a container image to deploy <span class="No-Break">your function</span></li>
				<li><strong class="bold">Browse serverless app repository</strong>: This is if you want to deploy a sample Lambda application from the AWS Serverless <span class="No-Break">Application Repository</span></li>
			</ul>
			<p>Starting from scratch, the next step is to set up your Lambda configurations. AWS splits these configurations between basic and advanced settings. In the basic configuration, you will <a id="_idTextAnchor1415"/>set your function name, runtime environment, and <a id="_idTextAnchor1416"/>permissions. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> shows <span class="No-Break">these configurations:</span></p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B21197_10_01.jpg" alt="Figure 10.1 – Creating a new Lambda function from the AWS Lambda console" width="986" height="1007"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor1417"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Creating a new Lambda function from the AWS Lambda console</p>
			<p>Here, you have a very important configuration that you should remember during your exam – the <strong class="bold">execution role</strong>. Your Lambda f<a id="_idTextAnchor1418"/>unction might need permissions to access other AWS resources, such as S3, Redshift, and more. The execution role grants permissions to your Lambda function so that it can access resources <span class="No-Break">as needed.</span></p>
			<p>You have t<a id="_idTextAnchor1419"/>o remember that your VPC and security group c<a id="_idTextAnchor1420"/>onfigurations will also interfere with how your Lambda function runs. For example, if you want to create a function that needs internet access to download something, then you have to deploy this function in a VPC with internet access. The same logic applies to other resources, such as access to relational databases, Kinesis, <span class="No-Break">and Redshift.</span></p>
			<p>Furthermore, to properly configure a Lambda function, you have to, at least, write its code, set the execution role, and make sure the VPC and security group configurations match your needs. Next, you will take a look at <span class="No-Break">other configurations.</span></p>
			<h2 id="_idParaDest-278">C<a id="_idTextAnchor1421"/><a id="_idTextAnchor1422"/>ompleting your configurations and deploying a Lambda function</h2>
			<p>Once your L<a id="_idTextAnchor1423"/>ambda is created in the AWS console, you can set additional configurations before deploying the function. One of these configurations is the event trigger. As mentioned earlier, your Lambda function can be triggered from a variety of services or <span class="No-Break">even manually.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">A very common e<a id="_idTextAnchor1424"/>xample of a trigger is <strong class="bold">Amazon EventBridge</strong>. This is an AWS service where you can schedule the execution of <span class="No-Break">your function.</span></p>
			<p>Depending on the event trigger you choose, your function will have access to different event metadata. For example, if your f<a id="_idTextAnchor1425"/>unction is triggered by a <strong class="source-inline">PUT</strong> event on S3 (for example, someone uploads a file to a particular S3 bucket), then your function will receive the metadata associated with this event – for example, the bucket name and object key. Other types of triggers will give you different types of <span class="No-Break">event metadata!</span></p>
			<p>You have access to that metadata through the event parameter that belongs to the signature of the entry point of your function. Not clear enough? You will now look at how your function code should be declared, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">def lambda_handler(event, context):</pre>
			<pre class="source-code">TODO</pre>
			<p>Here, <strong class="source-inline">lambda_handler</strong> is the method that represents the entry point of your function. When it is triggered, this method will be called, and it will receive the event metadata associated with the event trigger (through the <strong class="source-inline">event</strong> parameter). That is how you have access to the information associated with the underlying event that has triggered your function! The <strong class="source-inline">event</strong> parameter is a <span class="No-Break">JSON-like object.</span></p>
			<p>If you want t<a id="_idTextAnchor1426"/>o test your function but do not want to t<a id="_idTextAnchor1427"/>rigger it directly from the underlying event, that is no problem; you c<a id="_idTextAnchor1428"/>an use <strong class="bold">test events</strong>. They simulate the underlying event by preparing a JSON object that will be passed to <span class="No-Break">your function.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2</em> shows a very intuitive example. Suppose you have created a function that is triggered when a user uploads a file to S3, and now, you want to test your function. You can either upload a file to S3 (which forces the trigger) or create a <span class="No-Break">test event.</span></p>
			<p>By creating a test event, you can prepare a JSON object that simulates the <strong class="bold">S3-put</strong> event and then pass this object to <span class="No-Break">your function:</span></p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B21197_10_02.jpg" alt="Figure 10.2 – Creating a test event from the Lambda console" width="715" height="722"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor1429"/>Figure 10.2 – Creating a test event from the Lambda console</p>
			<p>Another type of co<a id="_idTextAnchor1430"/>nfiguration that you can set is an <strong class="bold">environment variable</strong>, which wi<a id="_idTextAnchor1431"/>ll be available on your function. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.3</em> shows how to ad<a id="_idTextAnchor1432"/>d environment variables in a <span class="No-Break">Lambda function:</span></p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B21197_10_03.jpg" alt="Figure 10.3 – Adding environment variables to a Lambda function" width="650" height="313"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">F<a id="_idTextAnchor1433"/>igure 10.3 – Adding environment variables to a Lambda function</p>
			<p>You can always come back to these basic settings to make adjustments as necessary. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.4</em> shows what you will find in the basic <span class="No-Break">settings section:</span></p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B21197_10_04.jpg" alt="Figure 10.4 – Changing the basic settings of a Lambda function" width="914" height="565"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Fi<a id="_idTextAnchor1434"/>gure 10.4 – Changing the basic settings of a Lambda function</p>
			<p>In terms of moni<a id="_idTextAnchor1435"/>toring, by default, Lambda functions produce a <strong class="bold">CloudWatch</strong> <strong class="bold">Logs</strong> stream and stan<a id="_idTextAnchor1436"/>dard metrics. You can access log information by n<a id="_idTextAnchor1437"/>avigating through your <strong class="source-inline">Lambda function monitoring</strong> section and clicking on <strong class="bold">View logs </strong><span class="No-Break"><strong class="bold">in CloudWatch</strong></span><span class="No-Break">.</span></p>
			<p>In CloudWatch, each Lambda function will<a id="_idTextAnchor1438"/> have a <strong class="bold">log group</strong> and, inside that log group, many <strong class="bold">log streams</strong>. Log streams stor<a id="_idTextAnchor1439"/>e the execution logs of the associated function. In other words, a log stream is a sequence of logs that share the same source, which, in this case, is your Lambda function. A log group is a group of log streams that share the same retention, monitoring, and access <span class="No-Break">control settings.</span></p>
			<p>You are now reaching the end of this section, but not the end of this topic on Lambda functions. As ment<a id="_idTextAnchor1440"/>ioned earlier, this AWS service has a lot <a id="_idTextAnchor1441"/>of use cases and integrates with many other services. In the next section, you will look at another AWS service that will help orchestrate executions of Lambda functions. This is known as <strong class="bold">AWS </strong><span class="No-Break"><strong class="bold">Step Functions</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-279">Work<a id="_idTextAnchor1442"/><a id="_idTextAnchor1443"/>ing with step functions</h1>
			<p>Step Functions is a<a id="_idTextAnchor1444"/>n AWS service that allows you to create workflows to orchestrate the execution of Lambda functions. This is so that you <a id="_idTextAnchor1445"/>can connect them in a sort of event sequence, known as <strong class="bold">steps</strong>. These steps are <a id="_idTextAnchor1446"/>grouped in a <span class="No-Break"><strong class="bold">state machine</strong></span><span class="No-Break">.</span></p>
			<p>Step Functions incorporates retry functionality so that you can configure your pipeline to proceed only after a particular step has succeeded. The way you set these retry configurations is b<a id="_idTextAnchor1447"/>y creating a <span class="No-Break"><strong class="bold">retry policy</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Just like the majority of AWS services, AWS Step Functions also integrates with other services, not only <span class="No-Break">AWS Lambda.</span></p>
			<p>Creating a state machine is relatively simple. All you have to do is navigate to the AWS Step Functions console and then create a new state machine. On the <strong class="bold">Create state machine</strong> page, you can specify whether you want to create your state machine from scratch or from a template, or whether you just want to run a <span class="No-Break">sample project.</span></p>
			<p>AWS will help you with this state machine creation, so even if you choose to create it from scratch, you will find code snippets for a various list of tasks, such as AWS Lambda invocation, SNS topic publication, and running <span class="No-Break">Athena queries.</span></p>
			<p>For the sake of demonstration, you will now create a very simple, but still helpful, example of how to use Step Functions to execute a Lambda function with the <strong class="source-inline">retry</strong> <span class="No-Break">option activated:</span></p>
			<pre class="console"><strong class="source-inline">{</strong></pre>
			<pre class="console"><strong class="source-inline">"Comment": "A very handy example of how to call a lamnbda function with retry option",</strong></pre>
			<pre class="console"><strong class="source-inline">"StartAt": "Invoke Lambda function",</strong></pre>
			<pre class="console"><strong class="source-inline">"States": {</strong></pre>
			<pre class="console"><strong class="source-inline">"Invoke Lambda function": {</strong></pre>
			<pre class="console"><strong class="source-inline">"Type": "Task",</strong></pre>
			<pre class="console"><strong class="source-inline">"Resource": "arn:aws:states:::lambda:invoke",</strong></pre>
			<pre class="console"><strong class="source-inline">"Parameters": {</strong></pre>
			<pre class="console"><strong class="source-inline">"FunctionName": "arn:aws:lambda:your-function-identification",</strong></pre>
			<pre class="console"><strong class="source-inline">"Payload": {</strong></pre>
			<pre class="console"><strong class="source-inline">"Input": {</strong></pre>
			<pre class="console"><strong class="source-inline">"env": "STAGE"</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<pre class="console"><strong class="source-inline">},</strong></pre>
			<pre class="console"><strong class="source-inline">"Retry": [</strong></pre>
			<pre class="console"><strong class="source-inline">{</strong></pre>
			<pre class="console"><strong class="source-inline">"ErrorEquals": ["States.ALL"],</strong></pre>
			<pre class="console"><strong class="source-inline">"IntervalSeconds": 60,</strong></pre>
			<pre class="console"><strong class="source-inline">"MaxAttempts": 5,</strong></pre>
			<pre class="console"><strong class="source-inline">"BackoffRate": 2.0</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<pre class="console"><strong class="source-inline">],</strong></pre>
			<pre class="console"><strong class="source-inline">"Next": "Example"</strong></pre>
			<pre class="console"><strong class="source-inline">},</strong></pre>
			<pre class="console"><strong class="source-inline">"Example": {</strong></pre>
			<pre class="console"><strong class="source-inline">"Type": "Pass",</strong></pre>
			<pre class="console"><strong class="source-inline">"Result": "Just to show you how to configure other steps",</strong></pre>
			<pre class="console"><strong class="source-inline">"End": true</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<p>In the prec<a id="_idTextAnchor1448"/>eding example, you created a state machine with <span class="No-Break">two steps:</span></p>
			<ul>
				<li><strong class="bold">Invoke a Lambda function</strong>: This will start the execution of your <span class="No-Break">underlying Lambda</span></li>
				<li><strong class="bold">Execute </strong><em class="italic">Example</em>: This is a simple pass task just to show you how to connect a second step in <span class="No-Break">the pipeline</span></li>
			</ul>
			<p>In the first step, you have also set up a retry policy, which will try to re-execute this task if there are any failures. You set up the interval (in seconds) to try again and to show the number of attempts. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.5</em> shows the <span class="No-Break">state machine:</span></p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B21197_10_05.jpg" alt="Figure 10.5 – The state machine" width="1384" height="508"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Fig<a id="_idTextAnchor1449"/>ure 10.5 – The state machine</p>
			<p>In the next section, you will explore various autoscaling scenarios and different ways to <span class="No-Break">handle them.</span></p>
			<h1 id="_idParaDest-280"><a id="_idTextAnchor1450"/>Scaling applications with SageMaker deployment and AWS Autoscaling</h1>
			<p>Autoscaling is a crucial aspect of deploying ML models in production environments, ensuring that applications can handle varying workloads efficiently. Amazon SageMaker, combined with AWS Auto Scaling, provides a robust solution for automatically adjusting resources based on demand. In this section, you will explore different scenarios where autoscaling is essential and how to achieve it, using SageMaker model deployment options and AWS <span class="No-Break">Auto Scaling.</span></p>
			<h2 id="_idParaDest-281"><a id="_idTextAnchor1451"/>Scenario 1 – Fluctuating inference workloads</h2>
			<p>In a retail application, the number of users making product recommendation requests can vary throughout the day, with peak loads during <span class="No-Break">specific hours.</span></p>
			<h3 id="_idParaDest-282"><a id="_idTextAnchor1452"/>Autoscaling solution</h3>
			<p>Implement autoscaling for SageMaker real-time endpoints to dynamically adjust the number of instances, based on the inference <span class="No-Break">request rate.</span></p>
			<h3 id="_idParaDest-283"><a id="_idTextAnchor1453"/>Steps</h3>
			<ol>
				<li>Configure the SageMaker endpoint to <span class="No-Break">use autoscaling.</span></li>
				<li>Set up minimum and maximum instance counts based on expected <span class="No-Break">workload variations.</span></li>
			</ol>
			<h3 id="_idParaDest-284"><a id="_idTextAnchor1454"/>Example code snippet</h3>
			<pre class="console"><strong class="source-inline">from sagemaker import get_execution_role</strong></pre>
			<pre class="console"><strong class="source-inline">from sagemaker.model import Model</strong></pre>
			<pre class="console"><strong class="source-inline">role = get_execution_role()</strong></pre>
			<pre class="console"><strong class="source-inline">model_artifact='s3://your-s3-bucket/path/to/model.tar.gz'</strong></pre>
			<pre class="console"><strong class="source-inline">model = Model(model_data=model_artifact, role=role)</strong></pre>
			<pre class="console"><strong class="source-inline">predictor = model.deploy(instance_type='ml.m4.xlarge', endpoint_name='real-time-endpoint', endpoint_auto_scaling=True)</strong></pre>
			<h2 id="_idParaDest-285"><a id="_idTextAnchor1455"/>Scenario 2 – The batch processing of large datasets</h2>
			<p>Performing batch inference on a large dataset periodically may lead to resource constraints if not <span class="No-Break">managed dynamically.</span></p>
			<h3 id="_idParaDest-286"><a id="_idTextAnchor1456"/>Autoscaling solution</h3>
			<p>Utilize AWS batch transform with SageMaker and configure autoscaling for efficient resource utilization during <span class="No-Break">batch processing.</span></p>
			<h3 id="_idParaDest-287"><a id="_idTextAnchor1457"/>Steps</h3>
			<ol>
				<li>Set up an AWS batch <span class="No-Break">transform job.</span></li>
				<li>Enable autoscaling for the underlying infrastructure to handle varying <span class="No-Break">batch sizes.</span></li>
			</ol>
			<h3 id="_idParaDest-288"><a id="_idTextAnchor1458"/>Example code snippet</h3>
			<pre class="console"><strong class="source-inline">from sagemaker.transformer import Transformer</strong></pre>
			<pre class="console"><strong class="source-inline">transformer = Transformer(model_name='your-model-name',</strong></pre>
			<pre class="console"><strong class="source-inline">                          instance_count=1,</strong></pre>
			<pre class="console"><strong class="source-inline">                          instance_type='ml.m4.xlarge',</strong></pre>
			<pre class="console"><strong class="source-inline">                          strategy='SingleRecord',</strong></pre>
			<pre class="console"><strong class="source-inline">                          assemble_with='Line',</strong></pre>
			<pre class="console"><strong class="source-inline">                          output_path='s3://your-s3-bucket/output',</strong></pre>
			<pre class="console"><strong class="source-inline">                          max_concurrent_transforms=4)  # Set max_concurrent_transforms for autoscaling</strong></pre>
			<h2 id="_idParaDest-289"><a id="_idTextAnchor1459"/>Scenario 3 – A multi-model endpoint with dynamic traffic</h2>
			<p>Multiple models are deployed on a single endpoint for A/B testing, and the traffic distribution between models <span class="No-Break">is dynamic.</span></p>
			<h3 id="_idParaDest-290"><a id="_idTextAnchor1460"/>Autoscaling solution</h3>
			<p>Leverage SageMaker multi-model endpoints with autoscaling to handle varying traffic loads across different <span class="No-Break">model versions.</span></p>
			<h3 id="_idParaDest-291"><a id="_idTextAnchor1461"/>Steps</h3>
			<ol>
				<li>Create and deploy multiple models to a SageMaker <span class="No-Break">multi-model endpoint.</span></li>
				<li>Enable autoscaling to adjust the instance count based on <span class="No-Break">traffic distribution.</span></li>
			</ol>
			<h3 id="_idParaDest-292"><a id="_idTextAnchor1462"/>Example code snippet</h3>
			<pre class="console"><strong class="source-inline">from sagemaker.multimodel import MultiModel</strong></pre>
			<pre class="console"><strong class="source-inline">multi_model = MultiModel(model_data_prefix='s3://your-s3-bucket/multi-models')</strong></pre>
			<pre class="console"><strong class="source-inline">predictor = multi_model.deploy(instance_type='ml.m4.xlarge', endpoint_name='multi-model-endpoint', endpoint_auto_scaling=True)</strong></pre>
			<h2 id="_idParaDest-293"><a id="_idTextAnchor1463"/>Scenario 4 – Continuous Model Monitoring with drift detection</h2>
			<p>You monitor models for concept drift or data quality issues and automatically adjust resources if model <span class="No-Break">performance degrades.</span></p>
			<h3 id="_idParaDest-294"><a id="_idTextAnchor1464"/>Autoscaling solution</h3>
			<p>Integrate SageMaker Model Monitor with AWS CloudWatch alarms to trigger autoscaling when drift or degradation <span class="No-Break">is detected.</span></p>
			<h3 id="_idParaDest-295"><a id="_idTextAnchor1465"/>Steps</h3>
			<ol>
				<li>Set up CloudWatch alarms to monitor model <span class="No-Break">quality metrics.</span></li>
				<li>Configure autoscaling policies to trigger when specific alarm thresholds <span class="No-Break">are breached.</span></li>
			</ol>
			<p>Scaling applications with SageMaker model deployment options and AWS Auto Scaling provides a flexible and efficient solution for handling varying workloads and ensuring optimal resource utilization. By understanding the different scenarios requiring autoscaling and following the outlined steps, you can seamlessly integrate autoscaling into your ML deployment strategy, enhancing the scalability and reliability of your applications. In the next section, you will learn and explore different ways of securing AWS <span class="No-Break">SageMaker applications.</span></p>
			<h1 id="_idParaDest-296"><a id="_idTextAnchor1466"/>Securing SageMaker applications</h1>
			<p>As ML applications become integral to business operations, securing AWS SageMaker applications is paramount to safeguard sensitive data, maintain regulatory compliance, and prevent unauthorized access. In this section, you will first dive into the reasons for securing SageMaker applications and then explore different strategies to <span class="No-Break">achieve security:</span></p>
			<ul>
				<li><strong class="bold">Reasons to secure </strong><span class="No-Break"><strong class="bold">SageMaker applications</strong></span><ul><li><strong class="bold">Data protection</strong>: ML models trained on sensitive data, such as customer information or financial records, pose a significant security risk if not adequately protected. Securing SageMaker ensures that data confidentiality and integrity are maintained throughout the ML <span class="No-Break">life cycle.</span></li><li><strong class="bold">Compliance requirements</strong>: Industries such as healthcare and finance are subject to stringent data protection regulations. Securing SageMaker helps organizations comply with standards such as the <strong class="bold">Health Insurance Portability and Accountability Act (HIPAA)</strong> or the <strong class="bold">General Data Protection Regulation (GDPR)</strong>, avoiding legal ramifications and <span class="No-Break">reputational damage.</span></li><li><strong class="bold">Preventing unauthorized access</strong>: SageMaker instances and endpoints should only be accessible to authorized personnel. Unauthorized access can lead to data breaches or misuse of ML capabilities. Robust authentication mechanisms are crucial for preventing such <span class="No-Break">security lapses.</span></li><li><strong class="bold">Model intellectual property protection</strong>: ML models represent intellectual property. Securing SageMaker ensures that the models, algorithms, and methodologies developed remain confidential and are not susceptible to intellectual property theft or <span class="No-Break">reverse engineering.</span></li></ul></li>
				<li><strong class="bold">Ways to secure </strong><span class="No-Break"><strong class="bold">SageMaker applications</strong></span><ul><li><strong class="bold">Virtual Private Cloud (VPC) endpoints for SageMaker</strong>: Deploy SageMaker instances within a VPC and use VPC endpoints for secure communication. This prevents public internet access to SageMaker endpoints, reducing the attack surface. An example code snippet is <span class="No-Break">given here:</span><pre class="source-code"><strong class="source-inline">from sagemaker import get_execution_role</strong></pre><pre class="source-code"><strong class="source-inline">from sagemaker import Session</strong></pre><pre class="source-code"><strong class="source-inline">role = get_execution_role()</strong></pre><pre class="source-code"><strong class="source-inline">sagemaker_session = Session()</strong></pre><pre class="source-code"><strong class="source-inline">vpc_config = {'SecurityGroupIds': ['sg-xxxxx'], 'Subnets': ['subnet-xxxxx']}</strong></pre><pre class="source-code"><strong class="source-inline">predictor = model.deploy(instance_type='ml.m4.xlarge', endpoint_name='secured-endpoint', vpc_config_override=vpc_config)</strong></pre></li><li><strong class="bold">Identity and Access Management (IAM) roles and policies</strong>: Leverage IAM roles to control access to SageMaker resources. Define granular policies to ensure users and services have the minimum required permissions. An example IAM policy is <span class="No-Break">given here:</span><pre class="source-code"><strong class="source-inline">{</strong></pre><pre class="source-code"><strong class="source-inline">  "Version": "2012-10-17",</strong></pre><pre class="source-code"><strong class="source-inline">  "Statement": [</strong></pre><pre class="source-code"><strong class="source-inline">   {</strong></pre><pre class="source-code"><strong class="source-inline">     "Effect": "Allow",</strong></pre><pre class="source-code"><strong class="source-inline">     "Action": "sagemaker:CreateModel",</strong></pre><pre class="source-code"><strong class="source-inline">     "Resource": "arn:aws:sagemaker:region:account-id:model/model-name"</strong></pre><pre class="source-code"><strong class="source-inline">   },</strong></pre><pre class="source-code"><strong class="source-inline">   {</strong></pre><pre class="source-code"><strong class="source-inline">      "Effect": "Deny",</strong></pre><pre class="source-code"><strong class="source-inline">      "Action": "sagemaker:CreateModel",</strong></pre><pre class="source-code"><strong class="source-inline">      "Resource": "*"</strong></pre><pre class="source-code"><strong class="source-inline">  }</strong></pre><pre class="source-code"><strong class="source-inline">]</strong></pre><pre class="source-code"><strong class="source-inline">}</strong></pre></li><li><strong class="bold">Encryption in transit and at rest</strong>: Enable encryption for data in transit and at rest. SageMaker supports encrypting data during model training, inference, and storage. An example code snippet is <span class="No-Break">given here:</span><pre class="source-code"><strong class="source-inline">from sagemaker import get_execution_role</strong></pre><pre class="source-code"><strong class="source-inline">from sagemaker import Session</strong></pre><pre class="source-code"><strong class="source-inline">role = get_execution_role()</strong></pre><pre class="source-code"><strong class="source-inline">sagemaker_session = Session()</strong></pre><pre class="source-code"><strong class="source-inline">predictor = model.deploy(instance_type='ml.m4.xlarge', endpoint_name='encrypted-endpoint', encrypt_parameters=True)</strong></pre></li><li><strong class="bold">Model monitoring with SageMaker Model Monitor</strong>: Implement continuous monitoring using SageMaker Model Monitor to detect and remediate data drift or model quality issues. This ensures that the deployed models remain accurate and reliable over time. An example code snippet is <span class="No-Break">given here:</span><pre class="source-code"><strong class="source-inline">from sagemaker.model_monitor import ModelQualityMonitor</strong></pre><pre class="source-code"><strong class="source-inline">from sagemaker.model_monitor import EndpointInput</strong></pre><pre class="source-code"><strong class="source-inline">from sagemaker import get_execution_role</strong></pre><pre class="source-code"><strong class="source-inline">role = get_execution_role()</strong></pre><pre class="source-code"><strong class="source-inline">monitor = ModelQualityMonitor(</strong></pre><pre class="source-code"><strong class="source-inline">    role=role,</strong></pre><pre class="source-code"><strong class="source-inline">    instance_count=1,</strong></pre><pre class="source-code"><strong class="source-inline">    instance_type='ml.m4.xlarge',</strong></pre><pre class="source-code"><strong class="source-inline">    volume_size_in_gb=20,</strong></pre><pre class="source-code"><strong class="source-inline">    max_runtime_in_seconds=1800</strong></pre><pre class="source-code"><strong class="source-inline">)</strong></pre></li></ul></li>
			</ul>
			<p>Securing AWS SageMaker applications is not just a best practice; it is a critical imperative in the era of data-driven decision-making. By implementing robust security measures, such as utilizing VPC endpoints, IAM roles, encryption, and continuous monitoring, organizations can fortify their SageMaker applications against potential threats and ensure the integrity of their ML workflows. As SageMaker continues to empower businesses with ML capabilities, a proactive approach to security becomes indispensable for sustained success. You have now r<a id="_idTextAnchor1467"/>eached the end of this section and the end of this chapter. Next, take a look at a summary of what you <span class="No-Break">have learned.</span></p>
			<h1 id="_idParaDest-297">Summa<a id="_idTextAnchor1468"/><a id="_idTextAnchor1469"/>ry</h1>
			<p>In this chapter, you dived into deploying ML models with Amazon SageMaker, exploring factors influencing deployment options. You looked at real-world scenarios and dissected them to try out hands-on solutions and code snippets for diverse use cases. You emphasized the crucial integration of SageMaker deployment with AWS Auto Scaling, dynamically adjusting resources based on workload variations. You focused on securing SageMaker applications, presenting practical strategies such as VPC endpoints, IAM roles, and encryption practices. Referring to the AWS documentation for clarifying any doubts is also the best option. It is always important to design your solutions in a cost-effective way, so exploring the cost-effective way to use these services is equally important as building <span class="No-Break">the solution.</span></p>
			<h1 id="_idParaDest-298"><a id="_idTextAnchor1470"/>Exam Readiness Drill – Chapter Review Questions</h1>
			<p>Apart from a solid understanding of key concepts, being able to think quickly under time pressure is a skill that will help you ace your certification exam. That is why working on these skills early on in your learning journey <span class="No-Break">is key.</span></p>
			<p>Chapter review questions are designed to improve your test-taking skills progressively with each chapter you learn and review your understanding of key concepts in the chapter at the same time. You’ll find these at the end of <span class="No-Break">each chapter.</span></p>
			<p class="callout-heading">How To Access These Resources</p>
			<p class="callout">To learn how to access these resources, head over to the chapter titled <a href="B21197_11.xhtml#_idTextAnchor1477"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Accessing the Online </em><span class="No-Break"><em class="italic">Practice Resources</em></span><span class="No-Break">.</span></p>
			<p>To open the Chapter Review Questions for this chapter, perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Click the link – <a href="https://packt.link/MLSC01E2_CH10"><span class="No-Break">https://packt.link/MLSC01E2_CH10</span></a><span class="No-Break">.</span><p class="list-inset">Alternatively, you can scan the following <strong class="bold">QR code</strong> (<span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">):</span></p></li>
			</ol>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B21197_10_06.jpg" alt="Figure 10.6 – QR code that opens Chapter Review Questions for logged-in users" width="550" height="150"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – QR code that opens Chapter Review Questions for logged-in users</p>
			<ol>
				<li value="2">Once you log in, you’ll see a page similar to the one shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B21197_10_07.jpg" alt="Figure 10.7 – Chapter Review Questions for Chapter 10" width="1398" height="745"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Chapter Review Questions for Chapter 10</p>
			<ol>
				<li value="3">Once ready, start the following practice drills, re-attempting the quiz <span class="No-Break">multiple times.</span></li>
			</ol>
			<h2 id="_idParaDest-299"><a id="_idTextAnchor1471"/>Exam Readiness Drill</h2>
			<p>For the first three attempts, don’t worry about the <span class="No-Break">time limit.</span></p>
			<h3 id="_idParaDest-300"><a id="_idTextAnchor1472"/>ATTEMPT 1</h3>
			<p>The first time, aim for at least <strong class="bold">40%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix your <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-301"><a id="_idTextAnchor1473"/>ATTEMPT 2</h3>
			<p>The second time, aim for at least <strong class="bold">60%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix any remaining <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-302"><a id="_idTextAnchor1474"/>ATTEMPT 3</h3>
			<p>The third time, aim for at least <strong class="bold">75%</strong>. Once you score 75% or more, you start working on <span class="No-Break">your timing.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">You may take more than <strong class="bold">three</strong> attempts to reach 75%. That’s okay. Just review the relevant sections in the chapter till you <span class="No-Break">get there.</span></p>
			<h1 id="_idParaDest-303"><a id="_idTextAnchor1475"/>Working On Timing</h1>
			<p>Target: Your aim is to keep the score the same while trying to answer these questions as quickly as possible. Here’s an example of how your next attempts should <span class="No-Break">look like:</span></p>
			<table id="table001-9" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Attempt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Time Taken</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">77%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>21 mins <span class="No-Break">30 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">78%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>18 mins <span class="No-Break">34 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">76%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>14 mins <span class="No-Break">44 seconds</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.1 – Sample timing practice drills on the online platform</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The time limits shown in the above table are just examples. Set your own time limits with each attempt based on the time limit of the quiz on <span class="No-Break">the website.</span></p>
			<p>With each new attempt, your score should stay above <strong class="bold">75%</strong> while your “time taken” to complete should “decrease”. Repeat as many attempts as you want till you feel confident dealing with the <span class="No-Break">time pressure.</span></p>
		</div>
	</div>
</div>
</body></html>