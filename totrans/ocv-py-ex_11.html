<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 11. Stereo Vision and 3D Reconstruction"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Stereo Vision and 3D Reconstruction</h1></div></div></div><p>In this chapter, we are going to learn about stereo vision and how we can reconstruct the 3D map of a scene. We will discuss epipolar geometry, depth maps, and 3D reconstruction. We will learn how to extract 3D information from stereo images and build a point cloud.</p><p>By the end of this chapter, you will know:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is stereo correspondence</li><li class="listitem" style="list-style-type: disc">What is epipolar geometry</li><li class="listitem" style="list-style-type: disc">What is a depth map</li><li class="listitem" style="list-style-type: disc">How to extract 3D information</li><li class="listitem" style="list-style-type: disc">How to build and visualize the 3D map of a given scene</li></ul></div><div class="section" title="What is stereo correspondence?"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec87"/>What is stereo correspondence?</h1></div></div></div><p>When we <a id="id363" class="indexterm"/>capture images, we project the 3D world around us on a 2D image plane. So technically, we only have 2D information when we capture those photos. Since all the objects in that scene are projected onto a flat 2D plane, the depth information is lost. We have no way of knowing how far an object is from the camera or how the objects are positioned with respect to each other in the 3D space. This is where stereo vision comes into the picture.</p><p>Humans are very good at inferring depth information from the real world. The reason is that we have two eyes positioned a couple of inches from each other. Each eye acts as a camera and we capture two images of the same scene from two different viewpoints, that is, one image each using the left and right eyes. So, our brain takes these two images and builds a 3D map using stereo vision. This is what we want to achieve using stereo vision algorithms. We can capture two photos of the same scene using different viewpoints, and then match the corresponding points to obtain the depth map of the scene.</p><p>Let's consider<a id="id364" class="indexterm"/> the following image:</p><div class="mediaobject"><img src="images/B04554_11_01.jpg" alt="What is stereo correspondence?"/></div><p>Now, if we capture the same scene from a different angle, it will look like this:</p><div class="mediaobject"><img src="images/B04554_11_02.jpg" alt="What is stereo correspondence?"/></div><p>As you can see, there <a id="id365" class="indexterm"/>is a large amount of movement in the positions of the objects in the image. If you consider the pixel coordinates, the values of the initial position and final position will differ by a large amount in these two images. Consider the following image:</p><div class="mediaobject"><img src="images/B04554_11_03.jpg" alt="What is stereo correspondence?"/></div><p>If we consider the same line of distance in the second image, it will look like this:</p><div class="mediaobject"><img src="images/B04554_11_04.jpg" alt="What is stereo correspondence?"/></div><p>The difference<a id="id366" class="indexterm"/> between <span class="strong"><strong>d1</strong></span> and <span class="strong"><strong>d2</strong></span> is large. Now, let's bring the box closer to the camera:</p><div class="mediaobject"><img src="images/B04554_11_05.jpg" alt="What is stereo correspondence?"/></div><p>Now, let's <a id="id367" class="indexterm"/>move the camera by the same amount as we did earlier, and capture the same scene from this angle:</p><div class="mediaobject"><img src="images/B04554_11_06.jpg" alt="What is stereo correspondence?"/></div><p>As you can see, the <a id="id368" class="indexterm"/>movement between the positions of the objects is not much. If you consider the pixel coordinates, you will see that the values are close to each other. The distance in the first image would be:</p><div class="mediaobject"><img src="images/B04554_11_07.jpg" alt="What is stereo correspondence?"/></div><p>If we consider the same line of distance in the second image, it will be as shown in the following image:</p><div class="mediaobject"><img src="images/B04554_11_08.jpg" alt="What is stereo correspondence?"/></div><p>The difference <a id="id369" class="indexterm"/>between <span class="strong"><strong>d3</strong></span> and <span class="strong"><strong>d4</strong></span> is small. We can say that the absolute difference between <span class="strong"><strong>d1</strong></span> and <span class="strong"><strong>d2</strong></span> is greater than the absolute difference between <span class="strong"><strong>d3</strong></span> and <span class="strong"><strong>d4</strong></span>. Even though the camera moved by the same amount, there is a big difference between the apparent distances between the initial and final positions. This happens because we can bring the object closer to the camera; the apparent movement decreases when you capture two images from different angles. This is the concept behind stereo correspondence: we capture two images and use this knowledge to extract the depth information from a given scene.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="What is epipolar geometry?"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec88"/>What is epipolar geometry?</h1></div></div></div><p>Before discussing<a id="id370" class="indexterm"/> epipolar geometry, let's discuss what happens when we capture two images of the same scene from two different viewpoints. Consider the following figure:</p><div class="mediaobject"><img src="images/B04554_11_09.jpg" alt="What is epipolar geometry?"/></div><p>Let's see how it happens in real life. Consider<a id="id371" class="indexterm"/> the following image:</p><div class="mediaobject"><img src="images/B04554_11_10.jpg" alt="What is epipolar geometry?"/></div><p>Now, let's capture the same scene from a different viewpoint:</p><div class="mediaobject"><img src="images/B04554_11_11.jpg" alt="What is epipolar geometry?"/></div><p>Our goal is to match <a id="id372" class="indexterm"/>the keypoints in these two images to extract the scene information. The way we do this is by extracting a matrix that can associate the corresponding points between two stereo images. This is called the <a id="id373" class="indexterm"/><span class="strong"><strong>fundamental matrix</strong></span>.</p><p>As we saw in the camera figure earlier, we can draw lines to see where they meet. These lines are called <a id="id374" class="indexterm"/><span class="strong"><strong>epipolar lines</strong></span>. The point at which the epipolar lines converge is called<a id="id375" class="indexterm"/> epipole. If you match the keypoints using SIFT, and draw the lines towards the meeting point on the left image, it will look like this:</p><div class="mediaobject"><img src="images/B04554_11_12.jpg" alt="What is epipolar geometry?"/></div><p>Following are the<a id="id376" class="indexterm"/> matching feature points in the right image:</p><div class="mediaobject"><img src="images/B04554_11_13.jpg" alt="What is epipolar geometry?"/></div><p>The lines are epipolar lines. If you take the second image as the reference, they will appear as shown in the next image:</p><div class="mediaobject"><img src="images/B04554_11_14.jpg" alt="What is epipolar geometry?"/></div><p>Following are the <a id="id377" class="indexterm"/>matching feature points in the first image:</p><div class="mediaobject"><img src="images/B04554_11_15.jpg" alt="What is epipolar geometry?"/></div><p>It's important to understand epipolar geometry and how we draw these lines. If two frames are positioned in 3D, then each epipolar line between the two frames must intersect the corresponding feature in each frame and each of the camera origins. This can be used to estimate the pose of the cameras with respect to the 3D environment. We will use this information<a id="id378" class="indexterm"/> later on, to extract 3D information from the scene. Let's take a look at the code:</p><div class="informalexample"><pre class="programlisting">import argparse

import cv2
import numpy as np

def build_arg_parser():
    parser = argparse.ArgumentParser(description='Find fundamental matrix \
            using the two input stereo images and draw epipolar lines')
    parser.add_argument("--img-left", dest="img_left", required=True,
            help="Image captured from the left view")
    parser.add_argument("--img-right", dest="img_right", required=True,
            help="Image captured from the right view")
    parser.add_argument("--feature-type", dest="feature_type",
            required=True, help="Feature extractor that will be used; can be either 'sift' or 'surf'")
    return parser

def draw_lines(img_left, img_right, lines, pts_left, pts_right):
    h,w = img_left.shape
    img_left = cv2.cvtColor(img_left, cv2.COLOR_GRAY2BGR)
    img_right = cv2.cvtColor(img_right, cv2.COLOR_GRAY2BGR)

    for line, pt_left, pt_right in zip(lines, pts_left, pts_right):
        x_start,y_start = map(int, [0, -line[2]/line[1] ])
        x_end,y_end = map(int, [w, -(line[2]+line[0]*w)/line[1] ])
        color = tuple(np.random.randint(0,255,2).tolist())
        cv2.line(img_left, (x_start,y_start), (x_end,y_end), color,1)
        cv2.circle(img_left, tuple(pt_left), 5, color, -1)
        cv2.circle(img_right, tuple(pt_right), 5, color, -1)

    return img_left, img_right

def get_descriptors(gray_image, feature_type):
    if feature_type == 'surf':
        feature_extractor = cv2.SURF()

    elif feature_type == 'sift':
        feature_extractor = cv2.SIFT()

    else:
        raise TypeError("Invalid feature type; should be either 'surf' or 'sift'")

    keypoints, descriptors = feature_extractor.detectAndCompute(gray_image, None)
    return keypoints, descriptors

if __name__=='__main__':
    args = build_arg_parser().parse_args()
    img_left = cv2.imread(args.img_left,0)  # left image
    img_right = cv2.imread(args.img_right,0)  # right image
    feature_type = args.feature_type

    if feature_type not in ['sift', 'surf']:
        raise TypeError("Invalid feature type; has to be either 'sift' or 'surf'")

    scaling_factor = 1.0
    img_left = cv2.resize(img_left, None, fx=scaling_factor,
                fy=scaling_factor, interpolation=cv2.INTER_AREA)
    img_right = cv2.resize(img_right, None, fx=scaling_factor,
                fy=scaling_factor, interpolation=cv2.INTER_AREA)

    kps_left, des_left = get_descriptors(img_left, feature_type)
    kps_right, des_right = get_descriptors(img_right, feature_type)

    # FLANN parameters
    FLANN_INDEX_KDTREE = 0
    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
    search_params = dict(checks=50)

    # Get the matches based on the descriptors
    flann = cv2.FlannBasedMatcher(index_params, search_params)
    matches = flann.knnMatch(des_left, des_right, k=2)

    pts_left_image = []
    pts_right_image = []

    # ratio test to retain only the good matches
    for i,(m,n) in enumerate(matches):
        if m.distance &lt; 0.7*n.distance:
            pts_left_image.append(kps_left[m.queryIdx].pt)
            pts_right_image.append(kps_right[m.trainIdx].pt)

    pts_left_image = np.float32(pts_left_image)
    pts_right_image = np.float32(pts_right_image)
    F, mask = cv2.findFundamentalMat(pts_left_image, pts_right_image, cv2.FM_LMEDS)

    # Selecting only the inliers
    pts_left_image = pts_left_image[mask.ravel()==1]
    pts_right_image = pts_right_image[mask.ravel()==1]

    # Drawing the lines on left image and the corresponding feature points on the right image
    lines1 = cv2.computeCorrespondEpilines (pts_right_image.reshape(-1,1,2), 2, F)
    lines1 = lines1.reshape(-1,3)
    img_left_lines, img_right_pts = draw_lines(img_left, img_right, lines1, pts_left_image, pts_right_image)

    # Drawing the lines on right image and the corresponding feature points on the left image
    lines2 = cv2.computeCorrespondEpilines (pts_left_image.reshape(-1,1,2), 1,F)
    lines2 = lines2.reshape(-1,3)
    img_right_lines, img_left_pts = draw_lines(img_right, img_left, lines2, pts_right_image, pts_left_image)

    cv2.imshow('Epi lines on left image', img_left_lines)
    cv2.imshow('Feature points on right image', img_right_pts)
    cv2.imshow('Epi lines on right image', img_right_lines)
    cv2.imshow('Feature points on left image', img_left_pts)
    cv2.waitKey()
    cv2.destroyAllWindows()</pre></div><p>Let's see what happens if<a id="id379" class="indexterm"/> we use the<a id="id380" class="indexterm"/> <span class="strong"><strong>SURF</strong></span> feature extractor. The lines in the left image will look like this:</p><div class="mediaobject"><img src="images/B04554_11_16.jpg" alt="What is epipolar geometry?"/></div><p>Following are the<a id="id381" class="indexterm"/> matching feature points in the right image:</p><div class="mediaobject"><img src="images/B04554_11_17.jpg" alt="What is epipolar geometry?"/></div><p>If you take the <a id="id382" class="indexterm"/>second image as the reference, you will see something like the following image:</p><div class="mediaobject"><img src="images/B04554_11_18.jpg" alt="What is epipolar geometry?"/></div><p>These are the <a id="id383" class="indexterm"/>matching feature points in the first image:</p><div class="mediaobject"><img src="images/B04554_11_19.jpg" alt="What is epipolar geometry?"/></div><div class="section" title="Why are the lines different as compared to SIFT?"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec48"/>Why are the lines different as compared to SIFT?</h2></div></div></div><p>SURF detects a different set of <a id="id384" class="indexterm"/>feature points, so the corresponding epipolar lines <a id="id385" class="indexterm"/>differ as well. As you can see in the images, there are more feature points detected when we use SURF. Since we have more information than before, the corresponding epipolar lines will also change accordingly.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Building the 3D map"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec89"/>Building the 3D map</h1></div></div></div><p>Now that we are<a id="id386" class="indexterm"/> familiar with epipolar geometry, let's see how to use it to build a 3D map based on stereo images. Let's consider the following figure:</p><div class="mediaobject"><img src="images/B04554_11_20.jpg" alt="Building the 3D map"/></div><p>The first step is to <a id="id387" class="indexterm"/>extract the disparity map between the two images. If you look at the figure, as we go closer to the object from the cameras along the connecting lines, the distance decreases between the points. Using this information, we can infer the distance of each point from the camera. This is called a depth map. Once we find the matching points between the two images, we can find the disparity by using epipolar lines to impose epipolar constraints.</p><p>Let's consider the following image:</p><div class="mediaobject"><img src="images/B04554_11_21.jpg" alt="Building the 3D map"/></div><p>If we capture the same<a id="id388" class="indexterm"/> scene from a different position, we get the following image:</p><div class="mediaobject"><img src="images/B04554_11_22.jpg" alt="Building the 3D map"/></div><p>If we reconstruct the <a id="id389" class="indexterm"/>3D map, it will look like this:</p><div class="mediaobject"><img src="images/B04554_11_23.jpg" alt="Building the 3D map"/></div><p>Bear in mind that <a id="id390" class="indexterm"/>these images were not captured using perfectly aligned stereo cameras. That's the reason the 3D map looks so noisy! This is just to demonstrate how we can reconstruct the real world using stereo images. Let's consider an image pair captured using stereo cameras that are properly aligned. Following is the left view image:</p><div class="mediaobject"><img src="images/B04554_11_24.jpg" alt="Building the 3D map"/></div><p>Next is the corresponding right view image:</p><div class="mediaobject"><img src="images/B04554_11_25.jpg" alt="Building the 3D map"/></div><p>If you extract the <a id="id391" class="indexterm"/>depth information and build the 3D map, it will look like this:</p><div class="mediaobject"><img src="images/B04554_11_26.jpg" alt="Building the 3D map"/></div><p>Let's rotate it to see<a id="id392" class="indexterm"/> if the depth is right for the different objects in the scene:</p><div class="mediaobject"><img src="images/B04554_11_27.jpg" alt="Building the 3D map"/></div><p>You need a software called <span class="strong"><strong>MeshLab</strong></span><a id="id393" class="indexterm"/> to visualize the 3D scene. We'll discuss about it soon. As we can see in the preceding images, the items are correctly aligned according to their distance from the camera. We can intuitively see that they are arranged in the right way, including the tilted position of the mask. We can use this technique to build many interesting things.</p><p>Let's see how to do it in OpenCV-Python:</p><div class="informalexample"><pre class="programlisting">import argparse

import cv2
import numpy as np

def build_arg_parser():
    parser = argparse.ArgumentParser(description='Reconstruct the 3D map from \
            the two input stereo images. Output will be saved in \'output.ply\'')
    parser.add_argument("--image-left", dest="image_left", required=True,
            help="Input image captured from the left")
    parser.add_argument("--image-right", dest="image_right", required=True,
            help="Input image captured from the right")
    parser.add_argument("--output-file", dest="output_file", required=True,
            help="Output filename (without the extension) where the point cloud will be saved")
    return parser

def create_output(vertices, colors, filename):
    colors = colors.reshape(-1, 3)
    vertices = np.hstack([vertices.reshape(-1,3), colors])

    ply_header = '''ply
        format ascii 1.0
        element vertex %(vert_num)d
        property float x
        property float y
        property float z
        property uchar red
        property uchar green
        property uchar blue
        end_header
    '''

    with open(filename, 'w') as f:
        f.write(ply_header % dict(vert_num=len(vertices)))
        np.savetxt(f, vertices, '%f %f %f %d %d %d')

if __name__ == '__main__':
    args = build_arg_parser().parse_args()
    image_left = cv2.imread(args.image_left)
    image_right = cv2.imread(args.image_right)
    output_file = args.output_file + '.ply'

    if image_left.shape[0] != image_right.shape[0] or \
            image_left.shape[1] != image_right.shape[1]:
        raise TypeError("Input images must be of the same size")

    # downscale images for faster processing
    image_left = cv2.pyrDown(image_left)
    image_right = cv2.pyrDown(image_right)

    # disparity range is tuned for 'aloe' image pair
    win_size = 1
    min_disp = 16
    max_disp = min_disp * 9
    num_disp = max_disp - min_disp   # Needs to be divisible by 16
    stereo = cv2.StereoSGBM(minDisparity = min_disp,
        numDisparities = num_disp,
        SADWindowSize = win_size,
        uniquenessRatio = 10,
        speckleWindowSize = 100,
        speckleRange = 32,
        disp12MaxDiff = 1,
        P1 = 8*3*win_size**2,
        P2 = 32*3*win_size**2,
        fullDP = True
    )

    print "\nComputing the disparity map ..."
    disparity_map = stereo.compute(image_left, image_right).astype(np.float32) / 16.0

    print "\nGenerating the 3D map ..."
    h, w = image_left.shape[:2]
    focal_length = 0.8*w

    # Perspective transformation matrix
    Q = np.float32([[1, 0, 0, -w/2.0],
                    [0,-1, 0,  h/2.0],
                    [0, 0, 0, -focal_length],
                    [0, 0, 1, 0]])

    points_3D = cv2.reprojectImageTo3D(disparity_map, Q)
    colors = cv2.cvtColor(image_left, cv2.COLOR_BGR2RGB)
    mask_map = disparity_map &gt; disparity_map.min()
    output_points = points_3D[mask_map]
    output_colors = colors[mask_map]

    print "\nCreating the output file ...\n"
    create_output(output_points, output_colors, output_file)</pre></div><p>To visualize the output, you need to download MeshLab<a id="id394" class="indexterm"/> from <a class="ulink" href="http://meshlab.sourceforge.net">http://meshlab.sourceforge.net</a>.</p><p>Just open the <code class="literal">output.ply</code> file<a id="id395" class="indexterm"/> using MeshLab and you'll see the 3D image. You can rotate it to get a complete 3D view of the reconstructed scene. Some of the alternatives to MeshLab are Sketchup on OS X and Windows, and Blender on Linux.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec90"/>Summary</h1></div></div></div><p>In this chapter, we learned about stereo vision and 3D reconstruction. We discussed how to extract the fundamental matrix using different feature extractors. We learned how to generate the disparity map between two images, and use it to reconstruct the 3D map of a given scene.</p><p>In the next chapter, we are going to discuss augmented reality, and how we can build a cool application where we overlay graphics on top of real world objects in a live video.</p></div></div>
</body></html>