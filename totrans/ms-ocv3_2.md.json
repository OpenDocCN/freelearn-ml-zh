["```py\n    Mat E = findEssentialMat(leftPoints, rightPoints, focal, pp);\n\n```", "```py\n    vector<KeyPoint> keypts1, keypts2; \n    Mat desc1, desc2; \n\n    // detect keypoints and extractORBdescriptors \n    Ptr<Feature2D>orb = ORB::create(2000); \n    orb->detectAndCompute(img1, noArray(), keypts1, desc1); \n    orb->detectAndCompute(img2, noArray(), keypts2, desc2); \n\n    // matching descriptors \n    Ptr<DescriptorMatcher>matcher \n    =DescriptorMatcher::create(\"BruteForce-Hamming\"); \n    vector<DMatch> matches; \n    matcher->match(desc1, desc2, matches);\n\n```", "```py\n    vector<KeyPoint> leftKpts, rightKpts; \n    // ... obtain keypoints using a feature extractor \n\n    vector<DMatch> matches; \n    // ... obtain matches using a descriptor matcher \n\n    //align left and right point sets \n    vector<Point2f>leftPts, rightPts; \n    for(size_ti = 0; i < matches.size(); i++){ \n      // queryIdx is the \"left\" image \n      leftPts.push_back(leftKpts[matches[i].queryIdx].pt); \n\n      // trainIdx is the \"right\" image \n      rightPts.push_back(rightKpts[matches[i].trainIdx].pt); \n    } \n\n    //robustly find the Essential Matrix \n    Mat status; \n    Mat E = findEssentialMat( \n      leftPts,      // points from left image \n      rightPts,     // points from right image \n      focal,        // camera focal length factor \n      pp,           // camera principal point \n      cv::RANSAC,   // use RANSAC for a robust solution \n      0.999,        // desired solution confidence level \n      1.0,          // point-to-epipolar-line threshold \n      status);        // binary vector for inliers\n\n```", "```py\n    Mat E; \n    // ... find the essential matrix \n\n    Mat R, t; //placeholders for rotation and translation \n\n    //Find Pright camera matrix from the essential matrix \n    //Cheirality check is performed internally. \n    recoverPose(E, leftPts, rightPts, R, t, focal, pp, mask);\n\n```", "```py\n    bool CheckCoherentRotation(const cv::Mat_<double>& R) { \n      if(fabsf(determinant(R))-1.0 >EPS) { \n        cerr <<\"rotation matrix is invalid\" <<endl; \n        return false;  \n      } \n      return true; \n    }\n\n```", "```py\n    #define EPS 1E-07\n\n```", "```py\n    typedef std::vector<cv::KeyPoint> Keypoints; \n    typedef std::vector<cv::Point2f>  Points2f; \n    typedef std::vector<cv::Point3f>  Points3f; \n    typedef std::vector<cv::DMatch>   Matching; \n\n    struct Features { //2D features  \n      Keypoints keyPoints; \n      Points2f  points; \n      cv::Mat   descriptors; \n    }; \n\n    struct Intrinsics { //camera intrinsic parameters \n      cv::Mat K; \n      cv::Mat Kinv; \n      cv::Mat distortion; \n    };\n\n```", "```py\n    void findCameraMatricesFromMatch( \n      const Intrinsics&   intrin, \n      const Matching&     matches, \n      const Features&     featuresLeft, \n      const Features&     featuresRight, \n      cv::Matx34f&        Pleft, \n      cv::Matx34f&        Pright) { \n      { \n        //Note: assuming fx = fy \n        const double focal = intrin.K.at<float>(0, 0);  \n        const cv::Point2d pp(intrin.K.at<float>(0, 2),     \n                             intrin.K.at<float>(1, 2)); \n\n        //align left and right point sets using the matching \n        Features left; \n        Features right; \n        GetAlignedPointsFromMatch( \n          featuresLeft,  \n          featuresRight,  \n          matches,  \n          left,  \n          right); \n\n        //find essential matrix \n        Mat E, mask; \n        E = findEssentialMat( \n          left.points,  \n          right.points,  \n          focal,  \n          pp,  \n          RANSAC,  \n          0.999,  \n          1.0,  \n          mask); \n\n        Mat_<double> R, t; \n\n        //Find Pright camera matrix from the essential matrix \n        recoverPose(E, left.points, right.points, R, t, focal, pp, mask); \n\n        Pleft = Matx34f::eye(); \n        Pright = Matx34f(R(0,0), R(0,1), R(0,2), t(0), \n                         R(1,0), R(1,1), R(1,2), t(1), \n                         R(2,0), R(2,1), R(2,2), t(2)); \n      }\n\n```", "```py\n    int findHomographyInliers( \n    const Features& left, \n    const Features& right, \n    const Matching& matches) { \n      //Get aligned feature vectors \n      Features alignedLeft; \n      Features alignedRight; \n      GetAlignedPointsFromMatch(left, right, matches, alignedLeft, \n      alignedRight); \n\n      //Calculate homography with at least 4 points \n      Mat inlierMask; \n      Mat homography; \n      if(matches.size() >= 4) { \n        homography = findHomography(alignedLeft.points,  \n                                    alignedRight.points, \n                                    cv::RANSAC, RANSAC_THRESHOLD, \n                                    inlierMask); \n      } \n\n      if(matches.size() < 4 or homography.empty()) { \n        return 0; \n      } \n\n      return countNonZero(inlierMask); \n    }\n\n```", "```py\n    //sort pairwise matches to find the lowest Homography inliers \n    map<float, ImagePair>pairInliersCt; \n    const size_t numImages = mImages.size(); \n\n    //scan all possible image pairs (symmetric) \n    for (size_t i = 0; i < numImages - 1; i++) { \n      for (size_t j = i + 1; j < numImages; j++) { \n\n        if (mFeatureMatchMatrix[i][j].size() < MIN_POINT_CT) { \n          //Not enough points in matching \n          pairInliersCt[1.0] = {i, j}; \n          continue; \n       } \n\n        //Find number of homography inliers \n        const int numInliers = findHomographyInliers( \n          mImageFeatures[i], \n          mImageFeatures[j], \n          mFeatureMatchMatrix[i][j]); \n\n        const float inliersRatio =  \n                    (float)numInliers /  \n                    (float)(mFeatureMatchMatrix[i][j].size()); \n\n        pairInliersCt[inliersRatio] = {i, j}; \n      } \n    }\n\n```", "```py\n    Vector<Point2f> points2d; //in 2D coordinates (x, y) \n    Mat normalizedPts;        //in homogeneous coordinates (x', y', 1) \n\n    undistortPoints(points2d, normalizedPts, K, Mat());\n\n```", "```py\n    Matx34f Pleft, Pright; \n    //... findCameraMatricesFromMatch \n\n    Mat normLPts; \n    Mat normRPts; \n    //... undistortPoints \n\n    //the result is a set of 3D points in homogeneous coordinates (4D) \n    Mat pts3dHomog; \n    triangulatePoints(Pleft, Pright, normLPts, normRPts, pts3dHomog); \n\n    //convert from homogeneous to 3D world coordinates \n    Mat points3d; \n    convertPointsFromHomogeneous(pts3dHomog.t(), points3d);\n\n```", "```py\n    Mat x34f P; //camera pose matrix \n    Mat points3d;     //triangulated points \n    Points2d imgPts; //2D image points that correspond to 3D points \n    Mat K;             //camera intrinsics matrix \n\n    // ... triangulate points \n\n    //get rotation and translation elements \n    Mat R; \n    Rodrigues(P.get_minor<3, 3>(0, 0), rvec); \n    Mat t = P.get_minor<3, 1>(0, 3); \n\n    //reproject 3D points back into image coordinates \n    Mat projPts; \n    projectPoints(points3d, R, t, K, Mat(),projPts); \n\n    //check individual reprojection error \n    for (size_t i = 0; i < points3d.rows; i++) { \n      const double err = norm(projPts.at<Point2f>(i) - imgPts[i]); \n\n      //check if point reprojection error is too big \n      if (err > MIN_REPROJECTION_ERROR){ \n        // Point reprojection error is too big. \n      } \n    }\n\n```", "```py\n    struct Point3DInMap { \n      // 3D point. \n      cv::Point3f p; \n\n      // Mapping from image index to a 2D point in that image's  \n      // list of features that correspond to this 3D point. \n      std::map<int, int> originatingViews; \n    };\n\n```", "```py\n    struct Image2D3DMatch { //Aligned vectors of 2D and 3D points \n      Points2f points2D; \n      Points3f points3D; \n    }; \n\n    //A mapping between an image and its set of 2D-3D aligned points \n    typedef std::map<int, Image2D3DMatch> Images2D3DMatches;\n\n```", "```py\n    Images2D3DMatches matches; \n\n    //scan all pending new views \n    for (size_tnewView = 0; newView<images.size(); newView++) { \n      if (doneViews.find(newView) != doneViews.end()) { \n        continue; //skip done views \n      } \n\n    Image2D3DMatch match2D3D; \n\n    //scan all current cloud's 3D points \n    for (const Point3DInMap&p : currentCloud) { \n\n      //scan all originating views for that 3D cloud point \n      for (const auto& origViewAndPoint : p.originatingViews) { \n\n        //check for 2D-2D matching via the match matrix \n        int origViewIndex        = origViewAndPoint.first; \n        int origViewFeatureIndex = origViewAndPoint.second; \n\n        //match matrix is upper-triangular (not symmetric)  \n        //so the left index must be the smaller one \n        bool isLeft = (origViewIndex <newView); \n        int leftVIdx = (isLeft) ? origViewIndex: newView; \n        int rightVIdx = (isLeft) ? newView : origViewIndex; \n\n        //scan all 2D-2D matches between originating and new views \n        for (const DMatch& m : matchMatrix[leftVIdx][rightVIdx]) { \n           int matched2DPointInNewView = -1; \n\n            //find a match for this new view with originating view \n            if (isLeft) { \n              //originating view is 'left' \n              if (m.queryIdx == origViewFeatureIndex) { \n                matched2DPointInNewView = m.trainIdx; \n              } \n            } else {\n              //originating view is 'right' \n              if (m.trainIdx == origViewFeatureIndex) { \n                matched2DPointInNewView = m.queryIdx; \n              } \n            } \n\n            if (matched2DPointInNewView >= 0) { \n              //This point is matched in the new view \n              const Features& newFeat = imageFeatures[newView]; \n\n              //Add the 2D point form the new view \n              match2D3D.points2D.push_back( \n                newFeat.points[matched2DPointInNewView] \n              ); \n\n              //Add the 3D point \n              match2D3D.points3D.push_back(cloudPoint.p); \n\n              break; //look no further \n            } \n          } \n        } \n      } \n      matches[viewIdx] = match2D3D;  \n    }\n\n```", "```py\n    Image2D3DMatch match; \n    //... find 2D-3D match \n\n    //Recover camera pose using 2D-3D correspondence \n    Mat rvec, tvec; \n    Mat inliers; \n    solvePnPRansac( \n      match.points3D,    //3D points \n      match.points2D,    //2D points \n      K,                   //Calibration intrinsics matrix \n      distortion,        //Calibration distortion coefficients \n      rvec,//Output extrinsics: Rotation vector \n      tvec,                //Output extrinsics: Translation vector \n      false,               //Don't use initial guess \n      100,                 //Iterations \n      RANSAC_THRESHOLD, //Reprojection error threshold \n      0.99,                //Confidence \n      inliers              //Output: inliers indicator vector \n    ); \n\n    //check if inliers-to-points ratio is too small \n    const float numInliers   = (float)countNonZero(inliers); \n    const float numPoints    = (float)match.points2D.size(); \n    const float inlierRatio = numInliers / numPoints; \n\n    if (inlierRatio < POSE_INLIERS_MINIMAL_RATIO) { \n      cerr << \"Inliers ratio is too small: \"  \n           << numInliers<< \" / \" <<numPoints<< endl; \n      //perhaps a 'return;' statement \n    } \n\n    Mat_<double>R; \n    Rodrigues(rvec, R); //convert to a 3x3 rotation matrix \n\n    P(0, 0) = R(0, 0); P(0, 1) = R(0, 1); P(0, 2) = R(0, 2); \n    P(1, 0) = R(1, 0); P(1, 1) = R(1, 1); P(1, 2) = R(1, 2); \n    P(2, 0) = R(2, 0); P(2, 1) = R(2, 1); P(2, 2) = R(2, 2); \n    P(0, 3) = tvec.at<double>(0, 3); \n    P(1, 3) = tvec.at<double>(1, 3); \n    P(2, 3) = tvec.at<double>(2, 3);\n\n```", "```py\n    // The pinhole camera is parameterized using 7 parameters: \n    // 3 for rotation, 3 for translation, 1 for focal length. \n    // The principal point is not modeled (assumed be located at the \n    // image center, and already subtracted from 'observed'),  \n    // and focal_x = focal_y. \n    struct SimpleReprojectionError { \n      using namespace ceres; \n\n      SimpleReprojectionError(double observed_x, double observed_y) : \n      observed_x(observed_x), observed_y(observed_y) {} \n\n      template<typenameT> \n      bool operator()(const T* const camera,  \n                      const T* const point, \n                      const T* const focal, \n                      T* residuals) const { \n        T p[3]; \n        // Rotate: camera[0,1,2] are the angle-axis rotation. \n        AngleAxisRotatePoint(camera, point, p); \n\n        // Translate: camera[3,4,5] are the translation. \n        p[0] += camera[3]; \n        p[1] += camera[4]; \n        p[2] += camera[5]; \n\n        // Perspective divide \n        const T xp = p[0] / p[2]; \n        const T yp = p[1] / p[2]; \n\n        // Compute projected point position (sans center of \n        // projection) \n        const T predicted_x = *focal * xp; \n        const T predicted_y = *focal * yp; \n\n        // The error is the difference between the predicted  \n        // and observed position. \n        residuals[0] = predicted_x - T(observed_x); \n        residuals[1] = predicted_y - T(observed_y); \n        return true; \n      } \n\n      // A helper construction function \n      static CostFunction* Create(const double observed_x,  \n      const double observed_y) { \n        return (newAutoDiffCostFunction<SimpleReprojectionError,  \n        2, 6, 3, 1>( \n        newSimpleReprojectionError(observed_x,  \n        observed_y))); \n      } \n      double observed_x; \n      double observed_y; \n    };\n\n```", "```py\ncmake -G \"Visual Studio 10\" \n\n```", "```py\ncmake -G \"Unix Makefiles\" \n\n```", "```py\ncmake -G Xcode\n\n```", "```py\nUSAGE ./build/ExploringSfM [options] <input-directory>\n-h [ --help ]                   Produce help message\n-d [ --console-debug ] arg (=2) Debug output to console log level \n(0 = Trace, 4 = Error).\n-v [ --visual-debug ] arg (=3)  Visual debug output to screen log    \n    level\n(0 = All, 4 = None).\n-s [ --downscale ] arg (=1)     Downscale factor for input images.\n-p [ --input-directory ] arg    Directory to find input images.\n-o [ --output-prefix ] arg (=output) Prefix for output files.\n\n```"]