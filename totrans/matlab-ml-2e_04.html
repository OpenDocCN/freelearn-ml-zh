<html><head></head><body>
<div id="_idContainer044">
<h1 class="chapter-number" id="_idParaDest-82"><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-83"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.2.1">Clustering Analysis and Dimensionality Reduction</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Clustering techniques aim to uncover concealed patterns or groupings within a dataset. </span><span class="koboSpan" id="kobo.3.2">These algorithms detect groupings without relying on any predefined labels. </span><span class="koboSpan" id="kobo.3.3">Instead, they select clusters based on the similarity between elements. </span><span class="koboSpan" id="kobo.3.4">Dimensionality reduction, on the other hand, involves transforming a dataset with numerous variables into one with fewer dimensions while preserving relevant information. </span><span class="koboSpan" id="kobo.3.5">Feature selection methods attempt to identify a subset of the original variables, while feature extraction reduces data dimensionality by transforming it into new features. </span><span class="koboSpan" id="kobo.3.6">This chapter shows us how to divide data into clusters, or groupings of similar items. </span><span class="koboSpan" id="kobo.3.7">We’ll also learn how to select features that best represent the set </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">of data.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Understanding clustering – basic concepts </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">and methods</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Understanding </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">hierarchical clustering</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Partitioning-based clustering algorithms </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">with MATLAB</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Grouping data using the </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">similarity measures</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Discovering dimensionality </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">reduction techniques</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Feature selection and feature extraction </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">using MATLAB</span></span></li>
</ul>
<h1 id="_idParaDest-84"><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.19.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.20.1">In this chapter, we will introduce basic concepts relating to machine learning. </span><span class="koboSpan" id="kobo.20.2">To understand these topics, a basic knowledge of algebra and mathematical modeling is needed. </span><span class="koboSpan" id="kobo.20.3">You will also need a working knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">MATLAB environment.</span></span></p>
<p><span class="koboSpan" id="kobo.22.1">To work with the MATLAB code in this chapter, you need the following files (available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">at </span></span><a href="https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition"><span class="No-Break"><span class="koboSpan" id="kobo.24.1">https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.25.1">):</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">Minerals.xls</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">PeripheralLocations.xls</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">YachtHydrodynamics.xlsx</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">SeedsDataset.xlsx</span></strong></span></li>
</ul>
<h1 id="_idParaDest-85"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.30.1">Understanding clustering – basic concepts and methods</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.31.1">Clustering</span></strong><span class="koboSpan" id="kobo.32.1"> is a fundamental </span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.33.1">concept in data analysis, aiming to identify meaningful </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.34.1">groupings or patterns within a dataset. </span><span class="koboSpan" id="kobo.34.2">It involves the partitioning of data points into distinct clusters based on their similarity or proximity to each other. </span><span class="koboSpan" id="kobo.34.3">In both clustering and classification, our goal is to discover the underlying rules that enable us to assign observations to the correct class. </span><span class="koboSpan" id="kobo.34.4">However, clustering differs from classification as it requires identifying a meaningful subdivision of classes as well. </span><span class="koboSpan" id="kobo.34.5">In classification, we benefit from the target variable, which provides the classification information in the training set. </span><span class="koboSpan" id="kobo.34.6">In contrast, clustering lacks such additional information, necessitating the deduction of classes by analyzing the spatial distribution of the data. </span><span class="koboSpan" id="kobo.34.7">Dense areas in the data correspond to groups of similar observations. </span><span class="koboSpan" id="kobo.34.8">If we can identify observations that are like each other but distinct from those in another cluster, we can infer that these two clusters represent different conditions. </span><span class="koboSpan" id="kobo.34.9">At this stage, three crucial aspects come </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">into play:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.36.1">How to </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">measure similarity</span></span></li>
<li><span class="koboSpan" id="kobo.38.1">How to find centroids </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">and centers</span></span></li>
<li><span class="koboSpan" id="kobo.40.1">How to define </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">a grouping</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.42.1">The notion of distance and the definition of a grouping are the fundamental components that characterize a </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">clustering algorithm.</span></span></p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.44.1">How to measure similarity</span></h2>
<p><span class="koboSpan" id="kobo.45.1">Clustering involves</span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.46.1"> the identification of data groupings based on the measure of proximity between elements. </span><span class="koboSpan" id="kobo.46.2">Proximity can refer to either similarity or dissimilarity. </span><span class="koboSpan" id="kobo.46.3">Thus, the definition of a data group relies on how we define similarity or dissimilarity. </span><span class="koboSpan" id="kobo.46.4">In many approaches, proximity is conceptualized in terms of distance within a multidimensional space. </span><span class="koboSpan" id="kobo.46.5">The effectiveness of clustering algorithms heavily depends on the choice of metric and how distance </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">is calculated.</span></span></p>
<p><span class="koboSpan" id="kobo.48.1">Clustering algorithms group elements based on their mutual distances, where membership in a particular set is determined by the proximity of an element to other members of the same set. </span><span class="koboSpan" id="kobo.48.2">Therefore, a set of observations forms a cluster when they tend to be closer to each other compared to observations in other sets. </span><span class="koboSpan" id="kobo.48.3">So, what do we mean by similarity and dissimilarity? </span><strong class="bold"><span class="koboSpan" id="kobo.49.1">Similarity</span></strong><span class="koboSpan" id="kobo.50.1"> refers</span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.51.1"> to a numerical measure of how alike two objects are. </span><span class="koboSpan" id="kobo.51.2">Higher similarity values indicate greater resemblance between objects. </span><span class="koboSpan" id="kobo.51.3">Similarities are typically non-negative and often range from 0 (no similarity) to 1 (complete similarity). </span><span class="koboSpan" id="kobo.51.4">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.52.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.53.1">.1</span></em><span class="koboSpan" id="kobo.54.1">, the difference between intercluster and intracluster distances </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">is shown.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<span class="koboSpan" id="kobo.56.1"><img alt="Figure 4.1 – Difference between inter cluster and intra cluster distances" src="image/B21156_04_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.57.1">Figure 4.1 – Difference between inter cluster and intra cluster distances</span></p>
<p><span class="koboSpan" id="kobo.58.1">On the contrary, </span><strong class="bold"><span class="koboSpan" id="kobo.59.1">dissimilarity</span></strong><span class="koboSpan" id="kobo.60.1"> represents </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.61.1">a numerical measure of how different two objects are. </span><span class="koboSpan" id="kobo.61.2">Lower dissimilarity values indicate greater similarity between objects. </span><span class="koboSpan" id="kobo.61.3">Dissimilarity is sometimes referred to as distance. </span><span class="koboSpan" id="kobo.61.4">Like similarity, dissimilarity values may also fall within the interval </span><em class="italic"><span class="koboSpan" id="kobo.62.1">[0,1]</span></em><span class="koboSpan" id="kobo.63.1">, but it is common for them to range from 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">to ∞.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">Dissimilarities between data objects can be quantified using distance metrics. </span><span class="koboSpan" id="kobo.65.2">Distances possess specific properties and can be employed to measure dissimilarity. </span><span class="koboSpan" id="kobo.65.3">For example, the </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">Euclidean distance</span></strong><span class="koboSpan" id="kobo.67.1">, denoted</span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.68.1"> as </span><em class="italic"><span class="koboSpan" id="kobo.69.1">d</span></em><span class="koboSpan" id="kobo.70.1">, can be utilized to measure the distance between two points, </span><em class="italic"><span class="koboSpan" id="kobo.71.1">x</span></em><span class="koboSpan" id="kobo.72.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.73.1">y</span></em><span class="koboSpan" id="kobo.74.1">, using the </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">following formula:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.76.1">d</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.77.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.78.1">x</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.79.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.80.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.81.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.82.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.83.1">√</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.84.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.85.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.86.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.87.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.88.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.89.1">k</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.90.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.91.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.92.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.93.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.94.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.95.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.96.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.97.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.98.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.99.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.100.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.101.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.102.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.103.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.104.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.105.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.106.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.107.1">In a two-dimensional plane, the Euclidean distance is the shortest distance between two points, represented by a straight line connecting them. </span><span class="koboSpan" id="kobo.107.2">This distance is calculated by taking the square </span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.108.1">root of the sum of the squared differences between the elements of two vectors, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">earlier formula.</span></span></p>
<p><span class="koboSpan" id="kobo.110.1">However, distance can be measured using various other metrics, such as the Minkowski distance. </span><span class="koboSpan" id="kobo.110.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.111.1">Minkowski distance</span></strong><span class="koboSpan" id="kobo.112.1"> metric </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.113.1">is a generalization of the Euclidean distance and can be expressed </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.115.1">d</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.116.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.117.1">x</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.118.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.119.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.120.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.121.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.122.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.123.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.124.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.125.1">k</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.126.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.127.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.128.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.129.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.130.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.131.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.132.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.133.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.134.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.135.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.136.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.137.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.138.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.139.1">|</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.140.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.141.1">r</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.142.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.143.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.144.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.145.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.146.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.147.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.148.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.149.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.150.1">Another option is to </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.151.1">utilize the </span><strong class="bold"><span class="koboSpan" id="kobo.152.1">Manhattan distance</span></strong><span class="koboSpan" id="kobo.153.1"> metric, which is derived from the Minkowski distance metric by setting </span><em class="italic"><span class="koboSpan" id="kobo.154.1">r = 1</span></em><span class="koboSpan" id="kobo.155.1">. </span><span class="koboSpan" id="kobo.155.2">Additionally, there is</span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.156.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.157.1">cosine distance</span></strong><span class="koboSpan" id="kobo.158.1">, which incorporates the dot product scaled by the product of the Euclidean distances from the origin. </span><span class="koboSpan" id="kobo.158.2">It quantifies the angular distance between two vectors while disregarding their scale. </span><span class="koboSpan" id="kobo.158.3">Once we have selected a suitable metric for our specific case, we </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">can proceed.</span></span></p>
<p><span class="koboSpan" id="kobo.160.1">Choosing an appropriate similarity metric for clustering holds great significance as it directly influences the quality and comprehensibility of your clustering outcomes. </span><span class="koboSpan" id="kobo.160.2">The selection of the metric should be in harmony with both the inherent traits of your data and the objectives of your clustering analysis. </span><span class="koboSpan" id="kobo.160.3">The default distance used in most clustering tools is Euclidean distance. </span><span class="koboSpan" id="kobo.160.4">However, this may be inappropriate in the presence of very noisy data or with a non-Gaussian distribution. </span><span class="koboSpan" id="kobo.160.5">In these situations, a robust alternative is the Manhattan distance, although it is important to keep in mind that robustness involves a loss of information, at least to some extent. </span><span class="koboSpan" id="kobo.160.6">In any case, you can choose between different dissimilarity measures based on the type of data and </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">research objectives.</span></span></p>
<p><span class="koboSpan" id="kobo.162.1">So far, we have examined different formulas for calculating distances between two objects. </span><span class="koboSpan" id="kobo.162.2">However, what if the objects under analysis are nominal instead of numerical? </span><span class="koboSpan" id="kobo.162.3">For nominal data, which can be represented as simple sequences or strings, various distance measures can </span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.163.1">be employed. </span><span class="koboSpan" id="kobo.163.2">One possible distance between two strings is determined by counting the number of symbols that differ between </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">the strings.</span></span></p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.165.1">How to find centroids and centers</span></h2>
<p><span class="koboSpan" id="kobo.166.1">Centroids and centers</span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.167.1"> are essential concepts in clustering that represent the central points or locations within each cluster. </span><span class="koboSpan" id="kobo.167.2">The </span><strong class="bold"><span class="koboSpan" id="kobo.168.1">centroid</span></strong><span class="koboSpan" id="kobo.169.1"> is a </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.170.1">point within the feature space that represents the average position of all the data points belonging to the associated cluster. </span><span class="koboSpan" id="kobo.170.2">It serves as a type of center of gravity for the cluster and, typically, does not coincide with any specific data point in the dataset. </span><span class="koboSpan" id="kobo.170.3">For </span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.171.1">example, in </span><strong class="bold"><span class="koboSpan" id="kobo.172.1">k-means clustering</span></strong><span class="koboSpan" id="kobo.173.1">, a centroid is the mean or average position of all the data points within a cluster. </span><span class="koboSpan" id="kobo.173.2">It serves as a representative point that minimizes the sum of squared distances between the data points and the centroid. </span><span class="koboSpan" id="kobo.173.3">The centroid is computed by taking the average of the feature values across all data points within the cluster. </span><span class="koboSpan" id="kobo.173.4">During the iterative process of k-means, data points are assigned to the cluster whose centroid they are closest to, and the centroids are updated accordingly </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">until convergence.</span></span></p>
<p><span class="koboSpan" id="kobo.175.1">The term </span><em class="italic"><span class="koboSpan" id="kobo.176.1">centers</span></em><span class="koboSpan" id="kobo.177.1"> is often used in other clustering algorithms, such as hierarchical clustering or density-based clustering. </span><span class="koboSpan" id="kobo.177.2">The centers can represent central points within clusters, but the computation method might differ depending on the algorithm. </span><span class="koboSpan" id="kobo.177.3">In </span><strong class="bold"><span class="koboSpan" id="kobo.178.1">hierarchical clustering</span></strong><span class="koboSpan" id="kobo.179.1">, centers </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.180.1">can be computed as the mean, median, or another representative point within each cluster. </span><span class="koboSpan" id="kobo.180.2">In </span><strong class="bold"><span class="koboSpan" id="kobo.181.1">density-based clustering</span></strong><span class="koboSpan" id="kobo.182.1">, the</span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.183.1"> center of a cluster can be defined as the data point with the highest density or as the medoid, which is the most centrally </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">located point.</span></span></p>
<p><span class="koboSpan" id="kobo.185.1">Both centroids and centers provide a way to summarize and represent clusters in clustering analysis. </span><span class="koboSpan" id="kobo.185.2">They offer insights into the characteristic properties of each cluster and can be used to classify </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.186.1">new data points based on their proximity to these </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">central points.</span></span></p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.188.1">How to define a grouping</span></h2>
<p><span class="koboSpan" id="kobo.189.1">A cluster is a set of </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.190.1">objects that are like each other but are dissimilar to objects contained in other clusters; a cluster can therefore be defined in terms of internal cohesion (homogeneity) and external cohesion (separation). </span><span class="koboSpan" id="kobo.190.2">The goal is to group – or segment – collections of objects into subsets, called clusters, such that objects in the same cluster have similar characteristics to each other, unlike elements assigned to different clusters. </span><span class="koboSpan" id="kobo.190.3">In this sense, the distance between points of the same cluster must be minimized while the distance between points belonging to different clusters must </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">be maximized.</span></span></p>
<p><span class="koboSpan" id="kobo.192.1">In cluster analysis, the definition of a measure that quantifies the degree of similarity between objects is crucial (see the measures in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.193.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.194.1">.1</span></em><span class="koboSpan" id="kobo.195.1">). </span><span class="koboSpan" id="kobo.195.2">The choice of this measurement greatly influences the formation of one object partition over another. </span><span class="koboSpan" id="kobo.195.3">When dealing with numeric data, the similarity measure is referred to as a distance function. </span><span class="koboSpan" id="kobo.195.4">In this case, the goal is to minimize the distance function because objects that are closer together exhibit similar characteristics and belong to the same cluster. </span><span class="koboSpan" id="kobo.195.5">On the other hand, when working with textual data, the similarity measure is used instead. </span><span class="koboSpan" id="kobo.195.6">In these cases, the aim is to maximize similarity since texts that are more similar are closer together and belong to the </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">same cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.197.1">A visual representation can help reveal the cohesion properties of clusters without explicitly and rigorously defining them. </span><span class="koboSpan" id="kobo.197.2">It is important to note that there is no universal definition that applies to every situation. </span><span class="koboSpan" id="kobo.197.3">Attempts to precisely quantify homogeneity and separation using explicit numerical indices have resulted in numerous and </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">diverse criteria.</span></span></p>
<p><span class="koboSpan" id="kobo.199.1">Once a distance measure has been selected, the next step is to determine how to form groups or clusters. </span><span class="koboSpan" id="kobo.199.2">There are two main </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.200.1">families of </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">clustering algorithms:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.202.1">Hierarchical clustering</span></strong><span class="koboSpan" id="kobo.203.1">: This approach constructs a hierarchical structure or taxonomy of the data. </span><span class="koboSpan" id="kobo.203.2">It involves creating a tree-like structure, often referred to as a </span><strong class="bold"><span class="koboSpan" id="kobo.204.1">dendrogram</span></strong><span class="koboSpan" id="kobo.205.1">, which </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.206.1">represents the relationships between the data points. </span><span class="koboSpan" id="kobo.206.2">Hierarchical clustering can be agglomerative, starting with individual data points and iteratively merging them into clusters, or divisive, beginning with one large cluster and recursively splitting it into </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">smaller clusters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.208.1">Partitioning clustering</span></strong><span class="koboSpan" id="kobo.209.1">: In this approach, the data space is divided into non-overlapping sub-zones or partitions. </span><span class="koboSpan" id="kobo.209.2">Each data point belongs to exactly one partition, and the union of all partitions covers the entire space. </span><span class="koboSpan" id="kobo.209.3">Partitioning algorithms aim to optimize a specific criterion, such as minimizing the within-cluster variance or maximizing the separation between clusters. </span><span class="koboSpan" id="kobo.209.4">Popular partitioning algorithms include k-means, where data points are assigned to the cluster with the nearest </span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.210.1">centroid, and </span><strong class="bold"><span class="koboSpan" id="kobo.211.1">Gaussian mixture models</span></strong><span class="koboSpan" id="kobo.212.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.213.1">GMMs</span></strong><span class="koboSpan" id="kobo.214.1">), which</span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.215.1"> represent clusters as a mixture of </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">Gaussian distributions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.217.1">Both hierarchical clustering and partitioning clustering methods have their own advantages and </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.218.1">applications. </span><span class="koboSpan" id="kobo.218.2">The choice between the two depends on the nature of the data, the desired interpretation of the results, and the specific goals of the </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">clustering analysis.</span></span></p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.220.1">Understanding hierarchical clustering</span></h1>
<p><span class="koboSpan" id="kobo.221.1">Hierarchical clustering</span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.222.1"> is a method of clustering that creates a hierarchy or tree-like structure of clusters. </span><span class="koboSpan" id="kobo.222.2">It iteratively merges or splits clusters based on the similarity or dissimilarity between data points. </span><span class="koboSpan" id="kobo.222.3">The resulting structure is often represented as a dendrogram, which visualizes the relationships and similarities among the </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">data points.</span></span></p>
<p><span class="koboSpan" id="kobo.224.1">There are two main types of </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">hierarchical clustering:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.226.1">Agglomerative hierarchical clustering</span></strong><span class="koboSpan" id="kobo.227.1">: This </span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.228.1">starts with each data point considered as an individual cluster and progressively merges similar clusters until all data points belong to a single cluster. </span><span class="koboSpan" id="kobo.228.2">At the beginning, each data point is treated as a separate cluster, and in each iteration, the two most similar clusters are merged into a larger cluster. </span><span class="koboSpan" id="kobo.228.3">This process continues until all data points are in one cluster. </span><span class="koboSpan" id="kobo.228.4">The merging process is guided by a distance or similarity measure, such as a Euclidean distance </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">or correlation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.230.1">Divisive hierarchical clustering</span></strong><span class="koboSpan" id="kobo.231.1">: This starts with all data points in a single cluster and recursively divides the cluster into smaller subclusters until each data point is in its own cluster. </span><span class="koboSpan" id="kobo.231.2">In each iteration, a cluster is split into two or more subclusters based on a dissimilarity measure. </span><span class="koboSpan" id="kobo.231.3">This process continues until each data point forms</span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.232.1"> its own cluster or until a stopping criterion </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">is met.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.234.1">Hierarchical clustering has several advantages. </span><span class="koboSpan" id="kobo.234.2">It does not require specifying the number of clusters in advance, as the desired number of clusters can be determined by cutting the dendrogram at a particular level. </span><span class="koboSpan" id="kobo.234.3">It also provides a visual representation of the clustering structure, allowing for interpretation and exploration of the data. </span><span class="koboSpan" id="kobo.234.4">However, hierarchical clustering can be computationally intensive for large datasets, and the choice of distance or similarity measure and the method of merging or splitting clusters can influence the results. </span><span class="koboSpan" id="kobo.234.5">Overall, hierarchical clustering is a flexible and widely used technique for exploring </span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.235.1">and understanding the underlying structure of </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">a dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.237.1">In MATLAB, hierarchical clustering is performed using several functions to create a cluster tree or dendrogram. </span><span class="koboSpan" id="kobo.237.2">The process involves grouping the data and creating a multilevel hierarchy where clusters at one level are joined as clusters at the next level. </span><span class="koboSpan" id="kobo.237.3">The </span><em class="italic"><span class="koboSpan" id="kobo.238.1">Statistics and Machine Learning Toolbox</span></em><span class="koboSpan" id="kobo.239.1"> provides the necessary functions to perform agglomerative </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">hierarchical clustering.</span></span></p>
<p><span class="koboSpan" id="kobo.241.1">The following functions are commonly used in </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">the process:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">pdist</span></strong><span class="koboSpan" id="kobo.244.1">: This function calculates pairwise distances between data points based on a chosen distance metric. </span><span class="koboSpan" id="kobo.244.2">It takes the data matrix as input and returns a </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">distance matrix.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">linkage</span></strong><span class="koboSpan" id="kobo.247.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">linkage</span></strong><span class="koboSpan" id="kobo.249.1"> function is used to compute the linkage matrix, which represents the distances between clusters. </span><span class="koboSpan" id="kobo.249.2">It takes the distance matrix as input and performs the linkage calculation based on a specified method, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">single</span></strong><span class="koboSpan" id="kobo.251.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">complete</span></strong><span class="koboSpan" id="kobo.253.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">average</span></strong><span class="koboSpan" id="kobo.255.1">. </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">single</span></strong><span class="koboSpan" id="kobo.257.1"> linkage, often referred to as the nearest neighbor method, employs the minimum distance between objects in the two clusters. </span><strong class="source-inline"><span class="koboSpan" id="kobo.258.1">complete</span></strong><span class="koboSpan" id="kobo.259.1"> linkage, also known as the farthest neighbor method, utilizes the maximum distance between objects in the two clusters. </span><span class="koboSpan" id="kobo.259.2">Finally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">average</span></strong><span class="koboSpan" id="kobo.261.1"> linkage computes the mean distance between all pairs of objects in any </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">two clusters.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.263.1">cluster</span></strong><span class="koboSpan" id="kobo.264.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.265.1">cluster</span></strong><span class="koboSpan" id="kobo.266.1"> function is responsible for forming the clusters based on the linkage matrix. </span><span class="koboSpan" id="kobo.266.2">It takes the linkage matrix and a cutoff parameter as input and returns the cluster assignments for each </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">data point.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.268.1">dendrogram</span></strong><span class="koboSpan" id="kobo.269.1">: Finally, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.270.1">dendrogram</span></strong><span class="koboSpan" id="kobo.271.1"> function is used to visualize the cluster tree or dendrogram. </span><span class="koboSpan" id="kobo.271.2">It takes the linkage matrix as input and plots the dendrogram, showing the merging of clusters at </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">each level.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.273.1">By utilizing these functions in sequence, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.274.1">clusterdata</span></strong><span class="koboSpan" id="kobo.275.1"> function in MATLAB performs agglomerative hierarchical clustering and generates the dendrogram. </span><span class="koboSpan" id="kobo.275.2">Understanding the sequence of function calls can be beneficial for comprehending the entire process and the </span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.276.1">flow of the hierarchical </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">clustering algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.278.1">Let’s explore how clustering can be performed in MATLAB by using the provided functions. </span><span class="koboSpan" id="kobo.278.2">Clustering involves identifying groups within a dataset based on proximity measures, which can refer to similarity or dissimilarity. </span><span class="koboSpan" id="kobo.278.3">Here’s an example using MATLAB. </span><span class="koboSpan" id="kobo.278.4">In MATLAB, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">pdist</span></strong><span class="koboSpan" id="kobo.280.1"> function is used to compute the distance between every pair of objects in a dataset. </span><span class="koboSpan" id="kobo.280.2">If the dataset has </span><em class="italic"><span class="koboSpan" id="kobo.281.1">k</span></em><span class="koboSpan" id="kobo.282.1"> objects, there are </span><em class="italic"><span class="koboSpan" id="kobo.283.1">k*(k – 1)/2</span></em><span class="koboSpan" id="kobo.284.1"> pairs in total. </span><span class="koboSpan" id="kobo.284.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">pdist()</span></strong><span class="koboSpan" id="kobo.286.1"> function calculates the Euclidean distance by default for a </span><em class="italic"><span class="koboSpan" id="kobo.287.1">k-by-p</span></em><span class="koboSpan" id="kobo.288.1"> data matrix, where rows correspond to observations and columns correspond to variables. </span><span class="koboSpan" id="kobo.288.2">It returns a row vector of length </span><em class="italic"><span class="koboSpan" id="kobo.289.1">k(k – 1)/2</span></em><span class="koboSpan" id="kobo.290.1">, representing pairs of observations in the source matrix. </span><span class="koboSpan" id="kobo.290.2">The distances are arranged in the order </span><em class="italic"><span class="koboSpan" id="kobo.291.1">(2,1), (3,1), ..., (k,1), (3,2), ..., (k,2), ..., (k,k–1))</span></em><span class="koboSpan" id="kobo.292.1">. </span><span class="koboSpan" id="kobo.292.2">To obtain the distance matrix, you can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">squareform()</span></strong><span class="koboSpan" id="kobo.294.1"> function, which we’ll </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">cover later.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">By default, </span><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">pdist()</span></strong><span class="koboSpan" id="kobo.298.1"> calculates the Euclidean distance, but there are various distance metrics available. </span><span class="koboSpan" id="kobo.298.2">You can specify a different metric by selecting one of the options provided in the function’s syntax. </span><span class="koboSpan" id="kobo.298.3">Some available choices include </span><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">euclidean</span></strong><span class="koboSpan" id="kobo.300.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.301.1">squaredeuclidean</span></strong><span class="koboSpan" id="kobo.302.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">seuclidean</span></strong><span class="koboSpan" id="kobo.304.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.305.1">cityblock</span></strong><span class="koboSpan" id="kobo.306.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.307.1">minkowski</span></strong><span class="koboSpan" id="kobo.308.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.309.1">chebychev</span></strong><span class="koboSpan" id="kobo.310.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.311.1">mahalanobis</span></strong><span class="koboSpan" id="kobo.312.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.313.1">cosine</span></strong><span class="koboSpan" id="kobo.314.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">correlation</span></strong><span class="koboSpan" id="kobo.316.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.317.1">spearman</span></strong><span class="koboSpan" id="kobo.318.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.319.1">hamming</span></strong><span class="koboSpan" id="kobo.320.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.321.1">jaccard</span></strong><span class="koboSpan" id="kobo.322.1">. </span><span class="koboSpan" id="kobo.322.2">Additionally, you can create a custom </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">distance function.</span></span></p>
<p><span class="koboSpan" id="kobo.324.1">Let's then analyze the MATLAB code line </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">by line</span></span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">.</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.327.1">As an example, let’s consider a matrix with six points in a Cartesian plane, represented by pairs of coordinates (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.328.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.330.1">y</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">):</span></span><pre class="source-code"><span class="koboSpan" id="kobo.332.1">
DataPoints = [100 100;90 90;10 10;10 20;90 70;50 50];</span></pre></li> <li><span class="koboSpan" id="kobo.333.1">Now, let’s apply the </span><strong class="source-inline"><span class="koboSpan" id="kobo.334.1">pdist</span></strong><span class="koboSpan" id="kobo.335.1"> function to calculate the distances between pairs of points. </span><span class="koboSpan" id="kobo.335.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.336.1">pdist</span></strong><span class="koboSpan" id="kobo.337.1"> function will return this distance information in a vector, where each element represents the distance between a pair </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">of points:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.339.1">
DistanceCalc = pdist(DataPoints);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.340.1">To gain a better understanding of the relationships among the distances, we can reformat the distance vector into a matrix using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.341.1">squareform()</span></strong><span class="koboSpan" id="kobo.342.1"> function. </span><span class="koboSpan" id="kobo.342.2">This matrix will have element </span><em class="italic"><span class="koboSpan" id="kobo.343.1">i,j</span></em><span class="koboSpan" id="kobo.344.1"> representing the distance between point </span><em class="italic"><span class="koboSpan" id="kobo.345.1">i</span></em><span class="koboSpan" id="kobo.346.1"> and point </span><em class="italic"><span class="koboSpan" id="kobo.347.1">j</span></em><span class="koboSpan" id="kobo.348.1">. </span><span class="koboSpan" id="kobo.348.2">In the resulting matrix, element </span><em class="italic"><span class="koboSpan" id="kobo.349.1">1,1</span></em><span class="koboSpan" id="kobo.350.1"> will be the distance between point 1 and itself, which</span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.351.1"> is 0. </span><span class="koboSpan" id="kobo.351.2">Element </span><em class="italic"><span class="koboSpan" id="kobo.352.1">1,2</span></em><span class="koboSpan" id="kobo.353.1"> will represent the distance between point 1 and point 2, and </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">so on:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.355.1">DistanceMatrix = squareform(DistanceCalc);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.356.1">The following matrix </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">was printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.358.1">        0   14.1421  127.2792  120.4159   31.6228   70.7107
   14.1421         0  113.1371  106.3015   20.0000   56.5685
  127.2792  113.1371         0   10.0000  100.0000   56.5685
  120.4159  106.3015   10.0000         0   94.3398   50.0000
   31.6228   20.0000  100.0000   94.3398         0   44.7214
   70.7107   56.5685   56.5685   50.0000   44.7214         0</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.359.1">It is worth noting that the distance matrix is symmetric, indicating that the distance between point </span><em class="italic"><span class="koboSpan" id="kobo.360.1">i</span></em><span class="koboSpan" id="kobo.361.1"> and point </span><em class="italic"><span class="koboSpan" id="kobo.362.1">j</span></em><span class="koboSpan" id="kobo.363.1"> is the same as the distance between point </span><em class="italic"><span class="koboSpan" id="kobo.364.1">j</span></em><span class="koboSpan" id="kobo.365.1"> and point </span><em class="italic"><span class="koboSpan" id="kobo.366.1">i</span></em><span class="koboSpan" id="kobo.367.1">. </span><span class="koboSpan" id="kobo.367.2">In certain cases, it is beneficial to normalize the values in the dataset before calculating the distance information. </span><span class="koboSpan" id="kobo.367.3">This normalization is important because variables in raw data can be measured on different scales. </span><span class="koboSpan" id="kobo.367.4">These variations in measurement scales can distort proximity calculations. </span><span class="koboSpan" id="kobo.367.5">To address this, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">zscore()</span></strong><span class="koboSpan" id="kobo.369.1"> function can be used to standardize all values in the dataset to a </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">common scale.</span></span></p></li> <li><span class="koboSpan" id="kobo.371.1">As previously mentioned, the concepts of distance and how to define groups are the key components of a clustering algorithm. </span><span class="koboSpan" id="kobo.371.2">After calculating the proximity between objects in the dataset, the next step is to determine how these objects should be grouped into clusters. </span><span class="koboSpan" id="kobo.371.3">This is where the </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">linkage</span></strong><span class="koboSpan" id="kobo.373.1"> function comes into play. </span><span class="koboSpan" id="kobo.373.2">Utilizing the distance measure generated by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">pdist()</span></strong><span class="koboSpan" id="kobo.375.1"> function, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.376.1">linkage</span></strong><span class="koboSpan" id="kobo.377.1"> function links pairs of objects that are close together, forming binary clusters. </span><span class="koboSpan" id="kobo.377.2">This process continues as the binary clusters are merged with other objects, creating larger clusters, until all objects in the original dataset are linked together in a</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.378.1"> hierarchical </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">tree structure:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.380.1">
GroupsMatrix = linkage(DistanceCalc)
GroupsMatrix =
3.0000    4.0000   10.0000
1.0000    2.0000   14.1421
5.0000    8.0000   20.0000
6.0000    9.0000   44.7214
7.0000   10.0000   50.0000</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.381.1">With the usage of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.382.1">linkage</span></strong><span class="koboSpan" id="kobo.383.1"> function, we have completed a significant portion of the task. </span><span class="koboSpan" id="kobo.383.2">This function has identified potential groupings based on the previously calculated distance measurements. </span><span class="koboSpan" id="kobo.383.3">To gain a deeper understanding of how the function operates, let’s examine the results </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">GroupsMatrix</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.387.1">In </span><strong class="source-inline"><span class="koboSpan" id="kobo.388.1">GroupsMatrix</span></strong><span class="koboSpan" id="kobo.389.1">, each row represents a newly formed cluster. </span><span class="koboSpan" id="kobo.389.2">By analyzing this diagram we can understand how the points are identified to group them into different groups. </span><span class="koboSpan" id="kobo.389.3">The first two columns indicate the points that have been linked, particularly in the earlier stages, while the subsequent columns denote the newly created clusters. </span><span class="koboSpan" id="kobo.389.4">It is important to note that hierarchical clustering, of the agglomerative type, begins with small clusters and proceeds to incorporate additional elements, generating larger clusters. </span><span class="koboSpan" id="kobo.389.5">The third column of the matrix presents the distance between these points, providing further insights into the clustering process. </span><span class="koboSpan" id="kobo.389.6">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.390.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.391.1">.2</span></em><span class="koboSpan" id="kobo.392.1">, we can see the dendrogram of the clustering method. </span><span class="koboSpan" id="kobo.392.2">By analyzing this diagram, we can understand how the points are treated to group them into </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">different groups.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.394.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.395.1">linkage</span></strong><span class="koboSpan" id="kobo.396.1"> function is responsible for utilizing the distances calculated by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.397.1">pdist()</span></strong><span class="koboSpan" id="kobo.398.1"> function to determine the order in which clusters are formed. </span><span class="koboSpan" id="kobo.398.2">Additionally, this </span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.399.1">function can calculate the distances between newly merged clusters. </span><span class="koboSpan" id="kobo.399.2">By default, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.400.1">linkage</span></strong><span class="koboSpan" id="kobo.401.1"> function employs the single </span><strong class="source-inline"><span class="koboSpan" id="kobo.402.1">linkage</span></strong><span class="koboSpan" id="kobo.403.1"> method. </span><span class="koboSpan" id="kobo.403.2">However, there are several other methods available for use, including </span><strong class="source-inline"><span class="koboSpan" id="kobo.404.1">average</span></strong><span class="koboSpan" id="kobo.405.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">centroid</span></strong><span class="koboSpan" id="kobo.407.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">complete</span></strong><span class="koboSpan" id="kobo.409.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">median</span></strong><span class="koboSpan" id="kobo.411.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">single</span></strong><span class="koboSpan" id="kobo.413.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">ward</span></strong><span class="koboSpan" id="kobo.415.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">weighted</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">.</span></span></p></li> <li><span class="koboSpan" id="kobo.419.1">Finally, to visualize the hierarchical binary cluster tree, we can utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.420.1">dendrogram()</span></strong><span class="koboSpan" id="kobo.421.1"> function. </span><span class="koboSpan" id="kobo.421.2">This function generates a dendrogram plot, which consists of U-shaped lines connecting data points in a hierarchical tree structure. </span><span class="koboSpan" id="kobo.421.3">The height of each U-shaped line represents the distance between the two connected </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">data points:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.423.1">
dendrogram(GroupsMatrix)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.424.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.425.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.426.1">.2</span></em><span class="koboSpan" id="kobo.427.1">, we can see a dendrogram of a hierarchical cluster obtained from a series of points on the </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">Cartesian plane.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer034">
<span class="koboSpan" id="kobo.429.1"><img alt="Figure 4.2 – Dendrogram of a hierarchical cluster" src="image/B21156_04_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.430.1">Figure 4.2 – Dendrogram of a hierarchical cluster</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.431.1">A dendrogram is a graphical representation in the form of a branching diagram that illustrates th</span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.432.1">e similarities among a group of entities. </span><span class="koboSpan" id="kobo.432.2">The horizontal axis of the dendrogram represents the elements being analyzed, while the vertical axis indicates the level of distance at which the fusion of two elements occurs. </span><span class="koboSpan" id="kobo.432.3">The strength of the relationships between two elements is depicted by the distance between the element’s corresponding vertical lines and the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.433.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.434.1"> axis.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.435.1">To understand the relationship between two elements in a dendrogram, we can trace a path from one element to another by following the tree diagrams and selecting the shortest path. </span><span class="koboSpan" id="kobo.435.2">The distance from the starting point to the outermost horizontal line crossed by the path reflects the degree of similarity between the </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">two elements.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.437.1">After building the dendrogram, we make a horizontal cut of the structure. </span><span class="koboSpan" id="kobo.437.2">All resulting subbranches below the horizontal cut represent a single cluster at the top level of the system and determine the corresponding cluster membership for each data sample (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.438.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.439.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">).</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.441.1">Finally, we can verify how hierarchical clustering is able to group objects. </span><span class="koboSpan" id="kobo.441.2">In a dendrogram, the disparity between branches is reflected by their respective heights: the taller the branch, the greater the dissimilarity. </span><span class="koboSpan" id="kobo.441.3">This vertical measurement is referred to as the cophenetic distance, representing the dissimilarity between the two objects that form the cluster. </span><span class="koboSpan" id="kobo.441.4">To evaluate and compare the cophenetic distances between two objects, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.442.1">cophenet()</span></strong><span class="koboSpan" id="kobo.443.1"> function can be employed. </span><span class="koboSpan" id="kobo.443.2">Verifying the effectiveness of the dissimilarity measurement can be accomplished by comparing the outcomes of clustering the same dataset using various distance calculation methods or clustering algorithms. </span><span class="koboSpan" id="kobo.443.3">This approach allows us to assess and contrast the performance of different techniques. </span><span class="koboSpan" id="kobo.443.4">To</span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.444.1"> begin, let’s apply this evaluation to the calculations that have just </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">been performed:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.446.1">
DistancesCheck = cophenet(GroupsMatrix, DistanceCalc)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.447.1">The following results </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">are returned:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.449.1">DistancesCheck =
    0.8096</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.450.1">The validity of a cluster tree is assessed based on how well it reflects the distances between objects as measured by the chosen distance metric. </span><span class="koboSpan" id="kobo.450.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">cophenet()</span></strong><span class="koboSpan" id="kobo.452.1"> function calculates the cophenetic correlation coefficient for the hierarchical cluster tree, which indicates the strength of the correlation between object distances and the linkage of the objects within the tree. </span><span class="koboSpan" id="kobo.452.2">A higher cophenetic correlation coefficient (closer to </span><em class="italic"><span class="koboSpan" id="kobo.453.1">1</span></em><span class="koboSpan" id="kobo.454.1">) indicates more accurate </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">hierarchical clustering.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.456.1">To enhance the model’s performance, we can recompute the distances between points using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.457.1">pdist()</span></strong><span class="koboSpan" id="kobo.458.1"> function, utilizing a different metric this time, while working with the </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">same dataset:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.460.1">NewDistanceCalc = pdist(DataPoints, 'cosine');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.461.1">Now, utilizing the new distance measure generated by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.462.1">pdist()</span></strong><span class="koboSpan" id="kobo.463.1"> function with the cosine metric, we will form clusters by linking objects that are in close proximity. </span><span class="koboSpan" id="kobo.463.2">However, this time we will employ a different algorithm for calculating the distance between clusters. </span><span class="koboSpan" id="kobo.463.3">Specifically, we will use the weighted method, which computes the weighted average distance. </span><span class="koboSpan" id="kobo.463.4">By doing so, we can establish clusters based on the closeness of objects, considering their </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">respective weights:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.465.1">NewGroupsMatrix = linkage(NewDistanceCalc,'weighted');</span></pre></li> <li><span class="koboSpan" id="kobo.466.1">Lastly, we can employ </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.467.1">the </span><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">cophenet()</span></strong><span class="koboSpan" id="kobo.469.1"> function to evaluate the </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">clustering solution:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.471.1">
NewDistancesCheck = cophenet(NewGroupsMatrix, NewDistanceCalc)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.472.1">The following results </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">are printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.474.1">NewDistancesCheck =
    0.9302</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.475.1">The outcome demonstrates an enhancement in the performance of hierarchical clustering by utilizing a different distance metric and </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">linkage method.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.477.1">Hierarchical clustering is a versatile clustering method with a wide range of applications across various domains. </span><span class="koboSpan" id="kobo.477.2">Some of the</span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.478.1"> typical areas where hierarchical clustering is</span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.479.1"> applied </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.480.1">include </span><strong class="bold"><span class="koboSpan" id="kobo.481.1">image processing</span></strong><span class="koboSpan" id="kobo.482.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.483.1">text analysis</span></strong><span class="koboSpan" id="kobo.484.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.485.1">market research</span></strong><span class="koboSpan" id="kobo.486.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.487.1">customer segmentation</span></strong><span class="koboSpan" id="kobo.488.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.489.1">ecology and </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.490.1">environmental science</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.492.1">Having </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.493.1">introduced</span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.494.1"> the basic concepts of clustering, we can now move on to a practical example to discover how to use the tools available in the MATLAB environment to correctly perform a partitioning </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">clustering analysis.</span></span></p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.496.1">Partitioning-based clustering algorithms with MATLAB</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.497.1">Partitioning-based clustering</span></strong><span class="koboSpan" id="kobo.498.1"> is a type of clustering algorithm that aims to divide a dataset into </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.499.1">distinct groups or</span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.500.1"> partitions. </span><span class="koboSpan" id="kobo.500.2">In this approach, each data point is assigned to exactly one cluster, and the goal is to minimize the intra-cluster distance while maximizing the inter-cluster distance. </span><span class="koboSpan" id="kobo.500.3">The most popular partitioning-based clustering algorithms include k-medoids, fuzzy c-means, and hierarchical k-means. </span><span class="koboSpan" id="kobo.500.4">These algorithms vary in their approach and objectives, but they all aim to partition the data into well-separated clusters based on some distance or </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">similarity measure.</span></span></p>
<h2 id="_idParaDest-91"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.502.1">Introducing the k-means algorithm</span></h2>
<p><span class="koboSpan" id="kobo.503.1">One of the most </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.504.1">well-known partitioning-based clustering algorithms is k-means. </span><span class="koboSpan" id="kobo.504.2">In k-means clustering, the algorithm attempts to partition the data into </span><em class="italic"><span class="koboSpan" id="kobo.505.1">k</span></em><span class="koboSpan" id="kobo.506.1"> clusters, where </span><em class="italic"><span class="koboSpan" id="kobo.507.1">k</span></em><span class="koboSpan" id="kobo.508.1"> is a predefined number specified by the user. </span><span class="koboSpan" id="kobo.508.2">The algorithm iteratively assigns data points to the nearest cluster centroid and recalculates the centroid positions until convergence is achieved. </span><span class="koboSpan" id="kobo.508.3">The result is a set of </span><em class="italic"><span class="koboSpan" id="kobo.509.1">k</span></em><span class="koboSpan" id="kobo.510.1"> clusters, each represented by its centroid. </span><span class="koboSpan" id="kobo.510.2">The process involves iteratively relocating data instances by transferring them from one cluster to another, beginning with an </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">initial partitioning.</span></span></p>
<p><span class="koboSpan" id="kobo.512.1">The k-means algorithm, developed by James MacQueen in 1967 (MacQueen, J. </span><span class="koboSpan" id="kobo.512.2">(1967) </span><em class="italic"><span class="koboSpan" id="kobo.513.1">Some methods for classification and analysis of multivariate observations</span></em><span class="koboSpan" id="kobo.514.1">, Proceedings of the Fifth Berkeley Symposium On Mathematical Statistics and Probabilities, 1, 281-296.), is a clustering algorithm that partitions a group of objects into </span><em class="italic"><span class="koboSpan" id="kobo.515.1">k</span></em><span class="koboSpan" id="kobo.516.1"> clusters based on their attributes. </span><span class="koboSpan" id="kobo.516.2">It is a variation of</span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.517.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.518.1">expectation-maximization</span></strong><span class="koboSpan" id="kobo.519.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.520.1">EM</span></strong><span class="koboSpan" id="kobo.521.1">) algorithm, with the objective of determining </span><em class="italic"><span class="koboSpan" id="kobo.522.1">k</span></em><span class="koboSpan" id="kobo.523.1"> data groups generated by Gaussian distributions. </span><span class="koboSpan" id="kobo.523.2">Unlike EM, k-means calculates the Euclidean distance to measure dissimilarity between </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">data items.</span></span></p>
<p><span class="koboSpan" id="kobo.525.1">In k-means, objects are represented as vectors in a vector space. </span><span class="koboSpan" id="kobo.525.2">The algorithm aims to minimize the total intra-cluster variance or standard deviation. </span><span class="koboSpan" id="kobo.525.3">Each cluster is represented by </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">a centroid.</span></span></p>
<p><span class="koboSpan" id="kobo.527.1">In the context of a set of observations (</span><em class="italic"><span class="koboSpan" id="kobo.528.1">x1, x2, …, xn</span></em><span class="koboSpan" id="kobo.529.1">), where each observation is represented as a d-dimensional real vector, the objective of k-means clustering is to divide the </span><em class="italic"><span class="koboSpan" id="kobo.530.1">n</span></em><span class="koboSpan" id="kobo.531.1"> observations into </span><em class="italic"><span class="koboSpan" id="kobo.532.1">k (≤ n)</span></em><span class="koboSpan" id="kobo.533.1"> sets, denoted as </span><em class="italic"><span class="koboSpan" id="kobo.534.1">S = {S1, S2, …, Sk}</span></em><span class="koboSpan" id="kobo.535.1">, in a way that minimizes the within-cluster variance. </span><span class="koboSpan" id="kobo.535.2">The objective equation is </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">shown here:</span></span></p>
<p><span class="_-----MathTools-_Math_Function_v-normal"><span class="koboSpan" id="kobo.537.1">Min</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.538.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.539.1">S</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.540.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.541.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.542.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.543.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.544.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.545.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.546.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.547.1">k</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.548.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.549.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.550.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.551.1">x</span></span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.552.1">∈</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.553.1">S</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.554.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.555.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.556.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.557.1">‖</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.558.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.559.1">x</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.560.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.561.1">μ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.562.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.563.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.564.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.565.1">‖</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.566.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.567.1">2</span></span></span></p>
<p><span class="koboSpan" id="kobo.568.1">In the previous equation, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.569.1">μ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.570.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.571.1">i</span></span><span class="koboSpan" id="kobo.572.1"> is the mean of the points in </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.573.1">S</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.574.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.575.1">i</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.577.1">The algorithm follows an </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">iterative procedure:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.579.1">Choose the</span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.580.1"> number of clusters, </span><em class="italic"><span class="koboSpan" id="kobo.581.1">k</span></em><span class="koboSpan" id="kobo.582.1">. </span><span class="koboSpan" id="kobo.582.2">Choosing the right value for </span><em class="italic"><span class="koboSpan" id="kobo.583.1">k</span></em><span class="koboSpan" id="kobo.584.1">, , is a crucial step as it directly impacts the quality of your clustering outcomes. </span><span class="koboSpan" id="kobo.584.2">Numerous techniques are available to ascertain the optimal </span><em class="italic"><span class="koboSpan" id="kobo.585.1">k</span></em><span class="koboSpan" id="kobo.586.1">, including the elbow method, silhouette score, cross-validation, and hierarchical clustering, </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">among others.</span></span></li>
<li><span class="koboSpan" id="kobo.588.1">Initialize </span><em class="italic"><span class="koboSpan" id="kobo.589.1">k</span></em><span class="koboSpan" id="kobo.590.1"> partitions and assign each data point randomly or using </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">heuristic information.</span></span></li>
<li><span class="koboSpan" id="kobo.592.1">Calculate the centroid for each cluster, which is the mean of all the points within </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">the cluster.</span></span></li>
<li><span class="koboSpan" id="kobo.594.1">Calculate the distance between each data point and each </span><span class="No-Break"><span class="koboSpan" id="kobo.595.1">cluster centroid.</span></span></li>
<li><span class="koboSpan" id="kobo.596.1">Create a new partition by assigning each data point to the cluster with the </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">closest centroid.</span></span></li>
<li><span class="koboSpan" id="kobo.598.1">Recalculate the centroids for the </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">new clusters.</span></span></li>
<li><span class="koboSpan" id="kobo.600.1">Repeat </span><em class="italic"><span class="koboSpan" id="kobo.601.1">steps 4</span></em><span class="koboSpan" id="kobo.602.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.603.1">6</span></em><span class="koboSpan" id="kobo.604.1"> until the </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">algorithm converges.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.606.1">The goal is to determine the position of </span><em class="italic"><span class="koboSpan" id="kobo.607.1">k</span></em><span class="koboSpan" id="kobo.608.1"> centroids, with each centroid representing a cluster. </span><span class="koboSpan" id="kobo.608.2">The initial positions of the centroids significantly impact the results, and it is beneficial to place them as far apart as possible. </span><span class="koboSpan" id="kobo.608.3">Each object is then associated with the nearest centroid, resulting in an initial grouping. </span><span class="koboSpan" id="kobo.608.4">In subsequent iterations, new centroids are recalculated as the cluster barycenter based on the previous iteration’s results. </span><span class="koboSpan" id="kobo.608.5">Data points are reassigned </span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.609.1">to the new closest centroids. </span><span class="koboSpan" id="kobo.609.2">This process continues until the centroids no longer move, indicating convergence. </span><span class="koboSpan" id="kobo.609.3">We can see how </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.610.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.611.1">.4</span></em><span class="koboSpan" id="kobo.612.1"> illustrates the positions of </span><em class="italic"><span class="koboSpan" id="kobo.613.1">k</span></em><span class="koboSpan" id="kobo.614.1"> centroids in the </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">data distribution.</span></span></p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.616.1">Using k-means in MATLAB</span></h2>
<p><span class="koboSpan" id="kobo.617.1">In MATLAB, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.618.1">kmeans()</span></strong><span class="koboSpan" id="kobo.619.1"> function</span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.620.1"> is used to perform</span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.621.1"> k-means clustering. </span><span class="koboSpan" id="kobo.621.2">This function partitions the data into </span><em class="italic"><span class="koboSpan" id="kobo.622.1">k</span></em><span class="koboSpan" id="kobo.623.1"> mutually exclusive clusters and returns the index of the cluster to which each object is assigned. </span><span class="koboSpan" id="kobo.623.2">The clusters are defined by the objects and their centroids. </span><span class="koboSpan" id="kobo.623.3">The centroid of each cluster is the point that minimizes the sum of distances from all objects in that cluster. </span><span class="koboSpan" id="kobo.623.4">The calculation of cluster centroids varies depending on the chosen distance measure, aiming to minimize the sum of the specified measure. </span><span class="koboSpan" id="kobo.623.5">Different distance measures and methods for minimizing distances can be specified as input parameters to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.624.1">kmeans()</span></strong><span class="koboSpan" id="kobo.625.1"> function. </span><span class="koboSpan" id="kobo.625.2">Here is a list summarizing the available </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">distance measures:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.627.1">sqeuclidean</span></strong><span class="koboSpan" id="kobo.628.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.629.1">Squared Euclidean</span></strong><span class="koboSpan" id="kobo.630.1"> distance (default). </span><span class="koboSpan" id="kobo.630.2">Each</span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.631.1"> centroid is the mean of the points in </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">that cluster.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.633.1">cityblock</span></strong><span class="koboSpan" id="kobo.634.1">: Sum of absolute differences. </span><span class="koboSpan" id="kobo.634.2">Each centroid is the component-wise median of the points in </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">that cluster.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.636.1">cosine</span></strong><span class="koboSpan" id="kobo.637.1">: 1 minus the cosine of the included angle between points. </span><span class="koboSpan" id="kobo.637.2">Each centroid is the mean of the points in that cluster after normalizing those points to the unit of the </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">Euclidean length.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.639.1">correlation</span></strong><span class="koboSpan" id="kobo.640.1">: 1 minus the sample correlation between points. </span><span class="koboSpan" id="kobo.640.2">Each centroid is the component-wise mean of the points in that cluster after centering and normalizing those points with a mean of 0 and a standard deviation </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">of 1</span></span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.643.1">hamming</span></strong><span class="koboSpan" id="kobo.644.1">: Suitable for binary data only. </span><span class="koboSpan" id="kobo.644.2">This measures the proportion of differing bits. </span><span class="koboSpan" id="kobo.644.3">Each centroid is the component-wise median of the points in </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">that cluster.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.646.1">By default, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.647.1">kmeans()</span></strong><span class="koboSpan" id="kobo.648.1"> function uses the k-means++ algorithm for cluster center initialization and the squared Euclidean metric to </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">determine distances.</span></span></p>
<p><span class="koboSpan" id="kobo.650.1">K-means++ enhances the k-means algorithm by introducing a more sophisticated approach to initializing cluster centroids. </span><span class="koboSpan" id="kobo.650.2">The initialization procedure in k-means++ comprises the </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.652.1">Begin by randomly selecting a single data point as the </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">initial centroid.</span></span></li>
<li><span class="koboSpan" id="kobo.654.1">For each subsequent centroid (up to </span><em class="italic"><span class="koboSpan" id="kobo.655.1">k</span></em><span class="koboSpan" id="kobo.656.1">), pick the next centroid from the data points with a probability that is proportional to the square of the distance between each data point and the nearest existing centroid. </span><span class="koboSpan" id="kobo.656.2">This probabilistic selection ensures that the initial centroids are distributed in a manner that optimally spaces them apart and reduces sensitivity to the initial </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">centroid choices.</span></span></li>
<li><span class="koboSpan" id="kobo.658.1">After initializing the centroids using the k-means++ method, the algorithm proceeds in the same fashion as standard k-means. </span><span class="koboSpan" id="kobo.658.2">It iteratively assigns data points to the closest centroids and updates the centroids until convergence </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">is achieved.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.660.1">K-means++ exhibits</span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.661.1"> a</span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.662.1"> tendency for swifter and more consistent convergence due to its enhanced initialization method, which diminishes the likelihood of it becoming trapped in local optima. </span><span class="koboSpan" id="kobo.662.2">As a consequence, it often yields superior clustering quality by virtue of the well-dispersed initial centroids, ultimately resulting in more advantageous cluster assignments. </span><span class="koboSpan" id="kobo.662.3">Let’s now analyze a practical application of </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">this algorithm:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.664.1">To illustrate the usage of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.665.1">kmeans()</span></strong><span class="koboSpan" id="kobo.666.1"> function, let’s consider a dataset containing measurements of the specific weight and hardness (Mohs scale) of minerals extracted from different quarries. </span><span class="koboSpan" id="kobo.666.2">These measurements are stored in an </span><strong class="source-inline"><span class="koboSpan" id="kobo.667.1">xls</span></strong><span class="koboSpan" id="kobo.668.1"> file named </span><strong class="source-inline"><span class="koboSpan" id="kobo.669.1">Minerals.xls</span></strong><span class="koboSpan" id="kobo.670.1">. </span><span class="koboSpan" id="kobo.670.2">We can start by importing the data into the </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">MATLAB workspace:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.672.1">
InputData = readmatrix('Minerals.xls');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.673.1">So, let’s look at the imported data by plotting a simple </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">scatter plot:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer035">
<span class="koboSpan" id="kobo.675.1"><img alt="Figure 4.3 – Distribution of dataset (x is the weight and y is the hardness (Mohs scale) of minerals)" src="image/B21156_04_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.676.1">Figure 4.3 – Distribution of dataset (x is the weight and y is the hardness (Mohs scale) of minerals)</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.677.1">By examining </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.678.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.679.1">.3</span></em><span class="koboSpan" id="kobo.680.1">, it </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.681.1">appears</span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.682.1"> that the data points are concentrated in four distinct regions, each characterized by different values of the two variables (</span><em class="italic"><span class="koboSpan" id="kobo.683.1">x,y</span></em><span class="koboSpan" id="kobo.684.1">). </span><span class="koboSpan" id="kobo.684.2">This observation suggests that a cluster analysis with a fixed value of </span><em class="italic"><span class="koboSpan" id="kobo.685.1">k = 4 </span></em><span class="koboSpan" id="kobo.686.1">should be conducted. </span><span class="koboSpan" id="kobo.686.2">To ensure reproducibility and obtain consistent results, we can set the following </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">parameters accordingly.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.688.1">We can set the seed to permit the reproducibility of the experiment. </span><span class="koboSpan" id="kobo.688.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.689.1">rgn()</span></strong><span class="koboSpan" id="kobo.690.1"> function is responsible for controlling the generation of random numbers in MATLAB. </span><span class="koboSpan" id="kobo.690.2">By setting the seed using </span><strong class="source-inline"><span class="koboSpan" id="kobo.691.1">rgn()</span></strong><span class="koboSpan" id="kobo.692.1">, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.693.1">rand()</span></strong><span class="koboSpan" id="kobo.694.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.695.1">randi()</span></strong><span class="koboSpan" id="kobo.696.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.697.1">randn()</span></strong><span class="koboSpan" id="kobo.698.1"> functions will produce a predictable sequence </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">of numbers:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.700.1">
rng(5);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.701.1">Now, we can proceed with the application of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.702.1">kmeans()</span></strong><span class="koboSpan" id="kobo.703.1"> function for </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">our analysis:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.705.1">[IdCluster,Centroid] = kmeans(InputData,4);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.706.1">Two variables have been generated: </span><strong class="source-inline"><span class="koboSpan" id="kobo.707.1">IdCluster</span></strong><span class="koboSpan" id="kobo.708.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.709.1">Centroid</span></strong><span class="koboSpan" id="kobo.710.1">. </span><strong class="source-inline"><span class="koboSpan" id="kobo.711.1">IdCluster</span></strong><span class="koboSpan" id="kobo.712.1"> is a vector that stores the predicted cluster indices for each observation in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.713.1">InputData</span></strong><span class="koboSpan" id="kobo.714.1">. </span><span class="koboSpan" id="kobo.714.2">The centroid is a 4-by-2 matrix that represents the final </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.715.1">centroid</span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.716.1"> locations for the four clusters. </span><span class="koboSpan" id="kobo.716.2">By default, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.717.1">kmeans()</span></strong><span class="koboSpan" id="kobo.718.1"> function utilizes the k-means++ algorithm for centroid initialization and employs the squared Euclidean </span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">distance measure.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.720.1">Now that we have assigned every data point in the original dataset to one of the four clusters, we can visualize the clusters in a </span><span class="No-Break"><span class="koboSpan" id="kobo.721.1">scatter plot:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.722.1">gscatter(InputData(:,1), InputData(:,2), IdCluster,'bgrm','x*o^')
hold on
plot(Centroid(:,1),Centroid(:,2),'x','LineWidth',4,'MarkerEdgeColor','k','MarkerSize',25)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.723.1">The plot shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.724.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.725.1">.4</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.726.1">is printed:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer036">
<span class="koboSpan" id="kobo.727.1"><img alt="Figure 4.4 –  Scatter plot of clusters (x is the weight and y is the hardness (Mohs scale) of minerals)" src="image/B21156_04_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.728.1">Figure 4.4 –  Scatter plot of clusters (x is the weight and y is the hardness (Mohs scale) of minerals)</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.729.1">Upon analyzing </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.730.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.731.1">.4</span></em><span class="koboSpan" id="kobo.732.1">, it is</span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.733.1"> evident that </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.734.1">the </span><strong class="source-inline"><span class="koboSpan" id="kobo.735.1">kmeans()</span></strong><span class="koboSpan" id="kobo.736.1"> function has successfully identified distinct clusters with clear separation. </span><span class="koboSpan" id="kobo.736.2">Each cluster is automatically visualized using a different color, making it easy to identify them, especially at the boundaries where the data points may overlap. </span><span class="koboSpan" id="kobo.736.3">Additionally, the use of different markers in the scatter plot aids in distinguishing the </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">data points.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.738.1">To highlight the position of each centroid, we have specifically set the marker as </span><strong class="source-inline"><span class="koboSpan" id="kobo.739.1">x</span></strong><span class="koboSpan" id="kobo.740.1"> and adjusted its appearance by setting the line thickness to </span><strong class="source-inline"><span class="koboSpan" id="kobo.741.1">4</span></strong><span class="koboSpan" id="kobo.742.1">, marker color to </span><strong class="source-inline"><span class="koboSpan" id="kobo.743.1">black</span></strong><span class="koboSpan" id="kobo.744.1">, and marker size to </span><strong class="source-inline"><span class="koboSpan" id="kobo.745.1">25</span></strong><span class="koboSpan" id="kobo.746.1">. </span><span class="koboSpan" id="kobo.746.2">The centroids serve as the center of mass for their respective clusters and minimize the sum of squared distances from the other data points, as </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">mentioned earlier.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.748.1">Upon further examination of the figure, it becomes apparent that cluster boundaries are not always well defined, particularly in this type of chart. </span><span class="koboSpan" id="kobo.748.2">Near the boundaries, there are data points that merge together, making it challenging to determine which cluster they </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">belong to.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.750.1">To gain a deeper understanding of the separation and boundaries of the clusters, we can utilize a </span><strong class="bold"><span class="koboSpan" id="kobo.751.1">silhouette plot</span></strong><span class="koboSpan" id="kobo.752.1">, which </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.753.1">provides insights into how well the clusters are separated. </span><span class="koboSpan" id="kobo.753.2">The silhouette plot measures the proximity of each data point in one cluster to the data points in the neighboring clusters. </span><span class="koboSpan" id="kobo.753.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.754.1">silhouette()</span></strong><span class="koboSpan" id="kobo.755.1"> function is specifically designed to generate cluster silhouettes based on the provided cluster indices from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.756.1">kmeans()</span></strong><span class="koboSpan" id="kobo.757.1"> function. </span><span class="koboSpan" id="kobo.757.2">This function takes a data matrix as input, where each row represents a data point and each column represents its coordinates. </span><span class="koboSpan" id="kobo.757.3">The cluster indices can be categorical variables, numeric vectors, character matrices, or cell arrays of character vectors, where </span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.758.1">each element represents </span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.759.1">the cluster assignment for a </span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">data point.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.761.1">When using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.762.1">silhouette()</span></strong><span class="koboSpan" id="kobo.763.1"> function, any </span><strong class="source-inline"><span class="koboSpan" id="kobo.764.1">NaN</span></strong><span class="koboSpan" id="kobo.765.1">or empty character vectors in the cluster indices are treated as missing values, and the corresponding rows in the data matrix are ignored. </span><span class="koboSpan" id="kobo.765.2">By default, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.766.1">silhouette()</span></strong><span class="koboSpan" id="kobo.767.1"> function calculates the squared Euclidean distance between the data points in the input matrix. </span><span class="koboSpan" id="kobo.767.2">Using the silhouette plot, we can gain a better understanding of the separation and proximity of the data points within and between clusters, providing valuable insights into the quality of the </span><span class="No-Break"><span class="koboSpan" id="kobo.768.1">clustering results:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.769.1">
silhouette(InputData, IdCluster)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.770.1">The following figure shows a silhouette plot for </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.771.1">kmeans</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.772.1"> clustering.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer037">
<span class="koboSpan" id="kobo.773.1"><img alt="Figure 4.5 – Silhouette plot for k-means clustering" src="image/B21156_04_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.774.1">Figure 4.5 – Silhouette plot for k-means clustering</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.775.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.776.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.777.1">.5</span></em><span class="koboSpan" id="kobo.778.1">, the </span><em class="italic"><span class="koboSpan" id="kobo.779.1">x</span></em><span class="koboSpan" id="kobo.780.1"> axis represents the silhouette values, which range from </span><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">+1</span></strong><span class="koboSpan" id="kobo.782.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.783.1">-1</span></strong><span class="koboSpan" id="kobo.784.1">. </span><span class="koboSpan" id="kobo.784.2">A silhouette value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.785.1">+1</span></strong><span class="koboSpan" id="kobo.786.1"> indicates that a data point is significantly distant from neighboring</span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.787.1"> clusters, while </span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.788.1">a value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.789.1">0</span></strong><span class="koboSpan" id="kobo.790.1"> suggests that the data point does not clearly belong to any specific cluster. </span><span class="koboSpan" id="kobo.790.2">A silhouette value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.791.1">-1</span></strong><span class="koboSpan" id="kobo.792.1"> implies that the data point is likely assigned to the wrong cluster. </span><span class="koboSpan" id="kobo.792.2">The silhouette plot returns these values as its </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">first output.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.794.1">The silhouette plot is useful for assessing the separation distance between the resulting clusters. </span><span class="koboSpan" id="kobo.794.2">It provides a visual representation of how close each data point in one cluster is to the points in neighboring clusters. </span><span class="koboSpan" id="kobo.794.3">By examining the silhouette plot, we can evaluate the clustering solution and determine whether the chosen parameters, such as the number of clusters, </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">are appropriate.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.796.1">In a successful clustering solution, we expect to see high silhouette values, indicating that the points are correctly assigned to their respective clusters and have minimal connections to neighboring clusters. </span><span class="koboSpan" id="kobo.796.2">If a majority of the data points have high silhouette values, it indicates a good clustering solution. </span><span class="koboSpan" id="kobo.796.3">However, if many points have low or negative silhouette values, it suggests that the clustering solution needs to </span><span class="No-Break"><span class="koboSpan" id="kobo.797.1">be reviewed.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.798.1">Based on the analysis of </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.799.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.800.1">.5</span></em><span class="koboSpan" id="kobo.801.1">, we can determine whether our assumption of choosing </span><em class="italic"><span class="koboSpan" id="kobo.802.1">k = 4</span></em><span class="koboSpan" id="kobo.803.1"> for the k-means algorithm has produced favorable results. </span><span class="koboSpan" id="kobo.803.2">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.804.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.805.1">.5</span></em><span class="koboSpan" id="kobo.806.1">, there are no clusters with below-average silhouette scores, indicating a good separation between the clusters. </span><span class="koboSpan" id="kobo.806.2">Additionally, the silhouette plots do not exhibit significant fluctuations in size. </span><span class="koboSpan" id="kobo.806.3">The uniform thickness of the silhouette plot indicates that the clusters are of similar sizes. </span><span class="koboSpan" id="kobo.806.4">These observations validate our choice of </span><em class="italic"><span class="koboSpan" id="kobo.807.1">k = 4</span></em><span class="koboSpan" id="kobo.808.1"> for the number of clusters and confirm that it is an </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">appropriate selection.</span></span></p>
<p><span class="koboSpan" id="kobo.810.1">After analyzing in detail how </span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.811.1">to</span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.812.1"> easily perform a k-means clustering analysis in MATLAB, we can now see a new clustering methodology: the </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">k-medoids algorithm.</span></span></p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.814.1">Grouping data using the similarity measures</span></h1>
<p><span class="koboSpan" id="kobo.815.1">The k-medoids </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.816.1">algorithm is a variation of the k-means</span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.817.1"> algorithm that uses medoids (actual data points) as representatives of each cluster instead of centroids. </span><span class="koboSpan" id="kobo.817.2">Unlike the k-means algorithm, which calculates the mean of the data points within each cluster, the k-medoids algorithm selects the most centrally located data point within each cluster as the medoid. </span><span class="koboSpan" id="kobo.817.3">This makes k-medoids more robust to outliers and suitable for data with </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">non-Euclidean distances.</span></span></p>
<p><span class="koboSpan" id="kobo.819.1">Here are some key differences between k-medoids </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">and k-means:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.821.1">Representative points</span></strong><span class="koboSpan" id="kobo.822.1">: In k-medoids, the</span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.823.1"> representatives of each cluster are actual data points from the dataset (medoids), while in k-means, the representatives are the centroids, which are calculated as the mean of the </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">data points.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.825.1">Distance measure</span></strong><span class="koboSpan" id="kobo.826.1">: The distance measure used in k-means is typically the Euclidean distance. </span><span class="koboSpan" id="kobo.826.2">On the other hand, k-medoids can handle various distance measures, including non-Euclidean distances. </span><span class="koboSpan" id="kobo.826.3">This flexibility allows k-medoids to work with different types of data, such as categorical or </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">ordinal variables.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.828.1">Robustness to outliers</span></strong><span class="koboSpan" id="kobo.829.1">: The k-medoids algorithm is generally more robust to outliers than k-means. </span><span class="koboSpan" id="kobo.829.2">Outliers can significantly affect the centroid calculation in k-means, pulling the centroid towards them. </span><span class="koboSpan" id="kobo.829.3">In k-medoids, the medoids are actual data points, so the impact of outliers is limited to the specific cluster that </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">contains them.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.831.1">Computational complexity</span></strong><span class="koboSpan" id="kobo.832.1">: K-means has a lower computational complexity compared to k-medoids. </span><span class="koboSpan" id="kobo.832.2">The </span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.833.1">selection of </span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.834.1">medoids in k-medoids requires the evaluation of pairwise distances between all data points, making it computationally more expensive, especially for </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">large datasets.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.836.1">Cluster shape</span></strong><span class="koboSpan" id="kobo.837.1">: The k-means algorithm tends to produce spherical-shaped clusters due to the use of Euclidean distance and the calculation of mean centroids. </span><span class="koboSpan" id="kobo.837.2">K-medoids, on the other hand, can produce clusters of arbitrary shapes, as the medoids can be any data point within </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">the cluster.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.839.1">When deciding between k-means and k-medoids, consider the nature of your data, the desired robustness to outliers, the availability of appropriate distance measures, and the computational resources. </span><span class="koboSpan" id="kobo.839.2">If</span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.840.1"> your </span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.841.1">data contains outliers or non-Euclidean distances are more appropriate, k-medoids may be a better choice. </span><span class="koboSpan" id="kobo.841.2">Otherwise, if the data is well behaved and the Euclidean distance is suitable, k-means can provide efficient </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">clustering results.</span></span></p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.843.1">Applying k-medoids in MATLAB</span></h2>
<p><span class="koboSpan" id="kobo.844.1">In MATLAB, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.845.1">kmedoids()</span></strong><span class="koboSpan" id="kobo.846.1"> function</span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.847.1"> is used to perform k-medoids</span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.848.1"> clustering. </span><span class="koboSpan" id="kobo.848.2">It partitions the observations in a matrix into </span><em class="italic"><span class="koboSpan" id="kobo.849.1">k</span></em><span class="koboSpan" id="kobo.850.1"> clusters and returns a vector containing the cluster indices for each observation. </span><span class="koboSpan" id="kobo.850.2">The input matrix should have rows corresponding to points and columns corresponding to variables. </span><span class="koboSpan" id="kobo.850.3">Similar to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.851.1">kmeans()</span></strong><span class="koboSpan" id="kobo.852.1"> function, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.853.1">kmedoids()</span></strong><span class="koboSpan" id="kobo.854.1"> function uses the squared Euclidean distance measure and the k-means++ algorithm for selecting initial cluster medoid positions </span><span class="No-Break"><span class="koboSpan" id="kobo.855.1">by default.</span></span></p>
<p><span class="koboSpan" id="kobo.856.1">Now, let’s see how the </span><strong class="source-inline"><span class="koboSpan" id="kobo.857.1">kmedoids()</span></strong><span class="koboSpan" id="kobo.858.1"> method is applied in a </span><span class="No-Break"><span class="koboSpan" id="kobo.859.1">practical example.</span></span></p>
<p><span class="koboSpan" id="kobo.860.1">Suppose a large distribution company wants to optimize the positioning of its offices to improve the efficiency of transferring goods from sorting hubs to peripheral locations. </span><span class="koboSpan" id="kobo.860.2">The company already has the geographic coordinates of its peripheral locations and wants to determine the optimal positions for the sorting hubs. </span><span class="koboSpan" id="kobo.860.3">The geographic coordinates have been transformed into relative coordinates for compatibility reasons with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.861.1">kmedoids()</span></strong><span class="koboSpan" id="kobo.862.1"> function. </span><span class="koboSpan" id="kobo.862.2">This information is stored in a file </span><span class="No-Break"><span class="koboSpan" id="kobo.863.1">named </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.864.1">PeripheralLocations.xls</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.865.1">:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.866.1">Let’s start by importing this data into the </span><span class="No-Break"><span class="koboSpan" id="kobo.867.1">MATLAB workspace:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.868.1">
DataKMedoids = readmatrix('PeripheralLocations.xls');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.869.1">We now have the recently imported MATLAB matrix called </span><strong class="source-inline"><span class="koboSpan" id="kobo.870.1">DataKMedoids</span></strong><span class="koboSpan" id="kobo.871.1"> with dimensions </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">of 2,650x2.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.873.1">Let’s now look at the data by drawing a </span><span class="No-Break"><span class="koboSpan" id="kobo.874.1">scatter plot:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.875.1">gscatter(DataKMedoids (:,1), DataKMedoids (:,2));</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.876.1">The following plot displays the distribution of the dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">being imported.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer038">
<span class="koboSpan" id="kobo.878.1"><img alt="Figure 4.6 – Locations of the peripheral offices (geographic coordinates of the office: x is the  latitude and y is the longitude)" src="image/B21156_04_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.879.1">Figure 4.6 – Locations of the peripheral offices (geographic coordinates of the office: x is the  latitude and y is the longitude)</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.880.1">Based on an initial visual analysis, three main regions can be identified where the company’s</span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.881.1"> peripheral </span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.882.1">offices are located. </span><span class="koboSpan" id="kobo.882.2">This implies the need to designate three current offices as hubs for distributing goods to these peripheral locations. </span><span class="koboSpan" id="kobo.882.3">To determine the position of these future hubs and the associated sites for each, a clustering analysis will </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">be conducted.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.884.1">It is evident that each hub’s location should serve as the center of its respective cluster. </span><span class="koboSpan" id="kobo.884.2">As these positions are concrete values and must correspond to existing offices, utilizing the k-medoids method appears to be a logical decision. </span><span class="koboSpan" id="kobo.884.3">K-means and k-medoids differ in their centroid determination and sensitivity to outliers. </span><span class="koboSpan" id="kobo.884.4">K-means minimizes the sum of squared distances between data points and cluster centroids, using means as centroids and proving efficient for spherical, equally sized clusters. </span><span class="koboSpan" id="kobo.884.5">However, it is sensitive to outliers and influenced by initialization. </span><span class="koboSpan" id="kobo.884.6">In contrast, k-medoids minimizes the sum of dissimilarities, using the most centrally located data point (medoid) as the cluster center. </span><span class="koboSpan" id="kobo.884.7">This makes k-medoids more robust to outliers and noise, offering better performance for non-spherical clusters or those with uneven sizes. </span><span class="koboSpan" id="kobo.884.8">K-medoids is less sensitive to initialization, but its computational cost, involving dissimilarity calculations and medoid selection, can be higher than k-means, especially for </span><span class="No-Break"><span class="koboSpan" id="kobo.885.1">large datasets.</span></span><pre class="source-code"><span class="koboSpan" id="kobo.886.1">
[IdCluster,Kmedoid,SumDist,Dist,IdClKm,info] = kmedoids(DataKMedoids,3);</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.887.1">In the previous code, the variables have the </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">following meaning:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.889.1">IdCluster</span></strong><span class="koboSpan" id="kobo.890.1"> contains cluster indices of </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">each observation</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.892.1">Kmedoid</span></strong><span class="koboSpan" id="kobo.893.1"> contains the </span><em class="italic"><span class="koboSpan" id="kobo.894.1">k</span></em><span class="koboSpan" id="kobo.895.1"> cluster </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">medoid locations</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.897.1">SumDist</span></strong><span class="koboSpan" id="kobo.898.1"> contains the within-cluster sums of </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">point-to-medoid distances</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.900.1">Dist</span></strong><span class="koboSpan" id="kobo.901.1"> contains distances from each point to </span><span class="No-Break"><span class="koboSpan" id="kobo.902.1">every medoid</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.903.1">IdClKm</span></strong><span class="koboSpan" id="kobo.904.1"> contains cluster indices of </span><span class="No-Break"><span class="koboSpan" id="kobo.905.1">each medoid</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.906.1">info</span></strong><span class="koboSpan" id="kobo.907.1"> contains </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.908.1">information </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.909.1">about the options used by the algorithm </span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">when executed</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.911.1">Let’s take a look at the information returned by the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.912.1">kmedoids()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.913.1"> function:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.914.1">&gt;&gt; info
info =
  struct with fields:
        algorithm: 'pam'
            start: 'plus'
         distance: 'sqeuclidean'
       iterations: 3
    bestReplicate: 1</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.915.1">The command provides information on the algorithm type, metrics used, and the number of iterations to achieve the best performance. </span><span class="koboSpan" id="kobo.915.2">The algorithm used is called </span><strong class="bold"><span class="koboSpan" id="kobo.916.1">PAM</span></strong><span class="koboSpan" id="kobo.917.1">, which stands for </span><strong class="bold"><span class="koboSpan" id="kobo.918.1">partitioning around medoids</span></strong><span class="koboSpan" id="kobo.919.1">, and it is a classic approach for</span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.920.1"> solving the k-medoids problem. </span><span class="koboSpan" id="kobo.920.2">This algorithm closely resembles k-means; however, it employs medoids, which are the data points positioned at the center of clusters, as cluster representatives rather than centroids. </span><span class="koboSpan" id="kobo.920.3">The primary objective of PAM is to minimize the total dissimilarity between data points and their </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">corresponding medoids.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.922.1">The second parameter denotes the method used to determine the initial positions of the cluster medoids. </span><span class="koboSpan" id="kobo.922.2">The chosen metric is the squared Euclidean distance. </span><span class="koboSpan" id="kobo.922.3">The command </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.923.1">also </span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.924.1">displays the number of iterations and identifies the </span><span class="No-Break"><span class="koboSpan" id="kobo.925.1">best iteration.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.926.1">To visualize the result, we can generate plots of the clusters and their </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">respective medoids:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.928.1">gscatter(DataKMedoids (:,1), DataKMedoids (:,2), IdCluster,'bgr','xo^')
hold on
plot(Kmedoid(:,1),Kmedoid(:,2),'x','LineWidth',4,'MarkerEdgeColor','k','MarkerSize',25)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.929.1">The scatter plot displays the clustering results along with the indicated position of each medoid. </span><span class="koboSpan" id="kobo.929.2">We have specifically adjusted the markers to highlight the </span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">medoids’ locations.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.931.1"><img alt="Figure 4.7 – Scatter plot of clustering and the position of each medoid (geographic coordinates of the office: x is the latitude and y is the longitude)" src="image/B21156_04_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.932.1">Figure 4.7 – Scatter plot of clustering and the position of each medoid (geographic coordinates of the office: x is the latitude and y is the longitude)</span></p>
<p><span class="koboSpan" id="kobo.933.1">By analyzing </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.934.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.935.1">.7</span></em><span class="koboSpan" id="kobo.936.1">, it becomes evident that the peripheral sites are organized into three clusters as determined by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.937.1">kmedoids()</span></strong><span class="koboSpan" id="kobo.938.1"> function. </span><span class="koboSpan" id="kobo.938.2">Similarly, the strategic positioning of the identified hubs is self-explanatory. </span><span class="koboSpan" id="kobo.938.3">The distinct colors and markers assigned to each cluster facilitate the quick identification of each site’s association with its </span><span class="No-Break"><span class="koboSpan" id="kobo.939.1">respective cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.940.1">After having </span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.941.1">adequately </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.942.1">explored the interesting world of clustering, it is time to move on to analyze how to reduce the size of data in cases with </span><span class="No-Break"><span class="koboSpan" id="kobo.943.1">many features.</span></span></p>
<h1 id="_idParaDest-95"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.944.1">Discovering dimensionality reduction techniques</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.945.1">Dimensionality reduction</span></strong><span class="koboSpan" id="kobo.946.1"> is a technique used in machine learning and data analysis to reduce the </span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.947.1">number of variables or features in a dataset. </span><span class="koboSpan" id="kobo.947.2">The goal of dimensionality reduction is to simplify the data while retaining important information, thereby improving the efficiency and effectiveness of subsequent </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">analysis tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.949.1">High-dimensional datasets can be challenging to work with due to </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">several reasons:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.951.1">Curse of dimensionality</span></strong><span class="koboSpan" id="kobo.952.1">: As</span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.953.1"> the number of features increases, the data becomes more sparse, making it difficult to find meaningful patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">or relationships</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.955.1">Computational complexity</span></strong><span class="koboSpan" id="kobo.956.1">: Many algorithms and models become computationally expensive as the dimensionality of the data increases, requiring more time and resources </span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">for analysis</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.958.1">Overfitting</span></strong><span class="koboSpan" id="kobo.959.1">: High-dimensional data is more susceptible to overfitting, where a model becomes too specialized to the training data and fails to generalize well to </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">new data</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.961.1">Dimensionality reduction methods aim to address these challenges by reducing the number of features while preserving important information. </span><span class="koboSpan" id="kobo.961.2">There are two main approaches to </span><span class="No-Break"><span class="koboSpan" id="kobo.962.1">dimensionality reduction:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.963.1">Feature selection</span></strong><span class="koboSpan" id="kobo.964.1">: This approach </span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.965.1">selects a subset of the original features based on certain criteria. </span><span class="koboSpan" id="kobo.965.2">It aims to identify the most informative features that contribute significantly to the prediction or analysis task. </span><span class="koboSpan" id="kobo.965.3">Common techniques for feature selection include correlation analysis, backward/forward selection, and regularization methods such as L1 </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">regularization (Lasso).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.967.1">Feature extraction</span></strong><span class="koboSpan" id="kobo.968.1">: This approach creates new features by combining or transforming the original features. </span><span class="koboSpan" id="kobo.968.2">It aims to capture the underlying structure or patterns in the </span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.969.1">data. </span><strong class="bold"><span class="koboSpan" id="kobo.970.1">Principal component analysis</span></strong><span class="koboSpan" id="kobo.971.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.972.1">PCA</span></strong><span class="koboSpan" id="kobo.973.1">) is a popular feature extraction technique that identifies orthogonal axes in the data that explain the maximum variance. </span><span class="koboSpan" id="kobo.973.2">Other methods, such</span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.974.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.975.1">singular value decomposition</span></strong><span class="koboSpan" id="kobo.976.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.977.1">SVD</span></strong><span class="koboSpan" id="kobo.978.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.979.1">non-negative matrix factorization</span></strong><span class="koboSpan" id="kobo.980.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.981.1">NMF</span></strong><span class="koboSpan" id="kobo.982.1">), can also be </span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.983.1">used for </span><span class="No-Break"><span class="koboSpan" id="kobo.984.1">feature extraction.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.985.1">Both feature selection and feature extraction techniques have their advantages and disadvantages. </span><span class="koboSpan" id="kobo.985.2">The choice of method depends on the specific problem, dataset characteristics, and the goals of </span><span class="No-Break"><span class="koboSpan" id="kobo.986.1">the analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.987.1">By reducing the dimensionality of the data, dimensionality reduction methods can lead to benefits such as faster computation, improved model performance, better visualization, and enhanced interpretability of the data. </span><span class="koboSpan" id="kobo.987.2">However, it is important to note that dimensionality </span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.988.1">reduction is not always necessary or beneficial, and it should be applied judiciously after considering the specific requirements and characteristics of the problem </span><span class="No-Break"><span class="koboSpan" id="kobo.989.1">at hand.</span></span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.990.1">Introducing feature selection methods</span></h2>
<p><span class="koboSpan" id="kobo.991.1">When dealing with </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.992.1">high-dimensional </span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.993.1">datasets, it is often beneficial to reduce the number of features to focus only on the most relevant ones, discarding the rest. </span><span class="koboSpan" id="kobo.993.2">This can result in simpler models that generalize more effectively. </span><span class="koboSpan" id="kobo.993.3">Feature selection involves the process of identifying the most important features while disregarding others during processing and analysis. </span><span class="koboSpan" id="kobo.993.4">It is crucial for creating a functional model that maintains a manageable number of features. </span><span class="koboSpan" id="kobo.993.5">In many cases, datasets contain redundant or excessive information, while in other cases, they may include incorrect information. </span><span class="koboSpan" id="kobo.993.6">Therefore, feature selection helps to eliminate such issues and improve the overall quality of </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.995.1">Feature selection enhances the efficiency of model creation by reducing the computational load on the CPU and the memory requirements for training algorithms. </span><span class="koboSpan" id="kobo.995.2">The selection of features serves </span><span class="No-Break"><span class="koboSpan" id="kobo.996.1">several purposes:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.997.1">Preparing clean and interpretable data</span></strong><span class="koboSpan" id="kobo.998.1">: Feature selection helps in selecting </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.999.1">the most relevant features, resulting in a cleaner and more understandable dataset. </span><span class="koboSpan" id="kobo.999.2">In MATLAB, cleaning a large dataset involves various data preprocessing steps to address missing values, outliers, </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">and inconsistencies.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1001.1">Simplifying models and improving interpretability</span></strong><span class="koboSpan" id="kobo.1002.1">: By focusing on a subset of important features, models become simpler and easier to interpret, allowing for better insights into the relationships </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">between variables.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1004.1">Reducing training times</span></strong><span class="koboSpan" id="kobo.1005.1">: With fewer features, the training process becomes faster, as there is less data to process </span><span class="No-Break"><span class="koboSpan" id="kobo.1006.1">and analyze.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1007.1">Mitigating overfitting</span></strong><span class="koboSpan" id="kobo.1008.1">: Overfitting occurs when a model is too complex and fits the training data too closely, leading to poor generalization. </span><span class="koboSpan" id="kobo.1008.2">Feature selection helps in </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.1009.1">reducing overfitting by decreasing the variance in </span><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">the model.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1011.1">Feature selection involves finding a subset of the original variables, typically through an iterative process. </span><span class="koboSpan" id="kobo.1011.2">By exploring various combinations of variables and comparing prediction errors, the subset that produces the minimum error is selected as the input for the machine </span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">learning algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.1013.1">To perform feature selection, appropriate criteria need to be defined beforehand. </span><span class="koboSpan" id="kobo.1013.2">These criteria typically involve minimizing a specific predictive error measure for models fitted to different subsets. </span><span class="koboSpan" id="kobo.1013.3">Feature selection algorithms aim to find a subset of predictors that optimally model the measured responses while considering constraints, such as required or excluded features and the desired </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">subset size.</span></span></p>
<p><span class="koboSpan" id="kobo.1015.1">Feature selection is </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.1016.1">particularly </span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.1017.1">valuable when the goal is to identify an influential subset, especially in cases involving categorical features where numerical transformations may not </span><span class="No-Break"><span class="koboSpan" id="kobo.1018.1">be adequate.</span></span></p>
<p><span class="koboSpan" id="kobo.1019.1">Feature selection methodologies can be grouped into three </span><span class="No-Break"><span class="koboSpan" id="kobo.1020.1">broad categories:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1021.1">Filter methods</span></strong><span class="koboSpan" id="kobo.1022.1">: These </span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.1023.1">methods evaluate characteristics independently of the learning model used subsequently. </span><span class="koboSpan" id="kobo.1023.2">Statistical measures are calculated for each feature, such as the correlation with the output or the relative importance of the features themselves. </span><span class="koboSpan" id="kobo.1023.3">Features are selected based on these measures, without considering the specific learning model. </span><span class="koboSpan" id="kobo.1023.4">Examples of filtering methods include correlation analysis, chi-square test, information gain, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">mutual information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1025.1">Wrapper methods</span></strong><span class="koboSpan" id="kobo.1026.1">: These methods use a specific learning model to evaluate the quality of different combinations of features. </span><span class="koboSpan" id="kobo.1026.2">Several subsets of features are created, and a learning model is trained and evaluated for each subset. </span><span class="koboSpan" id="kobo.1026.3">The goal is to select the subset of features that produce the best performance according to the specified metrics, such as accuracy or mean squared error. </span><span class="koboSpan" id="kobo.1026.4">However, this approach can be computationally expensive, requiring repeated training and evaluation of many models. </span><span class="koboSpan" id="kobo.1026.5">Examples of wrapper methods include forward selection, backward selection, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">bi-directional elimination.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1028.1">Embedded methods</span></strong><span class="koboSpan" id="kobo.1029.1">: These methods perform feature selection within the model training process itself. </span><span class="koboSpan" id="kobo.1029.2">The learning algorithms used incorporate feature selection mechanisms as part of their optimization process. </span><span class="koboSpan" id="kobo.1029.3">For example, linear regression algorithms with smoothing (such as ridge regression and Lasso regression) tend to reduce the importance of less relevant features, helping to automatically select the more significant features during training. </span><span class="koboSpan" id="kobo.1029.4">Examples of embedded methods</span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.1030.1"> include Lasso, elastic net, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">ridge regression.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1032.1">Each category of feature selection methods has specific advantages and limitations. </span><span class="koboSpan" id="kobo.1032.2">The choice of methodology depends on the nature of the problem, the number of features, the availability of data, and </span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.1033.1">the required performance. </span><span class="koboSpan" id="kobo.1033.2">It is important to experiment and compare different methodologies to find the most suitable for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1034.1">specific case.</span></span></p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.1035.1">Exploring feature extraction algorithms</span></h2>
<p><span class="koboSpan" id="kobo.1036.1">When dealing with </span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.1037.1">large</span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.1038.1"> datasets, it is often necessary to transform them into a reduced representation of features. </span><span class="koboSpan" id="kobo.1038.2">This transformation process is known as feature extraction. </span><span class="koboSpan" id="kobo.1038.3">As covered briefly earlier, feature extraction involves taking an initial set of measured data and creating derivative values that capture the essential information while eliminating redundant or unnecessary data. </span><span class="koboSpan" id="kobo.1038.4">The goal is to retain the relevant information contained in the original dataset while simplifying it and reducing its dimensionality. </span><span class="koboSpan" id="kobo.1038.5">By extracting meaningful features, the resulting representation can be more manageable and efficient for subsequent analysis or </span><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">modeling tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.1040.1">By performing feature extraction, subsequent learning and generalization stages are simplified, and in certain cases, it can result in improved interpretations. </span><span class="koboSpan" id="kobo.1040.2">It involves deriving new features from the original ones, aiming to reduce the measurement cost, enhance classifier efficiency, and achieve higher classification accuracy. </span><span class="koboSpan" id="kobo.1040.3">When the extracted features are carefully selected, it is anticipated that the reduced representation will effectively fulfill the desired task instead of using the full-sized input. </span><span class="koboSpan" id="kobo.1040.4">This enables more efficient and accurate processing as the focus shifts to a subset of features that capture the essential information required for the task </span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">at hand.</span></span></p>
<p><span class="koboSpan" id="kobo.1042.1">Feature extraction algorithms are used to derive new features from the original set of features. </span><span class="koboSpan" id="kobo.1042.2">These algorithms aim to capture the most relevant and informative aspects of the data while reducing </span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.1043.1">dimensionality. </span><span class="koboSpan" id="kobo.1043.2">Here are a few commonly used feature </span><span class="No-Break"><span class="koboSpan" id="kobo.1044.1">extraction algorithms:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1045.1">PCA</span></strong><span class="koboSpan" id="kobo.1046.1">: PCA is a</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.1047.1"> widely used technique for feature extraction. </span><span class="koboSpan" id="kobo.1047.2">It identifies the orthogonal axes (principal components) in the data that explain the maximum variance. </span><span class="koboSpan" id="kobo.1047.3">By projecting the data onto these components, it reduces the dimensionality while preserving the most </span><span class="No-Break"><span class="koboSpan" id="kobo.1048.1">important information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1049.1">Linear discriminant analysis (LDA)</span></strong><span class="koboSpan" id="kobo.1050.1">: LDA is primarily used for feature extraction in the</span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.1051.1"> context of classification tasks. </span><span class="koboSpan" id="kobo.1051.2">It aims to find a projection of the data that maximizes the separation between different classes while minimizing the variance within </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">each class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1053.1">Independent component analysis (ICA)</span></strong><span class="koboSpan" id="kobo.1054.1">: ICA seeks to identify statistically independent </span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.1055.1">components from a set of observed signals. </span><span class="koboSpan" id="kobo.1055.2">It assumes that the observed signals</span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.1056.1"> are linear combinations of hidden independent components. </span><span class="koboSpan" id="kobo.1056.2">ICA can be useful for extracting meaningful features in signal processing and blind source </span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">separation problems.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1058.1">NMF</span></strong><span class="koboSpan" id="kobo.1059.1">: NMF decomposes a non-negative matrix into two lower-rank matrices, where the elements</span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.1060.1"> are constrained to be non-negative. </span><span class="koboSpan" id="kobo.1060.2">It can uncover parts-based representations of the data and is often used for feature extraction in image and text </span><span class="No-Break"><span class="koboSpan" id="kobo.1061.1">analysis tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1062.1">Autoencoders</span></strong><span class="koboSpan" id="kobo.1063.1">: Autoencoders are neural network models that aim to reconstruct the input </span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.1064.1">data from a compressed representation (encoding) layer. </span><span class="koboSpan" id="kobo.1064.2">The encoding layer represents the extracted features. </span><span class="koboSpan" id="kobo.1064.3">By training the autoencoder to minimize the reconstruction error, meaningful features can </span><span class="No-Break"><span class="koboSpan" id="kobo.1065.1">be learned.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1066.1">Wavelet transform</span></strong><span class="koboSpan" id="kobo.1067.1">: Wavelet transform decomposes the data into different frequency bands, allowing</span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.1068.1"> the extraction of features at various scales. </span><span class="koboSpan" id="kobo.1068.2">It is commonly used in signal and image </span><span class="No-Break"><span class="koboSpan" id="kobo.1069.1">processing tasks.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1070.1">These are just a few examples of feature extraction algorithms. </span><span class="koboSpan" id="kobo.1070.2">The choice of algorithm depends on the nature of the data, the specific problem, and the desired outcome. </span><span class="koboSpan" id="kobo.1070.3">It is often beneficial to experiment with multiple algorithms and compare their performance to select the most suitable one for a </span><span class="No-Break"><span class="koboSpan" id="kobo.1071.1">given task.</span></span></p>
<p><span class="koboSpan" id="kobo.1072.1">Feature extraction </span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.1073.1">works </span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.1074.1">by transforming the original set of features into a new representation that captures the essential information while reducing dimensionality. </span><span class="koboSpan" id="kobo.1074.2">The process typically involves the </span><span class="No-Break"><span class="koboSpan" id="kobo.1075.1">following steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.1076.1">Data preprocessing</span></strong><span class="koboSpan" id="kobo.1077.1">: The input data is preprocessed to handle missing values and outliers, and normalize the features if necessary. </span><span class="koboSpan" id="kobo.1077.2">This ensures that the data is in a suitable form for </span><span class="No-Break"><span class="koboSpan" id="kobo.1078.1">feature extraction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1079.1">Dimensionality reduction</span></strong><span class="koboSpan" id="kobo.1080.1">: This step aims to reduce the number of features while preserving the most important information. </span><span class="koboSpan" id="kobo.1080.2">Techniques such as PCA, LDA, or NMF are commonly used for dimensionality reduction. </span><span class="koboSpan" id="kobo.1080.3">These methods identify a lower-dimensional subspace or combination of features that retain the most significant variations in </span><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">the data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1082.1">Feature construction</span></strong><span class="koboSpan" id="kobo.1083.1">: In some cases, new features are constructed from the original features. </span><span class="koboSpan" id="kobo.1083.2">This can involve mathematical operations, transformations, or aggregations. </span><span class="koboSpan" id="kobo.1083.3">The goal is to create features that capture specific patterns or relationships in the data that may be more informative for the task </span><span class="No-Break"><span class="koboSpan" id="kobo.1084.1">at hand.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1085.1">Feature selection</span></strong><span class="koboSpan" id="kobo.1086.1">: In addition to dimensionality reduction, feature selection methods may be applied to further filter out irrelevant or redundant features. </span><span class="koboSpan" id="kobo.1086.2">This helps to focus on the most informative features and reduce the noise in the data. </span><span class="koboSpan" id="kobo.1086.3">Feature selection can be performed using techniques such as correlation analysis, statistical tests, or wrapper methods that evaluate the impact of different feature subsets on a specific </span><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">learning algorithm.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1088.1">Feature</span></strong><strong class="bold"><span class="koboSpan" id="kobo.1089.1"> representation</span></strong><span class="koboSpan" id="kobo.1090.1">: Once the desired set of features is extracted and selected, the data is represented using these features. </span><span class="koboSpan" id="kobo.1090.2">This transformed representation is often lower-dimensional than the original data and contains the most relevant information for subsequent analysis or modeling tasks. </span><span class="koboSpan" id="kobo.1090.3">The choice of feature extraction techniques depends on the specific problem, the characteristics of the data, and the goals of the analysis. </span><span class="koboSpan" id="kobo.1090.4">In the following list, you’ll find instances across computer vision applications where a range of feature extraction techniques are put </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">to use:</span></span><ul><li><span class="koboSpan" id="kobo.1092.1">In face recognition, we employ</span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.1093.1"> PCA and </span><strong class="bold"><span class="koboSpan" id="kobo.1094.1">local binary patterns</span></strong><span class="koboSpan" id="kobo.1095.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1096.1">LBP</span></strong><span class="koboSpan" id="kobo.1097.1">) to extract </span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">facial features</span></span></li><li><span class="koboSpan" id="kobo.1099.1">Object detection benefits from the </span><strong class="bold"><span class="koboSpan" id="kobo.1100.1">histogram of oriented gradients (HOG)</span></strong><span class="koboSpan" id="kobo.1101.1">, which</span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.1102.1"> captures object shape </span><span class="No-Break"><span class="koboSpan" id="kobo.1103.1">and texture</span></span></li><li><span class="koboSpan" id="kobo.1104.1">Image classification </span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.1105.1">employs </span><strong class="bold"><span class="koboSpan" id="kobo.1106.1">convolutional neural networks</span></strong><span class="koboSpan" id="kobo.1107.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1108.1">CNNs</span></strong><span class="koboSpan" id="kobo.1109.1">) to engage in hierarchical </span><span class="No-Break"><span class="koboSpan" id="kobo.1110.1">feature learning</span></span></li></ul></li>
</ol>
<p><span class="koboSpan" id="kobo.1111.1">It is important to evaluate the extracted features and assess their impact on the performance of the downstream tasks, such as classification or regression. </span><span class="koboSpan" id="kobo.1111.2">Feature extraction is an iterative process, and it may require experimentation and fine-tuning to find the most effective combination of techniques for a </span><span class="No-Break"><span class="koboSpan" id="kobo.1112.1">given task.</span></span></p>
<p><span class="koboSpan" id="kobo.1113.1">After having</span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.1114.1"> analyzed</span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.1115.1"> in detail all the dimensionality reduction techniques, the time has come to move on to practice by addressing examples of these technologies in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1116.1">MATLAB environment.</span></span></p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.1117.1">Feature selection and feature extraction using MATLAB</span></h1>
<p><span class="koboSpan" id="kobo.1118.1">In MATLAB, there </span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.1119.1">are several built-in</span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.1120.1"> functions and toolboxes that can be used for dimensionality reduction. </span><span class="koboSpan" id="kobo.1120.2">In the next section, we will explore some practical examples of the dimensionality reduction algorithm in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1121.1">MATLAB environment.</span></span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.1122.1">Stepwise regression for feature selection</span></h2>
<p><span class="koboSpan" id="kobo.1123.1">Regression analysis is a</span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.1124.1"> valuable approach for</span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.1125.1"> understanding the impact of independent variables on a dependent variable. </span><span class="koboSpan" id="kobo.1125.2">It allows us to identify predictors that hold greater influence over the model’s response. </span><span class="koboSpan" id="kobo.1125.3">Stepwise regression is a variable selection method used to choose a subset of predictors that exhibit the strongest relationship with the dependent variable. </span><span class="koboSpan" id="kobo.1125.4">There are three common variable </span><span class="No-Break"><span class="koboSpan" id="kobo.1126.1">selection algorithms:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1127.1">Forward method</span></strong><span class="koboSpan" id="kobo.1128.1">: The forward method starts with an empty model, where no predictors are initially selected. </span><span class="koboSpan" id="kobo.1128.2">In the first step, the variable showing the most significant association at a statistical level is added. </span><span class="koboSpan" id="kobo.1128.3">In subsequent steps, the remaining variable with the highest statistically significant association is added to the model. </span><span class="koboSpan" id="kobo.1128.4">This process continues until no more variables demonstrate statistically significant associations with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1129.1">dependent variable.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1130.1">Backward method</span></strong><span class="koboSpan" id="kobo.1131.1">: The backward method begins with a model that includes all variables. </span><span class="koboSpan" id="kobo.1131.2">It then proceeds step by step to eliminate variables, starting with the one with the least significant association with the dependent variable on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1132.1">statistical plane.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1133.1">Stepwise method</span></strong><span class="koboSpan" id="kobo.1134.1">: The stepwise method alternates between the forward and backward processes. </span><span class="koboSpan" id="kobo.1134.2">It involves adding and removing variables that gain or lose significance during various model adjustments, including the addition or re-insertion </span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">of variables.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1136.1">These variable selection methods enable the identification of a subset of predictors that best explain the relationship with the dependent variable. </span><span class="koboSpan" id="kobo.1136.2">The choice of algorithm depends on the specific context and the researcher’s goals. </span><span class="koboSpan" id="kobo.1136.3">Each method has its own strengths and limitations, and it is essential to interpret the selected variables within the context of the overall </span><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">regression analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.1138.1">In MATLAB, you can create a stepwise regression model using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1139.1">stepwiselm()</span></strong><span class="koboSpan" id="kobo.1140.1"> function. </span><span class="koboSpan" id="kobo.1140.2">This function returns a linear model by performing stepwise regression, where predictors are added</span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.1141.1"> or</span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.1142.1"> removed based on their significance. </span><span class="koboSpan" id="kobo.1142.2">Here’s how </span><span class="No-Break"><span class="koboSpan" id="kobo.1143.1">it works:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1144.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1145.1">stepwiselm()</span></strong><span class="koboSpan" id="kobo.1146.1"> function takes a table or dataset array as input and performs stepwise regression on the variables </span><span class="No-Break"><span class="koboSpan" id="kobo.1147.1">within it.</span></span></li>
<li><span class="koboSpan" id="kobo.1148.1">The function uses both forward and backward stepwise regression to determine the final model. </span><span class="koboSpan" id="kobo.1148.2">It starts with an initial model specified using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1149.1">modelspec</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1"> attribute.</span></span></li>
<li><span class="koboSpan" id="kobo.1151.1">At each step, the function compares the explanatory power of incrementally larger and smaller models. </span><span class="koboSpan" id="kobo.1151.2">It searches for terms to add or remove based on the value of the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1152.1">Criterion</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1153.1"> argument.</span></span></li>
<li><span class="koboSpan" id="kobo.1154.1">The function computes the p-value of an F-statistic to test models with and without a potential term. </span><span class="koboSpan" id="kobo.1154.2">If a term is not currently in the model, the null hypothesis is that the term would have a zero coefficient if added. </span><span class="koboSpan" id="kobo.1154.3">If the evidence is strong enough to reject the null hypothesis, the term is added to </span><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">the model.</span></span></li>
<li><span class="koboSpan" id="kobo.1156.1">Conversely, if a term is already in the model, the null hypothesis is that the term has a zero coefficient. </span><span class="koboSpan" id="kobo.1156.2">If there is insufficient evidence to reject the null hypothesis, the term is removed from </span><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">the model.</span></span></li>
<li><span class="koboSpan" id="kobo.1158.1">The process continues until no single step improves the model. </span><span class="koboSpan" id="kobo.1158.2">The function terminates when no terms meet the entrance or </span><span class="No-Break"><span class="koboSpan" id="kobo.1159.1">exit criteria.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1160.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1161.1">stepwiselm()</span></strong><span class="koboSpan" id="kobo.1162.1"> function allows you to build different models from the same set of potential terms, depending on the initial model and the order in which terms are added or removed. </span><span class="koboSpan" id="kobo.1162.2">By using this function in MATLAB, you can perform stepwise regression to select the most significant predictors and determine the final model that best explains the relationship between </span><span class="No-Break"><span class="koboSpan" id="kobo.1163.1">the variables.</span></span></p>
<p><span class="koboSpan" id="kobo.1164.1">To get the data, we utilize the extensive collection of datasets provided by the UCI </span><em class="italic"><span class="koboSpan" id="kobo.1165.1">Machine Learning Repository</span></em><span class="koboSpan" id="kobo.1166.1"> (</span><a href="https://archive.ics.uci.edu/"><span class="koboSpan" id="kobo.1167.1">https://archive.ics.uci.edu/</span></a><span class="koboSpan" id="kobo.1168.1">). </span><span class="koboSpan" id="kobo.1168.2">This repository serves as a valuable resource for obtaining a wide range of datasets for various machine learning and data analysis tasks. </span><span class="koboSpan" id="kobo.1168.3">By leveraging this repository, we can access the necessary dataset required for our analysis or modeling purposes. </span><span class="koboSpan" id="kobo.1168.4">The UCI Machine Learning Repository offers a diverse collection of datasets contributed by researchers and practitioners, facilitating the exploration and experimentation with different </span><span class="No-Break"><span class="koboSpan" id="kobo.1169.1">data-driven applications.</span></span></p>
<p><span class="koboSpan" id="kobo.1170.1">The </span><em class="italic"><span class="koboSpan" id="kobo.1171.1">Yacht Hydrodynamics</span></em><span class="koboSpan" id="kobo.1172.1"> dataset is utilized for predicting the hydrodynamic performance of sailing yachts based on their dimensions and velocity. </span><span class="koboSpan" id="kobo.1172.2">This prediction plays a crucial role in evaluating the overall ship performance and estimating the necessary</span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.1173.1"> propulsive </span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.1174.1">power during the initial design phase. </span><span class="koboSpan" id="kobo.1174.2">The key inputs for this prediction include the fundamental hull dimensions and the velocity of the yacht. </span><span class="koboSpan" id="kobo.1174.3">Specifically, the inputs consist of hull geometry coefficients and the Froude number, while the output is the residuary resistance per unit weight of displacement. </span><span class="koboSpan" id="kobo.1174.4">By analyzing these inputs, valuable insights can be gained regarding the performance and efficiency of sailing yachts, aiding in the design and </span><span class="No-Break"><span class="koboSpan" id="kobo.1175.1">optimization processes.</span></span></p>
<p><span class="koboSpan" id="kobo.1176.1">Let's see how to practically carry out a stepwise regression </span><span class="No-Break"><span class="koboSpan" id="kobo.1177.1">using MATLAB.</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1178.1">To start, we will import the dataset into MATLAB using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1179.1">readtable()</span></strong><span class="koboSpan" id="kobo.1180.1"> function </span><span class="No-Break"><span class="koboSpan" id="kobo.1181.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1182.1">
YHData = readtable('YachtHydrodynamics.xlsx');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1183.1">Let’s check the size of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1184.1">table imported:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1185.1">size(YHData)
ans =
   308     7</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1186.1">The table imported contains 308 records of </span><span class="No-Break"><span class="koboSpan" id="kobo.1187.1">7 features.</span></span></p></li> <li><span class="koboSpan" id="kobo.1188.1">With the data now available in the MATLAB workspace in the form of a table, we can proceed to perform </span><span class="No-Break"><span class="koboSpan" id="kobo.1189.1">stepwise regression:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1190.1">
Model1 = stepwiselm(YHData,'constant');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1191.1">In this case we used as a starting model a model with a constant value equal to the intercept, this represents the </span><span class="No-Break"><span class="koboSpan" id="kobo.1192.1">default setting.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1193.1">The following text </span><span class="No-Break"><span class="koboSpan" id="kobo.1194.1">was printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1195.1">1. </span><span class="koboSpan" id="kobo.1195.2">Adding FroudeNumber, FStat = 584.1803, pValue = 6.233076e-73</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1196.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1197.1">stepwiselm()</span></strong><span class="koboSpan" id="kobo.1198.1"> function in MATLAB creates a linear model using stepwise regression to add or remove predictors from a table or dataset array, starting from a constant model. </span><span class="koboSpan" id="kobo.1198.2">The response variable used by </span><strong class="source-inline"><span class="koboSpan" id="kobo.1199.1">stepwiselm()</span></strong><span class="koboSpan" id="kobo.1200.1"> is the last variable in the table or dataset array. </span><span class="koboSpan" id="kobo.1200.2">The function employs both forward and backward </span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.1201.1">stepwise</span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.1202.1"> regression methods to determine the final model. </span><span class="koboSpan" id="kobo.1202.2">During each step, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1203.1">stepwiselm()</span></strong><span class="koboSpan" id="kobo.1204.1"> searches for terms to add or remove from the model based on the specified criterion value provided through the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1205.1">Criterion</span></strong><span class="koboSpan" id="kobo.1206.1"> argument. </span><span class="koboSpan" id="kobo.1206.2">Terms are evaluated for their significance and contribution to the model’s performance. </span><span class="koboSpan" id="kobo.1206.3">The function iteratively adds or removes predictors that improve the model’s fit based on the chosen criterion. </span><span class="koboSpan" id="kobo.1206.4">By leveraging the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1207.1">stepwiselm()</span></strong><span class="koboSpan" id="kobo.1208.1"> function in MATLAB, you can systematically identify and incorporate the most relevant predictors into your linear regression model, ultimately refining and optimizing the model’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1209.1">predictive power.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1210.1">Stepwise regression allows us to systematically select the most relevant predictors and build a regression model. </span><span class="koboSpan" id="kobo.1210.2">By executing the stepwise regression algorithm in MATLAB, we can automatically identify the significant predictors that have the strongest relationship with the dependent variable. </span><span class="koboSpan" id="kobo.1210.3">This enables us to create an optimized regression model that captures the essential information from </span><span class="No-Break"><span class="koboSpan" id="kobo.1211.1">the dataset.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1212.1">Let’s print some information about </span><span class="No-Break"><span class="koboSpan" id="kobo.1213.1">the model:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1214.1">Model1
Model1 =
Linear regression model:
    ResResistance ~ 1 + FroudeNumber
Estimated Coefficients:
            Estimate      SE       tStat       pValue
           ________    ______    _______    __________
(Intercept)    -24.484    1.5336   -15.965  3.6732e-42
FroudeNumber   121.67     5.0339    24.17   6.2331e-73
Number of observations: 308, Error degrees of freedom: 306
Root Mean Squared Error: 8.9
R-squared: 0.656,  Adjusted R-Squared: 0.655
F-statistic vs. </span><span class="koboSpan" id="kobo.1214.2">constant model: 584, p-value = 6.23e-73</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1215.1">We initiated the </span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.1216.1">stepwise</span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.1217.1"> regression with a constant model, and the function identified the only variable it deemed statistically significant (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1218.1">x6 = FroudeNumber</span></strong><span class="koboSpan" id="kobo.1219.1">) to include in </span><span class="No-Break"><span class="koboSpan" id="kobo.1220.1">the model.</span></span></p></li> <li><span class="koboSpan" id="kobo.1221.1">Let’s now try a different approach by starting with a linear model that includes an intercept and linear terms for each predictor. </span><span class="koboSpan" id="kobo.1221.2">Then, step by step, the function will remove terms that are found to be statistically insignificant. </span><span class="koboSpan" id="kobo.1221.3">This allows us to iteratively refine the model, retaining only the predictors that have a significant impact on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1222.1">dependent variable:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1223.1">
Model2 = stepwiselm(YHData,'linear');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1224.1">The following text </span><span class="No-Break"><span class="koboSpan" id="kobo.1225.1">was printed:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1226.1">1. </span><span class="koboSpan" id="kobo.1226.2">Removing PrismaticCoef, FStat = 0.021132, pValue = 0.88452
2. </span><span class="koboSpan" id="kobo.1226.3">Removing LongPos, FStat = 0.33493, pValue = 0.5632
3. </span><span class="koboSpan" id="kobo.1226.4">Removing LengDispRatio, FStat = 0.63427, pValue = 0.42642
4. </span><span class="koboSpan" id="kobo.1226.5">Removing LengthBeamRatio, FStat = 0.034179, pValue = 0.85345
5. </span><span class="koboSpan" id="kobo.1226.6">Removing BeamDraughtRatio, FStat = 0.13695, pValue = 0.71159</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1227.1">In this way, we can see the order in which the model has removed the features, starting with the ones that are less correlated with </span><span class="No-Break"><span class="koboSpan" id="kobo.1228.1">the response.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1229.1">Now, we can</span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.1230.1"> print the </span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.1231.1">summary of </span><span class="No-Break"><span class="koboSpan" id="kobo.1232.1">the model:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1233.1">Model2
Model2 =
Linear regression model:
    ResResistance ~ 1 + FroudeNumber
Estimated Coefficients:
            Estimate      SE       tStat       pValue
           ________    ______    _______    __________
(Intercept)  -24.484   1.5336    -15.965    3.6732e-42
FroudeNumber  121.67   5.0339      24.17    6.2331e-73
Number of observations: 308, Error degrees of freedom: 306
Root Mean Squared Error: 8.9
R-squared: 0.656,  Adjusted R-Squared: 0.655
F-statistic vs. </span><span class="koboSpan" id="kobo.1233.2">constant model: 584, p-value = 6.23e-73</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1234.1">The result is the same, but the procedure followed </span><span class="No-Break"><span class="koboSpan" id="kobo.1235.1">is different.</span></span></p></li> <li><span class="koboSpan" id="kobo.1236.1">To explore the full range of possibilities, we will now create a full quadratic model as the upper bound. </span><span class="koboSpan" id="kobo.1236.2">We will start with a model that includes an intercept, linear terms, interactions, and squared terms for each predictor. </span><span class="koboSpan" id="kobo.1236.3">This comprehensive model allows us to capture more complex relationships between the predictors and the dependent variable. </span><span class="koboSpan" id="kobo.1236.4">However, we will still utilize stepwise regression to iteratively remove terms that lack statistical significance. </span><span class="koboSpan" id="kobo.1236.5">By doing so, we can refine the model and focus on the predictors that have a substantial impact on the dependent variable, while disregarding those that are not </span><span class="No-Break"><span class="koboSpan" id="kobo.1237.1">statistically significant:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1238.1">
Model3 = stepwiselm(YHData,'quadratic');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1239.1">Lots of </span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.1240.1">information</span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.1241.1"> about how the variables were removed will </span><span class="No-Break"><span class="koboSpan" id="kobo.1242.1">be printed.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1243.1">Let’s see the form of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1244.1">model obtained:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1245.1">Model3
Model3 =
Linear regression model:
    ResResistance ~ 1 + PrismaticCoef*FroudeNumber + FroudeNumber^2
Estimated Coefficients:
           Estimate      SE        tStat        pValue
        ________    ______    _________    ___________
(Intercept) -0.34693    17.184 -0.020188       0.98391
PrismaticCoef 69.155    30.254    2.2858      0.022958
FroudeNumber -206.99    58.003   -3.5686    0.00041711
PrismaticCoef:FroudeNumber
             -305.22    99.309   -3.0735     0.0023082
FroudeNumber^2 871.03    25.817   33.739   1.2559e-104
Number of observations: 308, Error degrees of freedom: 303</span><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.1246.1">
Root Mean Squared Error: 4.08
R-squared: 0.928,  Adjusted R-Squared: 0.927
F-statistic vs. </span><span class="koboSpan" id="kobo.1246.2">constant model: 982, p-value = 4.93e-172</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1247.1">In this iteration, the resulting model is more intricate and comprehensive. </span><span class="koboSpan" id="kobo.1247.2">It includes the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1248.1">FroudeNumber</span></strong><span class="koboSpan" id="kobo.1249.1"> variable both as a squared term and as part of the interaction with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1250.1">PrismaticCoef</span></strong><span class="koboSpan" id="kobo.1251.1">. </span><span class="koboSpan" id="kobo.1251.2">This increased complexity better captures the underlying phenomenon, as indicated by the obtained results: an R-squared value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1252.1">0.928</span></strong><span class="koboSpan" id="kobo.1253.1">, an adjusted R-squared value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1254.1">0.927</span></strong><span class="koboSpan" id="kobo.1255.1">, and a highly significant p-value </span><span class="No-Break"><span class="koboSpan" id="kobo.1256.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1257.1">4.93e-172</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1258.1">.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.1259.1">These metrics suggest that the model provides a highly representative representation of the relationship between the predictors and the dependent variable, explaining a significant portion </span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.1260.1">of the</span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.1261.1"> variability observed in </span><span class="No-Break"><span class="koboSpan" id="kobo.1262.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.1263.1">Let’s now see how to perform a PCA by analyzing a </span><span class="No-Break"><span class="koboSpan" id="kobo.1264.1">practical case.</span></span></p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.1265.1">Carrying out PCA</span></h2>
<p><span class="koboSpan" id="kobo.1266.1">It is a statistical technique</span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.1267.1"> used for dimensionality reduction and data analysis. </span><span class="koboSpan" id="kobo.1267.2">PCA aims to transform a dataset with a potentially large number of variables into a smaller set of uncorrelated variables called principal components. </span><span class="koboSpan" id="kobo.1267.3">These components are linear combinations of the original variables and are ordered in such a way that the first component captures the maximum amount of variance in the data, the second component captures the next highest amount of variance, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1268.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.1269.1">PCA is often used in various fields, such as machine learning, data visualization, and exploratory data analysis. </span><span class="koboSpan" id="kobo.1269.2">It helps in identifying patterns and relationships in the data, reducing noise, and simplifying complex datasets. </span><span class="koboSpan" id="kobo.1269.3">By reducing the dimensionality of the data, PCA can aid in visualizing and interpreting high-dimensional data, as well as improving computational efficiency in subsequent analyses. </span><span class="koboSpan" id="kobo.1269.4">Additionally, PCA can be used for data preprocessing and feature extraction, allowing for more effective modeling and </span><span class="No-Break"><span class="koboSpan" id="kobo.1270.1">prediction tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.1271.1">One of the major challenges in multivariate statistical analysis is effectively displaying datasets with numerous variables. </span><span class="koboSpan" id="kobo.1271.2">Thankfully, in such datasets, it is common for certain variables to be closely interrelated. </span><span class="koboSpan" id="kobo.1271.3">These variables essentially contain the same information as they measure the same underlying quantity that influences the system’s behavior. </span><span class="koboSpan" id="kobo.1271.4">Consequently, these variables are redundant and do not contribute anything significant to the model we aim to construct. </span><span class="koboSpan" id="kobo.1271.5">To simplify the problem, we can replace this group of variables with a new variable that encapsulates the relevant information. </span><span class="koboSpan" id="kobo.1271.6">The following figure illustrates redundant data in </span><span class="No-Break"><span class="koboSpan" id="kobo.1272.1">a table.</span></span></p>
<p><span class="koboSpan" id="kobo.1273.1">PCA generates a set of new variables, known as principal components, which are uncorrelated with each other. </span><span class="koboSpan" id="kobo.1273.2">Each principal component is formed as a linear combination of the original variables. </span><span class="koboSpan" id="kobo.1273.3">The orthogonality between the principal components ensures that there is no redundant information (see </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1274.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1275.1">.8</span></em><span class="koboSpan" id="kobo.1276.1">). </span><span class="koboSpan" id="kobo.1276.2">Together, the principal components form an orthogonal basis for the data space. </span><span class="koboSpan" id="kobo.1276.3">The primary objective of PCA is to explain the maximum variance in the data using the fewest number of principal </span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.1277.1">components. </span><span class="koboSpan" id="kobo.1277.2">In numerous real-world datasets, particularly those with high dimensionality, redundancy or noise is frequently present. </span><span class="koboSpan" id="kobo.1277.3">Not all dimensions (features) play an equally vital role in shaping the inherent structure or patterns within the data. </span><span class="koboSpan" id="kobo.1277.4">By pinpointing the directions (principal components) along which the data showcases the greatest variance, PCA can trim down the data’s dimensionality by opting for a subset of these principal components. </span><span class="koboSpan" id="kobo.1277.5">This simplification streamlines the dataset while conserving its </span><span class="No-Break"><span class="koboSpan" id="kobo.1278.1">fundamental traits.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<span class="koboSpan" id="kobo.1279.1"><img alt="Figure 4.8 – PCA new feature space based on linear combinations of the original feature space" src="image/B21156_04_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1280.1">Figure 4.8 – PCA new feature space based on linear combinations of the original feature space</span></p>
<p><span class="koboSpan" id="kobo.1281.1">In MATLAB, you can perform PCA using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1282.1">pca()</span></strong><span class="koboSpan" id="kobo.1283.1"> function. </span><span class="koboSpan" id="kobo.1283.2">This function takes an n-by-p data matrix as input, where </span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.1284.1">each row corresponds to an observation and each column corresponds to a variable. </span><span class="koboSpan" id="kobo.1284.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1285.1">pca()</span></strong><span class="koboSpan" id="kobo.1286.1"> function returns the principal component coefficients, also known as loadings. </span><span class="koboSpan" id="kobo.1286.2">The coefficient matrix has dimensions p-by-p. </span><span class="koboSpan" id="kobo.1286.3">Each column of the coefficient matrix contains the coefficients for one principal component, arranged in descending order based on the component variance. </span><span class="koboSpan" id="kobo.1286.4">By default, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1287.1">pca()</span></strong><span class="koboSpan" id="kobo.1288.1"> function centers the data and utilizes the </span><span class="No-Break"><span class="koboSpan" id="kobo.1289.1">SVD algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.1290.1">In this study, we utilized a dataset consisting of measurements of the geometric properties of kernels from three distinct varieties of wheat. </span><span class="koboSpan" id="kobo.1290.2">The dataset includes kernels from the Kama, Rosa, and Canadian varieties, with a total of 70 samples randomly selected for the experiment. </span><span class="koboSpan" id="kobo.1290.3">The internal structure of the kernels was visualized using a soft X-ray technique, resulting in high-quality images captured on 13x18 cm X-ray Kodak plates. </span><span class="koboSpan" id="kobo.1290.4">The wheat grain used in the study was obtained from experimental fields and harvested using a combine harvester. </span><span class="koboSpan" id="kobo.1290.5">The research was conducted at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. </span><span class="koboSpan" id="kobo.1290.6">The data was collected from the UCI Machine </span><span class="No-Break"><span class="koboSpan" id="kobo.1291.1">Learning Repository:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1292.1">As always, we begin by collecting the data to be analyzed. </span><span class="koboSpan" id="kobo.1292.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1293.1">SeedsData -dataset</span></strong><span class="koboSpan" id="kobo.1294.1"> is multivariate, consisting of 210 instances. </span><span class="koboSpan" id="kobo.1294.2">Seven geometric parameters of the wheat kernel are used as real-valued attributes organizing an instance. </span><span class="koboSpan" id="kobo.1294.3">We will first import the dataset into the </span><span class="No-Break"><span class="koboSpan" id="kobo.1295.1">MATLAB workspace:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1296.1">
SeedsData = readtable('SeedsDataset');</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1297.1">Before applying the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1298.1">pca()</span></strong><span class="koboSpan" id="kobo.1299.1"> function to our data, let’s take a preliminary look at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1300.1">SeedsData</span></strong><span class="koboSpan" id="kobo.1301.1"> table. </span><span class="koboSpan" id="kobo.1301.2">The first seven columns of the table represent the measured variables, while the eighth column indicates the type </span><span class="No-Break"><span class="koboSpan" id="kobo.1302.1">of seed.</span></span></p></li> <li><span class="koboSpan" id="kobo.1303.1">To examine the potential relationships between these variables, we can use the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1304.1">plotmatrix()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1"> function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1306.1">
plotmatrix(SeedsData{:,1:7})</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1307.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1308.1">plotmatrix()</span></strong><span class="koboSpan" id="kobo.1309.1"> function generates a matrix of subaxes, where the diagonal subaxes display</span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.1310.1"> histogram plots of the data in each respective column. </span><span class="koboSpan" id="kobo.1310.2">The remaining subaxes are scatter plots, representing the relationships between different pairs of columns in the matrix. </span><span class="koboSpan" id="kobo.1310.3">In the following figure, each subplot in the ith row and jth column represents a scatter plot of the ith column against the jth column. </span><span class="koboSpan" id="kobo.1310.4">The visualization in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1311.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1312.1">.9</span></em><span class="koboSpan" id="kobo.1313.1"> provides insights into the data distribution and potential correlations </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">between variables.</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.1315.1"><img alt="Figure 4.9 – Scatter plot matrix of the measured variables (Area; Perimeter; Compactness; LengthK; WidthK; AsymCoef; LengthKG)" src="image/B21156_04_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1316.1">Figure 4.9 – Scatter plot matrix of the measured variables (Area; Perimeter; Compactness; LengthK; WidthK; AsymCoef; LengthKG)</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1317.1">As observed in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1318.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1319.1">.9</span></em><span class="koboSpan" id="kobo.1320.1">, scatter plot matrices provide a useful visual tool for identifying potential linear</span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.1321.1"> correlations among multiple variables. </span><span class="koboSpan" id="kobo.1321.2">This aids in pinpointing specific variables that may exhibit mutual correlations, indicating possible redundancy in the data. </span><span class="koboSpan" id="kobo.1321.3">Additionally, the diagonal of the matrix displays histogram plots, offering insights into the distribution of values for each measured variable. </span><span class="koboSpan" id="kobo.1321.4">The remaining plots represent scatter plots of the matrix columns, with each plot appearing twice: once in the corresponding row and again in the corresponding column as a </span><span class="No-Break"><span class="koboSpan" id="kobo.1322.1">mirror image.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1323.1">Upon analyzing </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1324.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1325.1">.9</span></em><span class="koboSpan" id="kobo.1326.1">, several plots demonstrate a linear relationship between variables. </span><span class="koboSpan" id="kobo.1326.2">For instance, the plot showing the relationship between </span><strong class="source-inline"><span class="koboSpan" id="kobo.1327.1">Area</span></strong><span class="koboSpan" id="kobo.1328.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1329.1">Perimeter</span></strong><span class="koboSpan" id="kobo.1330.1">, as well as the one between </span><strong class="source-inline"><span class="koboSpan" id="kobo.1331.1">Perimeter</span></strong><span class="koboSpan" id="kobo.1332.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1333.1">LengthK</span></strong><span class="koboSpan" id="kobo.1334.1">, exhibits such a correlation. </span><span class="koboSpan" id="kobo.1334.2">However, no correlation can be observed for certain variable pairs. </span><span class="koboSpan" id="kobo.1334.3">For example, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1335.1">LengthKG</span></strong><span class="koboSpan" id="kobo.1336.1"> variable appears to have no correlation with any other variable, as its data is scattered throughout the </span><span class="No-Break"><span class="koboSpan" id="kobo.1337.1">plot area.</span></span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.1338.1">To further validate this initial visual analysis, we can calculate the linear correlation coefficients between the measured variables. </span><span class="koboSpan" id="kobo.1338.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1339.1">corr()</span></strong><span class="koboSpan" id="kobo.1340.1"> function can be employed for this purpose, which returns a matrix containing the pairwise linear correlation coefficient </span><strong class="source-inline"><span class="koboSpan" id="kobo.1341.1">(r)</span></strong><span class="koboSpan" id="kobo.1342.1"> between each pair of columns in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1343.1">user-provided matrix:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1344.1">
CorrData=corr(SeedsData{:,1:7})</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1345.1">One might question the need to analyze both the scatter plots and the correlation coefficients. </span><span class="koboSpan" id="kobo.1345.2">The reason is that there are cases where the scatterplots provide information that the correlation coefficients alone cannot convey. </span><span class="koboSpan" id="kobo.1345.3">If the scatterplot </span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.1346.1">does not indicate a linear relationship between variables, the correlation calculation becomes less meaningful. </span><span class="koboSpan" id="kobo.1346.2">In such scenarios, two </span><span class="No-Break"><span class="koboSpan" id="kobo.1347.1">possibilities arise:</span></span></p><ul><li><span class="koboSpan" id="kobo.1348.1">If no relationship exists at all between the variables, calculating the correlation is not appropriate because correlation specifically applies to </span><span class="No-Break"><span class="koboSpan" id="kobo.1349.1">linear relationships.</span></span></li><li><span class="koboSpan" id="kobo.1350.1">If a strong relationship exists, but it is not linear, the correlation coefficient can be misleading. </span><span class="koboSpan" id="kobo.1350.2">In certain cases, a strong curved relationship may exist, which cannot be captured accurately by the </span><span class="No-Break"><span class="koboSpan" id="kobo.1351.1">correlation coefficient.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.1352.1">This emphasizes the critical importance of examining the scatterplots alongside the </span><span class="No-Break"><span class="koboSpan" id="kobo.1353.1">correlation coefficients.</span></span></p></li> <li><span class="koboSpan" id="kobo.1354.1">With this understanding, it is now appropriate to proceed with </span><span class="No-Break"><span class="koboSpan" id="kobo.1355.1">the PCA:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1356.1">
[coeff,score,latent,tsquared,explained,mu] = pca(SeedsData{:,1:7});</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1357.1">This code returns the </span><span class="No-Break"><span class="koboSpan" id="kobo.1358.1">following information:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.1359.1">coeff</span></strong><span class="koboSpan" id="kobo.1360.1">: The principal </span><span class="No-Break"><span class="koboSpan" id="kobo.1361.1">component coefficients</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1362.1">score</span></strong><span class="koboSpan" id="kobo.1363.1">: The principal </span><span class="No-Break"><span class="koboSpan" id="kobo.1364.1">component scores</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1365.1">latent</span></strong><span class="koboSpan" id="kobo.1366.1">: The principal </span><span class="No-Break"><span class="koboSpan" id="kobo.1367.1">component variances</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1368.1">tsquared</span></strong><span class="koboSpan" id="kobo.1369.1">: The Hotelling’s T-squared statistic for </span><span class="No-Break"><span class="koboSpan" id="kobo.1370.1">each observation</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1371.1">explained</span></strong><span class="koboSpan" id="kobo.1372.1">: The percentage of the total variance explained by each </span><span class="No-Break"><span class="koboSpan" id="kobo.1373.1">principal component</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1374.1">mu</span></strong><span class="koboSpan" id="kobo.1375.1">: The estimated mean of </span><span class="No-Break"><span class="koboSpan" id="kobo.1376.1">each variable</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.1377.1">In situations where the variables are measured in different units or the variance differs significantly across columns, it is often recommended to scale the data or apply weights. </span><span class="koboSpan" id="kobo.1377.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1378.1">pca()</span></strong><span class="koboSpan" id="kobo.1379.1"> function in MATLAB incorporates a default behavior of centering the user-provided</span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.1380.1"> matrix by subtracting the column means before conducting either SVD or </span><span class="No-Break"><span class="koboSpan" id="kobo.1381.1">eigenvalue decomposition.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1382.1">Additionally, for performing the PCA, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1383.1">pca()</span></strong><span class="koboSpan" id="kobo.1384.1"> function offers three </span><span class="No-Break"><span class="koboSpan" id="kobo.1385.1">different algorithms:</span></span></p><ul><li><span class="No-Break"><span class="koboSpan" id="kobo.1386.1">SVD (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1387.1">svd</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1388.1">)</span></span></li><li><span class="koboSpan" id="kobo.1389.1">Eigenvalue decomposition of the covariance </span><span class="No-Break"><span class="koboSpan" id="kobo.1390.1">matrix (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1391.1">eig</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1392.1">)</span></span></li><li><span class="koboSpan" id="kobo.1393.1">Alternating least squares </span><span class="No-Break"><span class="koboSpan" id="kobo.1394.1">algorithm (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1395.1">als</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1396.1">)</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.1397.1">By default, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1398.1">pca()</span></strong><span class="koboSpan" id="kobo.1399.1"> function utilizes the SVD algorithm for </span><span class="No-Break"><span class="koboSpan" id="kobo.1400.1">the analysis.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1401.1">Let’s analyze the results returned by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1402.1">pca()</span></strong><span class="koboSpan" id="kobo.1403.1"> function. </span><span class="koboSpan" id="kobo.1403.2">The coefficient matrix, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1404.1">coeff</span></strong><span class="koboSpan" id="kobo.1405.1">, obtained from the PCA, contains the coefficients for the first seven variables present in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1406.1">SeedsData</span></strong><span class="koboSpan" id="kobo.1407.1"> table. </span><span class="koboSpan" id="kobo.1407.2">The rows of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1408.1">coeff</span></strong><span class="koboSpan" id="kobo.1409.1"> represent the variables, while the columns correspond to the principal components. </span><span class="koboSpan" id="kobo.1409.2">The coefficients within each column determine the linear combination of the original variables that represent the information in the new dimensional space. </span><span class="koboSpan" id="kobo.1409.3">The columns of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1410.1">coeff</span></strong><span class="koboSpan" id="kobo.1411.1"> are arranged in descending order based on the variance of each </span><span class="No-Break"><span class="koboSpan" id="kobo.1412.1">principal component.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1413.1">A principal component is a linear combination of the original variables, denoted as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1414.1">p</span></strong><span class="koboSpan" id="kobo.1415.1">, weighted by a vector, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1416.1">u</span></strong><span class="koboSpan" id="kobo.1417.1">. </span><span class="koboSpan" id="kobo.1417.2">The first principal component is formed by combining the variables with the highest variance. </span><span class="koboSpan" id="kobo.1417.3">In contrast, the second principal component combines variables with a slightly lower variance, while also ensuring orthogonality to the previous component. </span><span class="koboSpan" id="kobo.1417.4">This pattern continues for subsequent principal components, each incorporating variables with progressively lower variances while maintaining orthogonality to the preceding components. </span><span class="koboSpan" id="kobo.1417.5">The number of principal components is equal to the number of observed variables. </span><span class="koboSpan" id="kobo.1417.6">Each principal component is derived as a linear combination that maximizes the variance while maintaining non-correlation with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1418.1">preceding components.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1419.1">The second output, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1420.1">score</span></strong><span class="koboSpan" id="kobo.1421.1">, comprises the coordinates of the original data in the new dimensional space </span><a id="_idIndexMarker464"/><span class="koboSpan" id="kobo.1422.1">defined by the principal components. </span><span class="koboSpan" id="kobo.1422.2">It represents how each observation aligns with the principal components and provides a representation of the data in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1423.1">transformed space.</span></span></p></li> <li><span class="koboSpan" id="kobo.1424.1">Our goal is to represent the dataset in a new space with reduced dimensions. </span><span class="koboSpan" id="kobo.1424.2">To achieve this, we plot the first two columns of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1425.1">score</span></strong><span class="koboSpan" id="kobo.1426.1"> matrix, which represent the coordinates of the original data in the new coordinate system defined by the principal components. </span><span class="koboSpan" id="kobo.1426.2">To make the graph more comprehensible, we group the data </span><span class="No-Break"><span class="koboSpan" id="kobo.1427.1">by class:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1428.1">
gscatter(score(:,1),score(:,2),SeedsData.Seeds,'brg','xo^')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1429.1">The following figure </span><span class="No-Break"><span class="koboSpan" id="kobo.1430.1">is printed:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer042">
<span class="koboSpan" id="kobo.1431.1"><img alt="Figure 4.10 – The scatter plot for the first two principal components" src="image/B21156_04_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1432.1">Figure 4.10 – The scatter plot for the first two principal components</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1433.1">The previous figure distinctly classifies the seeds into three distinct classes. </span><span class="koboSpan" id="kobo.1433.2">The data points are visibly distributed in separate areas of the plot, with minimal uncertainty observed only in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1434.1">border regions.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1435.1">Let’s revisit the results</span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.1436.1"> obtained from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1437.1">pca()</span></strong><span class="koboSpan" id="kobo.1438.1"> function. </span><span class="koboSpan" id="kobo.1438.2">The third output, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1439.1">latent</span></strong><span class="koboSpan" id="kobo.1440.1">, is a vector that represents the variance explained by each corresponding principal component. </span><span class="koboSpan" id="kobo.1440.2">The variance of each principal component is reflected in the sample variances of the columns in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1441.1">score</span></strong><span class="koboSpan" id="kobo.1442.1"> matrix, which align with the corresponding rows in </span><strong class="source-inline"><span class="koboSpan" id="kobo.1443.1">latent</span></strong><span class="koboSpan" id="kobo.1444.1">. </span><span class="koboSpan" id="kobo.1444.2">As previously mentioned, the columns of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1445.1">latent</span></strong><span class="koboSpan" id="kobo.1446.1"> are arranged in descending order based on the variance of each </span><span class="No-Break"><span class="koboSpan" id="kobo.1447.1">principal component.</span></span></p>
<ol>
<li value="6"><span class="koboSpan" id="kobo.1448.1">To complete the analysis, we can visualize both the principal component coefficients for each variable and the principal component scores for each observation in a single plot. </span><span class="koboSpan" id="kobo.1448.2">This type of plot is commonly referred to as a biplot. </span><span class="koboSpan" id="kobo.1448.3">Biplots serve as exploration plots that enable the simultaneous display of graphical information on both the samples and variables present in a data matrix. </span><span class="koboSpan" id="kobo.1448.4">In biplots, samples are represented as points, while variables are depicted </span><span class="No-Break"><span class="koboSpan" id="kobo.1449.1">as vectors:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1450.1">
biplot(coeff(:,1:2),'scores',score(:,1:2),'varlabels',SeedsData.Properties.VariableNames(1:7));</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1451.1">The following diagram depicts the principal components coefficients for </span><span class="No-Break"><span class="koboSpan" id="kobo.1452.1">each variable:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.1453.1"><img alt="Figure 4.11 – Biplot of the principal component coefficients for each variable and principal component scores for each observation" src="image/B21156_04_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1454.1">Figure 4.11 – Biplot of the principal component coefficients for each variable and principal component scores for each observation</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1455.1">In the biplot, each of the </span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.1456.1">seven variables is represented by a vector. </span><span class="koboSpan" id="kobo.1456.2">The direction and length of each vector indicate the contribution of that variable to the two principal components depicted in </span><span class="No-Break"><span class="koboSpan" id="kobo.1457.1">the plot.</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1458.1">For example, in the first principal component (horizontal axis), four variables have positive coefficients, while three variables have a negative coefficient. </span><span class="koboSpan" id="kobo.1458.2">This explains why four vectors are directed toward the right half of the plot, while three vectors are directed toward the left half. </span><span class="koboSpan" id="kobo.1458.3">The largest coefficient in the first principal component corresponds to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1459.1">AsymCoef</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1460.1"> variable.</span></span></p>
<p><span class="koboSpan" id="kobo.1461.1">Similarly, in the second principal component (vertical axis), all the variables have positive coefficients. </span><span class="koboSpan" id="kobo.1461.2">By examining the length of the vectors, we can clearly understand the weight of each variable in their respective principal components. </span><span class="koboSpan" id="kobo.1461.3">It is evident that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1462.1">AsymCoef</span></strong><span class="koboSpan" id="kobo.1463.1"> variable holds a significant weight compared to the others in the first principal component. </span><span class="koboSpan" id="kobo.1463.2">Similarly, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1464.1">LengthK</span></strong><span class="koboSpan" id="kobo.1465.1"> variable assumes a prominent role in the second </span><span class="No-Break"><span class="koboSpan" id="kobo.1466.1">principal</span></span><span class="No-Break"><a id="_idIndexMarker467"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1467.1"> component.</span></span></p>
<h1 id="_idParaDest-101"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.1468.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1469.1">In this chapter, we gained knowledge about performing accurate cluster analysis in the MATLAB environment. </span><span class="koboSpan" id="kobo.1469.2">Our exploration began by understanding the measurement of similarity, including concepts such as element proximity, similarity, and dissimilarity measures. </span><span class="koboSpan" id="kobo.1469.3">We delved into different methods for grouping objects, namely hierarchical clustering, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1470.1">partitioning clustering.</span></span></p>
<p><span class="koboSpan" id="kobo.1471.1">Regarding partitioning clustering, we focused on the k-means method. </span><span class="koboSpan" id="kobo.1471.2">We learned how to iteratively locate </span><em class="italic"><span class="koboSpan" id="kobo.1472.1">k</span></em><span class="koboSpan" id="kobo.1473.1"> centroids, each representing a cluster. </span><span class="koboSpan" id="kobo.1473.2">We also examined the effectiveness of cluster separation and how to generate a silhouette plot using cluster indices obtained from k-means. </span><span class="koboSpan" id="kobo.1473.3">The silhouette value for each data point serves as a measure of its similarity to other points within its own cluster, compared to points in other clusters. </span><span class="koboSpan" id="kobo.1473.4">Furthermore, we delved into k-medoids clustering, which involves identifying the centers of clusters using medoids instead of centroids. </span><span class="koboSpan" id="kobo.1473.5">We learned the procedure for locating these </span><span class="No-Break"><span class="koboSpan" id="kobo.1474.1">medoid centers.</span></span></p>
<p><span class="koboSpan" id="kobo.1475.1">Next, we explored the process of selecting a feature that best represents a given dataset, which is known as dimensionality reduction. </span><span class="koboSpan" id="kobo.1475.2">We gained an understanding of the fundamental concept behind dimensionality reduction and how it can be achieved through </span><span class="No-Break"><span class="koboSpan" id="kobo.1476.1">variable transformation.</span></span></p>
<p><span class="koboSpan" id="kobo.1477.1">To perform feature extraction and dimensionality reduction, we learned how to utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1478.1">stepwiselm()</span></strong><span class="koboSpan" id="kobo.1479.1"> function. </span><span class="koboSpan" id="kobo.1479.2">This function enables the creation of a linear model and the automatic addition or removal of variables based on their significance. </span><span class="koboSpan" id="kobo.1479.3">We also discovered how to construct small models starting from a constant model and large models from models containing </span><span class="No-Break"><span class="koboSpan" id="kobo.1480.1">numerous terms.</span></span></p>
<p><span class="koboSpan" id="kobo.1481.1">In addition, we discussed techniques for handling missing values within a dataset and explored various methods for extracting features. </span><span class="koboSpan" id="kobo.1481.2">Among these methods, we specifically analyzed PCA. </span><span class="koboSpan" id="kobo.1481.3">PCA is a robust quantitative approach that simplifies the data by identifying the most </span><span class="No-Break"><span class="koboSpan" id="kobo.1482.1">informative components.</span></span></p>
<p><span class="koboSpan" id="kobo.1483.1">In the next chapter, we will delve into the fundamental concepts of artificial neural networks and their implementation in the MATLAB environment. </span><span class="koboSpan" id="kobo.1483.2">Our focus will be on understanding the basic principles of neural networks and how to apply them effectively. </span><span class="koboSpan" id="kobo.1483.3">We will explore various aspects of neural network analysis, including data preparation, fitting, pattern recognition, and clustering analysis, all within the MATLAB framework. </span><span class="koboSpan" id="kobo.1483.4">Additionally, we will delve into the techniques of preprocessing, postprocessing, and network visualization, which play a crucial role in enhancing training efficiency and evaluating network performance. </span><span class="koboSpan" id="kobo.1483.5">By the end of the chapter, you will have gained practical knowledge on implementing and optimizing artificial neural networks using MATLAB, enabling you to tackle a wide range of data </span><span class="No-Break"><span class="koboSpan" id="kobo.1484.1">analysis tasks.</span></span></p>
</div>
</body></html>