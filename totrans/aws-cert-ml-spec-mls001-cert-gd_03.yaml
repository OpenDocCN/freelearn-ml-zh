- en: '*Chapter 2*: AWS Application Services for AI/ML'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about the AWS AI services for building chatbots,
    advanced text analysis, document analysis, transcription, and so on. This chapter
    has been designed in such a way that you can solve different use cases by integrating
    AWS AI services and get an idea of how they work. AWS is growing every day and
    they are adding new AI services regularly.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will approach different use cases programmatically or from
    the console. This will help you understand different APIs and their usages. We
    will use S3 for storage and AWS Lambda to execute any code. The examples in this
    chapter are in Python, but you can use other supported languages such as Java,
    Node.js, .NET, PowerShell, Ruby, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing images and videos with Amazon Rekognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text to speech with Amazon Polly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech to text with Amazon Transcribe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing natural language processing with Amazon Comprehend
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating documents with Amazon Translate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting text from documents with Amazon Textract
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating chatbots on Amazon Lex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All you will need for this chapter is an AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the code examples for this chapter from GitHub at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2).
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing images and videos with Amazon Rekognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you need to add powerful visual analysis to your applications, then **Amazon
    Rekognition** is the service to opt for. **Rekognition Image** lets you easily
    build powerful applications to search, verify, and organize millions of images.
    **Rekognition Video** lets you extract motion-based context from stored or live
    stream videos, and helps you analyze them. Rekognition Video also allows you to
    index metadata such as objects, activities, scene, celebrities, and faces, making
    video searches easy. Rekognition Image uses deep neural network models to detect
    and label thousands of objects and scenes in your images. It helps you capture
    text in an image, a bit like **Optical Character Recognition** (**OCR**). A perfect
    example is a T-shirt with quotes on it. If you were to take a picture of one and
    ask Amazon Rekognition to extract the text from it, it would be able to tell you
    what the text says. You can also perform celebrity recognition using Amazon Rekognition.
    I am not a celebrity, so I won't be using the celebrity recognition API for my
    face; instead, I will use the face comparison API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The official documentation, available at [https://aws.amazon.com/rekognition/faqs/](https://aws.amazon.com/rekognition/faqs/),
    states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"With Rekognition Image, you only pay for the images you analyze and the face
    metadata you store. You will not be charged for the compute resources if, at any
    point of time, your training fails."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Rekognition include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Image and video analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searchable image library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face-based user verification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text in image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image moderation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search index for video archives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy filtering for videos, for explicit and suggestive content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of explicit nudity – sexual activity, graphical nudity, adult toys,
    and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of suggestive content – partial nudity, swimwear or underwear, and
    so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Rekognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at some of the benefits of using Amazon Rekognition:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS manages the infrastructure it runs on. In short, just use the API for your
    image analysis. We just need to focus on building and managing our deep learning
    pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With or without knowledge of image processing, you can perform image and video
    analysis just by using the APIs provided in Amazon Rekognition, which can be used
    for any application or service on several platforms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The Labels API''s response will identify real-world entities within an image
    through the DetectLabels API. These labels include city, town, table, home, garden,
    animal, pets, food, drink, electronics, flowers, and more. The entities are classified
    based on their **Confidence** score, which indicates the probability that a given
    prediction is correct: the higher the better. Similarly, we can use the DetectText
    API to extract the text in an image. Amazon Rekognition may detect multiple lines
    based on the gap between words. Periods don''t represent the end of a line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Rekognition can be integrated with AWS Kinesis Video Stream, AWS S3,
    and AWS Lambda for seamless and affordable image and video analysis. With the
    AWS IAM service, Amazon Rekognition API calls can easily be secured and controlled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low cost. You only pay for the images and videos that are analyzed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through AWS CloudTrail, all the API calls for Amazon Rekognition can be captured
    as events. It captures all calls made from the console, or the CLI, or code calls
    for APIs, which further enables the user to create Amazon SNS notifications based
    on CloudTrail events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can create a VPC Endpoint policy for specific API calls to establish a private
    connection between your VPC and Amazon Rekognition. This helps you leverage enhanced
    security. As per the AWS shared responsibility model, AWS takes care of the security
    of the infrastructure and software, and we have to take care of the security of
    our content in the cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Rekognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will learn how to integrate AWS Lambda with Amazon Rekognition
    to detect the labels in our image (uploaded at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Rekognition%20Demo/images](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Rekognition%20Demo/images))
    and print the detected objects in the CloudWatch console. We will use the `detect_labels`
    API from Amazon Rekognition in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by creating an IAM role for Lambda:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the IAM console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Roles** from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Lambda** from the **Choose a use case** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following managed policies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3ReadOnlyAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonRekognitionFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchLogsFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Name the role `rekognition-lambda-role`:![Figure 2.1 – The Create role dialog
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_001.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.1 – The Create role dialog
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will create a Lambda function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the AWS Lambda console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create function**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `lambda-rekognition`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choose `Python 3.6` from the `rekognition-lambda-role`:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Creating the Lambda function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – Creating the Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following code in `lambda_function.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we will create a trigger for the Lambda Function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the AWS S3 console page. Create a bucket, for example, `rekognition-test-baba`,
    as shown in the following screenshot:![Figure 2.3 – AWS S3 console page
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.3 – AWS S3 console page
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on `images`. Click **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Properties** tab of our bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll to **Events** for that bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside the `rekognition_event`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object create events`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`images`*/*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`lambda-rekognition`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.4 –S3 bucket Events window'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 –S3 bucket Events window
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will upload the image from the shared GitHub repository to the S3 bucket
    `images` folder.
  prefs: []
  type: TYPE_NORMAL
- en: As soon as you upload, you can check the **Monitoring** tab in the Lambda console
    to monitor the events, as shown in the following screenshot:![Figure 2.5 – CloudWatch
    monitoring the event in the Lambda console
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.5 – CloudWatch monitoring the event in the Lambda console
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Navigate to **CloudWatch > CloudWatch Logs > Log groups > /aws/lambda/lambda-rekognition**.
    Select the latest stream from all the streams and scroll down in the logs to see
    your output, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.6 – CloudWatch logs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – CloudWatch logs
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to implement the Amazon Rekognition AI service
    to detect objects in an image and get a confidence score for each. We will see
    more use cases for Amazon Rekognition in the upcoming sections, when we detect
    text in images. In the next section, we will learn about Amazon's text-to-speech
    service and implement it.
  prefs: []
  type: TYPE_NORMAL
- en: Text to speech with Amazon Polly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Polly** is all about converting text into speech, and does so using
    pretrained deep learning models. It''s a fully managed service, so we don''t have
    to do anything. You provide the plain text as input for synthesizing or in **Speech
    Synthesis Markup Language** (**SSML**) format so that an audio stream is returned.
    It also gives you different languages and voices to choose from, with both male
    and female options. The output audio from Amazon Polly can be saved in MP3 format
    for further usage in the application (web or mobile) or can be a JSON output for
    written speech.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we were to input the text "Baba went to the library" into Amazon
    Polly, the output speech mark object would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The word `"went"` begins 370 milliseconds after the audio stream begins, and
    starts at byte 5 and ends at byte 9 of the given input text.
  prefs: []
  type: TYPE_NORMAL
- en: It also returns output in `ogg_vorbis` and `pcm` format. When `pcm` is used,
    the content that's returned is audio/pcm in a signed 16-bit, 1 channel (mono),
    little-endian format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Polly include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Can be used as an accessibility tool for reading web content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be integrated with Amazon Rekognition to help visually impaired people read
    signs. You can click a picture of the sign with text and feed it to Amazon Rekognition
    to extract text. The output text can be used as input for Polly, and it will return
    a voice as output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used in a public address system, where the admin team can just pass on
    the text to be announced and Amazon Polly does the magic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining Amazon Polly with **Amazon Connect** (telephony backend service),
    you can build an **audio/video receiver** (**AVR**) system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smart devices such as smart tvs, smart watches, and **Internet of Things** (**IoT**)
    devices can use this for audio output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narration generation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When combined with Amazon Lex, full-blown voice user interfaces for applications
    can be developed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's explore the benefits of Amazon Polly.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Polly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the benefits of using Amazon Polly include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: This service is fully managed and doesn't require any admin cost to maintain
    or manage resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides an instant speech corrections and enhancements facility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can develop your own access layer using the HTTP API from Amazon Polly.
    Development is easy due to the huge amount of language support that's available,
    such as Python, Ruby, Go, C++, Java, and Node.js.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For certain neural voices, speech can be synthesized by using the Newscaster
    style, to make them sound like a TV or radio broadcaster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Polly also allows you to modify the pronunciation of particular words
    or the use of new words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll get hands-on with Amazon Polly.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Polly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will build a pipeline where we can integrate AWS Lambda
    with Amazon Polly to read a text file, and then generate an **MP3** file of the
    same to another folder in the same bucket. We will monitor the task's progress
    in CloudWatch logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by creating an IAM Role for Lambda. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the IAM console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Roles** from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Lambda** as the trusted entity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following managed policies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3FullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonPollyFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the role as `polly-lambda-role`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will create a Lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to `polly-lambda`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the runtime to `python 3.6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use an existing role; that is, `polly-lambda-role`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paste the code at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Rekognition%20Demo/lambda_code](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Rekognition%20Demo/lambda_code)
    into your lambda function and check its progress in the CloudWatch console. We
    will be using the `start_speech_synthesis_task` API from Amazon Polly for this
    code; it is an asynchronous synthesis task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll down and in `59` **sec**, as shown in the following screenshot, and
    click **Save**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The default is 3 sec. Since this is an asynchronous operation, any retried attempts
    will create more files.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Edit basic settings window'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16735_02_007.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.7 – Edit basic settings window
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will create a bucket to trigger an event.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the AWS S3 console and create a bucket called `polly-test-baba`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder called `input-text` (in this example, we will only upload .`txt`
    files).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `polly_event`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object create events`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`input-text/`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`txt`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`polly-lambda`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will upload a file in order to trigger an event and check its progress
    in CloudWatchUpload, in this case, a file `test_file.txt` in `input-text`, as
    shown in the following screenshot. You can download the sample file from this
    book's GitHub repository at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Polly%20Demo/text_file](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Polly%20Demo/text_file):![Figure
    2.8 – The S3 bucket after uploading a text file for further processing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_008.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.8 – The S3 bucket after uploading a text file for further processing
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This will trigger the lambda function. You can monitor your logs by going to
    **CloudWatch> CloudWatch Logs> Log groups> /aws/lambda/polly-lambda**:![Figure
    2.9 – Log groups in the CloudWatch console
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.9 – Log groups in the CloudWatch console
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click on the latest stream; the log will look as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The logs sample is shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.10 – The logs in the CloudWatch console'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16735_02_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.10 – The logs in the CloudWatch console
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It will create output in MP3 format, as shown in the following screenshot.
    Download and listen to it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.11 – The output file that was created in the S3 bucket'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – The output file that was created in the S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The most scalable and cost-effective way for your mobile apps or web apps is
    to generate an AWS pre-signed URL for S3 buckets and provide it to your users.
    These S3 Put events asynchronously invoke downstream AI workflows to generate
    results and send a response to the end users. Many users can be served at the
    same time through this approach, and it may increase performance and throughput.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to implement text to speech. In the next section,
    we will learn about Amazon Transcribe, a speech-to-text AI service.
  prefs: []
  type: TYPE_NORMAL
- en: Speech to text with Amazon Transcribe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we learned about text to speech. In this section,
    we will learn about speech to text and the service that provides this: **Amazon
    Transcribe**. It is an automatic speech recognition service that uses pre-trained
    deep learning models, which means that we don''t have to train on petabytes of
    data to produce a model; Amazon does this for us. We just have to use the APIs
    that are available to transcribe audio files or video files; it supports a number
    of different languages and custom vocabulary too. Accuracy is the key and through
    custom vocabulary, you can enhance it based on the desired domain or industry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Block diagram of Amazon Transcribe''s input and output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 – Block diagram of Amazon Transcribe's input and output
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Transcribe include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Real-time audio streaming and transcription.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcripting pre-recorded audio files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable text searching from a media file by combining AWS Elasticsearch and Amazon
    Transcribe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing sentiment analysis on recorded audio files for voice helpdesk (contact
    center analytics).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Channel identification separation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll explore the benefits of Amazon Transcribe.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Transcribe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at some of the benefits of using Amazon Transcribe:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Content Redaction*: Customer privacy can be ensured by instructing Amazon
    Transcribe to identify and redact `VocabularyFilterName` and `VocabularyFilterMethod`,
    which are provided by the `StratTranscriptionJob` operation. For example, in financial
    organizations, this can be used to redact a caller''s details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Language Identification*: It can automatically identify the most used language
    in an audio file and generate transcriptions. If you have several audio files,
    then this service would help you classify them by language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Streaming Transcription*: You can send recorded audio files or live audio
    streams to Amazon Transcribe and output a stream of text in real time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Custom Vocabulary or Customized Transcription*: You can use your custom vocabulary
    list as per your custom needs to generate accurate transcriptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*TimeStamp Generation*: If you want to build or add subtitles for your videos,
    then Amazon Transcribe can return the timestamp for each word or phrase from the
    audio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost Effectiveness*: Being a managed service, there is no infrastructure cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's get hands-on with Amazon Transcribe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Transcribe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will build a pipeline where we can integrate AWS Lambda
    with Amazon Transcribe to read an audio file stored in a folder in an S3 bucket,
    and then store the output JSON file in another S3 bucket. We will monitor the
    task''s progress in CloudWatch logs too. We will use the `start_transcription_job`
    asynchronous function to start our job and we will constantly monitor the job
    through `get_transcription_job` until its status becomes `COMPLETED`. Let''s get
    started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create an IAM role called `transcribe-demo-role` for the Lambda function
    to execute. Ensure that it can read and write from/to S3, use Amazon Transcribe,
    and print the output in CloudWatch logs. Add the following policies to the IAM
    role:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3FullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonTranscribeFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will create a Lambda function called `transcribe-lambda` with our existing
    IAM role, `transcribe-demo-role`, and save it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please make sure you change the default timeout to a higher value in the`start_transcription_job`
    to start the task and monitor the same by using the `get_transcription_job` API.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Paste the code available at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/blob/master/Chapter-2/Amazon%20Transcribe%20Demo/lambda_function/lambda_function.py](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/blob/master/Chapter-2/Amazon%20Transcribe%20Demo/lambda_function/lambda_function.py)
    and click on **Deploy**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This should give us the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.13 – The Basic settings section of our created lambda function'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16735_02_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.13 – The Basic settings section of our created lambda function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we will be creating the an S3 bucket called `transcribe-demo-101` and
    a folder called `input`. Create an event by going to the `audio-event`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object create events`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`input/`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`transcribe-lambda`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the audio file in `.mp4`format to the `input` folder. This will trigger
    the Lambda function. As per the code, the output will be stored in the S3 bucket
    in JSON format, which you can then use to read the contents of the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to **CloudWatch > CloudWatch Logs > Log groups > aws/lambda/transcribe-lambda**.
    Choose the latest stream from the list. It will look as follows:![Figure 2.14
    – The logs in a Log Stream for the specified log groups in the CloudWatch console
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_014.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.14 – The logs in a Log Stream for the specified log groups in the CloudWatch
    console
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is saved to the S3 bucket in JSON format, as per the jobname mentioned
    in your code (you can use the the S3 `getObject` API to download and read it):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.15 – The output JSON file in an S3 bucket'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.15 – The output JSON file in an S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is a best practice not to overprovision your function's timeout settings.
    Always understand your code performance and set a function timeout accordingly.
    Overprovisioning a function timeout results in Lambda functions running longer
    and it causes unexpected costs. If you're using asynchronous API calls in your
    Lambda function, then it's good to write them into SNS topics on success and trigger
    another Lambda function from that. If it needs human intervention, then it is
    suggested that you use AWS Step Functions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned and applied Amazon Transcribe to convert speech
    into text. In the next section, we will learn about one of the most powerful AWS
    AI services we can use to get the maximum amount of insight from our text data.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing natural language processing with Amazon Comprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This service helps you extract insights from unstructured text. Unstructured
    text information is growing exponentially. A few data source examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Customer engagement*: Call center, issue triage, customer surveys, and product
    reviews'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Business processes*: Customer/vendor emails, product support messages, and
    operation support feedback'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Records and research*: Whitepapers and medical records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*News and social media*: Social media analytics, brand trends, and correlated
    events'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the question is, what can we do with this data? How can we analyze it and
    extract any value out of it? The answer is Amazon Comprehend, which is used to
    get insights from your unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Comprehend include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Information management system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More accurate search system on organized topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis of users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support ticket classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language detection from a document and then translating it into English using
    Amazon Translate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a system to label unstructured clinical data to assist in research
    and analysis purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting topics from the saved audio files of company meetings or TV news
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll explore the benefits of Amazon Comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Comprehend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the advantages of using Comprehend can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16 – A block diagram showing Amazon Comprehend''s capabilities'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 – A block diagram showing Amazon Comprehend's capabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at these in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: It detects the language of the text and extract key phrases. Amazon Comprehend
    can be used for sentiment analysis and topic modeling too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Comprehend Medical can be used to extract medical information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You pay for what you use since this is a fully managed service; you do not have
    to pay for the infrastructure. You don't need to train, develop, and deploy your
    own model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The topic modeling service works by extracting up to 100 topics. A topic is
    a keyword bucket so that you can see what's in the actual corpus of documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's accurate, continuously trained, and easy to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll get hands-on with Amazon Comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Comprehend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will build a pipeline where we can integrate AWS Lambda
    with Amazon Rekognition and Amazon Comprehend. We will then read an image file
    stored in an S3 bucket and detect the language of the text that''s been extracted
    from the image. We will also use CloudWatch to print out the output. The following
    is a diagram of our use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Architecture diagram of the required use case'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.17 – Architecture diagram of the required use case
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by creating an IAM Role:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the IAM console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Roles** from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Lambda** as the trusted entity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following managed `policies`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3ReadOnlyAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonRekognitionFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ComprehendFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the role as `language-detection-from-image-role`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's create the Lambda function. Navigate to **Lambda > Functions > Create
    Function**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the function `language-detection-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the runtime to `Python 3.6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use our existing role; that is, `language-detection-from-image-role`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the code from [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Transcribe%20Demo/lambda_function](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Transcribe%20Demo/lambda_function),
    paste it into the function, and click `detect_text` API from Amazon Rekognition
    to detect text from an image and the `batch_detect_dominant_language` API from
    Amazon Comprehend to detect the language of the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, go to your AWS S3 console and create a bucket called *language-detection-image*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder called `input-image` (in this example, we will only upload `.jpg`
    files).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to **Properties > Events> Add notification**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill in the required fields in the `image-upload-event`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object create events`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`input-image/`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.jpg`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`language-detection-from-image`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `sign-image.jpg` image in the folder. (This file is available in
    this book's GitHub repository at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Comprehend%20Demo/input_image](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Comprehend%20Demo/input_image)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This file upload will trigger the lambda function. You can monitor the logs
    from **CloudWatch> CloudWatch Logs> Log groups> /aws/lambda/language-detection-from-image**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the streams and select the latest one. The detected language is printed
    in the logs, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.18 – The logs in CloudWatch for verifying the output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.18 – The logs in CloudWatch for verifying the output
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'It is suggested that you use batch operations such as `BatchDetectSentiment`or
    `BatchDetectDominantLanguage` in your production environment. This is because
    single API operations can cause API-level throttling. More details are available
    here: [https://docs.aws.amazon.com/comprehend/latest/dg/functionality.html](https://docs.aws.amazon.com/comprehend/latest/dg/functionality.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to use Amazon Comprehend to detect the language
    of texts. The text is extracted into our Lambda function using Amazon Rekognition.
    In the next section, we will learn about translating the same text into English
    via Amazon Translate.
  prefs: []
  type: TYPE_NORMAL
- en: Translating documents with Amazon Translate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, people prefer to communicate in their own language, even on
    digital platforms. Amazon Translate is a text translation service. We can provide
    documents or strings of text in various languages and get it back in a different
    language. It uses pre-trained deep learning techniques, so we shouldn't be worried
    about the models, nor how they are managed. We can make API requests and get the
    results back.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Translate include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: If there's an organization-wide requirement to prepare documents in different
    languages, then Translate is the solution for converting one language into many.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Online chat applications can be translated in real time to provide a better
    customer experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To localize website content faster and more affordably into more languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis can be applied to different languages once they have been
    translated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To provide non-English language support for a news publishing website.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll explore the benefits of Amazon Translate.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Translate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the benefits of using Amazon Translate include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses neural machine translation, which mimics the way the human brain works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need to maintain your resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Produces high-quality results and maintains their consistency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can customize your brand names and model names, and any other unique terms
    get translated using the Custom Terminology Feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be easily integrated with applications through APIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Translate scales itself when you need it to do more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will get hands-on with Amazon Translate.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Translate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will build a product by integrating AWS Lambda with Amazon
    Rekognition, Amazon Comprehend, and Amazon Translate to read an image file stored
    in an S3 bucket. Then, we will detect the language of the text that''s been extracted
    from the image so that we can translate it into English. We will also use CloudWatch
    to print the translated output. The following is a diagram of our use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Architecture diagram of the required use case'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.19 – Architecture diagram of the required use case
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating an IAM role:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the IAM console page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Roles** from the left-hand menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create role**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Lambda** as the trusted entity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following managed policies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AmazonS3ReadOnlyAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonRekognitionFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ComprehendFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TranslateFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the role as `language-translation-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next immediate step is to create a Lambda function. Navigate to **Lambda
    > Functions > Create Function**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the function `language-detection-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the runtime to `Python 3.6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use an existing role; that is, `language-detection-from-image-role`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paste the code available at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/blob/master/Chapter-2/Amazon%20Translate%20Demo/lambda_function/lambda_function.py](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/blob/master/Chapter-2/Amazon%20Translate%20Demo/lambda_function/lambda_function.py)
    and click `translate_text` API to translate the input text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to create a bucket called `language-translation-from-image`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a folder named `image`. Then, navigate to **Properties > Events> Add
    notification**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fill in the required fields, as shown here, and click on `.jpg` as the suffix;
    otherwise, it will trigger the lambda function for any object creation process):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`All object create events`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image/`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.jpg`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Lambda Function`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`language-translation-from-image`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Navigate to `sign-image.jpg` image into the folder. This file is available
    in this book''s GitHub repository: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Translate%20Demo/input_image](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Translate%20Demo/input_image).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uploading this image will trigger the lambda function. You can monitor the logs
    by going to **CloudWatch> CloudWatch Logs> Log groups> /aws/lambda/language-translation-from-image**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the streams and select the latest one. It will look as follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.20 – The logs in CloudWatch for verifying the output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.20 – The logs in CloudWatch for verifying the output
  prefs: []
  type: TYPE_NORMAL
- en: 'The translation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For production use cases, it is recommended to use AWS Lambda with AWS Step
    Function if you have dependent services or a chain of services.
  prefs: []
  type: TYPE_NORMAL
- en: Using the same S3 bucket to store input and output objects is not recommended.
    Output object creation in the same bucket may trigger recursive Lambda invocation.
    If you're using same bucket, then we recommend that you use a prefix and suffix
    to trigger events. Similarly, we recommend using a prefixto store output objects.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to combine multiple services and chain their
    output to achieve a particular use case outcome. We learned how to integrate Amazon
    Rekognition to detect text in an image. The language can then be detected by using
    Amazon Comprehend. Then, we used the same input and translated it into English
    with the help of Amazon Translate. The translated output was then printed on CloudWatch
    logs for verification. In the next section, we will learn about Amazon Textract,
    which we can use to extract text from a document.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting text from documents with Amazon Textract
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Manually extracting information from documents is slow, expensive, and prone
    to errors. Traditional optical character recognition software needs a lot of customization,
    and it will still give erroneous output. To avoid such manual processes and errors,
    you should use **Amazon Textract**. Generally, we convert the documents into images
    in order to detect bounding boxes around the texts in images. We then apply character
    recognition technique to read the text from it. Textract does all this for you,
    and also extracts text, tables, forms, and other data for you with minimal effort.
    If you get low-confidence results from Amazon Textract, then Amazon A2I is the
    best solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Textract reduces the manual effort of extracting text from millions of scanned
    document pages. Once the information has been captured, actions can be taken on
    the text, such as storing it in different data stores, analyzing sentiments, or
    searching for keywords. The following diagram shows how Amazon Textract works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.21 – Block diagram representation of Amazon Textract and how it
    stores its output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.21 – Block diagram representation of Amazon Textract and how it stores
    its output
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Textract include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Documenting processing workflows to extract tables or forms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating search indexes from documents using Amazon Elasticsearch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redacting personally identifiable information in a workflow; Textract identifies
    data types and form labels automatically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll explore the benefits of Amazon Textract.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Textract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several reasons to use Textract, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero infrastructure cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully managed service (reduced development and management overhead).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helps you extract both structured and unstructured data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handwritten reviews can be analyzed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Textract performs better than OCR apps, which use flat bag of words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll get hands-on with Amazon Textract.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Textract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will use the Amazon Textract API to read an image file
    from our S3 bucket and print the "FORM" details on Cloudwatch. The same can be
    stored in S3 in your desired format for further use or can be stored in DynamoDB
    as a key-value pair. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create an IAM role called `textract-use-case-role` with the following
    policies. This will allow the Lambda function to execute so that it can read from
    S3, use Amazon Textract, and print the output in CloudWatch logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CloudWatchFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonTextractFullAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonS3ReadOnlyAccess`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's create an S3 bucket called `textract-document-analysis` and upload the
    `receipt.png` image file. This will be used to contain the FORM details that will
    be extracted. The image file is available here at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Textract%20Demo/input_doc](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-2/Amazon%20Textract%20Demo/input_doc):![Figure
    2.22 – An S3 bucket with an image (.png) file uploaded to the input folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_022.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.22 – An S3 bucket with an image (.png) file uploaded to the input folder
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next step is to create a Lambda function called `read-scanned-doc`, as shown
    in the following screenshot, with an existing execution role called `textract-use-case-role`:![Figure
    2.23 – The AWS Lambda Create function dialog
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_023.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.23 – The AWS Lambda Create function dialog
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the function has been created, paste the following code and deploy it.
    Scroll down to `analyze_document` API from Amazon Textract to get the `Table and
    Form` details via the `FeatureTypes` parameter of the API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Unlike the previous examples, we will create a test configuration to run our
    code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the dropdown left of the **Test** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Configure test events** and choose **Create new test event**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Amazon S3 Put** from the **Event template** dropdown.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the JSON body, change the highlighted values as per our bucket name and key,
    as shown here:![Figure 2.24 – The Event template for testing the lambda function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_024.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.24 – The Event template for testing the lambda function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the `TextractDemo`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your test configuration (`TextractDemo`) and click on **Test**:![Figure
    2.25 – Selecting the test configuration before running your test
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_025.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.25 – Selecting the test configuration before running your test
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This will trigger the lambda function. You can monitor the logs from **CloudWatch>
    CloudWatch Logs> Log groups> /aws/lambda/ read-scanned-doc**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the streams and select the latest one. It will look as follows; the
    key-value pairs can be seen in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.26 – The logs in CloudWatch for verifying the output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.26 – The logs in CloudWatch for verifying the output
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The most scalable and cost-effective way to generate S3 Put events for asynchronously
    invocating downstream AI workflows via Lambda is to generate an AWS Pre-Signed
    URL, and then provide it to your mobile or web application users. Many users can
    be served at the same time via this approach, and it may increase performance
    and throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the same region for your AWS AI services and S3 bucket may improve
    performance and reduce network latency. AWS VPC endpoints can leverage enhanced
    security without using the public internet. You can store the AWS AI results in
    an AWS S3 bucket and encrypt the rest to attain better security.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to extract text from a scanned document and
    print the form data out of it. Unlike the other sections, we used the testing
    feature of a lambda function by creating a test configuration that includes an
    event template. In the next section, we will learn about creating a chatbot for
    organizations and learn how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating chatbots on Amazon Lex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the features that are available in Alexa are powered by **Amazon Lex**.
    You can easily build a chatbot using Amazon Lex. It uses natural language understanding
    and automatic speech recognition behind the scenes. Through SLU, Amazon Lex takes
    natural language speech and text input, understands the intent, and fulfills the
    intent of the user. An Amazon Lex bot can be created either from the console or
    via APIs. Its basic requirements are shown in the upcoming diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common uses of Amazon Lex include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Apps that both listen and take input as text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversational AI products to provide a better customer and sales experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom business bots for assistance through AWS Lambda functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voice assistants for your call center, which can speak to a user, schedule a
    meeting, or request details of your account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By integrating with Amazon Cognito, you can control user management, authentication,
    and sync across all your devices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will explore the benefits of Amazon Lex.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the benefits of Amazon Lex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some reasons for using Lex include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots can be directly built and tested from the AWS management console. These
    chatbots can be easily integrated into Facebook Messenger, Slack, and Twilio SMS
    via its rich formatting capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversation logs can be stored in Amazon CloudWatch for further analysis. You
    can use them to monitor your bot and derive insights to improve your user experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Lex can be integrated into other AWS services such as Amazon Cognito,
    AWS Lambda, Amazon DynamoDB, Amazon CloudWatch, and AWS Mobile Hub to leverage
    application security, monitoring, user authentication, business logic, storage,
    and mobile app development in AWS platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Lex chatbots can be integrated into your custom web applications too.
    You just need to build a chatbot widget and integrate it into your UI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we'll get hands-on with Amazon Lex.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon Lex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Log into [https://console.aws.amazon.com/lex/](https://console.aws.amazon.com/lex/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Get Started** and select **Custom bot**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill in the following details and click on **Create**:![Figure 2.27 – The Create
    dialog of Amazon Lex
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_027.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.27 – The Create dialog of Amazon Lex
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on **Create Intent**. A dialog will appear. Select **Create Intent**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the new intent `MovieIntent` and click on **Add**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the `movie_type`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Slot type: `AMAZON.Genre`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prompt: `Which movie do you like?`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the `movie_type` is my variable:![Figure 2.28 – The Sample utterances
    section
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_028.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.28 – The Sample utterances section
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scroll down to the **Response** section to add a message:![Figure 2.29 – The
    Response section of Amazon Lex
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_029.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.29 – The Response section of Amazon Lex
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scroll down to **Save Intent** and click on **Build**. Upon successfully building
    the prompt, the following success message will appear:![Figure 2.30 – The Response
    section of Amazon Lex
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_02_030.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.30 – The Response section of Amazon Lex
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, you can test your bot, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.31 – The Test bot dialog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_02_031.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.31 – The Test bot dialog
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Not all Amazon Polly features are available within Alexa – particularly the
    Amazon Polly SSML features – which makes Amazon Polly and Alexa different.
  prefs: []
  type: TYPE_NORMAL
- en: That concludes this chapter's introduction to the various AWS application services
    that are available.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about a few of the AWS AI services that can be used
    to solve various problems. We used the Amazon Rekognition service, which detects
    objects and faces (including celebrity faces), and can also extract text from
    images. For text to speech, we used Amazon Polly, while for speech to text, we
    used Amazon Transcribe. Toward the end of this chapter, we built a chatbot in
    Amazon Lex.
  prefs: []
  type: TYPE_NORMAL
- en: For language detection and translation in an image, we used Amazon Rekognition,
    Amazon Comprehend, and Amazon Translate. We learned how to combine all of them
    into one Lambda function to solve our problem.
  prefs: []
  type: TYPE_NORMAL
- en: For the certification exam, you don't need to remember all the APIs we used
    in this chapter. There may be questions on a few of the best practices that we
    learned or on the names of services that solve a specific problem. It is always
    good to practice using these AWS AI services as it will enhance your architecting
    skills.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about data preparation and transformation,
    which is the most important aspect of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using pre-defined logic and rules to make product recommendations to online
    shoppers is an example of machine learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. TRUE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. FALSE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which level of the ML stack helps you build custom ML models without managing
    infrastructure?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Top level (the AI services)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Middle level (Amazon SageMaker)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Bottom level (ML frameworks and infrastructure)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. Your own infrastructure and code level
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following can you do with Amazon Textract?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Detect key-value pairs in documents
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Build a custom ML model for text extraction
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Send text extraction with low confidence scores for human review
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. Translate the detected text into English
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With Amazon Comprehend, a new model can be trained to help you extract custom
    entities from text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. FALSE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. TRUE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following is an example of the type of data Amazon Comprehend is
    designed to analyze?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Social media posts
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Data in a table
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Log files
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. GPS data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For log files, we can use CloudWatch Log Insights.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When calling the `DetectKeyPhrases` API, which of the following is not returned
    by Amazon Comprehend?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. The key phrases
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. The count of each key phrase
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. The confidence level for each key phrase
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. The sentiment of each key phrase
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It has nothing to do with sentiment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You want to create a Lex bot that can help you order pizza. Why is it important
    to add slots as part of intent configuration?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. So you can customize your orders with different pizza sizes and toppings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. So you can account for different ways you might convey your intent to order
    pizza.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. So that a lambda function can be automatically set up for you to fulfill
    the intent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's say you're responsible for building a system that analyzes the sentiment
    of a customer chat. Which service should you integrate with Amazon Lex to do this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Amazon Transcribe
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Amazon Comprehend
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Amazon Translate
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. Amazon Textract
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In which situation would an Amazon Lex fallback intent help?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. When a user orders pizza but, due to background noise, the bot needs the
    user to repeat what they said.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. When a bot has to use a previous exchange with a user to pretend to understand
    an unclear message from that user.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. When a bot is asked a question that is not programmed to answer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fallback intent is meant for those inputs that a bot doesn't expect.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which three of the following options can Amazon Textract handle that traditional
    OCR methods are not able to?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. Extracting words and lines from documents
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. Extracting forms (key/values) from documents without using any templates
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. Handling non-textual content such as radio buttons and checkboxes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. Preserving the composition of data stored in tables
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following is a common use case for integrating Amazon Textract
    with Amazon A2I (human review)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. You want to identify form labels and values from an image or PDF document.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. You are extracting data from a document that requires review due to regulatory
    requirements or sensitive business decisions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. You have tables in your document, and you need to preserve the composition
    of the data stored in those tables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are looking to extract form or table data in a document. You need to do
    this synchronously because your use is latency-sensitive, such as mobile capture.
    What API should you use?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. AnalyzeDocument
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. DetectDocumentText
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c. StartDocumentAnalysis
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d. GetDocumentTextDetection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. B
  prefs: []
  type: TYPE_NORMAL
- en: 2\. B
  prefs: []
  type: TYPE_NORMAL
- en: 3\. A, C
  prefs: []
  type: TYPE_NORMAL
- en: 4\. B
  prefs: []
  type: TYPE_NORMAL
- en: 5\. A
  prefs: []
  type: TYPE_NORMAL
- en: 6\. D
  prefs: []
  type: TYPE_NORMAL
- en: 7\. A
  prefs: []
  type: TYPE_NORMAL
- en: 8\. B
  prefs: []
  type: TYPE_NORMAL
- en: 9\. C
  prefs: []
  type: TYPE_NORMAL
- en: 10\. B, C, D
  prefs: []
  type: TYPE_NORMAL
- en: 11\. B
  prefs: []
  type: TYPE_NORMAL
- en: 12\. A
  prefs: []
  type: TYPE_NORMAL
