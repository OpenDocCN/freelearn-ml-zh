<html><head></head><body>
<div id="_idContainer087">
<h1 class="chapter-number" id="_idParaDest-138"><a id="_idTextAnchor218"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-139"><a id="_idTextAnchor219"/><span class="koboSpan" id="kobo.2.1">Decreasing Bias and Achieving Fairness</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Fairness is an important topic when it comes to using machine learning across different industries, as we discussed in </span><a href="B16369_03.xhtml#_idTextAnchor119"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.5.1">, </span><em class="italic"><span class="koboSpan" id="kobo.6.1">Debugging toward Responsible AI</span></em><span class="koboSpan" id="kobo.7.1">. </span><span class="koboSpan" id="kobo.7.2">In this chapter, we will provide you with some widely used notions and definitions of fairness in machine learning settings, as well as how to use fairness and explainability Python libraries that are designed to not only help you in assessing fairness in your models but also improve them in </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">this regard.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">This chapter includes many figures and code examples to help you better understand these concepts and start benefiting from them in your projects. </span><span class="koboSpan" id="kobo.9.2">Note that one chapter is far from enough to make you an expert on the topic of fairness, but this chapter will provide you with the necessary knowledge and tools to start practicing this subject in your projects. </span><span class="koboSpan" id="kobo.9.3">You can learn more about this topic using more advanced resources dedicated to machine </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">learning fairness.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">We will cover the following topics in </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">Fairness in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">learning modeling</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Sources </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">of bias</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Using </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">explainability techniques</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Fairness assessment and improvement </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">in Python</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.21.1">By the end of this chapter, you will have learned about some technical details and Python tools that you can use to assess fairness and reduce biases in your models. </span><span class="koboSpan" id="kobo.21.2">You will also learn how to benefit from the machine learning explainability techniques you learned about in </span><a href="B16369_06.xhtml#_idTextAnchor201"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.22.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.23.1">, </span><em class="italic"><span class="koboSpan" id="kobo.24.1">Interpretability and Explainability in Machine </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.25.1">Learning Modeling</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">.</span></span></p>
<h1 id="_idParaDest-140"><a id="_idTextAnchor220"/><span class="koboSpan" id="kobo.27.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.28.1">The following requirements should be considered for this chapter as they will help you better understand the concepts, use them in your projects, and practice with the </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">provided code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.30.1">Python </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">library requirements:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.32.1">sklearn</span></strong><span class="koboSpan" id="kobo.33.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">1.2.2</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.35.1">numpy</span></strong><span class="koboSpan" id="kobo.36.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">1.22.4</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.38.1">pytest</span></strong><span class="koboSpan" id="kobo.39.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">7.2.2</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.41.1">shap</span></strong><span class="koboSpan" id="kobo.42.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">0.41.0</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.44.1">aif360</span></strong><span class="koboSpan" id="kobo.45.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">0.5.0</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.47.1">fairlearn</span></strong><span class="koboSpan" id="kobo.48.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">0.8.0</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.50.1">Basic knowledge of the machine learning explainability concepts discussed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">previous chapter</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.52.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">at </span></span><a href="https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter07"><span class="No-Break"><span class="koboSpan" id="kobo.54.1">https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter07</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.55.1">.</span></span></p>
<h1 id="_idParaDest-141"><a id="_idTextAnchor221"/><span class="koboSpan" id="kobo.56.1">Fairness in machine learning modeling</span></h1>
<p><span class="koboSpan" id="kobo.57.1">To assess fairness, we need to </span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.58.1">have specific considerations in mind and then use proper metrics to quantify fairness in our models. </span><em class="italic"><span class="koboSpan" id="kobo.59.1">Table 7.1</span></em><span class="koboSpan" id="kobo.60.1"> provides you with some of the considerations, definitions, and approaches to </span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.61.1">either evaluate or achieve fairness in machine</span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.62.1"> learning modeling. </span><span class="koboSpan" id="kobo.62.2">We will go through the mathematical definitions of </span><strong class="bold"><span class="koboSpan" id="kobo.63.1">demographic parity</span></strong><span class="koboSpan" id="kobo.64.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.65.1">equality of odds</span></strong><span class="koboSpan" id="kobo.66.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.67.1">equalized odds</span></strong><span class="koboSpan" id="kobo.68.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.69.1">equality of opportunity</span></strong><span class="koboSpan" id="kobo.70.1"> here as </span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.71.1">different group </span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.72.1">fairness </span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.73.1">definitions. </span><span class="koboSpan" id="kobo.73.2">Group fairness definitions ensure the fairness of groups of people with common attributes and characteristics instead </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">of individuals:</span></span></p>
<table class="No-Table-Style" id="table001-6">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.75.1">Topics in Machine </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.76.1">Learning Fairness</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.77.1">Description</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.78.1">Demographic parity</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.79.1">Ensures predictions are not dependent on a given sensitive attribute, such as ethnicity, sex, </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">or race</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.81.1">Equality </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">of odds</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.83.1">Ensures the independence of predictions to a given sensitive attribute, such as ethnicity, sex, or race given a </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">true output</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.85.1">Equality </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">of opportunity</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.87.1">Ensures the equality of opportunities provided for individuals or groups </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">of people</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.89.1">Individual fairness</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.90.1">Ensures fairness for individuals rather than groups of people with </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">common attributes</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.92.1">Consistency</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.93.1">Provides consistency in decision-making not only between similar data points or users but also </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">across time</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.95.1">Fairness </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">through unawareness</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.97.1">Achieves fairness if you’re unaware of sensitive attributes in </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">decision making</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.99.1">Fairness </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">through transparency</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.101.1">Improves fairness through transparency and trust building </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">through explainability</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.103.1">Table 7.1 – Some important topics and considerations in fairness in machine learning and artificial intelligence</span></p>
<p><span class="koboSpan" id="kobo.104.1">Demographic parity is a</span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.105.1"> group fairness definition that ensures that a model’s predictions are not dependent on a given sensitive attribute, such as ethnicity or sex. </span><span class="koboSpan" id="kobo.105.2">Mathematically, we can define it as the equality of probability of predicting a class, such as </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.106.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.107.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.108.1">i</span></span><span class="koboSpan" id="kobo.109.1">, for different groups of a given attribute, </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">as follows:</span></span></p>
<p> <span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.111.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.112.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.113.1">C</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.114.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.115.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.116.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.117.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.118.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.119.1">G</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.120.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.121.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.122.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.123.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.124.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.125.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.126.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.127.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.128.1">C</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.129.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.130.1">C</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.131.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.132.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.133.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.134.1">G</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.135.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.136.1">g</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.137.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.138.1">2</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.139.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.140.1">To better understand the meaning of demographic parity, we can consider the following examples, which meet fairness according to </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">demographic parity:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.142.1">The same percentage of bail denial in each race group in COMPAS. </span><span class="koboSpan" id="kobo.142.2">We covered COMPAS in </span><a href="B16369_03.xhtml#_idTextAnchor119"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.143.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.144.1">, </span><em class="italic"><span class="koboSpan" id="kobo.145.1">Debugging toward </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.146.1">Responsible AI</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.148.1">The same acceptance rate for loan applications between men </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">and women.</span></span></li>
<li><span class="koboSpan" id="kobo.150.1">The same likelihood of hospitalization between poor and rich neighborhoods. </span><span class="koboSpan" id="kobo.150.2">We covered more about this problem in </span><a href="B16369_03.xhtml#_idTextAnchor119"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.151.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.152.1">, </span><em class="italic"><span class="koboSpan" id="kobo.153.1">Debugging toward </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.154.1">Responsible AI</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">.</span></span></li>
</ul>
<p><strong class="bold"><span class="koboSpan" id="kobo.156.1">Disparate impact ratio</span></strong><span class="koboSpan" id="kobo.157.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.158.1">DIR</span></strong><span class="koboSpan" id="kobo.159.1">) is a</span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.160.1"> metric that quantifies the deviation from equality based on </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">demographic parity:</span></span></p>
<p> <span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.162.1">D</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.163.1">I</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.164.1">R</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.165.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.166.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.167.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.168.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.169.1">C</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.170.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.171.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.172.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.173.1">G</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.174.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.175.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.176.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.177.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.178.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.179.1">  </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.180.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.181.1">____________</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.182.1">  </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.183.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.184.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.185.1">C</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.186.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.187.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.188.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.189.1">G</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.190.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.191.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.192.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.193.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.194.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.195.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.196.1">The DIR value’s range is </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.197.1">[</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.198.1">0</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.199.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.200.1">∞</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.201.1">)</span></span><span class="koboSpan" id="kobo.202.1">, where a value of 1 satisfies demographic parity while deviation toward higher or lower values translates to deviation from fairness based on this definition. </span><span class="koboSpan" id="kobo.202.2">DIR values of greater and less than 1 are referred to as negative and positive bias, respectively, considering the group we use in </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">the numerator.</span></span></p>
<p><span class="koboSpan" id="kobo.204.1">Despite the importance of demographic parity in fairness, it has its limitations. </span><span class="koboSpan" id="kobo.204.2">For example, in the case of </span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.205.1">DIR in the data itself (that is, the difference in class prevalence between different groups), a perfect model will not meet demographic parity criteria. </span><span class="koboSpan" id="kobo.205.2">Also, it doesn’t reflect the quality of predictions for each group. </span><span class="koboSpan" id="kobo.205.3">Other definitions help us improve our fairness assessment. </span><span class="koboSpan" id="kobo.205.4">Equality of odds or equalized odds is one such definition. </span><span class="koboSpan" id="kobo.205.5">Equalized odds is satisfied when a given prediction is independent of the group of a given sensitive attribute and the </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">real output:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.207.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.208.1">                      </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.209.1">                       </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.210.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.211.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.212.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.213.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.214.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.215.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.216.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.217.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.218.1">y</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.219.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.220.1">G</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.221.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.222.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.223.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.224.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.225.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.226.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.227.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.228.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.229.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.230.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.231.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.232.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.233.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.234.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.235.1">y</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.236.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.237.1">G</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.238.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.239.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.240.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.241.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.242.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.243.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.244.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.245.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.246.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.247.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.248.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.249.1">y</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.250.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.251.1">|</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.252.1">y</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.253.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.254.1">The definition of equality of opportunity</span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.255.1"> is very similar to equalized odds, which assesses the independence of a prediction concerning groups for a given real output. </span><span class="koboSpan" id="kobo.255.2">But equality of opportunity focuses on a particular label of true values. </span><span class="koboSpan" id="kobo.255.3">Usually, the positive class is considered the target class and is representative of providing an opportunity for individuals, such as admission to school or having a high salary. </span><span class="koboSpan" id="kobo.255.4">Here is a formula for equality </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">of opportunity:</span></span></p>
<p> <span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.257.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.258.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.259.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.260.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.261.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.262.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.263.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.264.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.265.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.266.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.267.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.268.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.269.1">G</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.270.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.271.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.272.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.273.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.274.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.275.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.276.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.277.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.278.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.279.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.280.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.281.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.282.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.283.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.284.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.285.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.286.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.287.1">,</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.288.1">G</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.289.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.290.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.291.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.292.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.293.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.294.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.295.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.296.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.297.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.298.1">ˆ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.299.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.300.1">y</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.301.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.302.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.303.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.304.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.305.1">1</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.306.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.307.1">According to these </span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.308.1">notions of fairness, each could give you a different result. </span><span class="koboSpan" id="kobo.308.2">You need to consider the differences between different notions so that you don’t generalize fairness based on one definition </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">or anot</span><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.310.1">her.</span></span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor223"/><span class="koboSpan" id="kobo.311.1">Proxies for sensitive variables</span></h2>
<p><span class="koboSpan" id="kobo.312.1">One of the challenges in</span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.313.1"> assessing fairness in machine learning models is the </span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.314.1">existence of proxies for sensitive attributes such as sex and race. </span><span class="koboSpan" id="kobo.314.2">These proxies could be among the major contributors in generating model outputs and could result in bias in our models to specific groups. </span><span class="koboSpan" id="kobo.314.3">However, we cannot simply remove them as this could have a significant effect on performance. </span><em class="italic"><span class="koboSpan" id="kobo.315.1">Table 7.2</span></em><span class="koboSpan" id="kobo.316.1"> provides some examples of these proxies for different </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">sensitive attributes:</span></span></p>
<table class="No-Table-Style" id="table002-3">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.318.1">Sensitive Variable</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.319.1">Example Proxies</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.320.1">Sex</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.321.1">Level of education, salary and income (in some countries), occupation, history of a felony charge, keywords in user-generated content (for example, in a resume or social media), being a </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">university faculty</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.323.1">Race</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.324.1">History of a felony charge, keywords in user-generated content (for example, in a resume or social media), ZIP or </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">postal code</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.326.1">Disabilities</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.327.1">Speed of walking, eye movement, </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">body posture</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.329.1">Marital status</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.330.1">Level of education, salary and income (in some countries), and house size and number </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">of bedrooms</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.332.1">Age</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.333.1">Posture and keywords in user-generated content (for example, in a resume or </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">social media)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor224"/><span class="koboSpan" id="kobo.335.1">Table 7.2 – Examples of proxies for some of the important sensitive variables, in the context of fairness (Caton and Haas, 2020)</span></p>
<p><span class="koboSpan" id="kobo.336.1">Now that you’ve learned </span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.337.1">about the </span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.338.1">importance of fairness and some important definitions under this topic, let’s review some of the possible sources of bias that play against your goal of achieving fairness in </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">your models.</span></span></p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor225"/><span class="koboSpan" id="kobo.340.1">Sources of bias</span></h1>
<p><span class="koboSpan" id="kobo.341.1">There are different sources of bias</span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.342.1"> in a machine learning life cycle. </span><span class="koboSpan" id="kobo.342.2">Bias could exist in the collected data, introduced in the data subsampling, cleaning and filtering, or model training and selection. </span><span class="koboSpan" id="kobo.342.3">Here, we will review examples of such sources to help you better understand how to avoid or detect such biases throughout the life cycle of a machine </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">learning projec</span><a id="_idTextAnchor226"/><span class="koboSpan" id="kobo.344.1">t.</span></span></p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor227"/><span class="koboSpan" id="kobo.345.1">Biases introduced in data generation and collection</span></h2>
<p><span class="koboSpan" id="kobo.346.1">The data that we feed into our </span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.347.1">models could be biased by default, even before the modeling starts. </span><span class="koboSpan" id="kobo.347.2">The first source of such biases we want to review here is the issue of dataset size. </span><span class="koboSpan" id="kobo.347.3">Consider a dataset as a sample of a bigger population – for example, a survey of 100 students or the loan application information of 200 customers of a bank. </span><span class="koboSpan" id="kobo.347.4">The small size of these datasets could increase the chance of bias. </span><span class="koboSpan" id="kobo.347.5">Let’s simulate this with a simple random data generation. </span><span class="koboSpan" id="kobo.347.6">We will write a function that generates two vectors of random binary values using </span><strong class="source-inline"><span class="koboSpan" id="kobo.348.1">np.random.randint()</span></strong><span class="koboSpan" id="kobo.349.1"> and then calculates </span><em class="italic"><span class="koboSpan" id="kobo.350.1">DIR</span></em><span class="koboSpan" id="kobo.351.1"> between the two groups of 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">and 1:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.353.1">
np.random.seed(42)def disparate_impact_randomsample(sample_size,
    sampling_num = 100): disparate_impact = []
    for sam_iter in range(0, sampling_num):
        # generating random array of 0 and 1 as two groups with different priviledges (e.g. </span><span class="koboSpan" id="kobo.353.2">male versus female)
        group_category = np.random.randint(2,
            size=sample_size)
    # generating random array of 0 and 1 as the output labels (e.g. </span><span class="koboSpan" id="kobo.353.3">accepted for loan or not)
    output_labels = np.random.randint(2, size=sample_size)
    group0_label1 = [iter for iter in range(0, len(
        group_category)) if group_category[iter] == 0 
        and output_labels[iter] == 1]
    group1_label1 = [iter for iter in range(0, len(
        group_category)) if group_category[iter] == 1 and 
        output_labels[iter] == 1]
    # calculating disparate impact 
    disparate_impact.append(len
        (group1_label1)/len(group0_label1))
    return disparate_impact</span></pre>
<p><span class="koboSpan" id="kobo.354.1">Now, let’s use this function to calculate DIR for 1,000 different groups of different sizes, including </span><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">50</span></strong><span class="koboSpan" id="kobo.356.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.357.1">100</span></strong><span class="koboSpan" id="kobo.358.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.359.1">1000</span></strong><span class="koboSpan" id="kobo.360.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">10000</span></strong><span class="koboSpan" id="kobo.362.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.363.1">1000000</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.364.1">data points:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.365.1">
sample_size_list = [50, 100, 1000, 10000, 1000000]disparate_impact_list = []
for sample_size_iter in sample_size_list:
    disparate_impact_list.append(
        disparate_impact_randomsample(
            sample_size = sample_size_iter,
            sampling_num = 1000))</span></pre>
<p><span class="koboSpan" id="kobo.366.1">The following</span><a id="_idIndexMarker464"/><span class="koboSpan" id="kobo.367.1"> boxplots show the distributions of </span><em class="italic"><span class="koboSpan" id="kobo.368.1">DIR</span></em><span class="koboSpan" id="kobo.369.1"> across different sample sizes. </span><span class="koboSpan" id="kobo.369.2">You can see that lower sample sizes have wider distributions covering very low or high </span><em class="italic"><span class="koboSpan" id="kobo.370.1">DIR</span></em><span class="koboSpan" id="kobo.371.1"> values, distant from the ideal case </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">of 1:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<span class="koboSpan" id="kobo.373.1"><img alt="Figure 7.1 – Distributions of DIR across different sampling sizes" src="image/B16369_07_01..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.374.1">Figure 7.1 – Distributions of DIR across different sampling sizes</span></p>
<p><span class="koboSpan" id="kobo.375.1">We can also </span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.376.1">calculate the percentage of sampled groups of different sizes that don’t pass a specific threshold, such as &gt;=0.8 and &lt;=1.2. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.377.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.378.1">.2</span></em><span class="koboSpan" id="kobo.379.1"> shows that higher dataset sizes result in a lower chance of having datasets that have positive or negative bias given a </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">sensitive attribute:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<span class="koboSpan" id="kobo.381.1"><img alt="Figure 7.2 – Percentage of sets of samples that don’t pass DIR thresholds" src="image/B16369_07_02..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.382.1">Figure 7.2 – Percentage of sets of samples that don’t pass DIR thresholds</span></p>
<p><span class="koboSpan" id="kobo.383.1">The source of existing bias in datasets might not just be an artifact of a small sample size. </span><span class="koboSpan" id="kobo.383.2">For example, if you were to train a model to predict if an individual will end up in STEM, which is an acronym for fields of science, technology, engineering, and math, then you must consider the reality of the existence of it being imbalanced toward men over women in the corresponding data in fields, such as engineering, even up until recently (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.384.1">Figure 7</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.385.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<span class="koboSpan" id="kobo.387.1"><img alt="Figure 7.3 – Percentage of women in STEM jobs between 1970 and 2019" src="image/B16369_07_03..jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.388.1">Figure 7.3 – Percentage of women in STEM jobs between 1970 and 2019</span></p>
<p><span class="koboSpan" id="kobo.389.1">Having less</span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.390.1"> than 20% of engineers being women over the years, because of their lower interest, bias in hiring processes, or stereotypes in society, has resulted in bias in the data on workers in this field. </span><span class="koboSpan" id="kobo.390.2">If this is not rectified with fairness in your data processing and modeling tasks, it could result in predicting a higher chance for men getting into STEM compared to women, despite their talents, knowledge, </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">and experience.</span></span></p>
<p><span class="koboSpan" id="kobo.392.1">There is another category of intrinsic bias in the data, although it needs to be considered when developing machine learning models. </span><span class="koboSpan" id="kobo.392.2">For example, less than 1% of breast cancer cases occur in men (</span><a href="https://www.breastcancer.org"><span class="koboSpan" id="kobo.393.1">www.breastcancer.org</span></a><span class="koboSpan" id="kobo.394.1">). </span><span class="koboSpan" id="kobo.394.2">This prevalence difference between men and women is not caused by any sort of bias in data generation or collection or biases that have existed in societies. </span><span class="koboSpan" id="kobo.394.3">It is the natural difference between the prevalence of breast cancer occurrence between men and women. </span><span class="koboSpan" id="kobo.394.4">But if you were responsible for developing a machine learning model to diagnose breast cancer, there could be a high chance of false negatives (that is, not diagnosing breast cancer) in men. </span><span class="koboSpan" id="kobo.394.5">If your model doesn’t consider the high prevalence of women over men, it will not be a fair model in breast cancer diagnosis for men. </span><span class="koboSpan" id="kobo.394.6">This was a high-level example to clarify this kind of bias. </span><span class="koboSpan" id="kobo.394.7">There are many other considerations in building a machine learning tool for </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">cancer diagnos</span><a id="_idTextAnchor228"/><span class="koboSpan" id="kobo.396.1">is.</span></span></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor229"/><span class="koboSpan" id="kobo.397.1">Bias in model training and testing</span></h2>
<p><span class="koboSpan" id="kobo.398.1">If a dataset </span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.399.1">has a high imbalance toward men or women, different ethnicities, or any sort of bias considering different sensitive attributes, our models could have biases due to the way the corresponding machine learning algorithms use the features in predicting the outcome of data points. </span><span class="koboSpan" id="kobo.399.2">For example, our models could be highly reliant on sensitive attributes or their proxies (</span><em class="italic"><span class="koboSpan" id="kobo.400.1">Table 7.2</span></em><span class="koboSpan" id="kobo.401.1">). </span><span class="koboSpan" id="kobo.401.2">This is an important consideration in model selection. </span><span class="koboSpan" id="kobo.401.3">In the model selection process, we need to select a model among the trained models, with different methods or hyperparameters of the same method, to be pushed for further testing or production. </span><span class="koboSpan" id="kobo.401.4">If we base our decision-making solely on performance, then we might select a model that is not fair. </span><span class="koboSpan" id="kobo.401.5">We need to consider both fairness and performance in our model selection process if we have sensitive attributes and those models will directly or indirectly affect individuals of </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">different gro</span><a id="_idTextAnchor230"/><span class="koboSpan" id="kobo.403.1">ups.</span></span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor231"/><span class="koboSpan" id="kobo.404.1">Bias in production</span></h2>
<p><span class="koboSpan" id="kobo.405.1">Bias and unfairness in </span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.406.1">production could happen because of differences in the distribution of data between training, testing, and production. </span><span class="koboSpan" id="kobo.406.2">For example, women and men could have some differences in the production stage that don’t exist in your training and test data. </span><span class="koboSpan" id="kobo.406.3">This situation could result in biases in production that might not have been detectable in previous stages of the life cycle. </span><span class="koboSpan" id="kobo.406.4">We will talk about such kinds of differences in more detail in </span><a href="B16369_11.xhtml#_idTextAnchor300"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.407.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.408.1">, </span><em class="italic"><span class="koboSpan" id="kobo.409.1">Avoiding and Detecting Data and </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.410.1">Concept Drifts</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.412.1">The next step in this chapter is to start practicing with techniques and Python libraries that help you in detecting and eliminating model biases. </span><span class="koboSpan" id="kobo.412.2">First, will practice using the explainability techniques that were introduced in </span><a href="B16369_06.xhtml#_idTextAnchor201"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.413.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.414.1">, </span><em class="italic"><span class="koboSpan" id="kobo.415.1">Interpretability and Explainability in Machine </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.416.1">Learning Mode</span><a id="_idTextAnchor232"/><span class="koboSpan" id="kobo.417.1">ling</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">.</span></span></p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor233"/><span class="koboSpan" id="kobo.419.1">Using explainability techniques</span></h1>
<p><span class="koboSpan" id="kobo.420.1">We can use explainability techniques</span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.421.1"> to identify potential biases in our models and then plan to improve them toward fairness. </span><span class="koboSpan" id="kobo.421.2">Here, we want to practice this concept with SHAP and identify fairness issues between male and female groups in the adult income dataset we practiced with in the previous chapter. </span><span class="koboSpan" id="kobo.421.3">Using the same SHAP explainer object we built for the XGBoost model we trained on adult income data in the previous chapter, in the following bar plots, we can see that there is a low, but non-negligible, dependency on </span><em class="italic"><span class="koboSpan" id="kobo.422.1">sex</span></em><span class="koboSpan" id="kobo.423.1"> regarding the whole dataset or only the incorrectly </span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.424.1">predicted </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">data points:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<span class="koboSpan" id="kobo.426.1"><img alt="Figure 7.4 – SHAP summary plot for the whole adult income dataset and incorrectly predicted data points" src="image/B16369_07_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.427.1">Figure 7.4 – SHAP summary plot for the whole adult income dataset and incorrectly predicted data points</span></p>
<p><span class="koboSpan" id="kobo.428.1">Now, we can extract the fraction of misclassified data points in each sex group, </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.430.1">
X_WithPred.groupby(['Sex', 'Correct Prediction']).size().unstack(fill_value=0)</span></pre> <p><span class="koboSpan" id="kobo.431.1">This will produce the </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<span class="koboSpan" id="kobo.433.1"><img alt="Figure 7.5 – Number of males and females among correct and incorrect predictions" src="image/B16369_07_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.434.1">Figure 7.5 – Number of males and females among correct and incorrect predictions</span></p>
<p><span class="koboSpan" id="kobo.435.1">Here, we have 6.83% and 20.08% misclassification percentages for female and male groups, respectively. </span><span class="koboSpan" id="kobo.435.2">The ROC-AUC of the predictions of the model for only male and female groups in the test set are 0.90 and </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">0.94, respectively.</span></span></p>
<p><span class="koboSpan" id="kobo.437.1">You might </span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.438.1">consider identifying the correlation between features as an approach to identifying proxies and potential ways of removing biases in your models. </span><span class="koboSpan" id="kobo.438.2">The following code and heatmap (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.439.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.440.1">.6</span></em><span class="koboSpan" id="kobo.441.1">) show a correlation between the features of </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">this dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.443.1">
corr_features = X.corr()corr_features.style.background_gradient(cmap='coolwarm')</span></pre>
<p><span class="koboSpan" id="kobo.444.1">The output will be </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<span class="koboSpan" id="kobo.446.1"><img alt="Figure 7.6 – Correlation DataFrame between the features of the adult income dataset" src="image/B16369_07_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.447.1">Figure 7.6 – Correlation DataFrame between the features of the adult income dataset</span></p>
<p><span class="koboSpan" id="kobo.448.1">However, there are disadvantages to using such correlation analysis as the way of approaching the problem of proxy identification or even for filtering features toward improving performance. </span><span class="koboSpan" id="kobo.448.2">Here are two of </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">these disadvantages:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.450.1">You need to consider proper correlation measures for each pair of features. </span><span class="koboSpan" id="kobo.450.2">For example, </span><em class="italic"><span class="koboSpan" id="kobo.451.1">Pearson</span></em><span class="koboSpan" id="kobo.452.1"> correlation</span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.453.1"> cannot be used for all feature pairs as the distribution of data for each pair has to satisfy the assumptions for this method. </span><span class="koboSpan" id="kobo.453.2">Both variables need to follow normal distributions and data should not have any outliers as two of the assumptions for proper use of </span><em class="italic"><span class="koboSpan" id="kobo.454.1">Pearson</span></em><span class="koboSpan" id="kobo.455.1"> correlation. </span><span class="koboSpan" id="kobo.455.2">This means that to have a proper use of the feature correlation analysis approach, you need to use proper correlation measures to compare the features. </span><span class="koboSpan" id="kobo.455.3">Non-parametric statistical measures such as </span><em class="italic"><span class="koboSpan" id="kobo.456.1">Spearman</span></em><span class="koboSpan" id="kobo.457.1"> rank correlation</span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.458.1"> could be more suitable as there are fewer assumptions behind its use across different </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">variable pairs.</span></span></li>
<li><span class="koboSpan" id="kobo.460.1">Not all numerical values have the same meaning. </span><span class="koboSpan" id="kobo.460.2">Some of the features are categorical and, through different methods, are transformed into numerical features. </span><span class="koboSpan" id="kobo.460.3">Sex is one of those features. </span><span class="koboSpan" id="kobo.460.4">Values of 0 and 1 can be used to show female and male groups but they don’t have any numerical meaning that you can find in numerical features such as age </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">or salary.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.462.1">Explainability</span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.463.1"> techniques such as SHAP tell you about dependencies to sensitive attributes and their contributions to the outcome of data points. </span><span class="koboSpan" id="kobo.463.2">However, by default, they don’t offer a way to improve the models in terms of fairness. </span><span class="koboSpan" id="kobo.463.3">In this example, we can try to split the data into male and female groups for training and testing. </span><span class="koboSpan" id="kobo.463.4">The following code shows this approach for the female group. </span><span class="koboSpan" id="kobo.463.5">Similarly, you can repeat this for the male group by separating the train and test input and output data with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.464.1">Sex</span></strong><span class="koboSpan" id="kobo.465.1"> feature of </span><strong class="source-inline"><span class="koboSpan" id="kobo.466.1">1</span></strong><span class="koboSpan" id="kobo.467.1">. </span><span class="koboSpan" id="kobo.467.2">The models that were built separately for male and female groups resulted in 0.90 and 0.93 ROC-AUCs, respectively, which is almost the same as the performance without the separation of </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">the groups:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.469.1">
X_train = X_train.reset_index(drop=True)X_test = X_test.reset_index(drop=True)
# training a model only for female category (Sex category of 0 in this dataset)
X_train_only0 = X_train[X_train['Sex'] == 0]
X_test_only0 = X_test[X_test['Sex'] == 0]
X_only0 = X[X['Sex'] == 0]
y_train_only0 = [y_train[iter] for iter in X_train.index[
    X_train['Sex'] == 0].tolist()]
y_test_only0 = [y_test[iter] for iter in X_test.index[
    X_test['Sex'] == 0].tolist()]
# initializing an XGboost model
xgb_model = xgboost.XGBClassifier(random_state=42)
# fitting the XGboost model with training data
xgb_model.fit(X_train_only0, y_train_only0)
# calculating roc-auc of predictions
print("ROC-AUC of predictions:
    {}".format(roc_auc_score(y_test_only0,
        xgb_model.predict_proba(X_test_only0)[:, 1])))
# generate the Tree explainer
explainer_xgb = shap.TreeExplainer(xgb_model)
# extract SHAP values from the explainer object
shap_values_xgb = explainer_xgb.shap_values(X_only0)
# create a SHAP beeswarm plot (i.e. </span><span class="koboSpan" id="kobo.469.2">SHAP summary plot)
shap.summary_plot(shap_values_xgb, X_only0,
    plot_type="bar")</span></pre>
<p><span class="koboSpan" id="kobo.470.1">We didn’t</span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.471.1"> remove the </span><strong class="source-inline"><span class="koboSpan" id="kobo.472.1">Sex</span></strong><span class="koboSpan" id="kobo.473.1"> feature from the models. </span><span class="koboSpan" id="kobo.473.2">This feature cannot contribute to the model’s performance as there is no difference between the values of this feature across the data points of each model. </span><span class="koboSpan" id="kobo.473.3">This is also shown by zero Shapely values in the </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">bar plots:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<span class="koboSpan" id="kobo.475.1"><img alt="Figure 7.7 – SHAP summary plot for models trained and tested on female and male groups separately" src="image/B16369_07_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.476.1">Figure 7.7 – SHAP summary plot for models trained and tested on female and male groups separately</span></p>
<p><span class="koboSpan" id="kobo.477.1">This </span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.478.1">approach of separating groups according to a sensitive attribute, although sometimes seen as taken, is not an ideal way of dealing with the issue of fairness. </span><span class="koboSpan" id="kobo.478.2">It might not be an effective approach as the model could be highly reliant on other sensitive features. </span><span class="koboSpan" id="kobo.478.3">Also, we cannot split the data into small chunks according to all combinations of all sensitive attributes in our dataset. </span><span class="koboSpan" id="kobo.478.4">There are fairness tools that could help you not only assess fairness and detect biases but select a model that better satisfies </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">fairness notions.</span></span></p>
<p><span class="koboSpan" id="kobo.480.1">In addition to libraries for explainability, there are Python libraries that are designed specifically for fairness detection and improvement in machine learning modeling, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">co</span><a id="_idTextAnchor234"/><span class="koboSpan" id="kobo.482.1">ver next.</span></span></p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor235"/><span class="koboSpan" id="kobo.483.1">Fairness assessment and improvement in Python</span></h1>
<p><span class="koboSpan" id="kobo.484.1">There are few widely used Python</span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.485.1"> libraries to assess fairness in your models (</span><em class="italic"><span class="koboSpan" id="kobo.486.1">Table 7.3</span></em><span class="koboSpan" id="kobo.487.1">). </span><span class="koboSpan" id="kobo.487.2">You can use these libraries to identify if the model satisfies fairness definitions according to the different sensitive attributes in a dataset you want to or have used </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">for modeling:</span></span></p>
<table class="No-Table-Style" id="table003-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.489.1">Library</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.490.1">Library Name for Importing </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.491.1">and Installation</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.492.1">URL</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.493.1">IBM AI </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">Fairness 360</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">aif360</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/aif360/"><span class="No-Break"><span class="koboSpan" id="kobo.496.1">https://pypi.org/project/aif360/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.497.1">Fairlearn</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.498.1">fairlearn</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/fairlearn/"><span class="No-Break"><span class="koboSpan" id="kobo.499.1">https://pypi.org/project/fairlearn/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.500.1">Black </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">Box Auditing</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.502.1">BlackBoxAuditing</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/BlackBoxAuditing/"><span class="No-Break"><span class="koboSpan" id="kobo.503.1">https://pypi.org/project/BlackBoxAuditing/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.504.1">Aequitas</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.505.1">aequitas</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/aequitas/"><span class="No-Break"><span class="koboSpan" id="kobo.506.1">https://pypi.org/project/aequitas/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.507.1">Responsible </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">AI Toolbox</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.509.1">responsibleai</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/responsibleai/"><span class="No-Break"><span class="koboSpan" id="kobo.510.1">https://pypi.org/project/responsibleai/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.511.1">Responsibly</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.512.1">responsibly</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/responsibly/"><span class="No-Break"><span class="koboSpan" id="kobo.513.1">https://pypi.org/project/responsibly/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.514.1">Amazon </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">Sagemaker Clarify</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">smclarify</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/smclarify/"><span class="No-Break"><span class="koboSpan" id="kobo.517.1">https://pypi.org/project/smclarify/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.518.1">Fairness-aware </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">machine learning</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">fairness</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/fairness/"><span class="No-Break"><span class="koboSpan" id="kobo.521.1">https://pypi.org/project/fairness/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.522.1">Bias correction</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.523.1">biascorrection</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/biascorrection/"><span class="No-Break"><span class="koboSpan" id="kobo.524.1">https://pypi.org/project/biascorrection/</span></span></a></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.525.1">Table 7.3 – Python libraries or repositories with available functionalities for machine learning fairness</span></p>
<p><span class="koboSpan" id="kobo.526.1">First, let’s </span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.527.1">load the adult income dataset, after importing the required libraries, and prepare the training and test sets, </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.529.1">
# loading UCI adult income dataset# classification task to predict if people made over $50k in the 90s or not
X,y = shap.datasets.adult()
# split the data to train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size = 0.3, random_state=10)
# making a dataframe out of y values with "Sex" being their indices
y_train = pd.DataFrame({'label': y_train},
    index = X_train['Sex'])
y_test = pd.DataFrame({'label': y_test},
    index = X_test['Sex'])</span></pre>
<p><span class="koboSpan" id="kobo.530.1">Now, we can train and test an </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">XGBoost model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.532.1">
xgb_model = xgboost.XGBClassifier(random_state=42)# fitting the XGboost model with training data
xgb_model.fit(X_train, y_train)
# calculating roc-auc of predictions
print("ROC-AUC of predictions:
    {}".format(roc_auc_score(y_test,
        xgb_model.predict_proba(X_test)[:, 1])))
# generating predictions for the test set
y_pred_train = xgb_model.predict(X_train)
y_pred_test = xgb_model.predict(X_test)</span></pre>
<p><span class="koboSpan" id="kobo.533.1">Here, we</span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.534.1"> want to use </span><strong class="source-inline"><span class="koboSpan" id="kobo.535.1">aif360</span></strong><span class="koboSpan" id="kobo.536.1"> to calculate the </span><em class="italic"><span class="koboSpan" id="kobo.537.1">DIR</span></em><span class="koboSpan" id="kobo.538.1"> of real and predicted outcomes in the training and test data according to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.539.1">Sex</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.540.1"> attribute:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.541.1">
# calculating disparate impact ratiodi_train_orig = disparate_impact_ratio(y_train,
    prot_attr='Sex', priv_group=1, pos_label=True)
di_test_orig = disparate_impact_ratio(y_test,
    prot_attr='Sex', priv_group=1, pos_label=True)
di_train = disparate_impact_ratio(y_train, y_pred_train,
    prot_attr='Sex', priv_group=1, pos_label=True)
di_test = disparate_impact_ratio(y_test, y_pred_test,
    prot_attr='Sex', priv_group=1, pos_label=True)</span></pre>
<p><span class="koboSpan" id="kobo.542.1">The following group bar plot shows that the predictions make the </span><em class="italic"><span class="koboSpan" id="kobo.543.1">DIR</span></em><span class="koboSpan" id="kobo.544.1"> even worse in both the training and </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">test sets:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<span class="koboSpan" id="kobo.546.1"><img alt="Figure 7.8 – Comparison of DIR in the original data and predicted outputs" src="image/B16369_07_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.547.1">Figure 7.8 – Comparison of DIR in the original data and predicted outputs</span></p>
<p><span class="koboSpan" id="kobo.548.1">We can</span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.549.1"> use </span><strong class="bold"><span class="koboSpan" id="kobo.550.1">reject option classification</span></strong><span class="koboSpan" id="kobo.551.1"> as an </span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.552.1">available class in </span><strong class="source-inline"><span class="koboSpan" id="kobo.553.1">aif360</span></strong><span class="koboSpan" id="kobo.554.1"> to improve our models toward fairness. </span><span class="koboSpan" id="kobo.554.2">Reject option classification is a postprocessing technique that gives favorable outcomes to unprivileged groups and unfavorable outcomes to privileged groups in a confidence band around the decision boundary with the highest uncertainty (</span><a href="https://aif360.readthedocs.io/"><span class="koboSpan" id="kobo.555.1">https://aif360.readthedocs.io/</span></a><span class="koboSpan" id="kobo.556.1">, Kamira et al., 2012). </span><span class="koboSpan" id="kobo.556.2">First, let’s import all the necessary libraries and functionalities we need for doing so </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">in Python:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.558.1">
# importing Reject option classification, a post processing technique that gives favorable outcomes to unprivileged groups and unfavourable outcomes to# privileged groups in a confidence band around the decision boundary
# with the highest uncertainty
from aif360.sklearn.postprocessing import RejectOptionClassifierCV
# importing PostProcessingMeta,  a meta-estimator which wraps a given
# estimator with a post-processing step.
</span><span class="koboSpan" id="kobo.558.2"># fetching adult dataset from aif360 library
X, y, sample_weight = fetch_adult()
X.index = pd.MultiIndex.from_arrays(X.index.codes,
    names=X.index.names)
y.index = pd.MultiIndex.from_arrays(y.index.codes,
    names=y.index.names)
y = pd.Series(y.factorize(sort=True)[0], index=y.index)
X = pd.get_dummies(X)</span></pre>
<p><span class="koboSpan" id="kobo.559.1">Then, we can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">RejectOptionClassifierCV()</span></strong><span class="koboSpan" id="kobo.561.1">to train and validate a random forest classifier on the adult dataset available in </span><strong class="source-inline"><span class="koboSpan" id="kobo.562.1">aif360</span></strong><span class="koboSpan" id="kobo.563.1">. </span><span class="koboSpan" id="kobo.563.2">We switched from XGBoost to random forest solely for the sake of practicing with different models. </span><span class="koboSpan" id="kobo.563.3">We need to fit a </span><strong class="source-inline"><span class="koboSpan" id="kobo.564.1">PostProcessingMeta()</span></strong><span class="koboSpan" id="kobo.565.1"> object with an initial random forest model and </span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">RejectOptionClassifierCV()</span></strong><span class="koboSpan" id="kobo.567.1">. </span><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">'sex'</span></strong><span class="koboSpan" id="kobo.569.1"> is considered the sensitive feature in </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">the process:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.571.1">
metric = 'disparate_impact'ppm = PostProcessingMeta(RF(n_estimators = 10,
    random_state = 42),
    RejectOptionClassifierCV('sex', scoring=metric,
        step=0.02, n_jobs=-1))
ppm.fit(X, y)</span></pre>
<p><span class="koboSpan" id="kobo.572.1">We can then plot the </span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.573.1">balanced accuracy and </span><em class="italic"><span class="koboSpan" id="kobo.574.1">DIR</span></em><span class="koboSpan" id="kobo.575.1"> across different attempts in the grid search to show the best-chosen parameters, which is the starred point in the scatter plot in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.576.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.577.1">.9</span></em><span class="koboSpan" id="kobo.578.1">. </span><span class="koboSpan" id="kobo.578.2">The points in cyan show you the Pareto front for the tradeoff between balanced accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.580.1">DIR</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<span class="koboSpan" id="kobo.582.1"><img alt="Figure 7.9 – Balanced accuracy versus DIR in a grid search" src="image/B16369_07_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.583.1">Figure 7.9 – Balanced accuracy versus DIR in a grid search</span></p>
<p><span class="koboSpan" id="kobo.584.1">As you can see, there is a compromise between performance and fairness in this case. </span><span class="koboSpan" id="kobo.584.2">But in this case, a less than 4% decrease in performance results in improving </span><em class="italic"><span class="koboSpan" id="kobo.585.1">DIR</span></em><span class="koboSpan" id="kobo.586.1"> from lower than 0.4 </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">to 0.8.</span></span></p>
<p><span class="koboSpan" id="kobo.588.1">As you saw in </span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.589.1">this example, we can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.590.1">aif360</span></strong><span class="koboSpan" id="kobo.591.1"> to assess fairness and improve our model’s fairness with little loss in performance. </span><span class="koboSpan" id="kobo.591.2">You can use other libraries in Python similarly. </span><span class="koboSpan" id="kobo.591.3">And each one has its functionality for the two objectives of fairness assessment and improvement in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">learning modeling.</span></span></p>
<p><span class="koboSpan" id="kobo.593.1">What we provided in this chapter was only the tip of the iceberg of fairness in machine learning. </span><span class="koboSpan" id="kobo.593.2">But at this point, you are ready to try different libraries and techniques and learn about them with the help of the practices we </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">went throu</span><a id="_idTextAnchor236"/><span class="koboSpan" id="kobo.595.1">gh.</span></span></p>
<h1 id="_idParaDest-149"><a id="_idTextAnchor237"/><span class="koboSpan" id="kobo.596.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.597.1">In this chapter, you learned more about the concept of fairness in the machine learning era, as well as the metrics, definitions, and challenges for assessing fairness. </span><span class="koboSpan" id="kobo.597.2">We talked about example proxies for sensitive attributes such as </span><em class="italic"><span class="koboSpan" id="kobo.598.1">sex</span></em><span class="koboSpan" id="kobo.599.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.600.1">race</span></em><span class="koboSpan" id="kobo.601.1">. </span><span class="koboSpan" id="kobo.601.2">We also talked about possible sources of bias, such as in data collection or model training. </span><span class="koboSpan" id="kobo.601.3">You also learned how you can use Python libraries for model explainability and fairness to assess fairness or improve it in your models, as well as avoid biases that not only would be unethical but could have legal and financial consequences for </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">your organization.</span></span></p>
<p><span class="koboSpan" id="kobo.603.1">In the next chapter, you will learn about test-driven development and concepts such as unit and differential testing. </span><span class="koboSpan" id="kobo.603.2">We will also talk about machine learning experiment tracking and how it helps us avoid issues in our models in the model training, testing, and </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">selection process</span><a id="_idTextAnchor238"/><span class="koboSpan" id="kobo.605.1">es.</span></span></p>
<h1 id="_idParaDest-150"><a id="_idTextAnchor239"/><span class="koboSpan" id="kobo.606.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.607.1">Does fairness depend only on </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">observable features?</span></span></li>
<li><span class="koboSpan" id="kobo.609.1">What are examples of proxy features </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">for </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.611.1">'sex'</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">?</span></span></li>
<li><span class="koboSpan" id="kobo.613.1">If one model is fair according to demographic parity, would it be fair according to other notions of fairness such as </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">equalized odds?</span></span></li>
<li><span class="koboSpan" id="kobo.615.1">What is the difference between demographic parity and equalized odds as two </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">fairness metrics?</span></span></li>
<li><span class="koboSpan" id="kobo.617.1">If you have a </span><strong class="source-inline"><span class="koboSpan" id="kobo.618.1">'sex'</span></strong><span class="koboSpan" id="kobo.619.1"> feature in your model and your model would have a low dependency on that, does it mean that your model is fair across different </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">sex groups?</span></span></li>
<li><span class="koboSpan" id="kobo.621.1">How could you use explainability techniques to assess fairness in </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">your mode</span><a id="_idTextAnchor240"/><span class="koboSpan" id="kobo.623.1">ls?</span></span></li>
</ol>
<h1 id="_idParaDest-151"><a id="_idTextAnchor241"/><span class="koboSpan" id="kobo.624.1">References</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.625.1">Barocas, Solon, Moritz Hardt, and Arvind Narayanan. </span><em class="italic"><span class="koboSpan" id="kobo.626.1">Fairness in machine learning</span></em><span class="koboSpan" id="kobo.627.1">. </span><span class="koboSpan" id="kobo.627.2">Nips tutorial 1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">2017): 2017.</span></span></li>
<li><span class="koboSpan" id="kobo.629.1">Mehrabi, Ninareh, et al. </span><em class="italic"><span class="koboSpan" id="kobo.630.1">A survey on bias and fairness in machine learning</span></em><span class="koboSpan" id="kobo.631.1">. </span><span class="koboSpan" id="kobo.631.2">ACM Computing Surveys (CSUR) 54.6 (</span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">2021): 1-35.</span></span></li>
<li><span class="koboSpan" id="kobo.633.1">Caton, Simon, and Christian Haas. </span><em class="italic"><span class="koboSpan" id="kobo.634.1">Fairness in machine learning: A survey</span></em><span class="koboSpan" id="kobo.635.1">. </span><span class="koboSpan" id="kobo.635.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">arXiv:2010.04053 (2020).</span></span></li>
<li><span class="koboSpan" id="kobo.637.1">Pessach, Dana, and Erez Shmueli. </span><em class="italic"><span class="koboSpan" id="kobo.638.1">A review on fairness in machine learning</span></em><span class="koboSpan" id="kobo.639.1">. </span><span class="koboSpan" id="kobo.639.2">ACM Computing Surveys (CSUR) 55.3 (</span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">2022): 1-44.</span></span></li>
<li><span class="koboSpan" id="kobo.641.1">Lechner, Tosca, et al. </span><em class="italic"><span class="koboSpan" id="kobo.642.1">Impossibility results for fair representations</span></em><span class="koboSpan" id="kobo.643.1">. </span><span class="koboSpan" id="kobo.643.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">arXiv:2107.03483 (2021).</span></span></li>
<li><span class="koboSpan" id="kobo.645.1">McCalman, Lachlan, et al. </span><em class="italic"><span class="koboSpan" id="kobo.646.1">Assessing AI fairness in finance</span></em><span class="koboSpan" id="kobo.647.1">. </span><span class="koboSpan" id="kobo.647.2">Computer 55.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">2022): 94-97.</span></span></li>
<li><span class="koboSpan" id="kobo.649.1">F. </span><span class="koboSpan" id="kobo.649.2">Kamiran, A. </span><span class="koboSpan" id="kobo.649.3">Karim, and X. </span><span class="koboSpan" id="kobo.649.4">Zhang, </span><em class="italic"><span class="koboSpan" id="kobo.650.1">Decision Theory for Discrimination-Aware Classification</span></em><span class="koboSpan" id="kobo.651.1">. </span><span class="koboSpan" id="kobo.651.2">IEEE International Conference on Data </span><span class="No-Break"><span class="koboSpan" id="kobo.652.1">Mining, 2012.</span></span></li>
</ul>
</div>


<div class="Content" id="_idContainer088">
<h1 id="_idParaDest-152" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor242"/><span class="koboSpan" id="kobo.1.1">Part 3:Low-Bug Machine Learning Development and Deployment</span></h1>
<p><span class="koboSpan" id="kobo.2.1">With this part of the book, we will provide the essential practices to ensure the robustness and reliability of machine learning models, especially in production. </span><span class="koboSpan" id="kobo.2.2">We will start with the adoption of Test-Driven Development, illustrating its crucial role in mitigating risks during model development. </span><span class="koboSpan" id="kobo.2.3">Subsequently, we will delve into the testing techniques and the significance of model monitoring, ensuring that our models remain dependable when deployed. </span><span class="koboSpan" id="kobo.2.4">We will then explain techniques and challenges in achieving reproducibility in machine learning through code, data, and model versioning. </span><span class="koboSpan" id="kobo.2.5">We will conclude this part by addressing the challenges of data and concept drifts to have reliable models </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">in production.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B16369_08.xhtml#_idTextAnchor243"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 8</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Controlling Risks Using Test-Driven Development</span></em></li>
<li><a href="B16369_09.xhtml#_idTextAnchor261"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 9</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Testing and Debugging for Production</span></em></li>
<li><a href="B16369_10.xhtml#_idTextAnchor286"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 10</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Versioning and Reproducible Machine Learning Modeling</span></em></li>
<li><a href="B16369_11.xhtml#_idTextAnchor300"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 11</span></em></a><span class="koboSpan" id="kobo.16.1">, </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Avoiding and Detecting Data and Concept Drifts</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer089">
</div>
</div>
<div>
<div id="_idContainer090">
</div>
</div>
<div>
<div id="_idContainer091">
</div>
</div>
<div>
<div id="_idContainer092">
</div>
</div>
<div>
<div id="_idContainer093">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer094">
</div>
</div>
<div>
<div id="_idContainer095">
</div>
</div>
<div>
<div id="_idContainer096">
</div>
</div>
</body></html>