<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Automatic Prose Generation with Recurrent Neural Networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have been interacting through this book for almost 200 pages, but I realized that I have not introduced myself properly to you! I guess it's time. You already know some bits about me through the author profile in this book; however, I want to tell you a bit about the city I live in. I am based in South India, in a city called Bengaluru, also know as Bangalore. The city is known for its IT talent and population diversity. I love the city, as it is filled with loads of positive energy. Each day, I get to meet people from all walks of life—people from multiple ethnicities, multiple backgrounds, people who speak multiple languages, and so on. Kannada is the official language spoken in the state of Karnataka where Bangalore is located. Though I can speak bits and pieces of Kannada, my proficiency with speaking the language is not as good as a native Kannada speaker. Of course, this is an area of improvement for me and I am working on it. Like me, many other migrants that moved to the city from other places also face problems while conversing in Kannada. Interestingly, not knowing the language does not stop any of us from interacting with locals in their own language. Guess what comes to our rescue: mobile apps such as Google translate, Google text-to-speech, and the like. These applications are built on NLP technologies called machine translation and speech recognition. These technologies in turn work on things known as <strong class="calibre3">language models</strong>. Language models is the topic we will delve into in this chapter.</p>
<p class="mce-root">The objectives of the chapter include exploring the following topics:</p>
<ul class="calibre9">
<li class="calibre10">The need for language modeling to address natural language processing <span>tasks</span></li>
<li class="calibre10">The working principle of language models</li>
<li class="calibre10">Application of language models</li>
<li class="calibre10">Relationship between language modeling and neural networks</li>
<li class="calibre10">Recurrent neural networks</li>
</ul>
<ul class="calibre9">
<li class="calibre10">Differences between a normal feedforward network and a recurrent neural network</li>
<li class="calibre10">Long short-term memory networks</li>
<li class="calibre10">A project to autogenerate text using recurrent neural networks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding language models</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the English language, the character <em class="calibre15">a</em> appears much more often in words and sentences than the character <em class="calibre15">x</em>. Similarly, we can also observe that the word <em class="calibre15">is</em> occurs more frequently than the word <em class="calibre15">specimen</em>. It is possible to learn the probability distributions of characters and words by examining large volumes of text. The following screenshot is a chart showing the probability distribution of letters given a corpus (text dataset):</p>
<p class="CDPAlignCenter1"><img class="aligncenter106" src="assets/fca6b8ed-0b69-43c5-9860-2f2822b70e52.png"/></p>
<div class="packtfigref">Probability distribution of letters in a corpus</div>
<p class="mce-root">We can observe that the probability distributions of characters are non-uniform. This essentially means that we can recover the characters in a word, even if they are lost due to noise. If a particular character is missing in a word, it can be reconstructed just based on the characters that are surrounding the missing character. The reconstruction of the missing character is not done randomly, but is done by picking the character that has the highest probability distribution of occurrence, given the characters that are surrounding the missing character. Technically speaking, the statistical structure of words in a sentence or characters in words follows the distance from maximal entropy.</p>
<p class="mce-root">A language model exploits the statistical structure of a language to express the following:</p>
<ul class="calibre9">
<li class="calibre10">Given <em class="calibre22">w_1, w_2, w_3,...w_N</em> words in a sentence, a language model assigns a probability to a sentence <em class="calibre22">P(w_1, w_2, w_3,.... w_N)</em>.</li>
<li class="calibre10"><span>It then assigns probability of an upcoming word (<em class="calibre22">w_4</em> in this case) as <em class="calibre22">P(w_4 | w_1, w_2, w_3)</em>.</span></li>
</ul>
<p class="mce-root">Language models enable a number of applications to be developed in NLP, and some of them are listed as follows:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Machine translation</strong>: P(enormous cyclone tonight) &gt; P(gain typhoon this evening)</li>
<li class="calibre10"><strong class="calibre1">Spelling correction</strong><span>: P(satellite constellation) &gt; P(satelitte constellation)</span></li>
<li class="calibre10"><strong class="calibre1">Speech recognition</strong><span>: P(I saw a van) &gt; P(eyes awe of an)</span></li>
<li class="calibre10"><span><strong class="calibre1">Typing prediction</strong>: Auto completion of in Google search, typing assistance apps</span></li>
</ul>
<p class="mce-root">Let's now look at how the probabilities are calculated for the words. Consider a simple sentence, <em class="calibre15">Decembers are cold</em>. The probability of this sentence is expressed as follows:</p>
<p class="mce-root"><em class="calibre15">P("Decembers are cold") = P("December") * P ("are" | "Decembers") * P("cold" | "Decembers are")</em></p>
<p class="mce-root">Mathematically, the probability computation of words in a sentence (or letters in a word) can be expressed as follows:</p>
<p class="CDPAlignCenter1"><img class="fm-editor-equation11" src="assets/c05cd3c8-05cf-49c4-a4e6-d282ae3c708f.png"/></p>
<p class="mce-root">Andrey Markov, a Russian mathematician, described a stochastic process with a property called <strong class="calibre3">Markov Property</strong> or <strong class="calibre3">Markov Assumption</strong>. This basically states that one can make predictions for the future of the process based solely on its present state, just as well as one could knowing the process's full history, hence independently from such history.</p>
<p class="mce-root">Based on Markov's assumption, we can rewrite the conditional probability of <em class="calibre15">cold</em> as follows:</p>
<p class="mce-root"><em class="calibre15">P("cold" | "Decembers are") is congruent to</em> <em class="calibre15">P("cold" | "are")</em></p>
<p class="mce-root">Mathematically, Markov's assumption can be expressed as follows:</p>
<p class="CDPAlignCenter1"><img class="fm-editor-equation12" src="assets/1d051944-7841-4762-86ff-9ebf71f50ad6.png"/></p>
<p class="mce-root">While this mathematical formulation represents the bigram model (two words taken into consideration at a time), it can be easily extended to an n-gram model. In the n-gram model, the conditional probability depends on just a couple more previous words.</p>
<p class="mce-root">Mathematically, an n-gram model is expressed as follows:</p>
<p class="CDPAlignCenter1"><img class="fm-editor-equation13" src="assets/f04d8634-6de8-4781-a727-58d55404307f.png"/></p>
<p class="mce-root">Consider the famous poem <em class="calibre15">A Girl</em> by <em class="calibre15">Ezra Pound</em> as our corpus for building a <strong class="calibre3">bigram</strong> model. The following is the text corpus:</p>
<pre class="calibre16">The tree has entered my hands,<br class="title-page-name"/>The sap has ascended my arms,<br class="title-page-name"/>The tree has grown in my breast-Downward,<br class="title-page-name"/>The branches grow out of me, like arms.<br class="title-page-name"/>Tree you are,<br class="title-page-name"/><span>Moss you are,<br class="title-page-name"/></span><span>You are violets with wind above them.<br class="title-page-name"/></span>A child - so high - you are,<br class="title-page-name"/>And all this is folly to the world.</pre>
<p class="mce-root">We are already aware that in a bigram model, the conditional probability is computed just based on the previous word. So, the probability of a word can be computed as follows:</p>
<p class="CDPAlignCenter1"><img class="fm-editor-equation14" src="assets/1b973bd5-ea2c-4c61-a4f6-756b8f7a49f5.png"/></p>
<p class="mce-root">If we were to compute the probability of the word <em class="calibre15">arms</em> given the word <em class="calibre15">my</em> in the poem, it is computed as the number of times the words <em class="calibre15">arms</em> and <em class="calibre15">my</em> appear together in the poem, divided by the number of times the word <em class="calibre15">my</em> appears in the poem.</p>
<p class="mce-root">We see that the words <em class="calibre15">my arms</em> appeared in the poem only once (in the sentence <em class="calibre15">The sap has ascended my arms</em>). However, the word <em class="calibre15">my</em> appeared in the poem three times (in the sentences <em class="calibre15">The tree has entered my hands</em>, <em class="calibre15">The sap has ascended my arms</em>, and <em class="calibre15">The tree has grown in my breast-Downward</em>).</p>
<p class="mce-root">Therefore, the conditional probability of the word <em class="calibre15">arms</em> given <em class="calibre15">my</em> is 1/3, formally represented as follows:</p>
<p class="mce-root"><em class="calibre15">P("arms" | "my") = P("arms", "my") / P("my") = 1 / 3</em></p>
<p class="mce-root">To calculate probability of the first and last words, the special tags &lt;BOS&gt; and &lt;EOS&gt; are added at the start and end of sentences, respectively. Similarly, <span class="calibre4">the</span> probability of a sentence or sequence of words can be calculated using the same approach by multiplying all the bigram probabilities.</p>
<p class="mce-root">As language modeling involves predicting the next word in a sequence, given the sequence of words already present, we can train a language model to create subsequent words in a sequence from a given starting sequence.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring recurrent neural networks</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong class="calibre3">Recurrent neural networks</strong> <span class="calibre4">(</span><strong class="calibre3">RNNs</strong><span class="calibre4">) are a family of neural networks for processing sequential data. RNNs are generally used to implement language models. </span>We, as humans, base much of our language understanding on the context. For example, let's consider the sentence <em class="calibre15">Christmas falls in the month of --------</em>. It is easy to fill in the blank with the word <em class="calibre15">December</em>. The essential idea here is that there is information about the last word encoded in the previous elements of the sentence.</p>
<p class="mce-root">The central theme behind the RNN architecture is to exploit the sequential structure of the data. As the name suggests, RNNs operate in a recurrent way. Essentially, this means that the same operation is performed for every element of a sequence or sentence, with its output depending on the current input and the previous operations.</p>
<p class="mce-root">An RNN works by looping an output of the network at time <em class="calibre15">t</em> with the input of the network at time <em class="calibre15">t+1</em>. These loops allow persistence of information from one time step to the next one. The following diagram is a circuit diagram representing an RNN:</p>
<p class="CDPAlignCenter1"><img class="aligncenter107" src="assets/94bd6b4e-b724-432d-b9f0-4f5c28137562.png"/></p>
<div class="packtfigref">Circuit diagram representing a RNN</div>
<p class="mce-root">The diagram indicates an RNN that remembers what it knows from previous input using a simple loop. This loop takes the information from the previous timestamp and adds it to the input of <span class="calibre4">the</span> current timestamp. At a particular time step <em class="calibre15">t</em>, <em class="calibre15">X<sub class="calibre40">t</sub></em> is the input to the network, <em class="calibre15">O<sub class="calibre40">t</sub></em> is the output of the network, <span class="calibre4">and</span> <em class="calibre15">h<sub class="calibre40">t</sub></em> <span class="calibre4">is the detail it remembered from previous nodes in the network. In between, there is the RNN cell, which contains neural networks just like a feedforward network.</span></p>
<div class="packtinfobox">One key point to ponder in terms of the definition of an RNN is the timestamps. The timestamps referred to in the definition have nothing to do with past, present, and future. They simply represent a word or an item in a sequence or a sentence.</div>
<p class="mce-root">Let's consider an example sentence: <em class="calibre15">Christmas Holidays are Awesome</em>. In this sentence, take a look at the following timestamp:</p>
<ul class="calibre9">
<li class="calibre10"><em class="calibre22">Christmas</em> is x<sub class="calibre40">0</sub></li>
<li class="calibre10"><em class="calibre22">Holidays</em> is x<sub class="calibre40">1</sub></li>
<li class="calibre10"><em class="calibre22">are</em> is x<sub class="calibre40">2</sub>;</li>
<li class="calibre10"><em class="calibre22">Awesome</em> is x<sub class="calibre40">3</sub></li>
</ul>
<p class="mce-root">If t=1, then take a look at the following:</p>
<ul class="calibre9">
<li class="calibre10">x<sub class="calibre40">t</sub> = <em class="calibre22">Holidays</em> → event at current timestamp</li>
<li class="calibre10">x<sub class="calibre40">t-1</sub> <span>= <em class="calibre22">Christmas</em> → event at previous timestamp</span></li>
</ul>
<p class="mce-root">It can be observed from the preceding circuit diagram that the same operation is performed in <span class="calibre4">the</span> RNN repeatedly on different nodes. There is also a black square in the diagram that represents a time delay of a single time step. It may be confusing to understand the RNN with the loops, so let's unfold the computational graph. The unfolded RNN computational graph is shown in the following diagram:</p>
<p class="CDPAlignCenter1"><img class="aligncenter108" src="assets/6a1b4211-5c6e-4d7e-b757-1459e993d077.png"/></p>
<div class="packtfigref">RNN—unfolded computational graph view</div>
<p class="mce-root">In the preceding diagram, each node is associated with a particular time. In the RNN architecture, each node receives different inputs at each time step <strong class="calibre3">x<sub class="calibre40">t</sub></strong>. It also has the capability of producing outputs at each time step <strong class="calibre3">o<sub class="calibre40">t</sub></strong>. The network also maintains a memory state <strong class="calibre3">h<sub class="calibre40">t</sub></strong>, which contains information about what happened in the network up to the time <em class="calibre15">t</em>. As this is the same process that is run across all the nodes in the network, it is possible to represent the whole network in a simplified form, as shown in the RNN circuit diagram.</p>
<p class="mce-root">Now, we understand that we see the word <strong class="calibre3">recurrent</strong> in RNNs because it performs the same task for every element of a sequence, with the output depending on previous computations. It may be noted that, theoretically, RNNs can make use of information in arbitrarily long sequences, but in practice, they are implemented to looking back only a few steps.</p>
<p class="mce-root">Formally, an RNN can be defined in an equation as follows:</p>
<p class="CDPAlignCenter1"><img class="fm-editor-equation15" src="assets/e30363fe-7188-4b4e-9d21-010c52cf2326.png"/></p>
<p class="mce-root">In the equation, <em class="calibre15">h<sub class="calibre40">t</sub></em> is the hidden state at timestamp <em class="calibre15">t</em>. An activation function such as Tanh, Sigmoid, or ReLU can be applied to compute the hidden state and it is represented in the equation as <sub class="calibre40"><img class="fm-editor-equation16" src="assets/de0d48dd-c787-4efe-a5e2-1f26ce1485de.png"/></sub>. <em class="calibre15">W</em> is the weight matrix for the input to the hidden layer at timestamp <em class="calibre15">t</em>. <em class="calibre15">X<sub class="calibre40">t</sub></em> is the input at timestamp <em class="calibre15">t</em>. <em class="calibre15">U</em> is the weight matrix for the hidden layer at time <em class="calibre15">t-1</em> to the hidden layer at time <em class="calibre15">t</em>, and <em class="calibre15">h<sub class="calibre40">t-1</sub></em> is the hidden state at timestamp <em class="calibre15">t</em>.</p>
<p class="mce-root">During backpropagation, <em class="calibre15">U</em> and <em class="calibre15">W</em> weights are learned by the RNN. At each node, the contribution of the hidden state and the contribution of the current input are decided by <em class="calibre15">U</em> and <em class="calibre15">W</em>. The proportions of <em class="calibre15">U</em> and <em class="calibre15">W</em> in turn result in the generation of output at the current node. The activation functions add non-linearity in RNNs, thus enabling the simplification of gradient calculations during the backpropagation process. The following diagram illustrates the idea of backpropagation:</p>
<p class="CDPAlignCenter1"><img class="aligncenter109" src="assets/4794f067-f59f-4ebd-9046-7519eddc4c4c.png"/></p>
<div class="packtfigref">Backpropagation in neural networks</div>
<p class="mce-root">The following diagram depicts the overall working mechanism of an RNN and the way the weights <em class="calibre15">U</em> and <em class="calibre15">W</em> are learned through backpropagation. It also depicts the use of the <em class="calibre15">U</em> and <em class="calibre15">W</em> weight matrices in the network to generate the output, as shown in the following diagram:</p>
<p class="CDPAlignCenter1"><img class="aligncenter110" src="assets/7848cff1-e4ef-40cb-a5fa-f439878690be.png"/></p>
<div class="packtfigref">Role of weights in RNN</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparison of feedforward neural networks and RNNs</h1>
                </header>
            
            <article>
                
<p class="mce-root">One fundamental difference between other neural networks and RNNs is that, in all other networks, the inputs are independent of each other. However, in an RNN, all the inputs are related to each other. In an application, to predict the next word in a given sentence, the relationship between all the previous words helps to predict the current output. In other words, an RNN remembers all these relationships while training itself. This is not the case with other types of neural networks. A representation of a feedforward network is illustrated in the following diagram:</p>
<p class="CDPAlignCenter1"><img class="aligncenter111" src="assets/f72615fe-2f5b-4848-8b9c-970e2468b790.png"/></p>
<div class="packtfigref">Feedforward neural network architecture</div>
<p class="mce-root">From the preceding diagram, we can see that no loops are involved in the feedforward network architecture. This is in contrast to that of the RNN architecture depicted in the diagrams of the RNN circuit diagram and <span class="calibre4">the</span> RNN unfolded computational graph. The series of mathematical operations in a feedforward network is performed at the nodes and the information is processed straight through, with no loops whatsoever.</p>
<p class="mce-root">Using supervised learning, the input that is fed to a feedforward network is transformed into output. The output in this case could be a label if it is classification, or it is a number in the case of regression. If we consider image classification, a label can be <em class="calibre15">cat</em> or <em class="calibre15">dog</em> for an image given as input.</p>
<p class="mce-root">A feedforward neural network is trained on labeled images until errors are minimized in predicting the labels. Once trained, the model is able to classify even images that it has not seen previously. A trained feedforward network can be exposed to any random collection of photographs; the categorization of the first photograph does not have any impact or influence on the second or subsequent photographs that the model needs to categorize. Let's discuss this with an example for better clarity on the concept: if the first image is seen as a <em class="calibre15">dog</em> by the feedforward network, it does not imply that the second image will be classified as <em class="calibre15">cat</em>. In other words, the predictions that the model arrives at have no notion of order in time, and the decision regarding the label is arrived at just based on the current input that is provided. To summarize, in feedforward networks no information on historical predictions is used for current predictions. This is very different from RNNs, where the previous prediction is considered in order to aid the current prediction.</p>
<p class="mce-root">Another important difference is that feedforward networks, by design, map one input to one output, whereas RNNs can take multiple forms: map one input to multiple outputs, many inputs to many outputs, or many inputs to one output. The following diagram depicts the various input-output mappings possible with RNNs:</p>
<p class="CDPAlignCenter1"><img class="aligncenter112" src="assets/7c169342-447e-45c2-8568-cc4001515979.png"/></p>
<div class="packtfigref">Input-output mapping possibilities with RNNs</div>
<p class="mce-root">Let's review some of the practical applications of the various input-output mappings possible with an RNN. Each rectangle in the preceding diagram is a vector and the arrows represent functions, for example, a matrix multiplication. The input vectors are the lower rectangles (colored in red), and the output vectors are the upper rectangles (colored in blue color). The middle rectangles (colored in green) are vectors that hold the RNN's state.</p>
<p class="mce-root">The following are the various forms of mapping illustrated in the diagram:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">One input to one output</strong>: The leftmost one is a vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output; for example, image classification.</li>
<li class="calibre10"><strong class="calibre1">One input to many outputs</strong><span>: Sequence output, for example, image captioning takes an image as input and it outputs a sentence of words.</span></li>
<li class="calibre10"><strong class="calibre1">Many inputs to one output</strong><span>: Sequence input, for example, sentiment analysis where a given sentence is given as input to the RNN, and the output is a classification expressing positive or negative sentiment of the sentence.</span></li>
<li class="calibre10"><strong class="calibre1">Many inputs to many outputs</strong><span>: Sequence input and sequence output; for example, for a machine translation task, an RNN reads a sentence in English as input and then outputs a sentence in Hindi or some other language.</span></li>
<li class="calibre10"><strong class="calibre1">Many inputs to many outputs</strong><span>: Synced sequence input and output, for example, video classification where we wish to label each frame of the video.</span></li>
</ul>
<p class="mce-root">Let's now review the final difference between a feedforward network and an RNN. The way backpropagation is done in order to set the weights in a feedforward neural network is different from that of what is called <strong class="calibre3">backpropagation through time</strong> (<strong class="calibre3"><span class="calibre4">BPTT</span></strong>), which is carried out in an RNN. We are already aware that the objective of the backpropagation algorithm in neural networks is to adjust the weights of a neural network to minimize the error of the network outputs compared to some expected output in response to corresponding inputs. Backpropagation itself is a supervised learning algorithm that allows the neural network to be corrected with regard to the specific errors made. The backpropagation algorithm involves the following steps:</p>
<ol class="calibre12">
<li class="calibre10">Provide training input to the neural network and propagate it through the network to get the output</li>
<li class="calibre10"><span>Compare the predicted output to the actual output and calculate the error</span></li>
<li class="calibre10"><span>Calculate the derivatives of the error with respect to the learned network weights</span></li>
<li class="calibre10"><span>Modify the weights to minimize the error</span></li>
<li class="calibre10"><span>Repeat</span></li>
</ol>
<p class="mce-root">In feedforward networks, it makes sense to run backpropagation at the end as the output is available only at the end. In RNNs, the output is produced at each time step and this output influences the output in the subsequent time steps. In other words, in RNNs, the error of a time step depends on the previous time step. Therefore, the normal backpropagation algorithms are not suitable for RNNs. Hence, a different algorithm known as BPTT is used to modify the weights in an RNN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backpropagation through time</h1>
                </header>
            
            <article>
                
<p class="mce-root">We are already aware that RNNs are cyclical graphs, unlike feedforward networks, which are acyclic directional graphs. In feedforward networks, the error derivatives are calculated from the layer above. However, in an RNN we don't have such layering to perform error derivative calculations. A simple solution to this problem is to unroll the RNN and make it similar to a feedforward network. To enable this, the hidden units from the RNN are replicated at each time step. Each time step replication forms a layer that is similar to layers in a feedforward network. Each time step <em class="calibre15">t</em> layer connects to all possible layers in the time step <em class="calibre15">t+1</em>. Therefore, we randomly initialize the weights, unroll the network, and then use backpropagation to optimize the weights in the hidden layer. The lowest layer is initialized by passing parameters. These parameters are also optimized as a part of backpropagation. The backpropagation through time algorithm involves the following steps:</p>
<ol class="calibre12">
<li class="calibre10">Provide a sequence of time steps of input and output pairs to the network</li>
<li class="calibre10"><span>Unroll the network, then calculate and accumulate errors across each time step</span></li>
<li class="calibre10"><span>Roll up the network and update weights</span></li>
<li class="calibre10"><span>Repeat</span></li>
</ol>
<p class="mce-root">In summary, with BPTT, the error is back propagated from the last to the first time step, while unrolling all the time steps. The error for each time step is calculated, which allows updating the weights. The following diagram is a visualization showing the backpropagation through time:</p>
<p class="CDPAlignCenter1"><img class="aligncenter113" src="assets/da6bf332-cf3c-4b93-b0d5-3267c455859b.png"/></p>
<div class="packtfigref">Backpropagation through time in an RNN</div>
<p class="mce-root">It should be noted that as the number of time steps increases, the BPTT algorithm can get computationally very expensive.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Problems and solutions to gradients in RNN</h1>
                </header>
            
            <article>
                
<p class="mce-root">RNNs are not perfect, there are two main issues namely <strong class="calibre3">exploding gradients</strong> and <strong class="calibre3">vanishing gradients</strong> that they suffer from. To understand the issues, let's first understand what a gradient means. A gradient is a partial derivative with respect to its inputs. In simple layman's terms, a gradient measures how much the output of a function changes, if one were to change the inputs a little bit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploding gradients</h1>
                </header>
            
            <article>
                
<p class="mce-root">Exploding gradients relate to a situation where the BPTT algorithm assigns an insanely high importance to the weights, without a rationale. The problem results in an unstable network. In extreme situations, the values of weights can become so large that the values overflow and result in NaN values.</p>
<p class="mce-root">The exploding gradients problem can be detected through observing the following subtle signs while training the network:</p>
<ul class="calibre9">
<li class="calibre10">The model weights quickly become very large during training</li>
<li class="calibre10"><span>The model weights become NaN values during training</span></li>
<li class="calibre10"><span>The error gradient values are consistently above 1.0 for each node and layer during training</span></li>
</ul>
<p class="mce-root">There are several ways in which one could handle the exploding gradients problem. The following are some of the popular techniques:</p>
<ul class="calibre9">
<li class="calibre10">This problem can be easily solved if we can truncate or squash the gradients. This is known as <strong class="calibre1">gradient clipping</strong>.</li>
<li class="calibre10"><span>Updating weights across fewer prior time steps during training may also reduce the exploding gradient problem. This technique of having fewer step updates is called <strong class="calibre1">truncated backpropagation through time</strong> (<strong class="calibre1">TBPTT</strong>). It is an altered version of the BPTT training algorithm where the sequence is processed one time step at a time, and periodically (<em class="calibre22">k1</em> time steps) the BPTT update is performed back for a fixed number of time steps (<em class="calibre22">k2</em> time steps). <em class="calibre22">k1</em> is the number of forward-pass time steps between updates. <em class="calibre22">k2</em> is the number of time steps to which to apply BPTT.</span></li>
<li class="calibre10"><span>Weight regularization can be done by checking the size of network weights and applying a penalty to the networks loss function for large weight values.</span></li>
</ul>
<ul class="calibre9">
<li class="calibre10"><span>By using <strong class="calibre1">long short-term memory units</strong> (<strong class="calibre1">LSTMs</strong>) or <strong class="calibre1">gated recurrent units</strong> (<strong class="calibre1">GRUs</strong>) instead of plain vanilla RNNs.</span></li>
<li class="calibre10"><span>Careful initialization of weights such as <strong class="calibre1">Xavier</strong> initialization or <strong class="calibre1">He</strong> initialization.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Vanishing gradients</h1>
                </header>
            
            <article>
                
<p class="mce-root">We are already aware that long-term dependencies are very important for RNNs to function correctly. RNNs can become too deep because of the long-term dependencies. The vanishing gradient problem arises in cases where the gradient of the activation function is very small. During backpropagation, when the weights are multiplied with the low gradients, they tend to become very small and vanish as they go further into the network. This makes the neural network forget the long-term dependency. The following diagram is an illustration showing the cause that leads to vanishing gradients:</p>
<p class="CDPAlignCenter1"><img class="aligncenter114" src="assets/eea2eb79-2b6a-44fe-8b72-e246a405ec79.png"/></p>
<div class="packtfigref">The cause of vanishing gradients</div>
<p class="mce-root">To summarize, due to the vanishing gradients problem, RNNs experience difficulty in memorizing previous words very far away in the sequence and are only able to make predictions based on the most recent words. This can impact the accuracy of RNN predictions. At times, the model may fail to predict or classify what it is supposed to do.</p>
<p class="mce-root">There are several ways in which one could handle the vanishing gradients problem. The following are some of the most popular techniques:</p>
<ul class="calibre9">
<li class="calibre10">Initialize network weights for the identity matrix so that the potential for a vanishing gradient is minimized.</li>
</ul>
<ul class="calibre9">
<li class="calibre10"><span>Setting the activation functions to ReLU instead of <kbd class="calibre11">sigmoid</kbd> or <kbd class="calibre11">tanh</kbd>. This makes the network computations stay close to the identity function. This works well because when the error derivatives are being propagated backwards through time, they remain constants of either 0 or 1, and so aren't likely to suffer from vanishing gradients.</span></li>
<li class="calibre10"><span>Using LSTMs, which are a variant of the regular recurrent network designed to make it easy to capture long-term dependencies in sequence data. The standard RNN operates in such a way that the hidden state activation is influenced by the other local activations closest to it, which corresponds to a <strong class="calibre1">short-term memory</strong>, while the network weights are influenced by the computations that take place over entire long sequences, which corresponds to a <strong class="calibre1">long-term memory</strong>. The RNN was redesigned so that it has an activation state that can also act like weights and preserve information over long distances, hence the name <strong class="calibre1">long short-term memory</strong>.</span></li>
</ul>
<p class="mce-root">In LSTMs, rather than each hidden node being simply a node with a single activation function, each node is a memory cell in itself that can store other information. Specifically, it maintains its own cell state. Normal RNNs take in their previous hidden state and the current input, and output a new hidden state. An LSTM does the same, except it also takes in its old cell state and will output its new cell state.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building an automated prose generator with an RNN</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this project, we will attempt to build a character-level language model using an RNN to generate prose given some initial seed characters. The main task of a character-level language model is to predict the next character given all previous characters in a sequence of data. In other words, the function of an RNN is to generate text character by character.</p>
<p class="mce-root">To start with, we feed the RNN a huge chunk of text as input and ask it to model the probability distribution of the next character in the sequence, given a sequence of previous characters. These probability distributions conceived by the RNN model will then allow us to generate new text, one character at a time.</p>
<p class="mce-root">The first requirement for building a language model is to secure a corpus of text that the model can use to compute the probability distribution of various characters. The larger the input text corpus, the better the RNN will model the probabilities.</p>
<p class="mce-root">We do not have to strive a lot to secure the big text corpus that is required to train the RNN. There are classical texts (books) such as <em class="calibre15">The Bible</em> that can be used as a corpus. The best part is many of the classical texts are no longer protected under copyright. Therefore, the texts can be downloaded and used freely in our models.</p>
<p class="mce-root">Project Gutenberg is the best place to get access to free books that are no longer protected by copyright. Project Gutenberg can be accessed through the URL <a href="http://www.gutenberg.org" class="calibre8">http://www.gutenberg.org</a>. There are several books such as <em class="calibre15">The Bible</em>, <em class="calibre15">Alice's Adventures in Wonderland</em>, and so on are available from Project Gutenberg. As of December 2018, there are 58,486 books available for download. The books are available in several formats for us to be able to download and use, not just for this project, but for any project where huge text corpus input is required. The following screenshot is of a sample book from Project Gutenberg and the various formats in which the book is available for download:</p>
<p class="CDPAlignCenter1"><img class="aligncenter115" src="assets/87b584f1-a1ca-477f-8cad-974bc49ba6aa.png"/></p>
<div class="packtfigref">Sample book available from Project Gutenberg in various formats</div>
<p class="mce-root">Irrespective of the format of the file that is downloaded, Project Gutenberg adds standard header and footer text to the actual book text. The following is an example of the header and footer that can be seen in a book:</p>
<pre class="calibre16">*** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***<br class="title-page-name"/><br class="title-page-name"/>THE END</pre>
<p class="mce-root">It is essential that we remove this header and footer text from the book text downloaded from Project Gutenberg website. For a text file that is downloaded, one can open the file in a text editor and delete the header and footer.</p>
<p class="mce-root">For our project in this chapter, let's use a favorite book from childhood as the text corpus: <em class="calibre15">Alice’s Adventures in Wonderland</em> by Lewis Carroll. While we have an option to download the text format of this book from Project Gutenberg and make use of it as a text corpus, the R language's <kbd class="calibre11">languageR</kbd> library makes the task even more easier for us. The <kbd class="calibre11">languageR</kbd> library already has the <em class="calibre15">Alice’s Adventures in Wonderland</em> book text. After installing the <kbd class="calibre11">languageR</kbd> library, use the following code to load the text data into the memory and print the loaded text:</p>
<pre class="calibre16"># including the languageR library<br class="title-page-name"/>library("languageR")<br class="title-page-name"/># loading the "Alice’s Adventures in Wonderland" to memory<br class="title-page-name"/>data(alice)<br class="title-page-name"/># printing the loaded text<br class="title-page-name"/>print(alice)</pre>
<p class="mce-root">You will get the following output:</p>
<pre class="calibre16">[1] "ALICE"           "S"                "ADVENTURES"       "IN"               "WONDERLAND"      <br class="title-page-name"/>[6] "Lewis"            "Carroll"          "THE"              "MILLENNIUM"       "FULCRUM"        <br class="title-page-name"/>  [11] "EDITION"          "3"                "0"                "CHAPTER"          "I"              <br class="title-page-name"/>  [16] "Down"             "the"              "Rabbit-Hole"      "Alice"            "was"            <br class="title-page-name"/>  [21] "beginning"        "to"               "get"              "very"             "tired"          <br class="title-page-name"/>  [26] "of"               "sitting"          "by"               "her"              "sister"         <br class="title-page-name"/>  [31] "on"               "the"              "bank"             "and"              "of"             <br class="title-page-name"/>  [36] "having"           "nothing"          "to"               "do"               "once"           <br class="title-page-name"/>  [41] "or"               "twice"            "she"              "had"              "peeped"         <br class="title-page-name"/>  [46] "into"             "the"              "book"             "her"              "sister"         <br class="title-page-name"/>  [51] "was"              "reading"          "but"              "it"               "had"            <br class="title-page-name"/>  [56] "no"         "pictures"         "or"               "conversations"    "in"              </pre>
<p class="mce-root">We see from the output that the book text is stored as a character vector, where each of the vector items is a word from the book text that is split by punctuation. It may also be noted that not all the punctuation is retained in the book text.</p>
<p class="mce-root">The following code reconstructs the sentences from the words in the character vector. Of course, we do not get things like sentence boundaries during the reconstruction process, as the character vector does not have as much punctuation as character vector items. Now, let's do the reconstruction of the book text from individual words:</p>
<pre class="calibre16">alice_in_wonderland&lt;-paste(alice,collapse=" ")<br class="title-page-name"/>print(alice_in_wonderland)</pre>
<p class="mce-root">You will get the following output:</p>
<pre class="calibre16">[1] "ALICE S ADVENTURES IN WONDERLAND Lewis Carroll THE MILLENNIUM FULCRUM EDITION 3 0 CHAPTER I Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do once or twice she had peeped into the book her sister was reading but it had no pictures or conversations in it and what is the use of a book thought Alice without pictures or conversation So she was considering in her own mind as well as she could for the hot day made her feel very sleepy and stupid whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies when suddenly a White Rabbit with pink eyes ran close by her There was nothing so VERY remarkable in that nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself Oh dear Oh dear I shall be late when she thought it over afterwards it occurred to her that she ought to have wondered at this but at the time it all seemed quite natural but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT- POCKET and looked at it and then hurried on Alice started to her feet for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket or a watch to take out of it and burning with curiosity she ran across the field after it and fortunately was just in time to see it pop down a large rabbit-hole under the hedge In another moment down went Alice after it never once considering how in the world she was to get out again The rabbit-hole we .......</pre>
<p class="mce-root">From the output, we see that a long string of text is constructed from the words. Now, we can move on to doing some preprocessing on this text to feed it to the RNN so that the model learns the dependencies between characters and the conditional probabilities of characters in sequences.</p>
<div class="packttip">One of the things to note is that, as with a character-level language model that generates the next character in a sequence, you can build a word-level language model too. However, the character-level language model has an advantage in that it can create its own unique words that are not in the vocabulary we train it on.</div>
<p class="mce-root">Let's now learn how RNN works to conceive the dependencies between characters in sequences. Assume that we only had a vocabulary of four possible letters, [<em class="calibre15">a</em>, <em class="calibre15">p</em>, <em class="calibre15">l</em>, <em class="calibre15">e</em>], and the intent is to train an RNN on the training sequence <em class="calibre15">apple</em>. This training sequence is in fact a source of four separate training examples:</p>
<ul class="calibre9">
<li class="calibre10">The probability of the letter <em class="calibre22">p</em> should be likely, given the context of <em class="calibre22">a, , </em>in other words, the conditional probability of <em class="calibre22">p</em> given the letter <em class="calibre22">a</em> in the word <em class="calibre22">apple</em></li>
<li class="calibre10"><span>Similar to the first point, <em class="calibre22">p</em> should be likely in the context of <em class="calibre22">ap</em></span></li>
<li class="calibre10"><span><span>The <em class="calibre22">letter l</em> should also be likely given the context of</span> <em class="calibre22"><span>app</span></em></span></li>
<li class="calibre10"><span><span>The <em class="calibre22">letter e</em> should be likely given the context of</span> <em class="calibre22">appl</em></span></li>
</ul>
<p class="mce-root">We start to encode each character in the word <em class="calibre15">apple</em> into a vector using 1-of-k encoding. 1-of-k encoding represents each character in the word as all zeros, except for the single 1 at the index of the character in the vocabulary. Each character thus represented with 1-of-k encoding is then fed into the RNN one at a time with the help of a step function. The RNN takes this input and generates a four-dimensional output vectors (one dimension per character, and recollect we only have four characters in our vocabulary). This output vector can be interpreted as the confidence that the RNN currently assigns to each character coming next in the sequence. The following diagram is a visualization of the RNN learning the characters:</p>
<p class="CDPAlignCenter1"><img class="aligncenter116" src="assets/709fd0f0-e868-420e-b67d-2100b54c96f3.png"/></p>
<div class="packtfigref">RNN learning the character language model</div>
<p class="mce-root">In the preceding diagram, we see an RNN with four-dimensional input and output layers. There is also a hidden layer with three neurons. The diagram displays the activations in the forward pass when the RNN is fed with the input of the characters <em class="calibre15">a</em>, <em class="calibre15">p</em>, <em class="calibre15">p</em>, and <em class="calibre15">l</em>. The output layer contains the confidence that the RNN assigned to each of the following characters. The expectation of the RNN is for the green numbers in the output layer to be higher than the red numbers. The high values of green numbers enable the prediction of the right characters as per the input.</p>
<p class="mce-root">We see that in the first time step, when the RNN is fed the input character <em class="calibre15">a</em>, it assigned a confidence of 1.0 to the next letter being <em class="calibre15">a</em>, 2.2 as confidence to letter <em class="calibre15">p</em>, -3.0 to <em class="calibre15">l</em>, and 4.1 to <em class="calibre15">e</em>. As per our training data, the sequence we considered is <em class="calibre15">apple</em>; therefore, the next correct character is <em class="calibre15">p</em> given the character <em class="calibre15">a</em> as input in the first time step. We would like our RNN to maximize the confidence in the first step (indicated in green) and minimize the confidence of all other letters (indicated in red). Likewise, we have a desired output character at each one of the four time steps that we would like our RNN to assign a greater confidence to.</p>
<p class="mce-root">Since the RNN consists entirely of differentiable operations, we can run the backpropagation algorithm to figure out in what direction we should adjust each one of its weights to increase the scores of the correct targets (the <span class="calibre4">bold</span> <span class="calibre4">green numbers).</span></p>
<p class="mce-root">Based on the gradient direction, the parameters are updated and the algorithm actually alters the weight by a tiny amount in the same direction as that of the gradient. Ideally, if gradient decent has successfully run and updated the weights, we would see a slightly higher weight for the right choice and lower weights for the incorrect characters. For example, we would find that the scores of the correct character <em class="calibre15">p</em> in the first time step would be slightly higher, say 2.3 instead of 2.2. At the same time, the scores for the other characters <em class="calibre15">a</em>, <em class="calibre15">l</em>, and <em class="calibre15">e</em> would be observed as lower than that of the score that was assigned prior to gradient descent.</p>
<p class="mce-root">The process of updating the parameters through gradient descent is repeated multiple times in the RNN until the network converges, in other words, until the predictions are consistent with the training data.</p>
<p class="mce-root">Technically speaking, we run the standard softmax classifier, otherwise called the cross-entropy loss, on every output vector simultaneously. The RNN is trained with mini-batch stochastic gradient descent or adaptive learning rate methods such as RMSProp or Adam to stabilize the updates.</p>
<p class="mce-root">You may notice that the first time the character <em class="calibre15">p</em> is input, the output is <em class="calibre15">p</em>; however, the second time the same input is fed, the output is <em class="calibre15">l</em>. An RNN, therefore, cannot rely only on the input that is given. This is where an RNN uses its recurrent connection to keep track of the context to perform the task and make the correct predictions. Without the context, it would have been challenging for the network to predict the right output specifically, given the same input.</p>
<p class="mce-root">When we have to generate text using the trained RNN model, we provide a seed input character to the network and get the distribution over what characters are likely to come next. The distribution is then sampled and fed it right back in, to get the next letter. The process is repeated until the maximum number of characters is reached (until a specific user-defined character length), or until the model encounters an end of line character such as &lt;EOS&gt; or &lt;END&gt;.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the project</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now that we know how an RNN is able to build a character-level model, let's implement the project to generate our own words and sentences through an RNN. Generally, RNN training is computationally intensive and it is suggested that we run the code on a <strong class="calibre3">graphical processing unit</strong> (<strong class="calibre3">GPU</strong>). However, due to infrastructure limitations, we are not going to use a GPU for the project code. The <kbd class="calibre11">mxnet</kbd> library allows a character-level language model with an RNN to be executed on the CPU itself, so let's start coding our project:</p>
<pre class="calibre16"># including the required libraries<br class="title-page-name"/>library("readr")<br class="title-page-name"/>library("stringr")<br class="title-page-name"/>library("stringi")<br class="title-page-name"/>library("mxnet")<br class="title-page-name"/>library("languageR")</pre>
<p class="mce-root">To use the <kbd class="calibre11">languageR</kbd> library's <em class="calibre15">ALICE'S ADVENTURES IN WONDERLAND</em> book text and load it into memory, use the following code:</p>
<pre class="calibre16">data(alice)</pre>
<p class="mce-root">Next, we transform the test into feature vectors that is fed into the RNN model. The <kbd class="calibre11">make_data</kbd> function reads the dataset, cleans it of any non-alphanumeric characters, splits it into individual characters and groups it into sequences of length <kbd class="calibre11">seq.len</kbd>. In this case, <kbd class="calibre11">seq.len</kbd> is set to <kbd class="calibre11">100</kbd>:</p>
<pre class="calibre16">make_data &lt;- function(txt, seq.len = 32, dic=NULL) {<br class="title-page-name"/>  text_vec &lt;- as.character(txt)<br class="title-page-name"/><span>  text_vec &lt;- stri_enc_toascii(str = text_vec)<br class="title-page-name"/></span>  text_vec &lt;- str_replace_all(string = text_vec, pattern = "[^[:print:]]", replacement = "")<br class="title-page-name"/><span>  text_vec &lt;- strsplit(text_vec, '') %&gt;% unlist<br class="title-page-name"/></span>  if (is.null(dic)) {<br class="title-page-name"/>    char_keep &lt;- sort(unique(text_vec))<br class="title-page-name"/>  } else char_keep &lt;- names(dic)[!dic == 0]</pre>
<p class="mce-root"><span class="calibre4">To remove those terms that are not part of dictionary, use the following code:</span></p>
<pre class="calibre16">text_vec &lt;- text_vec[text_vec %in% char_keep]</pre>
<p class="mce-root"><span class="calibre4">To build a dictionary and adjust it by <kbd class="calibre11">-1</kbd> to have a <kbd class="calibre11">1-lag</kbd> for labels, use the following code:</span></p>
<pre class="calibre16"><span>dic &lt;- 1:length(char_keep)<br class="title-page-name"/></span> names(dic) &lt;- char_keep<br class="title-page-name"/> <span># reversing the dictionary<br class="title-page-name"/></span> rev_dic &lt;- names(dic)<br class="title-page-name"/> names(rev_dic) &lt;- dic<br class="title-page-name"/> # Adjust by -1 to have a 1-lag for labels<br class="title-page-name"/> num.seq &lt;- (length(text_vec) - 1) %/% seq.len<br class="title-page-name"/> features &lt;- dic[text_vec[1:(seq.len * num.seq)]]<br class="title-page-name"/> labels &lt;- dic[text_vec[1:(seq.len*num.seq) + 1]]<br class="title-page-name"/> features_array &lt;- array(features, dim = c(seq.len, num.seq))<br class="title-page-name"/> labels_array &lt;- array(labels, dim = c(seq.len, num.seq))<br class="title-page-name"/> return (list(features_array = features_array, labels_array = labels_array, dic = dic, rev_dic<br class="title-page-name"/> = rev_dic))<br class="title-page-name"/> }</pre>
<p class="mce-root">Set the sequence length as <kbd class="calibre11">100</kbd>, then build the long sequence of text from individual words in <kbd class="calibre11">alice</kbd> data character vector. Then call the <kbd class="calibre11">make_data()</kbd> function on the <kbd class="calibre11">alice_in_wonderland</kbd> text file. Observe that <kbd class="calibre11">seq.ln</kbd> and an empty dictionary is passed as input. <kbd class="calibre11">seq.ln</kbd> dictates the context that is the number of characters that the RNN need to look back inorder to generate the next character. During the training <kbd class="calibre11">seq.ln</kbd> is utilized to get the right weights:</p>
<pre class="calibre16">seq.len &lt;- 100<br class="title-page-name"/> alice_in_wonderland&lt;-paste(alice,collapse=" ")<br class="title-page-name"/> data_prep &lt;- make_data(alice_in_wonderland, seq.len = seq.len, dic=NULL)</pre>
<p class="mce-root">To view the prepared data, use the following code:</p>
<pre class="calibre16">print(str(data_prep))</pre>
<p class="mce-root">This will give the following output:</p>
<pre class="calibre16">&gt; print(str(data_prep))<br class="title-page-name"/>List of 4<br class="title-page-name"/><span> $ features_array: int [1:100, 1:1351] 9 31 25 13 17 1 45 1 9 15 ...<br class="title-page-name"/></span> $ labels_array  : int [1:100, 1:1351] 31 25 13 17 1 45 1 9 15 51 ...<br class="title-page-name"/> $ dic           : Named int [1:59] 1 2 3 4 5 6 7 8 9 10 ...<br class="title-page-name"/>  ..- attr(*, "names")= chr [1:59] " " "-" "[" "]" ...<br class="title-page-name"/> $ rev_dic       : Named chr [1:59] " " "-" "[" "]" ...<br class="title-page-name"/>  ..- attr(*, "names")= chr [1:59] "1" "2" "3" "4" ...</pre>
<p class="mce-root">To view the <kbd class="calibre11">features</kbd> array, use the following code:</p>
<pre class="calibre16"># Viewing the feature array<br class="title-page-name"/>View(data_prep$features_array)</pre>
<p class="mce-root">This will give the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter117" src="assets/17b2ca08-f59e-455d-9cf1-370a3bfcf2cd.png"/></p>
<p class="mce-root">To view the <kbd class="calibre11">labels</kbd> array, use the following code:</p>
<pre class="calibre16"># Viewing the labels array<br class="title-page-name"/>View(data_prep$labels_array)</pre>
<p class="mce-root">You will get the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter118" src="assets/b8899b0a-8a08-4677-84d7-18a40051849c.png"/></p>
<p class="mce-root">Now, let's print the dictionary, which includes the unique characters, using the following code:</p>
<pre class="calibre16"># printing the dictionary - the unique characters<br class="title-page-name"/>print(data_prep$dic)</pre>
<p class="mce-root">You will get the following output:</p>
<pre class="calibre16">&gt; print(data_prep$dic)<br class="title-page-name"/>    -  [  ]  *  0  3  a  A  b  B  c  C  d  D  e  E  f  F  g  G  h  H  i  I  j  J  k  K  l  L  m  M  n  N  o  O  p<br class="title-page-name"/> 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38<br class="title-page-name"/> P  q  Q  r  R  s  S  t  T  u  U  v  V  w  W  x  X  y  Y  z  Z<br class="title-page-name"/>39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</pre>
<p class="mce-root">Use the following code to print the indexes of the characters:</p>
<pre class="calibre16"># printing the indexes of the characters<br class="title-page-name"/>print(data_prep$rev_dic)</pre>
<p class="mce-root">This will give the following output:</p>
<pre class="calibre16">  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28<br class="title-page-name"/>" " "-" "[" "]" "*" "0" "3" "a" "A" "b" "B" "c" "C" "d" "D" "e" "E" "f" "F" "g" "G" "h" "H" "i" "I" "j" "J" "k"<br class="title-page-name"/> 29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56<br class="title-page-name"/>"K" "l" "L" "m" "M" "n" "N" "o" "O" "p" "P" "q" "Q" "r" "R" "s" "S" "t" "T" "u" "U" "v" "V" "w" "W" "x" "X" "y"<br class="title-page-name"/> 57  58  59<br class="title-page-name"/>"Y" "z" "Z"</pre>
<p class="mce-root">Use the following code block to fetch the features and labels to train the model, split the data into training and evaluation in a 90:10 ratio:</p>
<pre class="calibre16">X &lt;- data_prep$features_array<br class="title-page-name"/>Y &lt;- data_prep$labels_array<br class="title-page-name"/>dic &lt;- data_prep$dic<br class="title-page-name"/>rev_dic &lt;- data_prep$rev_dic<br class="title-page-name"/>vocab &lt;- length(dic)<br class="title-page-name"/>samples &lt;- tail(dim(X), 1)<br class="title-page-name"/>train.val.fraction &lt;- 0.9<br class="title-page-name"/>X.train.data &lt;- X[, 1:as.integer(samples * train.val.fraction)]<br class="title-page-name"/>X.val.data &lt;- X[, -(1:as.integer(samples * train.val.fraction))]<br class="title-page-name"/>X.train.label &lt;- Y[, 1:as.integer(samples * train.val.fraction)]<br class="title-page-name"/><span>X.val.label &lt;- Y[, -(1:as.integer(samples * train.val.fraction))]<br class="title-page-name"/></span><span>train_buckets &lt;- list("100" = list(data = X.train.data, label = X.train.label))<br class="title-page-name"/></span><span>eval_buckets &lt;- list("100" = list(data = X.val.data, label = X.val.label))<br class="title-page-name"/></span>train_buckets &lt;- list(buckets = train_buckets, dic = dic, rev_dic = rev_dic)<br class="title-page-name"/>eval_buckets &lt;- list(buckets = eval_buckets, dic = dic, rev_dic = rev_dic)</pre>
<p class="mce-root"><span class="calibre4">Use the following code to create iterators for training and evaluation datasets:</span></p>
<pre class="calibre16">vocab &lt;- length(eval_buckets$dic)<br class="title-page-name"/>batch.size &lt;- 32<br class="title-page-name"/><span>train.data &lt;- mx.io.bucket.iter(buckets = train_buckets$buckets, batch.size = batch.size, data.mask.element = 0, shuffle = TRUE)<br class="title-page-name"/></span>eval.data &lt;- mx.io.bucket.iter(buckets = eval_buckets$buckets, batch.size = batch.size,data.mask.element = 0, shuffle = FALSE)</pre>
<p class="mce-root">Create a multi-layer RNN model to sample from character-level language models. It has a one-to-one model configuration since, for each character, we want to predict the next one. For a sequence of length <kbd class="calibre11">100</kbd>, there are also <kbd class="calibre11">100</kbd> labels, corresponding to the same sequence of characters but offset by a position of +1. The parameter's <kbd class="calibre11">output_last_state</kbd> is set to <kbd class="calibre11">TRUE</kbd>, this is to access the state of the RNN cells when performing inference and we can see <kbd class="calibre11">lstm</kbd> cells are used.</p>
<pre class="calibre16">rnn_graph_one_one &lt;- rnn.graph(num_rnn_layer = 3,<br class="title-page-name"/>                               num_hidden = 96,<br class="title-page-name"/>                               input_size = vocab,<br class="title-page-name"/>                               num_embed = 64,<br class="title-page-name"/>                               num_decode = vocab,<br class="title-page-name"/>                               dropout = 0.2,<br class="title-page-name"/>                               ignore_label = 0,<br class="title-page-name"/>                               cell_type = "lstm",<br class="title-page-name"/>                               masking = F,<br class="title-page-name"/>                               output_last_state = T,<br class="title-page-name"/>                               loss_output = "softmax",<br class="title-page-name"/>                               config = "one-to-one")</pre>
<p class="mce-root">Use the following code to visualize the RNN model:</p>
<pre class="calibre16">graph.viz(rnn_graph_one_one, type = "graph",<br class="title-page-name"/>          graph.height.px = 650, shape=c(500, 500))</pre>
<p class="mce-root">The following diagram shows the resultant output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter119" src="assets/51bf4444-de3a-4cff-b28d-5740549bfac6.png"/></p>
<p class="mce-root">Now, use the following line of code to set the CPU as the device to execute the code:</p>
<pre class="calibre16">devices &lt;- mx.cpu()</pre>
<p class="mce-root">Then, initializing the weights of the network through the Xavier initializer:</p>
<pre class="calibre16">initializer &lt;- mx.init.Xavier(rnd_type = "gaussian", factor_type = "avg", magnitude = 3)</pre>
<p class="mce-root">Use the <kbd class="calibre11">adadelta</kbd> optimizer to update the weights in the network through the learning process:</p>
<pre class="calibre16">optimizer &lt;- mx.opt.create("adadelta", rho = 0.9, eps = 1e-5, wd = 1e-8,<br class="title-page-name"/>                           clip_gradient = 5, rescale.grad = 1/batch.size)</pre>
<p class="mce-root">Use the following lines of code to set up logging of metrics and define a custom measurement function:</p>
<pre class="calibre16">logger &lt;- mx.metric.logger()<br class="title-page-name"/>epoch.end.callback &lt;- mx.callback.log.train.metric(period = 1, logger = logger)<br class="title-page-name"/>batch.end.callback &lt;- mx.callback.log.train.metric(period = 50)<br class="title-page-name"/>mx.metric.custom_nd &lt;- function(name, feval) {<br class="title-page-name"/>  init &lt;- function() {<br class="title-page-name"/>    c(0, 0)<br class="title-page-name"/>  }<br class="title-page-name"/><span>  update &lt;- function(label, pred, state) {<br class="title-page-name"/></span>    m &lt;- feval(label, pred)<br class="title-page-name"/>    state &lt;- c(state[[1]] + 1, state[[2]] + m)<br class="title-page-name"/>    return(state)<br class="title-page-name"/>  }<br class="title-page-name"/>  get &lt;- function(state) {<br class="title-page-name"/>    list(name=name, value = (state[[2]] / state[[1]]))<br class="title-page-name"/>  }<br class="title-page-name"/>  ret &lt;- (list(init = init, update = update, get = get))<br class="title-page-name"/>  class(ret) &lt;- "mx.metric"<br class="title-page-name"/>  return(ret)<br class="title-page-name"/>}</pre>
<p class="mce-root"><strong class="calibre3">Perplexity</strong> is a measure of how variable a prediction model is. If perplexity is a measure of prediction error, define a function to compute the error, using the following lines of code:</p>
<pre class="calibre16">mx.metric.Perplexity &lt;- mx.metric.custom_nd("Perplexity", function(label, pred) {<br class="title-page-name"/>  label &lt;- mx.nd.reshape(label, shape = -1)<br class="title-page-name"/>  label_probs &lt;- as.array(mx.nd.choose.element.0index(pred, label))<br class="title-page-name"/>  batch &lt;- length(label_probs)<br class="title-page-name"/>  NLL &lt;- -sum(log(pmax(1e-15, as.array(label_probs)))) / batch<br class="title-page-name"/>  Perplexity &lt;- exp(NLL)<br class="title-page-name"/>  return(Perplexity)<br class="title-page-name"/>}</pre>
<p class="mce-root">Use the following code to execute the model creation and you will see that in this project we are running it for 20 iterations:</p>
<pre class="calibre16">model &lt;- mx.model.buckets(symbol = rnn_graph_one_one,<br class="title-page-name"/>                          train.data = train.data, eval.data = eval.data,<br class="title-page-name"/>                          num.round = 20, ctx = devices, verbose = TRUE,<br class="title-page-name"/>                          metric = mx.metric.Perplexity,<br class="title-page-name"/>                          initializer = initializer,<br class="title-page-name"/>   optimizer = optimizer,<br class="title-page-name"/>                          batch.end.callback = NULL,<br class="title-page-name"/>                          epoch.end.callback = epoch.end.callback)</pre>
<p class="mce-root">This will give the following output:</p>
<pre class="calibre16">Start training with 1 devices<br class="title-page-name"/>[1] Train-Perplexity=23.490355102639<br class="title-page-name"/>[1] Validation-Perplexity=17.6250266989171<br class="title-page-name"/>[2] Train-Perplexity=14.4508382001841<br class="title-page-name"/>[2] Validation-Perplexity=12.8179427398927<br class="title-page-name"/>[3] Train-Perplexity=10.8156810097278<br class="title-page-name"/>[3] Validation-Perplexity=9.95208184606089<br class="title-page-name"/>[4] Train-Perplexity=8.6432934902383<br class="title-page-name"/>[4] Validation-Perplexity=8.21806492033906<br class="title-page-name"/>[5] Train-Perplexity=7.33073759154393<br class="title-page-name"/>[5] Validation-Perplexity=7.03574648385079<br class="title-page-name"/>[6] Train-Perplexity=6.32024660528852<br class="title-page-name"/>[6] Validation-Perplexity=6.1394327776089<br class="title-page-name"/>[7] Train-Perplexity=5.61888374338248<br class="title-page-name"/>[7] Validation-Perplexity=5.59925324885983<br class="title-page-name"/>[8] Train-Perplexity=5.14009899947491]<br class="title-page-name"/>[8] Validation-Perplexity=5.29671693342219<br class="title-page-name"/>[9] Train-Perplexity=4.77963053659987<br class="title-page-name"/>[9] Validation-Perplexity=4.98471501141549<br class="title-page-name"/>[10] Train-Perplexity=4.5523402301526<br class="title-page-name"/>[10] Validation-Perplexity=4.84636357676712<br class="title-page-name"/>[11] Train-Perplexity=4.36693337145912<br class="title-page-name"/>[11] Validation-Perplexity=4.68806078057635<br class="title-page-name"/>[12] Train-Perplexity=4.21294955131918<br class="title-page-name"/>[12] Validation-Perplexity=4.53026345109037<br class="title-page-name"/>[13] Train-Perplexity=4.08935886339982<br class="title-page-name"/>[13] Validation-Perplexity=4.50495393289961<br class="title-page-name"/>[14] Train-Perplexity=3.99260373800419<br class="title-page-name"/>[14] Validation-Perplexity=4.42576079641165<br class="title-page-name"/>[15] Train-Perplexity=3.91330125104996<br class="title-page-name"/>[15] Validation-Perplexity=4.3941619024578<br class="title-page-name"/>[16] Train-Perplexity=3.84730588206837<br class="title-page-name"/>[16] Validation-Perplexity=4.33288830915229<br class="title-page-name"/>[17] Train-Perplexity=3.78711049085869<br class="title-page-name"/>[17] Validation-Perplexity=4.28723362252784<br class="title-page-name"/>[18] Train-Perplexity=3.73198720637659<br class="title-page-name"/>[18] Validation-Perplexity=4.22839393379393<br class="title-page-name"/>[19] Train-Perplexity=3.68292148768833<br class="title-page-name"/>[19] Validation-Perplexity=4.22187018296206<br class="title-page-name"/>[20] Train-Perplexity=3.63728269095417<br class="title-page-name"/>[20] Validation-Perplexity=4.17983276293299</pre>
<p class="mce-root">Next, save the model for later use, then load the model from the disk to infer and sample the text character by character, and finally merge the predicted characters into a sentence using the following code:</p>
<pre class="calibre16">mx.model.save(model, prefix = "one_to_one_seq_model", iteration = 20)<br class="title-page-name"/><span># the generated text is expected to be similar to the training data<br class="title-page-name"/></span><span>set.seed(0)<br class="title-page-name"/></span><span>model &lt;- mx.model.load(prefix = "one_to_one_seq_model", iteration = 20)<br class="title-page-name"/></span><span>internals &lt;- model$symbol$get.internals()<br class="title-page-name"/></span><span>sym_state &lt;- internals$get.output(which(internals$outputs %in% "RNN_state"))<br class="title-page-name"/></span><span>sym_state_cell &lt;- internals$get.output(which(internals$outputs %in% "RNN_state_cell"))<br class="title-page-name"/></span><span>sym_output &lt;- internals$get.output(which(internals$outputs %in% "loss_output"))<br class="title-page-name"/></span><span>symbol &lt;- mx.symbol.Group(sym_output, sym_state, sym_state_cell)</span></pre>
<p class="mce-root"><span class="calibre4">Use the following code to provide the seed character to start the text with:</span></p>
<pre class="calibre16"><span>infer_raw &lt;- c("e")<br class="title-page-name"/></span><span>infer_split &lt;- dic[strsplit(infer_raw, '') %&gt;% unlist]<br class="title-page-name"/></span><span>infer_length &lt;- length(infer_split)<br class="title-page-name"/></span><span>infer.data &lt;- mx.io.arrayiter(data = matrix(infer_split), label = matrix(infer_split), batch.size = 1, shuffle = FALSE)<br class="title-page-name"/></span>infer &lt;- mx.infer.rnn.one(infer.data = infer.data,<br class="title-page-name"/><span>                          symbol = symbol,<br class="title-page-name"/></span><span>                          arg.params = model$arg.params,<br class="title-page-name"/></span><span>                          aux.params = model$aux.params,<br class="title-page-name"/></span><span>                          input.params = NULL,<br class="title-page-name"/></span><span>                          ctx = devices)<br class="title-page-name"/></span><span>pred_prob &lt;- as.numeric(as.array(mx.nd.slice.axis(infer$loss_output, axis = 0, begin = infer_length-1, end = infer_length)))<br class="title-page-name"/></span><span>pred &lt;- sample(length(pred_prob), prob = pred_prob, size = 1) - 1<br class="title-page-name"/></span><span>predict &lt;- c(predict, pred)<br class="title-page-name"/></span><span>for (i in 1:200) {<br class="title-page-name"/></span><span>  infer.data &lt;- mx.io.arrayiter(data = as.matrix(pred), label = as.matrix(pred), batch.size = 1,<br class="title-page-name"/>shuffle = FALSE) <br class="title-page-name"/></span><span>  infer &lt;- mx.infer.rnn.one(infer.data = infer.data,<br class="title-page-name"/></span><span>                            symbol = symbol,<br class="title-page-name"/></span><span>                            arg.params = model$arg.params,<br class="title-page-name"/></span><span>                            aux.params = model$aux.params,<br class="title-page-name"/></span><span>                            input.params = list(rnn.state = infer[[2]],<br class="title-page-name"/></span><span>                            rnn.state.cell = infer[[3]]),<br class="title-page-name"/></span><span>                            ctx = devices)<br class="title-page-name"/></span><span>  pred_prob &lt;- as.numeric(as.array(infer$loss_output))<br class="title-page-name"/></span><span>  pred &lt;- sample(length(pred_prob), prob = pred_prob, size = 1, replace = T) - 1<br class="title-page-name"/></span><span>  predict &lt;- c(predict, pred)<br class="title-page-name"/></span>}</pre>
<p class="mce-root"><span class="calibre4">Use the following lines of code to print the predicted text, after processing the predicted characters and merging them together into one sentence:</span></p>
<pre class="calibre16"><span>predict_txt &lt;- paste0(rev_dic[as.character(predict)], collapse = "")<br class="title-page-name"/></span><span>predict_txt_tot &lt;- paste0(infer_raw, predict_txt, collapse = "")<br class="title-page-name"/></span><span># printing the predicted text<br class="title-page-name"/></span><span>print(predict_txt_tot)</span></pre>
<p class="mce-root">This will give the following output:</p>
<pre class="calibre16">[1] "eNAHare I eat and in Heather where and fingo I ve next feeling or fancy to livery dust a large pived as a pockethion What isual child for of cigstening to get in a strutching voice into saying she got reaAlice glared in a Grottle got to sea-paticular and when she heard it would heard of having they began whrink bark of Hearnd again said feeting and there was going to herself up it Then does so small be THESE said Alice going my dear her before she walked at all can t make with the players and said the Dormouse sir your mak if she said to guesss I hadn t some of the crowd and one arches how come one mer really of a gomoice and the loots at encand something of one eyes purried asked to leave at she had Turtle might I d interesting tone hurry of the game the Mouse of puppled it They much put eagerly"</pre>
<p class="mce-root">We see from the output that our RNN is able to autogenerate text. Of course, the generated text is not very cohesive and it needs some improvement. There are several techniques we could rely upon to improve the cohesion and generate more meaningful text from an RNN. The following are some of these techniques:</p>
<ul class="calibre9">
<li class="calibre10">Implement a word-level language model instead of a character-level language model.</li>
<li class="calibre10"><span>Use a larger RNN network.</span></li>
</ul>
<ul class="calibre9">
<li class="calibre10"><span>In our project, we used LTSM cells to build our RNN. Instead of LSTM cells, we could use GRU cells, which are more advanced.</span></li>
<li class="calibre10"><span>We ran our RNN training for 20 iterations; this may be too little to get the right weights in place. We could try increasing the number of iterations and verifying the RNN yields better predictions.</span></li>
<li class="calibre10"><span>The current model used a dropout of 20%. This can be altered to check the effect on the overall predictions.</span></li>
<li class="calibre10"><span>Our corpus retained very little punctuation; therefore, our model did not have the ability to predict punctuation as characters while generating text. Including punctuation in the corpus on which an RNN gets trained may yield better sentences and word endings.</span></li>
<li class="calibre10"><span>The <kbd class="calibre11">seq.ln</kbd> parameter decides the number of characters that need to be looked up in the history, prior to predicting the next character. In our model, we have set this as 100. This may be altered to check whether the model produces better words and sentences.</span></li>
</ul>
<p class="mce-root">Due to space and time constraints, we are not going to be trying these options in this chapter. One or more of these options may be experimented with by interested readers to produce better words and sentences using a character RNN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">The major theme of this chapter was generating text automatically using RNNs. We started the chapter with a discussion about language models and their applications in the real world. We then carried out an in-depth overview of recurrent neural networks and their suitability for language model tasks. The differences between traditional feedforward networks and RNNs were discussed to get a clearer understanding of RNNs. We then went on to discuss problems and solutions related to the exploding gradients and vanishing gradients experienced by RNNs. After acquiring a detailed theoretical foundation of RNNs, we went ahead with implementing a character-level language model with an RNN. We used <em class="calibre15">Alice's Adventures in Wonderland</em> as a text corpus input to train the RNN model and then generated a string as output. Finally, we discussed some ideas for improving our character RNN model.</p>
<p class="mce-root">How about implementing a project to win more often when playing casino slot machines? This is something we will explore in the last but one chapter of this book. <a href="4b80233e-4fbe-4d90-ba32-5053930433c1.xhtml" class="calibre8">Chapter 9</a> is titled <em class="calibre15">Winning the Casino Slot Machine with Reinforcement Learning</em>. Come on, let's learn to earn free money.</p>


            </article>

            
        </section>
    </body></html>