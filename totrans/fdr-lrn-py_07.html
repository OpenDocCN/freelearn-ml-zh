<html><head></head><body>
		<div id="_idContainer072">
			<h1 id="_idParaDest-171" class="chapter-number"><a id="_idTextAnchor176"/>7</h1>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor177"/>Model Aggregation</h1>
			<p>In the <em class="italic">Model aggregation basics</em> section of <a href="B18369_03.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Workings of the Federated Learning System</em>, we introduced the concept of aggregation within the <strong class="bold">federated learning</strong> (<strong class="bold">FL</strong>) process at a high level. Recall that aggregation is the means by which an FL approach uses the models trained locally by each agent to produce a model with strong global performance. It is clear to see that the strength and robustness of the aggregation method employed are directly correlated to the resulting performance of the end <span class="No-Break">global model.</span></p>
			<p>As a result, choosing the appropriate aggregation method based on the local datasets, agents, and FL system hierarchy is key to achieving good performance with FL. In fact, the focal point of many publications in this field is providing mathematically backed convergence guarantees for these methods in a variety of <span class="No-Break">theoretical scenarios.</span></p>
			<p>The goal of this chapter is to cover some of the research that has been done on aggregation methods and their convergence in both ideal and non-ideal cases, tying these methods to their strengths in the different scenarios that arise in the practical applications of FL. After reading the chapter, you should be able to understand how different characterizations of an FL scenario call for different aggregation methods, and you should have an idea of how these algorithms can actually <span class="No-Break">be implemented.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Revisiting aggregation</span></li>
				<li><span class="No-Break">Understanding FedAvg</span></li>
				<li>Modifying aggregation for <span class="No-Break">non-ideal cases</span></li>
			</ul>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor178"/>Technical requirements</h1>
			<p>The Python algorithm implementations presented in the book can all be found in the <strong class="source-inline">ch7</strong> folder, which is located <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7"><span class="No-Break">https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7</span></a><span class="No-Break">.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can use the code files for personal or educational purposes. Please note that we will not support deployments for commercial use and will not be responsible for any errors, issues, or damages caused by using <span class="No-Break">the code.</span></p>
			<p>For the pure aggregation algorithms, auxiliary code is included to display example output from preset local parameters. The aggregation methods that modify the local training process require an FL system in order to operate – for these, full implementations using STADLE are included. Also, the pure aggregation algorithms can be directly tested with STADLE by configuring the aggregation method. Information on how to run the examples can be found in the associated <span class="No-Break"><strong class="source-inline">README</strong></span><span class="No-Break"> files.</span></p>
			<p>The installation of the <strong class="source-inline">stadle-client</strong> package through <strong class="source-inline">pip</strong> is necessary to run the full FL process examples. The following command can be used to perform <span class="No-Break">this installation:</span></p>
			<p class="source-code">pip install stadle-client</p>
			<p>Using a virtual environment is recommended to isolate the specific package versions installed with <strong class="source-inline">stadle-client</strong> from other installations on <span class="No-Break">the system.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor179"/>Revisiting aggregation</h1>
			<p>To solidly<a id="_idIndexMarker528"/> contextualize aggregation within FL, first, we describe the components of a system that are necessary for FL to <span class="No-Break">be applied:</span></p>
			<ul>
				<li>A set of computational agents that perform the local training portion <span class="No-Break">of FL.</span></li>
				<li>Each agent possesses a local dataset (static or dynamic), of which no portion can be communicated to another agent under the strictest <span class="No-Break">FL scenario.</span></li>
				<li>Each agent possesses a parameterized model that can be trained on the local dataset, a process that produces the local optima parameter set for <span class="No-Break">the model.</span></li>
				<li>A parameter server, or aggregator, which receives the locally trained models at each iteration from the agents and sends back the resulting model produced by the aggregation method chosen to <span class="No-Break">be used.</span></li>
			</ul>
			<p>Every FL communication round can then be broken down into the following <span class="No-Break">two phases:</span></p>
			<ul>
				<li>The <em class="italic">local training phase</em>, where agents train their local models on their local datasets for some number <span class="No-Break">of iterations</span></li>
				<li>The <em class="italic">aggregation phase</em>, where the agents send the resulting trained local models from the previous phase to the aggregator and receive the aggregated model for use as the starting model in the local training phase of the <span class="No-Break">next round.</span></li>
			</ul>
			<p>So, what exactly does it mean for an agent to send a locally trained model during the aggregation phase? The general approach is to use the <em class="italic">parameter sets</em> that define the local models, allowing for some degree of generalization across all models that can be parameterized in such a way. However, a second approach focuses on sending the <em class="italic">local gradients</em> accumulated during the local training when using a gradient-based optimization approach to the aggregator, with the agents updating their models using the received aggregate gradient at the end of the round. While this approach restricts usage to models with gradient-based local training methods, the prevalence of such methods when<a id="_idIndexMarker529"/> training deep learning models has led to a subset of aggregation methods based on gradient aggregation. In this chapter, we choose to frame model aggregation through the lens of the <span class="No-Break">FedAvg algorithm.</span></p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor180"/>Understanding FedAvg</h1>
			<p>In <a href="B18369_03.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Workings of the Federated Learning System</em>, the aggregation algorithm <a id="_idIndexMarker530"/>known as FedAvg was introduced to help clarify the general structure and represent the more abstract concepts discussed earlier with a specific example. FedAvg was used for two reasons: simplicity in the underlying algorithm, and generalizability across more model types than gradient-based approaches. It also benefits from extensive references by researchers, with performance analysis in different theoretical scenarios using FedAvg as a baseline when proposing new aggregation methods. This focus in the research community can most likely be attributed to the fact that the original FedAvg paper was published by the team working at Google that first brought exposure to the concept and benefits of FL. For further reading, this paper can be found <span class="No-Break">at </span><a href="https://arxiv.org/abs/1602.05629?context=cs"><span class="No-Break">https://arxiv.org/abs/1602.05629?context=cs</span></a><span class="No-Break">.</span></p>
			<p>FedAvg is predated by an aggregation approach known as <strong class="bold">Federated Stochastic Gradient Descent</strong> (<strong class="bold">FedSGD</strong>). FedSGD can be viewed as the gradient aggregation <a id="_idIndexMarker531"/>analog of the model parameter averaged performed by FedAvg. In addition, the concept of averaging model parameters was examined prior to FedAvg for parallelized SGD approaches, outside of the context of FL. Essentially, the analysis of these parallelized SGD approaches mirrors the <strong class="bold">Independently and Identically Distributed</strong> (<strong class="bold">IID</strong>) case of FedAvg – this concept will be discussed<a id="_idIndexMarker532"/> later in the section. Regardless, the simplicity, generalizability, and popularity of FedAvg make it a good base to delve deeper into, contextualizing the need for the numerous aggregation approaches discussed in later sections that have built upon or, otherwise, improved <span class="No-Break">on FedAvg.</span></p>
			<p>Previously, FedAvg was only presented as an algorithm that takes models <em class="italic"><img src="image/B18369_07_F05.png" alt=""/></em>with a respective local dataset size of <img src="image/B18369_07_F04.png" alt=""/>, where the sum equals <em class="italic">N</em> and returns: </p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B18369_07_F06.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Figure"> </p>
			<p>As shown in the <em class="italic">Aggregating local models</em> section of <a href="B18369_04.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Federated Learning Server Implementation with Python</em>, <em class="italic">simple-fl</em> uses the<a id="_idIndexMarker533"/> following function to compute a weighted average of the buffered models (models sent from clients during the current round) based on the amount of data used to locally train <span class="No-Break">each model:</span></p>
			<pre class="source-code">
def _average_aggregate(self,
                           buffer: List[np.array],
                           num_samples: List[int]) -&gt; np.array:
        """
        Given a list of models, compute the average model (FedAvg).
        This function provides a primitive mathematical operation.
        :param buffer: List[np.array] - A list of models to be aggregated
        :return: np.array - The aggregated models
        """
        denominator = sum(num_samples)
        # weighted average
        model = float(num_samples[0]) / denominator * buffer[0]
        for i in range(1, len(buffer)):
            model += float(num_samples[i]) / denominator * buffer[i]
        return model</pre>
			<p>The original algorithm does not differ too greatly from this portrayal. The high-level steps of the algorithm are <span class="No-Break">as follows:</span></p>
			<ol>
				<li>The server randomly samples <em class="italic">K * C</em> clients, where <em class="italic">K</em> is the total number of clients and <em class="italic">C</em> is a parameter between 0 <span class="No-Break">and 1.</span></li>
				<li>The selected <em class="italic">K * C</em> clients receive the most recent aggregate model and begin to train the model on their <span class="No-Break">local data.</span></li>
				<li>Each client<a id="_idIndexMarker534"/> sends its locally trained model back to the server after some desired amount of training <span class="No-Break">is completed.</span></li>
				<li>The server computes the parameter-wise arithmetic mean of the received models to compute the newest <span class="No-Break">aggregate model.</span></li>
			</ol>
			<p>Parallels can be immediately drawn between this formal representation and our presentation of the FL process, with <strong class="source-inline">ClientUpdate</strong> performing local training for an agent and the server performing aggregation using the same weighted averaging algorithm. One important point is the sampling of a subset of clients to perform the local training and model transmission during each round, allowing for client subsampling parameterized by C. This parameter is included to experimentally determine the convergence rates of various client set sizes – in an ideal scenario, this will be set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">.</span></p>
			<p>As previously mentioned, FedAvg is the ideal FL scenario that essentially mirrors an approach to parallelized stochastic <a id="_idIndexMarker535"/>gradient descent. In <strong class="bold">parallelized SGD</strong> (<strong class="bold">pSGD</strong>), the goal is to leverage hardware parallelization (for example, running on multiple cores in parallel) in order to speed up SGD convergence on a specific machine learning task. One approach for this task is for each core to train a base model on some subset of the data in parallel for some number of iterations, then aggregate the partially trained models and use the aggregated models as the next base for training. In this case, if the cores are considered to be agents in an FL scenario, the parallelized SGD approach is the same as FedAvg in an ideal scenario. This means all of the convergence guarantees and respective analyses that were done for pSGD can be directly applied to FedAvg, assuming the ideal FL scenario. From this prior work, it has, therefore, been shown that FedAvg demonstrates strong <span class="No-Break">convergence rates.</span></p>
			<p>After all this praise for FedAvg, it is only natural to question why more complex aggregation methods are even necessary. Recall that the phrase “ideal FL scenario” was used several times when discussing FedAvg convergence. The unfortunate reality is that most practical FL<a id="_idIndexMarker536"/> applications will fail to meet one or more of the conditions stipulated by <span class="No-Break">that phrase.</span></p>
			<p>The ideal FL scenario can be broken down into three <span class="No-Break">main conditions:</span></p>
			<ul>
				<li>The local datasets used for training are IID (the datasets are independently drawn from the same <span class="No-Break">data distribution).</span></li>
				<li>The computational agents are relatively homogeneous in <span class="No-Break">computational power.</span></li>
				<li>All agents can be assumed to <span class="No-Break">be non-adversarial.</span></li>
			</ul>
			<p>At a high level, it is clear why these qualities would be desirable in an FL scenario. To understand, in greater detail, why these three conditions are necessary, the performance of <a id="_idIndexMarker537"/>FedAvg in the absence of each condition will be examined in the <span class="No-Break">upcoming subsections.</span></p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor181"/>Dataset distributions</h2>
			<p>To examine<a id="_idIndexMarker538"/> FedAvg in the non-IID case, first, it is important to<a id="_idIndexMarker539"/> define what exactly is being referred to by the distribution of a dataset. In classification problems, the data distribution often refers to the distribution of the true classes associated with each data point. For example, consider the MNIST dataset, where each image is a handwritten digit from 0 to 9. If a uniform random sample of 1,000 images was to be taken from the dataset, the expected number of images from each class would be the same – this could be considered a uniform data distribution. Alternatively, a sample with 910 images of the digit 0 along with 10 images of the other digits would be a heavily skewed <span class="No-Break">data distribution.</span></p>
			<p>To generalize outside of classification tasks, this definition can be extended to refer to the distribution of <em class="italic">features</em> present across the dataset. These features could be manually crafted and provided to the model (such as linear regression), or they could be extracted from the raw data as part of the model pipeline (such as deep CNN models). For classification problems, the class distribution is generally contained within the feature distribution, due to the implicit belief that the features are sufficient for correctly predicting the class. The benefit of looking at feature distributions is the data-centric focus on features (versus the task-centric focus on classes), allowing for generalization across machine <span class="No-Break">learning tasks.</span></p>
			<p>However, in the context of experimental analysis, the ability to easily construct non-IID samples from a dataset makes classification tasks ideal for testing the robustness of FedAvg and different aggregation methods within an FL context. To examine FedAvg in this section, consider<a id="_idIndexMarker540"/> a toy FL scenario where each agent trains a CNN on<a id="_idIndexMarker541"/> data samples taken from the MNIST dataset described earlier. There are two main cases, which are <span class="No-Break">detailed next.</span></p>
			<h3>IID case</h3>
			<p>The convergence <a id="_idIndexMarker542"/>of the models can be represented <a id="_idIndexMarker543"/>through the use of the model parameter space. The parameter space of a model with <em class="italic">n</em> parameters can be thought of as an <em class="italic">n</em>-dimensional Euclidean space, where each parameter corresponds to one dimension in the space. Consider an initialized model; the initial parameters of this model can then be represented as a <em class="italic">point</em> in the parameter space. As local training and aggregation occur, this representative point will move in the parameter space, with the end goal being convergence to a point in the space corresponding to a local optimum of the loss or error function <span class="No-Break">being minimized.</span></p>
			<p>One key point of these functions is the dependence on the data used during the local training process – when the datasets across the agents are IID, there is a general tendency for the optima of the respective loss/error functions to be relatively close in the parameter space. Consider a trivial case where the datasets are IID and all models are initialized with the same parameters. As shown in the <em class="italic">Model aggregation basics </em>section of <a href="B18369_03.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Workings of the Federated Learning System</em>, a simplified version of the parameter space can <span class="No-Break">be depicted:</span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B18369_07_01.jpg" alt="Figure 7.1 – Models with the same initialization and IID datasets&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Models with the same initialization and IID datasets</p>
			<p>Observe how both<a id="_idIndexMarker544"/> models start at the same point (purple x) and <a id="_idIndexMarker545"/>move toward the same optima (purple dot), resulting in aggregate models close to the optima shared by <span class="No-Break">both models.</span></p>
			<p>Due to the resulting similarity of the error/loss functions across the agents, the models tend to converge toward the same or similar optima in the space during training. This means that the change in the models after each aggregation step is relatively small, resulting in the convergence rates mirroring the single local model case. If the underlying data distribution is representative of the true data distribution (for example, uniform across the 10 different digits for MNIST), the resulting aggregated model will demonstrate <span class="No-Break">strong performance.</span></p>
			<p>Next, consider the generalized IID case where each model is <span class="No-Break">initialized separately:</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B18369_07_02.jpg" alt="Figure 7.2 – Models with different initializations and IID datasets&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Models with different initializations and IID datasets</p>
			<p>In this scenario, observe<a id="_idIndexMarker546"/> how both models start at different<a id="_idIndexMarker547"/> points (bold/dotted <em class="italic">x</em>) and initially move toward different optima, producing a poor first model. However, after the first aggregation, both models start at the same point and move toward the same optima, resulting in similar convergence to the <span class="No-Break">first case.</span></p>
			<p>It should be clear that this reduces to the previous case after the first aggregation step since each model starts the second round with the resulting aggregated parameters. As a result, the convergence properties previously stated can be extended to the general case <a id="_idIndexMarker548"/>of FedAvg<a id="_idIndexMarker549"/> with IID <span class="No-Break">local datasets.</span></p>
			<h3>Non-IID Case</h3>
			<p>The key property <a id="_idIndexMarker550"/>in the IID local dataset case that <a id="_idIndexMarker551"/>allows for convergence speeds mirroring the single model case is the similarity of the local optima of the loss/error functions, due to their construction from similar data distributions. In the non-IID case, similarity in the optima is generally no <span class="No-Break">longer observed.</span></p>
			<p>Using the MNIST example, let’s consider an FL scenario with two agents such that the first agent only has images with digits 0 to 4 and the second agent only has images with digits 5 to 9; that is, the datasets are not IID. These datasets would essentially lead to two completely different five-class classification tasks at the local training level, as opposed to the original 10-class classification problem – this will result in completely different parameter space optima between the first agent and the second agent. Consider the simplified representation of this parameter space as follows, with both models having the <span class="No-Break">same initialization:</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B18369_07_03.jpg" alt="Figure 7.3 – Models with different initializations and non-IID datasets&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Models with different initializations and non-IID datasets</p>
			<p>Now that optima are no longer shared (triangles/squares representing optima for bold/dotted <a id="_idIndexMarker552"/>model, respectively), even repeated <a id="_idIndexMarker553"/>aggregations cannot create an aggregate model close to optima of either model. The models diverge, or drift, during each local training phase due to the different target optima in <span class="No-Break">each round.</span></p>
			<p>Only a small subset of the optima will be shared between the loss/error functions of both agents. As a result, there is a high probability that each model will move toward optima that are not shared during local training, leading the models to drift apart in the parameter space. Each aggregation step will then pull the models toward the wrong optima, reverting the progress made during local training and hampering convergence. Note that just taking the average of optima from different agents is very unlikely to be near optima from any of the agents in the parameter space, so in this case, the result of continued aggregation is generally a model that performs poorly across the whole dataset. Convergence to a shared optimum might eventually occur due to stochasticity observed during local training, inducing movement of the aggregate model in the parameter space, but this does not have theoretical guarantees and will be far slower than convergence in the IID case when it <span class="No-Break">does occur.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This MNIST example is a theoretical extreme of non-IID datasets. In practice, non-IID datasets might refer to different skews in the data distributions across agents (for example, twice as many images with digits 0–4 versus 5–9, and vice versa). The severity of the difference is correlated to the performance of FedAvg, so adequate performance can still be reached in less severe cases. However, in these cases, the performance of FedAvg will generally always be inferior to the analogous centralized training task where a single model is trained on all of the local datasets at once – the theoretically optimal model achievable <span class="No-Break">by FL.</span></p>
			<p>While this section<a id="_idIndexMarker554"/> focused on the statistical basis for <a id="_idIndexMarker555"/>the issues that arise from non-IID datasets, the next section examines a far more direct problem that can arise – especially when deploying at <span class="No-Break">larger scales.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor182"/>Computational power distributions</h2>
			<p>An unstated <a id="_idIndexMarker556"/>assumption of the agents <a id="_idIndexMarker557"/>participating in FL is that each agent is capable of performing local training if given infinite time. Agents with limited computational power (memory and speed) might take significantly more time than other agents to finish local training, or they might require techniques such as quantization to support the model and training process. However, agents that cannot complete local training during some rounds will trivially prevent convergence by stalling the <span class="No-Break">FL process.</span></p>
			<p>Generally, convergence bounds and experimental results focus on the number of communication rounds required to reach some level of performance. Under this metric and the aforementioned assumption, convergence is completely independent of the computational power afforded to each agent, since computational power only affects the actual time necessary to complete one round. However, convergence speed in practical applications is measured by the actual time taken, not the number of completed communication rounds – this means that the time to complete each round is as important as the number of rounds. This metric of the total time taken is where naïve FedAvg demonstrates poor performance when heterogeneous computation power is observed in the agents participating <span class="No-Break">in FL</span></p>
			<p>Specifically, the time to complete each round is bottlenecked by the local training time of the slowest agent participating in the round; this is because aggregation is trivially fast compared to training in most cases and must wait for all agents to complete local training. When all agents are participating in the round, this bottleneck becomes the slowest overall agent. In the homogeneous computational power case, the difference in local training time between the fastest agent and the slowest agent will be relatively insignificant. In the heterogeneous case, a single straggler will greatly reduce the convergence time of FedAvg and lead to significant idle time in the faster agents waiting to receive the <span class="No-Break">aggregated model.</span></p>
			<p>Two modifications<a id="_idIndexMarker558"/> to FedAvg with full agent participation <a id="_idIndexMarker559"/>might initially seem to address this problem; however, both have drawbacks that lead to <span class="No-Break">suboptimal performance:</span></p>
			<ul>
				<li>One approach is to rely on agent subsampling in each round, leading to the probability of the straggler effect occurring in each round depending on the number of agents and the sample size taken in each round. This can be sufficient in cases with only a few straggling agents, but it becomes proportionately worse as this number increases and does not completely eliminate the problem from occurring. In addition, small sample sizes lose out on the robustness benefits from aggregation over <span class="No-Break">more agents.</span></li>
				<li>A second approach is to allow all agents to begin local training at the beginning of each round and prematurely begin aggregation after some number of models have been received. This method has the benefit of being able to completely eliminate the straggler effect without greatly restricting the number of agents participating in aggregation during each round. However, it results in the slowest agents never participating in aggregation over all rounds, essentially reducing the number of active agents and potentially limiting the variety of data used during training. In addition, the agents that are too slow to participate in aggregation will have done computational work for <span class="No-Break">no benefit.</span></li>
			</ul>
			<p>It is clear that some local adjustment based on available computational power is necessary for aggregation to be performed efficiently, regardless of the end aggregation method applied to the received models at the end of <span class="No-Break">each round.</span></p>
			<p>Both the non-IID case and the heterogeneous computation power case focus on properties of an FL system that are generally easy to observe and under some level of administrative <a id="_idIndexMarker560"/>control. The next case we present deviates <a id="_idIndexMarker561"/>from this by challenging a key assumption when considering practical <span class="No-Break">FL systems.</span></p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor183"/>Protecting against adversarial agents</h2>
			<p>So far, it has been <a id="_idIndexMarker562"/>assumed that every agent <a id="_idIndexMarker563"/>participating in an FL scenario always acts in the desired way; that is, actively and correctly training the received model locally and participating in model transmission to/from the aggregator. This is easily achieved in a research setting, where the federated setting is simulated and the agents are singularly controlled; however, this assumption of agents behaving correctly does not always hold <span class="No-Break">in practice.</span></p>
			<p>One example that does not involve targeted malicious intent is an error in the model weights being transmitted by an agent to the aggregator. This can happen when the dataset used by an agent is flawed or the training algorithm is incorrectly implemented (the corruption of the parameter data during transmission is also possible). In the worst case, this can essentially lead to the parameters of one or many models being statistically equivalent to random noise. When the L2 norm (the extension of vector magnitude for <em class="italic">n</em>-dimensional tensors) of the random noise is not significantly greater than that of the valid models, FedAvg will suffer performance loss that is proportional to the ratio of faulty agents to all agents – which is relatively acceptable when this ratio is small. However, even a single faulty agent can induce a near-random aggregate model if the norm of the agent’s noise is significantly high. This is due to the nature of the arithmetic mean being performed internally during the <span class="No-Break">FedAvg aggregation.</span></p>
			<p>The problem becomes worse when agents can be controlled by malicious adversaries. A single malicious agent with sufficient information is capable of producing any desired model after aggregation through large modifications to the parameters of the model it submits. Even without direct knowledge of the model parameters and associated weights of the other agents, a malicious agent can leverage relatively small changes between the local models and the aggregate model in later rounds to use the previous aggregate model as an estimate of the expected local <span class="No-Break">model parameters.</span></p>
			<p>Therefore, FedAvg offers little to no robustness against both random and controlled adversarial agents in an FL setting. While one potential means of mitigation would be to separately monitor the agents and prevent adversarial agents from transmitting models, significant damage to the convergence of the final model might have already occurred in the time necessary to identify <span class="No-Break">such agents.</span></p>
			<p>It should now be clear that FedAvg trades robustness in these non-ideal cases for simplicity in the calculation. Unfortunately, this robustness is a key consideration for practical <a id="_idIndexMarker564"/>applications of FL due to the lack of <a id="_idIndexMarker565"/>control compared to the research setting. The next section focuses on methods of achieving robustness against the three non-ideal cases presented in <span class="No-Break">this section.</span></p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor184"/>Modifying aggregation for non-ideal cases</h1>
			<p>In practical FL <a id="_idIndexMarker566"/>applications, at least one of the aforementioned assumptions that constitute an ideal FL scenario generally does not hold; therefore, the usage of alternative aggregation methods might be necessary to best perform FL. The goal of this section is to cover examples of aggregation methods that target heterogeneous computational power, adversarial agents, and non-IID datasets, in order <span class="No-Break">of difficulty.</span></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor185"/>Handling heterogeneous computational power</h2>
			<p>As mentioned<a id="_idIndexMarker567"/> earlier, the<a id="_idIndexMarker568"/> ideal aggregation approach, in this case, consistently avoids the straggler effect while maximizing the number of agents participating in FL and allowing all agents to contribute to some extent, regardless of computational power differences. Agents become stragglers during a round when their local training takes significantly more time than the majority of the agents. Therefore, effectively addressing this problem actually requires some level of adaptability at the agent level in the local training <a id="_idIndexMarker569"/>process, based <a id="_idIndexMarker570"/>on the computational power available to <span class="No-Break">each agent.</span></p>
			<h3>Manual adjustment</h3>
			<p>One straightforward<a id="_idIndexMarker571"/> way of accomplishing this is to change the number of local training iterations based on the time necessary for each iteration. In other words, the local training time is fixed and each agent performs as many iterations as possible within this time, as opposed to performing a fixed number of iterations. This trivially eliminates the straggler problem but might result in poor performance if a large amount of local training time must be allocated for the slow agents to meaningfully contribute due to the model drift from faster agents potentially performing too many local training iterations. This can be mitigated by setting a maximum number of local training iterations. However, a careful balance in the allocated local training must be found to have enough time for slow agents to produce adequate models while preventing faster agents from sitting idle after reaching the maximum number of iterations. It is also unclear how such a threshold could be preemptively determined to achieve optimal performance instead of relying on experimental results to search for the <span class="No-Break">best configuration.</span></p>
			<h3>Automatic adjustment – FedProx</h3>
			<p>An aggregation <a id="_idIndexMarker572"/>method known<a id="_idIndexMarker573"/> as FedProx follows this same methodology of dynamically adjusting the local training processes for each agent based on computational power, while also revising the termination condition for local training to aid in the theoretical analysis of convergence. Specifically, the fixed number of local training iterations is replaced by a termination condition for the training loop that accommodates agents with varying levels of <span class="No-Break">computational power.</span></p>
			<p>The underlying concept for this termination condition is the γ-inexact solution, which is satisfied when the magnitude of the gradient at the γ-inexact optima is less than γ times the magnitude of the gradient at the beginning of local training. Intuitively, γ is a value between 0 and 1, with values closer to 0 leading to more local training iterations due to the stricter termination condition. Therefore, γ allows for the parameterization of an agent’s <span class="No-Break">computational power.</span></p>
			<p>One potential problem with the termination condition approach is the divergence of the locally trained model from the aggregate model after many iterations of local training resulting from a strict condition. To combat this, FedProx adds a proximal term to the objective function being minimized equal to <span class="No-Break">the following:</span></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B18369_07_F01.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/B18369_07_F02.png" alt=""/> represents the received aggregate <span class="No-Break">model weights.</span></p>
			<p>The proximal term penalizes differences between the current weights and the aggregated model weights, restricting the aforementioned local model divergence with the strength parameterized by μ. From these two concepts, FedProx allows for a variable number of iterations proportional to the computational power to be performed by each agent without requiring manually tuned iteration counts for each agent or a set amount of allocated training time. Because of the addition of the proximal term, FedProx requires gradient-based optimization methods to be employed in order to work – more information <a id="_idIndexMarker574"/>on the underlying<a id="_idIndexMarker575"/> theory and comparison to FedAvg can be found in the original paper (which is <span class="No-Break">at </span><a href="https://arxiv.org/abs/1812.06127"><span class="No-Break">https://arxiv.org/abs/1812.06127</span></a><span class="No-Break">).</span></p>
			<h3>Implementing FedProx</h3>
			<p>Because the<a id="_idIndexMarker576"/> modifications <a id="_idIndexMarker577"/>made by FedProx to FedAvg are all on the client side, the actual implementation of FedProx consists entirely of modifications to the local training framework. Specifically, FedProx involves a new termination condition for local training and the addition of a constraining term to the local loss function. Therefore, it is helpful to use an example of the local training code to frame exactly how FedProx can <span class="No-Break">be integrated.</span></p>
			<p>Let’s consider the following generic training code <span class="No-Break">using PyTorch:</span></p>
			<pre class="source-code">
agg_model = ... # Get aggregate model – abstracted out of example
model.load_state_dict(agg_model.state_dict())
for epoch in range(num_epochs):
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()</pre>
			<p>Let this be the code that performs <strong class="source-inline">num_epochs</strong> epochs of training on the local dataset using the received aggregate model for each round. The first necessary modification for FedProx is to <a id="_idIndexMarker578"/>replace the fixed<a id="_idIndexMarker579"/> number of epochs with a dynamic termination condition, checking whether a γ-inexact solution has been found with the aggregated model as the initial model. To do this, the total gradient over the entire training dataset for the aggregate model and the current local model must be stored – this can be performed <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
agg_model = ... # Get aggregated model from aggregator
model.load_state_dict(agg_model.state_dict())
agg_grad = None
curr_grad = None
gamma = 0.9
mu = 0.001</pre>
			<p>Values for the two FedProx parameters, <strong class="source-inline">gamma</strong> and <strong class="source-inline">mu</strong>, are set, and variables to store the gradients of both the aggregate model and the latest local model <span class="No-Break">are defined.</span></p>
			<p>We then define the γ-inexact new termination condition for local training using these <span class="No-Break">gradient variables:</span></p>
			<pre class="source-code">
def gamma_inexact_solution_found(curr_grad, agg_grad, gamma):
    if (curr_grad is None):
        return False
    return curr_grad.norm(p=2) &lt; gamma * agg_grad.norm(p=2)</pre>
			<p>This condition is now checked before each training loop iteration to determine when to stop local training. The <strong class="source-inline">total_grad</strong> variable is created to store the cumulative gradients that <a id="_idIndexMarker580"/>were created<a id="_idIndexMarker581"/> from each minibatch <span class="No-Break">during backpropagation:</span></p>
			<pre class="source-code">
model.train()
while (not gamma_inexact_solution_found(curr_grad, agg_grad, gamma)):
    total_grad = torch.cat([torch.zeros_like(param.data.flatten()) for param in model.parameters()])
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)</pre>
			<p>To compute the proximal term, the weights of both the aggregate model and the latest local model are computed. From these weights, the proximal term is computed and added to the <span class="No-Break">loss term:</span></p>
			<pre class="source-code">
        curr_weights = torch.cat([param.data.flatten() for param in model.parameters()])
        agg_weights = torch.cat([param.data.flatten() for param in agg_model.parameters()])
        prox_term = mu * torch.norm(curr_weights - agg_weights, p=2)**2
        loss += prox_term</pre>
			<p>The gradients are computed and added to the cumulative sum stored <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">total_grad</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
        loss.backward()
        grad = torch.cat([param.grad.flatten() for param in model.parameters()])
        total_grad += grad
        optimizer.step()</pre>
			<p>Finally, we<a id="_idIndexMarker582"/> update <strong class="source-inline">agg_grad</strong> (if the<a id="_idIndexMarker583"/> gradients were computed with the aggregate weights) and <strong class="source-inline">curr_grad</strong> after the current local training iteration <span class="No-Break">is completed:</span></p>
			<pre class="source-code">
    if (agg_grad == None):
        agg_grad = total_grad
    curr_grad = total_grad</pre>
			<p>These modifications allow for FedProx to be implemented on top of FedAvg. The full FL example using FedProx can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedprox_example"><span class="No-Break">https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedprox_example</span></a><span class="No-Break">.</span></p>
			<p>An auxiliary approach to handle the heterogeneous computational power scenario that helps with computational efficiency when only mild heterogeneity is observed is the idea of <em class="italic">compensation in aggregation</em>. Consider the case where aggregation occurs once the number of received models surpasses some threshold (generally, this is less than the number of participating agents). Using this threshold allows the straggler effect to be mitigated; however, the work done by slower agents ends up being discarded each round, leading to <span class="No-Break">training inefficiency.</span></p>
			<p>The core idea of compensation is to allow for the local training that is done by a slower agent in one round to instead be included in the model aggregation of a subsequent round. The age of the model is compensated for in this subsequent round by multiplying the weight used for the weighted average and a penalizing term during aggregation. By doing so, slower agents can be given two or three times as much training time as that used by the faster agents while avoiding the straggler effect. Mild heterogeneity is required in order to prevent cases where slower agents require too much extra time for training. This is due to the associated penalty given to the model after many rounds have passed; it will be severe enough to effectively lead to no contribution and reduce aggregation without compensation – this is necessary to prevent models that are too old from hampering the convergence of the <span class="No-Break">aggregate model.</span></p>
			<p>Finally, we examine methods that help to address the third non-ideal property, where some subset of<a id="_idIndexMarker584"/> agents are<a id="_idIndexMarker585"/> controlled by an adversary or are, otherwise, behaving in an <span class="No-Break">undesirable way.</span></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor186"/>Adversarial agents</h2>
			<p>In the previous<a id="_idIndexMarker586"/> section, it was shown that the <a id="_idIndexMarker587"/>core problem with FedAvg in the presence of adversarial agents was the lack of robustness to outliers in the underlying arithmetic mean used during aggregation. This naturally raises the question of whether this mean can be estimated in such a manner that does offer such robustness. The answer is the class of robust mean estimators. There are many such estimators that offer varying trade-offs between robustness, distance from the true arithmetic mean, and <span class="No-Break">computational efficiency.</span></p>
			<p>For use as a base for the implementation of the following aggregation methods, consider the following general <span class="No-Break">aggregation function:</span></p>
			<pre class="source-code">
def aggregate(parameter_vectors):
    # Perform some form of aggregation
    return aggregated_parameter_vector</pre>
			<p>This function takes a list of parameter vectors and returns the resulting aggregated <span class="No-Break">parameter vector.</span></p>
			<p>Now we will examine three example implementations of robust <span class="No-Break">mean estimators.</span></p>
			<h3>Aggregation using the geometric median</h3>
			<p>The geometric <a id="_idIndexMarker588"/>median of a sample<a id="_idIndexMarker589"/> is the point minimizing the sum of L1 distances <a id="_idIndexMarker590"/>between itself and the sample. This is conceptually similar to the arithmetic mean, which is the point minimizing the sum of L2 distances between itself and the sample. The use of L1 distances allows for greater robustness to outliers; in fact, an arbitrary point can only be induced in the geometric median if at least half of the points are from adversarial agents. However, the geometric median cannot be directly computed, instead relying on numerical approximations or iterative algorithms <span class="No-Break">to compute.</span></p>
			<p>To compute the geometric mean iteratively, Weiszfeld’s algorithm can be used <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
def geometric_median_aggregate(parameter_vectors, epsilon):
    vector_shape = parameter_vectors[0].shape
    vector_buffer = list(v.flatten() for v in parameter_vectors)
    prev_median = np.zeros(vector_buffer[0].shape)
    delta = np.inf
    vector_matrix = np.vstack(vector_buffer)
    while (delta &gt; epsilon):
        dists = np.sqrt(np.sum((vector_matrix - prev_median[np.newaxis, :])**2, axis=1))
        curr_median = np.sum(vector_matrix / dists[:, np.newaxis], axis=0) / np.sum(1 / dists)
        delta = np.linalg.norm(curr_median - prev_median)
        prev_median = curr_median
    return prev_median.reshape(vector_shape)</pre>
			<p>This algorithm uses the fact that the geometric median of a set of points is the point that minimizes the sum of Euclidean distances over the set, performing a form of weighted least<a id="_idIndexMarker591"/> squares with<a id="_idIndexMarker592"/> weights<a id="_idIndexMarker593"/> inversely proportional to the Euclidean distance between the point and the current median estimate at <span class="No-Break">each iteration.</span></p>
			<h3>Aggregation using the coordinate-wise median</h3>
			<p>The<a id="_idIndexMarker594"/> coordinate-wise median is<a id="_idIndexMarker595"/> constructed by taking the <a id="_idIndexMarker596"/>median of each coordinate across the sample, as the name suggests. This median can be directly computed, unlike the geometric median, and intuitively offers similar robustness to outliers due to the properties of the median in univariate statistics. However, it is unclear whether the resulting model displays any theoretical similarities to the arithmetic mean in regard to performance on the dataset <span class="No-Break">and convergence.</span></p>
			<p>NumPy makes the implementation of this function quite simple, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
def coordinate_median_aggregate(parameter_vectors):
    return np.median(parameter_vectors, axis=0)</pre>
			<p>It is clear that the coordinate-wise median is far more computationally efficient to compute than the geometric median, trading off theoretical guarantees <span class="No-Break">for speed.</span></p>
			<h3>Aggregation using the Krum algorithm</h3>
			<p>An alternative<a id="_idIndexMarker597"/> approach is to isolate<a id="_idIndexMarker598"/> outlier <a id="_idIndexMarker599"/>points from adversarial agents prior to aggregation. The most well-known example of this approach is<a id="_idIndexMarker600"/> the <strong class="bold">Krum algorithm</strong>, where distance-based scoring is performed prior to aggregation as a means of locating <span class="No-Break">outlier points.</span></p>
			<p>Specifically, the Krum algorithm first computes the pairwise L2 distance between each point – these distances are then used to compute a score for each point equal to the sum of the <em class="italic">n-f-2</em> smallest L2 distances (<em class="italic">f</em> is a parameter that is set). Then, Krum outputs the received point with the lowest score, effectively returning the point with a minimal total L2 distance with <em class="italic">f</em> outlier points that are ignored. Alternatively, the scoring approach used by Krum can be used to trim outlier points prior to the computation of the arithmetic mean. In both cases, for sufficiently large <em class="italic">n</em> and <em class="italic">2f+2 &lt; n</em>, convergence rates similar to those of FedAvg in the non-adversarial case are achieved. More information on the Krum algorithm can be found in the original paper, which is located <span class="No-Break">at </span><a href="https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html"><span class="No-Break">https://papers.nips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html</span></a><span class="No-Break">.</span></p>
			<p>The Krum <a id="_idIndexMarker601"/>algorithm can be used to <a id="_idIndexMarker602"/>perform<a id="_idIndexMarker603"/> aggregation <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
def krum_aggregate(parameter_vectors, f, use_mean=False):
    num_vectors = len(parameter_vectors)
    filtered_size = max(1, num_vectors-f-2)
    scores = np.zeros(num_vectors)
    for i in range(num_vectors):
        distances = np.zeros(num_vectors)
        for j in range(num_vectors):
            distances[j] = np.linalg.norm(parameter_vectors[i] - parameter_vectors[j])
        scores[i] = np.sum(np.sort(distances)[:filtered_size])
    if (use_mean):
        idx = np.argsort(scores)[:filtered_size]
        return np.mean(np.stack(parameter_vectors)[idx], axis=0)
    else:
        idx = np.argmin(scores)
        return parameter_vectors[idx]</pre>
			<p>Note that a flag has been included to determine which of the two Krum aggregation approaches (single selection versus trimmed mean) should be used. Vectorizing the distance computation is possible, but the iterative approach was preferred due to the expectation <a id="_idIndexMarker604"/>of large parameter <a id="_idIndexMarker605"/>vectors <a id="_idIndexMarker606"/>and smaller <span class="No-Break">agent counts.</span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor187"/>Non-IID datasets</h2>
			<p>The <a id="_idIndexMarker607"/>theoretical <a id="_idIndexMarker608"/>underpinning granted to FL by working with IID datasets plays a significant role in allowing performant aggregate models to be achieved through FL. At a high level, this can be explained by the discrepancy between the learning done by models in different datasets. No theoretical guarantees can be made for the convergence of such models when dataset-agnostic aggregation methods are applied –  unless constraints on the non-IID nature of the datasets are applied. The key hindering factor is the high probability of local models moving toward non-shared optima in the parameter space, leading to consistent drift between the local models and the aggregate model after each local <span class="No-Break">training phase.</span></p>
			<p>There are methods that attempt to restrict the modifications made to the aggregate model based on the local machine learning task, relying on the overparameterization of deep learning models to find relatively disjointed parameter subsets to optimize the aggregate model of each task. One such aggregation<a id="_idIndexMarker609"/> approach is <strong class="bold">FedCurv</strong>, which uses the Fisher information matrix of the previous aggregate model to act as a regulator for auxiliary parameter modifications during local training. However, the robustness of this approach for extreme non-IID cases in practical applications likely needs to be tested further to <a id="_idIndexMarker610"/>ensure <span class="No-Break">acceptable performance.</span></p>
			<h3>Implementing FedCurv</h3>
			<p>The implementation <a id="_idIndexMarker611"/>of FedCurv involves two key <a id="_idIndexMarker612"/>modifications to the standard FedAvg approach. First, the local loss function must be modified to include the regularization term incorporating the aggregated Fisher information from the previous round. Second, the Fisher information matrix of the parameters must be calculated and aggregated correctly for use in the <span class="No-Break">next round.</span></p>
			<p>The local training example code, as shown in the <em class="italic">Implementing FedProx</em> section, will be used again to demonstrate an implementation of FedCurv. In <a href="B18369_04.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Federated Learning Server Implementation with Python</em>, we saw that a model conversion layer allows for framework-agnostic model representations to be operated on by the aggregator. Previously, these representations only contained the respective parameters from the original models; however, this agnostic representation actually allows for any desired parameter to be aggregated, even those only loosely tied to the true model parameters. This means that the secondary parameters can be bundled and sent with the local model, aggregated, and then separated from the aggregate model in the <span class="No-Break">next round.</span></p>
			<p>In FedCurv, there are two sets of parameters that must be computed locally and aggregated for use in the next round; therefore, it can be assumed that these parameters are sent with the local model after training and separated from the aggregate model before training, for the sake of brevity in the example code (the implementation of this functionality is straightforward). As a result, the two key modifications for FedCurv, as mentioned earlier, can be simplified down into computing the Fisher information parameters after locally training the model and computing the regularization term with the received aggregate Fisher <span class="No-Break">information parameters.</span></p>
			<p>The Fisher information matrix refers to the covariance of the gradient of the log-likelihood function of a model with respect to its parameters, often empirically evaluated over the data present. FedCurv only utilizes the diagonal entries of this matrix, the variances between the gradient parameters, and their expected values <span class="No-Break">of zero.</span></p>
			<p>At a high level, this variance term can be considered an estimate of how influential the parameter is in changing the performance of the model on the data. This information is essential for preventing the modification of parameters key to good performance on one dataset during the local training of other agents – the underlying idea <span class="No-Break">behind FedCurv.</span></p>
			<p>Relaxing the measure of model performance from the gradient of the log-likelihood to the gradient of any objective function allows for the direct use of the gradient terms computed during backpropagation when computing the variance terms for models using gradient-based optimization methods, such as deep learning models. Specifically, the variance term of a parameter is equal to the square of its respective gradient term, allowing for the terms to be directly computed from the net gradients calculated during <span class="No-Break">local training.</span></p>
			<p>First, we create two variables to store the agent’s most recent Fisher information parameters and the received aggregate Fisher information parameters, which are used to determine the Fisher information from the other agents. The value of the lambda parameter of FedCurv is fixed, and <strong class="source-inline">total_grad</strong> is initialized as a container for the cumulative<a id="_idIndexMarker613"/> gradient<a id="_idIndexMarker614"/> from each <span class="No-Break">training loop:</span></p>
			<pre class="source-code">
agg_model = ... # Get aggregated model from aggregator
model.load_state_dict(agg_model.state_dict())
fisher_info_params = ... # Initialize at start, then maintain to store past round parameters
agg_fisher_info_params = ... # Separate aggregate Fisher information parameters from aggregate model parameters
# Only consider other agents, and convert to PyTorch tensor
agg_fisher_info_params = {k:torch.tensor(agg_fisher_info_params[k] - fisher_info_params[k]) for k in fisher_info_params.keys()}
# Scaling parameter for FedCurv regularization term
fedcurv_lambda = 1.0
total_grad = {i:torch.zeros_like(param.data) for i,param in enumerate(model.parameters())}</pre>
			<p>Then, we compute the FedCurv regularization term from the model weights and the aggregate Fisher information parameters. This term is weighted by lambda and added to the loss term before computing <span class="No-Break">the gradients:</span></p>
			<pre class="source-code">
model.train()
for epoch in range(num_epochs):
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        for i,param in enumerate(model.parameters()):
            # Factor out regularization term to use saved fisher info parameters
            reg_term = (param.data ** 2) * agg_fisher_info_params[f'fedcurv_u_{i}']
            reg_term += 2 * param.data * agg_fisher_info_params[f'fedcurv_v_{i}']
            reg_term += (agg_fisher_info_params[f'fedcurv_v_{i}'] ** 2) / agg_fisher_info_params[f'fedcurv_u_{i}']
            loss += fedcurv_lambda * reg_term.sum()</pre>
			<p>The<a id="_idIndexMarker615"/> gradients are <a id="_idIndexMarker616"/>then computed and stored in <strong class="source-inline">total_grad</strong> before updating the <span class="No-Break">model weights:</span></p>
			<pre class="source-code">
        loss.backward()
        for i,param in enumerate(model.parameters()):
            total_grad[i] += param.grad
        optimizer.step()</pre>
			<p>Finally, we compute and store the agent’s most recent Fisher information parameters for use in the <span class="No-Break">next round:</span></p>
			<pre class="source-code">
for i,param in enumerate(model.parameters()):
    fisher_info_params[f'fedcurv_u_{i}'] = (total_grad[i] ** 2).numpy()
    fisher_info_params[f'fedcurv_v_{i}'] = ((total_grad[i] ** 2) * param.data).numpy()</pre>
			<p>Therefore, framework-agnostic <a id="_idIndexMarker617"/>aggregation can <a id="_idIndexMarker618"/>be used to implement FedCurv on top of FedAvg. The full FL example using FedCurv can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedcurv_example"><span class="No-Break">https://github.com/PacktPublishing/Federated-Learning-with-Python/tree/main/ch7/agg_fl_examples/cifar_fedcurv_example</span></a><span class="No-Break">.</span></p>
			<h3>Data-sharing approach</h3>
			<p>To make further <a id="_idIndexMarker619"/>progress, changes to external aspects of <a id="_idIndexMarker620"/>the FL scenario are necessary. For example, let’s assume that the data privacy restriction is loosened, such that small subsets of the local datasets from each agent can be shared with the other agents. This data-sharing approach allows for homogeneity in the local data distributions proportional to the amount of shared data to be achieved, at the expense of the key stationary data property of FL that makes it desirable in many privacy-oriented applications. Thus, data-sharing approaches are generally unsuitable for the majority <span class="No-Break">of applications.</span></p>
			<h3>Personalization through fine-tuning</h3>
			<p>It is clear that <a id="_idIndexMarker621"/>producing a single model that demonstrates strong performance across the local datasets is not easy when the datasets are IID. However, what would happen if the single model restriction was removed from the FL process? If the goal is to produce local models that perform well on the same edge devices where training is conducted, removing the single model restriction allows for the use of different local models that have been trained on the exact data distributions where inference is <span class="No-Break">being applied.</span></p>
			<p>This concept is<a id="_idIndexMarker622"/> called <em class="italic">personalization</em>, in which agents use versions of the aggregate model tuned for the local data distribution to achieve strong performance. The key point of this approach is to balance the local performance of the locally trained model with the global performance and the resulting robustness of the aggregate model received in each round. One method of accomplishing this is for each agent to maintain their local models across the rounds, updating the local model with the weighted average of the previous local model and the received aggregate model during <span class="No-Break">each round.</span></p>
			<p>Alternatively, consider a relaxation that allows for multiple aggregate models to be produced in each round. In cases where the local data distributions can be clustered into just a few separated groups, distribution-aware aggregation would allow for the selective application of aggregation methods to groups of models belonging to the same <span class="No-Break">distribution cluster.</span></p>
			<p>One example of this approach is the <strong class="bold">Performance-Based Neighbor Selection</strong> (<strong class="bold">PENS</strong>) algorithm, where<a id="_idIndexMarker623"/> agents receive locally trained models from other agents and test them on their <a id="_idIndexMarker624"/>own local dataset during the first phase. Using the assumption that models trained on similar datasets will perform better than models trained on different datasets, the agents then determine the set of other agents with similar data distributions, allowing for aggregation to only be performed with similar agents in the <span class="No-Break">second phase.</span></p>
			<p>A second approach<a id="_idIndexMarker625"/> is to <em class="italic">add an intermediate aggregation</em> step between the local models and the global aggregate model called a cluster model. By leveraging knowledge about the agent data distributions or through a dynamic allocation method, agents with similar data distributions can be assigned to a cluster aggregator, which is then known to produce a strong model due to its agents having <span class="No-Break">IID datasets.</span></p>
			<p>Balancing the performance of the cluster models with the robustness of global aggregation leads to the concept of the semi-global model, in which subsamples of the cluster models can be selected (potentially based on data distribution) to create a smaller set of partially global aggregate models that maintain performance and robustness. Therefore, the cluster and semi-global model approach is beneficial for both aggregation and achieving a fully distributed <span class="No-Break">FL system.</span></p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor188"/>Summary</h1>
			<p>The goal of this chapter was to provide a conceptual overview of the current knowledge of aggregation, the key theoretical step in FL that allows for the disjoint training done by each agent to be pooled together with minimal transmission required. FedAvg is a simple, yet surprisingly powerful aggregation algorithm that performs well in an ideal FL scenario. This scenario is achieved when training is done across IID datasets using machines with similar levels of computational power and no adversarial or otherwise incorrectly <span class="No-Break">performing agents.</span></p>
			<p>Unfortunately, these conditions are often not met when deploying an FL system in the real world. To address these cases, we introduced and implemented modified aggregation approaches: FedProx, FedCurv, and three different robust mean estimators. After reading this chapter, you should have a solid understanding of the considerations that must be taken into account for practical FL applications, and you should be able to integrate the aforementioned algorithms into <span class="No-Break">these applications.</span></p>
			<p>In the next chapter, we will do a deep dive into some of the existing FL frameworks with several toy examples to demonstrate the functionalities provided <span class="No-Break">by each.</span></p>
		</div>
	

		<div id="_idContainer073" class="Content">
			<h1 id="_idParaDest-184"><a id="_idTextAnchor189"/>Part 3 Moving Toward the Production of Federated Learning Applications</h1>
			<p>In this part, you will be introduced to the existing <strong class="bold">federated learning</strong> (<strong class="bold">FL</strong>) frameworks, such as <strong class="bold">TensorFlow Federated</strong> (<strong class="bold">TFF</strong>), PySyft, Flower, and STADLE, and learn about their libraries and how to actually run those frameworks. In addition, you will understand what is happening with FL in the real world by learning about the current and potential use cases that are being implemented in industries worldwide, especially in global enterprise companies. The book concludes by looking at the future trends and developments of FL to understand where  AI technology itself is heading to envision a wisdom-driven future.</p>
			<p>This part comprises the following chapters:</p>
			<ul>
				<li><em class="italic"><a id="_idTextAnchor190"/></em><a href="B18369_08.xhtml#_idTextAnchor191"><em class="italic">Chapter 8</em></a>, <em class="italic">Introducing Existing Federated Learning Frameworks</em></li>
				<li><a href="B18369_09.xhtml#_idTextAnchor224"><em class="italic">Chapter 9</em></a>, <em class="italic">Case Studies with Key Use Cases of Federated Learning Applications</em></li>
				<li><a href="B18369_10.xhtml#_idTextAnchor256"><em class="italic">Chapter 10</em></a>, <em class="italic">Future Trends and Developments</em></li>
			</ul>
		</div>
	</body></html>