- en: Emotion Detection with CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until recently, interacting with a computer was not too dissimilar from interacting
    with, say, a power tool; we pick it up, turn it on, manually control it, and then
    put it down until the next time we require it for that specific task. But recently,
    we are seeing signs that this is about to change; computers allow natural forms
    of interaction and are becoming more ubiquitous, more capable, and more ingrained
    in our daily lives. They are becoming less like heartless dumb tools and more
    like friends, able to entertain us, look out for us, and assist us with our work.
  prefs: []
  type: TYPE_NORMAL
- en: With this shift comes a need for computers to be able to understand our emotional
    state. For example, you don't want your social robot cracking a joke after you
    arrive back from work having lost your job (to an AI bot!). This is a field of
    computer science known as **affective computing** (also referred to as **artificial
    emotional intelligence** or **emotional AI**), a field that studies systems that
    can recognize, interpret, process, and simulate human emotions. The first stage
    of this is being able to recognize emotional state, which is the topic of this
    chapter. We will first introduce the data and model we will be using, and then
    walk through how we approach the problem of expression recognition on the iPhone
    and how to appropriately preprocess the data for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of of this chapter, you will have achieved the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Built a simple application that will infer your mood in real time using the
    front camera feed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gained hands-on experience using the `Vision` framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developed a deeper understanding and intuition of how **convolutional neural
    networks** (**CNNs**) work and how they can be applied at the edge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by introducing the data and model we will be using.
  prefs: []
  type: TYPE_NORMAL
- en: Facial expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our face is one of the strongest indicators of emotions; as we laugh or cry,
    we put our emotions on display, allowing others to glimpse into our minds. It's
    a form of nonverbal communication that, apparently, accounts for over 50% of our
    communication with others. Forty independently controlled muscles make the face
    one of the most complex systems we possess, which could be the reason we use it
    as a medium for communicating something so important as our current emotional
    state. But can we classify it?
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2013, the **International Conference on Machine Learning** (**ICML**) ran
    a competition inviting contestants to build a facial expression classifier using
    a training dataset of over 28,000 grayscale images. They were labeled as either
    anger, disgust, fear, happiness, sadness, surprise, or neutral. The following
    are a few samples of this training data (available at [https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b20a9c86-91b5-48b3-bde6-93ab67ba7fed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As previously mentioned, the training dataset consists of 28,709 grayscale
    images of faces in 48 x 48 pixels, where each face is centered and associated
    with a label defining the assigned emotion. This emotion can be one of the following
    labels (textual description was added for legibility):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4ac4f1b-3cd4-480c-890e-24af9138d668.png)'
  prefs: []
  type: TYPE_IMG
- en: Neural networks (or any other machine learning algorithm) can't really do anything by
    themselves. All a neural network does is find a direct or indirect correlation
    between two datasets (inputs and their corresponding outputs). In order for a
    neural network to learn, we need to present it with two meaningful datasets where
    some true correlation exists between the inputs and outputs. A good practice when
    tackling any new data problem is to come up with a predictive theory of how you
    might approach it or search for correlation using techniques such as data visualization
    or some other explorational data analysis technique. In doing so, we also better
    understand how we need to prepare our data to align it with the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the results of a data visualization technique that can be performed
    on the training data; here, it''s our assumption that some pattern exists between
    each expression (happy, sad, angry, and so on). One way of visually inspecting
    this is by averaging each expression and the associated variance. This can be
    achieved simply by finding the mean and standard deviation across all images for
    their respective class (expression example, happy, angry, and so on). The results
    of some of the expressions can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34542a2e-b0a6-41d2-8f6c-f909ea917e36.png)'
  prefs: []
  type: TYPE_IMG
- en: After you get over the creepiness of the images, you get a sense that a pattern
    does exist, and you understand what our model needs to learn to be able to recognize
    facial expressions. Some other notable, and fairly visible, takeaways from this
    exercise include the amount of variance with the disgust expression; this hints
    that our model might find it difficult to effectively learn to recognize this
    expression. The other observation - and the one more applicable to our task in
    this chapter - is that the training data consists of forward-facing faces with
    little padding beyond the face, therefore highlighting what the model expects
    for its input. Now that we have a better sense of our data; let's move on and
    introduce the model we will be using in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [chapter 3](5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml), *Recognising Objects
    in the World,* we presented the intuition behind CNNs or ConvNets. So, given that
    we won''t be introducing any new concepts in this chapter, we will omit any discussion
    on the details of the model and just present it here for reference, with some
    commentary about its architecture and the format of the data it is expecting for
    its input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ca0c020-6418-4e82-9f76-bce205f70ae9.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure is a visualization of the architecture of the model; it's
    your typical CNN, with a stack of convolutional and pooling layers before being
    flattened and fed into a series of fully connected layers. Finally, it is fed
    into a softmax activation layer for multi-class classification. As mentioned earlier,
    the model is expecting a 3D tensor with the dimensions 48 x 48 x 1 (width, height,
    channels). To avoid feeding our model with large numbers (0 - 255), the input
    has been normalized (dividing each pixel by 255, which gives us a range of 0.0
    - 1.0). The model outputs the probability of a given input with respect to each
    class, that is, seven outputs with each class representing the probability of
    how likely it is correlated for the given input. To make a prediction, we simply
    take the class with the largest probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'This model was trained on 22,967 samples, reversing the other 5,742 samples
    for validation. After 15 epochs, the model achieved approximately 59% accuracy
    on the validation set, managing to squeeze into the 13^(th) place of the Kaggle
    competition (at the time of writing this chapter). The following graphs show the
    training accuracy and loss during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cf6c2bf-9e4e-43e8-8474-86c3fd69e450.png)'
  prefs: []
  type: TYPE_IMG
- en: This concludes our brief introduction of the data and model we will be using
    for this chapter. The two main takeaways are an appreciation of what data the
    model has been fed during training, and the fact that our model achieved just
    59% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The former dictates how we approach obtaining and process the data before feeding
    it into the model. The latter poses an opportunity for further investigation to
    better understand what is pulling the accuracy down and how to improve it; it
    also can be seen as a design challenge—a design to be made around this constraint.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are mainly concerned with the former so, in the next section,
    we will explore how to obtain and preprocess the data before feeding it to the
    model. Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Input data and preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will implement the preprocessing functionality required
    to transform images into something the model is expecting. We will build up this
    functionality in a playground project before migrating it across to our project
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you haven''t done so already, pull down the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter4/Start/` and open the Playground
    project `ExploringExpressionRecognition.playground`. Once loaded, you will see
    the playground for this chapter, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdcff650-0176-4b84-910f-8f1e8913e55a.png)'
  prefs: []
  type: TYPE_IMG
- en: Before starting, to avoid looking at images of me, please replace the test images
    with either personal photos of your own or royalty free images from the internet,
    ideally a set expressing a range of emotions.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the test images, this playground includes a compiled Core ML model
    (we introduced it in the previous image) with its generated set of wrappers for
    inputs, outputs, and the model itself. Also included are some extensions for `UIImage`,
    `UIImageView`, `CGImagePropertyOrientation`, and an empty `CIImage` extension,
    to which we will return later in the chapter. The others provide utility functions
    to help us visualize the images as we work through this playground.
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping into the code, let's quickly discuss the approach we will take
    in order to determine what we actually need to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, our process of performing machine learning has been fairly
    straightforward; apart from some formatting of input data, our model didn't require
    too much work. This is not the case here. A typical photo of someone doesn't normally
    have just a face, nor is their face nicely aligned to the frame unless you're
    processing passport photos. When developing machine learning applications, you
    have two broad paths.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first, which is becoming increasingly popular, is to use an end-to-end
    machine learning model capable of just being fed the raw input and producing adequate
    results. One particular field that has had great success with end-to-end models
    is speech recognition. Prior to end-to-end deep learning, speech recognition systems
    were made up of many smaller modules, each one focusing on extracting specific
    pieces of data to feed into the next module, which was typically manually engineered.
    Modern speech recognition systems use end-to-end models that take the raw input
    and output the result. Both of the described approaches can been seen in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11db396a-0e52-4e50-85c4-0f1d067eb6e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Obviously, this approach is not constrained to speech recognition and we have
    seen it applied to image recognition tasks, too, along with many others. But there
    are two things that make this particular case different; the first is that we
    can simplify the problem by first extracting the face. This means our model has
    less features to learn and offers a smaller, more specialized model that we can
    tune. The second thing, which is no doubt obvious, is that our training data consisted
    of only faces and not natural images. So, we have no other choice but to run our
    data through two models, the first to extract faces and the second to perform
    expression recognition on the extracted faces, as shown in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2d9524b-d2e3-40d2-a2e0-323d9fca879f.png)'
  prefs: []
  type: TYPE_IMG
- en: Luckily for us, Apple has mostly taken care of our first task of detecting faces
    through the `Vision` framework it released with iOS 11\. The `Vision` framework
    provides performant image analysis and computer vision tools, exposing them through
    a simple API. This allows for face detection, feature detection and tracking,
    and classification of scenes in images and video. The latter (expression recognition)
    is something we will take care of using the Core ML model introduced earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to the introduction of the `Vision` framework, face detection would typically
    be performed using the Core Image filter. Going back further, you had to use something
    like OpenCV. You can learn more about Core Image here: [https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html](https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have got a bird''s-eye view of the work that needs to be done,
    let''s turn our attention to the editor and start putting all of this together.
    Start by loading the images; add the following snippet to your playground:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding snippet, we are simply loading each of the images we have
    included in our resources'' `Images` folder and adding them to an array we can
    access conveniently throughout the playground. Once all the images are loaded,
    we set the constant `faceIdx`, which will ensure that we access the same images
    throughout our experiments. Finally, we create an `ImageView` to easily preview
    it. Once it has finished running, click on the eye icon in the right-hand panel
    to preview the loaded image, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b30cea31-9392-46f5-a018-527fa1fdb3b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will take advantage of the functionality available in the `Vision`
    framework to detect faces. The typical flow when working with the `Vision` framework
    is **defining a request**, which determines what analysis you want to perform,
    and **defining the handler**, which will be responsible for executing the request
    and providing means of obtaining the results (either through delegation or explicitly
    queried). The result of the analysis is a collection of observations that you
    need to cast into the appropriate observation type; concrete examples of each
    of these can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in the preceding diagram, the request determines what type of
    image analysis will be performed; the handler, using a request or multiple requests
    and an image, performs the actual analysis and generates the results (also known
    as **observations**). These are accessible via a property or delegate if one has
    been assigned. The type of observation is dependent on the request performed;
    it's worth highlighting that the `Vision` framework is tightly integrated into
    Core ML and provides another layer of abstraction and uniformity between you and
    the data and process. For example, using a classification Core ML model would
    return an observation of type `VNClassificationObservation`. This layer of abstraction
    not only simplifies things but also provides a consistent way of working with
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous figure, we showed a request handler specifically for static
    images. `Vision` also provides a specialized request handler for handling sequences
    of images, which is more appropriate when dealing with requests such as tracking.
    The following diagram illustrates some concrete examples of the types of requests
    and observations applicable to this use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/baf94581-ad26-4ebd-9bf2-0c2575426b4a.png)'
  prefs: []
  type: TYPE_IMG
- en: So, when do you use `VNImageRequestHandler` and `VNSequenceRequestHandler`?
    Though the names provide clues as to when one should be used over the other, it's
    worth outlining some differences.
  prefs: []
  type: TYPE_NORMAL
- en: The image request handler is for interactive exploration of an image; it holds
    a reference to the image for its life cycle and allows optimizations of various
    request types. The sequence request handler is more appropriate for performing
    tasks such as tracking and does not optimize for multiple requests on an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this all looks in code; add the following snippet to your playground:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are simply creating the request and handler; as discussed in the preceding
    code, the request encapsulates the type of image analysis while the handler is
    responsible for executing the request. Next, we will get `faceDetectionRequestHandler`
    to run `faceDetectionRequest`; add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `perform` function of the handler can throw an error if it fails; for this
    reason, we wrap the call with `try?` at the beginning of the statement and can
    interrogate the `error` property of the handler to identify the reason for failing.
    We pass the handler a list of requests (in this case, only our `faceDetectionRequest`),
    the image we want to perform the analysis on, and, finally, the orientation of
    the image that can be used by the request during analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the analysis is done, we can inspect the observation obtained through
    the `results` property of the request itself, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The type of observation is dependent on the analysis; in this case, we're expecting
    a `VNFaceObservation`. Hence, we cast it to the appropriate type and then iterate
    through all the observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will take each recognized face and extract the bounding box. Then,
    we''ll proceed to draw it in the image (using an extension method of `UIImageView`
    found within the `UIImageViewExtension.swift` file). Add the following block within
    the `for` loop shown in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can obtain the bounding box of each face via the let `boundingBox` property;
    the result is normalized, so we then need to scale this based on the dimensions
    of the image. For example, you can obtain the width by multiplying `boundingBox`
    with the width of the image: `bbox.width * imageSize.width`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we invert the *y *axis as the coordinate system of Quartz 2D is inverted
    with respect to that of UIKit''s coordinate system, as shown in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d810f17-d915-44c5-98f3-557955090048.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We invert our coordinates by subtracting the bounding box''s origin and height
    from height of the image and then passing this to our `UIImageView` to render
    the rectangle. Click on the eye icon in the right-hand panel in line with the
    statement `imageView.drawRect(rect: invertedFaceRect)` to preview the results;
    if successful, you should see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e9221cd-d50f-44f4-89d7-1b6bfad5379b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An alternative to inverting the face rectangle would be to use an `AfflineTransform`,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '`var transform = CGAffineTransform(scaleX: 1, y: -1)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`transform = transform.translatedBy(x: 0, y: -imageSize.height)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`let invertedFaceRect = faceRect.apply(transform)`'
  prefs: []
  type: TYPE_NORMAL
- en: This approach leads to less code and therefore less chances of errors. So, it
    is the recommended approach. The long approach was taken previously to help illuminate
    the details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a quick detour and experiment with another type of request;
    this time, we will analyze our image using `VNDetectFaceLandmarksRequest`. It
    is similar to `VNDetectFaceRectanglesRequest` in that this request will detect
    faces and expose their bounding boxes; but, unlike `VNDetectFaceRectanglesRequest`,
    `VNDetectFaceLandmarksRequest` also provides detected facial landmarks. A landmark
    is a prominent facial feature such as your eyes, nose, eyebrow, face contour,
    or any other feature that can be detected and describes a significant attribute
    of a face. Each detected facial landmark consists of a set of points that describe
    its contour (outline). Let''s see how this looks; add a new request as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding snippet should look familiar to you; it''s almost the same as
    what we did previously, but this time replacing `VNDetectFaceRectanglesRequest`
    with `VNDetectFaceLandmarksRequets`. We have also refreshed the image in our image
    view with the statement `imageView.image = images[faceIdx]`. As we did before,
    let''s iterate through each of the detected observations and extract some of the
    common landmarks. Start off by creating the outer loop, as shown in this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Up to this point, the code will look familiar; next, we will look at each of
    the landmarks. But first, let''s create a function to handle the transformation
    of our points from the Quartz 2D coordinate system to UIKit''s coordinate system.
    We add the following function but within the same block as our `faceRect` declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned before, each landmark consists of a set of points that describe
    the contour of that particular landmark, and, like our previous feature, the points
    are normalized between 0.0 - 1.0\. Therefore, we need to scale them based on the
    associated face rectangle, which is exactly what we did in the preceding example.
    For each point, we are scaling and transforming it into the appropriate coordinate
    space, and then returning the mapped array to the caller.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now define some constants that we will use to visualize each landmark;
    we add the following two constants in the function we implemented just now, `getTransformedPoints`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now step through a few of the landmarks, showing how we extract the
    features and occasionally showing the result. Let''s start with the left eye and
    right eye; continue adding the following code just after the constants you just
    defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Hopefully, as is apparent from the preceding code snippet, we get a reference
    to each of the landmarks by interrogating the face observations landmark property,
    which itself references the appropriate landmark. In the preceding code, we get reference
    to the landmarks `leftEye` and `rightEye`. And for each, we first render the contour
    of the eye, as shown in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11105948-31bf-4caa-8a12-e6371e1fbeac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we iterate through each of the points to find the center of the eye and
    render a circle using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This is slightly unnecessary as one of the landmarks available is `leftPupil`,
    but I wanted to use this instance to highlight the importance of inspecting the
    available landmarks. The next half of the block is concerned with performing the
    same tasks for the right eye; by the end of it, you should have an image resembling
    something like the following, with both eyes drawn:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/626871f2-e4cf-4623-844e-a85119d87c41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s continue highlighting some of the landmarks available. Next, we will
    inspect the face contour and nose; add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The patterns should be obvious now; here we can draw the landmarks `faceContour`,
    `nose`, and `noseCrest`; with that done, your image should look something like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an exercise, draw the lips (and any other facial landmark) using the landmarks
    `innerLips` and `outerLips`. With that implemented, you should end up with something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c95fd6d-891b-4152-a42d-3de1698b3c12.png)'
  prefs: []
  type: TYPE_IMG
- en: Before returning to our task of classifying facial expressions, let's quickly
    finish our detour with some practical uses for landmark detection (other than
    drawing or placing glasses on a face).
  prefs: []
  type: TYPE_NORMAL
- en: As highlighted earlier, our training set consists of images that are predominantly
    forward-facing and orientated fairly straight. With this in mind, one practical
    use of knowing the position of each eye is being able to qualify an image; that
    is, is the face sufficiently in view and orientated correctly? Another use would
    be to slightly realign the face so that it fits in better with your training set
    (keeping in mind that our images are reduced to 28 x 28, so some detriment to
    quality can be ignored).
  prefs: []
  type: TYPE_NORMAL
- en: For now, I'll leave the implementation of these to you but, by using the angle
    between the two eyes, you can apply an affine transformation to correct the orientation,
    that is, rotate the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now return to our main task of classification; as we did before, we
    will create a `VNDetectFaceRectanglesRequest` request to handle identifying each
    face within a given image and, for each face, we will perform some preprocessing
    before feeding it into our model. If you recall our discussion on the model, our
    model is expecting a single-channel (grayscale) image of a face with the size
    48 x 48 and its values normalized between 0.0 and 1.0\. Let''s walk through each
    part of the task piece by piece, starting with creating the request, as we did
    previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should look familiar to you now, with the only difference being the
    instantiation of our model (the bold statement): `let model = ExpressionRecognitionModelRaw()`.
    Next, we want to crop out the face from the image; in order to do this, we will
    need to write a utility function that will implement this. Since we want to carry
    this over to our application, let''s write it as an extension of the `CIImage`
    class. Click on the `CIImageExtension.swift` file within the `Sources` folder
    in the left-hand panel to open up the relevant file; currently, this file is just
    an empty extension body, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Go ahead and add the following snippet of code within the body of `CIImage` to
    implement the functionality of cropping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we are simply creating a new image of itself constrained
    to the region passed in; this method, `context.createCGImage`, returns a `CGImage`,
    which we then wrap in a `CIImage` before returning to the caller. With our crop
    method taken care of, we return to our main playground source and add the following
    snippet after the face rectangle declared previously to crop a face from our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We first create an instance of `CIImage` from `CGImage` (referenced by the `UIImage`
    instance); we then pad out our face rectangle. The reason for doing this is to
    better match it with our training data; if you refer to our previous experiments,
    the detected bounds fit tightly around the eyes and chin while our training data
    encompasses a more holistic view of the face. The numbers selected were through
    trial and error, but I imagine there is some statistically relevant ratio between
    the distance between the eyes and height of the face—maybe. We finally crop our
    image using the `crop` method we implemented earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will resize the image (to the size the model is expecting) but, once
    again, this functionality is not yet available. So, our next task! Jump back into
    the `CIImageExtension.swift` file and add the following method to handle resizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You may notice that we are not inverting the face rectangle here as we did before;
    the reason is that we were only required to do this to transform from the Quartz
    2D coordinate system to UIKit's coordinate system, which we are not doing here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the number of lines, the majority of the code is concerned with calculating
    the scale and translation required to center it. Once we have calculated these,
    we simply pass in a `CGAffineTransform`, with our scale, to the `transformed`
    method and then our centrally aligned rectangle to the `clamped` method. With
    this now implemented, let''s return to our main playground code and make use of
    it by resizing our cropped image, as shown in the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Three more steps are required before we can pass our data to our model for
    inference. The first is to convert it to a single channel, the second is to rescale
    the pixels so that they are between the values of 0.0 and 1.0, and finally we
    wrap it in a `MLMultiArray`, which we can then feed into our model''s `predict`
    method. To achieve the previous, we will add another extension to our `CIImage`
    class. It will render out the image using a single channel, along with extracting
    the pixel data and returning it in an array, which we can then easily access for
    rescaling. Jump back into the `CIImageExtension.swift` file and add the following
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, don''t be intimidated by the amount of code; there are two main
    tasks this method does. The first is rendering out the image to a `CVPixelBuffer`
    using a single channel, grayscale. To highlight this, the code responsible is
    shown in the following block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We render the image to a `CVPixelBuffer` to provide a convenient way for us
    to access the raw pixels that we can then use to populate our array. We then return
    this to the caller. The main chunk of code that is responsible for this is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, we first determine the dimensions by obtaining the width and height of
    our image, using `CVPixelBufferGetWidth` and `CVPixelBufferGetHeight` respectively.
    Then we use these to create an appropriately sized array to hold the pixel data.
    We then obtain the base address of our `CVPixelBuffer` and call its `assumingMemoryBound`
    method to give us a typed pointer. We can use this to access each pixel, which
    we do to populate our `pixelData` array before returning it.
  prefs: []
  type: TYPE_NORMAL
- en: 'With your `getGrayscalePixelData` method now implemented, return to the main
    source of the playground and resume where you left off by adding the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding snippet, we are obtaining the raw pixels of our cropped image
    using our `getGrayscalePixelData` method, before rescaling them by dividing each
    pixel by 255.0 (the maximum value). Our final task of preparation is putting our
    data into a data structure that our model will accept, a `MLMultiArray`. Add the
    following code to do just this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We start by creating an instance of `MLMultiArray` with the shape of our input
    data and then proceed to copy across our standardized pixel data.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our model instantiated and data prepared, we can now perform inference
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Previously, we dispatched inference on a background thread then printed out
    all probabilities of each class to the console. With that now complete, run your
    playground, and if everything is working fine, you should get something like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Angry  | 0.0341557003557682 |'
  prefs: []
  type: TYPE_TB
- en: '| Happy | 0.594196200370789 |'
  prefs: []
  type: TYPE_TB
- en: '| Disgust | 2.19011440094619e-06 |'
  prefs: []
  type: TYPE_TB
- en: '| Sad | 0.260873317718506 |'
  prefs: []
  type: TYPE_TB
- en: '| Fear | 0.013140731491148 |'
  prefs: []
  type: TYPE_TB
- en: '| Surprise | 0.000694742717314512 |'
  prefs: []
  type: TYPE_TB
- en: '| Neutral | 0.0969370529055595 |'
  prefs: []
  type: TYPE_TB
- en: 'As a designer and builder of intelligent systems, it is your task to interpret
    these results and present them to the user. Some questions you''ll want to ask
    yourself are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an acceptable threshold of a probability before setting the class as
    true?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can this threshold be dependent on probabilities of other classes to remove
    ambiguity? That is, if **Sad** and **Happy** have a probability of 0.3, you can
    infer that the prediction is inaccurate, or at least not useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a way to accept multiple probabilities?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it useful to expose the threshold to the user and have it manually set and/or
    tune it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are only a few questions you should ask. The specific questions, and their
    answers, will depend on your use case and users. At this point, we have everything
    we need to preprocess and perform inference; let's now turn our attention to the
    application for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you find that you are not getting any output, it could be that you need
    to flag the playground as running indefinitely so that it doesn''t exit before
    running the background thread. You can do this by adding the following statement
    in your playground: `PlaygroundPage.current.needsIndefiniteExecution = true`'
  prefs: []
  type: TYPE_NORMAL
- en: When this is set to `true`, you will need to explicitly stop the playground.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you haven't done already, pull down the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter4/Start/FacialEmotionDetection`
    and open the project `FacialEmotionDetection.xcodeproj`. Once loaded, you will
    hopefully recognize the project structure as it closely resembles our first example.
    For this reason, we will just concentrate on the main components that are unique
    for this project, and I suggest reviewing previous chapters for clarification
    on anything that is unclear.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by reviewing our project and its main components; your project
    should look similar to what is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3332a21-5276-4a59-9ba0-9780f0ca27b5.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding screenshot, the project looks a lot like our previous
    projects. I am going to make an assumption that the classes `VideoCapture`, `CaptureVideoPreviewView`,
    and `UIColorExtension` look familiar and you are comfortable with their contents. `CIImageExtension`
    is what we just implemented in the previous section, and therefore we won't be
    covering it here. The `EmotionVisualizerView` class is a custom view that visualizes
    the outputs from our model. And, finally, we have the bundled `ExpressionRecognitionModelRaw.mlmodel`.
    Our main focus in this section will be on wrapping the functionality we implemented
    in the previous section to handle preprocessing and hooking it up within the `ViewController`
    class. Before we start, let's quickly review what we are doing and consider some
    real-life applications for expression/emotion recognition.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are building a simple visualization of the detected faces;
    we will pass in our camera feed to our preprocessor, then hand it over to our
    model to perform inference, and finally feed the results to our `EmotionVisualizerView`
    to render the output as an overlay on the screen. It's a simple example but sufficiently
    implements the mechanics required to embed in your own creations. So, what are
    some of its practical uses?
  prefs: []
  type: TYPE_NORMAL
- en: In a broad sense, there are three main uses: **analytical**, **reactive**, and **anticipatory**.
    Analytical is generally what you are likely to hear. These applications typically
    observe reactions by the user in relation to the content being presented; for
    example, you might measure arousal from content observed by the user, which is
    then used to drive future decisions.
  prefs: []
  type: TYPE_NORMAL
- en: While analytical experiences remain mostly passive, reactive applications proactively
    adjust the experience based on live feedback. One example that illustrates this
    well is **DragonBot**, a research project from the *Social Robotics Group* at
    MIT exploring intelligent tutoring systems.
  prefs: []
  type: TYPE_NORMAL
- en: DragonBot uses emotional awareness to adapt to the student; for example, one
    of its applications is a reading game that adapts the words based on the recognized
    emotion. That is, the system can adjust the difficulty of the task (words in this
    case) based on the user's ability, determined by the recognized emotion.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have anticipatory applications. Anticipatory applications are semi-autonomous.
    They proactively try to infer the user's context and predict a likely action,
    therefore adjusting their state or triggering an action. An fictional example
    could be an email client that delays sending messages if the user had composed
    the message when angry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, this highlights some of the opportunities, but for now, let''s return
    to our example and start building out the class that will be responsible for handling
    the preprocessing. Start off by creating a new swift file called `ImageProcess.swift`;
    and, within the file, add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have defined the protocol for the delegate to handle the result once
    the preprocessing has completed, as well as the main class that exposes the method
    for initiating the task. Most of the code we will be using is what we have written
    in the playground; start off by declaring the request and request handler at the
    class level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now make use of the request by having our handler execute it within
    the body of the `getFaces` method''s background queue dispatch block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This should all look familiar to you. We pass in our request and image to the
    image handler. Then, we instantiate an array to hold the data for each face detected
    in the image. Finally, we obtain the observations and start iterating through
    each of them. It''s within this block that we will perform the preprocessing and
    populate our `facesData` array as we had done in the playground. Add the following
    code within the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding block, we obtain the detected face''s bounding box and create
    the cropping bounds, including padding. Our next task will be to crop the face
    from the image, resize it to our target size of 48 x 48, extract the raw pixel
    data along with normalizing the values, and finally populate an `MLMultiArray`.
    This is then added to our `facesData` array to be returned to the delegate; appending
    the following code to your script does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Nothing new has been introduced here apart from chaining the methods to make
    it more legible (at least for me). Our final task is to notify the delegate once
    we have finished; add the following just outside the observations loop block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with that complete, our `ImageProcessor` is ready to be used. Let''s hook
    everything up. Jump into the `ViewController` class, where we will hook our `ImageProcessor`.
    We will pass its results to our model and finally pass the output from our model
    to `EmotionVisualizerView` to present the results to the user. Let''s start by
    reviewing what currently exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Our `ViewController` has references to its IB counterpart, most notably the `previewView`
    and `viewVisualizer`. The former will render the captured camera frames and `viewVisualizer`
    will be responsible for visualizing the output of our model. We then have `videoCapture`,
    which is a utility class that encapsulates setting up, capturing, and tearing
    down the camera. We get access to the captured frames by assigning ourselves as
    the delegate and implement the appropriate protocol as we have done as an extension
    at the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by declaring the model and `ImageProcessor` variables required
    for our task; add the following at the class level of your `ViewController`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to assign ourselves as the delegate of `ImageProcessor` in order
    to receive the results once the processing has completed. Add the following statement
    to the bottom of your `viewDidLoad` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We will return shortly to implement the required protocol; for now, let''s
    make use of our `ImageProcessor` by passing in the frame we receive from the camera.
    Within the `onFrameCaptured` method, we add the following statement, which will
    pass each frame to our `ImageProcessor` instance. It''s shown in bold in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Our final task will be to implement the `ImageProcessorDelegate` protocol;
    this will be called when our `ImageProcessor` has completed identifying and extracting
    each face for a given camera frame along with performing the preprocessing necessary
    for our model. Once completed, we will pass the data to our model to perform inference,
    and finally pass these onto our `EmotionVisualizerView`. Because nothing new is
    being introduced here, let''s go ahead and add the block in its entirety:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The only notable thing to point out is that our model needs to perform inference
    in the background thread and our `ImageProcessor` calls its delegate on the main
    thread. For this reason, we dispatch inference to the background and then return
    the results on the main thread—this is necessary whenever you want to update the
    user interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that complete, we are now in a good place to build and deploy to test;
    if all goes well, you should see something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5daed44-e9db-4000-b067-b433927216f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's wrap up the chapter by reviewing what we have covered and point out some
    interesting areas to explore before moving on to the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we have taken a naive approach with respect to processing the
    captured frames; in a commercial application you would want to optimize this process
    such as utilizing **object tracking** from the `Vision` framework to replace explicit
    face detection, which is computationally cheaper.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we applied a CNN for the task of recognizing facial expressions.
    Using this, we could infer the emotional state of a given face. As usual, we again
    spent the majority of our time understanding the required input for the model
    and implementing the functionality to facilitate this. But, in doing so, we uncovered
    some important considerations when developing intelligent applications; the first
    is the explicit awareness of using either an end-to-end solution or a multi-step
    approach, with the multi-step approach being the most common one you will use.
  prefs: []
  type: TYPE_NORMAL
- en: This essentially means you, the designer and builder of intelligent applications,
    will be building data pipelines consisting of many models, each transforming the
    data in preparation for the next. This is similar to how deep networks work but
    provides greater flexibility. The second consideration is highlighting the availability
    of complementary frameworks available on iOS, in particular the `Vision` framework.
    It was used as one of the steps in our pipeline but offers a lot of convenience
    for common tasks, as well as a consistent workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, our pipeline consisted of only two steps, face detection and
    then emotion recognition. But we also briefly played with a feature of the `Vision`
    framework that can be used to identify facial landmarks. So, it is plausible to
    consider facial landmarks to train the emotional classifier rather than the raw
    pixels, in which case our pipeline would consist of three steps: face detection,
    landmark detection, and, finally, emotion recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we briefly explored some use cases showing how emotion recognition
    could be applied; as our computers shift away from being pure tools towards being
    companions, being able to detect and react to the emotional state of the user
    will become increasingly more important. So, it's an area well worth further exploring.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce the concept of transfer learning and
    how we can use it to transfer styles from one image onto another.
  prefs: []
  type: TYPE_NORMAL
