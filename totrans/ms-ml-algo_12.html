<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Generative Adversarial Networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to provide a brief introduction to a family of generative models based on some game theory concepts. Their main peculiarity is an adversarial training procedure that is aimed at learning to distinguish between true and fake samples, driving, at the same time, another component that generates samples more and more similar to the training examples.</p>
<p>In particular, we will be discussing:</p>
<ul>
<li>Adversarial training and standard<strong> Generative Adversarial Networks</strong> (<strong>GANs</strong>)</li>
<li><strong>Deep Convolutional GANs</strong> (<strong>DCGAN</strong>)</li>
<li><strong>Wasserstein GANs</strong> (<strong>WGAN</strong>)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adversarial training</h1>
                </header>
            
            <article>
                
<p>The brilliant idea of adversarial training, proposed by Goodfellow and others (in <em>Generative Adversarial Networks</em>,<em> </em><em>Goodfellow I</em>. <em>J</em>., <em>Pouget-Abadie J</em>., <em>Mirza M</em>., <em>Xu B</em>., <em>Warde-Farley D</em>., <em>Ozair S</em>., <em>Courville A</em>., <em>Bengio Y</em>., <em>arXiv:1406.2661</em> [<em>stat.ML</em>]), ushered in a new generation of generative models that immediately outperformed the majority of existing algorithms. All of the derived models are based on the same fundamental concept of adversarial training, which is an approach partially inspired by game theory.</p>
<p>Let's suppose that we have a data generating process, <em>p<sub>data</sub>(x)</em>, that represents an actual data distribution and a finite number of samples that we suppose are drawn from <em>p<sub>data</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/caf2bfbb-3f87-4f34-ad30-fa5c2fa2e697.png" style="width:22.17em;height:1.58em;"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">Our goal is to train a model called a generator, whose distribution must be as close as possible to <em>p<sub>data</sub></em>. This is the trickiest part of the algorithm, because instead of standard methods (for example, variational autoencoders), adversarial training is based on a minimax game between two players (we can simply say that, given an objective, the goal of both players is to minimize the maximum possible loss; but in this case, each of them works on different parameters). One player is the generator, we can define as a parameterized function of a noise sample:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/20c40179-342d-4870-83c7-10c7d02f44cb.png" style="width:21.42em;height:1.83em;"/></div>
<p class="mce-root">The generator is fed with a noise vector (in this case, we have employed a uniform distribution, but there are no particular restrictions; therefore, we are simply going to say that <em>z</em> is drawn from a noise distribution <em>p<sub>noise</sub></em>), and outputs a value that has the same dimensionality of the samples drawn from <em>p<sub>data</sub></em>. Without any further control, the generator distribution will be completely different from the data generating process, but this is the moment for the other player to enter the scene. The second model is called the <em>discriminator</em> (or Critic), and it has the responsibility of evaluating the samples drawn from <em>p<sub>data</sub></em> and the ones produced by the generator:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/975272aa-e4ee-40e0-8509-6ab22347e7d6.png" style="width:18.08em;height:1.75em;"/></div>
<p>The role of this model is to output a probability that must reflect the fact that the sample is drawn from <em>p<sub>data</sub></em>, instead of being generated by <em>G(z; θ<sub>g</sub>)</em>. What happens is very simple: the first player (the generator) outputs a sample, <em>x</em>. If <em>x</em> actually belongs to <em>p<sub>data</sub></em>, the discriminator will output a value close to 1, while if it's very different from the other true samples, <em>D(x; <span>θ</span><sub>d</sub>)</em> will output a very low probability. The real structure of the game is based on the idea of training the generator to deceive the discriminator, by producing samples that can potentially be drawn from <em>p<sub>data</sub></em>. This result can be achieved by trying to maximize the log-probability, <em>log(<span>D(x; </span><span>θ</span><sub>d</sub></em><span><em>))</em>, when <em>x</em> is a true sample (drawn from <em>p<sub>data</sub></em>), while minimizing the log-probability, <em>log(1 - D(G(z; θ<sub>g</sub>); θ<sub>d</sub>))</em>, with <em>z</em> sampled from a noise distribution.</span></p>
<p><span>The first operation forces the discriminator to become more and more aware of the true samples (this condition is necessary to avoid being deceived too easily). The second objective is a little bit more complex, because the discriminator has to evaluate a sample that can be acceptable or not. Let's suppose that the generator is not smart enough, and outputs a sample that cannot belong to <em>p<sub>data</sub></em>. As the discriminator is learning how <em>p<sub>data</sub></em> is structured, it will very soon distinguish the wrong sample, outputting a low probability. Hence, by minimizing <em>log(1 - D(G(z; θ<sub>g</sub>); θ<sub>d</sub>))</em>, we are forcing the discriminator to become more and more critical when the samples are quite different from the ones drawn from <em>p<sub>d</sub></em><sub><em>ata</em></sub>, and the becomes generator more and more able to produce acceptable samples. On the other hand, if the generator outputs a sample that belongs to the data generating process, the discriminator will output a high probability, and the minimization falls back in the previous case.</span></p>
<p>The authors expressed this minimax game using a shared value function, <em>V(G, D)</em>, that must be minimized by the generator and maximized by the discriminator:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9c2dfbf2-2dc2-4aa2-9776-d8789f81fba1.png" style="width:65.67em;height:2.08em;"/></div>
<p class="mce-root">This formula represents the dynamics of a non-cooperative game between two players (for further information, refer to <em>Tadelis S</em>., <em>Game Theory, Princeton University Press</em>) that theoretically admits a special configuration, called a <em>Nash equilibrium</em>, that can be described by saying that if the two players know each other's strategy, they have no reason to change their own strategy if the other player doesn't. In this case, both the discriminator and generator will pursue their strategies until no change is needed, reaching a final, stable configuration, which is potentially a Nash equilibrium (even if there are many factors that can prevent reaching this goal). A common problem is the premature convergence of the discriminator, which forces the gradients to vanish because the loss function becomes flat in a region close to 0. As this is a game, a fundamental condition is the possibility to provide information to allow the player to make corrections. If the discriminator learns how to separate true samples from fake ones too quickly, the generator convergence slows down, and the player can remain trapped in a sub-optimal configuration. In general, when the distributions are rather complex, the discriminator is slower than the generator; but, in some cases, it is necessary to update the generator more times after each single discriminator update. Unfortunately, there are no rules of thumb; but, for example, when working with images, it's possible to observe the samples generated after a sufficiently large number of iterations. If the discriminator loss has become very small and the samples appear corrupted or incoherent, it means that the generator did not have enough time to learn the distribution, and it's necessary to slow down the discriminator.</p>
<p class="mce-root">The authors in the aforementioned paper showed that, given a generator characterized by a distribution <em>p<sub>g</sub>(x)</em>, the optimal discriminator is:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6905cecd-0b6b-4604-8b67-992f19fee8df.png" style="width:15.25em;height:3.42em;"/></div>
<p>At this point, considering the previous value function <em>V(G</em>, <em>D)</em> and using the optimal discriminator, we can rewrite it in a single objective (as a function of <em>G</em>) that must be minimized by the generator:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/85879f7c-9cf7-408c-9672-ddabf7e0989d.png" style="width:42.17em;height:2.08em;"/></div>
<p class="mce-root">To better understand how a <strong>GAN</strong> works, we need to expand the previous expression:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0287e84f-5d7a-4c11-8cc3-54b8c26eb2f5.png" style="width:45.75em;height:3.67em;"/></div>
<p class="mce-root">Applying some simple manipulations, we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3058f142-4c04-4230-94dd-422eaaec8d37.png" style="width:56.58em;height:6.67em;"/></div>
<p class="mce-root">The last term represents the Jensen-Shannon divergence between <em>p<sub>data</sub></em> and <em>p<sub>g</sub></em>. This measure is similar to the Kullback-Leibler divergence, but it's symmetric and bounded between <em>0</em> and <em>log(2)</em>. When the two distributions are identical, <em>D<sub>JS</sub> = 0</em>, but if their supports (the value sets where <em>p(x) &gt; 0</em>) are disjoint, <em><span>D</span><sub>JS</sub></em><span><em> = log(2) (</em>while <em>D<sub>KL</sub> = ∞)</em>. Therefore, the value function can be expressed as:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1856a1c4-b41c-4ca3-8d60-a82a38f6a07e.png" style="width:18.00em;height:1.50em;"/></div>
<p>Now, it should be clearer that a GAN tries to minimize the Jensen-Shannon divergence between the data generating process and the generator distribution. In general, this procedure is quite effective; however, when the supports are disjointed, a GAN has no pieces of information about the true distance. This consideration (analyzed with more mathematical rigor in <em>Improved Techniques for Training GANs</em>, <em>Salimans T</em>., <em>Goodfellow I</em>., <em>Zaremba W</em>., <em>Cheung V</em>., <em>Radford A</em>.,<em> </em>and <em>Chen X</em>., <em>arXiv:1606.03498 [cs.LG]</em>) explains why training a GAN can become quite difficult, and, consequently, why the Nash equilibrium cannot be found in many cases. For these reasons, we are going to analyze an alternative approach in the next section.</p>
<p>The complete GAN algorithm (as proposed by the authors) is:</p>
<ol>
<li>Set the number of epochs, <em>N<sub>epochs</sub></em></li>
<li>Set the number of discriminator iterations, <em>N<sub>iter</sub></em> (in most cases, <em><span>N</span><sub>iter</sub> = 1</em>)</li>
<li>Set the batch size, <em>k</em></li>
<li>Define a noise generating process, <em>M</em> (for example, <em>U(-1, 1)</em>)</li>
<li>For <em>e=1</em> to <em>N<sub>epochs</sub></em>:
<ol>
<li>Sample <em>k</em> values from <em>X</em></li>
<li>Sample <em>k</em> values from <em>N</em></li>
<li>For <em>i=1</em> to <em>N<sub>iter</sub></em>:
<ol>
<li>Compute the gradients, <em>∇<sub>d</sub> V(G, D)</em> (only with respect to the discriminator variables). The expected value is approximated with a sample mean.</li>
<li>Update the discriminator parameters by <em>Stochastic Gradient Ascent</em> (as we are working with logarithms, it's possible to minimize the negative loss).</li>
</ol>
</li>
<li>Sample <em>k</em> values from <em>N</em></li>
<li>Compute the gradients, <em>∇<sub>g</sub><span> </span>V<sub>noise</sub>(G, D)</em> (only with respect to the generator variables)</li>
<li>Update the generator parameters by Stochastic Gradient Descent</li>
</ol>
</li>
</ol>
<div class="packt_infobox">As these models need to sample noisy vectors in order to guarantee the reproducibility, I suggest setting the random seed in both NumPy (<kbd>np.random.seed(...)</kbd>) and TensorFlow (<kbd>tf.set_random_seed(...)</kbd>). The default value chosen for all of these experiments is 1,000.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of DCGAN with TensorFlow</h1>
                </header>
            
            <article>
                
<p>In this example, we want to build a DCGAN (proposed in <em>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</em>, <em>Radford A</em>., <em>Metz L</em>., <em>Chintala S</em>., , <em>arXiv:1511.06434 [cs.LG]</em>) with the Fashion-MNIST dataset (obtained through the <kbd>keras</kbd> helper function). As the training speed is not very high, we limit the number of samples to 5,000, but I suggest repeating the experiment with larger values. The first step is loading and normalizing (between -1 and 1) the dataset:</p>
<pre>import numpy as np<br/><br/>from keras.datasets import fashion_mnist<br/><br/>nb_samples = 5000<br/><br/>(X_train, _), (_, _) = fashion_mnist.load_data()<br/>X_train = X_train.astype(np.float32)[0:nb_samples] / 255.0<br/>X_train = (2.0 * X_train) - 1.0<br/><br/>width = X_train.shape[1]<br/>height = X_train.shape[2] </pre>
<p>According to the original paper, the generator is based on four transpose convolutions with kernel sizes equal to <em>(4, 4)</em> and strides equal to <em>(2, 2)</em>. The input is a single multi-channel pixel (<em>1 × 1 × code_length</em>) that is expanded by subsequent convolutions. The number of filters is 1024, 512, 256, 128, and 1 (we are working with grayscale images). The authors suggest employing a symmetric-valued dataset (that's why we have normalized between -1 and 1), batch normalization after each layer, and leaky ReLU activation (with a default negative slope set to 0.2):</p>
<pre>import tensorflow as tf<br/><br/>def generator(z, is_training=True):<br/>    with tf.variable_scope('generator'):<br/>        conv_0 = tf.layers.conv2d_transpose(inputs=z,<br/>                                            filters=1024,<br/>                                            kernel_size=(4, 4),<br/>                                            padding='valid')<br/><br/>        b_conv_0 = tf.layers.batch_normalization(inputs=conv_0, training=is_training)<br/><br/>        conv_1 = tf.layers.conv2d_transpose(inputs=tf.nn.leaky_relu(b_conv_0),<br/>                                            filters=512,<br/>                                            kernel_size=(4, 4),<br/>                                            strides=(2, 2),<br/>                                            padding='same')<br/><br/>        b_conv_1 = tf.layers.batch_normalization(inputs=conv_1, training=is_training)<br/>        <br/>        conv_2 = tf.layers.conv2d_transpose(inputs=tf.nn.leaky_relu(b_conv_1),<br/>                                            filters=256,<br/>                                            kernel_size=(4, 4),<br/>                                            strides=(2, 2),<br/>                                            padding='same')<br/><br/>        b_conv_2 = tf.layers.batch_normalization(inputs=conv_2, training=is_training)<br/>        <br/>        conv_3 = tf.layers.conv2d_transpose(inputs=tf.nn.leaky_relu(b_conv_2),<br/>                                            filters=128,<br/>                                            kernel_size=(4, 4),<br/>                                            strides=(2, 2),<br/>                                            padding='same')<br/><br/>        b_conv_3 = tf.layers.batch_normalization(inputs=conv_3, training=is_training)<br/><br/>        conv_4 = tf.layers.conv2d_transpose(inputs=tf.nn.leaky_relu(b_conv_3),<br/>                                            filters=1,<br/>                                            kernel_size=(4, 4),<br/>                                            strides=(2, 2),<br/>                                            padding='same')<br/><br/>        return tf.nn.tanh(conv_4)</pre>
<p>The strides are set to work with 64 × 64 images (unfortunately, the Fashion-MNIST dataset has <span>28 × 28 samples, which cannot be generated with power-of-two modules); therefore, we are going to resize the samples while training. As we need to compute the gradients of the discriminator and generator separately, it's necessary to set the variable scope (using the context manager <kbd>tf.variable_scope()</kbd>) to immediately extract only the variables whose names have the scope as a prefix (for example, <kbd>generator/Conv_1_1/...</kbd>). The <kbd>is_training</kbd></span> parameter <span>is necessary to disable the batch normalization during the generation phase. </span></p>
<p>The discriminator is almost the same as a generator (the only main differences are the inverse convolution sequence and the absence of batch normalization after the first layer):</p>
<pre>import tensorflow as tf<br/><br/>def discriminator(x, is_training=True, reuse_variables=True):<br/>    with tf.variable_scope('discriminator', reuse=reuse_variables):<br/>        conv_0 = tf.layers.conv2d(inputs=x,<br/>                                  filters=128,<br/>                                  kernel_size=(4, 4),<br/>                                  strides=(2, 2),<br/>                                  padding='same')<br/><br/>        conv_1 = tf.layers.conv2d(inputs=tf.nn.leaky_relu(conv_0),<br/>                                  filters=256,<br/>                                  kernel_size=(4, 4),<br/>                                  strides=(2, 2),<br/>                                  padding='same')<br/>        <br/>        b_conv_1 = tf.layers.batch_normalization(inputs=conv_1, training=is_training)<br/>        <br/>        conv_2 = tf.layers.conv2d(inputs=tf.nn.leaky_relu(b_conv_1),<br/>                                  filters=512,<br/>                                  kernel_size=(4, 4),<br/>                                  strides=(2, 2),<br/>                                  padding='same')<br/>        <br/>        b_conv_2 = tf.layers.batch_normalization(inputs=conv_2, training=is_training)<br/>        <br/>        conv_3 = tf.layers.conv2d(inputs=tf.nn.leaky_relu(b_conv_2),<br/>                                  filters=1024,<br/>                                  kernel_size=(4, 4),<br/>                                  strides=(2, 2),<br/>                                  padding='same')<br/>        <br/>        b_conv_3 = tf.layers.batch_normalization(inputs=conv_3, training=is_training)<br/>        <br/>        conv_4 = tf.layers.conv2d(inputs=tf.nn.leaky_relu(b_conv_3),<br/>                                  filters=1,<br/>                                  kernel_size=(4, 4),<br/>                                  padding='valid')<br/>        <br/>        return conv_4</pre>
<p>In this case, we have an extra parameter (<kbd>reuse_variables</kbd>) that is necessary when building the loss functions. In fact, we need to declare two discriminators (fed with real samples and with the generator output), but they are not made up of separate layers; hence, the second one must reuse the variables defined by the first one. We can now create a graph and define all of the placeholders and operations:</p>
<pre>import tensorflow as tf<br/><br/>code_length = 100<br/><br/>graph = tf.Graph()<br/><br/>with graph.as_default():<br/>    input_x = tf.placeholder(tf.float32, shape=(None, width, height, 1))<br/>    input_z = tf.placeholder(tf.float32, shape=(None, code_length))<br/>    is_training = tf.placeholder(tf.bool)<br/>    <br/>    gen = generator(z=tf.reshape(input_z, (-1, 1, 1, code_length)), is_training=is_training)<br/>    <br/>    r_input_x = tf.image.resize_images(images=input_x, size=(64, 64))<br/>        <br/>    discr_1_l = discriminator(x=r_input_x, is_training=is_training, reuse_variables=False)<br/>    discr_2_l = discriminator(x=gen, is_training=is_training, reuse_variables=True)<br/>    <br/>    loss_d_1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(discr_1_l), logits=discr_1_l))<br/>    loss_d_2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(discr_2_l), logits=discr_2_l))<br/>    loss_d = loss_d_1 + loss_d_2<br/>    <br/>    loss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(discr_2_l), logits=discr_2_l))<br/>    <br/>    variables_g = [variable for variable in tf.trainable_variables() if variable.name.startswith('generator')]<br/>    variables_d = [variable for variable in tf.trainable_variables() if variable.name.startswith('discriminator')]<br/>    <br/>    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):<br/>        training_step_d = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(loss=loss_d, var_list=variables_d)<br/>        training_step_g = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(loss=loss_g, var_list=variables_g) </pre>
<p>The first step is defining the placeholders:</p>
<ul>
<li><kbd>input_x</kbd> contains the true samples drawn from <em>X</em></li>
<li><kbd>input_z</kbd> contains the noise samples</li>
<li><kbd>is_training</kbd> is a Boolean indicating whether or not the batch normalization must be active</li>
</ul>
<p>Then, we define the generator instance after reshaping the noise sample as a <em>(1 × 1 × code_length)</em> matrix (this is necessary to work efficiently with transpose convolutions). As this is a fundamental hyperparameter, I suggest testing different values and comparing the final performances.</p>
<p>As explained previously, the input images are resized before defining the two discriminators (the second one reuses the variables previously defined). The  <kbd>discr_1_l </kbd><span>instance</span> is fed with the true samples, while <kbd>discr_2_l</kbd> works with the generator output. </p>
<p>The next step is defining the loss functions. As we are working with logarithms, there can be stability problems when the values become close to 0. For this reason, it's preferable to employ the TensorFlow built-in function <kbd>tf.nn.sigmoid_cross_entropy_with_logits()</kbd>, which guarantees numerical stability in every case. This function takes a <em>logit</em> as input and applies the sigmoid transformation internally. In general, the output is:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/113fa46f-60de-4854-8b88-6afa017fec4a.png" style="width:29.25em;height:1.58em;"/></div>
<p>Therefore, setting the label equal to 1 forces the second term to be null, and vice versa. At this point, we need to create two lists containing the variables belonging to each scope (this can be easily achieved by using the <kbd>tf.trainable_variables()</kbd><span> function</span>, which outputs a list of all variables). The last step consists of defining the optimizers. As suggested in the official TensorFlow documentation, when working with batch normalizations, it's necessary to wrap the training operations in a context manager that checks whether all dependencies (in this case, batch average and variance) have been computed. We have employed the Adam optimizer with <em>η</em> = 0.0002, and a gradient momentum forgetting factor (<em>μ1</em>) equal to 0.5 (this is a choice motivated by the potential instability that a high momentum can yield). As it's possible to see, in both cases, the minimization is limited to a specific subset of the variables (providing a list through the <kbd>var_list</kbd><span> parameter</span>).</p>
<p>At this point, we can create a <kbd>Session</kbd> (we are going to use an <kbd>InteractiveSession</kbd>), initialize all variables, and start the training procedure (with 200 epochs and a batch size equal to 128):</p>
<pre>import numpy as np<br/>import tensorflow as tf<br/><br/>nb_epochs = 200<br/>batch_size = 128<br/>nb_iterations = int(nb_samples / batch_size)<br/><br/>session = tf.InteractiveSession(graph=graph)<br/>tf.global_variables_initializer().run()<br/><br/>samples_range = np.arange(nb_samples)<br/><br/>for e in range(nb_epochs * 5):<br/>    d_losses = []<br/>    g_losses = []<br/>    <br/>    for i in range(nb_iterations):<br/>        Xi = np.random.choice(samples_range, size=batch_size)<br/>        X = np.expand_dims(X_train[Xi], axis=3)<br/>        Z = np.random.uniform(-1.0, 1.0, size=(batch_size, code_length)).astype(np.float32)<br/>        <br/>        _, d_loss = session.run([training_step_d, loss_d], <br/>                                feed_dict={<br/>                                    input_x: X,<br/>                                    input_z: Z,<br/>                                    is_training: True<br/>                                })<br/>        d_losses.append(d_loss)<br/>        <br/>        Z = np.random.uniform(-1.0, 1.0, size=(batch_size, code_length)).astype(np.float32)<br/>        <br/>        _, g_loss = session.run([training_step_g, loss_g], <br/>                                feed_dict={<br/>                                    input_x: X,<br/>                                    input_z: Z,<br/>                                    is_training: True<br/>                                })<br/>        <br/>        g_losses.append(g_loss)<br/>        <br/>    print('Epoch {}) Avg. discriminator loss: {} - Avg. generator loss: {}'.format(e + 1, np.mean(d_losses), np.mean(g_losses)))</pre>
<p>The training step (with a single discriminator iteration) is split into two phases:</p>
<ol>
<li>Discriminator training with a batch of true images and noise samples</li>
<li>Generator training with a batch of noise samples</li>
</ol>
<p>Once the training process has finished, we can generate some images (50) by executing the generator with a matrix of noise samples:</p>
<pre>Z = np.random.uniform(-1.0, 1.0, size=(50, code_length)).astype(np.float32)<br/><br/>Ys = session.run([gen], <br/>                 feed_dict={<br/>                     input_z: Z,<br/>                     is_training: False<br/>                })<br/><br/>Ys = np.squeeze((Ys[0] + 1.0) * 0.5 * 255.0).astype(np.uint8)</pre>
<p>The result is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1f960533-fb13-4f56-8901-1f74b65f1635.png" style="width:43.33em;height:22.58em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Samples generated by a DCGAN trained with the Fashion-MNIST dataset</div>
<p>As an exercise, I invite the reader to employ more complex convolutional architectures and an RGB dataset such as CIFAR-10 (<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>). </p>
<div class="packt_infobox">The training phase of this example and the following one, even if limited to 5,000 samples, can be quite slow (around 12-15 hours) particularly when no GPU is available. The reader can simplify the examples by reducing the complexity of the networks (paying attention to the shapes) and reducing the number of samples. To avoid mismatches, I suggest adding the <kbd>print(gen.shape)</kbd> command after the generator instance. The expected shape should be <kbd>(?, 64, 64, 1)</kbd>. Alternatively, it's possible to employ smaller target dimensions (like 32 × 32), setting one of the strides (possibly the last one) equal to <kbd>(1, 1)</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Wasserstein GAN (WGAN)</h1>
                </header>
            
            <article>
                
<p>As explained in the previous section, one of the most difficult problems with standard GANs is caused by the loss function based on the Jensen-Shannon divergence, whose value becomes constant when two distributions have disjointed supports. This situation is quite common with high-dimensional, semantically structured datasets. For example, images are constrained to having particular features in order to represent a specific subject (this is a consequence of the manifold assumption discussed in <a href="f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml">Chapter 2</a>, <em>Introduction to Semi-Supervised Learning</em>). The initial generator distribution is very unlikely to overlap a true dataset, and in many cases, they are also very far from each other. This condition increases the risk of learning a wrong representation (a problem known as mode collapse), even when the discriminator is able to distinguish between true and generated samples (such a condition arises when the discriminator learns too quickly, with respect to the generator). Moreover, the Nash equilibrium becomes harder to achieve, and the GAN can easily remain blocked in a sub-optimal configuration.</p>
<p>In order to mitigate this problem, <span>Arjovsky, Chintala, and Bottou</span> (in <em>Wasserstein GAN</em>, <em>Arjovsky M</em>., <em>Chintala S</em>., <em>Bottou L.</em>, <em>arXiv:1701.07875 [stat.ML]</em>) proposed employing a different divergence, called the <em>Wasserstein distance</em> (or Earth Mover's distance), which is formally defined as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/db345cf3-03b3-4cd5-9293-99159663a83a.png" style="width:32.00em;height:2.17em;"/></div>
<p>The term <em>∏(p<sub>data</sub>, p<sub>g</sub>)</em> represents the set of all possible joint probability distributions between <em><span>p</span><sub>data</sub></em><span>, <em>p</em></span><em><sub>g</sub></em>. Hence, the Wasserstein distance is the infimum (considering all joint distributions) of the set of expected values of <em>||x - y||,</em> where <em>x</em> and <em>y</em> are sampled from the joint distribution μ. The main property of <em>D<sub>W</sub></em> is that, even when two distributions have disjointed support, its value is proportional to the actual distributional distance. The formal proof is not very complex, but it's easier to understand the concept intuitively. In fact, given two distributions with disjointed support, the infimum operator forces taking the shortest distance between each possible couple of samples. Clearly, this measure is more robust than the Jensen-Shannon divergence, but there's a practical drawback: it's extremely difficult to compute. As we cannot work with all possible joint distributions (nor with an approximation), a further step is necessary to employ this loss function. In the aforementioned paper, the authors proved that it's possible to apply a transformation, thanks to the Kantorovich-Rubinstein theorem (the topic is quite complex, but the reader can find further information in <em>On the Kantorovich–Rubinstein Theorem</em>, <em>Edwards D</em>. <em>A</em>., <em>Expositiones Mathematicae</em>, 2011):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d9eee8eb-17a7-4e32-93b1-43888c8c6ee6.png" style="width:37.00em;height:3.33em;"/></div>
<p class="mce-root">The first element to consider is the nature of <em>f(•)</em>. The theorem imposes considering only L-Lipschitz functions, which means that <span><em>f(•)</em> (assuming a real-valued function of a single variable) must obey:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c7dc640f-292c-4ed7-a144-4fa6ff1f6ff2.png" style="width:23.08em;height:1.50em;"/></div>
<p>At this point, the Wasserstein distance is proportional to the supremum (with respect to all <span>L-Lipschitz functions) of the difference between two expected values, which are extremely easy to compute. In a WGAN, the <em>f(•)</em> function is represented by a neural network; therefore, we have no warranties about the Lipschitz condition. To solve this problem, the author suggested a very simple procedure: clipping the discriminator (which is normally called Critic, and whose responsibility is to represent the parameterized function <em>f(•)</em>) variables after applying the corrections. If the input is bounded, all of the transformations will yield a bounded output; however, the clipping factor must be small enough (0.01, or even smaller) to avoid the additive effect of multiple operations leading to an inversion of the Lipschitz condition. This is not an efficient solution (because it slows down the training process when it's not necessary), but it allows for exploiting the Kantorovich-Rubinstein theorem, even when there are no formal constraints imposed on the function family.</span></p>
<p><span>Using a parameterized function (such as a Deep Convolutional Network), the Wasserstein distance becomes as follows (omitting the term <em>L</em>, which is constant):</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a63cc711-96c9-4bd8-ad48-731742488753.png" style="width:57.42em;height:1.75em;"/></div>
<p>In the previous expression, we explicitly extracted the generator output, and in the last step, separated the term that will be optimized separately. The reader has probably noticed that the computation is simpler than a standard GAN because, in this case, we have to average over only the <span><em>f(•)</em> values of a batch (there's no more need for a logarithm). However, as the Critic variables are clipped, the number of required iterations is normally larger, and in order to compensate the difference between the training speeds of the Critic and generator, it's often necessary to set <em>N<sub>critic</sub> &gt; 1</em> (the authors suggest a value equal to 5, but this is a hyperparameter that must be tuned in every specific context). </span></p>
<p>The complete<span> W</span>GAN<span> </span>algorithm is:</p>
<ol>
<li>Set the number of epochs, <em>N<sub>epochs.</sub></em></li>
<li>Set the number of Critic iterations, <em>N<sub>critic</sub></em><span> </span>(in most cases, <em><span>N</span><sub>iter</sub></em><span> </span>= 5).</li>
<li>Set the batch size, <em>k.</em></li>
<li>Set a clipping constant, c (for example, c = 0.01).</li>
<li>Define a noise generating process, M (for example, <em>U(-1, 1)</em>).</li>
<li>For <em>e=1</em> to <em>N<sub>epochs</sub></em>:
<ol>
<li>Sample <em>k</em> values from <em>X</em>.</li>
<li>Sample <em>k</em> values from <em>N</em>.</li>
<li>For <em>i=1</em> to <em>N<sub>critic</sub></em>:
<ol>
<li>Compute the gradients, <em>∇<sub>c</sub><span> D<sub>W</sub></span>(p<sub>data</sub>||p<sub>g</sub>)</em> (only with respect to the Critic variables). The expected values are approximated by sample means.</li>
<li>Update the Critic parameters by Stochastic Gradient Ascent.</li>
<li>Clip the Critic parameters in the range [<em>-c, c</em>].</li>
</ol>
</li>
<li>Sample <em>k</em> values from <em>N</em>.</li>
<li>Compute the gradients, <em>∇<sub>g</sub><span> </span>W<sub>noise</sub></em> (only with respect to the generator variables).</li>
<li>Update the generator parameters by Stochastic Gradient Descent.</li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of WGAN with TensorFlow</h1>
                </header>
            
            <article>
                
<p>This example can be considered a variant of the previous one because it uses the same dataset, generator, and discriminator. The only main difference is that in this case, the discriminator (together with its variable scope) has been renamed <kbd>critic()</kbd>:</p>
<pre>import tensorflow as tf<br/><br/>def critic(x, is_training=True, reuse_variables=True):<br/>    with tf.variable_scope('critic', reuse=reuse_variables):<br/>...</pre>
<p>At this point, we can step directly to the creation of the <kbd>Graph</kbd> containing all of the placeholders, operations, and loss functions:</p>
<pre>import tensorflow as tf<br/><br/>graph = tf.Graph()<br/><br/>with graph.as_default():<br/>    input_x = tf.placeholder(tf.float32, shape=(None, width, height, 1))<br/>    input_z = tf.placeholder(tf.float32, shape=(None, code_length))<br/>    is_training = tf.placeholder(tf.bool)<br/>    <br/>    gen = generator(z=tf.reshape(input_z, (-1, 1, 1, code_length)), is_training=is_training)<br/>    <br/>    r_input_x = tf.image.resize_images(images=input_x, size=(64, 64))<br/>        <br/>    crit_1_l = critic(x=r_input_x, is_training=is_training, reuse_variables=False)<br/>    crit_2_l = critic(x=gen, is_training=is_training, reuse_variables=True)<br/>    <br/>    loss_c = tf.reduce_mean(crit_2_l - crit_1_l)<br/>    loss_g = tf.reduce_mean(-crit_2_l)<br/>    <br/>    variables_g = [variable for variable in tf.trainable_variables() if variable.name.startswith('generator')]<br/>    variables_c = [variable for variable in tf.trainable_variables() if variable.name.startswith('critic')]<br/>    <br/>    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):<br/>        optimizer_c = tf.train.AdamOptimizer(0.00005, beta1=0.5, beta2=0.9).minimize(loss=loss_c, var_list=variables_c)<br/>        <br/>        with tf.control_dependencies([optimizer_c]):<br/>            training_step_c = tf.tuple(tensors=[tf.assign(variable, tf.clip_by_value(variable, -0.01, 0.01)) <br/>                                                for variable in variables_c])<br/>        <br/>        training_step_g = tf.train.AdamOptimizer(0.00005, beta1=0.5, beta2=0.9).minimize(loss=loss_g, var_list=variables_g)</pre>
<p>As it's possible to see, there are no differences in the placeholder section, in the definition of the generator, and in the image resizing to the target dimensions of 64 × 64. In the next block, we define the two Critic instances (which are perfectly analogous to the ones declared in the previous example).</p>
<p>The two loss functions are simpler than a standard GAN, as they work directly with the Critic outputs, computing the sample mean over a batch. In the original paper, the authors suggest using RMSProp as the standard optimizer, in order to avoid the instabilities that a momentum-based algorithm can produce. However, Adam, with lower forgetting factors (<em><span>μ</span><sub>1</sub> = 0.5</em> and <em><span>μ</span><sub>2</sub> = 0.9</em>) and a learning rate <em>η = 0.00005</em>, is faster than RMSProp, and doesn't lead to instabilities. I suggest testing both options, trying to maximize the training speed while preventing the mode collapse. Contrary to the previous example, in this case we need to clip all of the Critic variables after each training step. To avoid that, the internal concurrency can alter the order of some operations; it's necessary to employ a nested dependency control context manager. In this way, the actual <kbd>training_step_c</kbd> (responsible for clipping and reassigning the values to each variable) will be executed only after the <kbd>optimizer_c</kbd> step has completed.</p>
<p class="mce-root">Now, we can create the <kbd>InteractiveSession</kbd>, initialize the variables, and start the training process, which is very similar to the previous one:</p>
<pre>import numpy as np<br/>import tensorflow as tf<br/><br/>nb_epochs = 200<br/>nb_critic = 5<br/>batch_size = 64<br/>nb_iterations = int(nb_samples / batch_size)<br/><br/>session = tf.InteractiveSession(graph=graph)<br/>tf.global_variables_initializer().run()<br/><br/>samples_range = np.arange(nb_samples)<br/><br/>for e in range(nb_epochs):<br/>    c_losses = []<br/>    g_losses = []<br/>    <br/>    for i in range(nb_iterations):<br/>        for j in range(nb_critic):<br/>            Xi = np.random.choice(samples_range, size=batch_size)<br/>            X = np.expand_dims(X_train[Xi], axis=3)<br/>            Z = np.random.uniform(-1.0, 1.0, size=(batch_size, code_length)).astype(np.float32)<br/>            <br/>            _, c_loss = session.run([training_step_c, loss_c], <br/>                                    feed_dict={<br/>                                        input_x: X,<br/>                                        input_z: Z,<br/>                                        is_training: True<br/>                                    })<br/>            c_losses.append(c_loss)<br/>        <br/>        Z = np.random.uniform(-1.0, 1.0, size=(batch_size, code_length)).astype(np.float32)<br/>        <br/>        _, g_loss = session.run([training_step_g, loss_g], <br/>                                feed_dict={<br/>                                    input_x: np.zeros(shape=(batch_size, width, height, 1)),<br/>                                    input_z: Z,<br/>                                    is_training: True<br/>                                })<br/>        <br/>        g_losses.append(g_loss)<br/>        <br/>    print('Epoch {}) Avg. critic loss: {} - Avg. generator loss: {}'.format(e + 1, np.mean(c_losses), np.mean(g_losses)))</pre>
<p>The main difference is that, in this case, the Critic is trained <kbd>n_critic</kbd> times before each generator training step. The result of the generation of 50 random samples is shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e952ca1d-0bd6-4cdd-800d-3474c876432b.png" style="width:41.75em;height:22.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Samples generated by a WGAN trained with the Fashion MNIST dataset</span></div>
<p>As it's possible to see, the quality is slightly higher, and the samples are smoother. I invite the reader to also test this model with an RGB dataset, because the final quality is normally excellent.</p>
<div class="packt_tip">When working with these models, the training time can be very long. To avoid waiting to see the initial results (and to perform the required tuning), I suggest using Jupyter. In this way, it's possible to stop the learning process, check the generator ability, and restart it without any problem. Of course, the graph must remain the same, and the variable initialization must be performed only at the beginning.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed the main principles of adversarial training, and explained the roles of two players: the generator and discriminator. We described how to model and train them using a minimax approach whose double goal is to force the generator to learn the true data distribution <em>p<sub>data</sub></em>, and get the discriminator to distinguish perfectly between true samples (belonging to <em>p<sub>data</sub></em>) and unacceptable ones. In the same section, we analyzed the inner dynamics of a Generative Adversarial Network and some common problems that can slow down the training process and lead to a sub-optimal final configuration.</p>
<p>One of the most difficult problems experienced with standard GANs arises when the data generating process and the generator distribution have disjointed support. In this case, the Jensen-Shannon divergence becomes constant and doesn't provide precise information about the distance. An excellent alternative is provided by the Wasserstein measure, which is employed in a more efficient model, called WGAN. This method can efficiently manage disjointed distributions, but it's necessary to enforce the L-Lipschitz condition on the Critic. The standard approach is based on clipping the parameters after each gradient ascent update. This simple technique guarantees the L-Lipschitz condition, but it's necessary to use very small clipping factors, and this can lead to a slower conversion. For this reason, it's normally necessary to repeat the training of the Critic a fixed number of times (such as five) before each single generator training step.</p>
<p>In the next chapter, we are going to introduce another probabilistic generative neural model, based on a particular kind of neural network, called the Restricted Boltzmann Machine.</p>


            </article>

            
        </section>
    </body></html>