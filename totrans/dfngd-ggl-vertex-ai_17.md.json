["```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve\n%matplotlib inline\n```", "```py\nnews_df = pd.read_csv(\"WELFake_Dataset.csv\")\nprint(news_df.shape)\nnews_df.head()\n```", "```py\nnews_df.isnull().sum()\n```", "```py\nUnnamed: 0      0\ntitle         558\ntext           39\nlabel           0\ndtype: int64\n```", "```py\nnews_df.fillna('', inplace=True)\n```", "```py\nnews_df['content'] = [x + ' ' + y for x,y in zip(news_df.title, news_df.text)]\n```", "```py\nJoin words with spaces in between to create text.\nWe will require some NLP-specific libraries that will help in preparing the data. Here, we will utilize the nltk (Natural Language Toolkit) library to remove stopwords and apply the stemming operation. To convert our text data into a numerical format, we will utilize the TfidfVectorizer method from the sklearn library:\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n```", "```py\nnltk.download(\"stopwords\")\n```", "```py\ndef clean_and_prepare_content(text):\n    text = re.sub('[^a-zA-Z]',' ', text)\n    text = text.lower()\n    text_words = text.split()\n    imp_text_words = [word for word in text_words if not word in stopwords.words('english')]\n    stemmed_words = [porter_stemmer.stem(word) for word in imp_text_words]\n    processed_text = ' '.join(stemmed_words)\n    return processed_text\nporter_stemmer = PorterStemmer()\nnews_df['processed_content'] = news_df.content.apply(lambda content: clean_and_prepare_content(content))\n```", "```py\nX = news_df.processed_content.values\ny = news_df.label.values\nprint(X.shape, y.shape)\nHere's the output:\n(72134,) (72134,)\n```", "```py\nvectorizer = TfidfVectorizer()\nvectorizer.fit(X)\nX = vectorizer.transform(X)\nprint(X.shape)\nHere's the output:\n(72134, 162203)\n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y, random_state=42)\n```", "```py\nrf_model = RandomForestClassifier()\n```", "```py\nrf_model.fit(X_train, y_train)\n```", "```py\nRandomForestClassifier()\n```", "```py\ny_pred = rf_model.predict(X_test)\ny_proba = rf_model.predict_proba(X_test)\n```", "```py\n# classification report\nprint(\n    classification_report(\n        y_test,\n        y_pred,\n        target_names=['Real', 'Fake'],\n    )\n)\n```", "```py\n              precision    recall  f1-score   support\n        Real       0.94      0.92      0.93      7006\n        Fake       0.93      0.94      0.94      7421\n    accuracy                           0.93     14427\n   macro avg       0.93      0.93      0.93     14427\nweighted avg       0.93      0.93      0.93     14427\n```", "```py\ndef plot_roc_curve(y_true, y_prob):\n    \"\"\"\n    plots the roc curve based of the probabilities\n    \"\"\"\n    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n    plt.plot(fpr, tpr)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\nplot_roc_curve(y_test, y_proba[:,1])\n```", "```py\nconfusion_matrix(y_test, y_pred,)\n```", "```py\narray([[6455,  551],\n       [ 409, 7012]])\n```", "```py\nPROJECT_ID='417xxxxxxx97'\nREGION='us-west2'\nBUCKET_URI='gs://my-training-artifacts'\nDATA_LOCATION='gs://my-training-artifacts/WELFake_Dataset.csv'\n# prebuilt training containers\nTRAIN_VERSION = \"tf-cpu.2-9\"\nTRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n```", "```py\nfrom google.cloud import aiplatform\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n```", "```py\njob = aiplatform.CustomTrainingJob(\n    display_name=\"fake_news_detection\",\n    script_path=\"task.py\",\n    container_uri=TRAIN_IMAGE,\n    requirements=[\"nltk\"],\n)\n```", "```py\n# Start the training job\nmodel = job.run(\n    machine_type = \"n1-standard-16\",\n    replica_count=1,\n)\n```", "```py\nTraining script copied to:\ngs://my-training-artifacts/aiplatform-2023-09-04-04:41:36.367-aiplatform_custom_trainer_script-0.1.tar.gz.\nTraining Output directory:\ngs://my-training-artifacts/aiplatform-custom-training-2023-09-04-04:41:36.625\nView Training:\nhttps://console.cloud.google.com/ai/platform/locations/us-west2/training/8404xxxxxxxxxx898?project=417xxxxxxxx7\nCustomTrainingJob projects/417xxxxxxxx7/locations/us-west2/trainingPipelines/840xxxxxxxxxx92 current state:\nPipelineState.PIPELINE_STATE_RUNNING\nView backing custom job:\nhttps://console.cloud.google.com/ai/platform/locations/us-west2/training/678xxxxxxxxxxx48?project=417xxxxxxxx7\nCustomTrainingJob projects/417xxxxxxxx7/locations/us-west2/trainingPipelines/840xxxxxxxxxx92 current state:\nPipelineState.PIPELINE_STATE_RUNNING\n```", "```py\nimport torch\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import random_split, DataLoader\n```", "```py\nnews_df = pd.read_csv(\"WELFake_Dataset.csv\")\nnews_df.fillna('', inplace=True)\nnews_df['content'] = [x + ' ' + y for x,y in zip(news_df.title, news_df.text)]\n```", "```py\nnews_df['content'] = news_df['content'].apply(lambda text: text.lower())\n```", "```py\ntexts = news_df.content.values\nlabels = news_df.label.values\nprint(len(texts), len(labels))\n```", "```py\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n```", "```py\ndef prepare_tokenized_data(texts, labs='None'):\n    global labels\n    input_id_list = []\n    attention_masks = []\n```", "```py\n    for text in tqdm_notebook(texts):\n        encoded_dict = tokenizer.encode_plus(\n            text,\n            add_special_tokens = True,\n            truncation = 'longest_first',\n            max_length = 100,\n            pad_to_max_length = True,\n            return_attention_mask = True,\n            return_tensors = 'pt'\n        )\n        input_id_list.append(encoded_dict['input_ids'])\n       attention_masks.append(encoded_dict['attention_mask'])\n```", "```py\n    input_id_list = torch.cat(input_id_list, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    if labs != 'None':\n        labels = torch.tensor(labels)\n        return input_id_list, attention_masks, labels\n    else:\n        return input_id_list, attention_masks\n```", "```py\ninput_id_list, attention_masks, labels = (\n    prepare_tokenized_data(texts, labels)\n)\n```", "```py\ntensor_dataset = TensorDataset(input_id_list, attention_masks, labels)\n# lets keep 80% articles for training and 20% for test\ntrain_size = int(0.8 * len(tensor_dataset))\ntest_size = len(tensor_dataset) - train_size\ntrain_data, test_data = random_split(tensor_dataset, [train_size, test_size])\nprint(len(train_data.indices), len(test_data.indices))\n```", "```py\n(57707, 14427)\n```", "```py\nbatch_size = 32\nnum_workers = 4\ntrain_data_loader = DataLoader(\n    dataset=train_data,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=num_workers,\n)\n# test data loader with batch size of 1\ntest_data_loader = DataLoader(\n    dataset=test_data,\n    batch_size=1,\n    shuffle=False,\n)\n```", "```py\ndevice = 'cpu'\nbert_model = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=2,\n    output_attentions=False,\n    output_hidden_states=False,\n)\nbert_model.to(device)\n```", "```py\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n   . . . . . . . . . . .\n   . . . . . . . . . . .\n```", "```py\noptimizer = torch.optim.AdamW(\n    bert_model.parameters(),\n    lr=6e-6,\n    eps=1e-8,\n)\n```", "```py\nnum_epochs = 3\nsteps_per_epoch = len(train_data_loader)\ntotal_steps = steps_per_epoch * num_epochs\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = total_steps,\n)\n```", "```py\nbert_model.train()\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for i, (ids, masks, labels) in enumerate(train_data_loader):\n        ids = ids.to(device)\n        masks = masks.to(device)\n        labels = labels.to(device)\n        loss = bert_model(ids, token_type_ids=None, attention_mask=masks, labels=labels)[0]\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        total_loss += loss.item()\n    print('Epoch: {}, Loss: {:.4f}'.format(epoch+1, total_loss / steps_per_epoch))\n```", "```py\nEpoch: 1, Loss: 0.0803\nEpoch: 2, Loss: 0.0229\nEpoch: 3, Loss: 0.0112\n```", "```py\n# save trained model locally\ntorch.save(bert_model.state_dict(), 'BERT.ckpt')\n```", "```py\nbert_model.eval()\nbert_model.load_state_dict(\n    torch.load('BERT.ckpt'),\n)\nOutput:\n<All keys matched successfully>\n```", "```py\ncorrect_predictions = 0\npredictions = []\nreals = []\n```", "```py\nfor i, (ids, masks, labels) in enumerate(test_data_loader):\n    ids = ids.to(device)\n    masks = masks.to(device)\n    labels = labels.to(device)\n    bert_out = bert_model(ids, token_type_ids=None, attention_mask=masks, labels=labels)[1]\n    prediction = torch.max(bert_out, 1)[1][0].item()\n    true_label = labels[0].item()\n    correct_predictions += int(prediction == true_label)\n    predictions.append(prediction)\n    reals.append(true_label)\n```", "```py\navg_correct_predictions = correct_predictions / len(test_data)\nprint('Accuracy: {:.4f}\\n'.format(avg_correct_predictions))\n```", "```py\nAccuracy: 0.9902\n```", "```py\nprint(confusion_matrix(reals, predictions,))\n```", "```py\n[[7025   53]\n [  88 7261]]\n```", "```py\nprint(\n    classification_report(\n        reals,\n        predictions,\n        target_names=['Real', 'Fake'],\n    )\n)\n```", "```py\n              precision    recall  f1-score   support\n        Real       0.99      0.99      0.99      7078\n        Fake       0.99      0.99      0.99      7349\n    accuracy                           0.99     14427\n   macro avg       0.99      0.99      0.99     14427\nweighted avg       0.99      0.99      0.99     14427\n```"]