<html><head></head><body>
		<div id="_idContainer180">
			<h1 id="_idParaDest-203"><em class="italic"><a id="_idTextAnchor209"/>Chapter 10</em>: XAI Industry Best Practices</h1>
			<p>In the first section of this book, we discussed various concepts related to <strong class="bold">Explainable AI</strong> (<strong class="bold">XAI</strong>). These concepts were established through years of research, considering various application domains of <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>). However, the need for XAI for industrial applications has been felt very recently as AI adoption in industrial use cases is increasing. Unfortunately, the general awareness of XAI for industrial use cases is still lacking due to certain challenges and gaps in how to implement human-friendly explainability methods. </p>
			<p>In <em class="italic">Section 2</em>, <em class="italic">Practical Problem Solving</em>, we covered many XAI Python frameworks that are popularly used for interpreting the working of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models. However, only understanding how to apply the XAI Python frameworks in practice is not sufficient for industrial problems. Industrial problems require solutions that are scalable and sustainable. So, it is very important for us to discuss the best practices of XAI for scalable and sustainable AI solutions used for industrial problems.</p>
			<p>Over the years, XAI has evolved a lot. From being a topic of academic research, XAI is now a powerful tool in the toolkit of AI and ML industrial practitioners. However, XAI has many open challenges on which the research community is still working to bring AI closer to end users. So, we will discuss the existing challenges of XAI and the general recommendations for designing an explainable ML system while considering the open challenges. Also, the quality of AI/ML systems is as good as the quality of the underlying data. Therefore, we will also focus on the importance of adopting a data-first approach for model explainability. </p>
			<p>The XAI research community believes that XAI is a multi-disciplinary perspective that should be centered around the end user. So, we will discuss the concept of <strong class="bold">interactive machine learning</strong> (<strong class="bold">IML</strong>) to create high user engagement for industrial AI systems. Finally, we will cover the importance of providing actionable suggestions and insights using AI/ML as an approach to decipher the complex nature of AI models, thereby making AI explainable and increasing users' trust. </p>
			<p>Unlike the previous chapters, in this chapter, we will not focus on the practical applications or learn a new XAI framework. Instead, our goal is to understand the best practices of XAI for industrial use cases. So, in this chapter, we are going to discuss the following topics:</p>
			<ul>
				<li>Open challenges of XAI</li>
				<li>Guidelines for designing explainable ML systems</li>
				<li>Adopting a data-first approach for explainability</li>
				<li>Emphasizing IML for explainability</li>
				<li>Emphasizing prescriptive insights for explainability</li>
			</ul>
			<p>So, let's find out more about these topics next.</p>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor210"/>Open challenges of XAI</h1>
			<p>As <a id="_idIndexMarker720"/>briefly discussed, there have been some significant advances in the field of XAI. XAI is no longer just a topic of academic research; the availability of XAI frameworks has made XAI an essential tool for industrial practitioners. But are these frameworks sufficient to increase AI adoption? Unfortunately, the answer is no. XAI is yet to mature further as there are certain open challenges that, once resolved, can significantly bridge the gap between AI and the end user. Let's discuss these open challenges next:</p>
			<ul>
				<li><em class="italic">Shifting focus between the model developer and the end user</em>: After exploring many XAI frameworks throughout this book, you might have also felt that the explainability provided by most of the frameworks requires technical knowledge of ML, mathematics, or statistics to truly understand the working of the model. This is because the explainability methods or algorithms were primarily designed for ML experts or model developers. </li>
			</ul>
			<p>As more and more end users start utilizing AI models and systems, the need for non-technical human-friendly explanations is growing. So, for industrial applications, dynamically shifting the focus between the model developer and the end user is a challenge. </p>
			<p>To a non-technical end user, a simple explanation method such as feature importance visualization can become really complicated unless explicit information is provided. In order to mitigate this challenge, the general recommendation is to design user-centric AI systems. Similar to any software application or system, the user should be involved in the development process early on to understand their requirements and include their expertise while designing the application and not post-production of the application.</p>
			<ul>
				<li><em class="italic">Lack of stakeholder participation</em>: From the <a id="_idIndexMarker721"/>previous point, although the recommended action is to involve the end users early on in the development process of the AI system, onboarding a stakeholder in the development process can also be a challenge. For most industrial use cases, AI solutions are developed in isolation without involving the final stakeholder(s). Following the design principles from the field of <strong class="bold">Human-Computer Interaction</strong> (<strong class="bold">HCI</strong>), the <a id="_idIndexMarker722"/>user should be involved in the loop during the development process. </li>
			</ul>
			<p>Considering high stake domains such as healthcare, finance, legal and regulatory, getting stakeholders and domain experts can be an extremely tedious and expensive process. The stakeholder's availability can be a challenge. Their interest or motivation to participate in the development process can be low even with necessary incentives and compensation. Due to these difficulties in onboarding end users into the development process, designing a user-centric AI system is difficult. </p>
			<p>The most recommended action to tackle this challenge is through a collaboration between industry and academia. Usually, academic institutions such as medical schools, law schools, or other universities have broader access to real participants or students who belong to the respective fields and can be <em class="italic">pseudo</em> participants.</p>
			<p>The following diagram illustrates how XAI is a multi-disciplinary perspective:</p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/B18216_10_001.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – XAI is a multi-disciplinary perspective</p>
			<ul>
				<li><em class="italic">Application-specific challenges</em>: Different application domains need explainability of<a id="_idIndexMarker723"/> different types. For example, in an AI-based loan approval system, influence-based or example-based feature explanations can be really helpful. However, for an application to detect COVID-19 infections from X-ray images, highlighting or localizing the region of the infection can be more helpful. So, each application can have its own requirement and definition of explainability and, thus, any general XAI framework might not be very effective. </li>
				<li><em class="italic">Lack of quantitative evaluation metrics</em>: The quantitative evaluation of explanation methods has been an important research topic. Unfortunately, there is still no tool or framework that exists that can quantitatively evaluate the quality of explanation methods. This is mostly because many diverse AI algorithms are at work on different types of data. Consequently, there are many definitions of model explainability and many approaches for XAI. So, it is very hard to generalize quantitative evaluation metrics that can work with all of the different explanation methods. </li>
			</ul>
			<p>Currently, qualitative evaluation methods such as <em class="italic">Trust</em>, <em class="italic">Usefulness</em>,<em class="italic"> Actionability</em>, <em class="italic">Coherence with prior beliefs</em>, <em class="italic">Impact</em>, and more are used. To learn more about these metrics, take a look at <em class="italic">Understanding Machines: Explainable AI</em> from <em class="italic">Accenture Labs</em>, which is available at <a href="https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf">https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf</a>. Additionally, take a look at <em class="italic">Explanation in Artificial Intelligence: Insights from the Social Sciences</em> from <em class="italic">Tim Miller</em>, which is available at <a href="https://arxiv.org/pdf/1706.07269.pdf">https://arxiv.org/pdf/1706.07269.pdf</a>. </p>
			<p>The qualitative<a id="_idIndexMarker724"/> evaluation methods are, indeed, user-centric and use the principles of HCI to collect feedback from the end user, but usually, quantitative metrics are more useful when comparing different methods. However, I am hopeful that tools<a id="_idIndexMarker725"/> such as <em class="italic">Quantus</em> (<a href="https://github.com/understandable-machine-intelligence-lab/Quantus">https://github.com/understandable-machine-intelligence-lab/Quantus</a>), which is used to evaluate explanation methods for neural networks, will mature significantly in a few years and it will be easier to evaluate explanation methods.</p>
			<ul>
				<li><em class="italic">Lack of actionable explanations</em>: Most explanation methods don't provide actionable insights to the end user. So, designing explainable AI/ML systems that can provide actionable explanations can be challenging. Counterfactual explanations, what-if analysis, and interactive visualization-based explanations are the only explanation methods that allow the user to observe the change in outcome when the input features are altered. I would recommend increasing the usage of these actionable explanation methods to develop explainable AI/ML systems. </li>
				<li><em class="italic">Lack of contextual explanations</em>: Any ML algorithm that is deployed in production depends on the specific use case and the underlying data. Due to this, there is always a trade-off between explainability, model performance, fairness, and privacy. So, understanding the context of explainability is an existing challenge that any general XAI framework cannot provide accurately. So, the recommendation to mitigate this challenge is to design personalized explainable ML systems for a specific use case rather than a generalized implementation. </li>
			</ul>
			<p>If you want to explore more in this area, you can take a look at <em class="italic">Verma et al.'s</em> work, <em class="italic">Pitfalls of Explainable ML: An Industry Perspective</em> (<a href="https://arxiv.org/abs/2106.07758">https://arxiv.org/abs/2106.07758</a>), to learn more about the typical challenges of XAI. All of these open <a id="_idIndexMarker726"/>challenges are interesting research problems that you can explore to help the research community progress in this field. Now that we have discussed the open challenges of XAI, next, let's discuss the guidelines for designing explainable ML systems for industrial use cases, considering the open challenges.</p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor211"/>Guidelines for designing explainable ML systems</h1>
			<p>In this section, we <a id="_idIndexMarker727"/>will discuss the recommended guidelines for designing an explainable ML system from an industry perspective while considering the open challenges of XAI, as discussed in the previous section. All of these guidelines have been carefully collated from various publications, conference keynotes, and panel discussions from various experts in the field of XAI, ML, and software systems. It is true that every ML and AI problem is unique in its own way, and so, it is hard to generalize any recommendations. But many AI organizations have adopted the following list of guidelines for designing explainable and user-friendly ML systems:</p>
			<ul>
				<li><em class="italic">Identify the target audience of XAI and their usability context</em>: The definition of explainability depends on the user using the AI system. <em class="italic">Arrieta et al</em>., in their work <em class="italic">Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities, and Challenges toward Responsible AI</em>, have highlighted the importance of identifying the target audience of XAI when designing explainable AI systems. </li>
			</ul>
			<p>An AI system can have different audiences such as technical stakeholders (that is, data scientists, ML experts, product owners, and developers), business stakeholders (that is, managers and executive leaders), domain experts (that is, doctors, lawyers, insurance agents, and more), legal and regulatory agencies, and non-technical end users. Every audience might have a different need for explainability, so accordingly, the explanation methods should try to address the best needs of the audience. </p>
			<p>As a preliminary step, identifying the target audience of the explainable system along with the situation or context in which they are going to use the system helps a lot in the design process. For example, for medical experts relying on ML models to predict the risk of diabetes, the choice of explanation methods depends on their actual needs. If their need is to suggest actions to improve the health conditions of diabetic patients, then counterfactual examples can be really useful. However, if their purpose is to find out the factors that are leading to the increase in the <a id="_idIndexMarker728"/>risk of diabetes, then feature-based explanations methods are more relevant. </p>
			<p><em class="italic">Figure 10.2</em> illustrates the various target audience of explainable AI systems:</p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/B18216_10_002.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Identifying the target audience of XAI</p>
			<ul>
				<li><em class="italic">Shortlisting the XAI techniques based on the user's needs</em>: Once the target audience and their usability context have been identified, along with the necessary technical details about the type of the dataset (for instance, tabular, images, or textual) and the ML algorithm used for training the model, shortlisting a list of possible explanation methods, as covered in <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>,<em class="italic"> Model Explainability Methods</em>, is very important.</li>
			</ul>
			<p>These shortlisted explanation methods should fit in with the software system that the target users will use to interact with the AI models. This means the explanation techniques should be well integrated with the software applications or interfaces and should even be considered during the design process of the software for a consistent user experience. </p>
			<ul>
				<li><em class="italic">Human-centered XAI: An iterative process of translating and evaluating XAI in specific domains involving the end user</em>. Similar to the design life cycle of a software system using HCI, XAI is also an iterative process. It should be human-centered and should be evaluated continuously to assess the impact. In the <em class="italic">User-centric system design using XAI</em> section of <a href="B18216_11_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 11</em></a>, <em class="italic">End User-Centered Artificial Intelligence,</em> I have included other important aspects to consider for a human-centered XAI design process.</li>
				<li><em class="italic">The importance of the feedback loop in XAI</em>: All explainable AI systems should have the option to capture the end user's feedback to assess the impact, relevance, effectiveness, and trust of the explanations provided by the system. It is never possible to consider all edge cases and all preferences of the end users during the design and the initial development process. But using the feedback loop, developers can collect specific feedback about the explanation methods and <a id="_idIndexMarker729"/>modify them if needed.</li>
				<li><em class="italic">The importance of scalability in the design process</em>: Similar to serving ML models for production systems, explainability should also be served in modular and scalable approaches. The best way to serve model explanations is by designing <strong class="bold">scalable web APIs</strong> to be deployed in centralized cloud servers. So, when XAI is implemented in <a id="_idIndexMarker730"/>practice, do make sure that the explanations are being served through web APIs so that they can be easily integrated with any software interface or application.</li>
				<li><em class="italic">Toggling between the data, the interface, and actionable insights</em>: It has been observed by many experts that, for end users, their satisfaction with the model explanation method is a trade-off between how well the explanation is being connected to the underlying dataset (or their prior beliefs), how the users are able to interact with the ML system to gain more confidence in it, and how well the explanations<a id="_idIndexMarker731"/> encourage<a id="_idIndexMarker732"/> them to take actions to get their desired output. <strong class="bold">Data-centric XAI</strong>, <strong class="bold">IML</strong>, and <strong class="bold">actionable explanations</strong> are broader<a id="_idIndexMarker733"/> research topics that should be considered when designing the explainable AI system for <a id="_idIndexMarker734"/>industrial use cases.</li>
			</ul>
			<p>So, we have learned about the open challenges of XAI and discussed the design guidelines considering the open challenges. We now have a fair idea of what to consider when designing explainable ML systems. Next, let's elaborate on the last recommended guideline in the upcoming sections to carefully understand why it is important. Let's start our discussion with the importance of using a data-centric approach for explainability.</p>
			<h1 id="_idParaDest-206">Adopting a data-fi<a id="_idTextAnchor212"/>rst approach for explainability</h1>
			<p>In <a href="B18216_03_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, <em class="italic">Data-Centric Approaches</em>, we discussed the importance and various techniques<a id="_idIndexMarker735"/> of <strong class="bold">Data-Centric XAI</strong>. Now, in this section, we will elaborate on <a id="_idIndexMarker736"/>how adopting a data-first approach for explainability helps in gaining users' trust in industrial use cases. </p>
			<p>Data-centric AI is based on the fundamental idea that <em class="italic">the quality of the ML model is as good as the quality of the underlying dataset used for training the model</em>. For industrial use cases, dealing with poor-quality datasets is a major challenge for most data scientists. Unfortunately, data quality is often ignored as data scientists and ML experts are expected to cast their <em class="italic">magic</em> of ML to build models that are close to 100% accurate. Consequently, ML experts simply try to<a id="_idIndexMarker737"/> follow <strong class="bold">model-centric approaches</strong> such as tuning hyperparameters or using complex algorithms to boost model performance. Even if the model performance increases slightly, with the increase in complexity, explainability decreases. The lack of explainability increases the skepticism of the business stakeholders. Also, issues relating to data quality such as the presence of <em class="italic">data anomalies</em>, <em class="italic">data leakage</em>, <em class="italic">data drift</em>, and other issues, as discussed in <a href="B18216_03_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, <em class="italic">Data-Centric Approaches</em>, significantly increase. <em class="italic">In that case, what do we do?</em></p>
			<p>The answer is to adopt a data-centric approach to explain the ML process. Using data-centric explainability methods <a id="_idIndexMarker738"/>such as <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>), we can extract insights about the dataset such as any interesting patterns, correlations, monotonicity, or trends from the features used in the dataset. EDA and data analysis between the training data and the inference data also helps you to identify data quality issues. If there are issues in the dataset, it is always recommended that you inform the business stakeholder about the limitations of poor data quality and set the expectations correctly about the model performance. So, even if the model predictions are not correct, the business<a id="_idIndexMarker739"/> stakeholder will understand the limitations instead of doubting the ML system. </p>
			<p>But <em class="italic">why don't we try out the other XAI frameworks and methods covered throughout this book</em>? <em class="italic">How would adopting a data-first approach for explainability help</em>? Well, you can and you should try out other relevant XAI methods if applicable, but data-centric explainability is always easier to explain to a non-technical user. Especially, with the <em class="italic">data profiling method</em>, as discussed in <a href="B18216_03_ePub.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, <em class="italic">Data-Centric Approaches</em>, we can identify the range of values of features present in the dataset for each category (if there is a classification problem) or each bin of the prediction variable (if there is a regression problem) and compare the model predictions with the profiled values. Simple comparisons with the profiled values are easier to understand as compared to complicated mathematical concepts such as <em class="italic">Shapley values</em> or other algorithms used in XAI frameworks. </p>
			<p>Another reason why data-centric approaches are preferable is because of the user's trust in historical data. Generally, it is observed that most stakeholders have more trust in historical data as compared to AI models. For example, in the spring season, if an AI weather forecasting model predicts the occurrence of snowfall, most end users would be hesitant to trust the prediction. That's because spring is always associated with sunshine and flowers blooming due to the observations throughout the world over many years. But if the model also indicates the occurrence of snowfall in the last few years during the same time or even indicates that there was snowfall in close proximity in the last few days, the user's trust would be greater. So, it is recommended that you, first, explore data-centric explainability and then look at other explainability methods for any industrial ML problems.</p>
			<p>The following<a id="_idIndexMarker740"/> diagram illustrates how data-centric XAI can be very close to the natural ways of providing explainability, thereby improving the ease of understanding: </p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/B18216_10_003.jpg" alt="Figure 10.3 – The importance of a data-centric approach for explainability&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – The importance of a data-centric approach for explainability</p>
			<p> Next, we will discuss interactive ML to boost the end user's trust.</p>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor213"/>Emphasizing IML for explainability</h1>
			<p>IML is the <a id="_idIndexMarker741"/>paradigm of designing intelligent user interfaces to facilitate ML and AI algorithms with the help of user interactions. Using IML to steer the usage of ML systems to increase the trust of the end user has been an important research topic for the AI and HCI research community over the last few years. Many works of research literature recommend using IML to increase user engagement for AI systems. <em class="italic">Recent Research Advances on Interactive Machine Learning</em> by <em class="italic">Jiang et al</em>. (<a href="https://arxiv.org/abs/1811.04548">https://arxiv.org/abs/1811.04548</a>) talks about some of the significant progress that has been made in the field of IML and how it is closely associated with the increasing trust and transparency of ML algorithms. </p>
			<p>IML is another interesting approach that is used by the XAI community to explain ML models. Even in frameworks such as <em class="italic">DALEX</em> and <em class="italic">Explainerdashboards</em>, as covered in <a href="B18216_09_ePub.xhtml#_idTextAnchor172"><em class="italic">Chapter 9</em></a>, <em class="italic">Other Popular XAI Frameworks</em>, providing interactive dashboards and web interfaces that end users can interact with to explore the data, model, and predictions are considered as a way for model explainability. IML helps the user in the following ways:</p>
			<ul>
				<li>Explore the dataset through graphs and visuals, thereby making it easier for the user to observe and remember key insights from the data.</li>
				<li>Gain more confidence about the ML systems, as the intelligent user interfaces allow the user to make changes and observe the outcome. It makes it easier for the user to figure out how the model behaves while considering any changes in input.</li>
				<li>Typically, what-if analysis and local explainability are improved when interactive interfaces are provided. </li>
				<li>IML gives more control to the user to explore the system, and IML usually considers a user-centric design process for providing customized interfaces tailor-made for a specific use case.</li>
			</ul>
			<p>In short, IML<a id="_idIndexMarker742"/> improves the user experience and, thus, helps to boost the adoption of AI models. I would strongly recommend using interactive user interfaces as part of explainable ML systems along with serving model explainability using modularized web APIs. You can read the following article to find out more about the usefulness of IML for business problems: <a href="https://hub.packtpub.com/what-is-interactive-machine-learning/">https://hub.packtpub.com/what-is-interactive-machine-learning/</a>. </p>
			<p>The following diagram illustrates the difference between conventional ML and IML: </p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/B18216_10_004.jpg" alt="Figure 10. 4 – Comparing conventional ML with IML&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10. 4 – Comparing conventional ML with IML</p>
			<p>As you can see<a id="_idIndexMarker743"/> from the preceding diagram, using IML, the end user can directly interact with the intelligent user interface to get predictions, explanations, and insights. Next, let's discuss the importance of prescriptive insights for explainable ML systems.</p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor214"/>Emphasizing prescriptive insights for explainability</h1>
			<p>Prescriptive insight <a id="_idIndexMarker744"/>is a popular jargon used in data analysis. It means providing actionable recommendations derived from the dataset to achieve the desired outcome. It is often considered to be a catalyst in the entire process of data-driven decision-making. In the context of XAI, explanation methods such as <em class="italic">counterfactual examples</em>, <em class="italic">data-centric XAI</em>, and <em class="italic">what-if analysis</em> are prominently used for providing actionable suggestions to the user. </p>
			<p>Along with counterfactuals, the concept of <strong class="bold">actionable recourse in ML</strong> is also used for generating prescriptive insights. <strong class="bold">Actionable recourse</strong> is <a id="_idIndexMarker745"/>the ability of a user to alter the prediction of an ML model by modifying the features that are actionable. But <em class="italic">how is it different from counterfactuals?</em> Actionable recourse can be considered to be an extension of the idea of counterfactual examples, which uses actionable features instead of all the features present in the dataset. </p>
			<p>Now, <em class="italic">what do we mean by actionable features?</em> Considering a practical scenario, it is not feasible for us to change all the features present in a dataset in any direction to reach the desired outcome. For example, features such as <em class="italic">age</em>, <em class="italic">gender</em>, and <em class="italic">race</em> cannot be changed in any direction to obtain the desired output. Unfortunately, algorithms used for generating counterfactual examples do not consider the practical feasibility of changing a feature. </p>
			<p>Let's suppose that an ML model is being used to estimate the risk of diabetes. For a diabetic patient, if we want to use counterfactual examples to recommend how to reduce the risk of diabetes, it is not practically feasible for the patient to reduce their age by 10 years or change their gender to decrease the risk. So, these are non-actionable features. Even though theoretically altering these features can change the model prediction, it is not practical to change these features. Therefore, the concept of actionable recourse is more like a controlled counterfactual generation process that is applied to actionable features and considers a practically feasible boundary condition for the feature values. </p>
			<p>To generate<a id="_idIndexMarker746"/> prescriptive insights, I would recommend that you use actionable recourse as it considers the practical feasibility and difficulty of altering a feature value to get the desired outcome. You can find more about actionable recourse from <em class="italic">Ustun et al.'s</em> work, <em class="italic">Actionable Recourse in Linear Classification</em> (<a href="https://arxiv.org/abs/1809.06514">https://arxiv.org/abs/1809.06514</a>), along with their GitHub project at <a href="https://github.com/ustunb/actionable-recourse">https://github.com/ustunb/actionable-recourse</a>. </p>
			<p>But <em class="italic">are prescriptive insights really necessary in XAI</em>? Well, the answer is <em class="italic">yes</em>! The following list of reasons explains why prescriptive insights are important in XAI:</p>
			<ul>
				<li>Prescriptive insights are actions suggested to the user to get the desired result. In most industrial use cases, explainability is incomplete if the user is unaware of how to reach their desired outcome. </li>
				<li>Generating prescriptive insights is a proactive method for explaining the working of ML models. That's because it allows the user to take necessary proactive actions rather than trusting the passive explanations provided to them.</li>
				<li>It increases the user's faith in the system by giving a sense of control over the system. Using actionable explanations, the user is empowered to alter the model prediction.</li>
				<li>It increases the ability of business stakeholders for making data-driven decisions for the organization.</li>
			</ul>
			<p>These are the <a id="_idIndexMarker747"/>main reasons why you should always consider generating explanations that are actionable when designing explainable AI systems for industrial problems. <em class="italic">Figure 10.5</em> illustrates how prescriptive insights using XAI can provide actionable recommendations for the user to get their desired outcome: </p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/B18216_10_005.jpg" alt="Figure 10.5 – The importance of prescriptive insights for explainability&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – The importance of prescriptive insights for explainability</p>
			<p>With this, we have arrived at the end of this chapter. Let's summarize the topics discussed next. </p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor215"/>Summary</h1>
			<p>This chapter focused on the best practices for designing explainable AI systems for industrial problems. In this chapter, we discussed the open challenges of XAI and the necessary design guidelines for explainable ML systems, considering the open challenges. We also highlighted the importance of considering data-centric approaches of explainability, IML, and prescriptive insights for designing explainable AI/ML systems.</p>
			<p>If you are a technical expert, architect, or business leader responsible for using AI to solve industrial problems, this chapter has helped you to learn some of the most important guidelines for designing explainable AI/ML systems considering the open challenges in XAI. If you are a researcher in the field of AI or HCI, some of the open challenges discussed in the chapter could be interesting research topics to consider. Finding solutions to these challenges can lead to significant progress in the field of XAI.</p>
			<p>In the next chapter, we will cover the principles of <strong class="bold">End User-Centered Artificial Intelligence</strong> to bridge the AI-end user gap. </p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor216"/>References</h1>
			<p>For additional information about the topics covered in this chapter, please refer to the following resources:</p>
			<ul>
				<li><em class="italic">Pitfalls of Explainable ML: An Industry Perspective</em>: <a href="https://arxiv.org/abs/2106.07758">https://arxiv.org/abs/2106.07758</a> </li>
				<li>The Quantus framework in GitHub: <a href="https://github.com/understandable-machine-intelligence-lab/Quantus">https://github.com/understandable-machine-intelligence-lab/Quantus</a></li>
				<li><em class="italic">Explanation in Artificial Intelligence: Insights from the Social Sciences</em>: <a href="https://arxiv.org/pdf/1706.07269.pdf">https://arxiv.org/pdf/1706.07269.pdf</a> </li>
				<li><em class="italic">Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations</em>: <a href="https://arxiv.org/abs/2202.06861">https://arxiv.org/abs/2202.06861</a> </li>
				<li><em class="italic">Understanding Machines: Explainable AI</em> from <em class="italic">Accenture Labs</em>: <a href="https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf">https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf</a> </li>
				<li><em class="italic">Actionable Recourse in Linear Classification</em> by <em class="italic">Ustun et al</em>:<em class="italic"> </em><a href="https://arxiv.org/abs/1809.06514">https://arxiv.org/abs/1809.06514</a></li>
				<li>Actionable recourse in ML: <a href="https://github.com/ustunb/actionable-recourse">https://github.com/ustunb/actionable-recourse</a></li>
				<li><em class="italic">Questioning the AI: Informing Design Practices for Explainable AI User Experiences</em> by <em class="italic">Liao et al</em>: <a href="https://dl.acm.org/doi/10.1145/3313831.3376590">https://dl.acm.org/doi/10.1145/3313831.3376590</a> </li>
				<li><em class="italic">Advances and Open Questions in Explainable AI (XAI): A practical perspective from an HCI researcher</em> by <em class="italic">Q. Vera Liao</em>: <a href="http://qveraliao.com/aaai_panel.pdf">http://qveraliao.com/aaai_panel.pdf</a></li>
			</ul>
		</div>
	</body></html>