- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing Applications with Differential Privacy Using Open Source Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore open source frameworks (**PyDP**, **PipelineDP**,
    **tmlt-analytics**, **PySpark**, **diffprivlib**, **PyTorch**, and **Opacus**)
    used to develop machine learning, deep learning, and large-scale applications
    with the power of differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open source frameworks for implementing differential privacy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to the PyDP framework and its key features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples and demonstrations of PyDP in action
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a sample banking application with PyDP to showcase differential privacy
    techniques
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Protecting against membership inference attacks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding membership inference attacks and their potential risks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques and strategies to safeguard against membership inference attacks
    when applying differential privacy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applying differential privacy on large datasets to protect sensitive data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging the open source PipelineDP framework to apply differential privacy
    on large-scale datasets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging the open source Tumult Analytics and PySpark frameworks to apply
    differential privacy on large datasets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning with differential privacy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a fraud detection classification model on synthetic data using differential
    privacy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A clustering example, applying differential privacy using IBM’s open source
    **diffprivlib** framework
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep learning with differential privacy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a fraud detection model using the PyTorch deep learning framework
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing the open source PyTorch and Opacus frameworks to develop deep learning
    models for fraud detection with differential privacy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Differential privacy machine learning frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations of differential privacy and some strategies to overcome them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source frameworks to implement differential privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several open source frameworks available to implement differential
    privacy. We will go through the PyDP framework in detail in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the PyDP framework and its key features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google has released an open source framework called differential privacy that
    facilitates the implementation of differential privacy. This framework offers
    support for both ε- and (ε, δ)-differentially private statistics. It includes
    various features such as the ability to introduce noise using Laplace and Gaussian
    mechanisms. Additionally, the framework provides support for aggregated differential
    privacy algorithms including sum, count, mean, variance, and standard deviation.
    The libraries within this framework are implemented in the C++, Java, and Go languages,
    and it also offers a **command-line interface** (**CLI**) to execute differential
    privacy SQL queries. For further information, you can visit the GitHub repository
    at [https://github.com/google/differential-privacy](https://github.com/google/differential-privacy).
  prefs: []
  type: TYPE_NORMAL
- en: PyDP, developed by OpenMined in 2020, is another framework that implements Python
    wrapper functions for Google’s differential privacy tools. While PyDP is not an
    exhaustive implementation of Google’s differential privacy toolkit, it supports
    a subset of ε-differentially private algorithms from Google’s toolkit. These algorithms
    enable the generation of aggregate statistics over numeric datasets that contain
    private or sensitive information. You can find the PyDP framework at [https://github.com/OpenMined/PyDP](https://github.com/OpenMined/PyDP).
  prefs: []
  type: TYPE_NORMAL
- en: Examples and demonstrations of PyDP in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Installation of PyDP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyDP installation is done the same way as installing any other Python package.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Python 3.x, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For earlier versions of Python, use this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Sample program to calculate the mean using PyDP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PyDP supports the Laplacian noise mechanism and offers a range of aggregate
    functions such as sum, average, count, and more. When calculating the mean, PyDP
    requires the provision of bounds in the form of lower and upper values. To facilitate
    this, PyDP provides a class called `BoundedMean`, which offers the following constructors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `BoundedMean` class in PyDP is used to calculate the differentially private
    mean of bounded data. It utilizes the epsilon and delta parameters to provide
    privacy guarantees. This class supports the Laplacian noise mechanism to add privacy-preserving
    noise to the mean calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is example code to demonstrate the bounded mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: Sample_Mean_Using_PyDP.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Developing a sample banking application with PyDP to showcase differential privacy
    techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this application scenario, we take the example of a financial bank that
    aims to collaborate with merchants, companies, and other banks to launch a marketing
    campaign while safeguarding the sensitive personal information of its customers.
    Customers make purchases using their credit or debit cards in two different scenarios:
    offline transactions at merchant outlets, where the cards are physically swiped
    (known as card-present transactions), and online transactions, where card details
    such as the card number, name, expiry date, and CVV are entered (known as card-not-present
    transactions). The objective is to provide credit card transaction data to support
    the campaign without revealing individual customer details.'
  prefs: []
  type: TYPE_NORMAL
- en: To successfully launch marketing campaigns or loyalty programs, the bank needs
    to share certain transaction details such as the type of purchases, transaction
    volumes, and transaction values associated with specific locations. This information
    allows the bank to partner with interested banks/merchants, enabling the promotion
    of large-scale product sales and the delivery of benefits to customers.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, we will synthetically generate a significant number of transactions
    using predefined business rules and calculate statistics such as count, average,
    and sum for each location. The intention is to share these aggregated statistics
    with the partnering companies without compromising customer privacy.
  prefs: []
  type: TYPE_NORMAL
- en: There are two approaches to sharing these statistics. The first approach involves
    sharing the aggregates as they are, which may potentially reveal private information
    about individual customers. The second approach utilizes differential privacy
    to generate the aggregates, ensuring that they do not leak any sensitive customer
    information while preserving privacy.
  prefs: []
  type: TYPE_NORMAL
- en: By applying differential privacy techniques, the bank can protect customer privacy
    by introducing carefully calibrated noise to the aggregated statistics. This noise
    ensures that the shared aggregates do not disclose specific individuals’ details
    while still providing valuable insights for the marketing campaign and loyalty
    program planning.
  prefs: []
  type: TYPE_NORMAL
- en: By adopting differential privacy, the bank can strike a balance between data
    utility and privacy protection, allowing it to collaborate with companies and
    merchants while maintaining the confidentiality of customer information.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s generate the synthetic datasets needed to develop this banking application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the customer data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Customer Dataset](img/B16573_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Customer Dataset
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used in this context comprises various attributes, including the
    customer ID, the location represented as latitude and longitude coordinates, the
    number of transactions per day, and the average transaction amount. It is important
    to note that the actual credit card number, **Card Verification Value** (**CVV**)
    code, and the expiry date associated with each customer’s card are maintained
    in a separate table and are not shown in the provided dataset. Furthermore, these
    parameters are not utilized in generating the statistical information. In this
    particular example, the transactions generated pertain to card-present scenarios
    occurring at various merchant locations, using **point-of-sale** (**POS**) terminals
    to swipe the cards.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the sample POS terminal data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Terminal Dataset](img/B16573_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Terminal Dataset
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, it is assumed that customers predominantly visit nearby merchant
    outlets for their day-to-day purchases, with the distance between the customer’s
    location and the merchant locations typically falling within a range of around
    5 miles.
  prefs: []
  type: TYPE_NORMAL
- en: To generate the transactions, the Euclidean distance between the customer’s
    location and the available merchant locations is calculated. Using this distance
    information, merchants are randomly selected from the nearby options. This approach
    ensures that the generated transactions reflect the realistic behavior of customers
    going to nearby merchants within a specific distance radius for their purchases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the transaction data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Transaction Dataset](img/B16573_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Transaction Dataset
  prefs: []
  type: TYPE_NORMAL
- en: Let’s generate aggregates using differential privacy on this dataset so that
    it can then be shared with merchants/banks in order to design marketing campaigns
    and loyalty programs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: Sample_Finance_App_DP.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '**# unzip the transactions.csv.zip file (provided in this book’s GitHub repo**
    **as transactions.csv).**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter%205/transactions.csv.zip](https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter%205/transactions.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data loading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.4 – Sample transaction data](img/B16573_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Sample transaction data
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will generate differentially private aggregates for various
    scenarios. The focus will be on comparing the results obtained using traditional
    statistical methods with those achieved through differential privacy techniques.
    The following scenarios will be explored:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean transaction amount for a given terminal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean transaction amount for terminal IDs 1 to 100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of customers who make purchases worth $25 or more via terminals 1 to
    100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum transaction amount for a given terminal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sum of the transaction amounts for each terminal on a given day or month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean transaction amount for a given terminal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to calculate the average transaction amount for a given day or month
    via a specific POS terminal, we can define the following methods. These methods
    will be used to compare the results obtained using traditional statistical methods
    with the aggregates generated through differential privacy techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Traditional average calculation**: We will implement a method that calculates
    the average transaction amount for a given day or month on a particular POS terminal
    using traditional statistical methods. This method will take as input the relevant
    transaction data, such as the transaction amounts and the date of the day or month
    of interest. The average will be computed by summing all the transaction amounts
    and dividing the sum by the total number of transactions on the specified day
    or month on the chosen POS terminal. The traditional average will serve as a baseline
    for comparison.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differentially private average calculation**: We will develop a method that
    leverages differential privacy techniques to calculate the differentially private
    average transaction amount for a given day or month on the selected POS terminal.
    This method will take the same input as the traditional average calculation method.
    It will utilize differential privacy mechanisms, such as adding noise to the aggregated
    statistics, to protect the privacy of individual transactions while generating
    the average. The differentially private average will provide a privacy-preserving
    alternative to the traditional average. By utilizing these methods and comparing
    the results, we can assess the differences between the traditional average calculation
    and the average generated through differential privacy techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This analysis will demonstrate the impact of differential privacy on aggregate
    calculations and highlight the trade-off between accuracy and privacy preservation
    in generating average transaction amounts on a given day or month via a specific
    POS terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding calculation, we can see the average generated using the traditional
    method returns 56.22 whereas the differentially private version produces an average
    of 220.98 for POS terminal ID 1\. In this way, the private average helps not to
    disclose the actual average.
  prefs: []
  type: TYPE_NORMAL
- en: Mean transaction amount for terminal IDs 1 to 100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s generate the mean transaction amount for the terminals 1 to 100 and make
    use of the private mean function defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.5 – Actual Mean vs Privacy Mean](img/B16573_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Actual Mean vs Privacy Mean
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will also generate the count and sum of transaction
    amounts for a given day or month on a specific POS terminal and compare the results
    obtained using traditional statistical methods with those generated through differential
    privacy techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Number of customers who make purchases worth $25 or more via terminals 1 to
    100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we will implement a method that calculates the number of customers who
    made purchases of $25 or more on terminals 1 to 100.
  prefs: []
  type: TYPE_NORMAL
- en: This method will take the transaction data as input, including the terminal
    number and the corresponding transaction amounts. It will iterate through the
    transactions for each terminal ID from 1 to 100 and count the number of customers
    whose transaction amounts exceed $25\. The count will provide an indication of
    the customer base that made higher-value purchases, helping to analyze the impact
    of differential privacy on identifying such customers. By utilizing this method,
    we can compare the results obtained using traditional statistical methods with
    the counts generated through differential privacy techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'This analysis will shed light on the differences between the approaches and
    demonstrate the effectiveness of differential privacy in identifying customers
    who made purchases worth more than $25 for terminals 1 to 100 while preserving
    individual privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.6 – Actual transaction counts vs privacy added counts](img/B16573_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Actual transaction counts vs privacy added counts
  prefs: []
  type: TYPE_NORMAL
- en: By adopting this approach, we ensure that the actual count of customers who
    made purchases worth more than $25 is not revealed or shared with banks/merchants.
    Instead, we provide those parties with differentially private counts, thus preserving
    individual privacy while still offering valuable insights. This allows banks/merchants
    to launch loyalty programs based on the differentially private counts, tailoring
    their initiatives based on the available data. Thus, by employing differential
    privacy techniques, institutions can strike a balance between providing useful
    information for loyalty programs and safeguarding the sensitive details of individual
    customers.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum transaction amount for a given terminal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s define the functions to calculate the maximum transaction amount and
    the differentially private amount:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the preceding code, by employing differential privacy techniques,
    we can calculate an approximate maximum transaction amount for a given terminal
    while preserving the privacy of individual transactions. These values depend on
    the privacy budget. This example used 0.5 as the privacy budget. This allows us
    to share valuable aggregated information with banks/merchants without compromising
    the sensitive details of individual customers. In this example, the actual maximum
    transaction amount is 87\. With added noise based on the privacy budget (i.e.,
    0.5) the value becomes 167\. Thus, it may not be very useful in terms of utility.
    This illustrates the trade-off between privacy and utility. One needs to experiment
    with different privacy budgets to decide the best fit for the use case/application,
    deciding whether they want prioritize more privacy and less utility or more utility
    with less privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Sum of the transaction amounts for each terminal on a given day or month
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s define the functions to calculate the sum of the transaction amounts
    and the differentially private amounts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As an exercise, you can implement the `count` and `sum` functions for all POS
    terminals and compare the results obtained using traditional statistical methods
    with those generated through differential privacy techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting against membership inference attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Membership inference attacks** pose a significant threat to the privacy of
    individuals in machine learning systems. These attacks aim to determine whether
    a specific data point was part of the training dataset used to create a machine
    learning model, potentially exposing sensitive information about individuals.
    To mitigate the risk of such attacks, differential privacy techniques can be employed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To protect against membership inference attacks using differential privacy,
    several approaches can be adopted:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noise addition**: During the training process, noise is added to the computations
    to introduce randomness and mask individual data points. This makes it challenging
    for attackers to identify whether a specific data point was used in the training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy budget management**: Differential privacy operates under a privacy
    budget that determines the maximum amount of privacy loss allowed. By carefully
    managing and allocating the privacy budget, the risk of membership inference attacks
    can be minimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalization and aggregation**: Applying generalization and aggregation
    techniques helps in obfuscating individual data points. By grouping similar data
    points together, the information about any specific individual becomes less distinguishable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perturbation mechanisms**: Utilizing perturbation mechanisms, such as adding
    noise to the model’s outputs or gradients, enhances privacy protection. These
    mechanisms make it more challenging for attackers to infer membership status accurately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial training**: Incorporating adversarial training techniques helps
    in training models that are robust against membership inference attacks. This
    involves training the model against a sophisticated attacker who tries to distinguish
    the presence of specific data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining these strategies and adopting a privacy-by-design approach, machine
    learning systems can better protect against membership inference attacks. It is
    important to note that while differential privacy provides strong privacy guarantees,
    there might still be cases where additional privacy-preserving techniques or post-processing
    is necessary to address specific attack scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is an example to demonstrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s generate two datasets that differ by exactly one record. We’ll create
    a copy of the original dataset and refer to it as the **redacted dataset**. In
    the redacted dataset, we’ll remove one record to create the difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how to proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with the original dataset containing the desired records. This dataset
    represents the baseline or complete set of records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a copy of the original dataset and label it as **redact_dataset**. This
    dataset will closely resemble the original dataset but with one record removed.
    Choose any record from the redacted dataset and remove it to create the difference.
    In the example, the first record is removed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By creating the redacted dataset as a modified version of the original dataset,
    specifically by removing one record, we establish a distinct dataset that differs
    from the original by only that single record.
  prefs: []
  type: TYPE_NORMAL
- en: Use the following source code to create a redacted one.
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: Sample_Finance_App_DP.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.7 -  Sample transaction dataset](img/B16573_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 - Sample transaction dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.8 – Redact dataset](img/B16573_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Redact dataset
  prefs: []
  type: TYPE_NORMAL
- en: We have removed just one record (customer ID 2079) from the original dataset,
    who made a transaction of $36\. This was done to form the redacted dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the sum of transaction amounts from the original dataset and
    the redacted dataset to determine the difference. This difference will correspond
    to the exact transaction amount made by the customer with the ID 2079:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s apply differential privacy techniques to calculate the sum of transaction
    amounts from the original dataset and the redacted dataset. The difference between
    these two sums should not reveal the exact transaction amount made by the customer
    with ID 2079\. Here’s how you can approach this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sum using differential privacy on the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Sum using differential privacy on the redacted dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Difference between the two datasets using differential privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this example, when calculating the sum of transaction amounts using differential
    privacy for both the original dataset and the redacted dataset, the difference
    between these two sums resulted in negative numbers. However, it is important
    to note that these negative values do not represent the actual transaction amount
    made by customer ID 2079.
  prefs: []
  type: TYPE_NORMAL
- en: The negative values in the difference arise due to the inherent noise added
    during the differential privacy calculations. Differential privacy techniques
    introduce randomization to protect individual privacy, and this random noise can
    sometimes lead to negative perturbations in the aggregated results.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is crucial to interpret these negative values correctly. They
    should not be considered as the actual transaction amount made by customer ID
    2079, but rather as an indication that the differential privacy mechanisms have
    successfully introduced noise to protect individual privacy while providing approximate
    aggregate results.
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to understand that differential privacy focuses on preserving
    privacy rather than exactness in the calculated results. The negative difference
    serves as a reminder of the privacy guarantees provided by differential privacy,
    ensuring that individual transaction details are safeguarded even in the presence
    of aggregate computations.
  prefs: []
  type: TYPE_NORMAL
- en: Applying differential privacy to large datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous examples, we focused on calculating differentially private aggregates
    (such as count, sum, and average) on smaller datasets, involving a single terminal
    or a limited number of terminals. However, in this section, we will explore how
    to generate differentially private aggregates on large datasets, including millions
    or even billions of records. Specifically, we will consider a use case involving
    a dataset of approximately 5 million credit card transactions across 1,000 point-of-sale
    terminals and 5,000 customers.
  prefs: []
  type: TYPE_NORMAL
- en: Use case – generating differentially private aggregates on a large dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s generate the dataset comprising credit card transactions recorded on a
    daily basis across 1,000 POS terminals. These transactions involve a total of
    5,000 customers, resulting in an extensive collection of approximately 5 million
    records.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate differentially private aggregates on such a large dataset, specialized
    techniques and frameworks are employed to handle the scale and complexity of the
    data. These techniques ensure that privacy is preserved while providing meaningful
    aggregate statistics.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging differential privacy on large datasets, organizations can extract
    valuable insights without compromising the privacy of individual customers. The
    generated differentially private aggregates enable data-driven decision-making
    and analysis while protecting sensitive information. It is worth noting that the
    methods and frameworks used to apply differential privacy on large datasets may
    vary depending on the specific requirements and available resources. Techniques
    such as data partitioning, parallel processing, and optimized algorithms play
    a crucial role in efficiently computing differentially private aggregates on such
    vast datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'By understanding how to generate differentially private aggregates on large
    datasets, organizations can derive actionable insights from their data while upholding
    the privacy of individuals involved in the transactions. The dataset is organized
    in a specific format, with each day’s transactions stored in separate files that
    include the date in the filename. To illustrate, let’s consider an example where
    the dataset corresponds to transactions on February 1, 2022\. The file for this
    day uses the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Filename: `2022-02-01.csv`'
  prefs: []
  type: TYPE_NORMAL
- en: The filename consists of the specific date in the format `YYYY-MM-DD`. In this
    case, `2022-02-01` represents the date of the transactions contained in the file.
  prefs: []
  type: TYPE_NORMAL
- en: The actual content of the file will be the transaction data for that specific
    day, including details such as customer ID, transaction amount, POS terminal ID,
    and any other relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: This file format, where each day’s transactions are stored in separate files
    with the date included in the filename, helps in organizing and managing the dataset
    chronologically.
  prefs: []
  type: TYPE_NORMAL
- en: 'It enables easy retrieval and analysis of transaction data from specific dates,
    facilitating time-based analysis and reporting tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **TRANSACTION_ID** | **TX_DATETIME** | **CUSTOMER_ID** | **TERMINAL_ID**
    | **TX_AMOUNT** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2023-02-01 00:43:37 | 901 | 8047 | 82 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2023-02-01 01:20:13 | 2611 | 7777 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2023-02-01 01:22:52 | 4212 | 3336 | 53 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2023-02-01 01:26:40 | 1293 | 7432 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2023-02-01 01:52:23 | 2499 | 1024 | 25 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 2023-02-01 02:11:03 | 2718 | 168 | 68 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 2023-02-01 02:11:56 | 2998 | 5513 | 80 |'
  prefs: []
  type: TYPE_TB
- en: Table 5.1 – First few rows of transactions data
  prefs: []
  type: TYPE_NORMAL
- en: '| **TRANSACTION_ID** | **TX_DATETIME** | **CUSTOMER_ID** | **TERMINAL_ID**
    | **TX_AMOUNT** |'
  prefs: []
  type: TYPE_TB
- en: '| 24901 | 2023-02-02 01:34:52 | 4999 | 4536 | 43 |'
  prefs: []
  type: TYPE_TB
- en: '| 24902 | 2023-02-02 01:44:39 | 580 | 3511 | 29 |'
  prefs: []
  type: TYPE_TB
- en: '| 24903 | 2023-02-02 01:48:04 | 3309 | 7661 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| 24904 | 2023-02-02 01:58:12 | 2919 | 5322 | 94 |'
  prefs: []
  type: TYPE_TB
- en: '| 24905 | 2023-02-02 02:07:07 | 3868 | 3217 | 97 |'
  prefs: []
  type: TYPE_TB
- en: '| 24906 | 2023-02-02 02:08:43 | 1822 | 489 | 15 |'
  prefs: []
  type: TYPE_TB
- en: Table 5.2 – Last few rows of transactions data
  prefs: []
  type: TYPE_NORMAL
- en: For our use case scenario, where the core exercise is to createdifferentially
    private aggregates , let’s assume that the system receives data covering several
    months, and the objective is to generate differentially private aggregates of
    transactions for each POS terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '| **POS Terminal** | **Differentially** **private aggregates** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | Count | Sum | Average |'
  prefs: []
  type: TYPE_TB
- en: '| 1 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 3 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 4 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| .. |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: Table 5.3 – Differentially private aggregates
  prefs: []
  type: TYPE_NORMAL
- en: While the use of frameworks such as Pytorch, pandas, and PyDP can be effective
    for generating differentially private aggregates, it is true that processing large
    datasets using these methods alone may be time-consuming and not scalable. However,
    there are alternative approaches and tools available to address these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'These approaches include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallel processing**: We could utilize parallel processing techniques to
    distribute the computation across multiple processors or machines. This can significantly
    reduce the processing time and enable scalability when dealing with large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed computing**: We could employ distributed computing frameworks
    such as Apache Spark or Hadoop to handle big data processing. These frameworks
    provide distributed data processing capabilities, allowing for efficient processing
    of large-scale datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud computing**: We could leverage cloud computing platforms such as **Amazon
    Web Services** (**AWS**) or **Google Cloud Platform** (**GCP**) to harness the
    power of scalable infrastructure. These platforms offer services such as Amazon
    EMR, Google Dataproc, or Azure HDInsight, which can handle large-scale data processing
    in a cost-effective and scalable manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimized differential privacy libraries**: We could explore specialized
    differential privacy libraries, such as PipelineDP, TensorFlow Privacy, or Opacus,
    that are designed to provide efficient and scalable implementations of differential
    privacy algorithms. These libraries offer optimizations specific to privacy-preserving
    computations, enabling faster, more scalable processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data partitioning and pre-aggregation**: We could divide the data into manageable
    partitions and perform pre-aggregation to reduce the overall computational load.
    This approach can improve performance by minimizing the amount of data processed
    at each step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By incorporating these approaches and tools, it is possible to overcome the
    challenges of processing large datasets when generating differentially private
    aggregates. These methods can significantly reduce the processing time and enhance
    scalability, enabling organizations to efficiently analyze and derive insights
    from their data while preserving privacy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Here are some questions/designs** **to consider:**'
  prefs: []
  type: TYPE_NORMAL
- en: How can you solve or generate DP aggregates on large datasets?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you partition the data? (i.e., partitioning based on a date or on a
    POS terminal)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you apply DP within a given partition?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you find out the maximum and minimum value bounds to cacluate the (clip)
    function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we apply DP (bounds/sensitivity) for each data value in a partition to
    generate the DP aggregates? OR
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we generate statistical aggregates first and then apply DP within the
    partition?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PipelineDP high-level architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PipelineDP framework is designed to address the questions and considerations
    mentioned earlier when generating differentially private aggregates for large
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: PipelineDP ([https://pipelinedp.io](https://pipelinedp.io)) is an open source
    framework that supports generating differentially private aggregates on large
    datasets using open source frameworks such as Apache Spark and Apache Beam. The
    PipelineDP framework was developed by Google in collaboration with OpenMined.
    As of writing this book, the PipelineDP team has a disclaimer that this framework
    isn’t recommended for production deployments but is considered good enough for
    development mode and demonstration purposes to enhance your understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – PipelineDP architecture](img/B16573_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – PipelineDP architecture
  prefs: []
  type: TYPE_NORMAL
- en: PipelineDP provides APIs in three modes (Apache Spark, Apache Beam, and local
    mode) and access to their corresponding implementations through the DP engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key concepts used in PipelineDP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Example to show privacy unit, privacy-id, and partition keys
    for sample data](img/B16573_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Example to show privacy unit, privacy-id, and partition keys for
    sample data
  prefs: []
  type: TYPE_NORMAL
- en: '**Record** is an element in the input dataset in PipelineDP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition** is a subset of the data corresponding to a given value of the
    aggregation criterion. In our case, we want the results per POS terminal, so the
    partition will be **TERMINAL_D**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition key** is the aggregation key corresponding to a partition. TERMINAL_D
    is the partition key in this example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy unit** is the entity that we want to protect with differential privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy ID** is an identifier of a privacy unit. In our example, CUSTOMER_IDis
    the privacy ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the high-level architecture and key concepts, let’s implement
    differential privacy on a large dataset using PipelineDP.
  prefs: []
  type: TYPE_NORMAL
- en: The following line of code installs the PipelineDP framework.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*Source code :* *Large_Data_Sets-DP_PipelineDP.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.11 – Transactions data](img/B16573_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Transactions data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will produce the differentially private counts (total number
    of customers) for each terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now generate the actual counts and compare them with differential privacy ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.12 – Transactions aggregates](img/B16573_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Transactions aggregates
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing the PipelineDP framework, we can address the challenges and considerations
    involved in generating differentially private aggregates for large datasets. It
    provides a comprehensive solution that combines scalability, privacy preservation,
    and accurate aggregation, allowing us to effectively leverage differential privacy
    techniques for large-scale data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Tumult Analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tumult Analytics is a robust and feature-rich Python library designed for performing
    aggregate queries on tabular data while ensuring the principles of differential
    privacy. The library offers an intuitive interface, making it accessible to users
    familiar with SQL or PySpark. It provides a wide range of aggregation functions,
    data transformation operators, and privacy definitions, ensuring versatility in
    analytical tasks. Developed and maintained by a team of experts in differential
    privacy, Tumult Analytics guarantees reliability and is even utilized in production
    environments by reputable institutions such as the U.S. Census Bureau. Powered
    by Spark, the library demonstrates excellent scalability, enabling efficient processing
    of large datasets. With its comprehensive functionality and emphasis on privacy
    preservation, Tumult Analytics is a valuable tool for data analysis with a focus
    on maintaining data privacy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the citation for the Tumult Analytics open source framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Installation of Tumult Analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To utilize Tumult Analytics, it is essential to have Python installed, as the
    library is built using Python. It is compatible with Python versions 3.7 to 3.10\.
    Additionally, since Tumult Analytics leverages PySpark for its computations, it
    is necessary to have Java 8 or 11 installed as well.
  prefs: []
  type: TYPE_NORMAL
- en: The following code installs the Tumult Analytics framework.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Key features of Tumult Analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tumult Analytics provides classes and methods to build aggregates on large
    datasets using differential privacy. Some of the high-level classes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Methods** |'
  prefs: []
  type: TYPE_TB
- en: '| Session(tmlt.analytics.session) | The Session module offers a convenient
    interface for managing data sources and conducting differentially private queries
    on them. Creating a session is straightforward, with Session.from_dataframe()
    for a simple session involving a single private data source or Session.Builder
    for more complex scenarios involving multiple data sources. Once the session is
    set up, queries can be executed on the data using Session.evaluate(). When initializing
    instance type of Session , PrivcyBudget is specified to ensure that the queries
    performed on the private data do not exceed this allocated budget. By default,
    the Session instance enforces privacy protection at the row level. This means
    that the queries prevent any potential attacker from deducing whether individual
    rows have been added or removed from the private tables. However, this privacy
    guarantee applies only to the queries themselves and assumes that the private
    data is not utilized elsewhere in the computation process. |'
  prefs: []
  type: TYPE_TB
- en: '| PureDPBudget(tmlt.analytics.privacy_budget ) | A privacy budget that provides
    pure differential privacy. |'
  prefs: []
  type: TYPE_TB
- en: '| ApproxDPBudgettmlt.analytics.privacy_budget ) | A privacy budget that provides
    approximate differential privacy. |'
  prefs: []
  type: TYPE_TB
- en: '| QueryBuilder(tmlt.analytics.query_builder) | A high-level interface for specifying
    DP queries. The QueryBuilder class can apply transformations, such as joins or
    filters, as well as compute aggregations including counts, sums, and standard
    deviation. |'
  prefs: []
  type: TYPE_TB
- en: '| KeySet(tmlt.analytics.keyset) | A KeySet specifies a list of values for one
    or more columns. Currently, KeySets are used as a simpler way to specify domains
    for `groupb``y` transformations. |'
  prefs: []
  type: TYPE_TB
- en: Table 5.4 – tmlt.analytics.session description
  prefs: []
  type: TYPE_NORMAL
- en: Example application with Tumult Analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s generate the aggregates using tmlt-analytics on the large dataset used
    in the previous section (i.e., our transaction data).
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: DP_Large_Data_Sets_TMLT.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required Python packages as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, initialize the Spark session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s load the dataset that contains information about credit card transactions.
    We get the data from the local directory and load it into a Spark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '**#Create a downloads directory and copy the transactions.csv file by unzipping
    the** **transactions.csv.zip file:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter%205/transactions.csv.zip](https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter%205/transactions.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Initiating a tmlt-analytics session
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'When a session is initialized with a finite privacy budget, it offers a straightforward
    interface promise: all queries executed on this session, collectively, will yield
    differentially private outcomes with a maximum epsilon value of 3.5\. Epsilon
    serves as a metric for quantifying potential privacy loss, where a lower epsilon
    implies a more stringent constraint on privacy loss and, consequently, a higher
    level of protection. In this context, the interface promise corresponds to a privacy
    guarantee, ensuring a minimum level of safeguarding for private data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the data itself, several additional pieces of information are
    required:'
  prefs: []
  type: TYPE_NORMAL
- en: The **privacy_budget** parameter specifies the privacy guarantee that the session
    will provide.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **source_id** parameter serves as an identifier for the DataFrame. It will
    be used to reference this specific DataFrame when constructing queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **protected_change** parameter defines the unit of data for which the differential
    privacy guarantee is applied. In this example, **AddOneRow()** is used to protect
    individual rows within the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exeucting DP queries using Session
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our first query finds the number of total transactions in the data with DP
    enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute the query on the desired private data, we utilize `QueryBuilder("transactions")`
    in the first step, which indicates the specific source data (data source) we want
    to query, corresponding to the `source_id` parameter specified earlier. In the
    following line, the `count()` statement is used to retrieve the total number of
    records in the dataset. Once the query is constructed, we proceed to run it on
    the data by employing the `evaluate` method of our session. To accomplish this,
    we allocate a privacy budget to the evaluation process. In this case, we evaluate
    the query with differential privacy, setting ε=1 as the privacy parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of the query are returned as a Spark DataFrame. We can see them
    using the `show()` method of this DataFrame. We have utilized 1 out of 3.5 of
    our allocated privacy budget for this query, so the remaining privacy budget will
    be 2.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: If you are following along with the example and executing the code, you may
    observe varying values. This variation is a fundamental aspect of differential
    privacy, as it introduces randomization (referred to as noise) during query execution.
    To showcase this characteristic, let’s proceed to evaluate the same query one
    more time and see the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The amount of noise added to the query computation can vary based on the privacy
    parameters, the type of aggregation, and the underlying data. However, in many
    instances, the query result still provides reliable insights into the original
    data. In this particular scenario, we can confirm this by executing a count query
    directly on the original DataFrame, which will yield the true and accurate result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Playing with privacy budgets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Describe the session object to know the attributes and the remaining privacy
    budget.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Utilizing privacy budgets with privacy queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s find out the number of customers whose purchases are worth less than $25.
  prefs: []
  type: TYPE_NORMAL
- en: 'This query consumes epsilon=1 out of our total budget:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We have utilized 1 unit of our remaining total privacy budget of 2.5, so there
    is 1.5 left. Let’s try another query to consume another 1 unit of the privacy
    budget:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s find out the number of customers whose purchases are greater than $25
    but less than $50.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We have utilized a budget of 3 out of a total of 3.5, so there is 0.5 left.
    Let’s try another query to consume the privacy budget of 1 (run the query more
    than the available budget and observe the results).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'It will throw a runtime error as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Let’s display the remaining privacy budget for use in the subsequent queries.
  prefs: []
  type: TYPE_NORMAL
- en: '`print(session.remaining_privacy_budget)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Rewrite the last query so that it will make use of the remaining available
    budget so that the privacy budget is used completely, instead of wasting the unused
    budget. In this case, we will not specify the privacy budget as 1 or 2, but will
    make use of the remaining privacy budget from the `Session` class itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Total number of high purchase counts by applying differential privacy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Groupby queries**'
  prefs: []
  type: TYPE_NORMAL
- en: In Tumult Analytics, the KeySet class is utilized to define the list of groupby
    keys. It allows us to specify both the columns by which we intend to group the
    data and the potential values associated with those columns. The KeySet class
    serves as a convenient means of specifying the grouping criteria in Tumult Analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now write a query to find the average transaction amount on each terminal
    (taking just the first 10 terminals for this example).
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a session with the privacy budget set to 2.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Make use of the `KeySet` class and define the groupby column (TERMINAL_ID)
    or columns and values as well, to filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the query and provide the lower and upper bounds to clip `TX_AMOUNT`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we can generate the aggregates using differential privacy, ensuring
    they are not the same as the actual aggregates.
  prefs: []
  type: TYPE_NORMAL
- en: Queries using privacy IDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previously, we focused on working with tables where each individual in the dataset
    was linked to a single row. However, this is not always the case. In certain datasets,
    it is possible for the same individual to appear in multiple rows. In such cases,
    it is typical to assign a unique identifier to each person (i.e., across different
    rows). The objective then shifts toward concealing whether all the rows associated
    with a particular identifier are present in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Tumult Analytics refers to these identifiers as privacy IDs. Each privacy ID
    corresponds to a one-to-one mapping with a person or any other entity that requires
    protection. The aim is to safeguard the privacy of individuals or entities by
    preserving the anonymity of their presence within the dataset. This can be achieved
    by making use of the `AddRowsWithID` protected change. This protected change will
    prevent arbitrarily adding and removing many rows all sharing the same ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initializing a session with the privacy ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the query with the privacy ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s execute this code and observe the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This error arises due to the absence of a constraint on the number of rows a
    single individual can contribute to the dataset. It is possible for a single customer
    to do many transactions, even exceeding 1,000 or more. However, differential privacy
    necessitates concealing the influence of an individual’s data through the introduction
    of statistical noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to address this issue, it is necessary to establish a restriction
    on the maximum influence that a single customer can exert on the computed statistic
    before conducting aggregations. This constraint is enforced on the data to mitigate
    any potential impact. The most straightforward constraint, known as `MaxRowsPerID`,
    restricts the total number of rows contributed by each privacy ID. To enforce
    this constraint, we can simply pass it as a parameter to the `enforce()` operation.
    For the specific query at hand, we will set the maximum number of contributed
    rows per library member to 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output only showing the top 20 rows.
  prefs: []
  type: TYPE_NORMAL
- en: Tumult Analytics provides filters, joins, and transformations of the data to
    execute complex queries, apply differential privacy to large datasets, and make
    use of Spark distributed processing. We have covered the basic key features, but
    there are many more features to explore based on the use cases in the application/system.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning using differential privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, our objective is to develop a machine learning classification
    model that can accurately distinguish between fraudulent and genuine credit card
    transactions. To ensure privacy protection, we will also apply differential privacy
    techniques to the model. The classification model will be trained on a labeled
    dataset consisting of historical credit card transactions, where each transaction
    is labeled as either fraudulent or genuine. Popular machine learning algorithms
    such as logistic regression, decision trees, or neural networks can be applied
    to build the classification model and will make use of neural networks in our
    case.
  prefs: []
  type: TYPE_NORMAL
- en: To incorporate differential privacy, we will leverage techniques such as the
    addition of noise to the training process and the use of privacy-preserving algorithms.
    These techniques ensure that the model’s training process and subsequent predictions
    do not compromise the privacy of individual transactions or sensitive customer
    information.
  prefs: []
  type: TYPE_NORMAL
- en: By integrating differential privacy into the classification model, we can provide
    robust privacy guarantees while maintaining high accuracy in identifying fraudulent
    transactions. This ensures that the model can effectively protect customers’ privacy
    and prevent unauthorized access to sensitive financial data.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this section, we will explore the steps involved in training the
    classification model, evaluating its performance, and applying differential privacy
    techniques to enhance privacy protection. By the end, we will have a powerful
    model capable of accurately classifying credit card transactions as either fraudulent
    or genuine while ensuring the privacy of the individuals involved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Synthetic Dataset Generation: Introducing Fraudulent Transactions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will make use of the same transaction data used earlier, add another column
    to the dataset called `TX_FRAUD`, and mark any transaction greater than $75 as
    fraudulent. This obviously doesn’t reflect the real world, but we will generate
    our example synthetic data using this rule. In this dataset, roughly 25% of the
    data is marked as fraudulent transactions while 75% of the data is genuine. In
    a real-world scenario, fraudulent transactions will likely be less than 1% in
    most datasets, which is highly imbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: Fraud_Transactions_Generator.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.13 – Transactions Data](img/B16573_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Transactions Data
  prefs: []
  type: TYPE_NORMAL
- en: 4557166 rows × 6 columns
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.14 – First few rows of transactions data](img/B16573_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – First few rows of transactions data
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Develop a classification model using scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the high-level steps for developing a classification model:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the **fraud_transactions** dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into train and test in the ratio of (70:30).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the classifier (logistic regression from sci-kit learn).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the classifier with the training data (70% of the transactions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find out the accuracy of the classifier with the test dataset (30% of the transactions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find out the calculated weights/coefficient used in the **decision** function
    and intercept from the logistic regression model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Source* *code: Noise_Gradient_Final.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.15 – Fraud transactions dataset](img/B16573_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Fraud transactions dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we know the gradients/coefficients of the classifier and intercept as
    well, then it will be easy to calculate the predictions. In our case, we have
    three features, `CUSTOMER_ID`, `TERMINAL_ID`, and `TRANSACTON_AMOUNT`. The linear
    equation will come with three features:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y = w1 * x1 + w2 * x2 + w3 * x3 + b*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we know the feature values (x1, x2, x3…. xn), weights (w1, w2, w3, …,
    wn), and b value (bias /intercept), then we can calculate the y-hat value (predictions).
    Logistic regression uses a logistic function to estimate/predict the probabilities.
    In our case, it will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: y-hat = 1.0 / 1.0 + e – (w1 * x1 + w2 * x2 + w3 * x3 + b)
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take one actual transaction and calculate the prediction value using
    the weights obtained from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '| CUSTOMER_ID | TERMINAL_ID | TX_AMOUNT | TX_FRAUD |'
  prefs: []
  type: TYPE_TB
- en: '| 79 | 3115 | 78 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 5.16 – Prediction for CUSTOMER_ID
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Logistic regression uses the stochastic gradient descent algorithm internally
    in order to calculate gradients/coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement our own stochastic gradient descent algorithm to calculate the
    weights instead of using the scikit-learn-provided one in the logistic regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: High-level implementation of the SGD algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps to implement the SGD algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the initial weights with all zeros (one zero for each feature and
    the bias/intercept also as zero): Initial weights = [0,0,0] and intercept=0'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Do the following for each row in the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the predictions based on the initial weights and intercept.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Find out the error between the actual value and the predicted value:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Error = Actual Value –* *Predicted Value*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Update the intercept based on the error and the learning rate value:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: intercept= intercept + l_rate * error * yhat * (1.0 - yhat)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Update the weights for all training data in the training set.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the number of epochs, repeat the preceding steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we can calculate the coefficients. The final coefficients are calculated
    as the average of all coefficients/gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Applying differential privacy options using machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applying differential privacy to the preceding algorithm means generating gradients
    with differential privacy so that the model doesn’t reveal the details of the
    training examples. When applying differential privacy to SGD, the objective is
    to incorporate privacy protection into the training process of the machine learning
    model. This involves adding noise to the gradients computed during the SGD optimization
    steps to ensure that the trained model does not reveal specific details about
    any individual data point.
  prefs: []
  type: TYPE_NORMAL
- en: The addition of noise in SGD with differential privacy helps prevent potential
    privacy breaches by making it difficult to distinguish the impact of any particular
    training example on the model’s updates. It ensures that the model’s parameters
    do not memorize or overfit specific training samples, thereby offering privacy
    guarantees for the individuals whose data was used in the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Generating gradients using differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two approaches for generating gradients using differential privacy
    in the context of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Approach 1:**'
  prefs: []
  type: TYPE_NORMAL
- en: Generate the final gradients using the normal SGD algorithm on the training
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the sum of the gradients obtained from the preceding SGD step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add noise to the sum of the gradients using the Laplace mechanism, taking into
    account the desired sensitivity and privacy budget.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Approach 2:**'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the clipping method to each training example or the overall training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate gradients using the clipped training data inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the sum of the gradients obtained in the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add noise to the sum of the gradients using either the Gaussian or Laplace mechanism,
    considering the desired sensitivity and privacy budget.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the count of the training examples using differential privacy, treating
    it as a count query with sensitivity set to 1 and utilizing the required privacy
    budget.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the average of the noisy gradients sum and the noisy count to obtain
    the differentially private gradients average.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both approaches aim to incorporate differential privacy into the gradient calculation
    process, thereby protecting the privacy of individual training examples while
    training the machine learning model. The choice between the two approaches depends
    on the specific requirements of the application and the desired level of privacy
    guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: By adding appropriate noise and applying privacy-preserving mechanisms, these
    approaches ensure that the gradients used for updating the model parameters do
    not reveal sensitive information about individual training examples. This enables
    the training process to provide privacy guarantees while still achieving accurate
    and reliable model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now implement *Approach 2* for our fraud detection example.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, clipping is the process of setting the lower and upper bounds for
    the data, so implement the following `clip` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: By combining SGD with differential privacy, as we have done here, we can develop
    machine learning models that not only provide accurate predictions but also offer
    privacy protection for sensitive data. It enables organizations to leverage large-scale
    datasets while adhering to privacy regulations and ethical considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering using differential privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider a scenario where we have a dataset of users’ browsing behavior.
    The goal of k-means clustering in this context is to identify *k* points, referred
    to as cluster centers, that minimize the sum of squared distances of the data
    points from their nearest cluster center. This partitioning allows us to group
    the users based on their browsing patterns. Additionally, we can assign new users
    to a group based on the closest cluster center. However, the release of the cluster
    centers could potentially reveal sensitive information about specific users. For
    instance, if a particular user’s browsing behavior is significantly different
    from the majority, the standard k-means clustering algorithm might assign a cluster
    center specifically to this user, thereby disclosing sensitive details about their
    browsing habits. To address this privacy concern, we will implement clustering
    with differential privacy. By doing so, we aim to protect the privacy of individual
    users while still providing meaningful clustering results based on their browsing
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, to illustrate the nature of the problem, let’s generate our cluster
    centroids without differential privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Source* *code: Clustering_Differential_Privacy_diffprivlib.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we have a synthetic dataset with two sensitive data
    attributes. We perform clustering using the KMeans algorithm from scikit-learn
    without incorporating differential privacy. We retrieve the cluster centroids
    using the `cluster_centers_` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this approach is that the cluster centroids, which represent
    the center of each cluster, can reveal sensitive information about the data. In
    this case, the cluster centroids could potentially expose the mean values of the
    sensitive attributes. To address this privacy concern, differential privacy techniques
    can be applied to add noise to the cluster centroids, making it more challenging
    to infer sensitive information. However, note that the application of differential
    privacy in clustering algorithms requires careful consideration of privacy-utility
    trade-offs and appropriate privacy parameter selection.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s generate the centroids by adding noise to the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the **add_noise** function, which takes the original data, privacy parameter
    epsilon, and sensitivity as inputs. It generates Laplace-distributed noise and
    adds it to the data points. The noise is scaled by the sensitivity and privacy
    parameter epsilon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the sensitivity, which is the maximum change in data points due to
    the addition or removal of a single data point. In this case, we calculate the
    maximum absolute difference between any data point and the mean of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the privacy parameter epsilon, which determines the amount of noise
    to be added. Add noise to the data points using the **add_noise** function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform clustering on the noisy data using k-means clustering with two clusters.
    Retrieve the cluster labels assigned by the algorithm. Print the original data
    points, noisy data points, and the clusters assigned to each point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '**Generating differentially private centroids using IBM’s diffprivlib framework
    as an alternative method to the same use case “browsing** **behaviour scenario”.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The diffprivlib framework is a Python library that provides tools and algorithms
    for performing differentially private data analysis. `Diffprivlib` consists of
    four main components that contribute to its functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mechanisms**: These components serve as the fundamental building blocks of
    differential privacy and are utilized in all models implementing differential
    privacy. Mechanisms in diffprivlib are customizable and designed for use by experts
    who are implementing their own models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models**: This module encompasses machine learning models integrated with
    differential privacy. **Diffprivlib** provides a variety of models that implement
    differential privacy, including clustering, classification, regression, dimensionality
    reduction, and preprocessing. These models are designed to ensure privacy while
    performing their respective tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tools**: **Diffprivlib** offers a range of generic tools designed for differentially
    private data analysis. These tools provide functionalities such as differentially
    private histograms, which adhere to the same format as NumPy’s histogram function.
    They enable users to perform various data analysis tasks while preserving privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accountant**: The accountant component includes the **BudgetAccountant**
    class, which facilitates the tracking of privacy budgets and calculation of total
    privacy loss using advanced composition techniques. This feature is crucial for
    managing and controlling privacy expenditure across multiple differentially private
    operations, ensuring that privacy guarantees are maintained. Together, these components
    in diffprivlib contribute to a comprehensive framework for implementing differential
    privacy in various data analysis scenarios. They provide the necessary tools,
    models, mechanisms, and privacy accounting capabilities to support privacy-preserving
    data analysis tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s generate cluster centroids using the diffprivlib framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `diffprivlib` framework using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the **KMeans** class from **diffprivlib.models**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the epsilon privacy parameter=, which determines the strength of privacy
    protection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an instance of KMeans with the **epsilon** parameter and the desired
    number of clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the differentially private KMeans model to the data using the **fit** method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, retrieve the differentially private cluster centroids using the **cluster_centers_**
    attribute of the KMeans model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to note that differential privacy in diffprivlib operates under
    the assumption of a trusted curator, where privacy guarantees are provided if
    the curator follows the privacy-preserving mechanisms correctly.
  prefs: []
  type: TYPE_NORMAL
- en: However, for a complete implementation of differential privacy, additional considerations,
    such as the choice of appropriate privacy parameters and the impact on data utility,
    should be carefully addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning using differential privacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will focus on developing a fraud detection model using the
    PyTorch framework. Additionally, we will train deep learning models with differential
    privacy using open source frameworks such as PyTorch and Opacus. Using the PyTorch
    framework, we will develop a deep learning model specifically designed for fraud
    detection. PyTorch is a popular open source deep learning library that provides
    a flexible and efficient platform for building and training neural networks. Its
    rich set of tools and APIs make it well-suited for developing sophisticated machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: To incorporate differential privacy into the training process, we will utilize
    the Opacus library. Opacus is an open source PyTorch extension that provides tools
    for training deep learning models with differential privacy. It offers mechanisms
    such as gradient clipping, noise addition, and privacy analysis, which help ensure
    that the trained model preserves the privacy of individual data points.
  prefs: []
  type: TYPE_NORMAL
- en: By combining PyTorch and Opacus, we can train deep learning models with differential
    privacy for fraud detection. This approach allows us to benefit from the expressive
    power of deep learning while adhering to privacy regulations and protecting sensitive
    information. Throughout this section, we will explore techniques for data preprocessing,
    model architecture design, training, and evaluation. We will consider the unique
    challenges and considerations associated with fraud detection, such as imbalanced
    datasets, feature engineering, and performance evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this section, you will have a comprehensive understanding of how
    to develop a fraud detection model using PyTorch and train it with differential
    privacy using frameworks such as Opacus. This knowledge will empower you to build
    robust and privacy-preserving machine learning models for fraud detection and
    similar applications.
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection model using PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to develop a deep learning model using PyTorch, we can follow the
    steps outlined next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load the transaction data**: Start by loading the transaction data into a
    pandas DataFrame object.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Split the data**: Split the loaded data into training and testing sets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Convert the data to PyTorch tensors**: To work with PyTorch, we need to convert
    the data into PyTorch tensors. PyTorch tensors are efficient data structures that
    allow us to perform computations using the GPU for accelerated training. We use
    the **torch.tensor** function to convert the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a simple linear model**: Define a deep learning model architecture
    using PyTorch’s **nn.Module** class. For a simple linear model, we use the **nn.Linear**
    module to create a linear layer. To classify transactions as fraud or not, we
    add a sigmoid layer at the end of the model using the **nn.Sigmoid** activation
    function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train the model**: Set up the training loop to iterate over the training
    data and update the model’s parameters based on the defined loss function and
    optimization algorithm. Use PyTorch’s **nn.CrossEntropyLoss** as the loss function
    and select an appropriate optimizer such as **torch.optim.SGD** or **torch.optim.Adam**
    for updating the model’s parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Monitor the loss**: During training, keep track of the loss at each step.
    The loss represents the discrepancy between the predicted outputs of the model
    and the true labels. By monitoring the loss, you can assess the progress of the
    model’s training and make adjustments if necessary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By following these steps, we develop a deep learning model using PyTorch for
    fraud detection. It’s important to note that this is a simplified overview, and
    you may need to customize the model architecture, incorporate additional layers
    or techniques, and fine-tune hyperparameters based on the specific requirements
    of your fraud detection task.
  prefs: []
  type: TYPE_NORMAL
- en: '*Source Code:* *Fraud_Detection_Deep Learning.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '**# unzip the fraud_transactions.csv.zip file (provided in the GitHub repo
    of this book** **as fraud_transactions.csv)**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter%205/fraud_transactions.csv.zip](https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter%205/fraud_transactions.csv.zip)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Fraud detection model with differential privacy using the Opacus framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Opacus**, an open source library, is the PyTorch implementation of the SGD-DP
    algorithm that supports differential privacy. Opacus preserves the privacy of
    each training sample while limiting the impact on the accuracy of the final model.
    In this way, the privacy of outliers is also preserved. Opacus adds noise to the
    gradients in every iteration to prevent the model from simply memorizing the training
    examples. Opacus adds noise at the right scale (too much noise will reduce the
    accuracy, and too little won’t help to protect privacy) by looking at the norm
    of the gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: More details about Opacus can be found at [https://opacus.ai/](https://opacus.ai/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing the Opacus library is done with the following code (I used the following
    version in this example: `opacus==1.1.2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We will develop the same deep learning model and train it with differential
    privacy using the Opacus framework with PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the transaction data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into train and test data sets using pandas DataFrames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the data into PyTorch tensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a simple linear model and use the sigmoid layer at the end to classify
    transactions as fraud or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make the model a private one (i.e., apply differential privacy to the model)
    using the instance of **PrivacyEngine** provided by Opacus to protect the training
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model and measure the epsilon (privacy budget).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Source :* *Fraud_Detection_DP.ipynb*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: In each epoch, the training loss fluctuates, but at the same time, privacy loss
    (budget) ε increases.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, deep learning models can be trained for differential privacy with
    minimal code changes using Opacus and protect the training data’s privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Differential privacy machine learning frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some of the popular differential privacy machine learning
    frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | Implementation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Opacus | PyTorch. |'
  prefs: []
  type: TYPE_TB
- en: '| Tensor Flow Privacy | TensorFlow. |'
  prefs: []
  type: TYPE_TB
- en: '| Pyvacy | TensorFlow Privacy, but for PyTorch. |'
  prefs: []
  type: TYPE_TB
- en: '| JAX(DP) | JAX is Autograd and XLA, brought together for high-performance
    machine learning research. |'
  prefs: []
  type: TYPE_TB
- en: '| Pysyft | `Pysyft` is a Python library for private, secure machine learning
    using federated learning and differential privacy. It allows for secure and private
    training and inference of machine learning models across multiple devices. |'
  prefs: []
  type: TYPE_TB
- en: Table 5.17 – DP ML frameworks
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of differential privacy and strategies to overcome them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Differential privacy has gained significant attention and adoption in both
    academia and industry due to its ability to balance privacy and utility. However,
    like any other technique, differential privacy has its limitations and challenges
    that need to be addressed to ensure its effective implementation. Following are
    some of the major limitations of differential privacy and potential strategies
    to overcome them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noise and utility trade-off**: Differential privacy achieves privacy by adding
    noise to query responses, which introduces a trade-off between privacy and utility.
    The amount of noise added determines the level of privacy, but excessive noise
    can significantly reduce the utility of the released data. Striking the right
    balance between privacy and utility is a challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy to overcome**: One approach to mitigating this limitation is to
    design better algorithms that minimize the impact of noise on utility. Researchers
    are constantly developing advanced mechanisms and techniques to optimize the noise
    injection process, such as adaptive noise calibration, privacy amplification through
    subsampling, or leveraging machine learning to generate more accurate and privacy-preserving
    responses.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference attacks**: Differential privacy focuses on protecting individual
    privacy by limiting the influence of a single record. However, adversaries may
    employ sophisticated inference attacks to glean information by combining multiple
    noisy queries or utilizing external side information. These attacks exploit patterns
    or correlations present in the data to infer sensitive details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy to overcome**: To overcome inference attacks, additional privacy-preserving
    techniques can be combined with differential privacy. For instance, secure **multi-party
    computation (MPC)** protocols can be used to compute aggregate statistics without
    revealing individual data points, thereby enhancing privacy protection.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy budget exhaustion**: Differential privacy employs a privacy budget,
    which represents the maximum allowable privacy loss over a sequence of queries.
    Each query consumes a portion of this budget, and once it is depleted, no further
    queries can be made while maintaining differential privacy guarantees. This limitation
    poses a challenge in scenarios where a large number of queries need to be answered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy to overcome**: One approach to address privacy budget exhaustion
    is to allocate budgets dynamically based on the sensitivity of the data or the
    specific context of the queries. By adapting the budget allocation strategy, it
    is possible to optimize the utility of the released data and extend the number
    of allowable queries without compromising privacy. Additionally, advanced composition
    techniques, such as Rényi differential privacy, can be employed to manage privacy
    budgets more effectively and allow for finer-grained control.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External data and auxiliary information**: Differential privacy assumes that
    the released data is the only source of information available to an adversary.
    However, adversaries could potentially leverage external data sources or auxiliary
    information to improve their attacks. These external sources might reveal additional
    details about individuals or contain correlated data, making it challenging to
    maintain privacy guarantees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy to overcome**: To overcome this limitation, it is crucial to carefully
    analyze the potential impact of external data sources and auxiliary information
    on privacy. Adapting data integration techniques, such as secure multiparty computation
    or cryptographic protocols, can help protect against attacks that exploit external
    information. Moreover, proactive measures such as data de-identification and minimizing
    data linkage can further enhance privacy protection against such threats.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited support for complex data types**: Differential privacy has predominantly
    focused on numerical or categorical data, which limits its applicability to more
    complex data types such as text, images, or graphs. Preserving privacy in these
    domains while maintaining meaningful utility poses a challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy to overcome**: Researchers are actively exploring techniques to
    extend differential privacy to complex data types. For example, for text data,
    approaches such as differentially private text generation or privacy-preserving
    NLP models are being developed. For images, techniques such as differentially
    private deep learning or generative adversarial networks with privacy guarantees
    are being investigated. These advancements aim to provide privacy guarantees for
    a wider range of data types. LLMs trained with differential privacy also expose
    certain privacy leaks. For more information, we strongly suggest reading the research
    paper at [https://arxiv.org/pdf/2202.05520.pdf](https://arxiv.org/pdf/2202.05520.pdf).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited protection against insider attacks**: Differential privacy primarily
    focuses on protecting data against external adversaries. However, it may not be
    as effective in scenarios where insider attacks are a concern. Insiders with access
    to the raw data might intentionally modify the data or use their knowledge to
    breach privacy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy to overcome**: Combining differential privacy with additional security
    measures can help mitigate insider attacks. Techniques such as secure enclaves
    or secure hardware can be employed to protect the privacy of sensitive data even
    from those with direct access. Employing access controls, audit logs, and strict
    data governance policies can also deter insider threats and ensure accountability.
    We will learn more about secure enclaves in [*Chapter 9*](B16573_09.xhtml#_idTextAnchor204).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Difficulty in preserving temporal privacy**: Differential privacy is primarily
    designed for static datasets, and it can be challenging to preserve privacy when
    dealing with temporal data or time-series analysis. Temporal correlations in the
    data can potentially lead to privacy breaches or inference attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy to overcome**: To address temporal privacy concerns, researchers
    are exploring techniques such as personalized differential privacy, where privacy
    parameters are adjusted based on an individual’s data history. Another approach
    involves introducing temporal consistency mechanisms that consider the correlation
    between consecutive queries or time intervals while preserving privacy. These
    techniques aim to protect privacy in dynamic and evolving datasets.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited support for machine learning models**: Differential privacy techniques
    often pose challenges when applied to machine learning models, especially deep
    learning architectures. The perturbation of model parameters or gradients may
    degrade the model’s performance or introduce vulnerabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy to overcome**: To overcome this limitation, researchers are developing
    privacy-preserving machine learning techniques tailored to differential privacy.
    Techniques such as federated learning, where models are trained on decentralized
    data without sharing sensitive information, can ensure privacy while maintaining
    utility. Additionally, advancements in privacy-preserving deep learning algorithms,
    such as differentially private stochastic gradient descent, aim to strike a balance
    between model performance and privacy guarantees. We will cover federated learning
    in more depth in the next chapter.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of standardization and interoperability**: The absence of standardized
    frameworks and interoperability can hinder the widespread adoption of differential
    privacy. Different implementations and approaches make it challenging to compare
    results or integrate privacy-preserving techniques across different platforms
    or systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategy to overcome**: Establishing standardized guidelines and frameworks
    can help address the interoperability challenge. Organizations and industry consortia
    can collaborate to develop common APIs, protocols, and evaluation metrics for
    differential privacy. Efforts in open source libraries and tools, along with community-driven
    initiatives, can facilitate knowledge sharing and enable seamless integration
    of differential privacy techniques into existing systems and workflows.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, while differential algorithms have many advantages, they are not a
    one-size-fits-all solution, and careful consideration must be taken when using
    them in practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, in this chapter, we went through open source frameworks including
    PyDP, PipelineDP, Tumult Analytics, and PySpark in order to implement differential
    privacy. We implemented fraud detection machine learning models with and without
    differential privacy by developing a private stochastic gradient descent algorithm.
    We also implemented deep learning models and trained the models with differential
    privacy using the Opacus framework, which is based on PyTorch. Finally, we covered
    the limitations of differential privacy and strategies to overcome these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn about the need for federated learning as we
    deep dive into it, covering the algorithms used and the frameworks that support
    federated learning, and explore an end-to-end implementation of a fraud detection
    use case using federated learning.
  prefs: []
  type: TYPE_NORMAL
