<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer068">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 class="chapterTitle" id="_idParaDest-71"><span class="koboSpan" id="kobo.2.1">Exploring ML Algorithms</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">While ML algorithm </span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.4.1">design may not be the primary role of ML solutions architects, it is still essential for them to possess a comprehensive understanding of common real-world ML algorithms and their applications in solving business problems. </span><span class="koboSpan" id="kobo.4.2">This knowledge empowers ML solutions architects to identify suitable data science solutions and design the necessary technology infrastructure for deploying these algorithms effectively.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.5.1">By familiarizing themselves with a range of ML algorithms, ML solutions architects can grasp the strengths, limitations, and specific use cases of each algorithm. </span><span class="koboSpan" id="kobo.5.2">This enables them to evaluate business requirements accurately and select the most appropriate algorithmic approach to address a given problem. </span><span class="koboSpan" id="kobo.5.3">Whether it’s classification, regression, clustering, or recommendation systems, understanding the underlying algorithms equips architects with the knowledge required to make informed decisions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.6.1">In this chapter, we will explore the fundamentals of ML and delve into common ML and deep learning algorithms. </span><span class="koboSpan" id="kobo.6.2">We’ll cover tasks such as classification, regression, object detection, recommendation, forecasting, and natural language generation. </span><span class="koboSpan" id="kobo.6.3">By understanding the core principles and applications of these algorithms, you’ll gain the knowledge to identify suitable ML solutions for real-world problems. </span><span class="koboSpan" id="kobo.6.4">This chapter aims to equip you with the expertise to make informed decisions and design effective ML solutions across various domains.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.7.1">Specifically, we will cover the following topics in this chapter:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.8.1">How machines learn</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.9.1">Considerations for choosing ML algorithms</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.10.1">Algorithms for classification and regression</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.11.1">Algorithms for clustering</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.12.1">Algorithms for time series</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.13.1">Algorithms for recommendation</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.14.1">Algorithms for computer vision</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.15.1">Algorithms for natural language processing</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.16.1">Generative AI algorithms</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.17.1">Hands-on exercise</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.18.1">Note that this chapter provides an introduction to ML algorithms for readers who are new to applying these algorithms. </span><span class="koboSpan" id="kobo.18.2">If you already have experience as a data scientist or ML engineer, you may want to skip this chapter and go directly to </span><em class="italic"><span class="koboSpan" id="kobo.19.1">Chapter 4</span></em><span class="koboSpan" id="kobo.20.1">, where we discuss data management for ML.</span></p>
<h1 class="heading-1" id="_idParaDest-72"><span class="koboSpan" id="kobo.21.1">Technical requirements</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.22.1">You need a personal computer (</span><strong class="keyWord"><span class="koboSpan" id="kobo.23.1">Mac</span></strong><span class="koboSpan" id="kobo.24.1"> or </span><strong class="keyWord"><span class="koboSpan" id="kobo.25.1">Windows</span></strong><span class="koboSpan" id="kobo.26.1">) to complete the hands-on exercise portion of this chapter.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.27.1">You also need to download the dataset from </span><a href="https://www.kaggle.com/mathchi/churn-for-bank-customers"><span class="url"><span class="koboSpan" id="kobo.28.1">https://www.kaggle.com/mathchi/churn-for-bank-customers</span></span></a><span class="koboSpan" id="kobo.29.1">. </span><span class="koboSpan" id="kobo.29.2">Additional instructions will be provided in the </span><em class="italic"><span class="koboSpan" id="kobo.30.1">Hands-on exercise</span></em><span class="koboSpan" id="kobo.31.1"> section.</span></p>
<h1 class="heading-1" id="_idParaDest-73"><span class="koboSpan" id="kobo.32.1">How machines learn</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.33.1">In </span><em class="chapterRef"><span class="koboSpan" id="kobo.34.1">Chapter 1</span></em><span class="koboSpan" id="kobo.35.1">, </span><em class="italic"><span class="koboSpan" id="kobo.36.1">Navigating the ML Lifecycle with ML Solutions Architecture</span></em><span class="koboSpan" id="kobo.37.1">, we discussed the</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.38.1"> self-improvement capability of ML algorithms through data processing and parameter updates, leading to the generation of models akin to compiled binaries in computer source code. </span><span class="koboSpan" id="kobo.38.2">But how does an algorithm actually learn? </span><span class="koboSpan" id="kobo.38.3">In essence, ML algorithms learn by optimizing an objective function, also known as a loss function, which involves minimizing or maximizing it. </span><span class="koboSpan" id="kobo.38.4">An objective function can be seen as a business metric, such as the disparity between projected and actual product sales. </span><span class="koboSpan" id="kobo.38.5">The aim of optimization is to reduce this disparity. </span><span class="koboSpan" id="kobo.38.6">To achieve this, an ML algorithm iterates and processes extensive historical sales data (training data), adjusting its internal model parameters until the gaps between projected and actual values are minimized. </span><span class="koboSpan" id="kobo.38.7">This process of finding the optimal model parameters is referred to as optimization, with mathematical routines specifically designed for this purpose known</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.39.1"> as optimizers.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.40.1">To illustrate the concept of optimization, let’s consider a simple example of training an ML model to predict product sales based on its price. </span><span class="koboSpan" id="kobo.40.2">In this case, we can use a linear function as the ML algorithm, represented as follows:</span></p>
<p class="center"><span class="koboSpan" id="kobo.41.1">sales = W * price + B</span></p>
<p class="normal"><span class="koboSpan" id="kobo.42.1">In this example, our </span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.43.1">objective is to minimize the disparity between the predicted and actual sales values. </span><span class="koboSpan" id="kobo.43.2">To achieve this, we employ the </span><strong class="keyWord"><span class="koboSpan" id="kobo.44.1">mean square error</span></strong><span class="koboSpan" id="kobo.45.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.46.1">MSE</span></strong><span class="koboSpan" id="kobo.47.1">) as the </span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.48.1">loss function for optimization. </span><span class="koboSpan" id="kobo.48.2">The specific task is to determine the optimal values for the model parameters </span><em class="italic"><span class="koboSpan" id="kobo.49.1">W</span></em><span class="koboSpan" id="kobo.50.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.51.1">B</span></em><span class="koboSpan" id="kobo.52.1">, commonly referred to as weight and bias. </span><span class="koboSpan" id="kobo.52.2">The weight assigns a relative significance to each input variable, while the bias represents the average output value. </span><span class="koboSpan" id="kobo.52.3">Our aim is to identify the </span><em class="italic"><span class="koboSpan" id="kobo.53.1">W</span></em><span class="koboSpan" id="kobo.54.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.55.1">B</span></em><span class="koboSpan" id="kobo.56.1"> values that yield the lowest MSE in order to enhance the accuracy of the sales predictions:</span></p>
<p class="center"><span class="koboSpan" id="kobo.57.1"><img alt="" role="presentation" src="../Images/B20836_03_001.png"/></span></p>
<p class="normal"><span class="koboSpan" id="kobo.58.1">There are multiple techniques available for solving ML optimization problems. </span><span class="koboSpan" id="kobo.58.2">Among them, gradient descent and its variations are widely used for optimizing neural networks and various other ML algorithms. </span><span class="koboSpan" id="kobo.58.3">Gradient descent is an iterative approach that involves calculating the rate of error change (gradient) associated with each input variable. </span><span class="koboSpan" id="kobo.58.4">Based on this gradient, the model parameters (</span><em class="italic"><span class="koboSpan" id="kobo.59.1">W</span></em><span class="koboSpan" id="kobo.60.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.61.1">B</span></em><span class="koboSpan" id="kobo.62.1"> in this example) are updated step by step to gradually reduce the error. </span><span class="koboSpan" id="kobo.62.2">The learning rate, a hyperparameter of the ML algorithm, controls the magnitude of parameter updates at each iteration. </span><span class="koboSpan" id="kobo.62.3">This allows for fine-tuning the optimization process. </span><span class="koboSpan" id="kobo.62.4">The following figure illustrates the optimization of the W value using gradient descent:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.63.1"><img alt="Figure 3.1 – Gradient descent " src="../Images/B20836_03_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.64.1">Figure 3.1: Gradient descent</span></p>
<p class="normal"><span class="koboSpan" id="kobo.65.1">The </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.66.1">gradient descent optimization process involves several key steps:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.67.1">Initialize the value of </span><em class="italic"><span class="koboSpan" id="kobo.68.1">W</span></em><span class="koboSpan" id="kobo.69.1"> randomly.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.70.1">Calculate the error (loss) using the assigned value of </span><em class="italic"><span class="koboSpan" id="kobo.71.1">W</span></em><span class="koboSpan" id="kobo.72.1">.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.73.1">Compute the gradient (rate of change) of the error with respect to the loss function. </span><span class="koboSpan" id="kobo.73.2">The gradient can be positive, zero, or negative.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.74.1">If the gradient is positive or negative, update the value of </span><em class="italic"><span class="koboSpan" id="kobo.75.1">W</span></em><span class="koboSpan" id="kobo.76.1"> in a direction that reduces the error in the next iteration. </span><span class="koboSpan" id="kobo.76.2">In this example, we move </span><em class="italic"><span class="koboSpan" id="kobo.77.1">W</span></em><span class="koboSpan" id="kobo.78.1"> to the right to increase its value.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.79.1">Repeat </span><em class="italic"><span class="koboSpan" id="kobo.80.1">steps 2</span></em><span class="koboSpan" id="kobo.81.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.82.1">4</span></em><span class="koboSpan" id="kobo.83.1"> until the gradient becomes zero, indicating that the optimal value of </span><em class="italic"><span class="koboSpan" id="kobo.84.1">W</span></em><span class="koboSpan" id="kobo.85.1"> has been reached and convergence has been achieved.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.86.1">In addition</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.87.1"> to gradient descent, alternative optimization techniques like the normal equation can be used to find optimal parameters for ML algorithms such as linear regression. </span><span class="koboSpan" id="kobo.87.2">Unlike the iterative approach of gradient descent, the normal equation offers a one-step analytical solution for calculating the coefficients of linear regression models. </span><span class="koboSpan" id="kobo.87.3">Other ML algorithms may also have algorithm-specific optimization methods for model training, which will be discussed in the next section.</span></p>
<h1 class="heading-1" id="_idParaDest-74"><span class="koboSpan" id="kobo.88.1">Overview of ML algorithms</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.89.1">With</span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.90.1"> that brief overview of the fundamental concepts behind how machines learn, let’s now explore various ML algorithms in more depth. </span><span class="koboSpan" id="kobo.90.2">The field of ML has seen the development of numerous algorithms, with ongoing research and innovation from both academia and industry. </span><span class="koboSpan" id="kobo.90.3">In this section, we will explore several well-known traditional and deep learning algorithms, examining their applications across various types of ML problems such as forecasting, recommendation, and natural language processing. </span><span class="koboSpan" id="kobo.90.4">Additionally, we will look at the strengths and weaknesses of different algorithms and discuss which situations each one is best suited for. </span><span class="koboSpan" id="kobo.90.5">This will help you build an understanding of the different capabilities of each algorithm and the types of problems they can be used to solve.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.91.1">Before we delve into these algorithms, it’s important to discuss the factors to consider when selecting an appropriate algorithm for a given task.</span></p>
<h2 class="heading-2" id="_idParaDest-75"><span class="koboSpan" id="kobo.92.1">Consideration for choosing ML algorithms</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.93.1">When choosing a</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.94.1"> ML algorithm, there are several key considerations to keep in mind:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.95.1">Problem type</span></strong><span class="koboSpan" id="kobo.96.1">: Different algorithms are better suited for different types of problems. </span><span class="koboSpan" id="kobo.96.2">For example, classification algorithms are suitable for tasks where the goal is to categorize data into distinct classes, while regression algorithms are used for predicting continuous numerical values. </span><span class="koboSpan" id="kobo.96.3">Understanding the problem type is crucial in selecting the most appropriate algorithm.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.97.1">Dataset size</span></strong><span class="koboSpan" id="kobo.98.1">: The size of your dataset can impact the choice of algorithm. </span><span class="koboSpan" id="kobo.98.2">Some algorithms perform well with small datasets, while others require large amounts of data to generalize effectively. </span><span class="koboSpan" id="kobo.98.3">If you have limited data, simpler algorithms with fewer parameters may be preferable to prevent overfitting. </span><span class="koboSpan" id="kobo.98.4">Overfitting is when a trained model that learns the training data too well but fails to generalize to new, unseen data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.99.1">Feature space</span></strong><span class="koboSpan" id="kobo.100.1">: Consider the number and nature of features in your dataset. </span><span class="koboSpan" id="kobo.100.2">Some algorithms can handle high-dimensional feature spaces, while others are more suitable for datasets with fewer features. </span><span class="koboSpan" id="kobo.100.3">Feature engineering and dimensionality reduction techniques can also be applied to enhance algorithm performance.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.101.1">Computational efficiency</span></strong><span class="koboSpan" id="kobo.102.1">: The computational requirements of an algorithm should be taken into account, especially if you have large datasets or limited computational resources. </span><span class="koboSpan" id="kobo.102.2">Some algorithms are computationally expensive and may not be feasible for certain environments. </span><span class="koboSpan" id="kobo.102.3">Time complexity and space complexity are quantitative measures used to assess the efficiency of ML algorithms. </span><span class="koboSpan" id="kobo.102.4">Big </span><em class="italic"><span class="koboSpan" id="kobo.103.1">O</span></em><span class="koboSpan" id="kobo.104.1"> notation represents the upper bound estimation for time and space requirements. </span><span class="koboSpan" id="kobo.104.2">For example, linear search has a time complexity of </span><em class="italic"><span class="koboSpan" id="kobo.105.1">O(N)</span></em><span class="koboSpan" id="kobo.106.1">, while binary search has </span><em class="italic"><span class="koboSpan" id="kobo.107.1">O(log N)</span></em><span class="koboSpan" id="kobo.108.1">. </span><span class="koboSpan" id="kobo.108.2">Understanding these complexities helps evaluate algorithm efficiency and scalability, aiding in algorithm selection for specific tasks.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.109.1">Interpretability</span></strong><span class="koboSpan" id="kobo.110.1">: Depending on your application, the interpretability of the algorithm’s results may be important. </span><span class="koboSpan" id="kobo.110.2">Some algorithms, such as decision trees or linear models, offer easily interpretable outcomes, while others, like deep neural networks, provide more complex and abstract representations.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.111.1">Algorithm complexity and assumptions</span></strong><span class="koboSpan" id="kobo.112.1">: Different algorithms make different assumptions about the underlying data distribution. </span><span class="koboSpan" id="kobo.112.2">Consider whether</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.113.1"> these assumptions are valid for your dataset. </span><span class="koboSpan" id="kobo.113.2">Additionally, the complexity of the algorithm can impact its ease of implementation, training time, and ability to handle noisy or incomplete data.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.114.1">By considering these factors, you can make an informed decision when selecting a ML algorithm that best suits your specific problem and available resources.</span></p>
<h2 class="heading-2" id="_idParaDest-76"><span class="koboSpan" id="kobo.115.1">Algorithms for classification and regression problems</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.116.1">The vast</span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.117.1"> majority of ML problems today primarily involve classification and regression. </span><span class="koboSpan" id="kobo.117.2">Classification is a ML task that assigns categories or classes to data points, such as labeling a credit card transaction as fraudulent or not fraudulent. </span><span class="koboSpan" id="kobo.117.3">Regression, on the other hand, is a ML technique used to predict continuous numeric values, such as predicting the price of a house.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.118.1">In the upcoming section, we’ll explore common algorithms used for classification and regression tasks. </span><span class="koboSpan" id="kobo.118.2">We will explain how each algorithm works, the types of problems each algorithm is suited for, and their limitations. </span><span class="koboSpan" id="kobo.118.3">This will help build intuition on when to select different algorithms for the different tasks.</span></p>
<h3 class="heading-3" id="_idParaDest-77"><span class="koboSpan" id="kobo.119.1">Linear regression algorithms</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.120.1">Linear regression</span></strong><span class="koboSpan" id="kobo.121.1"> algorithms </span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.122.1">are developed </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.123.1">to solve regression problems by predicting continuous values based on independent inputs. </span><span class="koboSpan" id="kobo.123.2">They find wide applications in various practical scenarios, such as estimating product sales based on price or determining crop yield based on rainfall and fertilizer.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.124.1">Linear regression utilizes a linear function of a set of coefficients and input variables to predict a scalar output. </span><span class="koboSpan" id="kobo.124.2">The formula for the linear regression is expressed as follows:</span></p>
<p class="center"><span class="koboSpan" id="kobo.125.1"><img alt="" role="presentation" src="../Images/B20836_03_002.png"/></span></p>
<p class="normal"><span class="koboSpan" id="kobo.126.1">In the linear regression equation, the </span><em class="italic"><span class="koboSpan" id="kobo.127.1">X</span></em><sub class="italic"><span class="koboSpan" id="kobo.128.1">s</span></sub><span class="koboSpan" id="kobo.129.1"> represent the input variables, </span><em class="italic"><span class="koboSpan" id="kobo.130.1">W</span></em><sub class="italic"><span class="koboSpan" id="kobo.131.1">s</span></sub><span class="koboSpan" id="kobo.132.1"> denote the coefficients, and </span><em class="italic"><span class="koboSpan" id="kobo.133.1"><img alt="" role="presentation" src="../Images/B20836_03_new.png"/></span></em><span class="koboSpan" id="kobo.134.1"> represents the error term. </span><span class="koboSpan" id="kobo.134.2">Linear regression aims to estimate the output value by calculating the weighted sum of the inputs, assuming a linear relationship between the output and inputs. </span><span class="koboSpan" id="kobo.134.3">The intuition behind linear regression is to find a line or hyperplane that can estimate the value for a set of input values. </span><span class="koboSpan" id="kobo.134.4">Linear regression can work efficiently with small datasets, offering interpretability through the coefficients’ assessment of input and output variables. </span><span class="koboSpan" id="kobo.134.5">However, it may not perform well with complex, nonlinear datasets. </span><span class="koboSpan" id="kobo.134.6">Additionally, linear regression assumes independence among input features and struggles when there is co-linearity (the value of one feature influences the value of another feature), as it becomes challenging to assess the significance of correlated features.</span></p>
<h3 class="heading-3" id="_idParaDest-78"><span class="koboSpan" id="kobo.135.1">Logistic regression algorithms</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.136.1">Logistic regression</span></strong><span class="koboSpan" id="kobo.137.1"> is </span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.138.1">commonly employed </span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.139.1">for binary and multi-class classification tasks. </span><span class="koboSpan" id="kobo.139.2">It can predict the probability of an event occurring, such as whether a person will click on an advertisement or qualify for a loan. </span><span class="koboSpan" id="kobo.139.3">Logistic regression is a valuable tool in real-world scenarios where the outcome is binary and requires estimating the likelihood of a particular class. </span><span class="koboSpan" id="kobo.139.4">By utilizing a logistic function, this algorithm maps the input variables to a probability score, enabling effective classification decision-making.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.140.1">Logistic regression is a statistical model used to estimate the probability of an event or outcome, such as transaction fraud or passing an exam. </span><span class="koboSpan" id="kobo.140.2">It is a linear model similar to linear regression, but with a different output transformation. </span><span class="koboSpan" id="kobo.140.3">The goal of logistic regression is </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.141.1">to find a decision boundary, represented by a line or hyperplane, that effectively separates the two classes of data points. </span><span class="koboSpan" id="kobo.141.2">By applying a logistic function to the linear</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.142.1"> combination of input variables, logistic regression ensures that the predicted output falls within the range of 0 and 1, representing the probability of belonging to a particular class. </span><span class="koboSpan" id="kobo.142.2">The following formula is the function for the logistic regression, where</span><em class="italic"><span class="koboSpan" id="kobo.143.1"> X </span></em><span class="koboSpan" id="kobo.144.1">is a linear combination of input variables (</span><em class="italic"><span class="koboSpan" id="kobo.145.1">b+wx</span></em><span class="koboSpan" id="kobo.146.1">). </span><span class="koboSpan" id="kobo.146.2">Here, the w is the regression coefficient:</span></p>
<p class="center"><span class="koboSpan" id="kobo.147.1"><img alt="" role="presentation" src="../Images/B20836_03_003.png"/></span></p>
<p class="normal"><span class="koboSpan" id="kobo.148.1">Like linear regression, logistic regression offers fast training speed and interpretability as its advantages. </span><span class="koboSpan" id="kobo.148.2">However, due to its linear nature, logistic regression is not suitable for solving problems with complex non-linear relationships.</span></p>
<h3 class="heading-3" id="_idParaDest-79"><span class="koboSpan" id="kobo.149.1">Decision tree algorithms</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.150.1">Decision trees</span></strong><span class="koboSpan" id="kobo.151.1"> find </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.152.1">extensive application in various real-world ML scenarios, including heart disease prediction, target marketing, and loan </span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.153.1">default prediction. </span><span class="koboSpan" id="kobo.153.2">They are versatile and can be used for both classification and regression problems.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.154.1">A decision tree </span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.155.1">is motivated by the idea that data can be divided hierarchically based on rules, leading to similar data points following the same decision path. </span><span class="koboSpan" id="kobo.155.2">It achieves this by splitting the input data using different features at different branches of the tree. </span><span class="koboSpan" id="kobo.155.3">For example, if age is a feature used for splitting at a branch, a conditional check like age &gt; 50 would be used to divide the data. </span><span class="koboSpan" id="kobo.155.4">The decision of which feature to use for splitting and where to split is made using algorithms such as the Gini purity index and information gain. </span><span class="koboSpan" id="kobo.155.5">The Gini index measures the probability of misclassification, while information gain quantifies the reduction in entropy resulting from the split.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.156.1">In this book, we won’t delve into specific algorithm details. </span><span class="koboSpan" id="kobo.156.2">However, the general concept of decision tree involves experimenting with various split options and conditions, calculating metric values (e.g., information gain) for each split option, and selecting the option that yields the highest value. </span><span class="koboSpan" id="kobo.156.3">During prediction, input data traverses the tree based on the learned branching logic, and the final prediction is determined by the terminal </span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.157.1">node (leaf node). </span><span class="koboSpan" id="kobo.157.2">Refer to </span><em class="italic"><span class="koboSpan" id="kobo.158.1">Figure 3.2</span></em><span class="koboSpan" id="kobo.159.1"> for an example structure of a decision tree.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.160.1"><img alt="Figure 3.2 – Decision tree " src="../Images/B20836_03_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.161.1">Figure 3.2: Decision tree</span></p>
<p class="normal"><span class="koboSpan" id="kobo.162.1">The main </span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.163.1">advantage of decision trees over linear regression and logistic regression is their ability to capture non-linear relationships and interactions between features. </span><span class="koboSpan" id="kobo.163.2">Decision trees can handle complex data patterns and are not limited to linear relationships between input variables and the output. </span><span class="koboSpan" id="kobo.163.3">They can represent decision boundaries that are more flexible and can handle both numerical and categorical features. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.164.1">A decision tree is advantageous in that it can handle data with minimal preprocessing, accommodate both categorical and numerical features, and handle missing values and varying feature scales. </span><span class="koboSpan" id="kobo.164.2">It is also highly interpretable, allowing for easy visualization and analysis of decision paths. </span><span class="koboSpan" id="kobo.164.3">Furthermore, decision trees are computationally efficient. </span><span class="koboSpan" id="kobo.164.4">However, they can be sensitive to outliers and</span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.165.1"> prone to </span><strong class="keyWord"><span class="koboSpan" id="kobo.166.1">overfitting</span></strong><span class="koboSpan" id="kobo.167.1">, particularly when dealing with a large number of features and noisy data. </span><span class="koboSpan" id="kobo.167.2">Overfitting occurs when the model memorizes the training data but performs poorly on unseen data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.168.1">A notable limitation of decision trees and tree-based algorithms is their inability to extrapolate beyond the range of training inputs. </span><span class="koboSpan" id="kobo.168.2">For instance, if a housing price model is trained on square footage data ranging from 500 to 3,000 sq ft, a decision tree would be unable to make predictions beyond 3,000 sq ft. </span><span class="koboSpan" id="kobo.168.3">In contrast, a linear model would be capable of capturing the trend and making predictions beyond the observed range.</span></p>
<h3 class="heading-3" id="_idParaDest-80"><span class="koboSpan" id="kobo.169.1">Random forest algorithm</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.170.1">Random forest</span></strong><span class="koboSpan" id="kobo.171.1"> algorithm is widely employed in various real-world applications across e-commerce, healthcare, and finance sectors. </span><span class="koboSpan" id="kobo.171.2">They</span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.172.1"> are particularly valuable for classification and regression tasks. </span><span class="koboSpan" id="kobo.172.2">Real-world examples of these tasks include insurance underwriting decisions, disease prediction, loan payment default prediction, and targeted marketing efforts. </span><span class="koboSpan" id="kobo.172.3">The versatility of random forest algorithms allows them to be applied in a wide range of industries to address diverse business challenges.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.173.1">As discussed in the preceding decision tree section, a decision tree uses a single tree to make its decisions, and the root node of the tree (the first feature to split the tree) has the most influence on the final decision. </span><span class="koboSpan" id="kobo.173.2">The motivation behind this random forest is that combining the decisions of multiple trees can lead to improved overall performance. </span><span class="koboSpan" id="kobo.173.3">The </span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.174.1">way that a random forest works is to create multiple smaller </span><strong class="keyWord"><span class="koboSpan" id="kobo.175.1">subtrees</span></strong><span class="koboSpan" id="kobo.176.1">, also</span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.177.1"> called </span><strong class="keyWord"><span class="koboSpan" id="kobo.178.1">weak learner trees</span></strong><span class="koboSpan" id="kobo.179.1">, where each subtree uses a random subset of all the features to come to a decision, and the final decision is made by either majority voting (for classification) or averaging (for regression). </span><span class="koboSpan" id="kobo.179.2">This process of combining the decision from multiple models is also referred to</span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.180.1"> as </span><strong class="keyWord"><span class="koboSpan" id="kobo.181.1">ensemble learning</span></strong><span class="koboSpan" id="kobo.182.1">. </span><span class="koboSpan" id="kobo.182.2">Random forest algorithms also allow you to introduce different degrees of randomness, such</span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.183.1"> as </span><strong class="keyWord"><span class="koboSpan" id="kobo.184.1">bootstrap sampling</span></strong><span class="koboSpan" id="kobo.185.1">, which involves using the same sample multiple times in a single tree. </span><span class="koboSpan" id="kobo.185.2">This helps make the model more generalized and less prone to overfitting. </span><span class="koboSpan" id="kobo.185.3">The following figure illustrates how the random forest algorithm processes input data instances using multiple subtrees and combines their outputs.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.186.1"><img alt="Figure 3.3 – Random forest " src="../Images/B20836_03_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.187.1">Figure 3.3: Random forest</span></p>
<p class="normal"><span class="koboSpan" id="kobo.188.1">Random forests </span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.189.1">have several advantages over decision trees. </span><span class="koboSpan" id="kobo.189.2">They provide improved accuracy by combining the predictions of </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.190.1">multiple trees through majority voting or averaging. </span><span class="koboSpan" id="kobo.190.2">They also reduce overfitting by introducing randomness in the model and using diverse subsets of features. </span><span class="koboSpan" id="kobo.190.3">Random forests handle large feature sets better by focusing on different aspects of the data. </span><span class="koboSpan" id="kobo.190.4">They are robust to outliers and provide feature importance estimation. </span><span class="koboSpan" id="kobo.190.5">Additionally, random forests support parallel processing for training large datasets across multiple machines. </span><span class="koboSpan" id="kobo.190.6">The limitations of random forests include reduced interpretability compared to decision trees, longer training and prediction times, increased memory usage, and the need for hyperparameter tuning.</span></p>
<h3 class="heading-3" id="_idParaDest-81"><span class="koboSpan" id="kobo.191.1">Gradient boosting machine and XGBoost algorithms</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.192.1">Gradient boosting </span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.193.1">and XGBoost are also popular </span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.194.1">multi-tree-based ML algorithms used in various domains like credit scoring, fraud detection, and insurance claim prediction. </span><span class="koboSpan" id="kobo.194.2">Unlike random forests that combine results from weak learner trees at the end, gradient boosting </span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.195.1">sequentially aggregates results from different trees.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.196.1">Random forests utilize </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.197.1">parallel independent weak learners, while gradient boosting employs a sequential approach where each weak learner tree corrects the errors of the previous tree. </span><span class="koboSpan" id="kobo.197.2">Gradient boosting offers more hyperparameters to fine-tune and can achieve superior performance with proper tuning. </span><span class="koboSpan" id="kobo.197.3">It also allows for custom loss functions, providing flexibility in modeling real-world scenarios. </span><span class="koboSpan" id="kobo.197.4">Refer to the following figure for an illustration of how gradient boosting trees operate:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.198.1"><img alt="Figure 3.4 – Gradient boosting " src="../Images/B20836_03_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.199.1">Figure 3.4: Gradient boosting</span></p>
<p class="normal"><span class="koboSpan" id="kobo.200.1">Gradient boosting offers several key advantages. </span><span class="koboSpan" id="kobo.200.2">Firstly, it excels in handling imbalanced datasets, making it highly suitable for tasks such as fraud detection and risk management. </span><span class="koboSpan" id="kobo.200.3">Secondly, it has the potential to achieve higher performance than other algorithms when properly tuned. </span><span class="koboSpan" id="kobo.200.4">Additionally, gradient boosting supports custom loss functions, providing flexibility in modeling real-world applications. </span><span class="koboSpan" id="kobo.200.5">Lastly, it can effectively capture complex relationships in the data and produce accurate predictions. </span><span class="koboSpan" id="kobo.200.6">Gradient boosting, despite its advantages, also has some limitations to consider. </span><span class="koboSpan" id="kobo.200.7">Firstly, due to its sequential nature, it lacks parallelization capabilities, making it slower in training compared to algorithms that can be parallelized. </span><span class="koboSpan" id="kobo.200.8">Secondly, gradient boosting is sensitive to noisy data, including outliers, which can lead to overfitting and reduced generalization performance. </span><span class="koboSpan" id="kobo.200.9">Lastly, the complexity of gradient boosting models can make them less interpretable compared to simpler algorithms like decision trees, making it challenging to understand the underlying relationships in the data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.201.1">XGBoost, a widely-used</span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.202.1"> implementation of gradient boosting, has gained popularity for its success in Kaggle competitions. </span><span class="koboSpan" id="kobo.202.2">While it shares the same underlying concept as gradient boosting, XGBoost </span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.203.1">offers several improvements. </span><span class="koboSpan" id="kobo.203.2">It enables training a single tree across multiple cores and CPUs, leading to faster training times. </span><span class="koboSpan" id="kobo.203.3">XGBoost incorporates powerful regularization techniques to mitigate overfitting and reduce model complexity. </span><span class="koboSpan" id="kobo.203.4">It also excels in handling sparse datasets. </span><span class="koboSpan" id="kobo.203.5">In addition to XGBoost, other popular variations of gradient boosting trees include LightGBM and CatBoost.</span></p>
<h3 class="heading-3" id="_idParaDest-82"><span class="koboSpan" id="kobo.204.1">K-nearest neighbor algorithm</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.205.1">K-nearest neighbor</span></strong><span class="koboSpan" id="kobo.206.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.207.1">K-NN</span></strong><span class="koboSpan" id="kobo.208.1">) is a versatile </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.209.1">algorithm </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.210.1">used for both classification and regression tasks. </span><span class="koboSpan" id="kobo.210.2">It is also employed in search systems and recommendation systems. </span><span class="koboSpan" id="kobo.210.3">The underlying assumption of K-NN is that similar items tend to have close proximity to each other in the feature space. </span><span class="koboSpan" id="kobo.210.4">To determine this proximity, distances between different data points are measured, often using metrics like Euclidean distance.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.211.1">In the case of classification, K-NN starts by loading the training data along with their respective class labels. </span><span class="koboSpan" id="kobo.211.2">When a new data point needs to be classified, its distances to the existing data points are calculated, typically using Euclidean distance. </span><span class="koboSpan" id="kobo.211.3">The K nearest neighbors to the new data point are identified, and their class labels are retrieved. </span><span class="koboSpan" id="kobo.211.4">The class label for the new data point is then determined through majority voting, where the most frequent class among the K nearest neighbors is assigned to the new data point. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.212.1">The following diagram is an illustration of using K-NN for classification:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.213.1"><img alt="A diagram of a diagram of a diagram  Description automatically generated" src="../Images/B20836_03_05.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.214.1">Figure 3.5: K-NN for classification</span></p>
<p class="normal"><span class="koboSpan" id="kobo.215.1">For</span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.216.1"> regression tasks, K-NN follows a similar approach. </span><span class="koboSpan" id="kobo.216.2">The distances between the new data point and the existing data points are computed, and the K nearest neighbors are selected. </span><span class="koboSpan" id="kobo.216.3">The predicted scalar value for the new data point is obtained by averaging the values of the K closest data points.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.217.1">One</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.218.1"> advantage of K-NN is its simplicity and lack of the need for training or tuning with hyperparameters, apart from selecting the value of K. </span><span class="koboSpan" id="kobo.218.2">The dataset is loaded directly into the model without the need to train a model. </span><span class="koboSpan" id="kobo.218.3">It is worth noting that the choice of K significantly impacts the performance of the K-NN mode. </span><span class="koboSpan" id="kobo.218.4">The optimal K is often found through an iterative trial-and-error process by evaluation of hold-out dataset. </span><span class="koboSpan" id="kobo.218.5">The results of K-NN are also easily explainable, as each prediction can be understood by examining the properties of the nearest neighbors. </span><span class="koboSpan" id="kobo.218.6">However, K-NN has some limitations.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.219.1">As the number of data points increases, the complexity of the model grows, and predictions can become slower, especially with large datasets. </span><span class="koboSpan" id="kobo.219.2">K-NN is not suitable for high-dimensional datasets, as the concept of proximity becomes less meaningful in higher-dimensional spaces. </span><span class="koboSpan" id="kobo.219.3">The algorithm is also sensitive to noisy data and missing data, requiring outlier removal and data imputation techniques to handle such cases effectively.</span></p>
<h3 class="heading-3" id="_idParaDest-83"><span class="koboSpan" id="kobo.220.1">Multi-layer perceptron (MLP) networks</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.221.1">As</span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.222.1"> mentioned earlier, an </span><strong class="keyWord"><span class="koboSpan" id="kobo.223.1">artificial neural network</span></strong><span class="koboSpan" id="kobo.224.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.225.1">ANN</span></strong><span class="koboSpan" id="kobo.226.1">) emulates </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.227.1">the learning</span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.228.1"> process of the human brain. </span><span class="koboSpan" id="kobo.228.2">The brain comprises numerous interconnected neurons that process information. </span><span class="koboSpan" id="kobo.228.3">Each neuron in a network processes inputs (electrical impulses) from another neuron, processes and transforms the inputs, and sends the output to neurons in the network. </span><span class="koboSpan" id="kobo.228.4">Here is an illustration depicting a human neuron:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.229.1"><img alt="Figure 3.5 – Human brain neuron " src="../Images/B20836_03_06.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.230.1">Figure 3.6: Human brain neuron</span></p>
<p class="normal"><span class="koboSpan" id="kobo.231.1">An artificial neuron operates in a similar manner. </span><span class="koboSpan" id="kobo.231.2">The following diagram illustrates an artificial neuron, which consists of a linear function combined with an activation function. </span><span class="koboSpan" id="kobo.231.3">The activation function modifies the output of the linear function, such as compressing it within a specific range, such as 0 to 1 (sigmoid activation), -1 to 1 (tanh activation), or maintaining values above 0 (ReLU). </span><span class="koboSpan" id="kobo.231.4">The activation function is employed to capture non-linear relationships between inputs and outputs. </span><span class="koboSpan" id="kobo.231.5">Alternatively, each neuron can be viewed as a linear classifier, akin to logistic regression.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.232.1"><img alt="Figure 3.6 – Artificial neuron " src="../Images/B20836_03_07.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.233.1">Figure 3.7: Artificial neuron</span></p>
<p class="normal"><span class="koboSpan" id="kobo.234.1">When </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.235.1">you stack a large number of neurons into different layers (</span><em class="italic"><span class="koboSpan" id="kobo.236.1">input layer</span></em><span class="koboSpan" id="kobo.237.1">, </span><em class="italic"><span class="koboSpan" id="kobo.238.1">hidden layers</span></em><span class="koboSpan" id="kobo.239.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.240.1">output layer</span></em><span class="koboSpan" id="kobo.241.1">) and connect </span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.242.1">all of the neurons together between two adjacent layers, we have an ANN called </span><strong class="keyWord"><span class="koboSpan" id="kobo.243.1">multi-layer perceptron</span></strong><span class="koboSpan" id="kobo.244.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.245.1">MLP</span></strong><span class="koboSpan" id="kobo.246.1">). </span><span class="koboSpan" id="kobo.246.2">Here, the term </span><em class="italic"><span class="koboSpan" id="kobo.247.1">perceptron</span></em><span class="koboSpan" id="kobo.248.1"> means </span><em class="italic"><span class="koboSpan" id="kobo.249.1">artificial neuron</span></em><span class="koboSpan" id="kobo.250.1">, and it was originally invented by Frank Rosenblatt in 1957. </span><span class="koboSpan" id="kobo.250.2">The idea behind MLP is that each hidden layer will learn some higher-level representation (features) of the previous layer, and those higher-level features capture the more important information in the previous layer. </span><span class="koboSpan" id="kobo.250.3">When the output from the final hidden layer is used for prediction, the network has extracted the most important information from the raw inputs for training a classifier or regressor. </span><span class="koboSpan" id="kobo.250.4">The following figure shows the architecture of an MLP network:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.251.1"><img alt="Figure 3.7 – Multi-layer perceptron " src="../Images/B20836_03_08.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.252.1">Figure 3.8: Multi-layer perceptron</span></p>
<p class="normal"><span class="koboSpan" id="kobo.253.1">During </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.254.1">model training, the weights (</span><em class="italic"><span class="koboSpan" id="kobo.255.1">W</span></em><span class="koboSpan" id="kobo.256.1">) of each neuron in every layer are adjusted using gradient descent to optimize the training </span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.257.1">objective. </span><span class="koboSpan" id="kobo.257.2">This adjustment process is known as backpropagation. </span><span class="koboSpan" id="kobo.257.3">It involves propagating the total error back through the network, attributing a portion of the error to each neuron based on its contribution. </span><span class="koboSpan" id="kobo.257.4">This allows the fine-tuning of the weights in each neuron, ensuring that every neuron in every layer influences the final output to improve overall performance.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.258.1">MLP is a versatile neural network suitable for both classification and regression tasks, similar to random forest and XGBoost. </span><span class="koboSpan" id="kobo.258.2">While commonly applied to tabular data, it can also handle diverse data formats like images and text. </span><span class="koboSpan" id="kobo.258.3">MLP excels in capturing intricate nonlinear patterns within the dataset and exhibits efficient computational processing, thanks to its parallelization capabilities. </span><span class="koboSpan" id="kobo.258.4">However, MLP typically demands a larger training dataset to achieve optimal performance compared to traditional ML algorithms.</span></p>
<h2 class="heading-2" id="_idParaDest-84"><span class="koboSpan" id="kobo.259.1">Algorithms for clustering</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.260.1">Clustering is a data </span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.261.1">mining method that involves</span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.262.1"> grouping items together based on their shared attributes. </span><span class="koboSpan" id="kobo.262.2">One practical application of clustering is to create customer segments by analyzing demographics, transaction history, or behavior data. </span><span class="koboSpan" id="kobo.262.3">Other examples include social network analysis, document grouping, and anomaly detection. </span><span class="koboSpan" id="kobo.262.4">Various clustering algorithms exist, and we will focus on the K-means clustering algorithm in this section, which is one of the most widely used </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.263.1">clustering algorithms due to its simplicity. </span><span class="koboSpan" id="kobo.263.2">Some other popular clustering algorithms are hierarchical clustering and DBSCAN.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.264.1">K-means algorithm</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.265.1">The K-means algorithm </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.266.1">is widely employed in real-world applications, including customer segmentation analysis, document classification based on document attributes, and insurance fraud detection. </span><span class="koboSpan" id="kobo.266.2">It is a versatile algorithm that can effectively group data points in various domains for different purposes.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.267.1">K-means </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.268.1">aims to group similar data points together in clusters, and it is an unsupervised algorithm, meaning it doesn’t rely on labeled data. </span><span class="koboSpan" id="kobo.268.2">The algorithm begins by randomly assigning K centroids, which represent the centers of the clusters. </span><span class="koboSpan" id="kobo.268.3">It then iteratively adjusts the assignment of data points to the nearest centroid and updates the centroids to the mean of the data points in each cluster. </span><span class="koboSpan" id="kobo.268.4">This process continues until convergence, resulting in well-defined clusters based on similarity.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.269.1">K-means clustering offers several advantages, including its simplicity and ease of understanding, making it accessible to beginners. </span><span class="koboSpan" id="kobo.269.2">It is computationally efficient and can handle large datasets effectively. </span><span class="koboSpan" id="kobo.269.3">The resulting clusters are interpretable, providing valuable insights into the underlying patterns in the data. </span><span class="koboSpan" id="kobo.269.4">K-means is versatile and applicable to various types of data, including numerical, categorical, and mixed attribute datasets. </span><span class="koboSpan" id="kobo.269.5">However, there are some drawbacks to consider. </span><span class="koboSpan" id="kobo.269.6">Selecting the optimal number of clusters (</span><em class="italic"><span class="koboSpan" id="kobo.270.1">K</span></em><span class="koboSpan" id="kobo.271.1">) can be subjective and challenging. </span><span class="koboSpan" id="kobo.271.2">The algorithm is sensitive to the initial placement of centroids, which can lead to different cluster formations. </span><span class="koboSpan" id="kobo.271.3">K-means assumes spherical clusters with equal variance, which may not hold true in all cases. </span><span class="koboSpan" id="kobo.271.4">It is also sensitive to outliers and struggles with non-linear data relationships.</span></p>
<h2 class="heading-2" id="_idParaDest-85"><span class="koboSpan" id="kobo.272.1">Algorithms for time series analysis</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.273.1">A time series </span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.274.1">consists of a sequence of data points recorded at successive time intervals. </span><span class="koboSpan" id="kobo.274.2">It is commonly used to analyze and predict </span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.275.1">trends in various domains, such as finance, retail, and sales. </span><span class="koboSpan" id="kobo.275.2">Time series analysis allows us to understand past patterns and make future predictions based on the relationship between current and past values. </span><span class="koboSpan" id="kobo.275.3">Forecasting in time series relies on the assumption that future values are influenced by previous observations at different time points.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.276.1">Time series data exhibits</span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.277.1"> several important characteristics, including trend, seasonality, and stationarity. </span><strong class="keyWord"><span class="koboSpan" id="kobo.278.1">Trend</span></strong><span class="koboSpan" id="kobo.279.1"> refers to the </span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.280.1">long-term direction of the data, whether it shows an overall increase or decrease over time. </span><span class="koboSpan" id="kobo.280.2">It helps to identify the underlying pattern and understand</span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.281.1"> the general behavior of the time series. </span><strong class="keyWord"><span class="koboSpan" id="kobo.282.1">Seasonality</span></strong><span class="koboSpan" id="kobo.283.1">, on the other hand, captures repeating patterns within a fixed interval, often occurring in cycles or seasons. </span><span class="koboSpan" id="kobo.283.2">It helps to identify regular fluctuations that repeat over specific time periods, such as daily, weekly, or yearly patterns. </span><strong class="keyWord"><span class="koboSpan" id="kobo.284.1">Stationarity</span></strong><span class="koboSpan" id="kobo.285.1"> refers to the</span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.286.1"> property of a time series where statistical properties, such as mean and variance, remain constant over time. </span><span class="koboSpan" id="kobo.286.2">Stationarity is crucial because many forecasting techniques assume that the underlying data is stationary. </span><span class="koboSpan" id="kobo.286.3">Non-stationary time series can lead to inaccurate or unreliable forecasts. </span><span class="koboSpan" id="kobo.286.4">Therefore, it is important to assess and address the stationarity of a time series before applying forecasting techniques.</span></p>
<h3 class="heading-3" id="_idParaDest-86"><span class="koboSpan" id="kobo.287.1">ARIMA algorithm</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.288.1">The </span><strong class="keyWord"><span class="koboSpan" id="kobo.289.1">autoregressive integrated moving average</span></strong><span class="koboSpan" id="kobo.290.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.291.1">ARIMA</span></strong><span class="koboSpan" id="kobo.292.1">) algorithm finds practical applications</span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.293.1"> in various</span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.294.1"> real-world scenarios, including budget forecasting, sales forecasting, patient visit forecasting, and customer support call volume forecasting. </span><span class="koboSpan" id="kobo.294.2">ARIMA is a powerful tool for analyzing and predicting time series data, allowing organizations to make informed decisions and optimize their operations in these areas. </span><span class="koboSpan" id="kobo.294.3">By leveraging historical patterns and trends in the data, ARIMA enables accurate forecasts and assists businesses in effectively managing their resources and planning for the future.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.295.1">ARIMA operates on the premise that the value of a variable in a given period is influenced by its own previous values (autoregressive), the deviations from the mean follow a pattern based on previous deviations (moving average), and trend and seasonality can be eliminated by differencing (calculating the differences between consecutive data points). </span><span class="koboSpan" id="kobo.295.2">This differencing process aims to transform the time series into a stationary state, where statistical properties like mean and variance remain constant over time. </span><span class="koboSpan" id="kobo.295.3">These</span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.296.1"> three components of ARIMA can be mathematically represented </span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.297.1">using the following formulas:</span></p>
<p class="center"><span class="koboSpan" id="kobo.298.1"><img alt="" role="presentation" src="../Images/B20836_03_004.png"/></span></p>
<p class="normal"><span class="koboSpan" id="kobo.299.1">Where the </span><strong class="keyWord"><span class="koboSpan" id="kobo.300.1">autoregressive</span></strong><span class="koboSpan" id="kobo.301.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.302.1">AR</span></strong><span class="koboSpan" id="kobo.303.1">) component is expressed as a regression of previous values (also known as lags):</span></p>
<p class="center"><span class="koboSpan" id="kobo.304.1"><img alt="" role="presentation" src="../Images/B20836_03_005.png"/></span></p>
<p class="normal"><span class="koboSpan" id="kobo.305.1">The constant </span><em class="italic"><span class="koboSpan" id="kobo.306.1">C</span></em><span class="koboSpan" id="kobo.307.1"> represents a drift:</span></p>
<p class="center"><span class="koboSpan" id="kobo.308.1"><img alt="" role="presentation" src="../Images/B20836_03_006.png"/></span></p>
<p class="normal"><span class="koboSpan" id="kobo.309.1">The </span><strong class="keyWord"><span class="koboSpan" id="kobo.310.1">moving average</span></strong><span class="koboSpan" id="kobo.311.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.312.1">MA</span></strong><span class="koboSpan" id="kobo.313.1">) component is expressed as a weighted average of forecasting errors for the previous time periods, where it represents a constant:</span></p>
<p class="center"><span class="koboSpan" id="kobo.314.1"><img alt="" role="presentation" src="../Images/B20836_03_007.png"/></span></p>
<p class="normal"><span class="koboSpan" id="kobo.315.1">The </span><strong class="keyWord"><span class="koboSpan" id="kobo.316.1">integrated component</span></strong><span class="koboSpan" id="kobo.317.1"> (time series differencing) of a time series can be expressed as the difference between the values in one period from the previous period.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.318.1">ARIMA is a suitable choice for forecasting single time series (univariate) data as it doesn’t rely on additional variables. </span><span class="koboSpan" id="kobo.318.2">It outperforms simpler forecasting techniques like simple moving average, exponential smoothing, or linear regression. </span><span class="koboSpan" id="kobo.318.3">Additionally, ARIMA provides interpretability, allowing for a clear understanding of the underlying patterns. </span><span class="koboSpan" id="kobo.318.4">However, due to its backward-looking nature, ARIMA may struggle to accurately forecast unexpected events. </span><span class="koboSpan" id="kobo.318.5">Furthermore, being a linear-based model, ARIMA may not effectively capture complex non-linear relationships in time series data.</span></p>
<h3 class="heading-3" id="_idParaDest-87"><span class="koboSpan" id="kobo.319.1">DeepAR algorithm</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.320.1">Deep learning-based</span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.321.1"> forecasting algorithms offer solutions to the limitations of traditional models like ARIMA. </span><span class="koboSpan" id="kobo.321.2">They excel at capturing complex non-linear relationships and can effectively utilize multivariate datasets. </span><span class="koboSpan" id="kobo.321.3">These models enable </span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.322.1">the training of a global model, allowing for a single model to handle multiple similar target time series. </span><span class="koboSpan" id="kobo.322.2">This eliminates the need for creating separate models for each individual time series, providing a more efficient and scalable approach.</span></p>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.323.1">Deep Autoregressive</span></strong><span class="koboSpan" id="kobo.324.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.325.1">DeepAR</span></strong><span class="koboSpan" id="kobo.326.1">) is a state-of-the-art forecasting algorithm based on neural networks, designed to handle large datasets with multiple similar target time series. </span><span class="koboSpan" id="kobo.326.2">It has the capability to incorporate related time series, such as product prices or holiday schedules, to enhance the accuracy of its forecasting models. </span><span class="koboSpan" id="kobo.326.3">This feature proves particularly valuable when dealing with spiky events triggered by external variables, allowing for more precise and reliable predictions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.327.1">DeepAR </span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.328.1">utilizes a </span><strong class="keyWord"><span class="koboSpan" id="kobo.329.1">recurrent neural network</span></strong><span class="koboSpan" id="kobo.330.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.331.1">RNN</span></strong><span class="koboSpan" id="kobo.332.1">) as its underlying model to capture patterns in the target time series. </span><span class="koboSpan" id="kobo.332.2">It goes beyond single-variable forecasting by incorporating multiple target time series and additional external supporting time series. </span><span class="koboSpan" id="kobo.332.3">Instead of considering individual values, the RNN takes input vectors representing the values of various variables at each time period. </span><span class="koboSpan" id="kobo.332.4">By jointly learning the patterns of these combined vectors over time, DeepAR can effectively capture the intrinsic non-linear relationships and shared patterns among the different time series. </span><span class="koboSpan" id="kobo.332.5">This approach enables DeepAR to train a single global model that can be used for forecasting across multiple similar target time series.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.333.1">DeepAR excels in handling complex multivariate datasets; however, it performs best when trained with large amounts of data. </span><span class="koboSpan" id="kobo.333.2">It is particularly useful in real-world scenarios involving large-scale retail forecasting for numerous items, where external factors like marketing campaigns and holiday schedules need to be taken into account. </span><span class="koboSpan" id="kobo.333.3">By leveraging its capability to model multiple variables simultaneously, DeepAR can provide accurate predictions and insights in such practical use cases.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.334.1">A significant drawback of DeepAR is the black-box nature of the deep learning model, which lacks interpretability and transparency. </span><span class="koboSpan" id="kobo.334.2">This makes the forecasts more difficult to explain and justify than simpler statistical methods. </span><span class="koboSpan" id="kobo.334.3">Another major disadvantage is the data-hungry nature of DeepAR, whereby it performs poorly when the dataset is small.</span></p>
<h2 class="heading-2" id="_idParaDest-88"><span class="koboSpan" id="kobo.335.1">Algorithms for recommendation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.336.1">The </span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.337.1">recommender system is an essential ML technology that predicts a user’s preference for items, primarily relying on user or item attribute similarities or user-item interactions. </span><span class="koboSpan" id="kobo.337.2">It has gained widespread adoption in various industries, including retail, media and entertainment, finance, and healthcare. </span><span class="koboSpan" id="kobo.337.3">Over the</span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.338.1"> years, the field of recommendation algorithms has evolved significantly, from making recommendations based on preferences and behaviors of similar users, to a reinforcement-learning-based approach where algorithms learn to make sequential decisions over time, taking into account user feedback and interaction. </span><span class="koboSpan" id="kobo.338.2">In the following section, we will explore some commonly used algorithms in the realm of recommender systems.</span></p>
<h3 class="heading-3" id="_idParaDest-89"><span class="koboSpan" id="kobo.339.1">Collaborative filtering algorithm</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.340.1">Collaborative filtering</span></strong><span class="koboSpan" id="kobo.341.1"> is a </span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.342.1">popular recommendation algorithm that leverages the notion that individuals with similar</span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.343.1"> interests or preferences in one set of items are likely to have similar interests in other items as well. </span><span class="koboSpan" id="kobo.343.2">By analyzing the collective experiences and behaviors of different users, collaborative filtering can effectively recommend items to individual users based on the preferences of similar users. </span><span class="koboSpan" id="kobo.343.3">This approach taps into the experiences of the crowd to provide personalized and relevant recommendations.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.344.1">The following figure illustrates an</span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.345.1"> item-user interaction matrix in the context of movie ratings. </span><span class="koboSpan" id="kobo.345.2">As you can see, it is a </span><strong class="keyWord"><span class="koboSpan" id="kobo.346.1">sparse matrix</span></strong><span class="koboSpan" id="kobo.347.1">. </span><span class="koboSpan" id="kobo.347.2">This means that there are many empty entries in the matrix, which is expected as it is unlikely for any individual to have watched every movie:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.348.1"><img alt="Figure 3.8 – User-item interaction matrix for collaborative filtering " src="../Images/B20836_03_09.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.349.1">Figure 3.9: User-item interaction matrix for collaborative filtering</span></p>
<p class="normal"><span class="koboSpan" id="kobo.350.1">One of the major benefits of collaborative filtering is that it can provide highly personalized recommendations matched to each user’s unique interests. </span><span class="koboSpan" id="kobo.350.2">Unlike content-based systems, collaborative filtering models do not need to analyze and understand item features and content. </span><span class="koboSpan" id="kobo.350.3">Instead, they rely solely on behavioral patterns like ratings, purchases, clicks, and preferences across users to uncover correlations. </span><span class="koboSpan" id="kobo.350.4">This allows </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.351.1">collaborative systems to get a nuanced profile of a user’s likes and dislikes based on crowd wisdom. </span><span class="koboSpan" id="kobo.351.2">The algorithms can then generate recommendations tailored to that specific user, going beyond obvious suggestions. </span><span class="koboSpan" id="kobo.351.3">This level of personalization and ability to capture user preferences makes collaborative filtering a powerful approach, especially for large catalogs where analyzing content is infeasible. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.352.1">Collaborative filtering</span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.353.1"> also comes with some notable downsides. </span><span class="koboSpan" id="kobo.353.2">A major issue is the cold-start problem: collaborative models struggle when new users or items with no ratings are introduced. </span><span class="koboSpan" id="kobo.353.3">The algorithms rely heavily on crowd ratings, so they cannot effectively recommend to users new items that lack this historical data. </span><span class="koboSpan" id="kobo.353.4">Collaborative systems can also lead to limited diversity, creating filter bubbles and obvious recommendations rather than novel ones. </span><span class="koboSpan" id="kobo.353.5">They commonly face sparsity issues as the user-item matrix is often sparse, especially for large catalogs.</span></p>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.354.1">Matrix factorization</span></strong><span class="koboSpan" id="kobo.355.1"> is a </span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.356.1">technique commonly used in collaborative filtering for recommendation systems. </span><span class="koboSpan" id="kobo.356.2">It involves learning vector representations, or embeddings, for both users and items in the user-item interaction matrix. </span><span class="koboSpan" id="kobo.356.3">The goal is to approximate the original matrix by taking the product of the learned user and item embedding matrices. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.357.1">This allows us to predict the missing entries in the matrix, which represent the likely ratings that a user would give to unseen items. </span><span class="koboSpan" id="kobo.357.2">To make predictions, we simply compute the dot product between the user embedding and the item embedding.</span></p>
<div class="note">
<p class="normal"><span class="koboSpan" id="kobo.358.1">Embedding is a fundamental concept in ML that plays a crucial role in various domains. </span><span class="koboSpan" id="kobo.358.2">It involves creating numerical representations for entities, such as words or objects, in a manner that captures their semantic similarity. </span><span class="koboSpan" id="kobo.358.3">These representations are organized in a multi-dimensional space, where similar entities are positioned closer to each other. </span><span class="koboSpan" id="kobo.358.4">By using embeddings, we can uncover the underlying latent semantics of the objects, enabling more effective analysis and modeling. </span><span class="koboSpan" id="kobo.358.5">In the upcoming sections, we will delve deeper into embedding techniques and their applications in NLP algorithms.</span></p>
</div>
<p class="normal"><span class="koboSpan" id="kobo.359.1">Matrix factorization</span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.360.1"> provides </span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.361.1">major scalability benefits so collaborative filtering can be applied to extremely large catalogs. </span><span class="koboSpan" id="kobo.361.2">However, the algorithm loses some transparency due to the latent factor modeling. </span><span class="koboSpan" id="kobo.361.3">Overall, matrix factorization extends collaborative filtering to much bigger datasets but sacrifices some interpretability.</span></p>
<h3 class="heading-3" id="_idParaDest-90"><span class="koboSpan" id="kobo.362.1">Multi-armed bandit/contextual bandit algorithm</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.363.1">Collaborative filtering-based recommender </span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.364.1">systems heavily rely on prior interaction data between </span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.365.1">identified users and items to make accurate recommendations. </span><span class="koboSpan" id="kobo.365.2">However, these systems face challenges when there is a lack of prior interactions or when the user is anonymous, resulting in a cold-start problem. </span><span class="koboSpan" id="kobo.365.3">To address this issue, one approach is to utilize a </span><strong class="keyWord"><span class="koboSpan" id="kobo.366.1">multi-armed bandit</span></strong><span class="koboSpan" id="kobo.367.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.368.1">MAB</span></strong><span class="koboSpan" id="kobo.369.1">) based recommendation system. </span><span class="koboSpan" id="kobo.369.2">This approach draws inspiration from the concept of trial and error, similar to a gambler simultaneously playing multiple slot machines and observing which machine yields the best overall return. </span><span class="koboSpan" id="kobo.369.3">By employing reinforcement learning techniques, MAB-based recommendation systems dynamically explore and exploit different recommendations to optimize the user experience, even in the absence of substantial prior interaction data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.370.1">MAB algorithms operate under the paradigm of online learning, where there is no pre-existing training data to train a model prior to deployment. </span><span class="koboSpan" id="kobo.370.2">Instead, the model incrementally learns and adapts as data becomes available. </span><span class="koboSpan" id="kobo.370.3">In the initial stages of MAB learning, the model recommends all available options (such as products on an e-commerce site) with equal probabilities to users. </span><span class="koboSpan" id="kobo.370.4">As users begin to interact with a subset of the items and provide feedback (rewards), the MAB model adjusts its strategy. </span><span class="koboSpan" id="kobo.370.5">It starts to offer items that have yielded higher rewards (e.g., more user interactions) more frequently, exploiting the knowledge of their positive performance. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.371.1">However, the model also continues to allocate a smaller percentage of recommendations to new items, aiming to explore their potential for receiving interactions. </span><span class="koboSpan" id="kobo.371.2">This balance between exploration (offering new items) and exploitation (offering items with known rewards) is a fundamental tradeoff in MAB algorithms.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.372.1">MAB algorithms face several limitations. </span><span class="koboSpan" id="kobo.372.2">Striking the right balance between exploration and exploitation can be challenging, leading to suboptimal solutions in certain environments. </span><span class="koboSpan" id="kobo.372.3">Handling high-dimensional contextual information poses a challenge as well, and the algorithms may be sensitive to noisy rewards. </span><span class="koboSpan" id="kobo.372.4">Additionally, the cold-start problem arises when there is limited historical data for new items or users.</span></p>
<h2 class="heading-2" id="_idParaDest-91"><span class="koboSpan" id="kobo.373.1">Algorithms for computer vision problems</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.374.1">Computer vision </span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.375.1">refers to the ability of </span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.376.1">computers to interpret and understand visual representations, such as images and videos, in order to perform tasks like object identification, image classification, text detection, face recognition, and activity detection. </span><span class="koboSpan" id="kobo.376.2">These tasks rely on pattern recognition, where images are labeled with object names and bounding boxes, and computer vision models are trained to recognize these patterns and make predictions on new images. </span><span class="koboSpan" id="kobo.376.3">Computer vision technology finds numerous applications in practical domains such as content management, security, augmented reality, self-driving cars, medical</span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.377.1"> diagnosis, sports analytics, and quality inspection in manufacturing. </span><span class="koboSpan" id="kobo.377.2">In the following section, we will delve deeper into a few neural network architectures specifically designed for computer vision tasks.</span></p>
<div class="note">
<p class="normal"><span class="koboSpan" id="kobo.378.1">Although the upcoming sections involve deep learning architectures, embeddings, and other techniques—elements that may not strictly conform to the traditional definition of algorithms—we will be referring to them as “algorithms” for the sake of semantic consistency throughout this chapter. </span><span class="koboSpan" id="kobo.378.2">With this, we hope to facilitate a smoother understanding of the nuanced concepts we’ll be exploring.</span></p>
</div>
<h3 class="heading-3" id="_idParaDest-92"><span class="koboSpan" id="kobo.379.1">Convolutional neural networks</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.380.1">A </span><strong class="keyWord"><span class="koboSpan" id="kobo.381.1">convolutional neural network</span></strong><span class="koboSpan" id="kobo.382.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.383.1">CNN</span></strong><span class="koboSpan" id="kobo.384.1">) is a </span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.385.1">deep learning</span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.386.1"> architecture specifically designed for processing and analyzing image data. </span><span class="koboSpan" id="kobo.386.2">It takes inspiration from the functioning of the animal visual cortex. </span><span class="koboSpan" id="kobo.386.3">In the visual cortex, individual neurons respond to visual stimuli within specific subregions of the visual field. </span><span class="koboSpan" id="kobo.386.4">These subregions, covered by different neurons, partially overlap to cover the entire visual field. </span><span class="koboSpan" id="kobo.386.5">Similarly, in a CNN, different filters are applied to interact with subregions of an image, capturing and responding to the information within that region. </span><span class="koboSpan" id="kobo.386.6">This allows the CNN to extract meaningful features and patterns from the image data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.387.1">A CNN architecture consists of multiple layers that repeat in a pattern. </span><span class="koboSpan" id="kobo.387.2">Each layer has different sublayers with specific functions. </span><span class="koboSpan" id="kobo.387.3">The convolutional layer plays a crucial role in feature extraction from input images. </span><span class="koboSpan" id="kobo.387.4">It utilizes convolutional filters, which are matrices defined by height and width, to extract relevant features. </span><span class="koboSpan" id="kobo.387.5">These convolutional layers process the input images by convolving them with the filters, producing feature maps that are passed to the next layer in the network.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.388.1">The </span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.389.1">pooling layer, found after one </span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.390.1">or multiple convolutional layers, reduces the dimensionality of the extracted features. </span><span class="koboSpan" id="kobo.390.2">It combines multiple outputs into a single output, resulting in a more compact representation. </span><span class="koboSpan" id="kobo.390.3">Two commonly used pooling techniques are max pooling, which selects the maximum value from the outputs, and average pooling, which calculates the average value.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.391.1">Following the convolutional and pooling layers, a fully connected layer is employed to combine and flatten the outputs from the previous layer. </span><span class="koboSpan" id="kobo.391.2">This layer aggregates the extracted features and feeds them into an output layer, typically used for tasks like image classification.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.392.1">The architecture of a CNN is illustrated in the following figure, showcasing the flow of information through the various layers:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.393.1"><img alt="Figure 3.9 – CNN architecture " src="../Images/B20836_03_10.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.394.1">Figure 3.10: CNN architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.395.1">CNN-based models offer efficient training due to their high degree of parallelism. </span><span class="koboSpan" id="kobo.395.2">This is particularly advantageous for tasks involving large-scale image data, where parallel processing can significantly accelerate training time. </span><span class="koboSpan" id="kobo.395.3">While CNNs are primarily used for computer vision tasks, their success has led to their application in other domains as well, including</span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.396.1"> natural language processing. </span><span class="koboSpan" id="kobo.396.2">By adapting the principles of convolution and hierarchical </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.397.1">feature extraction, CNNs have shown promise in tasks such as text classification and sentiment analysis. </span><span class="koboSpan" id="kobo.397.2">This demonstrates the versatility and effectiveness of CNN-based models beyond their traditional application in computer vision.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.398.1">CNNs have their limitations. </span><span class="koboSpan" id="kobo.398.2">CNNs lack interpretability due to their complex architecture, behaving like black boxes. </span><span class="koboSpan" id="kobo.398.3">This makes them unsuitable when model explainability is critical. </span><span class="koboSpan" id="kobo.398.4">In addition, CNNs require large training datasets to properly learn features and avoid overfitting. </span><span class="koboSpan" id="kobo.398.5">Their performance suffers significantly on smaller datasets.</span></p>
<h3 class="heading-3" id="_idParaDest-93"><span class="koboSpan" id="kobo.399.1">ResNet</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.400.1">As computer vision </span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.401.1">tasks grow in complexity, the addition of </span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.402.1">more layers in CNNs enhances their capability for image classification by enabling the learning of increasingly intricate features. </span><span class="koboSpan" id="kobo.402.2">However, as the number of layers increases in a CNN architecture, performance may deteriorate. </span><span class="koboSpan" id="kobo.402.3">This is commonly referred to as the </span><strong class="keyWord"><span class="koboSpan" id="kobo.403.1">vanishing gradient</span></strong><span class="koboSpan" id="kobo.404.1"> problem, where</span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.405.1"> signals originating from the initial inputs, including crucial information, gradually diminish as they traverse through multiple layers of the CNN.</span></p>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.406.1">Residual networks</span></strong><span class="koboSpan" id="kobo.407.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.408.1">ResNet</span></strong><span class="koboSpan" id="kobo.409.1">) address the vanishing gradient problem by implementing a layer-skipping technique. </span><span class="koboSpan" id="kobo.409.2">Rather than processing signals sequentially through each layer, ResNet introduces skip connections that allow signals to bypass certain layers. </span><span class="koboSpan" id="kobo.409.3">This can be visualized as a highway with fewer exits, enabling the signals from earlier layers to be preserved and carried forward without loss. </span><span class="koboSpan" id="kobo.409.4">The ResNet architecture is depicted in the following figure.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.410.1"><img alt="Figure 3.10 – ResNet architecture " src="../Images/B20836_03_11.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.411.1">Figure 3.11: ResNet architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.412.1">ResNet </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.413.1">can be used for different computer vision tasks such as </span><em class="italic"><span class="koboSpan" id="kobo.414.1">image classification</span></em><span class="koboSpan" id="kobo.415.1">, </span><em class="italic"><span class="koboSpan" id="kobo.416.1">object detection</span></em><span class="koboSpan" id="kobo.417.1"> (detecting all objects in a picture), and producing</span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.418.1"> models with much higher accuracy than a vanilla CNN network. </span><span class="koboSpan" id="kobo.418.2">However, a potential disadvantage of ResNet is increased computational complexity due to the introduction of skip connections. </span><span class="koboSpan" id="kobo.418.3">The additional connections require more memory and computational resources, making training and inference more computationally expensive compared to shallower architectures.</span></p>
<h2 class="heading-2" id="_idParaDest-94"><span class="koboSpan" id="kobo.419.1">Algorithms for natural language processing (NLP) problems</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.420.1">NLP </span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.421.1">focuses</span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.422.1"> on the relationship between computers and human language. </span><span class="koboSpan" id="kobo.422.2">It involves the processing and analysis of extensive amounts of natural language data with the objective of enabling computers to comprehend the meaning behind human language and extract valuable information from it. </span><span class="koboSpan" id="kobo.422.3">NLP encompasses a wide range of tasks within the field of data science. </span><span class="koboSpan" id="kobo.422.4">Some of these tasks include document classification, topic modeling, converting speech to text, generating speech from text, extracting entities from text, language translation, understanding and answering questions, reading comprehension, and language generation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.423.1">ML algorithms cannot process raw text data directly. </span><span class="koboSpan" id="kobo.423.2">To train NLP models effectively, it is necessary to convert the words within an input text into numerical representations within the context of other words, sentences, or documents. </span><span class="koboSpan" id="kobo.423.3">Before the advancement of embedding, there were two widely used methods for representing the relevance of words in a text: </span><strong class="keyWord"><span class="koboSpan" id="kobo.424.1">bag-of-words</span></strong><span class="koboSpan" id="kobo.425.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.426.1">BOW</span></strong><span class="koboSpan" id="kobo.427.1">) and term </span><strong class="keyWord"><span class="koboSpan" id="kobo.428.1">frequency–inverse document frequency</span></strong><span class="koboSpan" id="kobo.429.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.430.1">TF-IDF</span></strong><span class="koboSpan" id="kobo.431.1">).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.432.1">BOW is </span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.433.1">simply the count of a word appearing in a text (document). </span><span class="koboSpan" id="kobo.433.2">For example, if the input documents are </span><code class="inlineCode"><span class="koboSpan" id="kobo.434.1">I need to go to the bank to make a deposit</span></code><span class="koboSpan" id="kobo.435.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.436.1">I am taking a walk along the river bank</span></code><span class="koboSpan" id="kobo.437.1">, and you count the number of appearances for each unique word in each input document, you will get </span><em class="italic"><span class="koboSpan" id="kobo.438.1">1</span></em><span class="koboSpan" id="kobo.439.1"> for the word </span><em class="italic"><span class="koboSpan" id="kobo.440.1">I</span></em><span class="koboSpan" id="kobo.441.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.442.1">3</span></em><span class="koboSpan" id="kobo.443.1"> for the word </span><em class="italic"><span class="koboSpan" id="kobo.444.1">to</span></em><span class="koboSpan" id="kobo.445.1"> in the first document, as an example. </span><span class="koboSpan" id="kobo.445.2">If we have a vocabulary for all the unique words in the two documents, the vector representation for the first document can be </span><code class="inlineCode"><span class="koboSpan" id="kobo.446.1">[1 1 3 1 1 1 1 1 1 0 0 0 0 0]</span></code><span class="koboSpan" id="kobo.447.1">, where each position represents a unique word in the vocabulary (for example, the first position represents the word </span><em class="italic"><span class="koboSpan" id="kobo.448.1">I</span></em><span class="koboSpan" id="kobo.449.1">, and the third position represents the word </span><em class="italic"><span class="koboSpan" id="kobo.450.1">to</span></em><span class="koboSpan" id="kobo.451.1">). </span><span class="koboSpan" id="kobo.451.2">Now, this vector can be fed into an ML algorithm to train a model such as text classification. </span><span class="koboSpan" id="kobo.451.3">The main idea behind BOW is that a word that appears more frequently has stronger weights in a text.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.452.1">TF-IDF has </span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.453.1">two components. </span><span class="koboSpan" id="kobo.453.2">The first component, </span><em class="italic"><span class="koboSpan" id="kobo.454.1">TF</span></em><span class="koboSpan" id="kobo.455.1">, is the ratio of the number of times a vocabulary word appears in a document over the total number of words in the document. </span><span class="koboSpan" id="kobo.455.2">Using the preceding first document, the word </span><em class="italic"><span class="koboSpan" id="kobo.456.1">I</span></em><span class="koboSpan" id="kobo.457.1"> would have a TF value of </span><em class="italic"><span class="koboSpan" id="kobo.458.1">1/11</span></em><span class="koboSpan" id="kobo.459.1"> for the first sentence, and the word </span><em class="italic"><span class="koboSpan" id="kobo.460.1">walk</span></em><span class="koboSpan" id="kobo.461.1"> would have a TF value of </span><em class="italic"><span class="koboSpan" id="kobo.462.1">0/11</span></em><span class="koboSpan" id="kobo.463.1">, since </span><em class="italic"><span class="koboSpan" id="kobo.464.1">walk</span></em><span class="koboSpan" id="kobo.465.1"> does not appear in the first sentence. </span><span class="koboSpan" id="kobo.465.2">While TF measures the importance of a word in the context of one text, the IDF component measures the importance of a word across all the documents. </span><span class="koboSpan" id="kobo.465.3">Mathematically, it is the log of the ratio of the number of documents over the number of documents where a word appears. </span><span class="koboSpan" id="kobo.465.4">The final value of TF-IDF for a word would be the </span><em class="italic"><span class="koboSpan" id="kobo.466.1">TF</span></em><span class="koboSpan" id="kobo.467.1"> term multiplied by the </span><em class="italic"><span class="koboSpan" id="kobo.468.1">IDF</span></em><span class="koboSpan" id="kobo.469.1"> term. </span><span class="koboSpan" id="kobo.469.2">In general, TF-IDF works better than BOW.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.470.1">Although BOW and TF-IDF are useful for NLP tasks, they lack the ability to capture the semantic meaning of words and often result in large and sparse input vectors. </span><span class="koboSpan" id="kobo.470.2">This is where the concept of embedding plays a crucial role.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.471.1">Embedding</span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.472.1"> is a technique used to generate low-dimensional</span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.473.1"> representations (mathematical vectors) for words or sentences, which capture the semantic meaning of the text. </span><span class="koboSpan" id="kobo.473.2">The underlying idea is that words or sentences with similar semantic meanings tend to occur in similar contexts. </span><span class="koboSpan" id="kobo.473.3">In a multi-dimensional space, the mathematical representations of semantically similar entities are closer to each other than those with different meanings. </span><span class="koboSpan" id="kobo.473.4">For instance, if we consider sports-related words like soccer, tennis, and bike, their embeddings would be close to each other in the high-dimensional embedding space, measured by metrics like cosine similarity, which measures how similar two vectors are by calculating the cosine of the angle between them. </span><span class="koboSpan" id="kobo.473.5">The embedding vector represents the intrinsic meaning of the word, with each dimension </span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.474.1">representing a specific attribute associated with the word. </span><span class="koboSpan" id="kobo.474.2">Visualizing embeddings in a multidimensional space shows the proximity of related entities. </span><span class="koboSpan" id="kobo.474.3">The </span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.475.1">following diagram provides a visual depiction of the closeness in this multidimensional space:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.476.1"><img alt="Figure 3.11 – Embedding representation " src="../Images/B20836_03_12.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.477.1">Figure 3.12: Embedding representation</span></p>
<p class="normal"><span class="koboSpan" id="kobo.478.1">Nowadays, embeddings have become a crucial component for achieving good results in most NLP tasks. </span><span class="koboSpan" id="kobo.478.2">Compared to other techniques like simple word counts, embeddings offer more meaningful representations of the underlying text. </span><span class="koboSpan" id="kobo.478.3">This has led to their widespread adoption in various ML algorithms designed for NLP. </span><span class="koboSpan" id="kobo.478.4">In this section, we will delve into several of these algorithms such as BERT and GPT, exploring their specific applications and benefits in the context of NLP tasks.</span></p>
<h3 class="heading-3" id="_idParaDest-95"><span class="koboSpan" id="kobo.479.1">Word2Vec</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.480.1">Thomas </span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.481.1">Mikolov created </span><strong class="keyWord"><span class="koboSpan" id="kobo.482.1">Word2Vec</span></strong><span class="koboSpan" id="kobo.483.1"> in 2013. </span><span class="koboSpan" id="kobo.483.2">It supports two different techniques for learning </span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.484.1">embedding: </span><strong class="keyWord"><span class="koboSpan" id="kobo.485.1">continuous bag-of-words</span></strong><span class="koboSpan" id="kobo.486.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.487.1">CBOW</span></strong><span class="koboSpan" id="kobo.488.1">) and </span><strong class="keyWord"><span class="koboSpan" id="kobo.489.1">continuous-skip-gram</span></strong><span class="koboSpan" id="kobo.490.1">. </span><span class="koboSpan" id="kobo.490.2">CBOW </span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.491.1">tries to predict a word for a given window of surrounding words, and continuous-skip-gram tries to predict surrounding words for a given word. </span><span class="koboSpan" id="kobo.491.2">The training dataset for Word2Vec could be any running text available, such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.492.1">Wikipedia</span></strong><span class="koboSpan" id="kobo.493.1">. </span><span class="koboSpan" id="kobo.493.2">The process of generating a training dataset for CBOW is to run a sliding window across running text (for example, a window of five words) and choose one of the words as the target and the rest as inputs (the order of words is not considered). </span><span class="koboSpan" id="kobo.493.3">In the</span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.494.1"> case of </span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.495.1">continuous-skip-gram, the target and inputs are reversed. </span><span class="koboSpan" id="kobo.495.2">With the training dataset, the problem can be turned into a multi-class classification problem, where the model will learn to predict the classes (for example, words in the vocabulary) for the target word and assign each predicted word with a probability distribution.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.496.1">Word2Vec</span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.497.1"> embeddings can be trained using a straightforward one-hidden-layer MLP network. </span><span class="koboSpan" id="kobo.497.2">In this approach, the input to the MLP network is a matrix that represents the neighboring words, while the output is a probability distribution for the target words. </span><span class="koboSpan" id="kobo.497.3">During training, the weights of the hidden layer are optimized, and once the training process is complete, these weights serve as the actual embeddings for the words. </span><span class="koboSpan" id="kobo.497.4">The resulting embeddings capture the semantic relationships and contextual meanings of the words, enabling them to be effectively utilized in various natural language processing tasks.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.498.1">As large-scale word embedding training can be expensive and time-consuming, Word2Vec embeddings are usually trained as a pre-training task so that they can be readily used for downstream tasks such as text classification or entity extraction. </span><span class="koboSpan" id="kobo.498.2">This approach of using embeddings as features for downstream tasks is called a </span><strong class="keyWord"><span class="koboSpan" id="kobo.499.1">feature-based application</span></strong><span class="koboSpan" id="kobo.500.1">. </span><span class="koboSpan" id="kobo.500.2">There</span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.501.1"> are pre-trained embeddings (for example, Tomas Mikolov’ </span><em class="italic"><span class="koboSpan" id="kobo.502.1">Word2Vec</span></em><span class="koboSpan" id="kobo.503.1"> and Stanford’s </span><em class="italic"><span class="koboSpan" id="kobo.504.1">GloVe</span></em><span class="koboSpan" id="kobo.505.1">) in the public domain that can be used directly. </span><span class="koboSpan" id="kobo.505.2">The embeddings are a </span><em class="italic"><span class="koboSpan" id="kobo.506.1">1:1</span></em><span class="koboSpan" id="kobo.507.1"> mapping between each word and its vector representation.</span></p>
<h3 class="heading-3" id="_idParaDest-96"><span class="koboSpan" id="kobo.508.1">BERT</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.509.1">Word2Vec </span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.510.1">produces a fixed embedding representation for each word in the vocabulary, disregarding the contextual variations in meaning. </span><span class="koboSpan" id="kobo.510.2">However, words can have different meanings depending on the specific context in which they are used. </span><span class="koboSpan" id="kobo.510.3">For instance, the word “bank” can refer to a financial institution or the land alongside a body of water. </span><span class="koboSpan" id="kobo.510.4">To address this issue, contextualized word embeddings have been developed. </span><span class="koboSpan" id="kobo.510.5">These embeddings take into account the surrounding words or the overall context in which a word appears, allowing for a more nuanced and context-aware representation. </span><span class="koboSpan" id="kobo.510.6">By considering context, these embeddings capture the diverse meanings a word can have, enabling more accurate and context-specific analyses in downstream tasks.</span></p>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.511.1">BERT</span></strong><span class="koboSpan" id="kobo.512.1">, which stands for </span><strong class="keyWord"><span class="koboSpan" id="kobo.513.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.514.1">, is a language model that takes context into consideration by the following:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.515.1">Predicting randomly masked words in sentences (the context) and taking the order of words into consideration. </span><span class="koboSpan" id="kobo.515.2">This </span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.516.1">is also known as </span><strong class="keyWord"><span class="koboSpan" id="kobo.517.1">language modeling</span></strong><span class="koboSpan" id="kobo.518.1">.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.519.1">Predicting the next sentence from a given sentence.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.520.1">Released </span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.521.1">in 2018, this context-aware embedding approach provides better representation for words and can significantly improve language tasks such as </span><em class="italic"><span class="koboSpan" id="kobo.522.1">reading comprehension</span></em><span class="koboSpan" id="kobo.523.1">, </span><em class="italic"><span class="koboSpan" id="kobo.524.1">sentiment analysis</span></em><span class="koboSpan" id="kobo.525.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.526.1">named entity recognition</span></em><span class="koboSpan" id="kobo.527.1">. </span><span class="koboSpan" id="kobo.527.2">Additionally, BERT generates embeddings at subword levels (a segment between a word and a character, for example, the word </span><em class="italic"><span class="koboSpan" id="kobo.528.1">embeddings</span></em><span class="koboSpan" id="kobo.529.1"> is broken up into </span><em class="italic"><span class="koboSpan" id="kobo.530.1">em</span></em><span class="koboSpan" id="kobo.531.1">, </span><em class="italic"><span class="koboSpan" id="kobo.532.1">bed</span></em><span class="koboSpan" id="kobo.533.1">, </span><em class="italic"><span class="koboSpan" id="kobo.534.1">ding</span></em><span class="koboSpan" id="kobo.535.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.536.1">s</span></em><span class="koboSpan" id="kobo.537.1">). </span><span class="koboSpan" id="kobo.537.2">This allows it to handle </span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.538.1">the </span><strong class="keyWord"><span class="koboSpan" id="kobo.539.1">out-of-vocabulary</span></strong><span class="koboSpan" id="kobo.540.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.541.1">OOV</span></strong><span class="koboSpan" id="kobo.542.1">) issue, another limitation of Word2Vec, which only generates embeddings on known words and will treat OOV words simply as unknown.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.543.1">To obtain word embeddings with BERT, the process differs from the straightforward word-to-vector mapping used in Word2Vec. </span><span class="koboSpan" id="kobo.543.2">Instead, sentences are inputted into a pre-trained BERT model, and embeddings are extracted dynamically. </span><span class="koboSpan" id="kobo.543.3">This approach generates embeddings that are contextualized within the context of the given sentences. </span><span class="koboSpan" id="kobo.543.4">Besides word-level embeddings, BERT is also capable of producing embeddings for </span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.544.1">entire sentences. </span><strong class="keyWord"><span class="koboSpan" id="kobo.545.1">Pre-training</span></strong><span class="koboSpan" id="kobo.546.1"> is the term used to describe the process of learning embeddings using input tokens, and the following diagram illustrates the components involved in a BERT model for this purpose.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.547.1"><img alt="Figure 3.13 – BERT model pre-training " src="../Images/B20836_03_13.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.548.1">Figure 3.13: BERT model pre-training</span></p>
<p class="normal"><span class="koboSpan" id="kobo.549.1">Architecturally, BERT mainly uses a building block called a </span><strong class="keyWord"><span class="koboSpan" id="kobo.550.1">transformer</span></strong><span class="koboSpan" id="kobo.551.1">. </span><span class="koboSpan" id="kobo.551.2">A transformer</span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.552.1"> has a stack of encoders and a stack of decoders inside it, and it transforms one sequence of inputs into another sequence. </span><span class="koboSpan" id="kobo.552.2">Each encoder has two components:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.553.1">A self-attention layer mainly calculates the strength of the connection between one token (represented as a vector) and all other tokens in the input sentence, and this connection helps with the encoding of each token. </span><span class="koboSpan" id="kobo.553.2">One way to think about self-attention is which words in a sentence are more connected than other words in a sentence. </span><span class="koboSpan" id="kobo.553.3">For example, if the input sentence is </span><em class="italic"><span class="koboSpan" id="kobo.554.1">The dog crossed a busy street</span></em><span class="koboSpan" id="kobo.555.1">, then we would say the words </span><em class="italic"><span class="koboSpan" id="kobo.556.1">dog</span></em><span class="koboSpan" id="kobo.557.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.558.1">crossed</span></em><span class="koboSpan" id="kobo.559.1"> have stronger connections with the word </span><em class="italic"><span class="koboSpan" id="kobo.560.1">The</span></em><span class="koboSpan" id="kobo.561.1"> than the word </span><em class="italic"><span class="koboSpan" id="kobo.562.1">a</span></em><span class="koboSpan" id="kobo.563.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.564.1">busy</span></em><span class="koboSpan" id="kobo.565.1">, which would have strong connections with the word </span><em class="italic"><span class="koboSpan" id="kobo.566.1">street</span></em><span class="koboSpan" id="kobo.567.1">. </span><span class="koboSpan" id="kobo.567.2">The output of the self-attention layer is a sequence of vectors; each vector represents the original input token as well as the importance it has with other words in the inputs.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.568.1">A feed-forward network layer (single hidden layer MLP) extracts higher-level representation from the output of the self-attention layer.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.569.1">Inside the decoder, there is also a self-attention layer and feed-forward layer, plus an extra encoder-decoder layer that helps the decoder to focus on the right places in the inputs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.570.1">In the case of BERT, only the encoder part of the transformer is used. </span><span class="koboSpan" id="kobo.570.2">BERT can be used for a number of NLP tasks, including </span><em class="italic"><span class="koboSpan" id="kobo.571.1">question answering</span></em><span class="koboSpan" id="kobo.572.1">, </span><em class="italic"><span class="koboSpan" id="kobo.573.1">text classification</span></em><span class="koboSpan" id="kobo.574.1">, </span><em class="italic"><span class="koboSpan" id="kobo.575.1">named entity extraction</span></em><span class="koboSpan" id="kobo.576.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.577.1">text summarization</span></em><span class="koboSpan" id="kobo.578.1">. </span><span class="koboSpan" id="kobo.578.2">It achieved state-of-the-art performance in many of the tasks when it was released. </span><span class="koboSpan" id="kobo.578.3">BERT pre-training has also been adopted for different domains, such as scientific text and biomedical text, to understand domain-specific languages. </span><span class="koboSpan" id="kobo.578.4">The following figure showcases how a pre-trained BERT model is used to train a model</span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.579.1"> for a question-answering task using the fine-tuning technique:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.580.1"><img alt="Figure 3.14 – BERT fine-tuning " src="../Images/B20836_03_14.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.581.1">Figure 3.14: BERT fine-tuning</span></p>
<p class="normal"><span class="koboSpan" id="kobo.582.1">While BERT’s pre-trained embeddings can be extracted for downstream tasks such as text classification and question answering, a more straightforward way to use its pre-trained embeddings is through a technique called </span><strong class="keyWord"><span class="koboSpan" id="kobo.583.1">fine-tuning</span></strong><span class="koboSpan" id="kobo.584.1">. </span><span class="koboSpan" id="kobo.584.2">With fine-tuning, an additional output layer is added to the BERT network to perform a specific task, such as question answering or entity extraction. </span><span class="koboSpan" id="kobo.584.3">During fine-tuning, the pre-trained model is loaded, and you plug in the task-specific input (for example, question/passage pairs in question answering) and output (start/end and span for the answers in the passage) to fine-tune a task-specific model. </span><span class="koboSpan" id="kobo.584.4">With fine-tuning, the pre-trained model weights are updated.</span></p>
<h2 class="heading-2" id="_idParaDest-97"><span class="koboSpan" id="kobo.585.1">Generative AI algorithms</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.586.1">Although </span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.587.1">technologies like ChatGPT have popularized Generative AI, the concept of generative models is not new. </span><strong class="keyWord"><span class="koboSpan" id="kobo.588.1">Generative Adversarial Networks</span></strong><span class="koboSpan" id="kobo.589.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.590.1">GANs</span></strong><span class="koboSpan" id="kobo.591.1">), a </span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.592.1">prominent example of generative AI technology, have been around for years and have been successfully</span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.593.1"> applied in various real-world domains, with image synthesis being a notable application. </span><span class="koboSpan" id="kobo.593.2">Generative AI has become one of the most transformative AI technologies, and I have devoted the entire </span><em class="italic"><span class="koboSpan" id="kobo.594.1">Chapter 15 and 16</span></em><span class="koboSpan" id="kobo.595.1"> to delve deeper into real-world generative AI use cases, practical technology solutions, and ethical considerations. </span><span class="koboSpan" id="kobo.595.2">In this chapter, we will familiarize ourselves with several generative AI algorithms.</span></p>
<h3 class="heading-3" id="_idParaDest-98"><span class="koboSpan" id="kobo.596.1">Generative adversarial network</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.597.1">The GAN is a </span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.598.1">type of generative model designed to generate realistic data instances, such as images. </span><span class="koboSpan" id="kobo.598.2">It employs a two-part network consisting of a Generator and a Discriminator. </span><span class="koboSpan" id="kobo.598.3">The Generator network is responsible for generating instances, while the Discriminator network learns to distinguish between real and fake instances generated by the Generator. </span><span class="koboSpan" id="kobo.598.4">This adversarial setup encourages the Generator to continually improve its ability to produce increasingly realistic data instances.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.599.1"><img alt="Figure 3.16 – Generative adversarial network  " src="../Images/B20836_03_15.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.600.1">Figure 3.15: GAN</span></p>
<p class="normal"><span class="koboSpan" id="kobo.601.1">During the training process, the </span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.602.1">Discriminator network in a GAN is exposed to two different data sources: one from a real dataset, which serves as positive examples, and one from the</span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.603.1"> Generator network, which generates synthetic or fake samples. </span><span class="koboSpan" id="kobo.603.2">The Discriminator is trained to classify and distinguish between the real and fake samples, optimizing its loss to accurately predict the source of each sample. </span><span class="koboSpan" id="kobo.603.3">Conversely, the Generator network is trained to produce synthetic data that appears indistinguishable from real data, aiming to deceive the </span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.604.1">Discriminator. </span><span class="koboSpan" id="kobo.604.2">The Generator is penalized when the Discriminator correctly identifies its generated data as fake. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.605.1">Both networks learn and update their parameters using backpropagation, enabling them to improve iteratively. </span><span class="koboSpan" id="kobo.605.2">During the generation phase, the Generator is provided with random inputs to produce new synthetic samples. </span><span class="koboSpan" id="kobo.605.3">Throughout training, the Generator and Discriminator networks are alternately trained in a connected manner, allowing them to learn and optimize their performance as a unified system.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.606.1">GANs have had a lot of success in generating realistic images that can fool humans. </span><span class="koboSpan" id="kobo.606.2">They can be applied to many applications, such as translating sketches to realistic-looking images, converting text inputs and generating images that correspond to the text, and generating realistic human faces. </span><span class="koboSpan" id="kobo.606.3">However, getting GANs to converge and stabilize during training can be difficult, leading to issues like failure to learn. </span><span class="koboSpan" id="kobo.606.4">Additionally, newer technologies have emerged in realistic image generation, which are a lot more capable than GANs.</span></p>
<h3 class="heading-3" id="_idParaDest-99"><span class="koboSpan" id="kobo.607.1">Generative pre-trained transformer (GPT)</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.608.1">Unlike </span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.609.1">BERT, which requires fine-tuning using a large domain-specific dataset for the different downstream </span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.610.1">NLP tasks, the </span><strong class="keyWord"><span class="koboSpan" id="kobo.611.1">Generative Pre-trained Transformer</span></strong><span class="koboSpan" id="kobo.612.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.613.1">GPT</span></strong><span class="koboSpan" id="kobo.614.1">), developed by </span><strong class="keyWord"><span class="koboSpan" id="kobo.615.1">OpenAI</span></strong><span class="koboSpan" id="kobo.616.1">, can learn how to perform a </span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.617.1">task with just seeing a few examples (or no example). </span><span class="koboSpan" id="kobo.617.2">This learning process is called </span><strong class="keyWord"><span class="koboSpan" id="kobo.618.1">few-shot learning</span></strong><span class="koboSpan" id="kobo.619.1"> or </span><strong class="keyWord"><span class="koboSpan" id="kobo.620.1">zero-shot learning</span></strong><span class="koboSpan" id="kobo.621.1">. </span><span class="koboSpan" id="kobo.621.2">In a few-shot scenario, the </span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.622.1">GPT model is provided with a few examples, a task description, and a prompt, and the model will use these inputs and start to generate output tokens one by one. </span><span class="koboSpan" id="kobo.622.2">For instance, when using GPT-3 for translation, the task description could be “translate English to Chinese,” and the training data would consist of a few examples of Chinese sentences translated from English sentences. </span><span class="koboSpan" id="kobo.622.3">To translate a new English sentence using the trained model, you provide the English sentence as a prompt, and the model generates the corresponding translated text in Chinese. </span><span class="koboSpan" id="kobo.622.4">It’s important to note that few-shot or zero-shot learning does not involve updating the model’s parameter weights, unlike the fine-tuning technique.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.623.1">GPT, like </span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.624.1">BERT, utilizes the Transformer architecture as its primary component and employs a training approach called next word prediction. </span><span class="koboSpan" id="kobo.624.2">This entails predicting the word that should follow a given sequence of input words. </span><span class="koboSpan" id="kobo.624.3">However, GPT diverges from BERT in that it exclusively employs the Transformer decoder block, whereas BERT employs the Transformer encoder block. </span><span class="koboSpan" id="kobo.624.4">Like BERT, GPT incorporates masked words to learn embeddings. </span><span class="koboSpan" id="kobo.624.5">However, unlike BERT, which randomly masks words and predicts the missing ones, GPT restricts the self-attention calculation to exclude words to the right of the target word. </span><span class="koboSpan" id="kobo.624.6">This approach is referred to as masked self-attention.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.625.1">GPT and its chatbot interface, ChatGPT, have showcased exceptional capabilities in numerous traditional NLP tasks like language modeling, language translation, and question answering. </span><span class="koboSpan" id="kobo.625.2">Additionally, they have proven their efficacy in pioneering domains such as generating programming code or ML code, composing website content, and answering questions. </span><span class="koboSpan" id="kobo.625.3">Consequently, GPT has paved the way for a novel AI paradigm known as Generative AI.</span></p>
<h3 class="heading-3" id="_idParaDest-100"><span class="koboSpan" id="kobo.626.1">Large Language Model</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.627.1">Large Language Models</span></strong><span class="koboSpan" id="kobo.628.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.629.1">LLMs</span></strong><span class="koboSpan" id="kobo.630.1">) are</span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.631.1"> a class of generative AI model that is capable of generating text, translating languages, producing creative content, and providing informative answers to questions. </span><span class="koboSpan" id="kobo.631.2">LLMs</span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.632.1"> are trained on extensive datasets comprising text and code, with billions of model parameters, allowing them to understand and learn the statistical patterns and relationships between words and phrases. </span><span class="koboSpan" id="kobo.632.2">For example, GPT-3, an LLM, has 175 billion parameters after training. </span><span class="koboSpan" id="kobo.632.3">This training enables LLMs to effectively process and generate human-like text across a wide range of applications. </span><span class="koboSpan" id="kobo.632.4">While GPTs serve as a notable example of LLMs, the open-source community and other companies have developed additional LLMs in recent years, mainly using similar Transformer-based architecture. </span><span class="koboSpan" id="kobo.632.5">LLMs are also called foundation models. </span><span class="koboSpan" id="kobo.632.6">Unlike traditional ML models, which are trained for specific tasks, foundation models are pre-trained on massive datasets and can handle multiple tasks. </span><span class="koboSpan" id="kobo.632.7">In addition, foundation models can be fine-tuned and adapted for additional tasks. </span><span class="koboSpan" id="kobo.632.8">The remarkable capabilities and adaptability of foundation models have found many exciting AI use cases that were not easily solvable before. </span><span class="koboSpan" id="kobo.632.9">Now, let’s briefly </span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.633.1">review a few other popular foundation models:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.634.1">Google’s Pathways Language Model</span></strong><span class="koboSpan" id="kobo.635.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.636.1">PaLM</span></strong><span class="koboSpan" id="kobo.637.1">) is a 540-billion-parameter, decoder-only </span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.638.1">Transformer model. </span><span class="koboSpan" id="kobo.638.2">It </span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.639.1">offers similar capabilities as GPT, including text generation, translation, code generation, question answering, summarization, and support for creating chatbots. </span><span class="koboSpan" id="kobo.639.2">PaLM is trained using a new architecture called Pathways. </span><span class="koboSpan" id="kobo.639.3">Pathways is a modular architecture, meaning that it is composed of modules, each is responsible for a specific task.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.640.1">Meta’s Large Language Model Meta AI</span></strong><span class="koboSpan" id="kobo.641.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.642.1">LLaMA</span></strong><span class="koboSpan" id="kobo.643.1">) is an LLM available in multiple</span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.644.1"> sizes</span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.645.1"> from 7 billion parameters to 65 billion parameters. </span><span class="koboSpan" id="kobo.645.2">While it is a smaller model compared to GPT and PaLM, it offers several advantages such as requiring fewer computational resources. </span><span class="koboSpan" id="kobo.645.3">LLaMA offers similar capabilities as other LLMs such as generating creative text, answering questions, and solving mathematical problems. </span><span class="koboSpan" id="kobo.645.4">Meta has issued a noncommercial license for LLaMA, emphasizing its usage in research contexts. </span><span class="koboSpan" id="kobo.645.5">LLaMA has also been found to perform extremely well when it is fine-tuned with additional training data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.646.1">Big Science BLOOM</span></strong><span class="koboSpan" id="kobo.647.1"> is a 176-billion-parameter LLM that can generate text in 46 </span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.648.1">different languages and 13 programming languages. </span><span class="koboSpan" id="kobo.648.2">The development of BLOOM involved the</span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.649.1"> collaborative efforts of more than 1,000 researchers from over 70 countries and 250 institutions. </span><span class="koboSpan" id="kobo.649.2">As part of the Responsible AI License, individuals and institutions who agree to its terms can use and build upon the model on their local machines or cloud platforms. </span><span class="koboSpan" id="kobo.649.3">The model is easily accessible within the Hugging Face ecosystem.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.650.1">Bloomberg BloombergGPT</span></strong><span class="koboSpan" id="kobo.651.1"> – While general-purpose LLMs like GPT and LLaMA can</span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.652.1"> perform well in a </span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.653.1">range of tasks across different domain domains, domains such as financial services and life science require domain-specific LLMs to solve tough domain-focused problems. </span><span class="koboSpan" id="kobo.653.2">BloombergGPT is an example of domain-focused LLMs that are specifically trained for industries. </span><span class="koboSpan" id="kobo.653.3">BloombergGPT represents a significant advancement in applying this technology to </span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.654.1">finance. </span><span class="koboSpan" id="kobo.654.2">The model will enhance existing financial NLP tasks such as sentiment analysis, named entity recognition, news classification, and </span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.655.1">question answering, among others. </span><span class="koboSpan" id="kobo.655.2">Drawing from its extensive collection and curation resources, Bloomberg utilized its four-decade archive of financial language documents, resulting in a comprehensive 363-billion-token dataset comprising English financial documents. </span><span class="koboSpan" id="kobo.655.3">This dataset was augmented with a 345-billion-token public dataset, yielding a training corpus with over 700 billion tokens.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.656.1">While these LLM </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.657.1">models have exhibited remarkable capabilities, they also come with significant limitations, including generating misinformation (hallucinations) and toxic content, and displaying potential bias. </span><span class="koboSpan" id="kobo.657.2">LLMs also consume significantly more resources to train and run. </span><span class="koboSpan" id="kobo.657.3">It is worth noting that while LLMs can help solve some new problems, many of the common problems (e.g., name entity extraction, document classification, sentiment analysis) have been solved with existing NLP techniques, which remain highly viable options for those tasks.</span></p>
<h3 class="heading-3" id="_idParaDest-101"><span class="koboSpan" id="kobo.658.1">Diffusion model</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.659.1">Recently, AI’s</span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.660.1"> remarkable ability to generate </span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.661.1">high-resolution, photorealistic images and never-seen-before generative art or manipulate images with precision has garnered significant attention. </span><span class="koboSpan" id="kobo.661.2">Behind all these amazing capabilities is a new type of deep learning model called the diffusion model. </span><span class="koboSpan" id="kobo.661.3">Building upon the foundations of deep learning and GANs, the diffusion model introduces a novel approach to generating high-quality, realistic data instances.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.662.1">Unlike GANs, which try to generate realistic fake images by fooling a discriminator network, the diffusion model works by first adding noises to the input data (e.g., images) incrementally over many steps until the input data is unrecognizable, a process called diffusion steps. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.663.1">The model is then trained to reverse the diffusion steps from noise to the original data. </span><span class="koboSpan" id="kobo.663.2">In more technical terms, the training process of a diffusion model involves optimizing a set of learnable parameters through backpropagation. </span><span class="koboSpan" id="kobo.663.3">The model learns to generate realistic samples by maximizing the likelihood of the training data given a sequence of diffusion steps. </span><span class="koboSpan" id="kobo.663.4">This iterative process allows the model to capture complex dependencies, intricate patterns, and structures, and generate highly realistic and diverse data instances. </span><span class="koboSpan" id="kobo.663.5">The following figure illustrates this process:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.664.1"><img alt="" role="presentation" src="../Images/B20836_03_16.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.665.1">Figure 3.16: How a diffusion model works</span></p>
<p class="normal"><span class="koboSpan" id="kobo.666.1">Furthermore, the </span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.667.1">diffusion model offers flexibility and controllability in the generation process. </span><span class="koboSpan" id="kobo.667.2">By adjusting the diffusion steps, one can </span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.668.1">control the trade-off between sample quality and diversity. </span><span class="koboSpan" id="kobo.668.2">This allows users to fine-tune the model to suit their specific needs, whether it’s emphasizing fidelity to the training data or encouraging more creative and novel outputs. </span><span class="koboSpan" id="kobo.668.3">Compared to GANs, diffusion models can generate more realistic images and are more stable than GANs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.669.1">The diffusion model has shown great promise in various domains, including computer vision, natural language processing, and audio synthesis. </span><span class="koboSpan" id="kobo.669.2">Its ability to generate high-quality data with fine-grained details has opened up exciting possibilities for applications such as image generation, video prediction, text generation, and more.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.670.1">The open-source community and private companies have developed many models based on this diffusion approach. </span><span class="koboSpan" id="kobo.670.2">Two of the more popular models worth mentioning are Stable Diffusion and DALL-E 2:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.671.1">DALL-E 2 by OpenAI</span></strong><span class="koboSpan" id="kobo.672.1">: DALL-E 2 is a </span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.673.1">text-to-image model </span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.674.1">developed by OpenAI. </span><span class="koboSpan" id="kobo.674.2">DALL-E 2 was trained using a dataset of images and text descriptions. </span><span class="koboSpan" id="kobo.674.3">First released in January 2022, DALL-E 2 has </span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.675.1">shown remarkable capabilities in generating and manipulating images from text descriptions. </span><span class="koboSpan" id="kobo.675.2">It has also been applied for inpainting (modifying regions in images), outpainting (extending an image), and image-to-image translation. </span><span class="koboSpan" id="kobo.675.3">The</span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.676.1"> images generated by DALL-E 2 are often indistinguishable from real images, and they can be used for a variety of purposes, such as creating art and generating marketing materials. </span><span class="koboSpan" id="kobo.676.2">From a model-training perspective, DALL-E 2 training comprises two key steps:</span><ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.677.1">Linking textual semantics and visual representations</span></strong><span class="koboSpan" id="kobo.678.1">: This step involves learning how a piece of text, such as “a man wearing a hat” is semantically linked to an actual image of “a man wearing a hat”. </span><span class="koboSpan" id="kobo.678.2">To do this, DALL-E 2 uses a model called </span><strong class="keyWord"><span class="koboSpan" id="kobo.679.1">Contrastive Language-Image Pre-training</span></strong><span class="koboSpan" id="kobo.680.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.681.1">CLIP</span></strong><span class="koboSpan" id="kobo.682.1">). </span><span class="koboSpan" id="kobo.682.2">CLIP</span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.683.1"> is trained with hundreds of millions of images and their associated descriptions. </span><span class="koboSpan" id="kobo.683.2">After it is trained, it can output a text-conditioned visual encoding given a piece of text description. </span><span class="koboSpan" id="kobo.683.3">You can learn more about CLIP at </span><a href="https://openai.com/research/clip"><span class="url"><span class="koboSpan" id="kobo.684.1">https://openai.com/research/clip</span></span></a><span class="koboSpan" id="kobo.685.1">.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.686.1">Generating images from visual embeddings</span></strong><span class="koboSpan" id="kobo.687.1">: This step learns to reverse the image from the visual embeddings generated by the CLIP. </span><span class="koboSpan" id="kobo.687.2">For this step, DALL-E 2 uses a model called GLIDE, which is based on the diffusion model. </span><span class="koboSpan" id="kobo.687.3">You can learn more about GLIDE at </span><a href="https://arxiv.org/abs/2112.10741"><span class="url"><span class="koboSpan" id="kobo.688.1">https://arxiv.org/abs/2112.10741</span></span></a><span class="koboSpan" id="kobo.689.1">.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.690.1">After the model is trained, DALL-E 2 can generate new images closely related to an input text description.</span></p> </li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.691.1">Stable Diffusion by Stability AI</span></strong><span class="koboSpan" id="kobo.692.1">: Stable Diffusion</span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.693.1"> is an</span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.694.1"> algorithm developed by Compvis (the Computer Vision research group at Ludwig Maximilian University of Munich) and sponsored primarily by Stability AI. </span><span class="koboSpan" id="kobo.694.2">The model is also a text-to-image model trained using a dataset of real images and text descriptions, which allows the model to generate realistic images using text descriptions. </span><span class="koboSpan" id="kobo.694.3">First released in August 2022, Stable Diffusion has been shown to be effective at generating high-quality images from text descriptions. </span><span class="koboSpan" id="kobo.694.4">Architecturally, it employs a CLIP encoder to condition the model on text descriptions, and it uses UNET as the denoising neural network to generate images from visual encodings. </span><span class="koboSpan" id="kobo.694.5">It is an open-source model with the code and model weights released to the public. </span><span class="koboSpan" id="kobo.694.6">You can get more details on Stable Diffusion</span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.695.1"> at </span><a href="https://github.com/CompVis/stable-diffusion"><span class="url"><span class="koboSpan" id="kobo.696.1">https://github.com/CompVis/stable-diffusion</span></span></a><span class="koboSpan" id="kobo.697.1">.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.698.1">As powerful as the diffusion models are, they do come with some concerns, including copyright infringement and the creation of harmful images.</span></p>
<h1 class="heading-1" id="_idParaDest-102"><span class="koboSpan" id="kobo.699.1">Hands-on exercise</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.700.1">In this</span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.701.1"> hands-on exercise, we will build a </span><strong class="keyWord"><span class="koboSpan" id="kobo.702.1">Jupyter</span></strong> <strong class="keyWord"><span class="koboSpan" id="kobo.703.1">Notebook</span></strong><span class="koboSpan" id="kobo.704.1"> environment</span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.705.1"> on your local machine and build and train an ML model in your local environment. </span><span class="koboSpan" id="kobo.705.2">The goal of the exercise is to get some familiarity with the installation process of setting up a local data science environment, and then learn how to analyze the data, prepare the data, and train an ML model using one of the algorithms we covered in the preceding sections. </span><span class="koboSpan" id="kobo.705.3">First, let’s take a look at the problem statement. </span><span class="koboSpan" id="kobo.705.4">The following diagram illustrates the flow:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.706.1"><img alt="" role="presentation" src="../Images/B20836_03_17.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.707.1">Figure 3.17: ML problem-solving flow</span></p>
<p class="normal"><span class="koboSpan" id="kobo.708.1">Let’s get started.</span></p>
<h2 class="heading-2" id="_idParaDest-103"><span class="koboSpan" id="kobo.709.1">Problem statement</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.710.1">Before we start, let’s first review the business problem that we need to solve. </span><span class="koboSpan" id="kobo.710.2">A retail bank has been experiencing a high customer churn rate for its retail banking business. </span><span class="koboSpan" id="kobo.710.3">To proactively implement preventive measures to reduce potential churn, the bank needs to know who the potential churners are, so the bank can target those customers with incentives directly to prevent them from leaving. </span><span class="koboSpan" id="kobo.710.4">From a business perspective, it is far more expensive to acquire a new customer than offering incentives to keep an existing customer.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.711.1">As an ML solutions architect, you have been tasked to run some quick experiments to validate the ML approach for this problem. </span><span class="koboSpan" id="kobo.711.2">There is no ML tooling available, so you have decided to set up a Jupyter environment on your local machine for this task.</span></p>
<h2 class="heading-2" id="_idParaDest-104"><span class="koboSpan" id="kobo.712.1">Dataset description</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.713.1">You will use a dataset from the Kaggle site for bank customers’ churn for modeling. </span><span class="koboSpan" id="kobo.713.2">You can access the dataset at </span><a href="https://www.kaggle.com/mathchi/churn-for-bank-customers"><span class="url"><span class="koboSpan" id="kobo.714.1">https://www.kaggle.com/mathchi/churn-for-bank-customers</span></span></a><span class="koboSpan" id="kobo.715.1">. </span><span class="koboSpan" id="kobo.715.2">Note that you will need Kaggle account to download the file. </span><span class="koboSpan" id="kobo.715.3">The dataset contains 14 columns for features such as credit score, gender, and balance, and a target variable column, </span><code class="inlineCode"><span class="koboSpan" id="kobo.716.1">Exited</span></code><span class="koboSpan" id="kobo.717.1">, to indicate whether a customer churned or not. </span><span class="koboSpan" id="kobo.717.2">We will review those features in more detail in later sections.</span></p>
<h2 class="heading-2" id="_idParaDest-105"><span class="koboSpan" id="kobo.718.1">Setting up a Jupyter Notebook environment</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.719.1">Now, let’s set </span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.720.1">up a local data science environment for data analysis and experimentation. </span><span class="koboSpan" id="kobo.720.2">We will be using the popular Jupyter Notebook on your local computer. </span><span class="koboSpan" id="kobo.720.3">Setting up a Jupyter Notebook environment on a local machine </span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.721.1">consists of the following key components:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.722.1">Python</span></strong><span class="koboSpan" id="kobo.723.1">: Python is a</span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.724.1"> general-purpose programming language, and it is one of the most popular programming languages for data science work. </span><span class="koboSpan" id="kobo.724.2">The installation instructions can be found at </span><a href="https://www.python.org/downloads"><span class="url"><span class="koboSpan" id="kobo.725.1">https://www.python.org/downloads</span></span></a><span class="koboSpan" id="kobo.726.1">.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.727.1">PIP</span></strong><span class="koboSpan" id="kobo.728.1">: PIP is a</span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.729.1"> Python package installer used for installing different Python library packages, such as ML algorithms, data manipulation libraries, or visualization. </span><span class="koboSpan" id="kobo.729.2">The installation instructions can be found at </span><a href="https://pip.pypa.io/en/stable/installation/"><span class="url"><span class="koboSpan" id="kobo.730.1">https://pip.pypa.io/en/stable/installation/</span></span></a><span class="koboSpan" id="kobo.731.1">.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.732.1">Jupyter Notebook</span></strong><span class="koboSpan" id="kobo.733.1">: Jupyter Notebook</span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.734.1"> is a web application designed for authoring documents (called notebooks) that contain code, description, and/or visualizations. </span><span class="koboSpan" id="kobo.734.2">It is one of the most popular tools used by data scientists for experimentation and modeling. </span><span class="koboSpan" id="kobo.734.3">The installation instructions can be found at </span><a href="https://jupyter.org/install"><span class="url"><span class="koboSpan" id="kobo.735.1">https://jupyter.org/install</span></span></a><span class="koboSpan" id="kobo.736.1">.</span></li>
</ul>
<h2 class="heading-2" id="_idParaDest-106"><span class="koboSpan" id="kobo.737.1">Running the exercise</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.738.1">Follow along with these steps to run the lab:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.739.1">With your environment configured, let’s get started with the actual data science work. </span><span class="koboSpan" id="kobo.739.2">First, download the data files:</span><ol class="alphabeticList" style="list-style-type: lower-alpha;">
<li class="alphabeticList" value="1"><span class="koboSpan" id="kobo.740.1">Let’s create a folder called </span><code class="inlineCode"><span class="koboSpan" id="kobo.741.1">MLSALab</span></code><span class="koboSpan" id="kobo.742.1"> on your local machine to store all the files. </span><span class="koboSpan" id="kobo.742.2">You can create the folder anywhere on your local machine as long as you can get to it. </span><span class="koboSpan" id="kobo.742.3">I have a Mac, so I created one directly inside the default user’s </span><code class="inlineCode"><span class="koboSpan" id="kobo.743.1">Documents</span></code><span class="koboSpan" id="kobo.744.1"> folder.</span></li>
<li class="alphabeticList"><span class="koboSpan" id="kobo.745.1">Create another subfolder called </span><code class="inlineCode"><span class="koboSpan" id="kobo.746.1">Lab1-bankchurn</span></code><span class="koboSpan" id="kobo.747.1"> under the </span><code class="inlineCode"><span class="koboSpan" id="kobo.748.1">MLSALab</span></code><span class="koboSpan" id="kobo.749.1"> folder.</span></li>
<li class="alphabeticList"><span class="koboSpan" id="kobo.750.1">Visit the </span><a href="https://www.kaggle.com/mathchi/churn-for-bank-customers"><span class="url"><span class="koboSpan" id="kobo.751.1">https://www.kaggle.com/mathchi/churn-for-bank-customers</span></span></a><span class="koboSpan" id="kobo.752.1"> site and download the data file (an archive file) and save it in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.753.1">MSSALab/Lab1-bankchurn</span></code><span class="koboSpan" id="kobo.754.1"> folder. </span><span class="koboSpan" id="kobo.754.2">Create a Kaggle account if you do not already have one. </span><span class="koboSpan" id="kobo.754.3">Extract the archive file inside the folder, and you will see a file called </span><code class="inlineCode"><span class="koboSpan" id="kobo.755.1">churn.csv</span></code><span class="koboSpan" id="kobo.756.1">. </span><span class="koboSpan" id="kobo.756.2">You can now delete the archive file.</span></li>
</ol>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.757.1">Launch Jupyter Notebook:</span><ol class="alphabeticList" style="list-style-type: lower-alpha;">
<li class="alphabeticList" value="1"><span class="koboSpan" id="kobo.758.1">Inside</span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.759.1"> the Terminal window (or the Command Prompt window for Windows systems), navigate to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.760.1">MLSALab</span></code><span class="koboSpan" id="kobo.761.1"> folder and run the following command to start the Jupyter Notebook server on your machine:
            </span><pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.762.1">jupyter notebook
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.763.1">A browser window will open up and display the Jupyter Notebook environment (see the following screenshot). </span><span class="koboSpan" id="kobo.763.2">Detailed instructions on how Jupyter Notebook works are out of scope for this lab. </span><span class="koboSpan" id="kobo.763.3">If you are not familiar with how Jupyter Notebook works, you can easily find information on the internet:</span></p>
<p class="normal"><span class="koboSpan" id="kobo.764.1"><img alt="Figure 3.17 – Jupyter Notebook " src="../Images/B20836_03_18.png"/></span></p>
</li>
</ol>
<figure class="mediaobject"><span class="koboSpan" id="kobo.765.1">Figure 3.18: Jupyter Notebook</span></figure>
<ol class="alphabeticList" style="list-style-type: lower-alpha;">
<li class="alphabeticList" value="2"><span class="koboSpan" id="kobo.766.1">Click on the </span><code class="inlineCode"><span class="koboSpan" id="kobo.767.1">Lab1-bankchurn</span></code><span class="koboSpan" id="kobo.768.1"> folder and you will see the </span><code class="inlineCode"><span class="koboSpan" id="kobo.769.1">churn.csv</span></code><span class="koboSpan" id="kobo.770.1"> file.</span></li>
</ol> </li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3"><span class="koboSpan" id="kobo.771.1">Now, let’s create a new data science notebook inside the Jupyter Notebook environment. </span><span class="koboSpan" id="kobo.771.2">To do this, click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.772.1">New</span></strong><span class="koboSpan" id="kobo.773.1"> dropdown and select </span><strong class="screenText"><span class="koboSpan" id="kobo.774.1">Python 3</span></strong><span class="koboSpan" id="kobo.775.1"> (see the following screenshot):
    </span><figure class="mediaobject"><span class="koboSpan" id="kobo.776.1"><img alt="Figure 3.18 – Creating a new Jupyter notebook " src="../Images/B20836_03_19.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.777.1">Figure 3.19: Creating a new Jupyter notebook</span></p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="4"><span class="koboSpan" id="kobo.778.1">You will see</span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.779.1"> a screen similar to the following screenshot. </span><span class="koboSpan" id="kobo.779.2">This is an empty notebook that we will use to explore data and build models. </span><span class="koboSpan" id="kobo.779.3">The section next to </span><strong class="screenText"><span class="koboSpan" id="kobo.780.1">In [ ]:</span></strong><span class="koboSpan" id="kobo.781.1"> is called a </span><strong class="screenText"><span class="koboSpan" id="kobo.782.1">cell</span></strong><span class="koboSpan" id="kobo.783.1">, and we will enter our code into the cell. </span><span class="koboSpan" id="kobo.783.2">To run the code in the cell, you click on the </span><strong class="screenText"><span class="koboSpan" id="kobo.784.1">Run</span></strong><span class="koboSpan" id="kobo.785.1"> button on the toolbar. </span><span class="koboSpan" id="kobo.785.2">To add a new cell, you click on the </span><strong class="keyWord"><span class="koboSpan" id="kobo.786.1">+</span></strong><span class="koboSpan" id="kobo.787.1"> button on the toolbar:
    </span><figure class="mediaobject"><span class="koboSpan" id="kobo.788.1"><img alt="Figure 3.19 – Empty Jupyter notebook " src="../Images/B20836_03_20.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.789.1">Figure 3.20: Empty Jupyter notebook</span></p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="5"><span class="koboSpan" id="kobo.790.1">Add a new cell by clicking on the </span><strong class="keyWord"><span class="koboSpan" id="kobo.791.1">+</span></strong><span class="koboSpan" id="kobo.792.1"> button in the toolbar, enter the following code block inside the first empty cell, and run the cell by clicking on the </span><strong class="screenText"><span class="koboSpan" id="kobo.793.1">Run</span></strong><span class="koboSpan" id="kobo.794.1"> button in the toolbar. </span><span class="koboSpan" id="kobo.794.2">This code block downloads a number of Python packages for data manipulation (</span><code class="inlineCode"><span class="koboSpan" id="kobo.795.1">pandas</span></code><span class="koboSpan" id="kobo.796.1">), visualization (</span><code class="inlineCode"><span class="koboSpan" id="kobo.797.1">matplotlib</span></code><span class="koboSpan" id="kobo.798.1">), and model training and evaluation (</span><code class="inlineCode"><span class="koboSpan" id="kobo.799.1">scikit-learn</span></code><span class="koboSpan" id="kobo.800.1">). </span><span class="koboSpan" id="kobo.800.2">We will cover scikit-learn in greater detail in </span><em class="chapterRef"><span class="koboSpan" id="kobo.801.1">Chapter 5</span></em><span class="koboSpan" id="kobo.802.1">, </span><em class="italic"><span class="koboSpan" id="kobo.803.1">Exploring Open-Source ML Libraries</span></em><span class="koboSpan" id="kobo.804.1">. </span><span class="koboSpan" id="kobo.804.2">We will use these packages in the following sections:
        </span><pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.805.1">! </span><span class="koboSpan" id="kobo.805.2">pip3 install pandas
! </span><span class="koboSpan" id="kobo.805.3">pip3 install matplotlib
! </span><span class="koboSpan" id="kobo.805.4">pip3 install scikit-learn
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.806.1">Now, we can load and explore the data. </span><span class="koboSpan" id="kobo.806.2">Add the following code block in a new cell to load the Python library packages and load the data from the </span><code class="inlineCode"><span class="koboSpan" id="kobo.807.1">churn.csv</span></code><span class="koboSpan" id="kobo.808.1"> file. </span><span class="koboSpan" id="kobo.808.2">You will see a table with 14 columns, where the </span><code class="inlineCode"><span class="koboSpan" id="kobo.809.1">Exited</span></code><span class="koboSpan" id="kobo.810.1"> column is the target column:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.811.1">import</span></span><span class="koboSpan" id="kobo.812.1"> pandas </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.813.1">as</span></span><span class="koboSpan" id="kobo.814.1"> pd
churn_data = pd.read_csv(</span><span class="hljs-string"><span class="koboSpan" id="kobo.815.1">"churn.csv"</span></span><span class="koboSpan" id="kobo.816.1">)
churn_data.head()
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.817.1">You can </span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.818.1">explore the dataset using a number of tools to understand information with the commands that follow, such as </span><em class="italic"><span class="koboSpan" id="kobo.819.1">dataset statistics</span></em><span class="koboSpan" id="kobo.820.1">, the </span><em class="italic"><span class="koboSpan" id="kobo.821.1">pairwise correlation between different features</span></em><span class="koboSpan" id="kobo.822.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.823.1">data distributions</span></em><span class="koboSpan" id="kobo.824.1">. </span><span class="koboSpan" id="kobo.824.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.825.1">describe()</span></code><span class="koboSpan" id="kobo.826.1"> function returns basic statistics about the data such as mean, standard deviation, min, and max, for each numerical column. 
    </span><p class="normal"><span class="koboSpan" id="kobo.827.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.828.1">hist()</span></code><span class="koboSpan" id="kobo.829.1"> function plots the histogram for the selected columns, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.830.1">corr()</span></code><span class="koboSpan" id="kobo.831.1"> calculates the correlation matrix between the different features in the data. </span><span class="koboSpan" id="kobo.831.2">Try them out one at a time in a new cell to understand the data:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.832.1"># The following command calculates the various statistics for the features.</span></span><span class="koboSpan" id="kobo.833.1">
churn_data.describe()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.834.1"># The following command displays the histograms for the different features.  </span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.835.1"># You can replace the column names to plot the histograms for other features</span></span><span class="koboSpan" id="kobo.836.1">
churn_data.hist([</span><span class="hljs-string"><span class="koboSpan" id="kobo.837.1">'CreditScore'</span></span><span class="koboSpan" id="kobo.838.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.839.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.840.1">Age'</span></span><span class="koboSpan" id="kobo.841.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.842.1">'Balance'</span></span><span class="koboSpan" id="kobo.843.1">])
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.844.1"># The following command calculate the correlations among features</span></span><span class="koboSpan" id="kobo.845.1">
churn_data.corr()
</span></code></pre></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="8"><span class="koboSpan" id="kobo.846.1">The dataset needs transformations in order to be used for model training. </span><span class="koboSpan" id="kobo.846.2">The following code block will convert the </span><code class="inlineCode"><span class="koboSpan" id="kobo.847.1">Geography</span></code><span class="koboSpan" id="kobo.848.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.849.1">Gender</span></code><span class="koboSpan" id="kobo.850.1"> values from categorical strings to ordinal numbers so they can be taken by the ML algorithm later. </span><span class="koboSpan" id="kobo.850.2">Please note that model accuracy is not the main purpose of this exercise, and we are performing ordinal transformation for demonstration purposes. </span><span class="koboSpan" id="kobo.850.3">We will be using a popular Python ML library called sklearn for this exercise. </span><span class="koboSpan" id="kobo.850.4">Sklearn is also one of the easiest libraries to use and understand, especially for beginners. </span><span class="koboSpan" id="kobo.850.5">We will also discuss this library in</span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.851.1"> more detail in </span><em class="italic"><span class="koboSpan" id="kobo.852.1">Chapter 5, Exploring Open-Source ML Libraries</span></em><span class="koboSpan" id="kobo.853.1">. </span><span class="koboSpan" id="kobo.853.2">Copy and run the following code block in a new cell:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.854.1">from</span></span><span class="koboSpan" id="kobo.855.1"> sklearn.preprocessing </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.856.1">import</span></span><span class="koboSpan" id="kobo.857.1"> OrdinalEncoder
encoder_1 = OrdinalEncoder()
encoder_2 = OrdinalEncoder()
churn_data[</span><span class="hljs-string"><span class="koboSpan" id="kobo.858.1">'Geography_code'</span></span><span class="koboSpan" id="kobo.859.1">] = encoder_1.fit_transform(
  churn_data[[</span><span class="hljs-string"><span class="koboSpan" id="kobo.860.1">'Geography'</span></span><span class="koboSpan" id="kobo.861.1">]]
)
churn_data[</span><span class="hljs-string"><span class="koboSpan" id="kobo.862.1">'Gender_code'</span></span><span class="koboSpan" id="kobo.863.1">] = encoder_2.fit_transform(
  churn_data[[</span><span class="hljs-string"><span class="koboSpan" id="kobo.864.1">'Gender'</span></span><span class="koboSpan" id="kobo.865.1">]]
)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.866.1">Often, there could be some columns not needed for model training, as they do not contribute to model predictive power or could cause bias from an inclusion perspective. </span><span class="koboSpan" id="kobo.866.2">We can drop them using the following code block:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.867.1">churn_data.drop(columns = [</span><span class="hljs-string"><span class="koboSpan" id="kobo.868.1">'Geography'</span></span><span class="koboSpan" id="kobo.869.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.870.1">'Gender'</span></span><span class="koboSpan" id="kobo.871.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.872.1">'RowNumber'</span></span><span class="koboSpan" id="kobo.873.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.874.1">'Surname'</span></span><span class="koboSpan" id="kobo.875.1">], inplace=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.876.1">True</span></span><span class="koboSpan" id="kobo.877.1">)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.878.1">Now, the dataset has only the features we care about. </span><span class="koboSpan" id="kobo.878.2">Next, we need to split the data for training and validation. </span><span class="koboSpan" id="kobo.878.3">We also prepare each dataset by splitting the target variable, </span><code class="inlineCode"><span class="koboSpan" id="kobo.879.1">Exited</span></code><span class="koboSpan" id="kobo.880.1">, from the rest of the input features. </span><span class="koboSpan" id="kobo.880.2">Enter and run the following code block in a new cell:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.881.1"># we import the train_test_split class for data split</span></span>
<span class="hljs-key ord"><span class="koboSpan" id="kobo.882.1">from</span></span><span class="koboSpan" id="kobo.883.1"> sk.model_selection </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.884.1">import</span></span><span class="koboSpan" id="kobo.885.1"> train_test_split
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.886.1"># Split the dataset into training (80%) and testing (20%).</span></span><span class="koboSpan" id="kobo.887.1">
churn_train, churn_test = train_test_split(
  churn_data, test_size=</span><span class="hljs-number"><span class="koboSpan" id="kobo.888.1">0.2</span></span><span class="koboSpan" id="kobo.889.1">
)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.890.1"># Split the features from the target variable "Exited" as it is required for model training</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.891.1"># and validation later.</span></span><span class="koboSpan" id="kobo.892.1">
churn_train_X = churn_train.loc[:, churn_train.columns != </span><span class="hljs-string"><span class="koboSpan" id="kobo.893.1">'Exited'</span></span><span class="koboSpan" id="kobo.894.1">]
churn_train_y = churn_train[</span><span class="hljs-string"><span class="koboSpan" id="kobo.895.1">'Exited'</span></span><span class="koboSpan" id="kobo.896.1">]
churn_test_X = churn_test.loc[:, churn_test.columns != </span><span class="hljs-string"><span class="koboSpan" id="kobo.897.1">'Exited'</span></span><span class="koboSpan" id="kobo.898.1">]
churn_test_y = churn_test[</span><span class="hljs-string"><span class="koboSpan" id="kobo.899.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.900.1">Exited'</span></span><span class="koboSpan" id="kobo.901.1">]
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.902.1">We are ready to train the model. </span><span class="koboSpan" id="kobo.902.2">Enter and run the following code block in a new cell. </span><span class="koboSpan" id="kobo.902.3">Here, we will use the random forest algorithm to train the model, and the </span><code class="inlineCode"><span class="koboSpan" id="kobo.903.1">fit()</span></code><span class="koboSpan" id="kobo.904.1"> function kicks off the model training:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.905.1"># We will use the Random Forest algorithm to train the model</span></span>
<span class="hljs-key ord"><span class="koboSpan" id="kobo.906.1">from</span></span><span class="koboSpan" id="kobo.907.1"> sklearn.ensemble </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.908.1">import</span></span><span class="koboSpan" id="kobo.909.1"> RandomForestClassifier
bank_churn_clf = RandomForestClassifier(
  max_depth=</span><span class="hljs-number"><span class="koboSpan" id="kobo.910.1">2</span></span><span class="koboSpan" id="kobo.911.1">, random_state=</span><span class="hljs-number"><span class="koboSpan" id="kobo.912.1">0</span></span><span class="koboSpan" id="kobo.913.1">
)
bank_churn_clf.fit(churn_train_X, churn_train_y)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.914.1">Finally, we</span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.915.1"> will test the accuracy of the model using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.916.1">test</span></code><span class="koboSpan" id="kobo.917.1"> dataset. </span><span class="koboSpan" id="kobo.917.2">Here, we get the predictions returned by the model using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.918.1">predict()</span></code><span class="koboSpan" id="kobo.919.1"> function, and then use the </span><code class="inlineCode"><span class="koboSpan" id="kobo.920.1">accuracy_score()</span></code><span class="koboSpan" id="kobo.921.1"> function to calculate the model accuracy using the predicted values (</span><code class="inlineCode"><span class="koboSpan" id="kobo.922.1">churn_prediction_y</span></code><span class="koboSpan" id="kobo.923.1">) and the true values (</span><code class="inlineCode"><span class="koboSpan" id="kobo.924.1">churn_test_y</span></code><span class="koboSpan" id="kobo.925.1">) for the test dataset:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.926.1"># We use the accuracy_score class of the sklearn library to calculate the accuracy.</span></span>
<span class="hljs-key ord"><span class="koboSpan" id="kobo.927.1">from</span></span><span class="koboSpan" id="kobo.928.1"> sklearn.metrics </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.929.1">import</span></span><span class="koboSpan" id="kobo.930.1"> accuracy_score
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.931.1"># We use the trained model to generate predictions using the test dataset</span></span><span class="koboSpan" id="kobo.932.1">
churn_prediction_y = bank_churn_clf.predict(churn_test_X)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.933.1"># We measure the accuracy using the accuracy_score class.</span></span><span class="koboSpan" id="kobo.934.1">
accuracy_score(churn_test_y, churn_prediction_y)
</span></code></pre>
</li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.935.1">Congratulations! </span><span class="koboSpan" id="kobo.935.2">You have successfully installed the Jupyter data science environment on your local machine and trained a model using the random forest algorithm. </span><span class="koboSpan" id="kobo.935.3">You have validated that an ML approach could potentially solve this business problem.</span></p>
<h1 class="heading-1" id="_idParaDest-107"><span class="koboSpan" id="kobo.936.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.937.1">In this chapter, we have explored various ML algorithms that can be applied to solve different types of ML problems. </span><span class="koboSpan" id="kobo.937.2">By now, you should have a good understanding of which algorithms are suitable for which specific tasks. </span><span class="koboSpan" id="kobo.937.3">Additionally, you have set up a basic data science environment on your local machine, utilized the scikit-learn ML libraries to analyze and preprocess data, and successfully trained an ML model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.938.1">In the upcoming chapter, our focus will shift to the intersection of data management and the ML lifecycle. </span><span class="koboSpan" id="kobo.938.2">We will delve into the significance of effective data management and discuss how to build a comprehensive data management platform on </span><strong class="keyWord"><span class="koboSpan" id="kobo.939.1">Amazon Web Services</span></strong><span class="koboSpan" id="kobo.940.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.941.1">AWS</span></strong><span class="koboSpan" id="kobo.942.1">) to support downstream ML tasks. </span><span class="koboSpan" id="kobo.942.2">This platform will provide the necessary infrastructure and tools to streamline data processing, storage, and retrieval, ultimately enhancing the overall ML workflow.</span></p>
<h1 class="heading-1"><span class="koboSpan" id="kobo.943.1">Join our community on Discord</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.944.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal"><a href="https://packt.link/mlsah "><span class="url"><span class="koboSpan" id="kobo.945.1">https://packt.link/mlsah</span></span></a></p>
<p class="normal"><span class="koboSpan" id="kobo.946.1"><img alt="" role="presentation" src="../Images/QR_Code7020572834663656.png"/></span></p>
</div>
</body></html>