["```py\nlet view = ScatterPlotView(frame: CGRect(x: 20, y: 20, width: 300, height: 300))\n\nPlaygroundPage.current.liveView = view\n```", "```py\nlet csvData = parseCSV(contents:loadCSV(file:\"SwedishAutoInsurance\"))\n\nlet dataPoints = extractDataPoints(data: csvData, xKey: \"claims\", yKey: \"payments\")\n\nview.scatter(dataPoints)\n```", "```py\nfunc train(\n    x:[CGFloat],\n    y:[CGFloat],\n    b:CGFloat=0.0,\n    w:CGFloat=0.0,\n    learningRate:CGFloat=0.00001,\n    epochs:Int=100,\n    trainingCallback: ((Int, Int, CGFloat, CGFloat) -> Void)? = nil) -> (b:CGFloat, w:CGFloat){\n\n    var B = b // bias\n    var W = w // weight\n\n    let N = CGFloat(x.count) // number of data points\n\n    for epoch in 0...epochs{\n        // TODO: create variable to store this epoch's gradient for b and w\n        for i in 0..<x.count{\n            // TODO: make a prediction (using the linear equation y = b + x * w\n            // TODO: calculate the absolute error (prediction - actual value)\n            // TODO: calculate the gradient with respect to the error and b (); adding it to the epochs bias gradient\n            // TODO: calculate the gradient with respect to the error and w (); adding it to the epochs weight gradient\n        }\n        // TODO: update the bias (B) using the learningRate\n        // TODO: update the weight (W) using the learningRate\n        if let trainingCallback = trainingCallback{\n            trainingCallback(epoch, epochs, W, B)\n        }\n    }\n\n    return (b:B, w:W)\n}   \n```", "```py\n// TODO: create variable to store this epoch's gradient for b and w\nvar bGradient : CGFloat = 0.0\nvar wGradient : CGFloat = 0.0\n```", "```py\n// TODO: make a prediction (using the linear equation y = b + x * w\nlet yHat = W * x[i] + B\n// TODO: calculate the absolute error (prediction - actual value)\nlet error = y[i] - yHat\n```", "```py\n// TODO: calculate the gradient with respect to the error and b (); adding it to the epochs bias gradient\nB = B - (learningRate * bGradient)\n\n// TODO: calculate the gradient with respect to the error and w (); adding it to the epochs weight gradient \nW = W - (learningRate * wGradient)\n```"]