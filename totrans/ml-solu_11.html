<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 11. Building Gaming Bot"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Building Gaming Bot</h1></div></div></div><p>In previous chapters, we covered applications that belong to the computer vision domain. In this chapter, we will be making a gaming bot. We will cover different approaches to build the gaming bot. These gaming bots can be used to play a variety of Atari games.</p><p>Let's do a quick recap of the past two years. Let's begin with 2015. A small London-based company called DeepMind published a research paper titled Playing Atari with Deep Reinforcement Learning, available at <a class="ulink" href="https://arxiv.org/abs/1312.5602">https://arxiv.org/abs/1312.5602</a> In this paper, they demonstrated how a computer can learn and play Atari 2600 video games. A computer can play the game just by observing the screen pixels. Our computer game agent (the computer game player) will receive rewards when the game score increases. The result presented in this paper is remarkable. The paper created a lot of buzz, and that was because each game has different scoring mechanisms and these games are designed in such a way that humans find it difficult to achieve the highest score. The beauty of this research paper is that we can use the concept and given model architecture without any changes to learn different games. This model architecture and algorithm are applied to seven games, and in three of them, the algorithm performed way better than a human! This is a big leap in the field of AI because the hope is that we can build a single algorithm that can master many tasks, as well as build a General Artificial Intelligence or Artificial General Intelligence (AGI) system at some point in the next few decades. You can read more about AGI at: <a class="ulink" href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">https://en.wikipedia.org/wiki/Artificial_general_intelligence</a>. We all know that DeepMind was immediately acquired by Google.</p><p>In 2017, Google DeepMind and OpenAI achieved a major milestone, which gives us hope that AGI will happen soon. Let's start with Google DeepMind first; you must have heard that Google DeepMind 's AlphaGo AI (a gaming bot) won a three-match series against the world's best Go player. Go is a complex game because it has a huge number of permutations and combinations for a single move. You can watch the video for this game by clicking on this YouTube video: <a class="ulink" href="https://www.youtube.com/watch?v=vFr3K2DORc8">https://www.youtube.com/watch?v=vFr3K2DORc8</a>. Now let's talk about OpenAI. If this is the first time you have heard about OpenAI, this is a short introduction. OpenAI is a non-profit AI research organization, cofounded by Elon Musk, which is trying to build AI that will be safe and ensure that the benefits of Artificial Intelligence (AI) systems are widely and evenly distributed as far as possible. In 2017, OpenAI's gaming bot beat the world's best Dota 2 players. You can watch this YouTube video for reference: <a class="ulink" href="https://www.youtube.com/watch?v=7U4-wvhgx0w">https://www.youtube.com/watch?v=7U4-wvhgx0w</a>. All this was achieved by AGI system environments created by tech giants. The goal of making an AGI system is that a single system can perform a variety of complex tasks. The ideal AGI system can help us solve lots of complex tasks in the fields of healthcare, agriculture, robotics, and so on without any changes to its algorithm. So, it is better for us if we can understand the basic concepts in order to develop the AGI system.</p><p>In this chapter, just to start with, we will be trying to make a gaming bot that can play simple Atari games. We will achieve this by using reinforcement learning.</p><p>In general, we will be covering the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing the problem statement</li><li class="listitem" style="list-style-type: disc">Setting up the coding environment</li><li class="listitem" style="list-style-type: disc">Understanding reinforcement learning (RL)</li><li class="listitem" style="list-style-type: disc">Basic Atari gaming bot for pathfinder<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the key concepts</li></ul></div></li><li class="listitem" style="list-style-type: disc">Implementing the basic version of the gaming bot</li><li class="listitem" style="list-style-type: disc">Building the Space Invaders gaming bot<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the key concepts</li></ul></div></li><li class="listitem" style="list-style-type: disc">Implementing the Space Invaders gaming bot</li><li class="listitem" style="list-style-type: disc">Building the Pong gaming bot<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the key concepts</li></ul></div></li><li class="listitem" style="list-style-type: disc">Implementing the Pong gaming bot</li><li class="listitem" style="list-style-type: disc">Just for fun - implementing the Flappy Bird gaming bot</li><li class="listitem" style="list-style-type: disc">Summary</li></ul></div><div class="section" title="Introducing the problem statement"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec114"/>Introducing the problem statement</h1></div></div></div><p>We know we are trying to develop a gaming bot: a program that can play simple Atari games. If we provide enough time and computation resources, then it can outperform humans who are experts at playing certain games. I will list down some famous Atari games so that you can see which types of games I'm talking about. You must have played one of these games for sure. Some of the famous Atari games are Casino, Space Invaders, Pac-man, Space War, Pong (ping-pong), and so on. In short, the problem statement that we are trying to solve is how can we build a bot that can learn to play Atari games?</p><p>In this chapter, we will be using already built-in gaming environments using <code class="literal">gym </code>and <code class="literal">dqn </code>libraries. So, we don't need to create a gaming visual environment and we can focus on the approach of making the best possible gaming bot. First, we need to set up the coding environment.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Setting up the coding environment"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec115"/>Setting up the coding environment</h1></div></div></div><p>In this <a id="id1279" class="indexterm"/>section, we will cover how to set up a coding environment that can help us implement our applications. We need to install the gym library. These are the steps that you can follow. I'm using <code class="literal">Ubuntu 16.04 LTS</code> as my operating system:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Step 1: Clone the gym repository from GitHub by executing this command: <code class="literal">$ sudo git clone</code><code class="literal"> https://github.com/openai/gym.gi</code><code class="literal">t</code></li><li class="listitem" style="list-style-type: disc">Step 2: Jump to the gym directory by executing this command: <code class="literal">$ cd gym</code></li><li class="listitem" style="list-style-type: disc">Step 3: Execute this command to install the minimum number of required libraries for <code class="literal">gym</code>:  <code class="literal">$ sudo pip install -e</code></li><li class="listitem" style="list-style-type: disc">Step 4: Install the gaming environment for Atari games by executing this command: <code class="literal">$ sudo pip install gym[atari]</code></li><li class="listitem" style="list-style-type: disc">Step 5: This step is optional. If you want to install all the gaming environments, then you can execute the following commands:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">$ sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev libsdl2-dev swig</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">$ sudo pip install gym[all]</code></li></ul></div></li></ul></div><p>This is how you can install the <code class="literal">gym </code>machine learning library, which we will be using to develop <a id="id1280" class="indexterm"/>the gaming bot. We will be using the TensorFlow<code class="literal"> </code>implementation of the <code class="literal">dqn </code>library, so there will be no need to install <code class="literal">dqn </code>separately, but you can definitely refer to this installation note: <a class="ulink" href="https://github.com/deepmind/dqn">https://github.com/deepmind/dqn</a>.</p><p>As we are ready with the environment setup, we need to move on to our next section, which will help us understand the techniques that will be useful in order to develop the gaming bots. So let's begin!</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding Reinforcement Learning (RL)"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec116"/>Understanding Reinforcement Learning (RL)</h1></div></div></div><p>In this chapter, we are making a gaming bot with the help of reinforcement learning techniques. The motivation behind reinforcement learning is simple. RL gives the machine <a id="id1281" class="indexterm"/>or any software agent a chance to learn its behavior based on the feedback this agent receives from the environment. This behavior can be learned once, or you can keep on adapting with time.</p><p>Let's understand RL with a fun example of a child learning to speak. These are the steps a child will take when they are learning how to speak:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Step 1: The first thing is that the child starts to observe you; how you are speaking and <a id="id1282" class="indexterm"/>how you are interacting with him or her. The child listens to the basic words and sentences from you and learns that they can make a similar sound too. So, the child tries to imitate you.</li><li class="listitem" style="list-style-type: disc">Step 2: The child wants to speak full sentences or words but they may not understand that even before speaking sentences, they need to learn simple words! This is a challenge that comes while they are trying to speak. Now the child attempts to make sounds, some sounds are funny or weird, but they are still determined to speak words and sentences.</li><li class="listitem" style="list-style-type: disc">Step 3: There is another challenge that the child faces, which is that they need to understand and remember the meaning behind the words they are trying to speak. But the child manages to overcome this challenge and learns to speak their first few words, which are very simple words, such as <span class="emphasis"><em>mama, papa, dadda, paa, maa</em></span>, and so on. They learn this task by constantly observing their surroundings.</li><li class="listitem" style="list-style-type: disc">Step 4: The real challenge begins with how to use a particular word, when to use which word, and remembering all the words they hear for the first time. Try to feed the meaning of all the words and the context in which the child needs to use them. Sounds like a challenging task, doesn't it?</li></ul></div><p>For a child, it is a difficult task, but once it starts understanding the language and practices the sentences, then it will become a part of the child's life. Within 2-3 years, the child could have enough practice to start interacting easily. If we think of ourselves speaking, it is an easy task for us because we have learned enough about how to interact within our environment.</p><p>Now, let's try to connect the dots. With the help of the preceding example, we will try to understand the concept of Reinforcement Learning. The problem statement of the given example is speaking, where the child is the agent who is trying to manipulate the environment (which word the child speaks first) by taking an action (here, the action is speaking), and they try to speak one word or the other. The child gets a reward—say, a chocolate—when they accomplish a submodule of the task, which means speaking some words <a id="id1283" class="indexterm"/>in a day, and will not receive any chocolate when they are not able to speak anything. This is a simplified description of reinforcement learning. You can refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_11_01.jpg" alt="Understanding Reinforcement Learning (RL)" width="644" height="597"/><div class="caption"><p>Figure 11.1: Pictorial representation of the basic concept of RL</p></div></div><p>So basically, RL allows machines and software agents to automatically determine the ideal and best possible behavior within a specific task or within a specific context in order to maximize the performance of software agents. Simple reward feedback is required for the agent to learn its behavior, and this is known as the reinforcement signal. Every time the software agent tries to take the kind of actions that lead it to gain maximum rewards. Eventually, it learns all the actions or moves that lead the agent to the optimum solution of the task so that it becomes the master of it. The algorithms for RL learn to react to an environment.</p><p>In order <a id="id1284" class="indexterm"/>to build the gaming bot, RL algorithms are the perfect choice, and there is a reason behind it. Suppose there are many slot machines with random payouts and you want to win the maximum amount of money. How do you win the maximum amount of money? One naive approach is to just select a single machine and pull its lever all day long, and it might give you some payouts. If you are lucky enough, then you may hit the jackpot. There are chances that in order to try this approach, you may lose some money. This approach is called a <span class="emphasis"><em>pure exploitation</em></span> <span class="emphasis"><em>approach</em></span>. It is not an optimal approach.</p><p>Let's take another approach. In this approach, we will pull the lever of every single slot machine and pray that at least one of them hits the jackpot. This too is a naive approach. In this approach, we need to keep pulling the lever all day long. This approach is called a <span class="emphasis"><em>pure exploration approach</em></span>. This approach is not optimal as well, so we need to find a proper balance between these two approaches in order to get maximum rewards. This is referred to as the exploration versus exploitation dilemma of RL. Now we need to solve this issue. Well, for that, we need a mathematical framework that can help us achieve the optimal solution, and that mathematical approach is <span class="emphasis"><em>Markov Decision Process (MDP)</em></span>. Let's explore this.</p><div class="section" title="Markov Decision Process (MDP)"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec188"/>Markov Decision Process (MDP)</h2></div></div></div><p>Markov <a id="id1285" class="indexterm"/>Decision Process <a id="id1286" class="indexterm"/>uses <a id="id1287" class="indexterm"/>the following parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Set of states, <span class="emphasis"><em>S</em></span></li><li class="listitem" style="list-style-type: disc">Set of actions,<span class="emphasis"><em> A</em></span></li><li class="listitem" style="list-style-type: disc">Reward function,<span class="emphasis"><em> R</em></span></li><li class="listitem" style="list-style-type: disc">Policy, <span class="emphasis"><em>π</em></span></li><li class="listitem" style="list-style-type: disc">Value, <span class="emphasis"><em>V</em></span></li></ul></div><p>In order to perform transition for one state to the end state <span class="emphasis"><em>(S)</em></span>, we have to take an action <span class="emphasis"><em>(A)</em></span> or a series of actions. We will get rewards<span class="emphasis"><em> (R)</em></span> for each action we take. Our action can provide us either a positive reward or a negative reward. The set of actions that we <a id="id1288" class="indexterm"/>take define our policy <span class="emphasis"><em>(π)</em></span>. The rewards that we get in return after performing each action define our value<span class="emphasis"><em> (V).</em></span> Our goal is to maximize the rewards by choosing the correct policy. We can do that by performing the best possible action. Mathematically, we can express this as shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_02.jpg" alt="Markov Decision Process (MDP)" width="95" height="34"/><div class="caption"><p>Figure 11.2: Mathematical representation of Markov Decision Process</p></div></div><p>We will be applying the preceding equation for all possible values of<span class="emphasis"><em> S</em></span> for a time t. We have a set of states and actions. We need to consider these states, actions, and rules for transitioning the agent from one state to another. When we perform an action that changes the state of the gaming agent, the the agent will get rewards for doing that. This entire process of state, action, and getting rewards makes up Markov Decision Process (MDP). One round of a game is considered <span class="emphasis"><em>one episode</em></span> of MDP. This process includes a finite sequence of states, actions, and rewards. Take a look at the following equation for a representation of the process: :</p><p>S0, a0, r1, s1, a1, r2, s2, a2, r3 ,…, sn-1, an-1, rn, sn</p><p>Here, s<sub>i</sub> represents the state, ai is the action, and r<sub>i+1</sub> is the reward that we will get after performing the action. sn indicates that a particular episode ends with a terminal state, and this happens when the <span class="strong"><strong>game over</strong></span> screen appears. A Markov Decision Process is based on the Markov assumption, the probability of the next state s<sub>i+1</sub> depends on the current <a id="id1289" class="indexterm"/>state s<sub>i</sub> and the performed action ai and <a id="id1290" class="indexterm"/>does not depend on the preceding states or actions.</p></div><div class="section" title="Discounted Future Reward"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec189"/>Discounted Future Reward</h2></div></div></div><p>In the <a id="id1291" class="indexterm"/>long term, if we want our gaming agent to do well, then we need to take into account the immediate rewards, but we also need to consider the future awards that our agent will get. How should we <a id="id1292" class="indexterm"/>approach this scenario? Well, the answer lies in the concept of discounted future rewards.</p><p>Given one run of MDP, we can calculate the <span class="emphasis"><em>total rewards</em></span> for one episode by using the following equation:</p><p>R = r1 + r2 + r3 + … + rn</p><p>Based on the preceding equation, we can calculate the <span class="emphasis"><em>total future rewards</em></span> from time stamp <span class="emphasis"><em>t</em></span> onward, and that can be expressed by the given equation:</p><p>Rt = rt + rt+1 + rt+2 + rt+3 + … + rn</p><p>Here, we are dealing with a gaming environment that is random, and we cannot be sure whether we will get the same rewards the next time we perform the same actions to play a specific game. The more you think about the future, the more it will get diverged. For that reason, it is better that we use <span class="emphasis"><em>discounted future rewards</em></span> instead of total rewards: </p><p>Rt  = rt+ γrt+1 + γ2rt+2+ … + γn-1 rn</p><p>Here, γ is the discount factor. Its value is between <span class="emphasis"><em>0 to 1</em></span>. It is easy to understand that the discounted future reward at particular time step t can be expressed with the help of the rewards of the current stare plus rewards at time step <span class="emphasis"><em>t+1</em></span>:</p><p>Rt = rt + γ (rt+1 + γ (rt+2 + …)) = rt + γRt+1</p><p>Now let me tell you what the practical meaning of tuning this discount factor is: if we set the value of the discount factor γ = 0, then our strategy of plying will be short-sighted and we take our gaming decision just based on the immediate rewards. We need to find a balance <a id="id1293" class="indexterm"/>between immediate rewards and future rewards, so we should set the value of the discount factor to something more than 0.7.</p><p>For example, we can set the value as γ = 0.9. If our gaming environment is deterministic and we <a id="id1294" class="indexterm"/>know that the same actions always lead us to the same reward, then we can set the value of the discount factor γ =1. A good strategy for a gaming agent would be to always <span class="emphasis"><em>choose an action that maximizes the discounted future reward.</em></span> </p><p>We have covered the basics of RL. From now onward, we will start implementing our gaming bot. So let's get ready for some fun!</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Basic Atari gaming bot"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec117"/>Basic Atari gaming bot</h1></div></div></div><p>In this <a id="id1295" class="indexterm"/>chapter, we are trying a hands-on approach to building some basic gaming bots. We are choosing some famous Atari games that nearly everybody has played at some point in their lives. We choose Atari games because we know how to play them, and that makes our life easy because we can understand what kind of action our bot should perform in order to get better over a period of time.</p><p>In this section, we are building our own game. This game is simple, so we can look at how we can apply the Q-Learning algorithms. Here, we will be designing the game world on our own. Let's begin!</p><div class="section" title="Understanding the key concepts"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec190"/>Understanding the key concepts</h2></div></div></div><p>In this <a id="id1296" class="indexterm"/>section, we will be looking at a lot of important aspects that will help us while coding, so here, we will be covering the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Rules for the game</li><li class="listitem" style="list-style-type: disc">Understanding the Q-Learning algorithm</li></ul></div><div class="section" title="Rules for the game"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec130"/>Rules for the game</h3></div></div></div><p>Before we <a id="id1297" class="indexterm"/>begin with the basic concepts or algorithms, we need to understand the rules of the game that we are building. The game is simple and easy to play. The rules for this game are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Rules of the game:</em></span> The gaming agent means a yellow box has to reach one of the goals to end the game: it can be either a green cell or a red cell. This means the yellow box should reach either the green cell or the red cell.</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Rewards:</em></span> Each step gives us a negative reward of - 0.04. If our gaming agent reaches the red cell, then the red cell gives us a negative reward of - 1. If our gaming agent reaches the green cell, then the green cell gives us a positive reward of +1.</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>States:</em></span> Each cell is a state for the agent that it takes to find its goal.</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Actions:</em></span> There are only four actions for this game: Up direction, Down direction, Right direction, Left direction.</li></ul></div><p>We need the <code class="literal">tkinter </code>library to implement this approach. I have already provided a description about how to install it at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/README.md">https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/README.md</a>.</p><p>Now let's look at the Q learning algorithm that we will use during this chapter to build the gaming bot.</p></div><div class="section" title="Understanding the Q-Learning algorithm"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec131"/>Understanding the Q-Learning algorithm</h3></div></div></div><p>This algorithm <a id="id1298" class="indexterm"/>was originally published by DeepMind in two papers. The first one was published with the title<span class="emphasis"><em> Playing Atari with Deep Reinforcement Learning </em></span>on NIPS 2013. The link for the paper is <a class="ulink" href="https://arxiv.org/pdf/1312.5602.pdf">https://arxiv.org/pdf/1312.5602.pdf</a>. The second one was published with the title <span class="emphasis"><em>Human-level control through deep reinforcement Learning</em></span> on Nature in 2015. The link for this paper is <a class="ulink" href="http://www.davidqiu.com:8888/research/nature14236.pdf">http://www.davidqiu.com:8888/research/nature14236.pdf</a>. You should definitely read these papers. I have simplified the main concepts of these papers for you.</p><p>In Q-learning, we need to define a <span class="emphasis"><em>Q (s, a)</em></span> function that represents the discount factor reward when we perform action<span class="emphasis"><em> a</em></span> in state <span class="emphasis"><em>s</em></span>, and it continues optimally from that point onward. You can see the equation that helps us choose the maximum reward in the followingscreenshot:</p><div class="mediaobject"><img src="Images/B08394_11_03.jpg" alt="Understanding the Q-Learning algorithm" width="165" height="34"/><div class="caption"><p>Figure 11.3: Equation for Q-function</p></div></div><p>We can think of the Q (s, a) function as giving us the best possible score at the end of the game <a id="id1299" class="indexterm"/>after performing action <span class="emphasis"><em>a</em></span> in the particular state <span class="emphasis"><em>s</em></span>. This function is the Q function because it indicates the <span class="emphasis"><em>quality</em></span> of a certain action in a certain given state.</p><p>Let me simplify this for you. Suppose you are in state <span class="emphasis"><em>s</em></span> and are thinking about whether you should perform action <span class="emphasis"><em>a</em></span> or <span class="emphasis"><em>b</em></span>. You really want to win the game with a high score. So, in order to achieve your goal, you want to select the action that gives you the highest score at the end of the game. If you have this Q-function with you, then the selection of actions become quite easy because you just need to pick the action that has the highest Q-value. You can see the equation that you can use to obtain the highest Q-value in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_04.jpg" alt="Understanding the Q-Learning algorithm" width="192" height="34"/><div class="caption"><p>Figure 11.4: Equation for choosing the maximum rewards using the Q-function</p></div></div><p>Here, π represents the policy. The policy indicates the rules of the game and the action. With the help of the policy, we can choose what kind of typical actions are available in each state. Our next step is to obtain this Q-function. For that, we need to concentrate on just one transition. This transition is made of four states: <span class="emphasis"><em>&lt; s, a, r, s' &gt;</em></span>. Remember the discount factor reward, where we can express the Q-value of the current state <span class="emphasis"><em>s</em></span> and the current action<span class="emphasis"><em> a</em></span> in terms of the Q-value of the next state <span class="emphasis"><em>s'</em></span>. The equation for calculating rewards is provided in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_05.jpg" alt="Understanding the Q-Learning algorithm" width="234" height="34"/><div class="caption"><p>Figure 11.5: The bellman equation for calculation rewards</p></div></div><p>The preceding equation is called the Bellman equation, and it is the main idea behind the Q-learning algorithm. This equation is quite logical, and it indicates that the maximum <a id="id1300" class="indexterm"/>future rewards for this state and action are the summation of the immediate rewards and the maximum future reward for the next state.</p><p>The main intuition is that with the help of the <span class="emphasis"><em>n number of iterations followed by approximation </em></span>step, we can generate the values for the Q-function. We will achieve this by using the <span class="emphasis"><em>Bellman equation</em></span>. In the simplest case, the Q-function is implemented in the form of a table where states are its rows and actions are its columns. The pseudo steps of this Q-learning algorithm are simple. You can take a look at them, as follows:  at them, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Step 1: Initialize Q [number of states, number of actions] arbitrarily</li><li class="listitem" style="list-style-type: disc">Step 2: Observe initial states</li><li class="listitem" style="list-style-type: disc">Step 3: Repeat<div class="informalexample"><pre class="programlisting">Select and perform an action a
Observe two things: reward r and new state s'
Q [s, a] = Q [s, a] + α (r + γmaxa' Q [s', a'] - Q [s, a])
s = s'</pre></div></li><li class="listitem" style="list-style-type: disc">Until terminated</li></ul></div><p>We need to follow these steps, where α is the learning rate. The learning rate verifies the difference between the previous Q-value and the newly proposed Q-value. This difference value is taken into account so that we can check when our model will converge. With the help of the learning rate, we can regulate the speed of training in such a way that our model won't become too slow to converge or too fast to converge in a way <a id="id1301" class="indexterm"/>that it cannot learn anything. We will be using <span class="emphasis"><em>maxa'Q [s', a']</em></span> to update <span class="emphasis"><em>Q [s, a]</em></span>In order to maximize the reward. This is the only operation that we need to perform. This estimation operation will give us the updated Q-value. In the early stages of training, when our agent is learning, there could be a situation where our estimations may go completely wrong, but the estimations and updated Q-values get more and more accurate with every iteration. If we perform this process enough times, then the Q-function will converge. It represents the true and optimized Q-value. For better understanding, we will implement the preceding algorithm. Refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_06.jpg" alt="Understanding the Q-Learning algorithm" width="793" height="631"/><div class="caption"><p>Figure 11.6: Code snippet for building and updating the Q-table</p></div></div><p>You can <a id="id1302" class="indexterm"/>see the output in the form of a Q-table in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_07.jpg" alt="Understanding the Q-Learning algorithm" width="638" height="417"/><div class="caption"><p>Figure 11.7: Q-table value</p></div></div><p>You can <a id="id1303" class="indexterm"/>see the implementation of the preceding algorithm by referring to this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/Demo_Q_table.ipynb">https://github.com/jalajthanaki/Q_learning_for_simple_atari_game/blob/master/Demo_Q_table.ipynb</a>.</p><p>Now let's start implementing the game.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Implementing the basic version of the gaming bot"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec118"/>Implementing the basic version of the gaming bot</h1></div></div></div><p>In this section, we will be implementing a simple game. I have already defined the rules of this game. Just to remind you quickly, our agent, yellow block tries to reach either the red <a id="id1304" class="indexterm"/>block or the green block. If the agent reaches the green block, we will receive + 1 as a reward. If it reaches the red block, we get -1. Each step the agent will take will be considered a - 0.04 reward. You can turn back the pages and refer to the section Rules for the game if you want. You can refer to the code for this basic version of a gaming bot by referring to this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Q_learning_for_simple_atari_game">https://github.com/jalajthanaki/Q_learning_for_simple_atari_game</a>.</p><p>For this game, the gaming world or the gaming environment is already built, so we do not need to worry about it. We need to include this gaming world by just using the import statement. The main script that we are running is <code class="literal">Lerner.py</code>. The code snippet for this code is given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_08.jpg" alt="Implementing the basic version of the gaming bot" width="488" height="823"/><div class="caption"><p>Figure 11.8: Code snippet for the basic version of the gaming bot - I</p></div></div><p>As you can see in the preceding code, we are keeping track of the agent's states and actions <a id="id1305" class="indexterm"/>with the help of the code given in loops. After that, we will define the four possible actions for this game, and based on that, we will calculate the reward values. We have also defined the <code class="literal">max_Q</code> function, which calculates the maximum Q value for us. You can also refer to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_09.jpg" alt="Implementing the basic version of the gaming bot" width="496" height="698"/><div class="caption"><p>Figure 11.9: Code snippet for basic version of gaming bot - II</p></div></div><p>As you can see in the preceding code snippet, the helper function uses the <code class="literal">inc_Q</code> method <a id="id1306" class="indexterm"/>in order to update Q. By using the <code class="literal">run </code>function, we can update Q values so that our bot will learn how to achieve the best solution. You can run this script by executing this command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python Learner.py</strong></span>
</pre></div><p>When you run the script, you can see the following output window, and within 1-2 minutes, this bot will find the optimal solution. You can find the bot's initial state and final state output in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_10.jpg" alt="Implementing the basic version of the gaming bot" width="467" height="1000"/><div class="caption"><p>Figure 11.10: Output of the basic version of the gaming bot</p></div></div><p>You can track the progress of the bot by using the reward score. You can refer to the following <a id="id1307" class="indexterm"/>screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_11.jpg" alt="Implementing the basic version of the gaming bot" width="278" height="396"/><div class="caption"><p>Figure 11.11: Tracking the progress of the gaming bot</p></div></div><p>As you can see, during the initial iteration, the gaming bot didn't perform well after some iterations bot started learning how to take action based on the experience it gained. We stopped the code when there was no significant improvement in the reward scores. That is because our gaming bot was able to achieve the best solution.</p><p>Now let's <a id="id1308" class="indexterm"/>build a more complex gaming bot; we will be using a deep Q-network for training. So let's begin.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the Space Invaders gaming bot"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec119"/>Building the Space Invaders gaming bot</h1></div></div></div><p>We are <a id="id1309" class="indexterm"/>going to build a gaming bot that can play Space Invaders. Most of you may have played this game or at least heard of it. If you haven't played it or you can't remember it at this moment, then take a look at the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_12.jpg" alt="Building the Space Invaders gaming bot" width="591" height="763"/><div class="caption"><p>Figure 11.12: Snippet of the Space Invaders game</p></div></div><p>Hopefully you remember the game now and how it was played. First, we will look at the concepts <a id="id1310" class="indexterm"/>that we will be using to build this version of the gaming bot. Let's begin!</p><div class="section" title="Understanding the key concepts"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec191"/>Understanding the key concepts</h2></div></div></div><p>In this version <a id="id1311" class="indexterm"/>of the gaming bot, we will be using the deep Q-network and training our bot. So before implementing this algorithm, we need to understand the concepts. Take a look at the following concepts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding a deep Q-network (DQN)</li><li class="listitem" style="list-style-type: disc">Understanding Experience Replay</li></ul></div><div class="section" title="Understanding a deep Q-network (DQN)"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec132"/>Understanding a deep Q-network (DQN)</h3></div></div></div><p>The deep <a id="id1312" class="indexterm"/>Q-network algorithm is basically a combination of two concepts. It uses the Q-learning logic for <a id="id1313" class="indexterm"/>a deep neural network. That is the reason why it is called a deep Q-network (DQN).</p><p>Every gaming world has a different environment. So, say, Super Mario looks different from Space Invaders. We can't feed the entire gaming environment for an individual game every time, so first of all, we need to decide on the universal representation of all games so that we use them as input for the DQN algorithm. The screen pixels are the obvious choice for input because clearly they contain all the relevant information about the game world and its situation. Without the help of the screen pixels we cannot capture the speed and direction of the gaming agent.</p><p>If we apply the same preprocessing steps to the game screens as mentioned in the DeepMind paper, then we need to follow these steps:</p><p>Step 1: We need to consider the last four screen images of the game as the input.</p><p>Step 2: We need to resize them to 84 x 84 and convert them into grayscale with 256 gray levels. That means we would have 256<sup>84x84x4</sup>, which is approximately 10<sup>67970</sup> possible gaming states. This means we have 10<sup>67970</sup> rows in our imaginary Q-table, and that is a big number. You could argue that many pixel combinations or states never occur so we can possibly <a id="id1314" class="indexterm"/>represent it as a sparse matrix. This sparse matrix contains only visited states. However, most of the states are rarely visited. So, it would take a long time for the Q-table to converge. Honestly, we would also like to take a good guess for Q-values for states we have never <a id="id1315" class="indexterm"/>seen before by the agent so that we can generate a reasonably good action for the gaming agent. This is the point where deep learning enters the picture.</p><p>Step 3: Neural networks are quite good for generating good features for highly structured data. With the help of the neural network, we can represent our Q-function. This neural network takes the states, which means four game screens and actions, as input and generates the corresponding Q-value as output. <span class="emphasis"><em>Alternatively, we could take only game screens as the input and generate the Q-value for each possible action as output.</em></span> This approach has a great advantage. Let me explain. There are two major things that we are doing here. First, we need to obtain the updated Q-value. Second, we need to pick up the action with the highest Q-value.</p><p>So if we have Q-values for all possible actions, then we can update the Q-value easily. We can also pick the action with the highest Q-value with a lot of ease. The interesting part is that we can generate the Q-values for all actions by performing a forward pass through the network. After a single forward pass, we can have a list of Q-values for all possible actions with us. This forward pass will save a lot of time and give the gaming agent good rewards.</p><div class="section" title="Architecture of DQN"><div class="titlepage"><div><div><h4 class="title"><a id="ch11lvl4sec65"/>Architecture of DQN</h4></div></div></div><p>You can <a id="id1316" class="indexterm"/>find the optimal architecture of a deep Q-network represented in the following diagram:</p><div class="mediaobject"><img src="Images/B08394_11_13.jpg" alt="Architecture of DQN" width="466" height="488"/><div class="caption"><p>Figure 11.13: Architecture of DQN</p></div></div><p>The preceding <a id="id1317" class="indexterm"/>architecture is used and published in a DeepMind paper. The architecture for the neural network is shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_14.jpg" alt="Architecture of DQN" width="811" height="147"/><div class="caption"><p>Figure 12.14: The DQN architecture</p></div></div><p>The provided architecture uses a classic convolutional neural network (CNN). There are three convolutional layers followed by two fully connected layers that we have seen in the CNN architecture for object detection and face recognition CNN with pooling layers. Here, there <a id="id1318" class="indexterm"/>are no pooling layers. That is because the main motive behind using pooling layers is that they make the neural network insensitive to the location. This means that if we use the pooling layer, then the placement of the objects in the image is not considered by the neural network. This kind of location insensitivity makes sense for a classification task, but for games, the location of the objects in a gaming environment is important. They help us determine the action as well as potential rewards, and we wouldn't want to discard this information. So, we are not using pooling layers here.</p></div><div class="section" title="Steps for the DQN algorithm"><div class="titlepage"><div><div><h4 class="title"><a id="ch11lvl4sec66"/>Steps for the DQN algorithm</h4></div></div></div><p>Let's see <a id="id1319" class="indexterm"/>the steps for DQN algorithm:</p><p>
<span class="emphasis"><em>Input of network:</em></span> Four 84 x 84 grayscale game screen pixels.</p><p>
<span class="emphasis"><em>Output of network:</em></span>  As output, we will generate Q-values for each possible action. Q- values take any real number, which means it can be any real number you can possibly imagine, and that makes it a regression task. We know we can optimize the regression function with a simple squared error loss. The equation of the error loss is shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_15.jpg" alt="Steps for the DQN algorithm" width="304" height="52"/><div class="caption"><p>Figure 11.15: Equation for the error loss function</p></div></div><p>
<span class="emphasis"><em>Q-table update step:</em></span> There's the transition <span class="emphasis"><em>&lt; s, a, r, s' &gt;</em></span>, but this time, the rules for updating the Q-table are not the same as Q-learning. There are some changes. So, the steps for <a id="id1320" class="indexterm"/>updating the Q-table are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Step 1: We need to perform a feedforward pass for the current state <span class="emphasis"><em>s</em></span> in order to get predicted Q-values for all actions.</li><li class="listitem" style="list-style-type: disc">Step 2: Perform a feedforward pass for the next sate <span class="emphasis"><em>s'</em></span> and calculate the maximum over all network output <span class="emphasis"><em>maxa'Q(s', a').</em></span></li><li class="listitem" style="list-style-type: disc">Step 3: Set a Q-value target for action <span class="emphasis"><em>a</em></span> to <span class="emphasis"><em>r + γmaxa'Q(s', a')</em></span>. Here, we can use the <span class="emphasis"><em>maxa'Q(s', a')</em></span> value that we have already calculated in step 2. For all other actions, set the Q-values that are originally from step 1, making the error zero for those outputs.</li><li class="listitem" style="list-style-type: disc">Step 4: We need to update the weights of the neural network using backpropagation.</li></ul></div><p>Now let's look at the concept of experience replay.</p></div></div><div class="section" title="Understanding Experience Replay"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec133"/>Understanding Experience Replay</h3></div></div></div><p>We are estimating the future rewards in each state using two concepts. We use Q-learning <a id="id1321" class="indexterm"/>and approximate the Q-function using a convolutional neural network. Here, the approximation of Q-values is done using a nonlinear function, and this function is not very stable for converging the model. So, we need to experiment with various hyperparameters. This takes a long time: almost a week on a single GPU to train the gaming bot.</p><p>We will be using a concept called experience replay. During the training, all the experiences <span class="emphasis"><em>&lt; s, a, r, s' &gt;</em></span> are stored in a replay memory. When we perform training, the network will use random samples from the replay memory instead of the most recent transition. This way, the training time will be less, plus there is another advantage. With the help of the experience replay, our training task will become more similar to the usual supervised learning. Now we can easily perform debugging and testing operations for the algorithm. With the help of the replay memory, we can store all our human experiences <a id="id1322" class="indexterm"/>of gameplay and then train the model based on this dataset.</p><p>So, the steps for the final Q-learning algorithm used in DQN will be as follows. This algorithm takes from the original DQN paper, which is available at <a class="ulink" href="https://arxiv.org/pdf/1312.5602.pdf">https://arxiv.org/pdf/1312.5602.pdf</a>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Step 1: We need to initialize the replay memory D</li><li class="listitem" style="list-style-type: disc">Step 2: We need to initialize the action-value function Q with random weights</li><li class="listitem" style="list-style-type: disc">Step 3: Observe value of the initial states</li><li class="listitem" style="list-style-type: disc">Step 4: Repeat<div class="informalexample"><pre class="programlisting">Choose an action a       
with probability ε we need to select a random action       
otherwise we need to select a = argmaxa'Q(s,a')   
Perform action a   
Check reward r and new state s'   
store the gameplay experience &lt;s, a, r, s'&gt; in replay memory D   
sample random transitions &lt;ss, aa, rr, ss'&gt; from replay memory D   
calculate target for each minibatch transition       
if ss' is terminal state then tt = rr       
otherwise tt = rr + γmaxa'Q(ss', aa')   
We need to train the Q network using (tt - Q(ss, aa))^2 as loss   
s = s'
until terminated</pre></div></li></ul></div><p>We are using Q-learning and DQN to implement the Space Invaders gaming bot. So let's start <a id="id1323" class="indexterm"/>coding.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Implementing the Space Invaders gaming bot"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec120"/>Implementing the Space Invaders gaming bot</h1></div></div></div><p>In this <a id="id1324" class="indexterm"/>section, we will be coding the Space Invaders game using DQN and Q-learning. For coding, we will be using the <code class="literal">gym</code>,<code class="literal"> TensorFlow</code>, and <code class="literal">virtualenv</code> libraries.  You can refer to the entire code by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/SpaceInvaders_gamingbot">https://github.com/jalajthanaki/SpaceInvaders_gamingbot</a>.</p><p>We are using a convolutional neural network (CNN). Here, we have defined the CNN in a separate file. The name of this file is <code class="literal">convnet.py</code>. Take a look at the following screenshot: at the following figure:</p><div class="mediaobject"><img src="Images/B08394_11_16.jpg" alt="Implementing the Space Invaders gaming bot" width="629" height="221"/><div class="caption"><p>Figure 11.16: Code snippet for Convnrt.py</p></div></div><p>You can refer to the code using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/SpaceInvaders_gamingbot/blob/master/convnet.py">https://github.com/jalajthanaki/SpaceInvaders_gamingbot/blob/master/convnet.py</a>.</p><p>We are defining the DQN algorithm in the <code class="literal">dqn.py</code> script. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_17.jpg" alt="Implementing the Space Invaders gaming bot" width="932" height="693"/><div class="caption"><p>Figure 11.17: Code snippet for dqn.py</p></div></div><p>For training, we have <a id="id1325" class="indexterm"/>defined our training logic in <code class="literal">train.py</code>. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_18.jpg" alt="Implementing the Space Invaders gaming bot" width="888" height="686"/><div class="caption"><p>Figure 11.18: Code snippet for train.py</p></div></div><p>At last, we import <a id="id1326" class="indexterm"/>all these separate scripts to the main <code class="literal">atari.py</code> script, and in that script, we define all the parameter values. You can refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_19.jpg" alt="Implementing the Space Invaders gaming bot" width="1000" height="570"/><div class="caption"><p>Figure 11.19: Code snippet for atari.py</p></div></div><p>You can <a id="id1327" class="indexterm"/>start training by executing the following command:</p><p>
<code class="literal">$ python atari.py --game SpaceInvaders-v0 --display true</code>
</p><p>Training this bot to pass human level performance requires at least 3-4 days of training. I have not provided that amount of training, but you can definitely do that. You can take a look at the output of the training in the following screenshot: the following figure:</p><div class="mediaobject"><img src="Images/B08394_11_20.jpg" alt="Implementing the Space Invaders gaming bot" width="615" height="393"/><div class="caption"><p>Figure 11.20: Output snippet of training step – I</p></div></div><p>You can <a id="id1328" class="indexterm"/>refer to the code snippet for the gaming environment initial score by referring to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_21.jpg" alt="Implementing the Space Invaders gaming bot" width="589" height="761"/><div class="caption"><p>Figure 11.21: Code snippet of the score for the initial few games from the gaming bot</p></div></div><p>To stop the <a id="id1329" class="indexterm"/>training, there will be two parameters: either we can end our training when our loss function value becomes constant for a few iterations, or we complete all the training steps. Here, we have defined 50,000 training steps. You can refer to the code snippet of the output of training in the following screenshot :</p><div class="mediaobject"><img src="Images/B08394_11_22.jpg" alt="Implementing the Space Invaders gaming bot" width="333" height="261"/><div class="caption"><p>Figure 11.22: Code snippet for the training log</p></div></div><p>You can <a id="id1330" class="indexterm"/>see the score of the gaming bot after 1,000 iterations by taking a look at the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_23.jpg" alt="Implementing the Space Invaders gaming bot" width="160" height="197"/><div class="caption"><p>Figure 11.23: Code snippet for the gaming bot after 1,000 iterations</p></div></div><p>I have already upload the pre-trained model for you. You can download it by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/SpaceInvaders_gamingbot/tree/master/model">https://github.com/jalajthanaki/SpaceInvaders_gamingbot/tree/master/model</a>.</p><p>Now it's <a id="id1331" class="indexterm"/>time to build the gaming bot for the Pong game. If you train this bot for a week using a single GPU, it can beat the AI rules that are written by the gaming manufacture team. So, our agent will surely act better than the computer agent.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the Pong gaming bot"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec121"/>Building the Pong gaming bot</h1></div></div></div><p>In this section, we will be looking at how we can build a gaming bot that can learn the game of <a id="id1332" class="indexterm"/>Pong. Before we start, we will look at the approach and concepts that we will be using for building the Pong gaming bot.</p><div class="section" title="Understanding the key concepts"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec192"/>Understanding the key concepts</h2></div></div></div><p> In this <a id="id1333" class="indexterm"/>section, we will be covering some aspects of building the Pong game bot, which are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Architecture of the gaming bot</li><li class="listitem" style="list-style-type: disc">Approach for the gaming bot</li></ul></div><div class="section" title="Architecture of the gaming bot"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec134"/>Architecture of the gaming bot</h3></div></div></div><p>In order <a id="id1334" class="indexterm"/>to develop the Pong gaming bot, we are choosing a neural-network-based approach. The architecture of our neural network is crucial. Let's look at the architectural components step by step:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We take the gaming screen as the input and preprocess it as per the DQN algorithm.</li><li class="listitem">We pass this preprocessed screen to an neural network (NN.)</li><li class="listitem">We use a gradient descent to update the weights of the NN.</li><li class="listitem">Weight [1]: This matrix holds the weights of pixels passing into the hidden layer. The dimension will be [200 x 80 x 80] – [200 x 6400].</li><li class="listitem">Weight [2]: This matrix holds the weights of the hidden layer passing into the output. The dimension will be [1 x 200].</li></ol></div><p>You can refer to the following diagram:</p><div class="mediaobject"><img src="Images/B08394_11_24.jpg" alt="Architecture of the gaming bot" width="615" height="287"/><div class="caption"><p>Figure 11.24: Architecture of NN for the Pong gaming bot</p></div></div><p>The tasks <a id="id1335" class="indexterm"/>for each component of the NN make more sense when we see the detailed approach for this gaming bot.</p></div><div class="section" title="Approach for the gaming bot"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lvl3sec135"/>Approach for the gaming bot</h3></div></div></div><p>In order <a id="id1336" class="indexterm"/>to build the Pong gaming bot, we will be using the following approach:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For implementation, we are using the preprocessed image vector, which is a [6400 x 1] dimension array.</li><li class="listitem" style="list-style-type: disc">With the help of NN, we can compute a probability of moving up.</li><li class="listitem" style="list-style-type: disc">With the help of that probability distribution, we will decide whether the agent is moving up or not.</li><li class="listitem" style="list-style-type: disc">If the gaming round is over, it means that the gaming agent as well as the opponent missed the ball. In this case, we need to find out whether our gaming agent won or lost.</li><li class="listitem" style="list-style-type: disc">When the <a id="id1337" class="indexterm"/>episode finishes, which means if either of the players scores 21 points, we need to pass the result. With the help of the loss function, we can find out the error values. We applied the gradient descent algorithm to find out the direction in which our neural network's weight should be updated. Based on the backpropagation algorithm, we propagate the error back to the network so that our network can update the weights.</li><li class="listitem" style="list-style-type: disc">Once 10 episodes have finished, we need to sum up the gradient, and after that, we update the weights in the direction of the gradient.</li><li class="listitem" style="list-style-type: disc">Repeat this process until our networks weights are tuned and we can beat the computer.</li></ul></div><p>Now let's cover the coding steps.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Implementing the Pong gaming bot"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec122"/>Implementing the Pong gaming bot</h1></div></div></div><p>These are <a id="id1338" class="indexterm"/>the implementation steps that we need to follow:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Initialization of the parameters</li><li class="listitem" style="list-style-type: disc">Weights stored in the form of matrices</li><li class="listitem" style="list-style-type: disc">Updating weights</li><li class="listitem" style="list-style-type: disc">How to move the agent</li><li class="listitem" style="list-style-type: disc">Understanding the process  using NN</li></ul></div><p>You can refer to the entire code by using this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Atari_Pong_gaming_bot">https://github.com/jalajthanaki/Atari_Pong_gaming_bot</a>.</p><div class="section" title="Initialization of the parameters"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec193"/>Initialization of the parameters</h2></div></div></div><p>First, we <a id="id1339" class="indexterm"/>define and initialize our parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">batch_size:</code> This parameter indicates how many rounds of games we should play before updating the weights of our network.</li><li class="listitem" style="list-style-type: disc"><code class="literal">gamma:</code> This is the discount factor. We use this to discount the effect of old actions of the game on the final result.</li><li class="listitem" style="list-style-type: disc"><code class="literal">decay_rate:</code> This parameter is used to update the weight.</li><li class="listitem" style="list-style-type: disc"><code class="literal">num_hidden_layer_neurons:</code> This parameter indicates how many neurons we should put in the hidden layer.</li><li class="listitem" style="list-style-type: disc"><code class="literal">learning_rate: </code>This is the speed at which our gaming agent learns from the results so that we can compute new weights. A higher learning rate means we react more strongly to results, and a lower rate means we don't react much to each result.</li></ul></div><p>You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_25.jpg" alt="Initialization of the parameters" width="354" height="242"/><div class="caption"><p>Figure 11.25: Initialization of parameters</p></div></div></div><div class="section" title="Weights stored in the form of matrices"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec194"/>Weights stored in the form of matrices</h2></div></div></div><p>The weights <a id="id1340" class="indexterm"/>of the neural network are stored in the form of matrices. The first layer of NN is a 200 x 6400 matrix that represents the weights for our hidden layer. If we use the notation <span class="emphasis"><em>w1_ij</em></span>, then that would mean that we are representing the weight of the <span class="emphasis"><em>i<sup>th</sup></em></span> neuron for the input pixel <span class="emphasis"><em>j</em></span> in layer 1. The second layer is a 200 x 1 matrix representing the weights. These weights are the output of the hidden layer. For layer 2, element <span class="emphasis"><em>w2_i</em></span> indicates the weights placed on the activation of the <span class="emphasis"><em>i<sup>th</sup></em></span> neuron in the hidden layer.</p><p>You can refer to the code snippet given in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_26.jpg" alt="Weights stored in the form of matrices" width="788" height="84"/><div class="caption"><p>Figure 11.26: Weight matrices</p></div></div></div><div class="section" title="Updating weights"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec195"/>Updating weights</h2></div></div></div><p>For updating <a id="id1341" class="indexterm"/>the weight, we will be using RMSprop. You can refer to this paper in order to understand more details about this function:</p><p>
<a class="ulink" href="http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop">http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop</a>. Refer to the following figure for the following figure.</p><div class="mediaobject"><img src="Images/B08394_11_27.jpg" alt="Updating weights" width="247" height="114"/><div class="caption"><p>Figure 11.27: Equation for RMSprop</p></div></div><p>The code is shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_28.jpg" alt="Updating weights" width="967" height="177"/><div class="caption"><p>Figure 11.28: Code snippet for updating weights</p></div></div></div><div class="section" title="How to move the agent"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec196"/>How to move the agent</h2></div></div></div><p>With the <a id="id1342" class="indexterm"/>help of the preprocessed input, we pass the weight matrix  to the neural network. We need to generate the probability of telling our agent to move up. You can refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_29.jpg" alt="How to move the agent" width="859" height="322"/><div class="caption"><p>Figure 11.29: Code snippet to move the agent</p></div></div><p>We are done <a id="id1343" class="indexterm"/>with all the major helper functions. We need to apply all this logic to the neural network so that it can take the observation and generate the probability of our gaming agent for going in upward direction.</p></div><div class="section" title="Understanding the process using NN"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lvl2sec197"/>Understanding the process using NN</h2></div></div></div><p>These are <a id="id1344" class="indexterm"/>the steps that can help us generate the probability for our agent so that it can decide when  they should move in upward direction</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We need to compute hidden layer values by applying the dot product between weights [1] and <code class="literal">observation_matrix</code>. Weight [1] is a 200 x 6400 matrix and <code class="literal">observation_matrix</code> is a 6400 x 1 matrix. The dimension of the output matrix is 200 x 1. Here, we are using 200 neurons. Each row of Q-function represents the output of one neuron.</li><li class="listitem" style="list-style-type: disc">We apply a nonlinear function ReLU to the hidden layer values.</li><li class="listitem" style="list-style-type: disc">We are using hidden layer activation values in order to calculate the values for the output layer. Again, we performed dot product between <code class="literal">hidden_layer_values</code> [200 x 1] and weights [2] [1 x 200]. This dot product gives us single value [1 x 1].</li><li class="listitem" style="list-style-type: disc">Finally, we apply the sigmoid function to the output value. This will give us the answer in terms of probability. The value of the output is between 0 and 1.</li></ul></div><p>You can <a id="id1345" class="indexterm"/>refer to the code snippet shown in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_30.jpg" alt="Understanding the process using NN" width="1000" height="651"/><div class="caption"><p>Figure 11.30: Code snippet for the process happens using NN</p></div></div><p>To run this code, you need to execute the following command:</p><div class="informalexample"><pre class="programlisting">$ python me_Pong.py</pre></div><p>If you want to build a bot that can beat the computer, then you need to train it for at least three to four days on a single GPU. You can refer to the output of the bot in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_31.jpg" alt="Understanding the process using NN" width="274" height="366"/><div class="caption"><p>Figure 11.31: The Pong gaming bot output</p></div></div><p>You can <a id="id1346" class="indexterm"/>see the training log in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_32.jpg" alt="Understanding the process using NN" width="629" height="26"/><div class="caption"><p>Figure 11.32: Training log for Pong gaming bot</p></div></div><p>Now let's build a gaming bot just for fun. This bot uses the Flappy Bird gaming environment.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Just for fun - implementing the Flappy Bird gaming bot"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec123"/>Just for fun - implementing the Flappy Bird gaming bot</h1></div></div></div><p>In this <a id="id1347" class="indexterm"/>section, we will be building the Flappy Bird gaming bot. This gaming bot has been built using DQN. You can find the entire code at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/DQN_FlappyBird">https://github.com/jalajthanaki/DQN_FlappyBird</a>.</p><p>This bot has a pre-trained model, so you test it using the pre-trained model. In order to run this bot, you need to execute this command:</p><div class="informalexample"><pre class="programlisting">$ python deep_q_network.py</pre></div><p>You can see the output in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_11_33.jpg" alt="Just for fun - implementing the Flappy Bird gaming bot" width="291" height="519"/><div class="caption"><p>Figure 11.33: Output of the Flappy Bird gaming bot</p></div></div><p>You can see the combination of all the concepts that we have studied so far in this implementation, so make sure you explore this code. Consider this your exercise for the chapter.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch11lvl1sec124"/>Summary</h1></div></div></div><p>Congratulations, readers; you have made it to the end! We covered basic concepts related to reinforcement learning in this chapter. You learned about the various concepts and algorithms of building the gaming bot. You also learned how the Deep Q Learner algorithm works. Using the <code class="literal">gym </code>library, we loaded the gaming world. By using the <code class="literal">dqn </code>library, we will be able to train the model. Training a gaming bot that can defeat human level experts takes a lot of time. So, I trained it for a few hours only. If you want to train for more hours, you can definitely do that. We tried to build a variety of simple Atari games, such as a simple pathfinder gaming bot, Space Invaders, Pong, and Flappy Bird. You can expand this basic approach to the bigger gaming environment. If you want to get yourself updated and contribute, then you can take a look at the OpenAI GitHub repository at: <a class="ulink" href="https://github.com/openai">https://github.com/openai</a>. Deep Mind news and the blog section are at this link: <a class="ulink" href="https://deepmind.com/blog/">https://deepmind.com/blog/</a> .</p><p>In the following section, you will find an appendix that can help you gain some extra information. This extra information will help you when you are building Machine Learning (ML) applications or taking part in a hackathon or other competitions. I have also provided some cheat sheets that can help you when you are building ML applications.</p></div></div>



  </body></html>