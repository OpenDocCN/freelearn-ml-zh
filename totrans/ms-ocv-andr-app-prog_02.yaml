- en: Chapter 2. Detecting Basic Features in Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading about the basics of image processing and manipulation in the previous
    chapter, we will take a look at some of the most widely used algorithms used to
    extract meaningful information from the images in the form of edges, lines, circles,
    ellipses, blobs or contours, user defined shapes, and corners. In context of *computer
    vision* and *image processing*, such information is often termed as *features*.
    In this chapter, we will take a look at the various feature detection algorithms,
    such as Edge and Corner detection algorithms, Hough transformations, and Contour
    detection algorithms and their implementations on an Android platform using OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: To make our lives simpler, and have a clear understanding of this chapter, we
    will first create a basic Android application to which we will keep adding implementations
    of different feature detection algorithms. This will reduce the amount of extra
    code that we would otherwise have to write for each algorithm in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's create a very basic Android application that will read images from your
    phone's gallery and display them on the screen using the *ImageView* control.
    The application will also have a menu option to open the gallery to choose an
    image.
  prefs: []
  type: TYPE_NORMAL
- en: We will start off by creating a new Eclipse (or an Android Studio) project with
    a blank activity, and let's call our application **Features App**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before doing anything to the application, initialize OpenCV in your application
    (refer to [Chapter 1](ch01.html "Chapter 1. Applying Effects to Images"), *Applying
    Effects to Images*, on how to initialize OpenCV in an Android project).
  prefs: []
  type: TYPE_NORMAL
- en: 'To the blank activity, add an `ImageView` control (used to display the image),
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the application menu, add an `OpenGallery` menu option to open the phone''s
    gallery and help us pick an image. For this, add a new menu item in the project''s
    menu resource XML file (default location of the file is `/res/menu/filename.xml`),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more detailed information on menus in Android, refer to [http://developer.android.com/guide/topics/ui/menus.html](http://developer.android.com/guide/topics/ui/menus.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now make the `OpenGallery` menu option functional. Android API exposes
    a `public boolean onOptionsItemSelected(MenuItem item)` function that allows the
    developer to program the option selection event. In this function, we will add
    a piece of code that will open the gallery of your phone to choose an image. Android
    API provides a predefined intent `Intent.ACTION_PICK` just for this task; that
    is, to open the gallery and pick an image. We will use this intent for our application,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let's modify the `public boolean onOptionsItemSelected(MenuItem item)` function
    and make it function as per our need.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final implementation of the function should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This code has nothing but a bunch of easy-to-understand if else statements.
    The thing you need to understand here is the `startActivityForResult()` function.
    As you might have realized, we want to bring the image data from `ACTION_PICK
    Intent` in to our application so that we can use it later as an input for our
    feature detection algorithms. For this reason, instead of using the `startActivity()`
    function, we use `startActivityForResult()`. After the user is done with the subsequent
    activity, the system calls the `onActivityResult()` function along with the result
    from the called intent, which is the gallery picker in our case. Our work now
    is to implement the `onActivityResult()` function in accordance with our application.
    Let's first enumerate what we want to do with the returned image. Not much actually;
    correct the orientation of the image and display it on the screen using `ImageView`
    that we added to our activity in the beginning of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You must be wondering what is meant by correcting the orientation of an image.
    In any Android phone, there can be multiple sources of images, such as the native
    camera application, the Java camera app, or any other third-party app. Each of
    them might have different ways of capturing and storing images. Now, in your application,
    when you load these images, it may so happen that they are rotated by some angle.
    Before these images can be used in our application, we should correct their orientation
    so that they appear meaningful to your application users. We will take a look
    at the code to do this now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `onActivityResult()` function for our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let's see what this long piece of code does. First, we do a sanity check and
    see whether the result is coming from the appropriate intent (that is, the gallery
    picker) by checking `requestCode` and `resultCode`. After this is done, we try
    to retrieve the path of the image in your phone's filesystem. From the `ACTION.PICK`
    intent, we get the `Uri` of the selected image, which we will store in `Uri selectedImage`.
    To get the exact path of the image, we make use of the `Cursor` class. We initialize
    a new `Cursor` class object with it pointing toward our `selectedImage`. Using
    `MediaStore.Images.Media.DATA`, we fetch the column index of the selected image,
    and then eventually, the path of the image using the cursor class declared earlier,
    and store it in a string, `picturePath`. After we have the path of the image,
    we create a new Bitmap object temp to store the image. So far, we have been able
    to read the image and store it in a bitmap object. Next we need to correct the
    orientation. For this, we first extract the orientation information from the image
    using the `ExifInterface` class. As you can see in the code, the `ExifInterface`
    class gives us the orientation information through `ExifInterface.TAG_ORIENTATION`.
    Using this orientation information, we rotate our bitmap accordingly using the
    `rotateBitmap()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For implementation of the `rotateBitmap()` function, refer to the code bundle
    that accompanies this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'After correcting the orientation, we make two copies of the bitmap: one to
    store the original image (`originalBitmap`) and the other one to store the processed
    bitmaps (`currentBitmap`), that is, to store the outputs of different algorithms
    applied to the original bitmap. The only part left is to display the image on
    the screen. Create a new function `loadImageToView()` and add the following lines
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first line creates an instance of `ImageView` and the second line sets that
    image onto the view. Simple!
  prefs: []
  type: TYPE_NORMAL
- en: 'One last thing and our application is ready! Since our application is going
    to read data from permanent storage (read images from external storage), we need
    permission. To the `AndroidManifest.xml` file, add the following lines that will
    allow the application to access external storage for reading data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our basic application in place, let's take a look at the different
    feature detection algorithms, starting with Edge and Corner detection, Hough transformation,
    and Contours.
  prefs: []
  type: TYPE_NORMAL
- en: Edge and Corner detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Edge detection and Corner detection are two of the most basic feature detection
    algorithms, and are very useful ones too. Having information about the edges in
    an image can be of great help in applications, where you want to find boundaries
    of different objects in an image, or you need to find corners in an image when
    you want to analyze how an object rotates or moves in a given sequence of images
    (or videos). In this section, we will take a look at the techniques and implementations
    of various Edge and Corner detection algorithms, such as Difference of Gaussian,
    Canny Edge detector, Sobel Operator, and Harris Corners.
  prefs: []
  type: TYPE_NORMAL
- en: The Difference of Gaussian technique
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start with the easiest and the most rudimentary technique. Before we understand
    how **Difference of Gaussian** (**DoG**) works, let's take a look at what exactly
    edges are. To put it simply, edges are points in an image where the pixel intensity
    changes appreciably. We will exploit this property of edges and by applying Gaussian
    blur on the image, we will compute the edge points (Edges).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a three-step explanation of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the given image to a grayscale image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the grayscale image, perform Gaussian blur using two different blurring radiuses
    (you should have two Gaussian blurred images after this step).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract (arithmetic subtraction) the two images generated in the previous step
    to get the resultant image with only edge points (Edges) in it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does this technique work? How can subtracting two Gaussian blurred images
    give us edge points? A Gaussian filter is used to smooth out an image and the
    extent of smoothening depends on the blurring radius. Consider an image of a chess
    board. When you apply a Gaussian filter to the chess board image, you will observe
    that there is almost no change near the center of the white and black squares,
    whereas the common side of the black and white squares (which is an edge point)
    gets smudged, implying loss of edge information. Gaussian blur makes the edge
    less prominent.
  prefs: []
  type: TYPE_NORMAL
- en: According to our technique, we have two Gaussian blurred images with different
    blurring radius. When you subtract these two images, you will lose all the points
    where no smoothening or smudging happened, that is, the center of the black and
    white squares in the case of a chess board image. However, pixel values near the
    edges would have changed because smudging pixel values and subtracting such points
    will give us a non-zero value, indicating an edge point. Hence, you get edge points
    after subtracting two Gaussian blurred images.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are only performing Gaussian blurs on images, it is one of the fastest
    ways of calculating edges. Having said that, it is also true that this technique
    does not return very promising results. This technique might work very well for
    some images and can completely fail in some scenarios. However, it doesn't hurt
    to know one extra algorithm!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s modify our Features App that we created in the last section and apply
    DoG to it. In the applications menu, we add a new menu option, *Difference of
    Gaussian*, to the menu resource XML file using these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a new function `public void DifferenceOfGaussian()`, which will compute
    edges in any given image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding piece of code, we first convert the image to a grayscale image.
    Then, we apply the Gaussian filter to the image twice, with two different blurring
    radiuses, using the `Imgproc.GaussianBlur()` function. The first and second parameters
    in this function are input and output images, respectively. The third parameter
    specifies the size of the kernel to be used while applying the filter, and the
    last parameter specifies the value of sigma used in the Gaussian function. Then
    we determine the absolute difference of the images using `Core.absdiff()`. Once
    this is done, we post-process our image to make it comprehensible by applying
    the *Inverse Binary Threshold* operation to set the edge point values to white
    (255). Finally, we convert the bitmap to Mat and display it on the screen using
    `loadImageToView()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the resulting image after applying DoG on Lenna:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Difference of Gaussian technique](img/B02052_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Difference of Gaussian is not often used because it has been superseded by other
    more sophisticated techniques that we are going to discuss later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The Canny Edge detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Canny Edge detection is a widely used algorithm in computer vision and is often
    considered as an optimal technique for edge detection. The algorithm uses more
    sophisticated techniques than Difference of Gaussian, such as intensity gradient
    in multiple directions, and thresholding with hysteresis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is broadly divided into four stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Smoothing the image**: This is the first step of the algorithm, where we
    reduce the amount of noise present in the image by performing a Gaussian blur
    with an appropriate blurring radius.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calculating the gradient of the image**: Here we calculate the intensity
    gradient of the image and classify the gradients as vertical, horizontal, or diagonal.
    The output of this step is used to calculate actual edges in the next stage.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Non-maximal supression**: Using the direction of gradient calculated in the
    previous step, we check whether or not a pixel is the local maxima in the positive
    and negative direction of the gradient if not then, we suppress the pixel (which
    means that a pixel is not a part of any edge). This is an edge thinning technique.
    Select edge points with the sharpest change.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Edge selection through hysteresis thresholding**: This is the final step
    of the algorithm. Here we check whether an edge is strong enough to be included
    in the final output, essentially removing all the less prominent edges.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to [http://en.wikipedia.org/wiki/Canny_edge_detector](http://en.wikipedia.org/wiki/Canny_edge_detector)
    for a more detailed explanation.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an implementation of the algorithm using OpenCV for Android.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Difference of Gaussian, first add the *Canny Edges* option to the application
    menu by adding a new item in the menu resource XML file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a new function, `public void Canny()`, and add the following lines of
    code to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we first convert our image to a grayscale image, and
    then simply call the `Imgproc.Canny()` function implemented in the OpenCV API
    for Android. The important thing to notice here are the last two parameters in
    `Imgproc.Canny()`. They are for low and high thresholds respectively. In Canny
    Edge detection algorithm, we classify each point in the image into one of three
    classes, `suppressed points`, `weak edge points`, and `strong edge points`. All
    the points that have the intensity gradient value less than the low threshold
    values are classified as suppressed points, points with the intensity gradient
    value between low and high threshold values are classified as weak edge points,
    and points with the intensity gradient value above the high threshold values are
    classified as strong edge points.
  prefs: []
  type: TYPE_NORMAL
- en: According to the algorithm, we ignore all the suppressed points. They will not
    be a part of any edge in the image. Strong edge points definitely form a part
    of an edge. For weak edge points, we check whether they are connected to any strong
    edge points in the image by checking the eight pixels around that weak point.
    If there are any strong edge points in those eight pixels, we count that weak
    point as a part of the edge. That's Canny Edge detection!
  prefs: []
  type: TYPE_NORMAL
- en: The Sobel operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another technique for computing edges in an image is using the Sobel operator
    (or Sobel filter). As in Canny Edge detection, we calculate the intensity gradient
    of the pixel, but in a different way. Here we calculate the approximate intensity
    gradient by convoluting the image with two 3x3 kernels for horizontal and vertical
    directions each:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Sobel operator](img/B02052_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Convolution matrices used in Sobel filter
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the horizontal and vertical gradient values, we calculate the absolute
    gradient at each pixel using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Sobel operator](img/B02052_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For an approximate gradient, the following formula is usually used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Sobel operator](img/B02052_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The steps involved in computing edges using a Sobel operator are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the image to a grayscale image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the absolute value of the intensity gradient in the horizontal direction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the absolute value of the intensity gradient in the vertical direction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the resultant gradient using the preceding formula. The resultant gradient
    values are essentially the edges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s now add a Sobel filter to our Features App. Start by adding a *Sobel
    filter* menu option in the menu''s XML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a Sobel filter using OpenCV for Android:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we first convert the image to a grayscale image. After this,
    using the grayscale image, we calculate the intensity gradient in the horizontal
    and vertical directions using the `Imgproc.Sobel()` function, and store the output
    in `grad_x` and `grad_y`. As per the formula mentioned in the algorithm, we calculate
    the absolute value of the gradients and add them together to get the resultant
    gradient value (basically the edges). The following code snippet performs the
    described step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we convert the Mat into a bitmap and display it on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You may also be interested to take a look at the Prewitt operator ([http://en.wikipedia.org/wiki/Prewitt_operator](http://en.wikipedia.org/wiki/Prewitt_operator)).
    It is similar to a Sobel operator, but uses a different matrix for convolution.
  prefs: []
  type: TYPE_NORMAL
- en: Harris Corner detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the literal sense of the term, corners are points of intersection of two
    edges or a point which has multiple prominent edge directions in its local neighborhood.
    Corners are often considered as points of interest in an image and are used in
    many applications, ranging from image correlation, video stabilization, 3D modelling,
    and the likes. Harris Corner detection is one of the most used techniques in corner
    detection; and in this section, we will take a look at how to implement it on
    an Android platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Harris corner detector uses a sliding window over the image to calculate the
    variation in intensity. Since corners will have large variations in the intensity
    values around them, we are looking for positions in the image where the sliding
    windows show large variations in intensity. We try to maximize the following term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Harris Corner detection](img/B02052_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, **I** is the image, **u** is the shift in the sliding window in the horizontal
    direction, and **v** is the shift in the vertical direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an implementation of Harris Corner using OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we start off by converting the image to a grayscale image
    and then use it as an input to the `Imgproc.cornerHarris()` function. The other
    inputs to the function are the block size, kernel size, and a parameter, `k`,
    that is used to solve one of the equations in the algorithm (for details on mathematics,
    refer to OpenCV's documentation on Harris Corner at [http://docs.opencv.org/doc/tutorials/features2d/trackingmotion/harris_detector/harris_detector.html](http://docs.opencv.org/doc/tutorials/features2d/trackingmotion/harris_detector/harris_detector.html)).
    The output of Harris Corner is a 16-bit scalar image, which is normalized to get
    the pixel values in the range 0 to 255\. After this, we run a `for` loop and draw
    all the circles on the image with the centers being points whose intensity value
    is greater than a certain user set threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Hough transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we looked at how to detect edges and corners in an image. Sometimes,
    for image analysis apart from edges and corners, you want to detect shapes, such
    as lines, circles, ellipses, or any other shape for that matter. Say for example,
    you want to detect coins in an image, or you want to detect a box or a grid in
    an image. A technique that comes handy in such scenarios is Hough transformations.
    It is a widely used technique that detects shapes in an image using their mathematical
    equations in their parameterized forms.
  prefs: []
  type: TYPE_NORMAL
- en: The generalized Hough transformation is capable of detecting any shape for which
    we can provide an equation in the parameterized form. As the shapes start getting
    complex (with an increase in the number of dimensions), such as spheres or ellipsoids,
    it gets computationally expensive; hence, we generally look at standard Hough
    transformations for simple 2D shapes, such as lines and circles.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will take a look at Hough transformations to detect lines
    and circles, but as mentioned earlier, it can be further extended to detect shapes,
    such as ellipses, and even simple 3D shapes, such as spheres.
  prefs: []
  type: TYPE_NORMAL
- en: Hough lines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Detecting lines is one of the simplest use cases of Hough transformations.
    In Hough lines, we select a pair of points from our image *(x1, y1)* and *(x2,
    y2)*, and solve the following pair of equations for *(a, m)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y1 = m(x1) + a*'
  prefs: []
  type: TYPE_NORMAL
- en: '*y2 = m(x2) + a*'
  prefs: []
  type: TYPE_NORMAL
- en: We maintain a table with two columns *(a, m)* and a count value. The count value
    keeps a record of how many times we get the *(a, m)* value after solving the preceding
    pair of equations. This is nothing but a voting procedure. After calculating the
    *(a, m)* values for all possible pairs of points, we take the *(a, m)* values
    that have count values greater than a certain threshold and these values are the
    desired lines in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For Hough transformations, we never run the algorithm directly on the image.
    First, we compute the edges in the image, and then apply the Hough transformation
    on the edges. The reason being, any prominent line in the image has to be an edge
    (the reverse is not true, every edge in the image will not be a line), and using
    only edges, we are reducing the number of points on which the algorithm runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV provides two implementations of Hough lines: standard Hough lines and
    probabilistic Hough lines. The major difference between the two is that, in probabilistic
    Hough lines, instead of using all edge points, we select a subset of the edge
    points by random sampling. This makes the algorithm run faster since there are
    fewer points to deal with, without compromising on its performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Time to write some code! First things first, add a new *Hough lines* menu option
    to our application menu. However, try to figure out the code to do this yourself
    this time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hopefully, the menu option is now in place! Let''s now take a look at a code
    snippet that uses the probabilistic Hough transformation to detect lines in an
    image using OpenCV for Android:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As explained earlier, we first compute edges in the image using any edge detection
    technique (the preceding code uses Canny). The output of the Canny Edge detector
    is used as an input to the `Imgproc.HoughLinesP()` function. The first and second
    parameters are input and output respectively. The third and fourth parameters
    specify the resolution of `r` and theta in pixels. The next two parameters are
    the threshold and minimum number of points that a line should have. Lines with
    fewer points than this are discarded.
  prefs: []
  type: TYPE_NORMAL
- en: The `For` loop in the code is used to draw all the lines on the image. This
    is only done to visualize the lines detected by the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Hough circles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analogous to Hough lines, Hough circles also follow the same procedure to detect
    circles, only the equations change (the parameterized form of a circle is used
    instead).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an implementation of Hough circles using OpenCV for Android:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The code is pretty much the same as Hough lines with a few changes. The output
    of `Imgproc.HoughCircles()` is a tuple of center coordinates and the radius of
    the circle (x, y, radius). To draw circles on the image, we use `Core.circle()`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A nice coding exercise would be to implement Hough lines/circles without using
    the predefined OpenCV functions.
  prefs: []
  type: TYPE_NORMAL
- en: Contours
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are often required to break down the image into smaller segments to have
    a more focused view of the object of interest. Say for instance, you have an image
    with balls from different sports, such as a golf ball, cricket ball, tennis ball,
    and football. However, you are only interested in analyzing the football. One
    way of doing this could be by using Hough circles that we looked at in the last
    section. Another way of doing this is using contour detection to segment the image
    into smaller parts, with each segment representing a particular ball.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to choose the segment having the largest area, that is, your
    football (it is safe to assume that the football would be the largest of all!).
  prefs: []
  type: TYPE_NORMAL
- en: Contours are nothing but connected curves in an image or boundaries of connected
    components in an image. Contours are often computed using edges in an image, but
    a subtle difference between edges and contours is that contours are closed, whereas
    edges can be anything. The concept of edges is very local to the point and its
    neighboring pixels; however, contours take care of the object as a whole (they
    return boundaries of objects).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the implementation of Contour detection using OpenCV
    for Android. Let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: OpenCV does make our life simple! In this code, we first convert our image to
    a grayscale image (it is not necessary to use the grayscale version of the image,
    you can directly work with colored images as well), and then find edges in it
    using Canny Edge detection. After we have the edges with us, we pass this image
    to a predefined function `Imgproc.findContours()`. The output of this function
    is `List<MatOfPoint>` which stores all the contours computed in that image. The
    parameters passed in to the `Imgproc.findContours()` function are interesting.
    The first and the second parameters are the input images and list of contours
    respectively. The third and the fourth parameters are interesting; they give the
    hierarchy of contours in the image. The third parameter stores the hierarchy,
    while the fourth parameter specifies the nature of hierarchy the users want. The
    hierarchy essentially tells us the overall arrangement of contours in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Refer to [http://docs.opencv.org/master/d9/d8b/tutorial_py_contours_hierarchy.html](http://docs.opencv.org/master/d9/d8b/tutorial_py_contours_hierarchy.html)
    for a detailed explanation of hierarchies in Contours.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `for` loop in the code is used to draw contours on a new image. The `Imgproc.drawContours()`
    function draws the contours on the image. In this function, the first parameter
    is a Mat, for where you want to draw the contours. The second parameter is the
    list of contours returned by `Imgproc.findContours()`. The third parameter is
    the index of the contour that we want to draw, and the fourth parameter is the
    color to be used to draw the contour. While drawing contours, you have two options:
    either you draw the boundary of the contour or you fill the entire contour. The
    fifth parameter in the function helps you to specify your choice. A negative value
    means you need to fill the entire contour, whereas any positive value specifies
    the thickness of the boundary.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, convert `Mat` to `Bitmap` and display it on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: With contour detection, we successfully completed our Features App.
  prefs: []
  type: TYPE_NORMAL
- en: Project – detecting a Sudoku puzzle in an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's try to apply our learning from this chapter and create a simple application
    for detecting a Sudoku grid in an image. You see Sudoku puzzles every day in newspapers
    or magazines; sometimes they provide a solution and sometimes they don't. Why
    not build an application for your mobile phone that can click a picture of the
    Sudoku, analyze the numbers, run some intelligent algorithms to solve the puzzle,
    and within seconds, you have the solution on your mobile screen.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, we can easily work on the first part of the application;
    that is, localizing (detecting) the Sudoku grid from an image. As you read through
    this book, you will come across algorithms and techniques that will help you build
    the other parts of this application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break down our problem statement into subproblems and work each one
    out to have a fully functioning Sudoku localizing application:'
  prefs: []
  type: TYPE_NORMAL
- en: Capture the image using a camera or load it from your gallery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess the image by running your favorite edge detection algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find a rectangular grid in the image. The possible options are to use Hough
    lines to detect lines and then look for four lines that form a rectangle (a little
    tedious), or find contours in the image and assume the largest contours in your
    image to be the Sudoku grid that you were looking for (the assumption made here
    is safe providing you click or load a picture that focuses on the grid more than
    anything else).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After you have narrowed down on the grid, create a new image and copy only the
    contour region to the new image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have successfully detected the Sudoku grid!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a sample image with a Sudoku grid in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Project – detecting a Sudoku puzzle in an image](img/B02052_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the narrowed down grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Project – detecting a Sudoku puzzle in an image](img/B02052_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter covers everything that one should know about in order to create
    this application. Once you have tried doing it by yourself, you can download the
    code from the Packt Publishing website.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learnt about some basic features in images, such as
    edges, corners, lines, and circles. We have looked at different algorithms, such
    as Canny Edge detection and Harris corners, that can be implemented on an Android
    device. These are the basic set of algorithms that will come handy in many applications
    that we are going to build in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
