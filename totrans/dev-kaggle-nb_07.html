<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer184">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 class="chapterTitle" id="_idParaDest-92"><span class="koboSpan" id="kobo.2.1">Text Analysis Is All You Need</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">In this chapter, we will learn how to analyze text data and create machine learning models to help us. </span><span class="koboSpan" id="kobo.3.2">We will use the </span><em class="italic"><span class="koboSpan" id="kobo.4.1">Jigsaw Unintended Bias in Toxicity Classification</span></em><span class="koboSpan" id="kobo.5.1"> dataset (see </span><em class="italic"><span class="koboSpan" id="kobo.6.1">Reference 1</span></em><span class="koboSpan" id="kobo.7.1">). </span><span class="koboSpan" id="kobo.7.2">This competition had the objective of building models that detect toxicity and reduce unwanted bias toward minorities that might be wrongly associated with toxic comments. </span><span class="koboSpan" id="kobo.7.3">With this competition, we introduce the field of </span><strong class="keyWord"><span class="koboSpan" id="kobo.8.1">Natural Language Processing</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.10.1">NLP</span></strong><span class="koboSpan" id="kobo.11.1">).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.12.1">The data used in the competition originates from the Civil Comments platform, which was founded by Aja Bogdanoff and Christa Mrgan in 2015 (see </span><em class="italic"><span class="koboSpan" id="kobo.13.1">Reference 2</span></em><span class="koboSpan" id="kobo.14.1">) with the aim of solving the problem of civility in online discussions. </span><span class="koboSpan" id="kobo.14.2">When the platform was closed in 2017, they chose to keep around 2 million comments for researchers who want to understand and improve civility in online conversations. </span><span class="koboSpan" id="kobo.14.3">Jigsaw was the organization that sponsored this effort and then started a competition for language toxicity classification. </span><span class="koboSpan" id="kobo.14.4">In this chapter, we’re going to transform pure text into meaningful, model-ready numbers to be able to classify them into groups according to the toxicity of the comments.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.15.1">In a nutshell, this chapter will cover the following topics:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.16.1">Data exploration of the </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Jigsaw Unintended Bias in Toxicity Classification</span></em><span class="koboSpan" id="kobo.18.1"> competition dataset</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.19.1">Introduction to NLP-specific processing and analysis techniques, including word frequency, tokenization, part-of-speech tagging, named entity recognition, and word embeddings</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.20.1">The iterative refinement of the preprocessing of text data to prepare a model baseline</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.21.1">A model baseline for this text classification competition</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-93"><span class="koboSpan" id="kobo.22.1">What is in the data?</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.23.1">The data from the </span><em class="italic"><span class="koboSpan" id="kobo.24.1">Jigsaw Unintended Bias in Toxicity Classification</span></em><span class="koboSpan" id="kobo.25.1"> competition dataset contains 1.8 million rows in the training set and 97,300 rows in the test set. </span><span class="koboSpan" id="kobo.25.2">The test data contains only a </span><strong class="screenText"><span class="koboSpan" id="kobo.26.1">comment</span></strong><span class="koboSpan" id="kobo.27.1"> column and does not contain a target (the value to predict) column. </span><span class="koboSpan" id="kobo.27.2">Training data </span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.28.1">contains, besides the </span><strong class="screenText"><span class="koboSpan" id="kobo.29.1">comment</span></strong><span class="koboSpan" id="kobo.30.1"> column, another 43 columns, including the target feature. </span><span class="koboSpan" id="kobo.30.2">The target is a number between 0 and 1, which represents the annotation that is the objective of the prediction for this competition. </span><span class="koboSpan" id="kobo.30.3">This target value represents the degree of toxicity of a comment (</span><code class="inlineCode"><span class="koboSpan" id="kobo.31.1">0</span></code><span class="koboSpan" id="kobo.32.1"> means zero/no toxicity and </span><code class="inlineCode"><span class="koboSpan" id="kobo.33.1">1</span></code><span class="koboSpan" id="kobo.34.1"> means maximum toxicity), and the other 42 columns are flags related to the presence of certain sensitive topics in the comments. </span><span class="koboSpan" id="kobo.34.2">The topic is related to five categories: race and ethnicity, gender, sexual orientation, religion, and disability. </span><span class="koboSpan" id="kobo.34.3">In more detail, these are the flags per each of the five categories:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.35.1">Race and ethnicity</span></strong><span class="koboSpan" id="kobo.36.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.37.1">asian</span></code><span class="koboSpan" id="kobo.38.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.39.1">black</span></code><span class="koboSpan" id="kobo.40.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.41.1">jewish</span></code><span class="koboSpan" id="kobo.42.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.43.1">latino</span></code><span class="koboSpan" id="kobo.44.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.45.1">other_race_or_ethnicity</span></code><span class="koboSpan" id="kobo.46.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.47.1">white</span></code></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.48.1">Gender</span></strong><span class="koboSpan" id="kobo.49.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.50.1">female</span></code><span class="koboSpan" id="kobo.51.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.52.1">male</span></code><span class="koboSpan" id="kobo.53.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.54.1">transgender</span></code><span class="koboSpan" id="kobo.55.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.56.1">other_gender</span></code></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.57.1">Sexual orientation</span></strong><span class="koboSpan" id="kobo.58.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.59.1">bisexual</span></code><span class="koboSpan" id="kobo.60.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.61.1">heterosexual</span></code><span class="koboSpan" id="kobo.62.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.63.1">homosexual_gay_or_lesbian</span></code><span class="koboSpan" id="kobo.64.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.65.1">other_sexual_orientation</span></code></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.66.1">Religion</span></strong><span class="koboSpan" id="kobo.67.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.68.1">atheist</span></code><span class="koboSpan" id="kobo.69.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.70.1">buddhist</span></code><span class="koboSpan" id="kobo.71.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.72.1">christian</span></code><span class="koboSpan" id="kobo.73.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.74.1">hindu</span></code><span class="koboSpan" id="kobo.75.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.76.1">muslim</span></code><span class="koboSpan" id="kobo.77.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.78.1">other_religion</span></code></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.79.1">Disability</span></strong><span class="koboSpan" id="kobo.80.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.81.1">intellectual_or_learning_disability</span></code><span class="koboSpan" id="kobo.82.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.83.1">other_disability</span></code><span class="koboSpan" id="kobo.84.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.85.1">physical_disability</span></code><span class="koboSpan" id="kobo.86.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.87.1">psychiatric_or_mental_illness</span></code></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.88.1">There are also a few features (a.k.a. </span><span class="koboSpan" id="kobo.88.2">columns in the dataset) that serve for identifying the comment: </span><code class="inlineCode"><span class="koboSpan" id="kobo.89.1">created_data</span></code><span class="koboSpan" id="kobo.90.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.91.1">publication_id</span></code><span class="koboSpan" id="kobo.92.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.93.1">parent_id</span></code><span class="koboSpan" id="kobo.94.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.95.1">article_id</span></code><span class="koboSpan" id="kobo.96.1">. </span><span class="koboSpan" id="kobo.96.2">Also provided are several user feedback information features associated with the comments: </span><code class="inlineCode"><span class="koboSpan" id="kobo.97.1">rating</span></code><span class="koboSpan" id="kobo.98.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.99.1">funny</span></code><span class="koboSpan" id="kobo.100.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.101.1">wow</span></code><span class="koboSpan" id="kobo.102.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.103.1">sad</span></code><span class="koboSpan" id="kobo.104.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.105.1">likes</span></code><span class="koboSpan" id="kobo.106.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.107.1">disagree</span></code><span class="koboSpan" id="kobo.108.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.109.1">sexual_explicit</span></code><span class="koboSpan" id="kobo.110.1">. </span><span class="koboSpan" id="kobo.110.2">Finally, there are also two fields relative to annotations: </span><code class="inlineCode"><span class="koboSpan" id="kobo.111.1">identity_annotator_count</span></code><span class="koboSpan" id="kobo.112.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.113.1">toxicity_annotator_count</span></code><span class="koboSpan" id="kobo.114.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.115.1">Let’s start with a quick analysis of the target feature and the sensitive features.</span></p>
<h2 class="heading-2" id="_idParaDest-94"><span class="koboSpan" id="kobo.116.1">Target feature</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.117.1">We would like to look first </span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.118.1">at the distribution of the target feature. </span><span class="koboSpan" id="kobo.118.2">Let’s look at the histogram for these values’ distribution in </span><em class="italic"><span class="koboSpan" id="kobo.119.1">Figure 7.1</span></em><span class="koboSpan" id="kobo.120.1">:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.121.1"><img alt="A picture containing text, plot, line, diagram  Description automatically generated" src="../Images/B20963_07_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.122.1">Figure 7.1: Distribution of target values (training data, 1.9 million columns)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.123.1">For this histogram, we’ve used a logarithmic scale on the </span><em class="italic"><span class="koboSpan" id="kobo.124.1">y</span></em><span class="koboSpan" id="kobo.125.1"> axis; the reason behind this is that we want to see the skewed distribution of values toward 0. </span><span class="koboSpan" id="kobo.125.2">As we do this, we observe that we have a bimodal distribution: peak values at around 0.1 intervals, decreasing in amplitude, and</span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.126.1"> less frequent values with a slowly rising trend, superposed. </span><span class="koboSpan" id="kobo.126.2">Most of the target values (above 1 million) are </span><code class="inlineCode"><span class="koboSpan" id="kobo.127.1">0</span></code><span class="koboSpan" id="kobo.128.1">.</span></p>
<h2 class="heading-2" id="_idParaDest-95"><span class="koboSpan" id="kobo.129.1">Sensitive features</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.130.1">We will look at the distribution of sensitive features as listed earlier (race and ethnicity, gender, sexual orientation, religion, and </span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.131.1">disability). </span><span class="koboSpan" id="kobo.131.2">We will again use a logarithmic scale on the </span><em class="italic"><span class="koboSpan" id="kobo.132.1">y</span></em><span class="koboSpan" id="kobo.133.1"> axis due to the skewness of the distribution (similar to the target, we have a concentration at </span><code class="inlineCode"><span class="koboSpan" id="kobo.134.1">0</span></code><span class="koboSpan" id="kobo.135.1">).</span></p>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.136.1">Figure 7.2</span></em><span class="koboSpan" id="kobo.137.1"> shows the distribution of race and ethnicity feature values. </span><span class="koboSpan" id="kobo.137.2">These look discontinuous and very discrete, with the histogram showing a few separate peaks:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.138.1"><img alt="A graph with numbers and lines  Description automatically generated with medium confidence" src="../Images/B20963_07_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.139.1">Figure 7.2: Distribution of race and ethnicity feature values</span></p>
<p class="normal"><span class="koboSpan" id="kobo.140.1">We can observe a similar distribution</span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.141.1"> for the gender feature values in </span><em class="italic"><span class="koboSpan" id="kobo.142.1">Figure 7.3</span></em><span class="koboSpan" id="kobo.143.1">:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.144.1"><img alt="A picture containing text, screenshot, plot, line  Description automatically generated" src="../Images/B20963_07_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.145.1">Figure 7.3: Distribution of gender feature values</span></p>
<p class="normal"><span class="koboSpan" id="kobo.146.1">In </span><em class="italic"><span class="koboSpan" id="kobo.147.1">Figure 7.4</span></em><span class="koboSpan" id="kobo.148.1">, we show the distribution of the additional toxicity features (</span><code class="inlineCode"><span class="koboSpan" id="kobo.149.1">severe_toxicity</span></code><span class="koboSpan" id="kobo.150.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.151.1">obscene</span></code><span class="koboSpan" id="kobo.152.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.153.1">identity_attack</span></code><span class="koboSpan" id="kobo.154.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.155.1">insult</span></code><span class="koboSpan" id="kobo.156.1">, or </span><code class="inlineCode"><span class="koboSpan" id="kobo.157.1">threat</span></code><span class="koboSpan" id="kobo.158.1">) values. </span><span class="koboSpan" id="kobo.158.2">As you can see, the distribution is more </span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.159.1">even, and with an increasing trend for </span><code class="inlineCode"><span class="koboSpan" id="kobo.160.1">insult</span></code><span class="koboSpan" id="kobo.161.1">:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.162.1"><img alt="A picture containing screenshot, text, plot, line  Description automatically generated" src="../Images/B20963_07_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.163.1">Figure 7.4: Distribution of additional toxicity feature values</span></p>
<p class="normal"><span class="koboSpan" id="kobo.164.1">Let us also look at the correlation between the target values and the race or ethnicity, gender, sexual orientation, religion, and disability feature values. </span><span class="koboSpan" id="kobo.164.2">We are not showing here the correlation matrix for all the features, but you can inspect it in the notebook associated with this chapter. </span><span class="koboSpan" id="kobo.164.3">Here, we only show the first 16 features correlated with the target, ordered by correlation factor:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.165.1">train_corr[</span><span class="hljs-string"><span class="koboSpan" id="kobo.166.1">'target'</span></span><span class="koboSpan" id="kobo.167.1">].sort_values(ascending=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.168.1">False</span></span><span class="koboSpan" id="kobo.169.1">)[</span><span class="hljs-number"><span class="koboSpan" id="kobo.170.1">1</span></span><span class="koboSpan" id="kobo.171.1">:</span><span class="hljs-number"><span class="koboSpan" id="kobo.172.1">16</span></span><span class="koboSpan" id="kobo.173.1">]
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.174.1">Let’s look at the top 15 features ordered by correlation factor with the target in </span><em class="italic"><span class="koboSpan" id="kobo.175.1">Figure 7.5</span></em><span class="koboSpan" id="kobo.176.1">:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.177.1"><img alt="A screen shot of a computer code  Description automatically generated" src="../Images/B20963_07_05.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.178.1">Figure 7.5: Top 15 correlation factors of other features with the target feature</span></p>
<p class="normal"><span class="koboSpan" id="kobo.179.1">Next, in </span><em class="italic"><span class="koboSpan" id="kobo.180.1">Figure 7.6</span></em><span class="koboSpan" id="kobo.181.1">, we represent</span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.182.1"> the correlation matrix for these selected features and the target feature:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.183.1"><img alt="A chart with numbers and a red line  Description automatically generated with medium confidence" src="../Images/B20963_07_06.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.184.1">Figure 7.6: Correlation matrix between the target and 15 features with the highest correlation with it</span></p>
<p class="normal"><span class="koboSpan" id="kobo.185.1">We can observe that </span><code class="inlineCode"><span class="koboSpan" id="kobo.186.1">target</span></code><span class="koboSpan" id="kobo.187.1"> is highly correlated with </span><code class="inlineCode"><span class="koboSpan" id="kobo.188.1">insult</span></code><span class="koboSpan" id="kobo.189.1"> (0.93), </span><code class="inlineCode"><span class="koboSpan" id="kobo.190.1">obscene</span></code><span class="koboSpan" id="kobo.191.1"> (0.49), and </span><code class="inlineCode"><span class="koboSpan" id="kobo.192.1">identity_attack</span></code><span class="koboSpan" id="kobo.193.1"> (0.45). </span><span class="koboSpan" id="kobo.193.2">Also, </span><code class="inlineCode"><span class="koboSpan" id="kobo.194.1">severe_toxicity</span></code><span class="koboSpan" id="kobo.195.1"> is correlated positively with </span><code class="inlineCode"><span class="koboSpan" id="kobo.196.1">insult</span></code><span class="koboSpan" id="kobo.197.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.198.1">obscene</span></code><span class="koboSpan" id="kobo.199.1">. </span><code class="inlineCode"><span class="koboSpan" id="kobo.200.1">identity_attack</span></code><span class="koboSpan" id="kobo.201.1"> has a </span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.202.1">small correlation with being </span><code class="inlineCode"><span class="koboSpan" id="kobo.203.1">white</span></code><span class="koboSpan" id="kobo.204.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.205.1">black</span></code><span class="koboSpan" id="kobo.206.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.207.1">muslim</span></code><span class="koboSpan" id="kobo.208.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.209.1">homosexual_gay_or_lesbian</span></code><span class="koboSpan" id="kobo.210.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.211.1">We investigated the distribution of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.212.1">target</span></code><span class="koboSpan" id="kobo.213.1"> feature (feature to predict) and of the sensitive features. </span><span class="koboSpan" id="kobo.213.2">Now we will move to the main topic of analysis for this chapter: the comments text. </span><span class="koboSpan" id="kobo.213.3">We will apply several NLP-specific analysis techniques.</span></p>
<h1 class="heading-1" id="_idParaDest-96"><span class="koboSpan" id="kobo.214.1">Analyzing the comments text</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.215.1">NLP is a field of AI that involves the use of computational techniques to enable computers to understand, interpret, transform, and </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.216.1">even generate human language. </span><span class="koboSpan" id="kobo.216.2">NLP uses several techniques, algorithms, and models to process and analyze large datasets of text. </span><span class="koboSpan" id="kobo.216.3">Among </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.217.1">these techniques, we can mention:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.218.1">Tokenization</span></strong><span class="koboSpan" id="kobo.219.1">: Breaks down text into smaller units, like words, parts of words, or characters</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.220.1">Lemmatization or stemming</span></strong><span class="koboSpan" id="kobo.221.1">: Reduces the words to dictionary form or removes the last few characters to get to a common form (stem)</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.222.1">Part-of-Speech </span></strong><span class="koboSpan" id="kobo.223.1">(</span><strong class="keyWord"><span class="koboSpan" id="kobo.224.1">POS</span></strong><span class="koboSpan" id="kobo.225.1">)</span><strong class="keyWord"><span class="koboSpan" id="kobo.226.1"> tagging</span></strong><span class="koboSpan" id="kobo.227.1">: Assigns a grammatical category (for example, nouns, verbs, proper nouns, and adjectives) to each word in a sequence</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.228.1">Named Entity Recognition</span></strong><span class="koboSpan" id="kobo.229.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.230.1">NER</span></strong><span class="koboSpan" id="kobo.231.1">): Identifies </span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.232.1">and classifies entities (for example, names of people, organizations, and places)</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.233.1">Word embeddings</span></strong><span class="koboSpan" id="kobo.234.1">: Use a high-dimensional space to represent the words, a space in which the position of each word is determined by its relationship with other words</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.235.1">Machine learning models</span></strong><span class="koboSpan" id="kobo.236.1">: Train models on annotated datasets to learn patterns and relationships in language data</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.237.1">NLP applications can include sentiment analysis, machine translation, question answering, text summarization, and text classification.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.238.1">With that quick introduction to NLP, let us inspect the actual comment text. </span><span class="koboSpan" id="kobo.238.2">We will build a few word cloud graphs (using a 20,000-comment subset of the entire dataset). </span><span class="koboSpan" id="kobo.238.3">We will look first at the overall word distribution (see the notebook associated with this chapter), then at the distribution of words with target values above 0.75 and below 0.25:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.239.1"><img alt="" role="presentation" src="../Images/B20963_07_07.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.240.1">Figure 7.7: Prevalent words (1-gram) in comments with low target score &lt; 0.25 (left) and high target score &gt; 0.75 (right)</span></p>
<p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.241.1">target</span></code><span class="koboSpan" id="kobo.242.1"> is very highly correlated with </span><code class="inlineCode"><span class="koboSpan" id="kobo.243.1">insult</span></code><span class="koboSpan" id="kobo.244.1">, and we expect to see a rather close distribution of words in the word clouds for the </span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.245.1">two features. </span><span class="koboSpan" id="kobo.245.2">This hypothesis is confirmed, and </span><em class="italic"><span class="koboSpan" id="kobo.246.1">Figure 7.8</span></em><span class="koboSpan" id="kobo.247.1"> illustrates this very well (both for low score and high score):</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.248.1"><img alt="" role="presentation" src="../Images/B20963_07_08.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.249.1">Figure 7.8: Prevalent words (1-gram) in comments with low insult score &lt; 0.25 (left) and high insult score &gt; 0.75 (right)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.250.1">As you can see, distributions show similar words with high frequency for both low scores and high scores for </span><strong class="screenText"><span class="koboSpan" id="kobo.251.1">target</span></strong><span class="koboSpan" id="kobo.252.1"> and </span><strong class="screenText"><span class="koboSpan" id="kobo.253.1">insult</span></strong><span class="koboSpan" id="kobo.254.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.255.1">More word clouds are available in the associated notebook for </span><code class="inlineCode"><span class="koboSpan" id="kobo.256.1">threat</span></code><span class="koboSpan" id="kobo.257.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.258.1">obscene</span></code><span class="koboSpan" id="kobo.259.1">, and other features. </span><span class="koboSpan" id="kobo.259.2">These word clouds give us a good initial intuition for the most frequent words. </span><span class="koboSpan" id="kobo.259.3">We will perform a more detailed analysis of word frequency in the entire corpus vocabulary in the </span><em class="italic"><span class="koboSpan" id="kobo.260.1">Building the vocabulary</span></em><span class="koboSpan" id="kobo.261.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.262.1">Checking vocabulary coverage</span></em><span class="koboSpan" id="kobo.263.1"> sections. </span><span class="koboSpan" id="kobo.263.2">For now, we can observe that the analysis we performed is limited to individual word frequency, without capturing how these words are grouped over the entire corpus – in other words, how various words are used together and, based on this, identifying the main themes in the corpus. </span><span class="koboSpan" id="kobo.263.3">Such processing, aimed to reveal the underlying semantic structure of the entire corpus, is called </span><strong class="keyWord"><span class="koboSpan" id="kobo.264.1">topic modeling</span></strong><span class="koboSpan" id="kobo.265.1">. </span><span class="koboSpan" id="kobo.265.2">The </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.266.1">analysis of co-occurrence patterns of words in this approach allows us to reveal the latent topics existent in the text.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.267.1">The inspiration for the implementation of the topic modeling approach in the associated notebook is from a set of articles and tutorials on topic modeling using latent Dirichlet allocation (see </span><em class="italic"><span class="koboSpan" id="kobo.268.1">References</span></em> <em class="italic"><span class="koboSpan" id="kobo.269.1">5</span></em><span class="koboSpan" id="kobo.270.1">–</span><em class="italic"><span class="koboSpan" id="kobo.271.1">10</span></em><span class="koboSpan" id="kobo.272.1">). </span></p>
<h2 class="heading-2" id="_idParaDest-97"><span class="koboSpan" id="kobo.273.1">Topic modeling</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.274.1">We start by preprocessing the comments text, using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.275.1">gensim</span></code><span class="koboSpan" id="kobo.276.1"> library to eliminate special characters, frequently used </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.277.1">words, connection words (or stopwords), and </span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.278.1">words with lengths less than 2:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.279.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.280.1">preprocess</span></span><span class="koboSpan" id="kobo.281.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.282.1">text</span></span><span class="koboSpan" id="kobo.283.1">):
    result = []
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.284.1">for</span></span><span class="koboSpan" id="kobo.285.1"> token </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.286.1">in</span></span><span class="koboSpan" id="kobo.287.1"> gensim.utils.simple_preprocess(text):
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.288.1">if</span></span><span class="koboSpan" id="kobo.289.1"> token </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.290.1">not</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.291.1">in</span></span><span class="koboSpan" id="kobo.292.1"> gensim.parsing.preprocessing.STOPWORDS </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.293.1">and</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.294.1">len</span></span><span class="koboSpan" id="kobo.295.1">(token) &gt; </span><span class="hljs-number"><span class="koboSpan" id="kobo.296.1">2</span></span><span class="koboSpan" id="kobo.297.1">:
            result.append(token)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.298.1">return</span></span><span class="koboSpan" id="kobo.299.1"> result
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.300.1">The following code applies the defined </span><code class="inlineCode"><span class="koboSpan" id="kobo.301.1">preprocess</span></code><span class="koboSpan" id="kobo.302.1"> function to all the comments:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.303.1">%%time
preprocessed_comments = train_subsample[</span><span class="hljs-string"><span class="koboSpan" id="kobo.304.1">'comment_text'</span></span><span class="koboSpan" id="kobo.305.1">].</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.306.1">map</span></span><span class="koboSpan" id="kobo.307.1">(preprocess)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.308.1">Then, we create a dictionary of words using </span><code class="inlineCode"><span class="koboSpan" id="kobo.309.1">dictionary</span></code><span class="koboSpan" id="kobo.310.1"> from </span><code class="inlineCode"><span class="koboSpan" id="kobo.311.1">gensim</span></code><span class="koboSpan" id="kobo.312.1">/</span><code class="inlineCode"><span class="koboSpan" id="kobo.313.1">corpora</span></code><span class="koboSpan" id="kobo.314.1">. </span><span class="koboSpan" id="kobo.314.2">We also filter extremes, to eliminate less frequent words and limit the size of the vocabulary:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.315.1">%%time
dictionary = gensim.corpora.Dictionary(preprocessed_comments)
dictionary.filter_extremes(no_below=</span><span class="hljs-number"><span class="koboSpan" id="kobo.316.1">10</span></span><span class="koboSpan" id="kobo.317.1">, no_above=</span><span class="hljs-number"><span class="koboSpan" id="kobo.318.1">0.5</span></span><span class="koboSpan" id="kobo.319.1">, keep_n=</span><span class="hljs-number"><span class="koboSpan" id="kobo.320.1">75000</span></span><span class="koboSpan" id="kobo.321.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.322.1">With these restrictions, we go then to the next step and generate a </span><em class="italic"><span class="koboSpan" id="kobo.323.1">bag of words</span></em><span class="koboSpan" id="kobo.324.1"> (</span><code class="inlineCode"><span class="koboSpan" id="kobo.325.1">bow</span></code><span class="koboSpan" id="kobo.326.1">) corpus from the dictionary. </span><span class="koboSpan" id="kobo.326.2">Then we apply </span><strong class="keyWord"><span class="koboSpan" id="kobo.327.1">TF-IDF</span></strong><span class="koboSpan" id="kobo.328.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.329.1">Term Frequency-Inverse Document Frequency</span></strong><span class="koboSpan" id="kobo.330.1">) to this corpus, which </span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.331.1">provides a numerical representation of the importance of a word within a document in a collection or corpus of documents.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.332.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.333.1">tf</span></code><span class="koboSpan" id="kobo.334.1"> component measures how frequently a word appears in a document. </span><span class="koboSpan" id="kobo.334.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.335.1">idf</span></code><span class="koboSpan" id="kobo.336.1"> component shows the significance of a word across the entire corpus of documents (in our case, over the full set of comments). </span><span class="koboSpan" id="kobo.336.2">This factor decreases with a higher occurrence of a term in the documents. </span><span class="koboSpan" id="kobo.336.3">Therefore, after the </span><code class="inlineCode"><span class="koboSpan" id="kobo.337.1">tfidf</span></code><span class="koboSpan" id="kobo.338.1"> transform, the coefficient for one word and one document is larger for words that are infrequent at the corpus level and appear with higher frequency</span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.339.1"> inside the current document:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.340.1">%%time
bow_corpus = [dictionary.doc2bow(doc) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.341.1">for</span></span><span class="koboSpan" id="kobo.342.1"> doc </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.343.1">in</span></span><span class="koboSpan" id="kobo.344.1"> preprocessed_comments]
tfidf = models.TfidfModel(bow_corpus)
corpus_tfidf = tfidf[bow_corpus]
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.345.1">We then apply </span><em class="italic"><span class="koboSpan" id="kobo.346.1">Latent Dirichlet Allocation</span></em><span class="koboSpan" id="kobo.347.1"> (</span><code class="inlineCode"><span class="koboSpan" id="kobo.348.1">lda</span></code><span class="koboSpan" id="kobo.349.1">), a topic model that generates topics based on word frequency on this corpus, using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.350.1">gensim</span></code><span class="koboSpan" id="kobo.351.1"> implementation for parallel processing (</span><code class="inlineCode"><span class="koboSpan" id="kobo.352.1">LdaMulticore</span></code><span class="koboSpan" id="kobo.353.1">):</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.354.1">%%time
lda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=</span><span class="hljs-number"><span class="koboSpan" id="kobo.355.1">20</span></span><span class="koboSpan" id="kobo.356.1">,
                                    id2word=dictionary, passes=</span><span class="hljs-number"><span class="koboSpan" id="kobo.357.1">2</span></span><span class="koboSpan" id="kobo.358.1">, workers=</span><span class="hljs-number"><span class="koboSpan" id="kobo.359.1">2</span></span><span class="koboSpan" id="kobo.360.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.361.1">Let’s represent the first 10 topics, with 5 words for each of them:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.362.1">topics = lda_model.print_topics(num_words=</span><span class="hljs-number"><span class="koboSpan" id="kobo.363.1">5</span></span><span class="koboSpan" id="kobo.364.1">)
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.365.1">for</span></span><span class="koboSpan" id="kobo.366.1"> i, topic </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.367.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.368.1">enumerate</span></span><span class="koboSpan" id="kobo.369.1">(topics[:</span><span class="hljs-number"><span class="koboSpan" id="kobo.370.1">10</span></span><span class="koboSpan" id="kobo.371.1">]):
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.372.1">print</span></span><span class="koboSpan" id="kobo.373.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.374.1">"Train topic {}: {}"</span></span><span class="koboSpan" id="kobo.375.1">.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.376.1">format</span></span><span class="koboSpan" id="kobo.377.1">(i, topic))
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.378.1">The topic words are </span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.379.1">shown with the associated relative weight in the topic, as shown here:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.380.1"><img alt="" role="presentation" src="../Images/B20963_07_09.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.381.1">Figure 7.9: Top 10 topics, with 5 words (most relevant) selected per topic</span></p>
<p class="normal"><span class="koboSpan" id="kobo.382.1">Once we extract the topics, we can go through the documents and identify which topics are present in the current document (in our case, comment). </span><span class="koboSpan" id="kobo.382.2">In </span><em class="italic"><span class="koboSpan" id="kobo.383.1">Figure 7.10</span></em><span class="koboSpan" id="kobo.384.1">, we show the dominant topics (with the relative weights) for one document (the following is the code used to generate the list of</span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.385.1"> topics for a selected comment):</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.386.1">for</span></span><span class="koboSpan" id="kobo.387.1"> index, score </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.388.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.389.1">sorted</span></span><span class="koboSpan" id="kobo.390.1">(lda_model[bd5], key=</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.391.1">lambda</span></span><span class="koboSpan" id="kobo.392.1"> tup: -</span><span class="hljs-number"><span class="koboSpan" id="kobo.393.1">1</span></span><span class="koboSpan" id="kobo.394.1">*tup[</span><span class="hljs-number"><span class="koboSpan" id="kobo.395.1">1</span></span><span class="koboSpan" id="kobo.396.1">]):
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.397.1">print</span></span><span class="koboSpan" id="kobo.398.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.399.1">"\nScore: {}\t \nTopic: {}"</span></span><span class="koboSpan" id="kobo.400.1">.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.401.1">format</span></span><span class="koboSpan" id="kobo.402.1">(score, lda_model.print_topic(index, </span><span class="hljs-number"><span class="koboSpan" id="kobo.403.1">5</span></span><span class="koboSpan" id="kobo.404.1">)))
</span></code></pre>
<figure class="mediaobject"><span class="koboSpan" id="kobo.405.1"><img alt="" role="presentation" src="../Images/B20963_07_10.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.406.1">Figure 7.10: Topics associated (each with relative importance) with one comment</span></p>
<p class="normal"><span class="koboSpan" id="kobo.407.1">We prefer the </span><strong class="keyWord"><span class="koboSpan" id="kobo.408.1">pyLDAvis</span></strong><span class="koboSpan" id="kobo.409.1"> visualization tool to </span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.410.1">represent the topics. </span><span class="koboSpan" id="kobo.410.2">In </span><em class="italic"><span class="koboSpan" id="kobo.411.1">Figure 7.11</span></em><span class="koboSpan" id="kobo.412.1">, we show a screenshot of this tool (in the notebook, we generated 20 topics for train data and, separately, for test data). </span><span class="koboSpan" id="kobo.412.2">The dashboard in </span><em class="italic"><span class="koboSpan" id="kobo.413.1">Figure 7.11</span></em><span class="koboSpan" id="kobo.414.1"> displays the </span><strong class="screenText"><span class="koboSpan" id="kobo.415.1">Intertopic Distance Map</span></strong><span class="koboSpan" id="kobo.416.1">. </span><span class="koboSpan" id="kobo.416.2">Here, the topic’s relative dimension (or influence) is represented by the size of the disks and the topic’s relative </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.417.1">distance by their mutual distance. </span><span class="koboSpan" id="kobo.417.2">On the right side, for the currently selected topic (in the left-side panel), we can see the top 30 most relevant terms per topic.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.418.1">The disks with a light color (blue in the notebook) represent the overall word frequency. </span><span class="koboSpan" id="kobo.418.2">The darker colored disks (red in the notebook) represent the estimated word frequency within the selected topic. </span><span class="koboSpan" id="kobo.418.3">We can use a slide as well to adjust the relevance metric (in the picture, this is set to </span><code class="inlineCode"><span class="koboSpan" id="kobo.419.1">1</span></code><span class="koboSpan" id="kobo.420.1">). </span><span class="koboSpan" id="kobo.420.2">We can further refine this analysis by improving the preprocessing step (for example, we can add more stopwords, specific to this corpus), adjusting the parameters for the dictionary formation, and controlling the parameters for </span><code class="inlineCode"><span class="koboSpan" id="kobo.421.1">tfidf</span></code><span class="koboSpan" id="kobo.422.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.423.1">lda</span></code><span class="koboSpan" id="kobo.424.1">. </span><span class="koboSpan" id="kobo.424.2">Due to the complexity of the LDA procedure, we also reduced the size of the corpus, by subsampling the train data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.425.1">As shown in the following screenshot, on the left panel of the topic modeling dashboard generated using the pyLDAvis tool, we see the </span><strong class="screenText"><span class="koboSpan" id="kobo.426.1">Intertopic Distance Map</span></strong><span class="koboSpan" id="kobo.427.1"> – with relative dimension of topic influence in the corpus and the relative topic’s distance:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.428.1"><img alt="" role="presentation" src="../Images/B20963_07_11.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.429.1">Figure 7.11: Topic modeling dashboard generated using the pyLDAvis tool (left panel)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.430.1">On the right-side panel of the</span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.431.1"> topic modeling dashboard generated using the pyLDAvis tool, for the selected topic, we see the top 30 most relevant terms per topic, with blue for the overall term</span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.432.1"> frequency in the corpus and red for the estimated term frequency within the selected topic:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.433.1"><img alt="" role="presentation" src="../Images/B20963_07_12.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.434.1">Figure 7.12: Topic modeling dashboard generated using the pyLDAvis tool (right panel)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.435.1">We can repeat the analysis </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.436.1">over the entire corpus, but this will require </span><a id="_idIndexMarker317"/><span class="koboSpan" id="kobo.437.1">more computational resources than are available on Kaggle.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.438.1">And with that, we have explored the topics in the comments text corpus, using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.439.1">lda</span></code><span class="koboSpan" id="kobo.440.1"> method. </span><span class="koboSpan" id="kobo.440.2">With this procedure, we revealed one of the hidden (or latent) structures in the corpus of the text. </span><span class="koboSpan" id="kobo.440.3">Now we can better understand not only the frequency of the words but also how words are associated in comments, to form topics discussed by the commentators. </span><span class="koboSpan" id="kobo.440.4">Let’s continue to explore the corpus, from a different perspective. </span><span class="koboSpan" id="kobo.440.5">We will take every comment and analyze, using NER, what types of concepts are present in the text. </span><span class="koboSpan" id="kobo.440.6">Then, we will start to look at the syntactic elements and use POS tagging to extract the nouns, verbs, adjectives, and other parts of speech.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.441.1">The reason we review these NLP techniques is twofold. </span><span class="koboSpan" id="kobo.441.2">First, we want to give you a glimpse of the richness of the </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.442.1">tools and techniques available in NLP. </span><span class="koboSpan" id="kobo.442.2">Second, for more complex machine learning models, you can include features derived </span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.443.1">using these methods. </span><span class="koboSpan" id="kobo.443.2">For example, you can add, besides other features extracted from the text, features obtained by the use of NER or POS tagging.</span></p>
<h2 class="heading-2" id="_idParaDest-98"><span class="koboSpan" id="kobo.444.1">Named entity recognition</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.445.1">Let’s perform NER on a selection of comments. </span><span class="koboSpan" id="kobo.445.2">NER is an information extraction task that aims to identify and extract named entities in </span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.446.1">unstructured data (text). </span><span class="koboSpan" id="kobo.446.2">Named entities are people, organizations, geographical places, dates and times, amounts, and currencies. </span><span class="koboSpan" id="kobo.446.3">There are several available methods to identify and</span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.447.1"> extract named entities, with the most frequently used being </span><code class="inlineCode"><span class="koboSpan" id="kobo.448.1">spacy</span></code><span class="koboSpan" id="kobo.449.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.450.1">transformers</span></code><span class="koboSpan" id="kobo.451.1">. </span><span class="koboSpan" id="kobo.451.2">In our case, we will use </span><code class="inlineCode"><span class="koboSpan" id="kobo.452.1">spacy</span></code><span class="koboSpan" id="kobo.453.1"> to perform NER. </span><span class="koboSpan" id="kobo.453.2">We prefer </span><code class="inlineCode"><span class="koboSpan" id="kobo.454.1">spacy</span></code><span class="koboSpan" id="kobo.455.1"> because it requires fewer resources compared with transformers and yet gives good results. </span><span class="koboSpan" id="kobo.455.2">Something to note here is that </span><code class="inlineCode"><span class="koboSpan" id="kobo.456.1">spacy</span></code><span class="koboSpan" id="kobo.457.1"> is also available in 23 languages, including English, Portuguese, Spanish, Russian, and Chinese.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.458.1">First, we initialize an </span><code class="inlineCode"><span class="koboSpan" id="kobo.459.1">nlp</span></code><span class="koboSpan" id="kobo.460.1"> object using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.461.1">spacy.load</span></code><span class="koboSpan" id="kobo.462.1"> function:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.463.1">import</span></span><span class="koboSpan" id="kobo.464.1"> spacy
nlp = spacy.load(</span><span class="hljs-string"><span class="koboSpan" id="kobo.465.1">'en_core_web_sm'</span></span><span class="koboSpan" id="kobo.466.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.467.1">This will load the </span><code class="inlineCode"><span class="koboSpan" id="kobo.468.1">'en_core_web_sm'</span></code><span class="koboSpan" id="kobo.469.1"> (</span><code class="inlineCode"><span class="koboSpan" id="kobo.470.1">sm</span></code><span class="koboSpan" id="kobo.471.1"> stands for small) </span><code class="inlineCode"><span class="koboSpan" id="kobo.472.1">spacy</span></code><span class="koboSpan" id="kobo.473.1"> pipeline, which includes the </span><code class="inlineCode"><span class="koboSpan" id="kobo.474.1">tok2vec</span></code><span class="koboSpan" id="kobo.475.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.476.1">tagger</span></code><span class="koboSpan" id="kobo.477.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.478.1">parser</span></code><span class="koboSpan" id="kobo.479.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.480.1">senter</span></code><span class="koboSpan" id="kobo.481.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.482.1">ner</span></code><span class="koboSpan" id="kobo.483.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.484.1">attribute_ruler</span></code><span class="koboSpan" id="kobo.485.1">, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.486.1">lemmatizer</span></code><span class="koboSpan" id="kobo.487.1"> components. </span><span class="koboSpan" id="kobo.487.2">We will not use all the functionality provided by this pipeline; we are interested in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.488.1">nlp</span></code><span class="koboSpan" id="kobo.489.1"> component.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.490.1">Then, we create a selection of comments. </span><span class="koboSpan" id="kobo.490.2">We filter documents that contain either the name </span><code class="inlineCode"><span class="koboSpan" id="kobo.491.1">Obama</span></code><span class="koboSpan" id="kobo.492.1"> or the name </span><code class="inlineCode"><span class="koboSpan" id="kobo.493.1">Trump</span></code><span class="koboSpan" id="kobo.494.1"> and have less than 100 characters. </span><span class="koboSpan" id="kobo.494.2">For the purpose of this demonstration, we do not want to manipulate large sentences; it will be easier to follow the demonstrations if we operate with smaller sentences. </span><span class="koboSpan" id="kobo.494.3">The next code fragment will perform the selection:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.495.1">selected_text = train.loc[train[</span><span class="hljs-string"><span class="koboSpan" id="kobo.496.1">'comment_text'</span></span><span class="koboSpan" id="kobo.497.1">].</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.498.1">str</span></span><span class="koboSpan" id="kobo.499.1">.contains(</span><span class="hljs-string"><span class="koboSpan" id="kobo.500.1">"Trump"</span></span><span class="koboSpan" id="kobo.501.1">) | train[</span><span class="hljs-string"><span class="koboSpan" id="kobo.502.1">'comment_text'</span></span><span class="koboSpan" id="kobo.503.1">].</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.504.1">str</span></span><span class="koboSpan" id="kobo.505.1">.contains(</span><span class="hljs-string"><span class="koboSpan" id="kobo.506.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.507.1">Obama"</span></span><span class="koboSpan" id="kobo.508.1">)] 
selected_text[</span><span class="hljs-string"><span class="koboSpan" id="kobo.509.1">"len"</span></span><span class="koboSpan" id="kobo.510.1">] = selected_text[</span><span class="hljs-string"><span class="koboSpan" id="kobo.511.1">'comment_text'</span></span><span class="koboSpan" id="kobo.512.1">].apply(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.513.1">lambda</span></span><span class="koboSpan" id="kobo.514.1"> x: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.515.1">len</span></span><span class="koboSpan" id="kobo.516.1">(x))
selected_text = selected_text.loc[selected_text.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.517.1">len</span></span><span class="koboSpan" id="kobo.518.1"> &lt; </span><span class="hljs-number"><span class="koboSpan" id="kobo.519.1">100</span></span><span class="koboSpan" id="kobo.520.1">]
selected_text.shape
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.521.1">We can visualize the result of applying NER in two ways. </span><span class="koboSpan" id="kobo.521.2">One way is to print out the text’s start and end characters and the entity label for each entity identified in the current comment. </span><span class="koboSpan" id="kobo.521.3">An alternative way is to use </span><code class="inlineCode"><span class="koboSpan" id="kobo.522.1">displacy</span></code><span class="koboSpan" id="kobo.523.1"> rendering from </span><code class="inlineCode"><span class="koboSpan" id="kobo.524.1">spacy</span></code><span class="koboSpan" id="kobo.525.1">, which will decorate each entity with a selected </span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.526.1">color and add the entity name beside the entity text (see </span><em class="italic"><span class="koboSpan" id="kobo.527.1">Figure 7.13</span></em><span class="koboSpan" id="kobo.528.1">).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.529.1">The following code is for the</span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.530.1"> extraction of entities using </span><code class="inlineCode"><span class="koboSpan" id="kobo.531.1">nlp</span></code><span class="koboSpan" id="kobo.532.1"> and the preparation of visualization using </span><code class="inlineCode"><span class="koboSpan" id="kobo.533.1">displacy</span></code><span class="koboSpan" id="kobo.534.1">. </span><span class="koboSpan" id="kobo.534.2">Before showing the annotated text using </span><code class="inlineCode"><span class="koboSpan" id="kobo.535.1">displacy</span></code><span class="koboSpan" id="kobo.536.1">, we are printing each entity text, followed by its position (start and end character positions) and the entity label:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.537.1">for</span></span><span class="koboSpan" id="kobo.538.1"> sentence </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.539.1">in</span></span><span class="koboSpan" id="kobo.540.1"> selected_text[</span><span class="hljs-string"><span class="koboSpan" id="kobo.541.1">"comment_text"</span></span><span class="koboSpan" id="kobo.542.1">].head(</span><span class="hljs-number"><span class="koboSpan" id="kobo.543.1">5</span></span><span class="koboSpan" id="kobo.544.1">):
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.545.1">print</span></span><span class="koboSpan" id="kobo.546.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.547.1">"\n"</span></span><span class="koboSpan" id="kobo.548.1">)
    doc = nlp(sentence)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.549.1">for</span></span><span class="koboSpan" id="kobo.550.1"> ent </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.551.1">in</span></span><span class="koboSpan" id="kobo.552.1"> doc.ents:
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.553.1">print</span></span><span class="koboSpan" id="kobo.554.1">(ent.text, ent.start_char, ent.end_char, ent.label_)
    displacy.render(doc, style=</span><span class="hljs-string"><span class="koboSpan" id="kobo.555.1">"ent"</span></span><span class="koboSpan" id="kobo.556.1">,jupyter=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.557.1">True</span></span><span class="koboSpan" id="kobo.558.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.559.1">There are multiple labels predefined in </span><code class="inlineCode"><span class="koboSpan" id="kobo.560.1">spacy</span></code> <code class="inlineCode"><span class="koboSpan" id="kobo.561.1">nlp</span></code><span class="koboSpan" id="kobo.562.1">. </span><span class="koboSpan" id="kobo.562.2">We can extract the meaning of each one with a simple piece of code:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.563.1">import</span></span><span class="koboSpan" id="kobo.564.1"> spacy
nlp = spacy.load(</span><span class="hljs-string"><span class="koboSpan" id="kobo.565.1">"en_core_web_sm"</span></span><span class="koboSpan" id="kobo.566.1">)
labels = nlp.get_pipe(</span><span class="hljs-string"><span class="koboSpan" id="kobo.567.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.568.1">ner"</span></span><span class="koboSpan" id="kobo.569.1">).labels
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.570.1">for</span></span><span class="koboSpan" id="kobo.571.1"> label </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.572.1">in</span></span><span class="koboSpan" id="kobo.573.1"> labels:
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.574.1">print</span></span><span class="koboSpan" id="kobo.575.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.576.1">f"</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.577.1">{label}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.578.1"> - </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.579.1">{spacy.explain(label)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.580.1">"</span></span><span class="koboSpan" id="kobo.581.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.582.1">Here is the resulting list of labels and the meaning of each one:</span></p>
<ul>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.583.1">CARDINAL</span></strong><span class="koboSpan" id="kobo.584.1">: Numerals that do not fall under another type</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.585.1">DATE</span></strong><span class="koboSpan" id="kobo.586.1">: Absolute or relative dates or periods</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.587.1">EVENT</span></strong><span class="koboSpan" id="kobo.588.1">: Named hurricanes, battles, wars, sports events, and so on</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.589.1">FAC</span></strong><span class="koboSpan" id="kobo.590.1">: Buildings, airports, highways, bridges, and so on</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.591.1">GPE</span></strong><span class="koboSpan" id="kobo.592.1">: Countries, cities, or states</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.593.1">LANGUAGE</span></strong><span class="koboSpan" id="kobo.594.1">: Any named language</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.595.1">LAW</span></strong><span class="koboSpan" id="kobo.596.1">: Named documents made into laws</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.597.1">LOC</span></strong><span class="koboSpan" id="kobo.598.1">: Non-GPE locations, mountain ranges, or bodies of water</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.599.1">MONEY</span></strong><span class="koboSpan" id="kobo.600.1">: Monetary values, including units</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.601.1">NORP</span></strong><span class="koboSpan" id="kobo.602.1">: Nationalities or religious or political groups</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.603.1">ORDINAL</span></strong><span class="koboSpan" id="kobo.604.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.605.1">first</span></code><span class="koboSpan" id="kobo.606.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.607.1">second</span></code><span class="koboSpan" id="kobo.608.1">, and so on</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.609.1">ORG</span></strong><span class="koboSpan" id="kobo.610.1">: Companies, agencies, institutions, and so on</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.611.1">PERCENT</span></strong><span class="koboSpan" id="kobo.612.1">: Percentage, including </span><code class="inlineCode"><span class="koboSpan" id="kobo.613.1">%</span></code></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.614.1">PERSON</span></strong><span class="koboSpan" id="kobo.615.1">: People, including fictional</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.616.1">PRODUCT</span></strong><span class="koboSpan" id="kobo.617.1">: Objects, vehicles, foods, and so on (not services)</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.618.1">QUANTITY</span></strong><span class="koboSpan" id="kobo.619.1">: Measurements, such as weight or distance</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.620.1">TIME</span></strong><span class="koboSpan" id="kobo.621.1">: Times less than a day</span></li>
<li class="bulletList"><strong class="screenText"><span class="koboSpan" id="kobo.622.1">WORK_OF_ART</span></strong><span class="koboSpan" id="kobo.623.1">: Titles of </span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.624.1">books, songs, and so on</span></li>
</ul>
<figure class="mediaobject"><span class="koboSpan" id="kobo.625.1"><img alt="A picture containing text, screenshot, font, line  Description automatically generated" src="../Images/B20963_07_13.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.626.1">Figure 7.13: NER using spacy and displacy for visualization of NER results</span></p>
<p class="normal"><span class="koboSpan" id="kobo.627.1">In the top example of the preceding screenshot, Bernie (Sanders) is recognized correctly as a person (</span><strong class="screenText"><span class="koboSpan" id="kobo.628.1">PERSON</span></strong><span class="koboSpan" id="kobo.629.1">), while (Donald) Trump</span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.630.1"> is identified as an organization (</span><strong class="screenText"><span class="koboSpan" id="kobo.631.1">ORG</span></strong><span class="koboSpan" id="kobo.632.1">). </span><span class="koboSpan" id="kobo.632.2">This might be because former president Trump frequently used his name as part of the name of several of the organizations he founded while being a businessperson. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.633.1">In the bottom example, Obama (also a former president and a frequent topic in disputed political debates) is recognized correctly as </span><strong class="screenText"><span class="koboSpan" id="kobo.634.1">PERSON</span></strong><span class="koboSpan" id="kobo.635.1">. </span><span class="koboSpan" id="kobo.635.2">In both cases, we are also showing the list of extracted entities, complemented with the starting and ending positions of each identified entity.</span></p>
<h2 class="heading-2" id="_idParaDest-99"><span class="koboSpan" id="kobo.636.1">POS tagging</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.637.1">With NER analysis, we identified names specific to various entities like people, organizations, places, and so on. </span><span class="koboSpan" id="kobo.637.2">These</span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.638.1"> help us to associate various terms with a certain semantic group. </span><span class="koboSpan" id="kobo.638.2">We can go further and</span><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.639.1"> explore the comments text so that we understand what POS (like noun or verb) each word is, and understand the syntax of each phrase.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.640.1">Let’s start with using </span><code class="inlineCode"><span class="koboSpan" id="kobo.641.1">nltk</span></code><span class="koboSpan" id="kobo.642.1"> (an alternative </span><code class="inlineCode"><span class="koboSpan" id="kobo.643.1">nlp</span></code><span class="koboSpan" id="kobo.644.1"> library) to extract parts of speech from the same small selection of phrases we used for NER experiments. </span><span class="koboSpan" id="kobo.644.2">We chose </span><code class="inlineCode"><span class="koboSpan" id="kobo.645.1">nltk</span></code><span class="koboSpan" id="kobo.646.1"> here because, as well as being even less resource-hungry than spacy, it provides good-quality results. </span><span class="koboSpan" id="kobo.646.2">We also want to be able to compare the results of both (</span><code class="inlineCode"><span class="koboSpan" id="kobo.647.1">spacy</span></code><span class="koboSpan" id="kobo.648.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.649.1">nltk</span></code><span class="koboSpan" id="kobo.650.1">):</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.651.1">for</span></span><span class="koboSpan" id="kobo.652.1"> sentence </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.653.1">in</span></span><span class="koboSpan" id="kobo.654.1"> selected_text[</span><span class="hljs-string"><span class="koboSpan" id="kobo.655.1">"comment_text"</span></span><span class="koboSpan" id="kobo.656.1">].head(</span><span class="hljs-number"><span class="koboSpan" id="kobo.657.1">5</span></span><span class="koboSpan" id="kobo.658.1">):
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.659.1">print</span></span><span class="koboSpan" id="kobo.660.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.661.1">"\n"</span></span><span class="koboSpan" id="kobo.662.1">)
    tokens = twt().tokenize(sentence)
    tags = nltk.pos_tag(tokens, tagset = </span><span class="hljs-string"><span class="koboSpan" id="kobo.663.1">"universal"</span></span><span class="koboSpan" id="kobo.664.1">)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.665.1">for</span></span><span class="koboSpan" id="kobo.666.1"> tag </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.667.1">in</span></span><span class="koboSpan" id="kobo.668.1"> tags:
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.669.1">print</span></span><span class="koboSpan" id="kobo.670.1">(tag, end=</span><span class="hljs-string"><span class="koboSpan" id="kobo.671.1">" "</span></span><span class="koboSpan" id="kobo.672.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.673.1">The results will be as follows:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.674.1"><img alt="" role="presentation" src="../Images/B20963_07_14.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.675.1">Figure 7.14: POS tagging using nltk</span></p>
<p class="normal"><span class="koboSpan" id="kobo.676.1">We can perform the same analysis using spacy as well:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.677.1">for</span></span><span class="koboSpan" id="kobo.678.1"> sentence </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.679.1">in</span></span><span class="koboSpan" id="kobo.680.1"> selected_text[</span><span class="hljs-string"><span class="koboSpan" id="kobo.681.1">"comment_text"</span></span><span class="koboSpan" id="kobo.682.1">].head(</span><span class="hljs-number"><span class="koboSpan" id="kobo.683.1">5</span></span><span class="koboSpan" id="kobo.684.1">):
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.685.1">print</span></span><span class="koboSpan" id="kobo.686.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.687.1">"\n"</span></span><span class="koboSpan" id="kobo.688.1">)
    doc = nlp(sentence)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.689.1">for</span></span><span class="koboSpan" id="kobo.690.1"> token </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.691.1">in</span></span><span class="koboSpan" id="kobo.692.1"> doc:
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.693.1">print</span></span><span class="koboSpan" id="kobo.694.1">(token.text, token.pos_, token.ent_type_, end=</span><span class="hljs-string"><span class="koboSpan" id="kobo.695.1">" | "</span></span><span class="koboSpan" id="kobo.696.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.697.1">The </span><a id="_idIndexMarker328"/><span class="koboSpan" id="kobo.698.1">results will be as </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.699.1">follows:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.700.1"><img alt="" role="presentation" src="../Images/B20963_07_15.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.701.1">Figure 7.15: POS tagging using spacy</span></p>
<p class="normal"><span class="koboSpan" id="kobo.702.1">Let’s compare the two outputs in </span><em class="italic"><span class="koboSpan" id="kobo.703.1">Figures 7.14</span></em><span class="koboSpan" id="kobo.704.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.705.1">7.15</span></em><span class="koboSpan" id="kobo.706.1">. </span><span class="koboSpan" id="kobo.706.2">The two libraries generate slightly different POS results. </span><span class="koboSpan" id="kobo.706.3">Some of the differences are due to the different mapping of actual parts of speech on categories. </span><span class="koboSpan" id="kobo.706.4">For </span><code class="inlineCode"><span class="koboSpan" id="kobo.707.1">nltk</span></code><span class="koboSpan" id="kobo.708.1">, the word “is” represents an </span><code class="inlineCode"><span class="koboSpan" id="kobo.709.1">AUX</span></code><span class="koboSpan" id="kobo.710.1"> (auxiliary), while the same “is” for </span><code class="inlineCode"><span class="koboSpan" id="kobo.711.1">spacy</span></code><span class="koboSpan" id="kobo.712.1"> is a verb. </span><code class="inlineCode"><span class="koboSpan" id="kobo.713.1">spacy</span></code><span class="koboSpan" id="kobo.714.1"> distinguishes between proper nouns (names of persons, places, and so on) and regular nouns (</span><code class="inlineCode"><span class="koboSpan" id="kobo.715.1">NOUN</span></code><span class="koboSpan" id="kobo.716.1">), whereas </span><code class="inlineCode"><span class="koboSpan" id="kobo.717.1">nltk</span></code><span class="koboSpan" id="kobo.718.1"> does not differentiate.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.719.1">With some phrases having a non-standard structure, both outputs wrongly identify the verb “Go” as a noun (</span><code class="inlineCode"><span class="koboSpan" id="kobo.720.1">nltk</span></code><span class="koboSpan" id="kobo.721.1">) and a proper noun (</span><code class="inlineCode"><span class="koboSpan" id="kobo.722.1">spacy</span></code><span class="koboSpan" id="kobo.723.1">). </span><span class="koboSpan" id="kobo.723.2">In the case of </span><code class="inlineCode"><span class="koboSpan" id="kobo.724.1">spacy</span></code><span class="koboSpan" id="kobo.725.1">, it is somewhat expected, since “Go” is written in uppercase after a comma. </span><span class="koboSpan" id="kobo.725.2">spacy differentiates between a coordinating conjunction (</span><strong class="keyWord"><span class="koboSpan" id="kobo.726.1">CCONJ</span></strong><span class="koboSpan" id="kobo.727.1">) and a subordinating conjunction (</span><strong class="keyWord"><span class="koboSpan" id="kobo.728.1">SCONJ</span></strong><span class="koboSpan" id="kobo.729.1">), while </span><code class="inlineCode"><span class="koboSpan" id="kobo.730.1">nltk</span></code><span class="koboSpan" id="kobo.731.1"> will only recognize that there are conjunctions (</span><strong class="keyWord"><span class="koboSpan" id="kobo.732.1">CONJ</span></strong><span class="koboSpan" id="kobo.733.1">).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.734.1">With the same library extension for spacy that we used to highlight NER in the previous subsection, we can also represent the syntactic structure of phrases and paragraphs. </span><span class="koboSpan" id="kobo.734.2">In </span><em class="italic"><span class="koboSpan" id="kobo.735.1">Figure 7.16</span></em><span class="koboSpan" id="kobo.736.1">, we show one example of </span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.737.1">such a representation. </span><span class="koboSpan" id="kobo.737.2">In the notebook, we show</span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.738.1"> all the comment (set of phrases) visualizations using </span><code class="inlineCode"><span class="koboSpan" id="kobo.739.1">displacy</span></code><span class="koboSpan" id="kobo.740.1"> with the “dep” (dependency) flag.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.741.1"><img alt="A picture containing diagram, sketch  Description automatically generated" src="../Images/B20963_07_16.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.742.1">Figure 7.16: POS tagging using spacy and dependency to show phrase structure with the dependency between the parts of speech</span></p>
<p class="normal"><span class="koboSpan" id="kobo.743.1">We saw how we can use dependency to show entities and the category for each of them using dependency, and also how we can use the same function to show both the parts of speech and the phrase structure. </span><span class="koboSpan" id="kobo.743.2">With inspiration from </span><em class="italic"><span class="koboSpan" id="kobo.744.1">Reference 11</span></em><span class="koboSpan" id="kobo.745.1">, we extended the code sample given there (and transitioned from using </span><code class="inlineCode"><span class="koboSpan" id="kobo.746.1">nltk</span></code><span class="koboSpan" id="kobo.747.1"> to </span><code class="inlineCode"><span class="koboSpan" id="kobo.748.1">spacy</span></code><span class="koboSpan" id="kobo.749.1"> for POS extraction, given that nltk is not fully aligned with spacy) so that we can show the parts of speech highlighted in the same way as we represented the named entities.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.750.1">The modified code (including some minor bug fixing, besides the changes mentioned already) from </span><em class="italic"><span class="koboSpan" id="kobo.751.1">Reference 11</span></em><span class="koboSpan" id="kobo.752.1"> is given here (code explanation follows after the code block):</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.753.1">import</span></span><span class="koboSpan" id="kobo.754.1"> re
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.755.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.756.1">visualize_pos</span></span><span class="koboSpan" id="kobo.757.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.758.1">sentence</span></span><span class="koboSpan" id="kobo.759.1">):
    colors = {</span><span class="hljs-string"><span class="koboSpan" id="kobo.760.1">"PRON"</span></span><span class="koboSpan" id="kobo.761.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.762.1">"blueviolet"</span></span><span class="koboSpan" id="kobo.763.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.764.1">"VERB"</span></span><span class="koboSpan" id="kobo.765.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.766.1">"lightpink"</span></span><span class="koboSpan" id="kobo.767.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.768.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.769.1">NOUN"</span></span><span class="koboSpan" id="kobo.770.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.771.1">"turquoise"</span></span><span class="koboSpan" id="kobo.772.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.773.1">"PROPN"</span></span><span class="koboSpan" id="kobo.774.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.775.1">"lightgreen"</span></span><span class="koboSpan" id="kobo.776.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.777.1">"ADJ"</span></span><span class="koboSpan" id="kobo.778.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.779.1">"lime"</span></span><span class="koboSpan" id="kobo.780.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.781.1">"ADP"</span></span><span class="koboSpan" id="kobo.782.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.783.1">"khaki"</span></span><span class="koboSpan" id="kobo.784.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.785.1">"ADV"</span></span><span class="koboSpan" id="kobo.786.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.787.1">"orange"</span></span><span class="koboSpan" id="kobo.788.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.789.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.790.1">AUX"</span></span><span class="koboSpan" id="kobo.791.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.792.1">"gold"</span></span><span class="koboSpan" id="kobo.793.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.794.1">"CONJ"</span></span><span class="koboSpan" id="kobo.795.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.796.1">"cornflowerblue"</span></span><span class="koboSpan" id="kobo.797.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.798.1">"CCONJ"</span></span><span class="koboSpan" id="kobo.799.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.800.1">"magenta"</span></span><span class="koboSpan" id="kobo.801.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.802.1">"SCONJ"</span></span><span class="koboSpan" id="kobo.803.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.804.1">"lightmagenta"</span></span><span class="koboSpan" id="kobo.805.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.806.1">"DET"</span></span><span class="koboSpan" id="kobo.807.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.808.1">"forestgreen"</span></span><span class="koboSpan" id="kobo.809.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.810.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.811.1">NUM"</span></span><span class="koboSpan" id="kobo.812.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.813.1">"salmon"</span></span><span class="koboSpan" id="kobo.814.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.815.1">"PRT"</span></span><span class="koboSpan" id="kobo.816.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.817.1">"yellow"</span></span><span class="koboSpan" id="kobo.818.1">,
              </span><span class="hljs-string"><span class="koboSpan" id="kobo.819.1">"PUNCT"</span></span><span class="koboSpan" id="kobo.820.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.821.1">"lightgrey"</span></span><span class="koboSpan" id="kobo.822.1">}
        
    pos_tags = [</span><span class="hljs-string"><span class="koboSpan" id="kobo.823.1">"PRON"</span></span><span class="koboSpan" id="kobo.824.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.825.1">"VERB"</span></span><span class="koboSpan" id="kobo.826.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.827.1">"NOUN"</span></span><span class="koboSpan" id="kobo.828.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.829.1">"PROPN"</span></span><span class="koboSpan" id="kobo.830.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.831.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.832.1">ADJ"</span></span><span class="koboSpan" id="kobo.833.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.834.1">"ADP"</span></span><span class="koboSpan" id="kobo.835.1">,
                </span><span class="hljs-string"><span class="koboSpan" id="kobo.836.1">"ADV"</span></span><span class="koboSpan" id="kobo.837.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.838.1">"AUX"</span></span><span class="koboSpan" id="kobo.839.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.840.1">"CONJ"</span></span><span class="koboSpan" id="kobo.841.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.842.1">"CCONJ"</span></span><span class="koboSpan" id="kobo.843.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.844.1">"SCONJ"</span></span><span class="koboSpan" id="kobo.845.1">,  </span><span class="hljs-string"><span class="koboSpan" id="kobo.846.1">"DET"</span></span><span class="koboSpan" id="kobo.847.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.848.1">"NUM"</span></span><span class="koboSpan" id="kobo.849.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.850.1">"PRT"</span></span><span class="koboSpan" id="kobo.851.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.852.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.853.1">PUNCT"</span></span><span class="koboSpan" id="kobo.854.1">]
    
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.855.1"># Fix for issues in the original code</span></span><span class="koboSpan" id="kobo.856.1">
    sentence = sentence.replace(</span><span class="hljs-string"><span class="koboSpan" id="kobo.857.1">"."</span></span><span class="koboSpan" id="kobo.858.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.859.1">" ."</span></span><span class="koboSpan" id="kobo.860.1">)
    sentence = sentence.replace(</span><span class="hljs-string"><span class="koboSpan" id="kobo.861.1">"'"</span></span><span class="koboSpan" id="kobo.862.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.863.1">""</span></span><span class="koboSpan" id="kobo.864.1">)
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.865.1"># Replace nltk tokenizer with spacy tokenizer and POS tagging</span></span><span class="koboSpan" id="kobo.866.1">
    doc = nlp(sentence)
    tags = []
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.867.1">for</span></span><span class="koboSpan" id="kobo.868.1"> token </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.869.1">in</span></span><span class="koboSpan" id="kobo.870.1"> doc:
        tags.append((token.text, token.pos_))
    
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.871.1"># Get start and end index (span) for each token</span></span><span class="koboSpan" id="kobo.872.1">
    span_generator = twt().span_tokenize(sentence)
    spans = [span </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.873.1">for</span></span><span class="koboSpan" id="kobo.874.1"> span </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.875.1">in</span></span><span class="koboSpan" id="kobo.876.1"> span_generator]
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.877.1"># Create dictionary with start index, end index, </span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.878.1"># pos_tag for each token</span></span><span class="koboSpan" id="kobo.879.1">
    ents = []
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.880.1">for</span></span><span class="koboSpan" id="kobo.881.1"> tag, span </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.882.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.883.1">zip</span></span><span class="koboSpan" id="kobo.884.1">(tags, spans):
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.885.1">if</span></span><span class="koboSpan" id="kobo.886.1"> tag[</span><span class="hljs-number"><span class="koboSpan" id="kobo.887.1">1</span></span><span class="koboSpan" id="kobo.888.1">] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.889.1">in</span></span><span class="koboSpan" id="kobo.890.1"> pos_tags:
            ents.append({</span><span class="hljs-string"><span class="koboSpan" id="kobo.891.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.892.1">start"</span></span><span class="koboSpan" id="kobo.893.1"> : span[</span><span class="hljs-number"><span class="koboSpan" id="kobo.894.1">0</span></span><span class="koboSpan" id="kobo.895.1">], 
                         </span><span class="hljs-string"><span class="koboSpan" id="kobo.896.1">"end"</span></span><span class="koboSpan" id="kobo.897.1"> : span[</span><span class="hljs-number"><span class="koboSpan" id="kobo.898.1">1</span></span><span class="koboSpan" id="kobo.899.1">], 
                         </span><span class="hljs-string"><span class="koboSpan" id="kobo.900.1">"label"</span></span><span class="koboSpan" id="kobo.901.1"> : tag[</span><span class="hljs-number"><span class="koboSpan" id="kobo.902.1">1</span></span><span class="koboSpan" id="kobo.903.1">] })
    doc = {</span><span class="hljs-string"><span class="koboSpan" id="kobo.904.1">"text"</span></span><span class="koboSpan" id="kobo.905.1"> : sentence, </span><span class="hljs-string"><span class="koboSpan" id="kobo.906.1">"ents"</span></span><span class="koboSpan" id="kobo.907.1"> : ents}
    options = {</span><span class="hljs-string"><span class="koboSpan" id="kobo.908.1">"ents"</span></span><span class="koboSpan" id="kobo.909.1"> : pos_tags, </span><span class="hljs-string"><span class="koboSpan" id="kobo.910.1">"colors"</span></span><span class="koboSpan" id="kobo.911.1"> : colors}
    
    displacy.render(doc, 
                    style = </span><span class="hljs-string"><span class="koboSpan" id="kobo.912.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.913.1">ent"</span></span><span class="koboSpan" id="kobo.914.1">, 
                    options = options, 
                    manual = </span><span class="hljs-literal"><span class="koboSpan" id="kobo.915.1">True</span></span><span class="koboSpan" id="kobo.916.1">,
                   )
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.917.1">Let’s understand the preceding</span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.918.1"> code better. </span><span class="koboSpan" id="kobo.918.2">In the function </span><code class="inlineCode"><span class="koboSpan" id="kobo.919.1">visualise_pos</span></code><span class="koboSpan" id="kobo.920.1">, we first </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.921.1">define a mapping between parts of speech and colors (how a part of speech will be highlighted). </span><span class="koboSpan" id="kobo.921.2">Then, we define the parts of speech that we will consider. </span><span class="koboSpan" id="kobo.921.3">Then, we correct a bug existing in the original code (from </span><em class="italic"><span class="koboSpan" id="kobo.922.1">Reference 11</span></em><span class="koboSpan" id="kobo.923.1">) using some replacements for special characters. </span><span class="koboSpan" id="kobo.923.2">We also use a spacy tokenizer and add, in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.924.1">tags</span></code><span class="koboSpan" id="kobo.925.1"> list, the text and part of speech for each </span><code class="inlineCode"><span class="koboSpan" id="kobo.926.1">pos</span></code><span class="koboSpan" id="kobo.927.1"> extracted using </span><code class="inlineCode"><span class="koboSpan" id="kobo.928.1">nlp</span></code><span class="koboSpan" id="kobo.929.1"> from </span><code class="inlineCode"><span class="koboSpan" id="kobo.930.1">spacy</span></code><span class="koboSpan" id="kobo.931.1">. </span><span class="koboSpan" id="kobo.931.2">Then, we calculate the position of each </span><code class="inlineCode"><span class="koboSpan" id="kobo.932.1">pos</span></code><span class="koboSpan" id="kobo.933.1"> identified and create a dictionary with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.934.1">pos</span></code><span class="koboSpan" id="kobo.935.1"> tokens and their position in the text, to be able to highlight them with different colors. </span><span class="koboSpan" id="kobo.935.2">At the end, we render the document with all </span><code class="inlineCode"><span class="koboSpan" id="kobo.936.1">pos</span></code><span class="koboSpan" id="kobo.937.1"> highlighted using </span><code class="inlineCode"><span class="koboSpan" id="kobo.938.1">displacy</span></code><span class="koboSpan" id="kobo.939.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.940.1">In </span><em class="italic"><span class="koboSpan" id="kobo.941.1">Figure 7.17</span></em><span class="koboSpan" id="kobo.942.1">, we show the result of </span><a id="_idIndexMarker334"/><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.943.1">applying this procedure to our sample of comments. </span><span class="koboSpan" id="kobo.943.2">We can now see</span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.944.1"> some of the errors of spacy easier. </span><span class="koboSpan" id="kobo.944.2">In the second comment, it misinterprets the second “Go” as a proper noun (</span><strong class="screenText"><span class="koboSpan" id="kobo.945.1">PROPN</span></strong><span class="koboSpan" id="kobo.946.1">). </span><span class="koboSpan" id="kobo.946.2">This is somewhat explainable, since normally, after a comma, only proper nouns will be written in uppercase in English.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.947.1"><img alt="" role="presentation" src="../Images/B20963_07_17.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.948.1">Figure 7.17: POS tagging using spacy and dependency with the procedure modified from Reference 8 to show POSs highlighted inline in text</span></p>
<p class="normal"><span class="koboSpan" id="kobo.949.1">We can observe other errors as well. </span><span class="koboSpan" id="kobo.949.2">In the first comment, “Trump” appears as </span><strong class="screenText"><span class="koboSpan" id="kobo.950.1">NOUN</span></strong><span class="koboSpan" id="kobo.951.1"> – that is, a simple noun. </span><span class="koboSpan" id="kobo.951.2">The term “republicans” is categorized as </span><strong class="screenText"><span class="koboSpan" id="kobo.952.1">PROPN</span></strong><span class="koboSpan" id="kobo.953.1">, which is likely accurate in the context of U.S. </span><span class="koboSpan" id="kobo.953.2">politics where “Republicans” is treated as a proper noun. </span><span class="koboSpan" id="kobo.953.3">However, in our context, this is inaccurate, as it represents a simple noun in plural form, identifying a group of individuals advocating for a republican government.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.954.1">We reviewed several NLP techniques that helped us to get a better understanding of the word distribution, topics, POS, and concepts present in the text. </span><span class="koboSpan" id="kobo.954.2">Optionally, we can also use these techniques to generate features to include in a machine learning model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.955.1">In the next section, we will start the analysis targeted at preparing a supervised NLP model for the classification of comments.</span></p>
<h1 class="heading-1" id="_idParaDest-100"><span class="koboSpan" id="kobo.956.1">Preparing the model</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.957.1">The model preparation, depending on the method we will implement, might be more or less complex. </span><span class="koboSpan" id="kobo.957.2">In our case, we opt to start the first baseline model with a simple deep learning architecture (which was</span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.958.1"> the standard approach at the time of the competition), including a word embeddings layer (using pretrained word embeddings) and one or more bidirectional LSTM layers. </span><span class="koboSpan" id="kobo.958.2">This architecture was a common choice at the time when this competition took place, and it is still a good option for a baseline for a text classification problem. </span><strong class="keyWord"><span class="koboSpan" id="kobo.959.1">LSTM</span></strong><span class="koboSpan" id="kobo.960.1"> stands for </span><strong class="keyWord"><span class="koboSpan" id="kobo.961.1">Long Short-Term Memory</span></strong><span class="koboSpan" id="kobo.962.1">. </span><span class="koboSpan" id="kobo.962.2">It is a type of</span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.963.1"> recurrent neural network architecture designed to capture and remember long-term dependencies in sequential data. </span><span class="koboSpan" id="kobo.963.2">It is particularly effective for text classification problems due to its ability to handle and model intricate relationships and dependencies in sequences of text.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.964.1">For this, we will need to perform some comment data preprocessing (we also performed preprocessing when preparing to build the topic modeling model). </span><span class="koboSpan" id="kobo.964.2">This time, we will perform the preprocessing steps gradually, and monitor how these steps are affecting the result not of the model, but of one prerequisite of a well-performing language model, which is the vocabulary coverage of the word embeddings.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.965.1">We will then use word embeddings in the first baseline model to extend the generalization power of our model so that the words that are not present in the train set but are in the test set would benefit from the vicinity of words that exist in the word embeddings. </span><span class="koboSpan" id="kobo.965.2">Finally, to ensure that our approach will be effective, we will need the pretrained word embeddings to have as large a vocabulary coverage as possible. </span><span class="koboSpan" id="kobo.965.3">Thus, we will also measure the vocabulary coverage and suggest methods for improving it.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.966.1">For now, we start by building the initial vocabulary.</span></p>
<h2 class="heading-2" id="_idParaDest-101"><span class="koboSpan" id="kobo.967.1">Building the vocabulary</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.968.1">We performed the earlier</span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.969.1"> experiments with word frequency, word distribution associated with various values of the target and other features, topic modeling, NER, and POS tagging with a subset of the entire comments corpus. </span><span class="koboSpan" id="kobo.969.2">For the following experiment, we will start using the entire dataset. </span><span class="koboSpan" id="kobo.969.3">We will use word embeddings with an embedding size of 300.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.970.1">Word embeddings are numerical representations of a word. </span><span class="koboSpan" id="kobo.970.2">They map words to vectors. </span><span class="koboSpan" id="kobo.970.3">The embedding size refers to the number of components (or dimensions) of these vectors. </span><span class="koboSpan" id="kobo.970.4">This procedure enables computers to understand and compare relationships between words. </span><span class="koboSpan" id="kobo.970.5">Because all the words are first transformed using word embeddings (and in the word embeddings space, the relationship between words is represented by relationships between vectors), words with similar meanings will be represented by vectors aligned in the word embeddings space. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.971.1">At testing time, new words, not present in the training data, will be represented in the word embeddings space as well, and their relationship with other words, present</span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.972.1"> in the training data, will be exploited by the algorithm. </span><span class="koboSpan" id="kobo.972.2">The effect will be to enhance the algorithm we are using for text classification.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.973.1">Additionally, we will set the number of characters (or length of comments) to a fixed number; we chose this dimension to be 220. </span><span class="koboSpan" id="kobo.973.2">For shorter comments, we will pad the comment sequence (that is, add spaces), and for larger comment sequences, we will truncate them (to 220 characters). </span><span class="koboSpan" id="kobo.973.3">This procedure will ensure we will have inputs for the machine learning model with the same dimension. </span><span class="koboSpan" id="kobo.973.4">Let’s first define a function for building the vocabulary. </span><span class="koboSpan" id="kobo.973.5">For building these functions, we used sources from </span><em class="italic"><span class="koboSpan" id="kobo.974.1">References 12</span></em><span class="koboSpan" id="kobo.975.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.976.1">13</span></em><span class="koboSpan" id="kobo.977.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.978.1">The following code is used to build the vocabulary (that is, the corpus of words present in the comments). </span><span class="koboSpan" id="kobo.978.2">We apply a split on each comment and gather all data in a list of sentences. </span><span class="koboSpan" id="kobo.978.3">We then parse all these sentences to create a dictionary with the vocabulary. </span><span class="koboSpan" id="kobo.978.4">Each time a word parsed is found as a key in the dictionary, we increment the value associated with the key. </span><span class="koboSpan" id="kobo.978.5">What we obtain is a vocabulary dictionary with the count (or overall frequency) of each word in the vocabulary:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.979.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.980.1">build_vocabulary</span></span><span class="koboSpan" id="kobo.981.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.982.1">texts</span></span><span class="koboSpan" id="kobo.983.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.984.1">"""</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.985.1">    Build the vocabulary from the corpus</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.986.1">    Credits to: [9] [10]</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.987.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.988.1">        texts: list of list of words</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.989.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.990.1">        dictionary of words and their count</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.991.1">    """</span></span><span class="koboSpan" id="kobo.992.1">
    sentences = texts.apply(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.993.1">lambda</span></span><span class="koboSpan" id="kobo.994.1"> x: x.split()).values
    vocab = {}
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.995.1">for</span></span><span class="koboSpan" id="kobo.996.1"> sentence </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.997.1">in</span></span><span class="koboSpan" id="kobo.998.1"> tqdm(sentences):
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.999.1">for</span></span><span class="koboSpan" id="kobo.1000.1"> word </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1001.1">in</span></span><span class="koboSpan" id="kobo.1002.1"> sentence:
            </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1003.1">try</span></span><span class="koboSpan" id="kobo.1004.1">:
                vocab[word] += </span><span class="hljs-number"><span class="koboSpan" id="kobo.1005.1">1</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1006.1">except</span></span><span class="koboSpan" id="kobo.1007.1"> KeyError:
                vocab[word] = </span><span class="hljs-number"><span class="koboSpan" id="kobo.1008.1">1</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1009.1">return</span></span><span class="koboSpan" id="kobo.1010.1"> vocab
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1011.1">We create the overall vocabulary by concatenating </span><code class="inlineCode"><span class="koboSpan" id="kobo.1012.1">train</span></code><span class="koboSpan" id="kobo.1013.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.1014.1">test</span></code><span class="koboSpan" id="kobo.1015.1">:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1016.1"># populate the vocabulary</span></span><span class="koboSpan" id="kobo.1017.1">
df = pd.concat([train ,test], sort=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1018.1">False</span></span><span class="koboSpan" id="kobo.1019.1">)
vocabulary = build_vocabulary(df[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1020.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1021.1">])
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1022.1">We can check the first 10 elements in the vocabulary to have an intuition of what this vocabulary looks like:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.1023.1"># display the first 10 elements and their count</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.1024.1">print</span></span><span class="koboSpan" id="kobo.1025.1">({k: vocabulary[k] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1026.1">for</span></span><span class="koboSpan" id="kobo.1027.1"> k </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1028.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1029.1">list</span></span><span class="koboSpan" id="kobo.1030.1">(vocabulary)[:</span><span class="hljs-number"><span class="koboSpan" id="kobo.1031.1">10</span></span><span class="koboSpan" id="kobo.1032.1">]})
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1033.1">The following image shows the result of running the preceding code. </span><span class="koboSpan" id="kobo.1033.2">It shows the most frequent words in the text. </span><span class="koboSpan" id="kobo.1033.3">As expected, the most used words are some of the most frequently used words in the English language.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1034.1"><img alt="" role="presentation" src="../Images/B20963_07_18.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1035.1">Figure 7.18: Vocabulary without any preprocessing – uppercase and lowercase words, and possibly wrongly spelled expressions</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1036.1">We will use the earlier</span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.1037.1"> introduced function, </span><code class="inlineCode"><span class="koboSpan" id="kobo.1038.1">build_vocabulary</span></code><span class="koboSpan" id="kobo.1039.1">, repeatedly every time we perform an additional (sometimes repeated) text transformation. </span><span class="koboSpan" id="kobo.1039.2">We perform successive text transformations to ensure that, while using pretrained word embeddings, we have good coverage with the words in the pretrained word embeddings of the vocabulary in the comments. </span><span class="koboSpan" id="kobo.1039.3">With a larger coverage, we ensure a better accuracy of the model that we are building. </span><span class="koboSpan" id="kobo.1039.4">Let’s continue by loading some pretrained word embeddings.</span></p>
<h2 class="heading-2" id="_idParaDest-102"><span class="koboSpan" id="kobo.1040.1">Embedding index and embedding matrix</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1041.1">We will now build a dictionary </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.1042.1">with the words in word embeddings as keys and the arrays of their embedding representations as values. </span><span class="koboSpan" id="kobo.1042.2">We call this </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.1043.1">dictionary the embedding index. </span><span class="koboSpan" id="kobo.1043.2">We will then also build the embedding matrix, which is a matrix representation of embeddings. </span><span class="koboSpan" id="kobo.1043.3">We will use GloVe’s pretrained embeddings (with 300 dimensions) for our experiments. </span><strong class="keyWord"><span class="koboSpan" id="kobo.1044.1">GloVe</span></strong><span class="koboSpan" id="kobo.1045.1"> stands for </span><strong class="keyWord"><span class="koboSpan" id="kobo.1046.1">Global Vectors for Word Representation</span></strong><span class="koboSpan" id="kobo.1047.1"> and is an unsupervised</span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.1048.1"> algorithm that produces word embeddings. </span><span class="koboSpan" id="kobo.1048.2">It works by analyzing global text statistics over a very large text corpus to create vector representations and capture</span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.1049.1"> semantic relationships between words.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1050.1">The following code loads the pretrained word embeddings:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1051.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1052.1">load_embeddings</span></span><span class="koboSpan" id="kobo.1053.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1054.1">file</span></span><span class="koboSpan" id="kobo.1055.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1056.1">"""</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1057.1">    Load the embeddings</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1058.1">    Credits to: [9] [10]</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1059.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1060.1">        file: embeddings file</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1061.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1062.1">        embedding index</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1063.1">    """</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1064.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1065.1">get_coefs</span></span><span class="koboSpan" id="kobo.1066.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1067.1">word,*arr</span></span><span class="koboSpan" id="kobo.1068.1">): 
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1069.1">return</span></span><span class="koboSpan" id="kobo.1070.1"> word, np.asarray(arr, dtype=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1071.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1072.1">float32'</span></span><span class="koboSpan" id="kobo.1073.1">)
    embeddings_index = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1074.1">dict</span></span><span class="koboSpan" id="kobo.1075.1">(get_coefs(*o.split(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1076.1">" "</span></span><span class="koboSpan" id="kobo.1077.1">)) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1078.1">for</span></span><span class="koboSpan" id="kobo.1079.1"> o </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1080.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1081.1">open</span></span><span class="koboSpan" id="kobo.1082.1">(file, encoding=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1083.1">'latin'</span></span><span class="koboSpan" id="kobo.1084.1">))
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1085.1">return</span></span><span class="koboSpan" id="kobo.1086.1"> embeddings_index
%%time
GLOVE_PATH = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1087.1">'../input/glove840b300dtxt/'</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.1088.1">print</span></span><span class="koboSpan" id="kobo.1089.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1090.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1091.1">Extracting GloVe embedding started"</span></span><span class="koboSpan" id="kobo.1092.1">)
embed_glove = load_embeddings(os.path.join(GLOVE_PATH,</span><span class="hljs-string"><span class="koboSpan" id="kobo.1093.1">'glove.840B.300d.txt'</span></span><span class="koboSpan" id="kobo.1094.1">))
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1095.1">print</span></span><span class="koboSpan" id="kobo.1096.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1097.1">"Embedding completed"</span></span><span class="koboSpan" id="kobo.1098.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1099.1">The size of the embedding structure obtained is 2.19 million items. </span><span class="koboSpan" id="kobo.1099.2">Next, we create the embedding matrix using the word</span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.1100.1"> index and the embedding index we just created:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1101.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1102.1">embedding_matrix</span></span><span class="koboSpan" id="kobo.1103.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1104.1">word_index, embeddings_index</span></span><span class="koboSpan" id="kobo.1105.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1106.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1107.1">    Create the embedding matrix</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1108.1">    credits to: [9] [10] </span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1109.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1110.1">        word_index: word index (from vocabulary)</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1111.1">        embedding_index: embedding index (from embeddings file)</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1112.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1113.1">        embedding matrix</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1114.1">    '''</span></span><span class="koboSpan" id="kobo.1115.1">
    all_embs = np.stack(embeddings_index.values())
    emb_mean, emb_std = all_embs.mean(), all_embs.std()
    EMBED_SIZE = all_embs.shape[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1116.1">1</span></span><span class="koboSpan" id="kobo.1117.1">]
    nb_words = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1118.1">min</span></span><span class="koboSpan" id="kobo.1119.1">(MAX_FEATURES, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1120.1">len</span></span><span class="koboSpan" id="kobo.1121.1">(word_index))
    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBED_SIZE))
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1122.1">for</span></span><span class="koboSpan" id="kobo.1123.1"> word, i </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1124.1">in</span></span><span class="koboSpan" id="kobo.1125.1"> tqdm(word_index.items()):
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1126.1">if</span></span><span class="koboSpan" id="kobo.1127.1"> i &gt;= MAX_FEATURES:
            </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1128.1">continue</span></span><span class="koboSpan" id="kobo.1129.1">
        embedding_vector = embeddings_index.get(word)
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1130.1">if</span></span><span class="koboSpan" id="kobo.1131.1"> embedding_vector </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1132.1">is</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1133.1">not</span></span> <span class="hljs-literal"><span class="koboSpan" id="kobo.1134.1">None</span></span><span class="koboSpan" id="kobo.1135.1">:
            embedding_matrix[i] = embedding_vector
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1136.1">return</span></span><span class="koboSpan" id="kobo.1137.1"> embedding_matrix
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1138.1">We use the </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.1139.1">parameter </span><code class="inlineCode"><span class="koboSpan" id="kobo.1140.1">MAX_FEATURES</span></code><span class="koboSpan" id="kobo.1141.1"> to limit the dimension of</span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.1142.1"> the embedding matrix.</span></p>
<h2 class="heading-2" id="_idParaDest-103"><span class="koboSpan" id="kobo.1143.1">Checking vocabulary coverage</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1144.1">We introduced the functions to read the word embeddings and compute the embedding matrix. </span><span class="koboSpan" id="kobo.1144.2">Now we will continue</span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.1145.1"> with introducing the functions to evaluate the vocabulary coverage with words from the word embeddings. </span><span class="koboSpan" id="kobo.1145.2">The larger the vocabulary coverage, the better the accuracy of the model we are building.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1146.1">To check the coverage of the vocabulary by the embeddings, we are going to use the following function:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1147.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1148.1">check_coverage</span></span><span class="koboSpan" id="kobo.1149.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1150.1">vocab, embeddings_index</span></span><span class="koboSpan" id="kobo.1151.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1152.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1153.1">    Check the vocabulary coverage by the embedding terms</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1154.1">    credits to: [9] [10]</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1155.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1156.1">        vocab: vocabulary</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1157.1">        embedding_index: embedding index (from embeddings file)</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1158.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1159.1">        list of unknown words; also prints the vocabulary coverage of embeddings and </span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1160.1">        the % of comments text covered by the embeddings</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1161.1">    '''</span></span><span class="koboSpan" id="kobo.1162.1">
    known_words = {}
    unknown_words = {}
    nb_known_words = </span><span class="hljs-number"><span class="koboSpan" id="kobo.1163.1">0</span></span><span class="koboSpan" id="kobo.1164.1">
    nb_unknown_words = </span><span class="hljs-number"><span class="koboSpan" id="kobo.1165.1">0</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1166.1">for</span></span><span class="koboSpan" id="kobo.1167.1"> word </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1168.1">in</span></span><span class="koboSpan" id="kobo.1169.1"> tqdm(vocab.keys()):
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1170.1">try</span></span><span class="koboSpan" id="kobo.1171.1">:
            known_words[word] = embeddings_index[word]
            nb_known_words += vocab[word]
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1172.1">except</span></span><span class="koboSpan" id="kobo.1173.1">:
            unknown_words[word] = vocab[word]
            nb_unknown_words += vocab[word]
            </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1174.1">pass</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.1175.1">print</span></span><span class="koboSpan" id="kobo.1176.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1177.1">'Found embeddings for {:.3%} of vocabulary'</span></span><span class="koboSpan" id="kobo.1178.1">.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1179.1">format</span></span><span class="koboSpan" id="kobo.1180.1">(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1181.1">len</span></span><span class="koboSpan" id="kobo.1182.1">(known_words)/</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1183.1">len</span></span><span class="koboSpan" id="kobo.1184.1">(vocab)))
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1185.1">print</span></span><span class="koboSpan" id="kobo.1186.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1187.1">'Found embeddings for {:.3%} of all text'</span></span><span class="koboSpan" id="kobo.1188.1">.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1189.1">format</span></span><span class="koboSpan" id="kobo.1190.1">(nb_known_words/(nb_known_words + nb_unknown_words)))
    unknown_words = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1191.1">sorted</span></span><span class="koboSpan" id="kobo.1192.1">(unknown_words.items(), key=operator.itemgetter(</span><span class="hljs-number"><span class="koboSpan" id="kobo.1193.1">1</span></span><span class="koboSpan" id="kobo.1194.1">))[::-</span><span class="hljs-number"><span class="koboSpan" id="kobo.1195.1">1</span></span><span class="koboSpan" id="kobo.1196.1">]
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1197.1">return</span></span><span class="koboSpan" id="kobo.1198.1"> unknown_words
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1199.1">The preceding code browses through all the vocabulary items (that is, the words present in the comments text) and counts the unknown words (that is, words in the text but not in the list of embeddings words). </span><span class="koboSpan" id="kobo.1199.2">Then, it calculates the percentage of words in the vocabulary that exist in the word embeddings index. </span><span class="koboSpan" id="kobo.1199.3">This percentage is calculated in two ways: with each word in the</span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.1200.1"> vocabulary unweighted and with words weighted by their frequency in the text.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1201.1">We will apply this function repeatedly to check the vocabulary coverage after each step of preprocessing. </span><span class="koboSpan" id="kobo.1201.2">Let’s start with checking the vocabulary coverage for the initial vocabulary, where we haven’t applied any preprocessing to the comments text yet.</span></p>
<h2 class="heading-2" id="_idParaDest-104"><span class="koboSpan" id="kobo.1202.1">Iteratively improving vocabulary coverage</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1203.1">We apply the function </span><code class="inlineCode"><span class="koboSpan" id="kobo.1204.1">check_coverage</span></code><span class="koboSpan" id="kobo.1205.1"> to check the vocabulary coverage, passing the two parameters: vocabulary and embedding</span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.1206.1"> matrix. </span><span class="koboSpan" id="kobo.1206.2">In the</span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.1207.1"> following notation, </span><strong class="keyWord"><span class="koboSpan" id="kobo.1208.1">oov</span></strong><span class="koboSpan" id="kobo.1209.1"> stands for </span><strong class="keyWord"><span class="koboSpan" id="kobo.1210.1">out of vocabulary</span></strong><span class="koboSpan" id="kobo.1211.1">:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1212.1">print</span></span><span class="koboSpan" id="kobo.1213.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1214.1">"Verify the initial vocabulary coverage"</span></span><span class="koboSpan" id="kobo.1215.1">)
oov_glove = check_coverage(vocabulary, embed_glove)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1216.1">The result of the first iteration is not great. </span><span class="koboSpan" id="kobo.1216.2">Although we have almost 90% of the text covered, only 15.5% of the words in the vocabulary are covered by the word embeddings:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1217.1"><img alt="" role="presentation" src="../Images/B20963_07_19.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1218.1">Figure 7.19: Vocabulary coverage – first iteration</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1219.1">We can also look at the list of not covered terms. </span><span class="koboSpan" id="kobo.1219.2">Because, in </span><code class="inlineCode"><span class="koboSpan" id="kobo.1220.1">oov_glove</span></code><span class="koboSpan" id="kobo.1221.1">, we stored the not covered terms in descending order by the number of appearances in the corpus, we can see, by selecting the </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.1222.1">first terms in this list, the most important words not included in the word embeddings. </span><span class="koboSpan" id="kobo.1222.2">In </span><em class="italic"><span class="koboSpan" id="kobo.1223.1">Figure 7.20</span></em><span class="koboSpan" id="kobo.1224.1">, we show the first</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.1225.1"> 10 terms in this list – the top 10 words not covered. </span><span class="koboSpan" id="kobo.1225.2">Here, </span><em class="italic"><span class="koboSpan" id="kobo.1226.1">not covered</span></em><span class="koboSpan" id="kobo.1227.1"> refers to words that appear in the vocabulary (are present in the comments texts) but not in the word embeddings index (are not present in the pretrained word embeddings):</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1228.1"><img alt="" role="presentation" src="../Images/B20963_07_20.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1229.1">Figure 7.20: Most frequent 10 words from the vocabulary not covered by the word embeddings in the first iteration</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1230.1">By quickly inspecting the list in </span><em class="italic"><span class="koboSpan" id="kobo.1231.1">Figure 7.20</span></em><span class="koboSpan" id="kobo.1232.1">, we see that some of the frequent words are either contracted, or colloquial, non-standard forms of spoken English. </span><span class="koboSpan" id="kobo.1232.2">It is normal to see such forms in online comments. </span><span class="koboSpan" id="kobo.1232.3">We will perform several steps of preprocessing to try to improve the vocabulary coverage by correcting the issues we find. </span><span class="koboSpan" id="kobo.1232.4">After each step, we will also measure the vocabulary coverage again.</span></p>
<h3 class="heading-3" id="_idParaDest-105"><span class="koboSpan" id="kobo.1233.1">Transforming to lowercase</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.1234.1">We will start by converting all text to</span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.1235.1"> lowercase and adding it to the vocabulary. </span><span class="koboSpan" id="kobo.1235.2">In word embeddings, the words will be all lowercase:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1236.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1237.1">add_lower</span></span><span class="koboSpan" id="kobo.1238.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1239.1">embedding, vocab</span></span><span class="koboSpan" id="kobo.1240.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1241.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1242.1">    Add lower case words</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1243.1">    credits to: [9] [10]</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1244.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1245.1">        embedding: embedding matrix</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1246.1">        vocab: vocabulary</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1247.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1248.1">        None</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1249.1">        modify the embeddings to include the lower case from vocabulary</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1250.1">    '''</span></span><span class="koboSpan" id="kobo.1251.1">
    count = </span><span class="hljs-number"><span class="koboSpan" id="kobo.1252.1">0</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1253.1">for</span></span><span class="koboSpan" id="kobo.1254.1"> word </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1255.1">in</span></span><span class="koboSpan" id="kobo.1256.1"> tqdm(vocab):
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1257.1">if</span></span><span class="koboSpan" id="kobo.1258.1"> word </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1259.1">in</span></span><span class="koboSpan" id="kobo.1260.1"> embedding </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1261.1">and</span></span><span class="koboSpan" id="kobo.1262.1"> word.lower() </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1263.1">not</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1264.1">in</span></span><span class="koboSpan" id="kobo.1265.1"> embedding:  
            embedding[word.lower()] = embedding[word]
            count += </span><span class="hljs-number"><span class="koboSpan" id="kobo.1266.1">1</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.1267.1">print</span></span><span class="koboSpan" id="kobo.1268.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1269.1">f"Added </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1270.1">{count}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1271.1"> words to embedding"</span></span><span class="koboSpan" id="kobo.1272.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1273.1">We apply this lowercase transformation to both the train and test sets and then we rebuild the vocabulary and calculate the vocabulary coverage:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1274.1">train[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1275.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1276.1">] = train[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1277.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1278.1">].apply(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1279.1">lambda</span></span><span class="koboSpan" id="kobo.1280.1"> x: x.lower())
test[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1281.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1282.1">] = test[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1283.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1284.1">].apply(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1285.1">lambda</span></span><span class="koboSpan" id="kobo.1286.1"> x: x.lower())
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1287.1">print</span></span><span class="koboSpan" id="kobo.1288.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1289.1">"Check coverage for vocabulary with lower case"</span></span><span class="koboSpan" id="kobo.1290.1">)
oov_glove = check_coverage(vocabulary, embed_glove)
add_lower(embed_glove, vocabulary) </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1291.1"># operates on the same vocabulary</span></span><span class="koboSpan" id="kobo.1292.1">
oov_glove = check_coverage(vocabulary, embed_glove)
</span></code></pre>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.1293.1">Figure 7.21</span></em><span class="koboSpan" id="kobo.1294.1"> shows the new vocabulary coverage after we applied the lowercase transformation:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1295.1"><img alt="A number with black text  Description automatically generated with medium confidence" src="../Images/B20963_07_21.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1296.1">Figure 7.21: Vocabulary coverage – second iteration with lowercase of all words</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1297.1">We can observe a few small improvements in the word percentage and text percentage coverage. </span><span class="koboSpan" id="kobo.1297.2">Let’s continue by</span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.1298.1"> removing contractions in the comments text.</span></p>
<h3 class="heading-3" id="_idParaDest-106"><span class="koboSpan" id="kobo.1299.1">Removing contractions</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.1300.1">Next, we will remove contractions. </span><span class="koboSpan" id="kobo.1300.2">These are modified forms of words and expressions. </span><span class="koboSpan" id="kobo.1300.3">We will use a predefined dictionary </span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.1301.1">of usually encountered contractions. </span><span class="koboSpan" id="kobo.1301.2">These will be mapped on words that exist in embeddings. </span><span class="koboSpan" id="kobo.1301.3">Because of limited space, we are just including here a few examples of items in the contractions dictionary, but the entire resource is available in the notebook associated with this chapter:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1302.1">contraction_mapping = {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1303.1">"ain't"</span></span><span class="koboSpan" id="kobo.1304.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1305.1">"is not"</span></span><span class="koboSpan" id="kobo.1306.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1307.1">"aren't"</span></span><span class="koboSpan" id="kobo.1308.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1309.1">"are not"</span></span><span class="koboSpan" id="kobo.1310.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.1311.1">"can't"</span></span><span class="koboSpan" id="kobo.1312.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1313.1">"cannot"</span></span><span class="koboSpan" id="kobo.1314.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1315.1">"'cause"</span></span><span class="koboSpan" id="kobo.1316.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1317.1">"because"</span></span><span class="koboSpan" id="kobo.1318.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1319.1">"could've"</span></span><span class="koboSpan" id="kobo.1320.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1321.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1322.1">could have"</span></span><span class="koboSpan" id="kobo.1323.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1324.1">"couldn't"</span></span><span class="koboSpan" id="kobo.1325.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1326.1">"could not"</span></span><span class="koboSpan" id="kobo.1327.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1328.1">"didn't"</span></span><span class="koboSpan" id="kobo.1329.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1330.1">"did not"</span></span><span class="koboSpan" id="kobo.1331.1">,  </span><span class="hljs-string"><span class="koboSpan" id="kobo.1332.1">"doesn't"</span></span><span class="koboSpan" id="kobo.1333.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1334.1">"does not"</span></span><span class="koboSpan" id="kobo.1335.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1336.1">"don't"</span></span><span class="koboSpan" id="kobo.1337.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1338.1">"do not"</span></span><span class="koboSpan" id="kobo.1339.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1340.1">"hadn't"</span></span><span class="koboSpan" id="kobo.1341.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1342.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1343.1">had not"</span></span><span class="koboSpan" id="kobo.1344.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1345.1">"hasn't"</span></span><span class="koboSpan" id="kobo.1346.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1347.1">"has not"</span></span><span class="koboSpan" id="kobo.1348.1">,
...
</span><span class="koboSpan" id="kobo.1348.2">}
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1349.1">With the following function, we can get the list of known contractions in GloVe embeddings:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1350.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1351.1">known_contractions</span></span><span class="koboSpan" id="kobo.1352.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1353.1">embed</span></span><span class="koboSpan" id="kobo.1354.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1355.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1356.1">    Add know contractions</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1357.1">    credits to: [9] [10]</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1358.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1359.1">        embed: embedding matrix</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1360.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1361.1">        known contractions (from embeddings)</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1362.1">    '''</span></span><span class="koboSpan" id="kobo.1363.1">
    known = []
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1364.1">for</span></span><span class="koboSpan" id="kobo.1365.1"> contract </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1366.1">in</span></span><span class="koboSpan" id="kobo.1367.1"> tqdm(contraction_mapping):
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1368.1">if</span></span><span class="koboSpan" id="kobo.1369.1"> contract </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1370.1">in</span></span><span class="koboSpan" id="kobo.1371.1"> embed:
            known.append(contract)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1372.1">return</span></span><span class="koboSpan" id="kobo.1373.1"> known
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1374.1">We can use the next function to clean the known contractions from the vocabulary – that is, replace them by using the contractions dictionary:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1375.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1376.1">clean_contractions</span></span><span class="koboSpan" id="kobo.1377.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1378.1">text, mapping</span></span><span class="koboSpan" id="kobo.1379.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1380.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1381.1">    Clean the contractions</span></span>
<span class="hljs-string"> </span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1382.1">    credits to: [9] [10]</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1383.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1384.1">        text: current text</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1385.1">        mapping: contraction mappings</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1386.1">    Returns: modify the comments to use the base form from contraction mapping</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1387.1">    '''</span></span><span class="koboSpan" id="kobo.1388.1">
    specials = [</span><span class="hljs-string"><span class="koboSpan" id="kobo.1389.1">"’"</span></span><span class="koboSpan" id="kobo.1390.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1391.1">"‘"</span></span><span class="koboSpan" id="kobo.1392.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1393.1">"´"</span></span><span class="koboSpan" id="kobo.1394.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1395.1">"`"</span></span><span class="koboSpan" id="kobo.1396.1">]
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1397.1">for</span></span><span class="koboSpan" id="kobo.1398.1"> s </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1399.1">in</span></span><span class="koboSpan" id="kobo.1400.1"> specials:
        text = text.replace(s, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1401.1">"'"</span></span><span class="koboSpan" id="kobo.1402.1">)
    text = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1403.1">' '</span></span><span class="koboSpan" id="kobo.1404.1">.join([mapping[t] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1405.1">if</span></span><span class="koboSpan" id="kobo.1406.1"> t </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1407.1">in</span></span><span class="koboSpan" id="kobo.1408.1"> mapping </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1409.1">else</span></span><span class="koboSpan" id="kobo.1410.1"> t </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1411.1">for</span></span><span class="koboSpan" id="kobo.1412.1"> t </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1413.1">in</span></span><span class="koboSpan" id="kobo.1414.1"> text.split(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1415.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1416.1"> "</span></span><span class="koboSpan" id="kobo.1417.1">)])
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1418.1">return</span></span><span class="koboSpan" id="kobo.1419.1"> text
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1420.1">After we apply </span><code class="inlineCode"><span class="koboSpan" id="kobo.1421.1">clean_contractions</span></code><span class="koboSpan" id="kobo.1422.1"> to both the train and test sets and again apply the function to build the vocabulary and </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.1423.1"> measure vocabulary coverage, we get the new stats about the vocabulary coverage:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1424.1"><img alt="A number with numbers on it  Description automatically generated with medium confidence" src="../Images/B20963_07_22.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1425.1">Figure 7.22: Vocabulary coverage – third iteration, after replacing contractions using the contractions dictionary</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1426.1">Further refinement of the contractions dictionary is possible by inspecting expressions without coverage and enhancing it to equate not covered expressions in the corpus with words or groups of words where each word is represented in the embedding vector.</span></p>
<h3 class="heading-3" id="_idParaDest-107"><span class="koboSpan" id="kobo.1427.1">Removing punctuation and special characters</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.1428.1">Next, we will remove punctuation and </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.1429.1">special characters. </span><span class="koboSpan" id="kobo.1429.2">The following lists and functions are useful for this step. </span><span class="koboSpan" id="kobo.1429.3">First, we list the unknown punctuation:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1430.1">punct_mapping = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1431.1">"/-'?!.,#$%\'()*+-/:;&lt;=&gt;@[\\]^_`{|}~"</span></span><span class="koboSpan" id="kobo.1432.1"> + </span><span class="hljs-string"><span class="koboSpan" id="kobo.1433.1">'""“”’'</span></span><span class="koboSpan" id="kobo.1434.1"> + </span><span class="hljs-string"><span class="koboSpan" id="kobo.1435.1">'∞θ÷α•à−β</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1436.1">ø</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1437.1">³π'₹´°£€\×™√²—–&amp;'</span></span><span class="koboSpan" id="kobo.1438.1">
punct_mapping += </span><span class="hljs-string"><span class="koboSpan" id="kobo.1439.1">'©^®` &lt;→°€™' ♥←×§″′Â█½à…"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1440.1">✶</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1441.1">"–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—'─▒: ¼</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1442.1">⊕</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1443.1">▼▪†■’▀¨▄♫</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1444.1">⭐</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1445.1">é¯♦¤▲è¸¾Ã‘'∞∙</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1446.1">）</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1447.1">↓</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1448.1">、</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1449.1">│</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1450.1">（»，</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1451.1">♪╩╚³</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1452.1">・</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1453.1">╦╣╔╗▬</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1454.1">❤</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1455.1">ïØ¹≤‡√'</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1456.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1457.1">unknown_punct</span></span><span class="koboSpan" id="kobo.1458.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1459.1">embed, punct</span></span><span class="koboSpan" id="kobo.1460.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1461.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1462.1">    Find the unknown punctuation</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1463.1">    credits to: [9] [10] </span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1464.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1465.1">        embed: embedding matrix</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1466.1">        punct: punctuation</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1467.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1468.1">        unknown punctuation</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1469.1">    '''</span></span><span class="koboSpan" id="kobo.1470.1">
    unknown = </span><span class="hljs-string"><span class="koboSpan" id="kobo.1471.1">''</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1472.1">for</span></span><span class="koboSpan" id="kobo.1473.1"> p </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1474.1">in</span></span><span class="koboSpan" id="kobo.1475.1"> punct:
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1476.1">if</span></span><span class="koboSpan" id="kobo.1477.1"> p </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1478.1">not</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1479.1">in</span></span><span class="koboSpan" id="kobo.1480.1"> embed:
            unknown += p
            unknown += </span><span class="hljs-string"><span class="koboSpan" id="kobo.1481.1">' '</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1482.1">return</span></span><span class="koboSpan" id="kobo.1483.1"> unknown
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1484.1">Then we clean the special </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.1485.1">characters and punctuation:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1486.1">puncts = {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1487.1">"‘"</span></span><span class="koboSpan" id="kobo.1488.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1489.1">"'"</span></span><span class="koboSpan" id="kobo.1490.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1491.1">"´"</span></span><span class="koboSpan" id="kobo.1492.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1493.1">"'"</span></span><span class="koboSpan" id="kobo.1494.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1495.1">"°"</span></span><span class="koboSpan" id="kobo.1496.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1497.1">""</span></span><span class="koboSpan" id="kobo.1498.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1499.1">"€"</span></span><span class="koboSpan" id="kobo.1500.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1501.1">"euro"</span></span><span class="koboSpan" id="kobo.1502.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1503.1">"—"</span></span><span class="koboSpan" id="kobo.1504.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1505.1">"-"</span></span><span class="koboSpan" id="kobo.1506.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1507.1">"–"</span></span><span class="koboSpan" id="kobo.1508.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1509.1">"-"</span></span><span class="koboSpan" id="kobo.1510.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1511.1">"’"</span></span><span class="koboSpan" id="kobo.1512.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1513.1">"'"</span></span><span class="koboSpan" id="kobo.1514.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1515.1">"_"</span></span><span class="koboSpan" id="kobo.1516.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1517.1">"-"</span></span><span class="koboSpan" id="kobo.1518.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1519.1">"`"</span></span><span class="koboSpan" id="kobo.1520.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1521.1">"'"</span></span><span class="koboSpan" id="kobo.1522.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1523.1">'“'</span></span><span class="koboSpan" id="kobo.1524.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1525.1">'"'</span></span><span class="koboSpan" id="kobo.1526.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1527.1">'”'</span></span><span class="koboSpan" id="kobo.1528.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1529.1">'"'</span></span><span class="koboSpan" id="kobo.1530.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1531.1">'“'</span></span><span class="koboSpan" id="kobo.1532.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1533.1">'"'</span></span><span class="koboSpan" id="kobo.1534.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1535.1">"£"</span></span><span class="koboSpan" id="kobo.1536.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1537.1">"pound"</span></span><span class="koboSpan" id="kobo.1538.1">,
          </span><span class="hljs-string"><span class="koboSpan" id="kobo.1539.1">'∞'</span></span><span class="koboSpan" id="kobo.1540.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1541.1">'infinity'</span></span><span class="koboSpan" id="kobo.1542.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1543.1">'θ'</span></span><span class="koboSpan" id="kobo.1544.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1545.1">'theta'</span></span><span class="koboSpan" id="kobo.1546.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1547.1">'÷'</span></span><span class="koboSpan" id="kobo.1548.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1549.1">'/'</span></span><span class="koboSpan" id="kobo.1550.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1551.1">'α'</span></span><span class="koboSpan" id="kobo.1552.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1553.1">'alpha'</span></span><span class="koboSpan" id="kobo.1554.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1555.1">'•'</span></span><span class="koboSpan" id="kobo.1556.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1557.1">'.'</span></span><span class="koboSpan" id="kobo.1558.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1559.1">'à'</span></span><span class="koboSpan" id="kobo.1560.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1561.1">'a'</span></span><span class="koboSpan" id="kobo.1562.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1563.1">'−'</span></span><span class="koboSpan" id="kobo.1564.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1565.1">'-'</span></span><span class="koboSpan" id="kobo.1566.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1567.1">'β'</span></span><span class="koboSpan" id="kobo.1568.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1569.1">'beta'</span></span><span class="koboSpan" id="kobo.1570.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1571.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1572.1">∅</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1573.1">'</span></span><span class="koboSpan" id="kobo.1574.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1575.1">''</span></span><span class="koboSpan" id="kobo.1576.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1577.1">'³'</span></span><span class="koboSpan" id="kobo.1578.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1579.1">'3'</span></span><span class="koboSpan" id="kobo.1580.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1581.1">'π'</span></span><span class="koboSpan" id="kobo.1582.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1583.1">'pi'</span></span><span class="koboSpan" id="kobo.1584.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1585.1">'…'</span></span><span class="koboSpan" id="kobo.1586.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1587.1">' '</span></span><span class="koboSpan" id="kobo.1588.1">}
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1589.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1590.1">clean_special_chars</span></span><span class="koboSpan" id="kobo.1591.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1592.1">text, punct, mapping</span></span><span class="koboSpan" id="kobo.1593.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1594.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1595.1">    Clean special characters</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1596.1">    credits to: [9] [10]</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1597.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1598.1">        text: current text</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1599.1">        punct: punctuation</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1600.1">        mapping: punctuation mapping</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1601.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1602.1">        cleaned text</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1603.1">    '''</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1604.1">for</span></span><span class="koboSpan" id="kobo.1605.1"> p </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1606.1">in</span></span><span class="koboSpan" id="kobo.1607.1"> mapping:
        text = text.replace(p, mapping[p])
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1608.1">for</span></span><span class="koboSpan" id="kobo.1609.1"> p </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1610.1">in</span></span><span class="koboSpan" id="kobo.1611.1"> punct:
        text = text.replace(p, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1612.1">f' </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1613.1">{p}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1614.1"> '</span></span><span class="koboSpan" id="kobo.1615.1">) 
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1616.1">return</span></span><span class="koboSpan" id="kobo.1617.1"> text
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1618.1">Let’s check the vocabulary coverage again in </span><em class="italic"><span class="koboSpan" id="kobo.1619.1">Figure 7.23</span></em><span class="koboSpan" id="kobo.1620.1">. </span><span class="koboSpan" id="kobo.1620.2">This time, we increased the word vocabulary coverage by word embeddings from around 15% to 54%. </span><span class="koboSpan" id="kobo.1620.3">Additionally, text coverage increased from 90% to 99.7%.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1621.1"><img alt="A close-up of numbers  Description automatically generated" src="../Images/B20963_07_23.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1622.1">Figure 7.23: Vocabulary coverage – fourth iteration, after cleaning punctuation and special characters</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1623.1">Looking at the top 20 words not covered, we see that we have small words with accents, special characters, and idiomatic expressions. </span><span class="koboSpan" id="kobo.1623.2">We extend the punctuation dictionary to include the most frequent special characters, and after we run </span><code class="inlineCode"><span class="koboSpan" id="kobo.1624.1">build_vocabulary</span></code><span class="koboSpan" id="kobo.1625.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.1626.1">check_coverage</span></code><span class="koboSpan" id="kobo.1627.1"> again, we get a new status of the vocabulary coverage:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1628.1">more_puncts = {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1629.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1630.1">▀'</span></span><span class="koboSpan" id="kobo.1631.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1632.1">'.'</span></span><span class="koboSpan" id="kobo.1633.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1634.1">'▄'</span></span><span class="koboSpan" id="kobo.1635.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1636.1">'.'</span></span><span class="koboSpan" id="kobo.1637.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1638.1">'é'</span></span><span class="koboSpan" id="kobo.1639.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1640.1">'e'</span></span><span class="koboSpan" id="kobo.1641.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1642.1">'è'</span></span><span class="koboSpan" id="kobo.1643.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1644.1">'e'</span></span><span class="koboSpan" id="kobo.1645.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1646.1">'ï'</span></span><span class="koboSpan" id="kobo.1647.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1648.1">'i'</span></span><span class="koboSpan" id="kobo.1649.1">,</span><span class="hljs-string"><span class="koboSpan" id="kobo.1650.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1651.1">⭐</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1652.1">'</span></span><span class="koboSpan" id="kobo.1653.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1654.1">'star'</span></span><span class="koboSpan" id="kobo.1655.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1656.1">'ᴀ'</span></span><span class="koboSpan" id="kobo.1657.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1658.1">'A'</span></span><span class="koboSpan" id="kobo.1659.1">,  </span><span class="hljs-string"><span class="koboSpan" id="kobo.1660.1">'ᴀɴᴅ'</span></span><span class="koboSpan" id="kobo.1661.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1662.1">'and'</span></span><span class="koboSpan" id="kobo.1663.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1664.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1665.1">»</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1666.1">'</span></span><span class="koboSpan" id="kobo.1667.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1668.1">' '</span></span><span class="koboSpan" id="kobo.1669.1">}
train[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1670.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1671.1">] = train[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1672.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1673.1">].apply(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1674.1">lambda</span></span><span class="koboSpan" id="kobo.1675.1"> x: clean_special_chars(x, punct_mapping, more_puncts))
test[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1676.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1677.1">] = test[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1678.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1679.1">].apply(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1680.1">lambda</span></span><span class="koboSpan" id="kobo.1681.1"> x: clean_special_chars(x, punct_mapping, more_puncts))
%%time
df = pd.concat([train ,test], sort=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1682.1">False</span></span><span class="koboSpan" id="kobo.1683.1">)
vocab = build_vocabulary(df[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1684.1">'comment_text'</span></span><span class="koboSpan" id="kobo.1685.1">])
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1686.1">print</span></span><span class="koboSpan" id="kobo.1687.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1688.1">"Check coverage after additional punctuation replacement"</span></span><span class="koboSpan" id="kobo.1689.1">)
oov_glove = check_coverage(vocab, embed_glove)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1690.1">There is a trivial improvement this time, but we can continue with addressing either frequent expressions or frequent special character replacements until we get a significant improvement.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1691.1">An alternative way to further improve the comments corpus vocabulary coverage by the embeddings is to add an additional embedding source to current pretrained embeddings. </span><span class="koboSpan" id="kobo.1691.2">Let’s try this. </span><span class="koboSpan" id="kobo.1691.3">We used pretrained embeddings from </span><code class="inlineCode"><span class="koboSpan" id="kobo.1692.1">GloVe</span></code><span class="koboSpan" id="kobo.1693.1">. </span><span class="koboSpan" id="kobo.1693.2">We can also use </span><code class="inlineCode"><span class="koboSpan" id="kobo.1694.1">FastText</span></code><span class="koboSpan" id="kobo.1695.1"> from Facebook. </span><code class="inlineCode"><span class="koboSpan" id="kobo.1696.1">FastText</span></code><span class="koboSpan" id="kobo.1697.1"> is a very practical industry-standard library that is commonly used in search and recommendation engines in several companies daily. </span><span class="koboSpan" id="kobo.1697.2">Let us load the embeddings and recreate the embeddings index with the combined embeddings vectors.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1698.1">After we merge both word embedding dictionaries, with dimensions of 2.19 million and 2.0 million entries (both with a </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.1699.1">vector dimension of 300), we obtain a dictionary with a dimension of 2.8 million entries (due to many common words in the two dictionaries). </span><span class="koboSpan" id="kobo.1699.2">We then recalculate the vocabulary coverage. </span><span class="koboSpan" id="kobo.1699.3">In </span><em class="italic"><span class="koboSpan" id="kobo.1700.1">Figure 7.24</span></em><span class="koboSpan" id="kobo.1701.1">, we show the result of this operation.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1702.1"><img alt="A close-up of numbers  Description automatically generated" src="../Images/B20963_07_24.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1703.1">Figure 7.24: Vocabulary coverage – fifth iteration, after adding the FastText pretrained word embeddings to the initial GloVe embedding dictionary</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1704.1">To summarize our process here, our intention was to build a baseline solution based on the use of pretrained word embeddings. </span><span class="koboSpan" id="kobo.1704.2">We introduced two pretrained word embedding algorithms, </span><code class="inlineCode"><span class="koboSpan" id="kobo.1705.1">GloVe</span></code><span class="koboSpan" id="kobo.1706.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.1707.1">FastText</span></code><span class="koboSpan" id="kobo.1708.1">. </span><span class="koboSpan" id="kobo.1708.2">Pretrained means that we used the already trained algorithms; we didn’t calculate the word embeddings from the corpus of comments in our dataset. </span><span class="koboSpan" id="kobo.1708.3">To be effective, we need to ensure that we have good coverage with these word embeddings of the comments text vocabulary. </span><span class="koboSpan" id="kobo.1708.4">Initially, the coverage was rather poor (15% of the vocabulary and 86% of the entire text). </span><span class="koboSpan" id="kobo.1708.5">We improved these statistics gradually by transforming to lowercase, removing contractions, removing punctuation, and replacing special characters. </span><span class="koboSpan" id="kobo.1708.6">In the last step, we extended the embeddings dictionary by adding pretrained embeddings from an alternative source. </span><span class="koboSpan" id="kobo.1708.7">In the end, we were able to ensure a 56% coverage of the vocabulary and 99.75% of the entire text.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1709.1">The next step is to go ahead and create a baseline model in a separate notebook. </span><span class="koboSpan" id="kobo.1709.2">We will only reuse a part of the functions we created for the experiments in the current notebook.</span></p>
<h1 class="heading-1" id="_idParaDest-108"><span class="koboSpan" id="kobo.1710.1">Building a baseline model</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1711.1">These days, everybody will build a baseline model by at least fine-tuning a Transformer architecture. </span><span class="koboSpan" id="kobo.1711.2">Since the 2017 paper </span><em class="italic"><span class="koboSpan" id="kobo.1712.1">Attention Is All You Need</span></em><span class="koboSpan" id="kobo.1713.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.1714.1">Reference 14</span></em><span class="koboSpan" id="kobo.1715.1">), the performance of these solutions has continuously improved, and for competitions like </span><em class="italic"><span class="koboSpan" id="kobo.1716.1">Jigsaw Unintended Bias in Toxicity Classification</span></em><span class="koboSpan" id="kobo.1717.1">, a recent Transformer-based solution will probably take you easily into the gold zone.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1718.1">In this exercise, we will start with a</span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.1719.1"> more classical baseline. </span><span class="koboSpan" id="kobo.1719.2">The core of this solution is based on contributions from Christof Henkel (Kaggle nickname: Dieter), Ane Berasategi (Kaggle nickname: Ane), Andrew Lukyanenko (Kaggle nickname: Artgor), Thousandvoices (Kaggle nickname), and Tanrei (Kaggle nickname); see </span><em class="italic"><span class="koboSpan" id="kobo.1720.1">References</span></em> <em class="italic"><span class="koboSpan" id="kobo.1721.1">12</span></em><span class="koboSpan" id="kobo.1722.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1723.1">13</span></em><span class="koboSpan" id="kobo.1724.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1725.1">15</span></em><span class="koboSpan" id="kobo.1726.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1727.1">16</span></em><span class="koboSpan" id="kobo.1728.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1729.1">17</span></em><span class="koboSpan" id="kobo.1730.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.1731.1">18</span></em><span class="koboSpan" id="kobo.1732.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1733.1">The solution includes four steps. </span><span class="koboSpan" id="kobo.1733.2">In the first step, we load the train and test data as </span><code class="inlineCode"><span class="koboSpan" id="kobo.1734.1">pandas</span></code><span class="koboSpan" id="kobo.1735.1"> datasets and then we perform preprocessing on the two datasets. </span><span class="koboSpan" id="kobo.1735.2">The preprocessing is largely based on the preprocessing steps we performed before, and hence, we won’t repeat those steps here.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1736.1">In the second step, we perform tokenization and prepare the data to present it to the model. </span><span class="koboSpan" id="kobo.1736.2">The tokenization is performed as shown in the following code excerpt (we are not showing the entire procedure here):</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.1737.1">    logger.info(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1738.1">'Fitting tokenizer'</span></span><span class="koboSpan" id="kobo.1739.1">)
    tokenizer = Tokenizer() 
    tokenizer.fit_on_texts(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1740.1">list</span></span><span class="koboSpan" id="kobo.1741.1">(train[COMMENT_TEXT_COL]) + </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1742.1">list</span></span><span class="koboSpan" id="kobo.1743.1">(test[COMMENT_TEXT_COL]))
    word_index = tokenizer.word_index
    X_train = tokenizer.texts_to_sequences(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1744.1">list</span></span><span class="koboSpan" id="kobo.1745.1">(train[COMMENT_TEXT_COL]))
    X_test = tokenizer.texts_to_sequences(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1746.1">list</span></span><span class="koboSpan" id="kobo.1747.1">(test[COMMENT_TEXT_COL]))
    X_train = pad_sequences(X_train, maxlen=MAX_LEN)
    X_test = pad_sequences(X_test, maxlen=MAX_LEN)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1748.1">We used a basic tokenizer here from </span><code class="inlineCode"><span class="koboSpan" id="kobo.1749.1">keras.preprocessing.text</span></code><span class="koboSpan" id="kobo.1750.1">. </span><span class="koboSpan" id="kobo.1750.2">After tokenization, each input sequence is padded with a predefined </span><code class="inlineCode"><span class="koboSpan" id="kobo.1751.1">MAX_LEN</span></code><span class="koboSpan" id="kobo.1752.1">, which was selected as an optimum considering the average/median length of sequences for the entire comments corpus and also considering the available memory and runtime constraints.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1753.1">In the third step, we build the embedding matrix and the model structure. </span><span class="koboSpan" id="kobo.1753.2">The code for building the embedding matrix is largely based on the procedures we already presented in the previous sections. </span><span class="koboSpan" id="kobo.1753.3">Here, we just systematize it:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1754.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1755.1">build_embedding_matrix</span></span><span class="koboSpan" id="kobo.1756.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1757.1">word_index, path</span></span><span class="koboSpan" id="kobo.1758.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1759.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1760.1">     Build embeddings</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1761.1">    '''</span></span><span class="koboSpan" id="kobo.1762.1">
    logger.info(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1763.1">'Build embedding matrix'</span></span><span class="koboSpan" id="kobo.1764.1">)
    embedding_index = load_embeddings(path)
    embedding_matrix = np.zeros((</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1765.1">len</span></span><span class="koboSpan" id="kobo.1766.1">(word_index) + </span><span class="hljs-number"><span class="koboSpan" id="kobo.1767.1">1</span></span><span class="koboSpan" id="kobo.1768.1">, EMB_MAX_FEAT))
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1769.1">for</span></span><span class="koboSpan" id="kobo.1770.1"> word, i </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1771.1">in</span></span><span class="koboSpan" id="kobo.1772.1"> word_index.items():
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1773.1">try</span></span><span class="koboSpan" id="kobo.1774.1">:
            embedding_matrix[i] = embedding_index[word]
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1775.1">except</span></span><span class="koboSpan" id="kobo.1776.1"> KeyError:
            </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1777.1">pass</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.1778.1">except</span></span><span class="koboSpan" id="kobo.1779.1">:
            embedding_matrix[i] = embeddings_index[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1780.1">"unknown"</span></span><span class="koboSpan" id="kobo.1781.1">]
            
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1782.1">del</span></span><span class="koboSpan" id="kobo.1783.1"> embedding_index
    gc.collect()
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1784.1">return</span></span><span class="koboSpan" id="kobo.1785.1"> embedding_matrix
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1786.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1787.1">build_embeddings</span></span><span class="koboSpan" id="kobo.1788.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1789.1">word_index</span></span><span class="koboSpan" id="kobo.1790.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1791.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1792.1">     Build embeddings</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1793.1">    '''</span></span><span class="koboSpan" id="kobo.1794.1">
    logger.info(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1795.1">'Load and build embeddings'</span></span><span class="koboSpan" id="kobo.1796.1">)
    embedding_matrix = np.concatenate(
        [build_embedding_matrix(word_index, f) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1797.1">for</span></span><span class="koboSpan" id="kobo.1798.1"> f </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1799.1">in</span></span><span class="koboSpan" id="kobo.1800.1"> EMB_PATHS], axis=-</span><span class="hljs-number"><span class="koboSpan" id="kobo.1801.1">1</span></span><span class="koboSpan" id="kobo.1802.1">) 
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1803.1">return</span></span><span class="koboSpan" id="kobo.1804.1"> embedding_matrix
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1805.1">The model is a deep learning architecture with a word embeddings layer, a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1806.1">SpatialDropout1D</span></code><span class="koboSpan" id="kobo.1807.1"> layer, two bidirectional LSTM layers, a concatenation of </span><code class="inlineCode"><span class="koboSpan" id="kobo.1808.1">GlobalMaxPooling1D</span></code><span class="koboSpan" id="kobo.1809.1"> with a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1810.1">GlobalAveragePooling1D</span></code><span class="koboSpan" id="kobo.1811.1">, two dense layers with </span><code class="inlineCode"><span class="koboSpan" id="kobo.1812.1">'relu'</span></code><span class="koboSpan" id="kobo.1813.1"> activation, and one dense layer with </span><code class="inlineCode"><span class="koboSpan" id="kobo.1814.1">'sigmoid'</span></code><span class="koboSpan" id="kobo.1815.1"> activation for the target output.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1816.1">In the word embedding layer, the input is transformed so that each word is represented by its corresponding vector. </span><span class="koboSpan" id="kobo.1816.2">After this transformation, the information about the semantic distance between words in the input is captured by the model. </span><span class="koboSpan" id="kobo.1816.3">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.1817.1">SpatialDropout1D</span></code><span class="koboSpan" id="kobo.1818.1"> layer helps prevent overfitting by randomly deactivating neurons during training (the coefficient gives the percentage of neurons deactivated each epoch). </span><span class="koboSpan" id="kobo.1818.2">The bidirectional LSTM layer’s role is to process the input sequences in both forward and backward directions, enhancing</span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.1819.1"> contextual understanding for better predictions. </span><span class="koboSpan" id="kobo.1819.2">The role of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1820.1">GlobalAveragePooling1D</span></code><span class="koboSpan" id="kobo.1821.1"> layer is to compute the average of each feature across the entire sequence, reducing the dimensionality while retaining essential information in the 1D (sequential) data. </span><span class="koboSpan" id="kobo.1821.2">This amounts to revealing a latent representation of the sequences. </span><span class="koboSpan" id="kobo.1821.3">The dense layers’ output is the prediction of the model. </span><span class="koboSpan" id="kobo.1821.4">See </span><em class="italic"><span class="koboSpan" id="kobo.1822.1">References 17</span></em><span class="koboSpan" id="kobo.1823.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1824.1">18</span></em><span class="koboSpan" id="kobo.1825.1"> for more details regarding the implementation:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1826.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1827.1">build_model</span></span><span class="koboSpan" id="kobo.1828.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1829.1">embedding_matrix, num_aux_targets, loss_weight</span></span><span class="koboSpan" id="kobo.1830.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1831.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1832.1">        Build model</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1833.1">    '''</span></span><span class="koboSpan" id="kobo.1834.1">
    logger.info(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1835.1">'Build model'</span></span><span class="koboSpan" id="kobo.1836.1">)
    words = Input(shape=(MAX_LEN,))
    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1837.1">False</span></span><span class="koboSpan" id="kobo.1838.1">)(words)
    x = SpatialDropout1D(</span><span class="hljs-number"><span class="koboSpan" id="kobo.1839.1">0.3</span></span><span class="koboSpan" id="kobo.1840.1">)(x)
    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1841.1">True</span></span><span class="koboSpan" id="kobo.1842.1">))(x)
    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1843.1">True</span></span><span class="koboSpan" id="kobo.1844.1">))(x)
    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])
    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1845.1">'relu'</span></span><span class="koboSpan" id="kobo.1846.1">)(hidden)])
    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1847.1">'relu'</span></span><span class="koboSpan" id="kobo.1848.1">)(hidden)])
    result = Dense(</span><span class="hljs-number"><span class="koboSpan" id="kobo.1849.1">1</span></span><span class="koboSpan" id="kobo.1850.1">, activation=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1851.1">'sigmoid'</span></span><span class="koboSpan" id="kobo.1852.1">)(hidden)
    aux_result = Dense(num_aux_targets, activation=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1853.1">'sigmoid'</span></span><span class="koboSpan" id="kobo.1854.1">)(hidden)
    
    model = Model(inputs=words, outputs=[result, aux_result])
    model.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1855.1">compile</span></span><span class="koboSpan" id="kobo.1856.1">(loss=[custom_loss,</span><span class="hljs-string"><span class="koboSpan" id="kobo.1857.1">'binary_crossentropy'</span></span><span class="koboSpan" id="kobo.1858.1">], loss_weights=[loss_weight, </span><span class="hljs-number"><span class="koboSpan" id="kobo.1859.1">1.0</span></span><span class="koboSpan" id="kobo.1860.1">], optimizer=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1861.1">'adam'</span></span><span class="koboSpan" id="kobo.1862.1">)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1863.1">return</span></span><span class="koboSpan" id="kobo.1864.1"> model
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1865.1">In the fourth step, we run the </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.1866.1">training, prepare the submission, and submit. </span><span class="koboSpan" id="kobo.1866.2">To reduce the memory used during runtime, we are using temporary storage and performing garbage collection after deleting non-used allocated data. </span><span class="koboSpan" id="kobo.1866.3">We run the model twice for a specified number of </span><code class="inlineCode"><span class="koboSpan" id="kobo.1867.1">NUM_EPOCHS</span></code><span class="koboSpan" id="kobo.1868.1"> (representing one complete pass of training data through the algorithm) and then average the test predictions using variable weights. </span><span class="koboSpan" id="kobo.1868.2">Then we submit the predictions:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1869.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1870.1">run_model</span></span><span class="koboSpan" id="kobo.1871.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1872.1">X_train, y_train, y_aux_train, embedding_matrix, word_index, loss_weight</span></span><span class="koboSpan" id="kobo.1873.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.1874.1">'''</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1875.1">        Run model</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.1876.1">    '''</span></span><span class="koboSpan" id="kobo.1877.1">
    logger.info(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1878.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1879.1">Run model'</span></span><span class="koboSpan" id="kobo.1880.1">)
    
    checkpoint_predictions = []
    weights = []
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1881.1">for</span></span><span class="koboSpan" id="kobo.1882.1"> model_idx </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1883.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1884.1">range</span></span><span class="koboSpan" id="kobo.1885.1">(NUM_MODELS):
        model = build_model(embedding_matrix, y_aux_train.shape[-</span><span class="hljs-number"><span class="koboSpan" id="kobo.1886.1">1</span></span><span class="koboSpan" id="kobo.1887.1">], loss_weight)
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1888.1">for</span></span><span class="koboSpan" id="kobo.1889.1"> global_epoch </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1890.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1891.1">range</span></span><span class="koboSpan" id="kobo.1892.1">(NUM_EPOCHS):
            model.fit(
                X_train, [y_train, y_aux_train],
                batch_size=BATCH_SIZE, epochs=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1893.1">1</span></span><span class="koboSpan" id="kobo.1894.1">, verbose=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1895.1">1</span></span><span class="koboSpan" id="kobo.1896.1">,
                callbacks=[LearningRateScheduler(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1897.1">lambda</span></span><span class="koboSpan" id="kobo.1898.1"> epoch: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1899.1">1.1e-3</span></span><span class="koboSpan" id="kobo.1900.1"> * (</span><span class="hljs-number"><span class="koboSpan" id="kobo.1901.1">0.55</span></span><span class="koboSpan" id="kobo.1902.1"> ** global_epoch))]
            )
            </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1903.1">with</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1904.1">open</span></span><span class="koboSpan" id="kobo.1905.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1906.1">'temporary.pickle'</span></span><span class="koboSpan" id="kobo.1907.1">, mode=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1908.1">'rb'</span></span><span class="koboSpan" id="kobo.1909.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1910.1">as</span></span><span class="koboSpan" id="kobo.1911.1"> f:
                X_test = pickle.load(f) </span><span class="hljs-comment"><span class="koboSpan" id="kobo.1912.1"># use temporary file to reduce memory</span></span><span class="koboSpan" id="kobo.1913.1">
            checkpoint_predictions.append(model.predict(X_test, batch_size=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1914.1">1024</span></span><span class="koboSpan" id="kobo.1915.1">)[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1916.1">0</span></span><span class="koboSpan" id="kobo.1917.1">].flatten())
            </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1918.1">del</span></span><span class="koboSpan" id="kobo.1919.1"> X_test
            gc.collect()
            weights.append(</span><span class="hljs-number"><span class="koboSpan" id="kobo.1920.1">2</span></span><span class="koboSpan" id="kobo.1921.1"> ** global_epoch)
        </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1922.1">del</span></span><span class="koboSpan" id="kobo.1923.1"> model
        gc.collect()
    
    preds = np.average(checkpoint_predictions, weights=weights, axis=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1924.1">0</span></span><span class="koboSpan" id="kobo.1925.1">)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1926.1">return</span></span><span class="koboSpan" id="kobo.1927.1"> preds
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1928.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1929.1">submit</span></span><span class="koboSpan" id="kobo.1930.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1931.1">sub_preds</span></span><span class="koboSpan" id="kobo.1932.1">):
    logger.info(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1933.1">'Prepare submission'</span></span><span class="koboSpan" id="kobo.1934.1">)
    submission = pd.read_csv(os.path.join(JIGSAW_PATH,</span><span class="hljs-string"><span class="koboSpan" id="kobo.1935.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1936.1">sample_submission.csv'</span></span><span class="koboSpan" id="kobo.1937.1">), index_col=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1938.1">'id'</span></span><span class="koboSpan" id="kobo.1939.1">)
    submission[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1940.1">'prediction'</span></span><span class="koboSpan" id="kobo.1941.1">] = sub_preds
    submission.reset_index(drop=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1942.1">False</span></span><span class="koboSpan" id="kobo.1943.1">, inplace=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1944.1">True</span></span><span class="koboSpan" id="kobo.1945.1">)
    submission.to_csv(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1946.1">'submission.csv'</span></span><span class="koboSpan" id="kobo.1947.1">, index=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1948.1">False</span></span><span class="koboSpan" id="kobo.1949.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.1950.1">With this solution (for the full code, see </span><em class="italic"><span class="koboSpan" id="kobo.1951.1">Reference 16</span></em><span class="koboSpan" id="kobo.1952.1">), we can obtain, via a late submission, a core of 0.9328 and, consequently, a ranking in the upper half of the private leaderboard. </span><span class="koboSpan" id="kobo.1952.2">Next, we </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.1953.1">will show how, by using a Transformer-based solution, we can obtain a higher score, in the upper silver medal or even gold medal zone for this competition.</span></p>
<h1 class="heading-1" id="_idParaDest-109"><span class="koboSpan" id="kobo.1954.1">Transformer-based solution</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1955.1">At the time of the competition, BERT and some other Transformer models were already available and a few solutions with high scores were provided. </span><span class="koboSpan" id="kobo.1955.2">Here, we will not attempt to replicate them but we will just point out the most accessible implementations.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1956.1">In </span><em class="italic"><span class="koboSpan" id="kobo.1957.1">Reference 20</span></em><span class="koboSpan" id="kobo.1958.1">, Qishen Ha combines a few solutions, including BERT-Small V2, BERT-Large V2, XLNet, and GPT-2 (fine-tuned </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.1959.1">models using competition data included as datasets) to obtain a 0.94656 private leaderboard score (late submission), which would put you in the top 10 (both gold medal and prize area for this competition).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1960.1">A solution with only the BERT-Small model (see </span><em class="italic"><span class="koboSpan" id="kobo.1961.1">Reference 21</span></em><span class="koboSpan" id="kobo.1962.1">) will yield a private leaderboard score of 0.94295. </span><span class="koboSpan" id="kobo.1962.2">Using the BERT-Large model (see </span><em class="italic"><span class="koboSpan" id="kobo.1963.1">Reference 22</span></em><span class="koboSpan" id="kobo.1964.1">) will result in a private leaderboard score of 0.94388. </span><span class="koboSpan" id="kobo.1964.2">Both these solutions will be in the silver medal zone (around places 130 and 80, respectively, in the private leaderboard, as late submissions).</span></p>
<h1 class="heading-1" id="_idParaDest-110"><span class="koboSpan" id="kobo.1965.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1966.1">In this chapter, we learned how to work with text data, using various approaches to explore this type of data. </span><span class="koboSpan" id="kobo.1966.2">We started by analyzing our target and text data, preprocessing text data to include it in a machine learning model. </span><span class="koboSpan" id="kobo.1966.3">We also explored various NLP tools and techniques, including topic modeling, NER, and POS tagging, and then prepared the text to build a baseline model, passing through an iterative process to gradually improve the data quality for the objective set (in this case, the objective being to improve the coverage of word embeddings for the vocabulary in the corpus of text from the competition dataset).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1967.1">We introduced and discussed a baseline model (based on the work of several Kaggle contributors). </span><span class="koboSpan" id="kobo.1967.2">This baseline model architecture includes a word embedding layer and bidirectional LSTM layers. </span><span class="koboSpan" id="kobo.1967.3">Finally, we looked at some of the most advanced solutions available, based on Transformer architectures, either as single models or combined, to get a late submission with a score in the upper part of the leaderboard (silver and gold zones).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1968.1">In the next chapter, we will start working with signal data. </span><span class="koboSpan" id="kobo.1968.2">We will introduce data formats specific to various signal modalities (sound, image, video, experimental, or sensor data). </span><span class="koboSpan" id="kobo.1968.3">We will analyze the data from the </span><em class="italic"><span class="koboSpan" id="kobo.1969.1">LANL Earthquake Prediction</span></em><span class="koboSpan" id="kobo.1970.1"> Kaggle competition.</span></p>
<h1 class="heading-1" id="_idParaDest-111"><span class="koboSpan" id="kobo.1971.1">References</span></h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.1972.1">Jigsaw Unintended Bias in Toxicity Classification, Kaggle competition dataset: </span><a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/ "><span class="url"><span class="koboSpan" id="kobo.1973.1">https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1974.1">Aja Bogdanoff, Saying goodbye to Civil Comments, Medium: </span><a href="mailto:https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d"><span class="url"><span class="koboSpan" id="kobo.1975.1">https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1976.1">Gabriel Preda, Jigsaw Comments Text Exploration: </span><a href="https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-comments-text-exploration.ipynb"><span class="url"><span class="koboSpan" id="kobo.1977.1">h</span></span><span class="url"><span class="koboSpan" id="kobo.1978.1">ttps://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-comments-text-exploration.ipynb</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1979.1">Gabriel Preda, Jigsaw Simple Baseline: </span><a href="https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-simple-baseline.ipynb"><span class="url"><span class="koboSpan" id="kobo.1980.1">https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-simple-baseline.ipynb</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1981.1">Susan Li, Topic Modeling and Latent Dirichlet Allocation (LDA) in Python: </span><a href="https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"><span class="url"><span class="koboSpan" id="kobo.1982.1">https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1983.1">Aneesha Bakharia, Improving the Interpretation of Topic Models: </span><a href="https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d "><span class="url"><span class="koboSpan" id="kobo.1984.1">https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1985.1">Carson Sievert, Kenneth Shirley, LDAvis: A method for visualizing and interpreting topics: </span><a href="https://www.aclweb.org/anthology/W14-3110"><span class="url"><span class="koboSpan" id="kobo.1986.1">https://www.aclweb.org/anthology/W14-3110</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1987.1">Lucia Dosin, Experiments on Topic Modeling – PyLDAvis: </span><a href="https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/"><span class="url"><span class="koboSpan" id="kobo.1988.1">https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1989.1">Renato Aranha, Topic Modelling (LDA) on Elon Tweets: </span><a href="https://www.kaggle.com/errearanhas/topic-modelling-lda-on-elon-tweets"><span class="url"><span class="koboSpan" id="kobo.1990.1">https://www.kaggle.com/errearanhas/topic-modelling-lda-on-elon-tweets</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1991.1">Latent Dirichlet Allocation, Wikipedia: </span><a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"><span class="url"><span class="koboSpan" id="kobo.1992.1">https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1993.1">Leonie Monigatti, Visualizing Part-of-Speech Tags with NLTK and SpaCy: </span><a href="https://towardsdatascience.com/visualizing-part-of-speech-tags-with-nltk-and-spacy-42056fcd777e"><span class="url"><span class="koboSpan" id="kobo.1994.1">https://towardsdatascience.com/visualizing-part-of-speech-tags-with-nltk-and-spacy-42056fcd777e</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1995.1">Ane, Quora preprocessing + model: </span><a href="https://www.kaggle.com/anebzt/quora-preprocessing-model"><span class="url"><span class="koboSpan" id="kobo.1996.1">https://www.kaggle.com/anebzt/quora-preprocessing-model</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1997.1">Christof Henkel (Dieter), How to: Preprocessing when using embeddings: </span><a href="https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings"><span class="url"><span class="koboSpan" id="kobo.1998.1">https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1999.1">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. </span><span class="koboSpan" id="kobo.1999.2">Gomez, Lukasz Kaiser, Illia Polosukhin, Attention Is All You Need: </span><a href="https://arxiv.org/abs/1706.03762"><span class="url"><span class="koboSpan" id="kobo.2000.1">https://arxiv.org/abs/1706.03762</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.2001.1">Christof Henkel (Dieter), keras baseline lstm + attention 5-fold: </span><a href="https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold "><span class="url"><span class="koboSpan" id="kobo.2002.1">https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.2003.1">Andrew Lukyanenko, CNN in keras on folds: </span><a href="https://www.kaggle.com/code/artgor/cnn-in-keras-on-folds"><span class="url"><span class="koboSpan" id="kobo.2004.1">https://www.kaggle.com/code/artgor/cnn-in-keras-on-folds</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.2005.1">Thousandvoices, Simple LSTM: </span><a href="https://www.kaggle.com/code/thousandvoices/simple-lstm/s"><span class="url"><span class="koboSpan" id="kobo.2006.1">https://www.kaggle.com/code/thousandvoices/simple-lstm/s</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.2007.1">Tanrei, Simple LSTM using Identity Parameters Solution: </span><a href="https://www.kaggle.com/code/tanreinama/simple-lstm-using-identity-parameters-solution"><span class="url"><span class="koboSpan" id="kobo.2008.1">https://www.kaggle.com/code/tanreinama/simple-lstm-using-identity-parameters-solution</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.2009.1">Gabriel Preda, Jigsaw Simple Baseline: </span><a href="https://www.kaggle.com/code/gpreda/jigsaw-simple-baseline"><span class="url"><span class="koboSpan" id="kobo.2010.1">https://www.kaggle.com/code/gpreda/jigsaw-simple-baseline</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.2011.1">Qishen Ha, Jigsaw_predict: </span><a href="https://www.kaggle.com/code/haqishen/jigsaw-predict/"><span class="url"><span class="koboSpan" id="kobo.2012.1">https://www.kaggle.com/code/haqishen/jigsaw-predict/</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.2013.1">Gabriel Preda, Jigsaw_predict_BERT_small: </span><a href="https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-small"><span class="url"><span class="koboSpan" id="kobo.2014.1">https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-small</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.2015.1">Gabriel Preda, Jigsaw_predict_BERT_large: </span><a href="https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-large"><span class="url"><span class="koboSpan" id="kobo.2016.1">https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-large</span></span></a></li>
</ol>
<h1 class="heading-1"><span class="koboSpan" id="kobo.2017.1">Join our book’s Discord space</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.2018.1">Join our Discord community to meet like-minded people and learn alongside more than 5000 members at:</span></p>
<p class="normal"><a href="https://packt.link/kaggle"><span class="url"><span class="koboSpan" id="kobo.2019.1">https://packt.link/kaggle</span></span></a></p>
<p class="normal"><span class="koboSpan" id="kobo.2020.1"><img alt="" role="presentation" src="../Images/QR_Code9220780366773140.png"/></span></p>
</div>
</body></html>