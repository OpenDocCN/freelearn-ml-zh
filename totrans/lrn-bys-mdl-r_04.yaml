- en: Chapter 4. Machine Learning Using Bayesian Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned about Bayesian inference and R, it is time to use both
    for machine learning. In this chapter, we will give an overview of different machine
    learning techniques and discuss each of them in detail in subsequent chapters.
    Machine learning is a field at the intersection of computer science and statistics,
    and a subbranch of artificial intelligence or AI. The name essentially comes from
    the early works in AI where researchers were trying to develop learning machines
    that automatically learned the relationship between input and output variables
    from data alone. Once a machine is trained on a dataset for a given problem, it
    can be used as a black box to predict values of output variables for new values
    of input variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is useful to set this learning process of a machine in a mathematical framework.
    Let *X* and *Y* be two random variables such that we seek a learning machine that
    learns the relationship between these two variables from data and predicts the
    value of *Y*, given the value of *X*. The system is fully characterized by a joint
    probability distribution *P(X,Y)*, however, the form of this distribution is unknown.
    The goal of learning is to find a function *f(X)*, which maps from *X* to *Y*,
    such that the predictions ![Machine Learning Using Bayesian Inference](img/image00379.jpeg)
    contain as small error as possible. To achieve this, one chooses a loss function
    *L(Y, f(X))* and finds an *f(X)* that minimizes the expected or average loss over
    the joint distribution of *X* and *Y* given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine Learning Using Bayesian Inference](img/image00380.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In Statistical Decision Theory, this is called **empirical risk minimization**.
    The typical loss function used is **square loss function** (![Machine Learning
    Using Bayesian Inference](img/image00381.jpeg)), if *Y* is a continuous variable,
    and **Hinge loss function** (![Machine Learning Using Bayesian Inference](img/image00382.jpeg)),
    if *Y* is a binary discrete variable with values ![Machine Learning Using Bayesian
    Inference](img/image00383.jpeg). The first case is typically called **regression**
    and second case is called **binary classification**, as we will see later in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical framework described here is called **supervised learning**,
    where the machine is presented with a training dataset containing ground truth
    values corresponding to pairs *(Y, X)*. Let us consider the case of square loss
    function again. Here, the learning task is to find an *f(X)* that minimizes the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine Learning Using Bayesian Inference](img/image00384.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the objective is to predict values of *Y* for the given values of *X*,
    we have used the conditional distribution *P(Y|X)* inside the integral using factorization
    of *P(X, Y)*. It can be shown that the minimization of the preceding loss function
    leads to the following solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine Learning Using Bayesian Inference](img/image00385.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The meaning of the preceding equation is that the best prediction of *Y* for
    any input value *X* is the mean or expectation denoted by *E*, of the conditional
    probability distribution *P(Y|X)* conditioned at *X*.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](part0030.xhtml#aid-SJGS2 "Chapter 3. Introducing Bayesian Inference"),
    *Introducing Bayesian Inference*, we mentioned **maximum likelihood estimation**
    (**MLE**) as a method for learning the parameters ![Machine Learning Using Bayesian
    Inference](img/image00386.jpeg) of any distribution *P(X)*. In general, MLE is
    the same as the minimization of a square loss function if the underlying distribution
    is a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in empirical risk minimization, we are learning the parameter, *E[(Y|X)]*,
    the mean of the conditional distribution, for a given value of *X*. We will use
    one particular machine learning task, linear regression, to explain the advantage
    of Bayesian inference over the classical method of learning. However, before this,
    we will briefly explain some more general aspects of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of supervised machine learning models, namely generative
    models and discriminative models. In the case of generative models, the algorithm
    tries to learn the joint probability of *X* and *Y*, which is *P(X,Y)*, from data
    and uses it to estimate mean *P(Y|X)*. In the case of discriminative models, the
    algorithm tries to directly learn the desired function, which is the mean of *P(Y|X)*,
    and no modeling of the *X* variable is attempted.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling values of the target variable in the training data is done manually.
    This makes supervised learning very expensive when one needs to use very large
    datasets as in the case of text analytics. However, very often, supervised learning
    methods produce the most accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: If there is not enough training data available for learning, one can still use
    machine learning through **unsupervised learning**. Here, the learning is mainly
    through the discovery of patterns of associations between variables in the dataset.
    Clustering data points that have similar features is a classic example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement learning** is the third type of machine learning, where the
    learning takes place in a dynamic environment where the machine needs to perform
    certain actions based on its current state. Associated with each action is a reward.
    The machine needs to learn what action needs to be taken at each state so that
    the total reward is maximized. This is typically how a robot learns to perform
    tasks, such as driving a vehicle, in a real-life environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Bayesian inference for machine learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed the advantages of Bayesian statistics over classical
    statistics in the last chapter. In this chapter, we will see in more detail how
    some of the concepts of Bayesian inference that we learned in the last chapter
    are useful in the context of machine learning. For this purpose, we take one simple
    machine learning task, namely linear regression. Let us consider a learning task
    where we have a dataset *D* containing *N* pair of points ![Why Bayesian inference
    for machine learning?](img/image00387.jpeg) and the goal is to build a machine
    learning model using linear regression that it can be used to predict values of
    ![Why Bayesian inference for machine learning?](img/image00388.jpeg), given new
    values of ![Why Bayesian inference for machine learning?](img/image00389.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'In linear regression, first, we assume that *Y* is of the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why Bayesian inference for machine learning?](img/image00390.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *F(X)* is a function that captures the true relationship between *X* and
    *Y*, and ![Why Bayesian inference for machine learning?](img/image00391.jpeg)
    is an error term that captures the inherent noise in the data. It is assumed that
    this noise is characterized by a normal distribution with mean 0 and variance
    ![Why Bayesian inference for machine learning?](img/image00392.jpeg). What this
    implies is that if we have an infinite training dataset, we can learn the form
    of *F(X)* from data and, even then, we can only predict *Y* up to an additive
    noise term ![Why Bayesian inference for machine learning?](img/image00391.jpeg).
    In practice, we will have only a finite training dataset *D*; hence, we will be
    able to learn only an approximation for *F(X)* denoted by ![Why Bayesian inference
    for machine learning?](img/image00393.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are discussing two types of errors here. One is an error term ![Why
    Bayesian inference for machine learning?](img/image00391.jpeg) that is due to
    the inherent noise in the data that we cannot do much about. The second error
    is in learning *F(X)*, approximately through the function ![Why Bayesian inference
    for machine learning?](img/image00393.jpeg) from the dataset *D*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, ![Why Bayesian inference for machine learning?](img/image00393.jpeg),
    which the approximate mapping between input variable *X* and output variable *Y*,
    is a function of *X* with a set of parameters ![Why Bayesian inference for machine
    learning?](img/image00386.jpeg). When ![Why Bayesian inference for machine learning?](img/image00393.jpeg)
    is a linear function of the parameters ![Why Bayesian inference for machine learning?](img/image00386.jpeg),
    we say the learning model is linear. It is a general misconception that linear
    regression corresponds to the case only if ![Why Bayesian inference for machine
    learning?](img/image00393.jpeg) is a linear function of *X*. The reason for linearity
    in the parameter and not in *X* is that, during the minimization of the loss function,
    one actually minimizes over the parameter values to find the best ![Why Bayesian
    inference for machine learning?](img/image00393.jpeg). Hence, a function that
    is linear in ![Why Bayesian inference for machine learning?](img/image00386.jpeg)
    will lead to a linear optimization problem that can be tackled analytically and
    numerically more easily. Therefore, linear regression corresponds to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why Bayesian inference for machine learning?](img/image00394.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is an expansion over a set of *M* basis functions ![Why Bayesian inference
    for machine learning?](img/image00395.jpeg). Here, each basis function ![Why Bayesian
    inference for machine learning?](img/image00396.jpeg) is a function of *X* without
    any unknown parameters. In machine learning, these are called feature functions
    or model features. For the linear regression problem, the loss function, therefore,
    can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why Bayesian inference for machine learning?](img/image00397.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Why Bayesian inference for machine learning?](img/image00398.jpeg) is
    the transpose of the parameter vector ![Why Bayesian inference for machine learning?](img/image00399.jpeg)
    and *B(X)* is the vector composed of the basis functions ![Why Bayesian inference
    for machine learning?](img/image00400.jpeg). Learning ![Why Bayesian inference
    for machine learning?](img/image00393.jpeg) from a dataset implies estimating
    the values of ![Why Bayesian inference for machine learning?](img/image00386.jpeg)
    by minimizing the loss function through some optimization schemes such as gradient
    descent.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to choose as many basis functions as possible to capture interesting
    patterns in the data. However, choosing more numbers of basis functions or features
    will overfit the model in the sense that it will even start fitting the noise
    contained in the data. Overfit will lead to poor predictions on new input data.
    Therefore, it is important to choose an optimum number of best features to maximize
    the predictive accuracy of any machine learning model. In machine learning based
    on classical statistics, this is achieved through what is called **bias-variance
    tradeoff** and **model regularization**. Whereas, in machine learning through
    Bayesian inference, accuracy of a predictive model can be maximized through Bayesian
    model averaging, and there is no need to impose model regularization or bias-variance
    tradeoff. We will learn each of these concepts in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Model overfitting and bias-variance tradeoff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The expected loss mentioned in the previous section can be written as a sum
    of three terms in the case of linear regression using squared loss function, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model overfitting and bias-variance tradeoff](img/image00401.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *Bias* is the difference ![Model overfitting and bias-variance tradeoff](img/image00402.jpeg)
    between the true model *F(X)* and average value of ![Model overfitting and bias-variance
    tradeoff](img/image00393.jpeg) taken over an ensemble of datasets. *Bias* is a
    measure of how much the average prediction over all datasets in the ensemble differs
    from the true regression function *F(X)*. *Variance* is given by ![Model overfitting
    and bias-variance tradeoff](img/image00403.jpeg). It is a measure of extent to
    which the solution for a given dataset varies around the mean over all datasets.
    Hence, *Variance* is a measure of how much the function ![Model overfitting and
    bias-variance tradeoff](img/image00393.jpeg) is sensitive to the particular choice
    of dataset *D*. The third term *Noise*, as mentioned earlier, is the expectation
    of difference ![Model overfitting and bias-variance tradeoff](img/image00404.jpeg)
    between observation and the true regression function, over all the values of *X*
    and *Y*. Putting all these together, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model overfitting and bias-variance tradeoff](img/image00405.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The objective of machine learning is to learn the function ![Model overfitting
    and bias-variance tradeoff](img/image00393.jpeg) from data that minimizes the
    expected loss *E[L]*. One can keep minimizing the bias by keeping more and more
    basis functions in the model and thereby increasing the model's complexity. However,
    since each of the model parameters ![Model overfitting and bias-variance tradeoff](img/image00406.jpeg)
    are learned from a given dataset, the more complex the model becomes, the more
    sensitive its parameter estimation would be to the dataset used. This results
    in increased variance for more complex models. Hence, in any supervised machine
    learning task, there is a tradeoff between model bias and model complexity. One
    has to choose a model of optimum complexity to minimize the error of prediction
    on an unseen dataset. In the classical or frequentist approach, this is done by
    partitioning the labeled data into three sets. One is the training set, the second
    is the validation set, and the third is the test set. Models of different complexity
    that are trained using the training set are evaluated using the validation dataset
    to choose the model with optimum complexity. It is then, finally, evaluated against
    the test set to estimate the prediction error.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting models of optimum complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different ways of selecting models with the right complexity so that
    the prediction error on unseen data is less. Let's discuss each of these approaches
    in the context of the linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Subset selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the subset selection approach, one selects only a subset of the whole set
    of variables, which are significant, for the model. This not only increases the
    prediction accuracy of the model by decreasing model variance, but it is also
    useful from the interpretation point of view. There are different ways of doing
    subset selection, but the following two are the most commonly used approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward selection**: In forward selection, one starts with no variables (intercept
    alone), and by using a greedy algorithm, adds other variables one by one. For
    each step, the variable that most improves the fit is chosen to add to the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backward selection**: In backward selection, one starts with the full model
    and sequentially deletes the variable that has the least impact on the fit. At
    each step, the variable with the least Z-score is selected for elimination. In
    statistics, the Z-score of a random variable is a measure of the standard deviation
    between an element and its mean. A small value of Z-score (typically < 2) indicates
    that the effect of the variable is more likely by chance and is not statistically
    significant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this approach, one adds a penalty term to the loss function that does not
    allow the size of the parameter to become very large during minimization. There
    are two main ways of doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ridge regression**: This simple type of regularization is where the additional
    term is proportional to the magnitude of the parameter vector given by ![Model
    regularization](img/image00407.jpeg). The loss function for linear regression
    with the regularization term can be written as follows:![Model regularization](img/image00408.jpeg)Parameters
    ![Model regularization](img/image00409.jpeg) having a large magnitude will contribute
    more to the loss. Hence, minimization of the preceding loss function will typically
    produce parameters having small values and reduce the overfit. The optimum value
    of ![Model regularization](img/image00410.jpeg) is found from the validation set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lasso**: In Lasso also, one adds a penalty term similar to ridge regression,
    but the term is proportional to the sum of modulus of each parameter and not its
    square:![Model regularization](img/image00411.jpeg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though this looks like a simple change, Lasso has some very important differences
    with respect to ridge regression. First of all, the presence of the ![Model regularization](img/image00412.jpeg)
    term makes the loss function nonlinear in parameters ![Model regularization](img/image00386.jpeg).
    The corresponding minimization problem is called the quadratic programming problem
    compared to the linear programming problem in ridge regression, for which a closed
    form solution is available. Due to the particular form ![Model regularization](img/image00412.jpeg)
    of the penalty, when the coefficients shrink as a result of minimization, some
    of them eventually become zero. So, Lasso is also in some sense a subset selection
    problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A detailed discussion of various subset selection and model regularization approaches
    can be found in the book by Trevor Hastie et.al (reference 1 in the *References*
    section of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian averaging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned that simply minimizing the loss function (or equivalently
    maximizing the log likelihood function in the case of normal distribution) is
    not enough to develop a machine learning model for a given problem. One has to
    worry about models overfitting the training data, which will result in larger
    prediction errors on new datasets. The main advantage of Bayesian methods is that
    one can, in principle, get away from this problem, without using explicit regularization
    and different datasets for training and validation. This is called Bayesian model
    averaging and will be discussed here. This is one of the answers to our main question
    of the chapter, *why Bayesian inference for machine learning?*
  prefs: []
  type: TYPE_NORMAL
- en: For this, let's do a full Bayesian treatment of the linear regression problem.
    Since we only want to explain how Bayesian inference avoids the overfitting problem,
    we will skip all the mathematical derivations and state only the important results
    here. For more details, interested readers can refer to the book by Christopher
    M. Bishop (reference 2 in the *References* section of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear regression equation ![Bayesian averaging](img/image00413.jpeg),
    with ![Bayesian averaging](img/image00391.jpeg) having a normal distribution with
    zero mean and variance ![Bayesian averaging](img/image00414.jpeg) (equivalently,
    precision ![Bayesian averaging](img/image00415.jpeg)), can be cast in a probability
    distribution form with *Y* having a normal distribution with mean *f(X)* and precision
    ![Bayesian averaging](img/image00416.jpeg). Therefore, linear regression is equivalent
    to estimating the mean of the normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian averaging](img/image00417.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Since ![Bayesian averaging](img/image00418.jpeg), where the set of basis functions
    *B(X)* is known and we are assuming here that the noise parameter ![Bayesian averaging](img/image00416.jpeg)
    is also a known constant, only ![Bayesian averaging](img/image00386.jpeg) needs
    to be taken as an uncertain variable for a fully Bayesian treatment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in Bayesian inference is to compute a posterior distribution
    of parameter vector ![Bayesian averaging](img/image00386.jpeg). For this, we assume
    that the prior distribution of ![Bayesian averaging](img/image00386.jpeg) is an
    *M* dimensional normal distribution (since there are *M* components) with mean
    ![Bayesian averaging](img/image00419.jpeg) and covariance matrix ![Bayesian averaging](img/image00420.jpeg).
    As we have seen in [Chapter 3](part0030.xhtml#aid-SJGS2 "Chapter 3. Introducing
    Bayesian Inference"), *Introducing Bayesian Inference*, this corresponds to taking
    a conjugate distribution for the prior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian averaging](img/image00421.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The corresponding posterior distribution is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian averaging](img/image00422.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Bayesian averaging](img/image00423.jpeg) and ![Bayesian averaging](img/image00424.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *B* is an *N x M* matrix formed by stacking basis vectors *B*, at different
    values of *X*, on top of each other as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian averaging](img/image00425.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have the posterior distribution for ![Bayesian averaging](img/image00386.jpeg)
    as a closed-form analytical expression, we can use it to predict new values of
    *Y*. To get an analytical closed-form expression for the predictive distribution
    of *Y*, we make an assumption that ![Bayesian averaging](img/image00426.jpeg)
    and ![Bayesian averaging](img/image00427.jpeg). This corresponds to a prior with
    zero mean and isotropic covariance matrix characterized by one precision parameter
    ![Bayesian averaging](img/image00428.jpeg). The predictive distribution or the
    probability that the prediction for a new value of *X = x* is *y*, is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian averaging](img/image00429.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This equation is the central theme of this section. In the classical or frequentist
    approach, one estimates a particular value ![Bayesian averaging](img/image00430.jpeg)
    for the parameter ![Bayesian averaging](img/image00386.jpeg) from the training
    dataset and finds the probability of predicting *y* by simply using ![Bayesian
    averaging](img/image00431.jpeg). This does not address the overfitting of the
    model unless regularization is used. In Bayesian inference, we are integrating
    out the parameter variable ![Bayesian averaging](img/image00386.jpeg) by using
    its posterior probability distribution ![Bayesian averaging](img/image00432.jpeg)
    learned from the data. This averaging will remove the necessity of using regularization
    or keeping the parameters to an optimal level through bias-variance tradeoff.
    This can be seen from the closed-form expression for *P(y|x)*, after we substitute
    the expressions for ![Bayesian averaging](img/image00433.jpeg) and ![Bayesian
    averaging](img/image00434.jpeg) for the linear regression problem and do the integration.
    Since both are normal distributions, the integration can be done analytically
    that results in the following simple expression for *P(y|x)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian averaging](img/image00435.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Bayesian averaging](img/image00436.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: This equation implies that the variance of the predictive distribution consists
    of two terms. One term, 1/![Bayesian averaging](img/image00416.jpeg), coming from
    the inherent noise in the data and the second term coming from the uncertainty
    associated with the estimation of model parameter ![Bayesian averaging](img/image00386.jpeg)
    from data. One can show that as the size of training data *N* becomes very large,
    the second term decreases and in the limit ![Bayesian averaging](img/image00437.jpeg)
    it becomes zero.
  prefs: []
  type: TYPE_NORMAL
- en: The example shown here illustrates the power of Bayesian inference. Since one
    can take care of uncertainty in the parameter estimation through Bayesian averaging,
    one doesn't need to keep separate validation data and all the data can be used
    for training. So, a full Bayesian treatment of a problem avoids the overfitting
    issue. Another major advantage of Bayesian inference, which we will not go into
    in this section, is treating latent variables in a machine learning model. In
    the next section, we will give a high-level overview of the various common machine
    learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of common machine learning tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is a prequel to the following chapters, where we will discuss different
    machine learning techniques in detail. At a high level, there are only a handful
    of tasks that machine learning tries to address. However, for each of such tasks,
    there are several approaches and algorithms in place.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical tasks in any machine learning are one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensional reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In classification, the objective is to assign a new data point to one of the
    predetermined classes. Typically, this is either a supervised or semi-supervised
    learning problem. The well-known machine learning algorithms used for classification
    are logistic regression, **support vector machines** (**SVM**), decision trees,
    Naïve Bayes, neural networks, Adaboost, and random forests. Here, Naïve Bayes
    is a Bayesian inference-based method. Other algorithms, such as logistic regression
    and neural networks, have also been implemented in the Bayesian framework.
  prefs: []
  type: TYPE_NORMAL
- en: Regression is probably the most common machine learning problem. It is used
    to determine the relation between a set of input variables (typically, continuous
    variables) and an output (dependent) variable that is continuous. We discussed
    the simplest example of linear regression in some detail in the previous section.
    More complex examples of regression are generalized linear regression, spline
    regression, nonlinear regression using neural networks, support vector regression,
    and Bayesian network. Bayesian formulations of regression include the Bayesian
    network and Bayesian linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is a classic example of unsupervised learning. Here, the objective
    is to group together similar items in a dataset based on certain features of the
    data. The number of clusters is not known in advance. Hence, clustering is more
    of a pattern detection problem. The well-known clustering algorithms are K-means
    clustering, hierarchical clustering, and **Latent Dirichlet allocation** (**LDA**).
    In this, LDA is formulated as a Bayesian inference problem. Other clustering methods
    using Bayesian inference include the Bayesian mixture model.
  prefs: []
  type: TYPE_NORMAL
- en: Association rule mining is an unsupervised method that finds items that are
    co-occurring in large transactions of data. The market basket analysis, which
    finds the items that are sold together in a supermarket, is based on association
    rule mining. The Apriori algorithm and frequent pattern matching algorithm are
    two main methods used for association rule mining.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting is similar to regression, except that the data is a time series
    where there are observations with different values of time stamp and the objective
    is to predict future values based on the current and past values. For this purpose,
    one can use methods such as ARIMA, neural networks, and dynamic Bayesian networks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the fundamental issues in machine learning is called *the* *curse of
    dimensionality*. Since there can be a large number of features in a machine learning
    model, the typical minimization of error that one has to do to estimate model
    parameters will involve search and optimization in a large dimensional space.
    Most often, data will be very sparse in this higher dimensional space. This can
    make the search for optimal parameters very inefficient. To avoid this problem,
    one tries to project this higher dimensional space into a lower dimensional space
    containing a few important variables. One can then use these lower dimensional
    variables as features. The two well-known examples of dimensional reduction are
    principal component analysis and self-organized maps.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the probability distribution of a population is directly estimated, without
    any parametric models, from a small amount of observed data for making inferences.
    This is called **density estimation**. The simplest form of density estimation
    is histograms, though it is not adequate for many practical applications. The
    more sophisticated density estimations are **kernel density estimation** (**KDE**)
    and vector quantization.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Friedman J., Hastie T., and Tibshirani R. *The Elements of Statistical Learning
    – Data Mining, Inference, and Prediction*. Springer Series in Statistics. 2009
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bishop C.M. *Pattern Recognition and Machine Learning (Information Science
    and Statistics)*. Springer. 2006\. ISBN-10: 0387310738'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we got an overview of what machine learning is and what some
    of its high-level tasks are. We also discussed the importance of Bayesian inference
    in machine learning, particularly in the context of how it can help to avoid important
    issues, such as model overfit and how to select optimum models. In the coming
    chapters, we will learn some of the Bayesian machine learning methods in detail.
  prefs: []
  type: TYPE_NORMAL
