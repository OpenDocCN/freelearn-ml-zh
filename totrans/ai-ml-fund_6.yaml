- en: '6'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Summarize the basics of clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform flat clustering with the k-means algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform hierarchical clustering with the mean shift algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the fundamentals of clustering, which
    will be illustrated with two unsupervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapters, we dealt with supervised learning algorithms to perform
    classification and regression. We used training data to train our classification
    or regression model, and then we validated our model using testing data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will perform unsupervised learning by using clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We may use clustering to analyze data to find certain patterns and create groups.
    Apart from that, clustering can be used for many purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Market segmentation detects the best stocks in the market you should be focusing
    on fundamentally. We can detect trends, segment customers, or recommend certain
    products to certain customer types using clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In computer vision, image segmentation is performed using clustering, where
    we find different objects in an image that a computer processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering can be combined with classification, where clustering may generate
    a compact representation of multiple features, which can then be fed to a classifier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering may also filter data points by detecting outliers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of whether we are applying clustering to genetics, videos, images,
    or social networks, if we analyze data using clustering, we may find similarities
    between data points that are worth treating uniformly.
  prefs: []
  type: TYPE_NORMAL
- en: We perform clustering without specified labels. Clustering defines clusters
    based on the distance between their data points. While; in classification, we
    define exact label classes to group classified data points, in clustering, there
    are no labels. We just give the machine learning model the features, and the model
    has to figure out the clusters in which those feature sets are grouped.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Clustering Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose you are a store manager who's responsible for ensuring the profitability
    of your store. Your products are divided into categories. Different customers
    of the store prefer different items.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a customer interested in bio products tends to select products
    that are bio in nature. If you check out Amazon, you will also find suggestions
    for different groups of products. This is based on what users are likely to be
    interested in.
  prefs: []
  type: TYPE_NORMAL
- en: We will define the clustering problem in such a way that we will be able to
    find these similarities between our data points. Suppose we have a dataset that
    consists of points. Clustering helps us understand this structure by describing
    how these points are distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of data points in a two-dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00055.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Data points in a two-dimensional space'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this example, it is evident that there are three clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00056.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Three clusters formed using the data points in a two-dimensional
    space'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The three clusters were easy to detect because the points are close to one another.
    Clustering determines data points that are close to each other. There are also
    some outlier points that do not belong to any cluster. The clustering algorithm
    should be prepared to treat these outlier points properly, without moving them
    into a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: While it is easy to recognize clusters in a two-dimensional space, we normally
    have multidimensional data points. Therefore, it is important to know which data
    points are close to one other. Also, it is important to define distance metrics
    that detect whether data points are close to each other. One well-known distance
    metric is the Euclidean distance. In mathematics, we often use Euclidean distance
    to measure the distance between two points. Therefore, Euclidean distance is an
    intuitive choice when it comes to clustering algorithms so that we can determine
    the proximity of data points when locating clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one drawback to most distance metrics, including Euclidean distance:
    the more we increase the dimensions, the more uniform these distances will become
    compared to each other. Therefore, getting rid of features that act as noise rather
    than useful information may greatly increase the accuracy of the clustering model.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are two types of clustering: **flat** and **hierarchical** .'
  prefs: []
  type: TYPE_NORMAL
- en: In flat clustering, we specify the number of clusters we would like the machine
    to find. One example of flat clustering is the k-means algorithm, where K specifies
    the number of clusters, we would like the algorithm to use.
  prefs: []
  type: TYPE_NORMAL
- en: In hierarchical clustering, the machine learning algorithm finds out the number
    of clusters that are needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hierarchical clustering also has two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bottom-up hierarchical clustering** treats each point as a cluster. This
    approach unites clusters that are close to each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-down hierarchical clustering** treats data points as if they were in
    one cluster spanning the whole state space. Then, the clustering algorithm splits
    our clusters into smaller ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Point assignment clustering** assigns new data points to existing clusters
    based on how close the new data point is to these clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering Algorithms Supported by scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about two clustering algorithms supported by
    scikit-learn: the **k-means** algorithm and the **mean shift** algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '**k-means** is an example of flat clustering, where we have to specify the
    number of clusters in advance. k-means is a generic purpose clustering algorithm
    that performs well if the number of clusters is not too high and the size of the
    clusters is even.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean-shift** is an example of hierarchical clustering, where the clustering
    algorithm determines the number of clusters. Mean shift is used when we don''t
    know the number of clusters in advance. In contrast with k-means, mean shift supports
    use cases where many clusters are present, even if the size of the clusters greatly
    differs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn provides other clustering algorithms. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Affinity Propagation** : Performs similarly to Mean Shift'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spectral clustering** : Performs better if only a few clusters are present,
    with even cluster sizes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ward hierarchical clustering** : Used when many clusters are expected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agglomerative clustering** : Used when many clusters are expected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DBSCAN clustering** : Supports uneven cluster sizes and non-flat geometry
    of point distributions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian mixtures** : Uses flat geometry, which is good for density estimations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Birch clustering** : Supports large datasets, removes outliers, and supports
    data reduction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a complete description of clustering algorithms, including performance comparisons,
    visit the clustering page of scikit-learn at [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)
    .
  prefs: []
  type: TYPE_NORMAL
- en: The k-means Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The k-means algorithm is a flat clustering algorithm. It works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the value of K.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose K data points from the dataset that are initial centers of the individual
    clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the distance of each data point to the chosen center points, and group
    each point in the cluster whose initial center is the closest to the data point.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once all of the points are in one of the K clusters, calculate the center point
    of each cluster. This center point does not have to be an existing data point
    in the dataset; it is just an average.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat this process of assigning each data point into the cluster that has a
    center closest to the data point. Repetition continues until the center points
    no longer move.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make sure that the k-means algorithm terminates, we need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A maximum level of tolerance when we exit in case the centroids move less than
    the tolerance value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A maximum number of repetitions of shifting the moving points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to the nature of the k-means algorithm, it will have a hard time dealing
    with clusters that greatly differ in size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means algorithm has many use cases that are part of our everyday lives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Market segmentation:** Companies gather all sorts of data from their customer
    base. Performing k-means clustering analysis on the customer base of a company
    reveals market segments that have defined characteristics. Customers belonging
    to the same segment can be treated similarly. Different segments receive different
    treatment.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification of books, movies, or other documents:** When influencers build
    their personal brand, authors write books and create books, or a company manages
    its social media accounts, content is king. Content is often described by hashtags
    and other data. This data can be used as a basis for clustering to locate groups
    of documents that are similar in nature.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Detection of fraud and criminal activities:** Fraudsters often leaves clues
    in the form of unusual customer or visitor behavior. For instance, car insurance
    protects drivers from theft and damage arising from accidents. Real theft and
    fake theft are characterized by different feature values. Similarly, wrecking
    a car on purpose leaves different traces than wrecking a car by accident. Clustering
    can often detect fraud, helping industry professionals understand the behavior
    of their worst customers better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 19: k-means in scikit-learn'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To plot data points in a two-dimensional plane and execute the k-means algorithm
    on them to perform clustering, execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create an artificial dataset as a NumPy Array to demonstrate the k-means
    algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can plot these data points in the two-dimensional plane using `matplotlib.pyplot`
    :'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/Image00057.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.3: Graph showing the data points on a two-dimensional plane using
    matplotlib.pyplot'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We used the `transpose array` method to get the values of the first feature
    and the second feature. This is in alignment with the previous chapters. We could
    also use proper array indexing to access these columns: `dataPoints[:,0]` is equivalent
    to `dataPoints.transpose()[0]` .'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have the data points, it''s time to execute the k-means algorithm
    on them. If we define K as `3` in the k-means algorithm, we expect a cluster on
    the bottom-left, top-left, and bottom-right corner of the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the clustering is done, we can access the center point of each cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Indeed, the center points of the clusters appear to be in the bottom-left, top-left,
    and bottom-right corners of the graph. The X-coordinate of the top-left cluster
    is 3.1, most likely because it contains our outlier data point [10, 10].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s plot the clusters with different colors and their center points. To
    know which data point belongs to which cluster, we have to query the `labels_`
    property of the k-means classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output array shows which data point belongs to which cluster. This is all
    we need to plot the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/Image00058.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.4: Graph showing the data points in red, green, and blue while selecting
    three clusters'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The blue center points are indeed inside their clusters, which are represented
    by the red points, the green points, and the yellow points.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s see what happens if we choose only two clusters instead of three:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/Image00059.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.5: Graph showing the datapoints in red, blue, and green while selecting
    two clusters'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: This time, we only have red and green points, and we have a bottom cluster and
    a top cluster. Interestingly, the top red cluster in the second example contains
    the same points as the top cluster in the first example. The bottom cluster of
    the second example consists of the data points joining the bottom-left and the
    bottom-right clusters of the first example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can also use the k-means model for prediction. The output is an array containing
    the cluster numbers belonging to each data point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Parameterization of the k-means Algorithm in scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like the classification and regression models in Chapters 3, 4, and 5, the k-means
    algorithm can also be parameterized. The complete list of parameters can be found
    at [http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_clusters` : The number of clusters in which the data points are separated.
    The default value is **8** .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_iter` : The maximum number of iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tol` : The tolerance for checking whether we can exit the k-means algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the previous section, we used two attributes to retrieve the cluster center
    points and the clusters themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cluster_centers_` : This returns the coordinates of the cluster center points.'
  prefs: []
  type: TYPE_NORMAL
- en: '`labels_` : This returns an array of integers symbolizing the number of clusters
    the data point belongs to. Numbering starts from zero.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 20: Retrieving the Center Points and the Labels'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand the usage of `cluster_centers_` and `labels_` , perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the example that we had from executing the k-means algorithm in scikit-learn.
    We had 12 data points and three clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the **labels_** property on the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the **cluster_centers_** property is obvious: it shows the X
    and Y coordinates of the center points. The **labels_** property is an array of
    length 12, showing the cluster of each of the 12 data points it belongs to. The
    first cluster is associated with the number 0, the second is associated with 1,
    the third is associated with 2, and so on.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: k-means Clustering of Sales Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the upcoming activity, we will be considering sales data and we will perform
    k-means clustering on that sales data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 12: k-means Clustering of Sales Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In th is section, we will detect product sales that perform similarly to recognize
    trends in product sales.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the Sales Transactions Weekly Dataset, found at the following
    URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly](https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly)
    Perform clustering on the dataset using the k-means algorithm. Make sure that
    you prepare your data for clustering based on what you have learned in the previous
    chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the default settings for the k-means algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset using pandas. If you examine the data in the CSV file, you
    will realize that the first column contains product ID strings. These values just
    add noise to the clustering process. Also, notice that for weeks 0 to 51, there
    is a W-prefixed label and a normalized label. Using the normalized label makes
    more sense so that we can drop the regular weekly labels from the dataset.Create
    a k-means clustering model and fit the data points into 8 clusters.Retrieve the
    center points and the labels from the clustering algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The labels belonging to each data point can be retrieved using the **labels_**
    property. These labels determine the clustering of the rows of the original data
    frame. How are these labels beneficial?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose that, in the original data frame, the product names are given. You can
    easily recognize the fact that similar types of products sell similarly. There
    are also products that fluctuate a lot, and products that are seasonal in nature.
    For instance, if some products promote fat loss and getting into shape, they tend
    to sell during the first half of the year, before the beach season.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution to this activity is available at page 291.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Shift Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mean shift is a hierarchical clustering algorithm. Unlike the k-means algorithm,
    in mean shift, the clustering algorithm determines how many clusters are needed,
    and also performs the clustering. This is advantageous because we rarely know
    how many clusters we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm also has many use cases in our everyday lives. For instance,
    the Xbox Kinect device detects human body parts using the mean shift algorithm.
    Some mobile phones also use the Mean Shift algorithm to detect faces. With the
    growth of social media platforms, image segmentation is a feature that many users
    have gotten used to. As image segmentation is also a basis of computer vision,
    some applications can be found there. The mean shift algorithm may also save lives,
    as it is built into the car detection software of many modern cars. Imagine that
    someone emergency brakes in front of you. The image segmentation software of your
    car detects that the car in front of you is getting alarmingly close to you and
    applies the emergency brake before you even realize the emergency situation. These
    driver aids are widespread in modern cars. Self-driving cars are just one step
    away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 21: Illustrating Mean Shift in 2D'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To learn clustering by using the mean shift algorithm, execute the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s recall the data points from the previous topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our task now is to find a point P (x, y), for which the number of data points
    within a radius R from point P is maximized. The points are distributed as follows:![](img/Image00060.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.6: Graph showing the data points from the data_points array'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Suppose we initially equate point P to the first data point, [1, 1]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s find the points that are within a distance of R from this point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s calculate the mean of the data points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the new mean has been calculated, we can retrieve the points within
    the given radius again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These are the same three points, so we can stop here. Three points have been
    found around the mean of `[1.3333333333333333, 1.5]` . The points around this
    center within a radius of 2 form a cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we examined the data points [1, 1.5] and [2, 2], we would get the same result.
    Let''s continue with the fourth point in our list, [8, 1]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This time, all four points in the area were found. Therefore, we can simply
    calculate their mean:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This mean will not change, as in the next iteration, we will find the same data
    points.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Notice that we got lucky with the selection of the point [8, 1]. If we started
    with `P = [8, 0]` or `P = [8.5, 1]` , we would only find three points instead
    of four:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After calculating the mean of these three points, we would have to rerun the
    distance calculation with the shifted mean:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for point P = [8.5, 1] is the following array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We only found the same three points again. This means that starting from [8,1],
    we got a larger cluster than starting from [8, 0] or [8.5, 1]. Therefore, we have
    to take the center point that contains the maximum number of data points.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let''s examine what would happen if we started the discovery from the
    fourth data point, **[6, 1]** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We successfully found the data point [8, 1]. Therefore, we have to shift the
    mean from [6, 1] to the calculated new mean, [7, 1]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s check if we found more points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Yes â€“ we successfully found all four points! Therefore, we have successfully
    defined a cluster of size 4\. The mean will be the same as before: `[7.625, 0.75]`
    .'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This was a simple clustering example that applied the mean shift algorithm.
    We only provided an illustration of what the algorithm considers to find the clusters.
    There is one remaining question, though: the value of the radius.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that if the radius of 2 was not set, we could simply start either with
    a huge radius including all data points and then reduce the radius or start with
    a very small radius, making sure that each data point is in its own cluster, and
    then increase the radius until we get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Mean Shift Algorithm in scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s use the same data points as in the k-means algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The syntax of the mean shift clustering algorithm is similar to the k-means
    clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Once clustering is done, we can access the center point of each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The Mean Shift model found 5 clusters with the centers shown in the preceding
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to k-means,, we can also get the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output array shows which data point belongs to which cluster. This is all
    we need to plot the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image00061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Graph based on k-means,'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The three blue points are the center points of the red, green, and yellow clusters.
    There are two more single dot clusters in the coordinate system, belonging to
    the points (6,1) and (10,10).
  prefs: []
  type: TYPE_NORMAL
- en: Image Processing in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To solve the upcoming activity, you need to know how to process images in Python.
    We will use the SciPy library for this.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways that you can read an image file from a path.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest one is the **Image** interface from the Python Imaging Library
    (PIL):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code assumes that the file path specified in the string argument
    of the **open** method points to a valid image file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the size of the image by querying the size property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a two-dimensional NumPy array from the image containing the RGB
    values of each pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the pixel array has been constructed, we can easily retrieve and manipulate
    each pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The pixels of the image can also be made accessible using the `load()` method
    of the image. Once we get access to these pixels, we can get the RGB or RGBA values
    of each pixel, depending on the file format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the order of pixel coordinates is the opposite, that is, `pixel_array[411][740]`
    when reading from left to right. We are reading the exact same pixel, but we have
    to supply the coordinates differently.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set pixels to a new value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to save changes, use the `save()` method of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform clustering analysis on the pixels of the image, we need to convert
    the image to a data frame. This implies that we have to convert the pixels of
    the image to a tuple or array of `[''x'', ''y'', ''red'', ''green'', ''blue'']`
    values. Once we have a one-dimensional array of these values, we can convert them
    to a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This is all you need to know to complete the activity on processing images using
    the Mean Shift algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 13: Shape Recognition with the Mean Shift Algorithm'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will learn how images can be clustered. Imagine you are
    working for a company that detects human emotions from photos. Your task is to
    extract pixels making up a face in an avatar photo.
  prefs: []
  type: TYPE_NORMAL
- en: Create a clustering algorithm with Mean Shift to cluster pixels of images. Examine
    the results of the Mean Shift algorithm and check whether any of the clusters
    contain a face when used on avatar images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, apply the k-means, algorithm with a fixed default number of clusters
    (8, in this case). Compare your results with the Mean Shift clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Select an image you would like to cluster and load the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the pixels into a data frame to perform clustering. Perform Mean Shift
    clustering on the image using scikit-learn. Note that, this time, we will skip
    normalizing the features, because the proximity of the pixels and the proximity
    of the color components are represented in a close to equal weight. The algorithm
    will find two clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on the image you use, notice how the Mean Shift algorithm treats human
    skin color, and what other parts of the image are placed in the same cluster.
    The cluster containing most of the skin in the avatar often includes data points
    that are very near and/or have a similar color as the color of the skin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's use the k-means algorithm to formulate eight clusters on the same data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will see that the clustering algorithm indeed located data points that are
    close and contain similar colors.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution to this activity is available at page 293.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we learned how clustering works. Clustering is a form of unsupervised
    learning, where the features are given, and the clustering algorithm finds the
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of clustering: flat and hierarchical.'
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm is a flat clustering algorithm, where we determine K center
    points for our K clusters, and the algorithm finds the data points.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Shift is an example of a hierarchical clustering algorithm, where the number
    of distinct label values is to be determined by the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The final chapter will introduce a field that has become popular this decade
    due to the explosion of computation power and cheap, scalable online server capacity.
    This field is the science of neural networks and deep learning.
  prefs: []
  type: TYPE_NORMAL
