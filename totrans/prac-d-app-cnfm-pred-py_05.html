<html><head></head><body>
<div id="_idContainer021">
<h1 class="chapter-number" id="_idParaDest-48"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-49"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.2.1">Types of Conformal Predictors</span></h1>
<p><span class="koboSpan" id="kobo.3.1">This chapter describes different families of conformal predictors, exploring various approaches to quantifying uncertainty. </span><span class="koboSpan" id="kobo.3.2">Through practical examples, we provide an intermediate-level understanding of these techniques and how they can be applied to </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">real-world situations.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Here are examples of how companies are using </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">conformal prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">At a high-profile AI developer conference called GTC 2023 (</span><a href="https://www.nvidia.com/gtc/"><span class="koboSpan" id="kobo.8.1">https://www.nvidia.com/gtc/</span></a><span class="koboSpan" id="kobo.9.1">), Bill Dally, NVIDIA’s chief scientist and SVP of research, offered insights into one of NVIDIA’s R&amp;D primary focuses, which is in conformal </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">prediction (</span></span><a href="https://www.hpcwire.com/2023/03/28/whats-stirring-in-nvidias-rd-lab-chief-scientist-bill-dally-provides-a-peek/"><span class="No-Break"><span class="koboSpan" id="kobo.11.1">https://www.hpcwire.com/2023/03/28/whats-stirring-in-nvidias-rd-lab-chief-scientist-bill-dally-provides-a-peek/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.12.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">Traditional machine learning models for autonomous vehicles output a single classification (e.g., pedestrian or no pedestrian on the road) and position estimate for detected objects. </span><span class="koboSpan" id="kobo.13.2">However, NVIDIA wants to produce a set of potential outputs with probabilities; for example, an object could be a pedestrian (80% probability) or cyclist (20% probability) at a position of 20 +/- </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">1 meters.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">This allows the vehicle’s planner to guarantee safe actions accounting for multiple possible outcomes. </span><span class="koboSpan" id="kobo.15.2">Rather than just the most likely label and position, conformal prediction provides a range of plausible options, such as “pedestrian at 19–21 meters” with </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">80% confidence.</span></span></p>
<p><span class="koboSpan" id="kobo.17.1">NVIDIA uses a nonconformity function to calculate probabilities that measure how strange or different each potential label and position is compared to the training data. </span><span class="koboSpan" id="kobo.17.2">This generates a multi-modal predictive distribution </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">reflecting uncertainty.</span></span></p>
<p><span class="koboSpan" id="kobo.19.1">Conformal prediction gives NVIDIA’s vehicles a reliable way to quantify uncertainty and consider multiple interpretations of the environment. </span><span class="koboSpan" id="kobo.19.2">By planning for the entire set of plausible outcomes rather than just the single most likely one, conformal prediction improves robustness </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">and safety.</span></span></p>
<p><span class="koboSpan" id="kobo.21.1">In the realm of machine learning, quantifying uncertainty and providing reliable predictions is of significant importance. </span><span class="koboSpan" id="kobo.21.2">Conformal prediction is an innovative technique that allows us to construct prediction sets (in classification) and prediction intervals (in regression), offering a measure of confidence in </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">our predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.23.1">This chapter aims to provide a deeper understanding of the different types of conformal predictors and their respective approaches to quantifying uncertainty. </span><span class="koboSpan" id="kobo.23.2">Through practical examples, we will illustrate how these techniques can be applied to various machine </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">learning tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.25.1">In this chapter, we will explore the </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.27.1">Foundations of classical and inductive </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">conformal predictors</span></span></li>
<li><span class="koboSpan" id="kobo.29.1">Examining algorithmic descriptions of </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">conformal predictors</span></span></li>
<li><span class="koboSpan" id="kobo.31.1">Mathematical formulations and </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">practical examples</span></span></li>
<li><span class="koboSpan" id="kobo.33.1">Advantages and limitations of </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">conformal predictors</span></span></li>
<li><span class="koboSpan" id="kobo.35.1">Guidelines for choosing the most suitable conformal predictor for specific </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">problem domains</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.37.1">Let’s start with the classical </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">conformal predictors.</span></span></p>
<h1 id="_idParaDest-50"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.39.1">Understanding classical predictors</span></h1>
<p><span class="koboSpan" id="kobo.40.1">Before we deep dive into the intricacies of conformal predictors, let’s briefly recap the key concepts from the previous chapters. </span><span class="koboSpan" id="kobo.40.2">Conformal prediction is a framework that enables creating </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.41.1">confidence regions for our predictions while controlling the </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">error rate.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">This approach is especially beneficial in situations where a measure of uncertainty is essential, such as in medical diagnosis, self-driving cars, or financial risk management. </span><span class="koboSpan" id="kobo.43.2">The framework encompasses two main types of conformal predictors: </span><strong class="bold"><span class="koboSpan" id="kobo.44.1">classical</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.45.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.46.1">inductive</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.48.1">Classical transductive conformal prediction</span></strong><span class="koboSpan" id="kobo.49.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.50.1">TCP</span></strong><span class="koboSpan" id="kobo.51.1">) is the original form of conformal prediction developed by the inventors of</span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.52.1"> Conformal prediction. </span><span class="koboSpan" id="kobo.52.2">It forms the basis for understanding the general principles of conformal predictors. </span><span class="koboSpan" id="kobo.52.3">Classical Conformal prediction was developed to construct prediction regions that conform to a specified confidence level. </span><span class="koboSpan" id="kobo.52.4">The critical aspect of classical Conformal prediction is its distribution-free nature, meaning it makes no assumptions about the data distribution. </span><span class="koboSpan" id="kobo.52.5">Thus, it can be applied to any machine learning algorithm, making </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">it algorithm-agnostic.</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">In contrast to the widely used inductive conformal prediction, classical TCP does not require a separate </span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.55.1">calibration set, enabling a more efficient utilization of the </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">entire dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.57.1">As a result, for smaller datasets, it can generate more accurate predictions. </span><span class="koboSpan" id="kobo.57.2">This approach allows statistical, machine learning, and deep learning models to fully capitalize on all available data, potentially leading to more efficient (narrower) prediction sets </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">and intervals.</span></span></p>
<p><span class="koboSpan" id="kobo.59.1">Let’s discuss how classical TCP can be used in </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">classification problems.</span></span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.61.1">Applying TCP for classification problems</span></h2>
<p><span class="koboSpan" id="kobo.62.1">In classification tasks, not only do we seek to assign labels, but we aim to do so confidently and accurately. </span><span class="koboSpan" id="kobo.62.2">This is </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.63.1">where classical TCP shines. </span><span class="koboSpan" id="kobo.63.2">As we delve into its use in classification, we will cover its</span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.64.1"> unique approach and advantages over traditional techniques. </span><span class="koboSpan" id="kobo.64.2">Ready to explore? </span><span class="koboSpan" id="kobo.64.3">Let’s dive into the nuances of TCP </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">for classification!</span></span></p>
<p><span class="koboSpan" id="kobo.66.1">In the previous chapters, we discussed the core concept of the conformal prediction framework, which involves assigning a nonconformity measure (or strangeness) to each object in the dataset. </span><span class="koboSpan" id="kobo.66.2">This measure is utilized to rank the objects within the dataset. </span><span class="koboSpan" id="kobo.66.3">Subsequently, when predicting a new object, a prediction region is created that encompasses values linked to a specific proportion of the dataset objects, determined by their strangeness scores. </span><span class="koboSpan" id="kobo.66.4">This proportion corresponds to the desired confidence level for </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">the predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.68.1">In contrast to the more popular inductive conformal prediction method, which relies on a calibration set to rank objects based on their nonconformity scores, classical TCP employs the entire dataset in conjunction with the features of the new object to establish the </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">prediction region.</span></span></p>
<p><span class="koboSpan" id="kobo.70.1">While this approach can be computationally expensive, it allows you to fully leverage the whole dataset to capture changes in the data distribution, providing more accurate </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">prediction regions.</span></span></p>
<p><span class="koboSpan" id="kobo.72.1">However, classical conformal prediction has some limitations, too. </span><span class="koboSpan" id="kobo.72.2">For instance, it may not be feasible for large datasets or real-time applications because it requires retraining the underlying point prediction model for each </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">new prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.74.1">Classical conformal prediction</span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.75.1"> is a process that involves several key steps. </span><span class="koboSpan" id="kobo.75.2">Here, we outline these steps to provide a clear</span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.76.1"> understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">the procedure:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.78.1">Dataset preparation</span></strong><span class="koboSpan" id="kobo.79.1">: Divide the dataset into training and test sets. </span><span class="koboSpan" id="kobo.79.2">The training set is used to train the machine learning model, while the test set is used to evaluate the performance of the </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">conformal predictor.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.81.1">Model training</span></strong><span class="koboSpan" id="kobo.82.1">: Train the underlying machine learning model using the training dataset. </span><span class="koboSpan" id="kobo.82.2">This point prediction model will generate point predictions for </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">new objects.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.84.1">Nonconformity measure calculation</span></strong><span class="koboSpan" id="kobo.85.1">: Define a nonconformity (strangeness) measure that quantifies how different an object is from the other objects in the dataset. </span><span class="koboSpan" id="kobo.85.2">For each object in the training dataset, calculate its nonconformity score using the </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">trained model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.87.1">New object nonconformity score</span></strong><span class="koboSpan" id="kobo.88.1">: When a new object (without its label) is introduced, calculate its nonconformity score using the same nonconformity measure and the trained point </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">prediction model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.90.1">Ranking</span></strong><span class="koboSpan" id="kobo.91.1">: Based on calculated nonconformity scores, rank all objects, including the objects in the training dataset and the </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">new object.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.93.1">Prediction region</span></strong><span class="koboSpan" id="kobo.94.1">: Determine the desired confidence level for the prediction. </span><span class="koboSpan" id="kobo.94.2">Identify the proportion of objects in the ranked set corresponding to this confidence level. </span><span class="koboSpan" id="kobo.94.3">Form a prediction set that includes the values associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">these objects.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.96.1">Let’s clarify these concepts using a practical example with the hinge loss nonconformity measure, which we discussed in the previous chapters. </span><span class="koboSpan" id="kobo.96.2">As a reminder, hinge loss (also known as inverse probability or LAC loss) is a nonconformity measure calculated as </span><em class="italic"><span class="koboSpan" id="kobo.97.1">1-P(y|x)</span></em><span class="koboSpan" id="kobo.98.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.99.1">P(y|x)</span></em><span class="koboSpan" id="kobo.100.1"> represents the class score produced by the underlying model for the </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">actual class.</span></span></p>
<p><span class="koboSpan" id="kobo.102.1">The hinge loss nonconformity measure intuitively measures the difference between the probability score generated by an ideal classifier for the correct class (which should ideally be 1) and the</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.103.1"> classification score produced by the classifier model. </span><span class="koboSpan" id="kobo.103.2">It quantifies how far the model’s prediction is from the perfect classification, with larger nonconformity scores</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.104.1"> indicating a more significant discrepancy between the ideal and </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">actual predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.106.1">To compute the inverse probability (hinge) nonconformity score, consider an example where your classifier generates two scores: </span><em class="italic"><span class="koboSpan" id="kobo.107.1">class_0 = 0.6</span></em><span class="koboSpan" id="kobo.108.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.109.1">class_1 = 0.4,</span></em><span class="koboSpan" id="kobo.110.1"> with the actual label </span><em class="italic"><span class="koboSpan" id="kobo.111.1">y=1</span></em><span class="koboSpan" id="kobo.112.1">. </span><span class="koboSpan" id="kobo.112.2">To determine the nonconformity score, subtract the probability of the true class (in this case, 0.4 </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">from 1).</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">The resulting inverse probability (hinge) nonconformity score </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">is 0.6.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">The hinge (inverse probability) score is lower when the underlying machine learning classification model performs better. </span><span class="koboSpan" id="kobo.116.2">This performance is influenced by a range of factors, such as the size and complexity of the dataset, the type of machine learning model employed, and the quality of the model’s construction. </span><span class="koboSpan" id="kobo.116.3">In other words, a well-built model using an appropriate machine learning technique for the given dataset will generally yield lower hinge scores, indicating </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">better predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.118.1">The training process is the critical difference between TCP and </span><strong class="bold"><span class="koboSpan" id="kobo.119.1">inductive conformal prediction</span></strong><span class="koboSpan" id="kobo.120.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.121.1">ICP</span></strong><span class="koboSpan" id="kobo.122.1">). </span><span class="koboSpan" id="kobo.122.2">In ICP, the underlying classifier is trained only once on the training set and the calibration of the </span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.123.1">conformal prediction model happens on the </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">calibration dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.125.1">In contrast, with TCP, the classifier is trained by appending each test point to the training set twice, each time assigning potential labels 0 and 1. </span><span class="koboSpan" id="kobo.125.2">This procedure is repeated for every point in the test set. </span><span class="koboSpan" id="kobo.125.3">As a result, the underlying classifier model is trained </span><em class="italic"><span class="koboSpan" id="kobo.126.1">2 x m</span></em><span class="koboSpan" id="kobo.127.1"> times, where m is the number of points in your test set. </span><span class="koboSpan" id="kobo.127.2">This may become computationally expensive for large datasets, and for such datasets, using ICP might be a more </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">suitable choice.</span></span></p>
<p><span class="koboSpan" id="kobo.129.1">However, the computational cost is typically manageable for medium and small datasets. </span><span class="koboSpan" id="kobo.129.2">To obtain potentially better point predictions and narrower probability intervals, you might consider TCP, which achieves better prediction intervals by training the classifier model </span><em class="italic"><span class="koboSpan" id="kobo.130.1">2 x m</span></em><span class="koboSpan" id="kobo.131.1"> times. </span><span class="koboSpan" id="kobo.131.2">Many algorithms, such as logistic regression, are fast and well suited for </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">this approach.</span></span></p>
<p><span class="koboSpan" id="kobo.133.1">The overall methodology for training TCP remains fundamentally unchanged. </span><span class="koboSpan" id="kobo.133.2">The TCP algorithm process</span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.134.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.136.1">Train the </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.137.1">underlying classifier on the entire </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">training set.</span></span></li>
<li><span class="koboSpan" id="kobo.139.1">Append each test point to the training set with each possible class label one class label at </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">a time.</span></span></li>
<li><span class="koboSpan" id="kobo.141.1">For each appended test point with a postulated label, retrain the classifier and compute the nonconformity score for the test point given the </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">postulated label.</span></span></li>
<li><span class="koboSpan" id="kobo.143.1">Calculate the p-values for each postulated label, comparing the test point’s nonconformity score to the scores of the points in the </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">training set.</span></span></li>
<li><span class="koboSpan" id="kobo.145.1">For each test point and each postulated label, include the postulated label in the prediction set if its p-value is greater than or equal to the chosen </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">significance level.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.147.1">We will illustrate the TCP approach with a practical classification task example using the German credit dataset (</span><a href="https://www.openml.org/d/31"><span class="koboSpan" id="kobo.148.1">https://www.openml.org/d/31</span></a><span class="koboSpan" id="kobo.149.1">), a classical dataset describing good and bad credit risk based on features such as loan duration, credit history, employment, property, age, housing, and </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.151.1">In the GitHub repo for the book, you will find a notebook  (</span><a href="https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_05_TCP.ipynb"><span class="koboSpan" id="kobo.152.1">https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_05_TCP.ipynb</span></a><span class="koboSpan" id="kobo.153.1">) describing how TCP works that you can work through to understand the key concepts of TCP </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">in practice.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<span class="koboSpan" id="kobo.155.1"><img alt="Figure 5.1 – German credit dataset" src="image/B19925_05_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.156.1">Figure 5.1 – German credit dataset</span></p>
<p><span class="koboSpan" id="kobo.157.1">For clarity, let’s examine the first test point with the original index </span><strong class="source-inline"><span class="koboSpan" id="kobo.158.1">30</span></strong><span class="koboSpan" id="kobo.159.1">, which has now been appended to the end of the training set. </span><span class="koboSpan" id="kobo.159.2">We will use this extended training set we created to train the</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.160.1"> classical transductive conformal predictor. </span><span class="koboSpan" id="kobo.160.2">This new dataset created using the code</span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.161.1"> in the notebook incorporates all points from the original training set and the single </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">test point.</span></span></p>
<p><span class="koboSpan" id="kobo.163.1">We now have a feature set to train two classification models: one with an assumed label of the test point of 0 and another with an assumed label of 1. </span><span class="koboSpan" id="kobo.163.2">We train two models using any classifier (in this case, </span><em class="italic"><span class="koboSpan" id="kobo.164.1">Logistic Regression</span></em><span class="koboSpan" id="kobo.165.1"> from scikit-learn) and calculate nonconformity scores using the </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">described procedure.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<span class="koboSpan" id="kobo.167.1"><img alt="Figure 5.2 – Distribution of nonconformity scores" src="image/B19925_05_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.168.1">Figure 5.2 – Distribution of nonconformity scores</span></p>
<p><span class="koboSpan" id="kobo.169.1">From the distribution of nonconformity scores, we observe that the nonconformity score for </span><em class="italic"><span class="koboSpan" id="kobo.170.1">label 0</span></em><span class="koboSpan" id="kobo.171.1"> (represented by the green vertical line) is relatively typical (more conforming) to the</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.172.1"> training set. </span><span class="koboSpan" id="kobo.172.2">In contrast, the nonconformity score for the potential </span><em class="italic"><span class="koboSpan" id="kobo.173.1">label 1</span></em><span class="koboSpan" id="kobo.174.1"> (represented</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.175.1"> by the red vertical line) is in a low-density </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">probability area.</span></span></p>
<p><span class="koboSpan" id="kobo.177.1">This suggests that the test object is likelier to be assigned a label of 0, while label 1 is less probable. </span><span class="koboSpan" id="kobo.177.2">However, conformal prediction is a robust mathematical machine learning framework, so we must quantify and statistically test this decision. </span><span class="koboSpan" id="kobo.177.3">This is where p-values come </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">into play.</span></span></p>
<p><span class="koboSpan" id="kobo.179.1">Let’s take a moment to revisit the conventional process of calculating p-values, which we previously explored in </span><a href="B19925_03.xhtml#_idTextAnchor033"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.180.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.181.1">, using the formula from Vovk’s book </span><em class="italic"><span class="koboSpan" id="kobo.182.1">Algorithmic Learning in a </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.183.1">Random World</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.185.1">p-values can be computed </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.187.1">p</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.188.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.189.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.190.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.191.1">z</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.192.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.193.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.194.1">:</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.195.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.196.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.197.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.198.1">≥</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.199.1">:</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.200.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.201.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.202.1">T</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.203.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.204.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.205.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.206.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.207.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.208.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.209.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.210.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.211.1">1</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.212.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.213.1">Here, the nonconformity score of a new test point is compared with the nonconformity scores of points in the training set. </span><span class="koboSpan" id="kobo.213.2">Essentially, the nonconformity score quantifies the </span><em class="italic"><span class="koboSpan" id="kobo.214.1">strangeness</span></em><span class="koboSpan" id="kobo.215.1"> or novelty of the new test object compared to the previously encountered objects in the </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">training dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">According to the formula, what we need to do is to check (for each test point and each potential value of label 0 and 1) how many objects in the set of training data appended with the test </span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.218.1">point using the postulated label have nonconformity values that are larger or equal to the nonconformity score of the </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">test point.</span></span></p>
<p><span class="koboSpan" id="kobo.220.1">We then divide it by the number of training points </span><em class="italic"><span class="koboSpan" id="kobo.221.1">(n+1)</span></em><span class="koboSpan" id="kobo.222.1">  (+1 accounts for the test point that we appended to the training set). </span><span class="koboSpan" id="kobo.222.2">As a result, we obtain two p-values for each test point: one for class 0 </span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.223.1">and one for </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">class 1.</span></span></p>
<p><span class="koboSpan" id="kobo.225.1">The central concept of conformal prediction revolves around utilizing nonconformity values for each test point to evaluate how well it aligns with the training set. </span><span class="koboSpan" id="kobo.225.2">By computing p-values based on this evaluation, we can conduct robust statistical tests to determine if each potential label value should be included in the </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">prediction set.</span></span></p>
<p><span class="koboSpan" id="kobo.227.1">Let’s say we have a postulated label (either 0 or 1). </span><span class="koboSpan" id="kobo.227.2">If there are sufficient instances in the training set with nonconformity values equal to or greater than the test point’s nonconformity value, then we infer that this postulated label aligns well with the observed data. </span><span class="koboSpan" id="kobo.227.3">As a result, we incorporate this label into our prediction set. </span><span class="koboSpan" id="kobo.227.4">Conversely, if the postulated label does not correspond well with the observed data, we refrain from including it in the </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">prediction set.</span></span></p>
<p><span class="koboSpan" id="kobo.229.1">In essence, this procedure echoes the principles of statistical hypothesis testing. </span><span class="koboSpan" id="kobo.229.2">For each hypothesized label value, we establish a null hypothesis, asserting that the label could be part of the prediction set if its associated p-value exceeds a pre-defined significance level. </span><span class="koboSpan" id="kobo.229.3">If the p-value falls short of this threshold, we discard the null hypothesis. </span><span class="koboSpan" id="kobo.229.4">This implies that the proposed label doesn’t adequately match the pattern found in the training data, leading us to exclude it from our </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">prediction set.</span></span></p>
<p><span class="koboSpan" id="kobo.231.1">For example, let’s say we have calculated two p-values for the first </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">test object:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.233.1">Assume for label 0 that the p-value is 0.55. </span><span class="koboSpan" id="kobo.233.2">Since the p-value is larger than the significance level (0.05), we include the hypothesized label (0 in this case) in the prediction set for this </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">test point.</span></span></li>
<li><span class="koboSpan" id="kobo.235.1">Now assume for label 1 that the p-value is 0.002. </span><span class="koboSpan" id="kobo.235.2">Since the p-value is smaller than the significance level (0.05), we cannot include the hypothesized label (1 in this case) in the prediction set for this </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">test point.</span></span></li>
<li><span class="koboSpan" id="kobo.237.1">Thus, the final prediction set for this point </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">is 0.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.239.1">In the context of TCP, the</span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.240.1"> key distinction between binary and multiclass classification lies in the number of potential labels taken into account for each test point. </span><span class="koboSpan" id="kobo.240.2">In a binary</span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.241.1"> classification scenario, only two labels exist (0 and 1). </span><span class="koboSpan" id="kobo.241.2">In contrast, multiclass classification involves a greater number of classes (for instance, C1, C2, </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">C3, ...).</span></span></p>
<p><span class="koboSpan" id="kobo.243.1">The main difference from the binary classification is that we will have to repeat the process for each possible class label, increasing computational complexity as you need to retrain the classifier for each test point and each potential label. </span><span class="koboSpan" id="kobo.243.2">However, the overall method for obtaining the prediction set remains </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">the same.</span></span></p>
<p><span class="koboSpan" id="kobo.245.1">After delving into the nuances of TCP for classification, let’s pivot our focus. </span><span class="koboSpan" id="kobo.245.2">Next up, we’ll explore the intricacies of employing TCP in regression contexts. </span><span class="koboSpan" id="kobo.245.3">This approach offers unique challenges and benefits, so let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">dive in!</span></span></p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.247.1">Applying TCP for regression problems</span></h2>
<p><span class="koboSpan" id="kobo.248.1">TCP can also be applied to </span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.249.1">regression problems. </span><span class="koboSpan" id="kobo.249.2">The process for TCP in regression is similar to the one used for classification, with some differences in computing nonconformity scores and prediction intervals. </span><span class="koboSpan" id="kobo.249.3">Here is an algorithmic description of the TCP for </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">regression problems:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.251.1">Train the underlying regression model on the entire </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">training set.</span></span></li>
<li><span class="koboSpan" id="kobo.253.1">For each test point, create a grid of potential target values. </span><span class="koboSpan" id="kobo.253.2">The granularity of this grid depends on the desired precision and the </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">problem’s nature.</span></span></li>
<li><span class="koboSpan" id="kobo.255.1">For each test point and each potential target value on the grid, append the test point to the training set with the associated </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">target value.</span></span></li>
<li><span class="koboSpan" id="kobo.257.1">For each appended test point with a postulated target value, retrain the regression model and compute the nonconformity score for the given postulated target value. </span><span class="koboSpan" id="kobo.257.2">The nonconformity score can be computed as the absolute difference between the predicted value and the true value of the appended point, or you can </span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.258.1">calculate it by using other error metrics such as </span><strong class="bold"><span class="koboSpan" id="kobo.259.1">mean squared </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.260.1">error</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.261.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.262.1">MSE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">).</span></span></li>
<li><span class="koboSpan" id="kobo.264.1">Calculate the p-values </span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.265.1">for each postulated target value by comparing the test point’s nonconformity </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.266.1">score for each value on the grid of potential target values to the scores of the points in the </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">training set.</span></span></li>
<li><span class="koboSpan" id="kobo.268.1">For each test point, include the postulated target value in the prediction interval if its p-value is greater than or equal to the chosen </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">significance level.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.270.1">The prediction set for a regression problem will be an interval rather than a set of discrete labels, as in classification. </span><span class="koboSpan" id="kobo.270.2">The main difference from the classification is that you will have to repeat the process for each potential target value in the grid, which could increase computational complexity. </span><span class="koboSpan" id="kobo.270.3">However, the overall method for obtaining the prediction interval remains </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">the same.</span></span></p>
<p><span class="koboSpan" id="kobo.272.1">We conclude the section about TCP by summarizing the advantages and limitations </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">of TCP.</span></span></p>
<h2 id="_idParaDest-53"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.274.1">Advantages</span></h2>
<p><span class="koboSpan" id="kobo.275.1">TCP has several </span><a id="_idIndexMarker161"/><span class="koboSpan" id="kobo.276.1">advantages over alternative methods of </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">uncertainty quantification:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.278.1">Distribution-free</span></strong><span class="koboSpan" id="kobo.279.1">: Transductive conformal predictors do not make any assumptions about the distribution of the data, making them suitable for various types </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">of data</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.281.1">Validity</span></strong><span class="koboSpan" id="kobo.282.1">: They provide prediction intervals with a guaranteed coverage probability, allowing for a reliable measure </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">of uncertainty</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.284.1">Adaptability</span></strong><span class="koboSpan" id="kobo.285.1">: Conformal predictors can be applied to various machine learning models, making them versatile and easily adaptable to </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">different settings</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.287.1">Better prediction intervals</span></strong><span class="koboSpan" id="kobo.288.1">: Transductive conformal predictors generally produce more precise prediction intervals compared to inductive conformal predictors since</span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.289.1"> they fully utilize the dataset for training the underlying point </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">prediction model</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.291.1">But there are a few limitations </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">as well:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.293.1">Computational expense</span></strong><span class="koboSpan" id="kobo.294.1">: TCP requires retraining the model for each test point and for each</span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.295.1"> potential class label (in classification) or each potential target value on the grid regression, making it computationally expensive, particularly for </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">large datasets</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.297.1">Not ideal for online learning</span></strong><span class="koboSpan" id="kobo.298.1">: Due to the computational expense, transductive conformal predictors are not well suited for online learning scenarios where models must be </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">continuously updated</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.300.1">Complexity</span></strong><span class="koboSpan" id="kobo.301.1">: Implementing transductive conformal predictors can be more complicated than traditional machine learning models, potentially posing a barrier to </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">widespread adoption</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.303.1">Transductive conformal predictors offer several advantages in providing reliable, distribution-free prediction intervals. </span><span class="koboSpan" id="kobo.303.2">However, their computational expense and scalability limitations should be considered, particularly for large-scale or online </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">learning applications.</span></span></p>
<p><span class="koboSpan" id="kobo.305.1">Building upon our exploration of TCP, it’s time to turn our attention to another intriguing variant: inductive conformal predictors. </span><span class="koboSpan" id="kobo.305.2">Differing from its classical counterpart in key ways, this approach brings a new set of strategies and benefits to the table. </span><span class="koboSpan" id="kobo.305.3">Ready to delve into the mechanics and merits of inductive conformal predictors? </span><span class="koboSpan" id="kobo.305.4">Let’s embark on this </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">enlightening journey!</span></span></p>
<h1 id="_idParaDest-54"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.307.1">Understanding inductive conformal predictors</span></h1>
<p><span class="koboSpan" id="kobo.308.1">ICP is a variant of conformal prediction that provides valid predictive regions under the same assumptions as classical</span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.309.1"> conformal prediction and has the added benefit of improved computational efficiency, which is particularly useful when dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">large datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.311.1">ICPs present a highly efficient and effective solution within the realm of machine learning. </span><span class="koboSpan" id="kobo.311.2">They provide a form of conformal prediction that caters to larger datasets, making it highly suitable for real-world applications that involve extensive data volumes. </span><span class="koboSpan" id="kobo.311.3">ICPs divide the dataset into training and calibration sets during the model-building process. </span><span class="koboSpan" id="kobo.311.4">The training set is used to develop the model, while the calibration set helps calculate the nonconformity scores. </span><span class="koboSpan" id="kobo.311.5">This two-step process optimizes computation and delivers precise </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">prediction regions.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<span class="koboSpan" id="kobo.313.1"><img alt="Figure 5.3 – Inductive conformal prediction" src="image/B19925_05_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.314.1">Figure 5.3 – Inductive conformal prediction</span></p>
<p><span class="koboSpan" id="kobo.315.1">A predictive model, such as a neural network or a decision tree, is first trained on the proper training set. </span><span class="koboSpan" id="kobo.315.2">Then, the nonconformity of each example in the calibration set is computed using the trained model. </span><span class="koboSpan" id="kobo.315.3">The nonconformity measure is a real-valued function that describes how much an example contradicts the rest of the data. </span><span class="koboSpan" id="kobo.315.4">The nonconformity scores of the calibration set are then used to determine the size of the prediction region for </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">new examples.</span></span></p>
<p><span class="koboSpan" id="kobo.317.1">The inductive approach offers a significant computational advantage, particularly for large datasets. </span><span class="koboSpan" id="kobo.317.2">By creating the predictive model only once, ICP reduces the algorithm’s time complexity, unlike TCP, which requires retraining the model for each new prediction. </span><span class="koboSpan" id="kobo.317.3">However, it’s important to note that ICP assumes the data are exchangeable, meaning the data’s order doesn’t carry </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">any information.</span></span></p>
<p><span class="koboSpan" id="kobo.319.1">In terms of </span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.320.1">applications, inductive conformal predictors can be used for both classification (binary and multiclass) and regression tasks. </span><span class="koboSpan" id="kobo.320.2">The method offers a flexible and efficient way of providing a measure of uncertainty associated with predictions, a valuable feature in many </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">practical applications.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">ICP involves several steps, most of which center around the calculation of nonconformity scores. </span><span class="koboSpan" id="kobo.322.2">Here’s a rough outline of the algorithm along with the associated </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">mathematical formulation:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.324.1">Data partitioning</span></strong><span class="koboSpan" id="kobo.325.1">: Split the initial dataset </span><em class="italic"><span class="koboSpan" id="kobo.326.1">D</span></em><span class="koboSpan" id="kobo.327.1"> into a proper training set </span><em class="italic"><span class="koboSpan" id="kobo.328.1">D_train</span></em><span class="koboSpan" id="kobo.329.1">, and a calibration </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">set </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.331.1">D_cal</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.333.1">Model training</span></strong><span class="koboSpan" id="kobo.334.1">: Train a predictive model </span><em class="italic"><span class="koboSpan" id="kobo.335.1">M</span></em><span class="koboSpan" id="kobo.336.1"> on </span><em class="italic"><span class="koboSpan" id="kobo.337.1">D_train</span></em><span class="koboSpan" id="kobo.338.1">. </span><span class="koboSpan" id="kobo.338.2">This model is used to generate predictions on new instances. </span><span class="koboSpan" id="kobo.338.3">The type of model (e.g., SVM, decision tree, linear regression, etc.) depends on the problem </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">at hand.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.340.1">Nonconformity measure calculation</span></strong><span class="koboSpan" id="kobo.341.1">: Use the trained model </span><em class="italic"><span class="koboSpan" id="kobo.342.1">M</span></em><span class="koboSpan" id="kobo.343.1"> to predict outcomes for instances in the calibration set </span><em class="italic"><span class="koboSpan" id="kobo.344.1">D_cal</span></em><span class="koboSpan" id="kobo.345.1">. </span><span class="koboSpan" id="kobo.345.2">For each instance of, </span><em class="italic"><span class="koboSpan" id="kobo.346.1">(x_i, y_i)</span></em><span class="koboSpan" id="kobo.347.1"> in </span><em class="italic"><span class="koboSpan" id="kobo.348.1">D_cal</span></em><span class="koboSpan" id="kobo.349.1">, compute a nonconformity score </span><em class="italic"><span class="koboSpan" id="kobo.350.1">α_i</span></em><span class="koboSpan" id="kobo.351.1">, representing the </span><em class="italic"><span class="koboSpan" id="kobo.352.1">strangeness</span></em><span class="koboSpan" id="kobo.353.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.354.1">abnormality</span></em><span class="koboSpan" id="kobo.355.1"> of the instance. </span><span class="koboSpan" id="kobo.355.2">The nonconformity measure </span><em class="italic"><span class="koboSpan" id="kobo.356.1">α</span></em><span class="koboSpan" id="kobo.357.1"> is generally problem-specific. </span><span class="koboSpan" id="kobo.357.2">For instance, classification tasks could be hinge loss </span><em class="italic"><span class="koboSpan" id="kobo.358.1">1 - p_yi,</span></em><span class="koboSpan" id="kobo.359.1"> where </span><em class="italic"><span class="koboSpan" id="kobo.360.1">p_yi</span></em><span class="koboSpan" id="kobo.361.1"> is the predicted probability of the correct class </span><em class="italic"><span class="koboSpan" id="kobo.362.1">y_i</span></em><span class="koboSpan" id="kobo.363.1"> according to the model </span><em class="italic"><span class="koboSpan" id="kobo.364.1">M</span></em><span class="koboSpan" id="kobo.365.1">. </span><span class="koboSpan" id="kobo.365.2">For regression, it could be the absolute error </span><em class="italic"><span class="koboSpan" id="kobo.366.1">|y_i - y_hat_i|</span></em><span class="koboSpan" id="kobo.367.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.368.1">y_hat_i</span></em><span class="koboSpan" id="kobo.369.1"> is the model’s prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">for </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.371.1">x_i</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.373.1">Prediction interval generation</span></strong><span class="koboSpan" id="kobo.374.1">: Given a new test point </span><em class="italic"><span class="koboSpan" id="kobo.375.1">x</span></em><span class="koboSpan" id="kobo.376.1">, calculate its nonconformity score </span><em class="italic"><span class="koboSpan" id="kobo.377.1">α_x</span></em><span class="koboSpan" id="kobo.378.1"> using model </span><em class="italic"><span class="koboSpan" id="kobo.379.1">M</span></em><span class="koboSpan" id="kobo.380.1">. </span><span class="koboSpan" id="kobo.380.2">Then, calculate the p-value for </span><em class="italic"><span class="koboSpan" id="kobo.381.1">x</span></em><span class="koboSpan" id="kobo.382.1">, which is the proportion of instances in </span><em class="italic"><span class="koboSpan" id="kobo.383.1">D_cal </span></em><span class="koboSpan" id="kobo.384.1">with nonconformity scores greater than or equal to </span><em class="italic"><span class="koboSpan" id="kobo.385.1">α_x</span></em><span class="koboSpan" id="kobo.386.1">. </span><em class="italic"><span class="koboSpan" id="kobo.387.1">p_x</span></em><span class="koboSpan" id="kobo.388.1"> can be computed </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">as follows:</span></span><p class="list-inset"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.390.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.391.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.392.1">x</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.393.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.394.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.395.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.396.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.397.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.398.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.399.1">:</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.400.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.401.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.402.1">i</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.403.1">≥</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.404.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.405.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.406.1">x</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.407.1">|</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.408.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.409.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.410.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.411.1">|</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.412.1">D</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.413.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.414.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.415.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.416.1">|</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.417.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.418.1">1</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.419.1">)</span></span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.420.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.421.1">|{i: α_i ≥ α_x}|</span></strong><span class="koboSpan" id="kobo.422.1"> denotes the number of instances in the calibration set with nonconformity scores greater than or equal to </span><em class="italic"><span class="koboSpan" id="kobo.423.1">α_x</span></em><span class="koboSpan" id="kobo.424.1">. </span><span class="koboSpan" id="kobo.424.2">This p-value represents</span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.425.1"> how often we expect to observe a nonconformity score at least as large as </span><em class="italic"><span class="koboSpan" id="kobo.426.1">α_x</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.427.1">by chance.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.428.1">Prediction output</span></strong><span class="koboSpan" id="kobo.429.1">: Using the calculated p-value, create a prediction set </span><em class="italic"><span class="koboSpan" id="kobo.430.1">Γ(x)</span></em><span class="koboSpan" id="kobo.431.1"> for the new test point </span><em class="italic"><span class="koboSpan" id="kobo.432.1">x</span></em><span class="koboSpan" id="kobo.433.1">. </span><span class="koboSpan" id="kobo.433.2">For classification, the prediction set contains all classes </span><em class="italic"><span class="koboSpan" id="kobo.434.1">y</span></em><span class="koboSpan" id="kobo.435.1"> for which the p-value is at least the chosen significance level </span><em class="italic"><span class="koboSpan" id="kobo.436.1">ε</span></em><span class="koboSpan" id="kobo.437.1">:  Γ(x) = {y: p_y ≥ ε}. </span><span class="koboSpan" id="kobo.437.2">For regression, an interval prediction (</span><em class="italic"><span class="koboSpan" id="kobo.438.1">y_lower, </span></em><em class="italic"><span class="koboSpan" id="kobo.439.1">y_upper</span></em><span class="koboSpan" id="kobo.440.1">) is typically outputted, where </span><em class="italic"><span class="koboSpan" id="kobo.441.1">y_lower</span></em><span class="koboSpan" id="kobo.442.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.443.1">y_upper </span></em><span class="koboSpan" id="kobo.444.1">are the lowest and highest values, respectively, for which the p-value is at least as large as the chosen </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">significant level.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.446.1">Please note that this is a high-level description of the algorithm and mathematical formulation. </span><span class="koboSpan" id="kobo.446.2">The exact details may vary based on the specific form of ICP used and the type of problem (classification, regression, etc.) </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">being addressed.</span></span></p>
<p><span class="koboSpan" id="kobo.448.1">As we’ve unpacked the complexities and capabilities of both classical and inductive approaches, it’s now essential to discern how to choose the optimal method for a given situation. </span><span class="koboSpan" id="kobo.448.2">Let’s navigate the factors and guidelines that will guide you in selecting the best-fit conformal predictor for your specific needs in the upcoming section, </span><em class="italic"><span class="koboSpan" id="kobo.449.1">Choosing the right </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.450.1">conformal predictor</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">.</span></span></p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.452.1">Choosing the right conformal predictor</span></h1>
<p><span class="koboSpan" id="kobo.453.1">Both classical and inductive</span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.454.1"> conformal predictors offer valuable approaches to building reliable machine learning models. </span><span class="koboSpan" id="kobo.454.2">However, they each come with unique strengths </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">and weaknesses.</span></span></p>
<p><span class="koboSpan" id="kobo.456.1">Classical transductive conformal predictors are highly adaptable and do not make any assumptions about data distribution. </span><span class="koboSpan" id="kobo.456.2">However, they tend to be computationally expensive, requiring the model’s retraining for each </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">new prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.458.1">Inductive conformal predictors, conversely, are computationally more efficient, as they only require the model to be </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">trained once.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">Choosing the right conformal predictor largely depends on the specific requirements of the problem at hand. </span><span class="koboSpan" id="kobo.460.2">Some considerations might include </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.462.1">Computation resources</span></strong><span class="koboSpan" id="kobo.463.1">: If computation </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.464.1">resources or time are a concern, inductive conformal predictors might be more suitable due to their reduced </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">computational cost</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.466.1">Data size</span></strong><span class="koboSpan" id="kobo.467.1">: For smaller datasets, classical conformal predictors might be more suitable, while for larger datasets, inductive conformal predictors are usually the preferred choice due to </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">computational efficiency</span></span></li>
<li> <strong class="bold"><span class="koboSpan" id="kobo.469.1">Data quality</span></strong><span class="koboSpan" id="kobo.470.1">: If data quality is high, inductive conformal predictors can be a </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">good choice</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.472.1">Real-time requirements</span></strong><span class="koboSpan" id="kobo.473.1">: If the model needs to make real-time predictions, inductive conformal predictors might be more suitable due to their one-time </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">training process</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.475.1">Here are real-life scenarios illustrating when one might opt for transductive or inductive </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">conformal predictors.</span></span></p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.477.1">Transductive conformal predictors</span></h2>
<p><span class="koboSpan" id="kobo.478.1">medical diagnostics with </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">limited</span></span><span class="No-Break"><a id="_idIndexMarker169"/></span><span class="No-Break"><span class="koboSpan" id="kobo.480.1"> data:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.481.1">Scenario</span></strong><span class="koboSpan" id="kobo.482.1">: A hospital</span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.483.1"> uses machine learning to diagnose a rare disease but only has a limited dataset of </span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">past patients.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.485.1">Reasoning</span></strong><span class="koboSpan" id="kobo.486.1">: Given the smaller dataset and the critical nature of accurate predictions, classical TCP is favored. </span><span class="koboSpan" id="kobo.486.2">Its adaptability and distribution-free nature might lead to more accurate predictions, even if it requires more computational power </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">per prediction.</span></span></li>
</ul>
<h2 id="_idParaDest-57"><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.488.1">Inductive conformal predictors</span></h2>
<p><span class="koboSpan" id="kobo.489.1">e-commerce </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">recommendation systems:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.491.1">Scenario</span></strong><span class="koboSpan" id="kobo.492.1">: A large e-commerce platform wants to provide real-time product recommendations to millions of its users</span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.493.1"> based on their </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">browsing habits.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.495.1">Reasoning</span></strong><span class="koboSpan" id="kobo.496.1">: Due to the massive scale, the </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.497.1">system can’t afford to retrain models for every recommendation. </span><span class="koboSpan" id="kobo.497.2">ICP’s one-time training process, combined with its computational efficiency for larger datasets, makes it a </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">suitable choice.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.499.1">To effectively choose the appropriate type of conformal predictor, it’s essential to gain a deep understanding of both classical and inductive conformal predictors, their working principles, and their strengths and weaknesses. </span><span class="koboSpan" id="kobo.499.2">Furthermore, understanding the nature and requirements of the problem domain, such as the specific characteristics of the data, the computational resources available, the need for real-time predictions, and the importance of model interpretability, can significantly aid in making an informed choice. </span><span class="koboSpan" id="kobo.499.3">Always remember that the best conformal predictor is the one that best meets the needs of your specific </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">problem domain.</span></span></p>
<h1 id="_idParaDest-58"><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.501.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.502.1">This chapter explored the fascinating world of conformal predictors, their types, and their distinctive features. </span><span class="koboSpan" id="kobo.502.2">The key concepts and skills we touched upon include covering the foundational principles of conformal prediction and its application in machine learning. </span><span class="koboSpan" id="kobo.502.3">It also highlighted the differences between classical transductive and inductive conformal predictors. </span><span class="koboSpan" id="kobo.502.4">We also covered how to effectively choose the appropriate type of conformal predictor based on the specific requirements of the problem. </span><span class="koboSpan" id="kobo.502.5">Finally, the practical applications of conformal predictors in binary classification, multiclass classification, and regression were </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">also included.</span></span></p>
<p><span class="koboSpan" id="kobo.504.1">The chapter also provided a detailed algorithmic description and mathematical formulation of classical and inductive conformal predictors, adding to our theoretical understanding. </span><span class="koboSpan" id="kobo.504.2">To deepen our learning, we also took a hands-on approach, looking at practical examples </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">in Python.</span></span></p>
<p><span class="koboSpan" id="kobo.506.1">For those interested in further exploring conformal predictors, several avenues exist for you to consider. </span><span class="koboSpan" id="kobo.506.2">A more detailed study of the mathematical underpinnings of conformal prediction could be pursued, along with implementing conformal predictors in more complex machine </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.508.1">Exploring the advanced versions of conformal predictors, such as Mondrian conformal predictors, or understanding how conformal prediction can be integrated with other machine learning techniques, such as neural networks and ensemble learning, are also exciting areas for </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">further research.</span></span></p>
<p><span class="koboSpan" id="kobo.510.1">In closing, we hope this chapter has given a solid grounding in the principles and applications of conformal prediction. </span><span class="koboSpan" id="kobo.510.2">Moving into the next chapter, we’ll delve deeper into conformal prediction for classification problems. </span><span class="koboSpan" id="kobo.510.3">As always, keep exploring, keep learning, and enjoy </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">the journey!</span></span></p>
</div>


<div class="Content" id="_idContainer022">
<h1 id="_idParaDest-59" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.1.1">Part 3: Applications of Conformal Prediction</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In this part, we will provide more details about conformal prediction for classification problems. </span><span class="koboSpan" id="kobo.2.2">It will introduce the calibration concept and illustrate how conformal prediction compares with other calibration methods, explaining how it can quantify uncertainty in regression to produce well-calibrated prediction intervals. </span><span class="koboSpan" id="kobo.2.3">This part will also explain how conformal prediction can produce prediction intervals for point forecasting models, illustrate applications using open source libraries, and detail recent innovations in conformal prediction for NLP. </span><span class="koboSpan" id="kobo.2.4">Finally, this part will explain how conformal prediction can be applied to produce state-of-the-art uncertainty quantification for NLP and illustrate applications using open </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">source libraries.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This section has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B19925_06.xhtml#_idTextAnchor058"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Conformal Prediction for Classification</span></em></li>
<li><a href="B19925_07.xhtml#_idTextAnchor073"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Conformal Prediction for Regression</span></em></li>
<li><a href="B19925_08.xhtml#_idTextAnchor090"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 8</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Conformal Prediction for Time Series and Forecasting</span></em></li>
<li><a href="B19925_09.xhtml#_idTextAnchor111"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 9</span></em></a><span class="koboSpan" id="kobo.16.1">, </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Conformal Prediction for Computer Vision</span></em></li>
<li><a href="B19925_10.xhtml#_idTextAnchor130"><em class="italic"><span class="koboSpan" id="kobo.18.1">Chapter 10</span></em></a><span class="koboSpan" id="kobo.19.1">, </span><em class="italic"><span class="koboSpan" id="kobo.20.1">Conformal Prediction for Natural Language Processing</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer023">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer024">
</div>
</div>
</body></html>