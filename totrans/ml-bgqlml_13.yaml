- en: '*Chapter 10*: Predicting Boolean Values Using XGBoost'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**eXtreme Gradient Boosting** (**XGBoost**) is one of the most powerful **machine
    learning** (**ML**) libraries that data scientists can leverage to solve complex
    use cases in an efficient and flexible way. It started as a research project,
    and the first version was released in 2014\. The popularity of this ML library
    grew very quickly, thanks to its capabilities and portability. In fact, it was
    used in important Kaggle ML contests and is now available for different programming
    languages and on different operating systems.'
  prefs: []
  type: TYPE_NORMAL
- en: This library can be used to tackle different ML problems and is specifically
    designed for structured data. XGBoost was also recently released for BigQuery
    ML. Thanks to this technique, BigQuery users are allowed to implement classification
    and regression ML models using this library.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll see all the stages necessary to implement a XGBoost classification
    model to classify New York City trees into different species according to their
    characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the BigQuery ML SQL dialect, we''ll go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the business scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering the XGBoost Boosted Tree classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring and understanding the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the XGBoost classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the XGBoost classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the XGBoost classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing business conclusions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires you to have access to a web browser and to be able to
    leverage the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A **Google Cloud Platform** (**GCP**) account to access the Google Cloud Console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GCP project to host the BigQuery datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we're ready in terms of the technical requirements, let's dive into
    the analysis and development of our BigQuery ML XGBoost classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/3ujnzH3](https://bit.ly/3ujnzH3)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the business scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll introduce the business scenario that will be tackled
    with the XGBoost classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The business scenario is very similar to the use case presented and used in
    [*Chapter 6*](B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088), *Classifying Trees
    with Multiclass Logistic Regression*. In this chapter, we'll use the same dataset
    but will apply a more advanced ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We can summarize and remember that the goal of the ML model is to automatically
    classify the trees of New York City into different species according to their
    characteristics, such as their position, their size, and their health status.
  prefs: []
  type: TYPE_NORMAL
- en: As we've done in [*Chapter 9*](B16722_09_Final_ASB_ePub.xhtml#_idTextAnchor133),
    *Suggesting the Right Product by Using Matrix Factorization*, we can focus our
    attention only on the five most common species of trees present in the city.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've explained and understood the business scenario, let's take a
    look at the ML technique that we can use to automatically classify trees according
    to their features.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the XGBoost Boosted Tree classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll learn what the **XGBoost Boosted Trees** classification
    model is, and we'll understand which classification use cases can be tackled with
    this ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost is an open source library that provides a portable gradient boosting
    framework for different languages. The XGBoost library is available for different
    programming languages such as C++, Java, Python, R, and Scala, and can work on
    different operating systems. XGBoost is used to deal with supervised learning
    use cases, where we use labeled training data to predict target variables.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost's popularity has grown in the ML community over the years because it
    has often been the choice of many winning teams during ML competitions, such as
    the *Kaggle - High Energy Physics meets Machine Learning award* in 2016.
  prefs: []
  type: TYPE_NORMAL
- en: The classification capabilities of **XGBoost Boosted Trees** are based on the
    usage of multiple decision trees that classify data to enable predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see a simple representation of a decision
    tree that classifies animals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16722_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Representation of a decision tree
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost classification models can answer the same questions addressed by multiclass
    logistic regression, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the comment of my customer *neutral*, *positive*, or *negative*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does my customer belong to the *gold*, *silver*, or *bronze* level?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the probability of churn of a specific customer *high*, *medium*, or *low*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the image recognition algorithm identify a *cat*, a *dog*, a *mouse*, or
    a *cow*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our business scenario, we can classify New York City trees into five different
    species by leveraging the XGBoost Boosted Tree classification model. In fact,
    we're interested in predicting the species according to the characteristics of
    each tree.
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase of a XGBoost algorithm, the ML model tries to find
    the best values to assign to each tree in order to minimize the final error metric.
  prefs: []
  type: TYPE_NORMAL
- en: After the training, we'll compare the results of this model with the outcomes
    that we got in [*Chapter 6*](B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088),
    *Classifying Trees with Multiclass Logistic Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've learned the basics of the XGBoost Boosted Tree algorithm, it's
    time to take a look at the dataset that we'll use to build our ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and understanding the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll analyze and prepare the dataset for our use case. We'll
    start with some data quality checks, and then we'll segment the data into training,
    evaluation, and test tables.
  prefs: []
  type: TYPE_NORMAL
- en: Since the dataset has already been used in [*Chapter 6*](B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088),
    *Classifying Trees with Multiclass Logistic Regression*, we will not start the
    analysis from the beginning. Instead, we'll focus on the most relevant queries
    for our business scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start our exploration of the data and to carry out data quality checks,
    we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to our Google Cloud Console and access the **BigQuery** **User Interface**
    (**UI**) from the navigation menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new dataset under the project that we created in [*Chapter 2*](B16722_02_Final_ASB_ePub.xhtml#_idTextAnchor039),
    *Setting Up Your GCP and BigQuery Environment*. For this use case, we'll create
    a `10_nyc_trees_xgboost` dataset with the default options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First of all, let''s check if all the records contain a valid value in the
    `spc_latin` field by executing the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we can see from the following screenshot, there are `spc_latin` column.
    These records will be filtered out during the training stage:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.2 – The query result shows that some records should be filtered
    out'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16722_10_002.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.2 – The query result shows that some records should be filtered out
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After this first check, we need to verify if any potential feature is characterized
    by `NULL` values. Let's run the following `COUNT` of three records due to the
    presence of `NULL` values in the `sidewalk` and `health` fields. Despite the low
    number, we'll filter out these records in the following queries, to use only meaningful
    records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we can extract the most common five tree species from the BigQuery public
    dataset. Let''s execute the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The query calculates `total` as the number of records for each `spc_latin` field.
    An `ORDER BY` clause is used to sort the results from the largest to the smallest
    values of the `total` field. Then, a `LIMIT 5` clause is used to return only the
    first five records.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see the results of the query, which show
    the five species most frequently present in the dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.3 – The most frequent tree species in the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16722_10_003.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.3 – The most frequent tree species in the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to materialize these five species into a table, let''s execute the
    following code to create a `` `10_nyc_trees_xgboost.top5_species` `` table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The only difference with the query executed in the previous *Step 5* is represented
    by the use of `CREATE OR REPLACE TABLE` keywords that are leveraged to materialize
    the results of the query into the new table.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this section, we've analyzed the data quality of the BigQuery public dataset.
    Now, let's start segmenting it into three different tables for the training, evaluation,
    and classification stages.
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before implementing our XGBoost classification model, let''s segment our dataset
    according to the main stages of the ML development life cycle: training, evaluation,
    and use. In order to randomly divide the records into three different tables,
    we''ll use a `MOD` function on the `tree_id` numerical field. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let''s create a table that will contain the training dataset.
    To do this, we execute the following SQL statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The query creates a `` `10_nyc_trees_xgboost.training_table` `` table with all
    the columns available in the original dataset, through a `SELECT *` statement.
    It applies all the filters necessary to get not empty values for the `spc_latin`
    label and for all the other features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using an `IN` clause, `training_table` will contain only the records related
    to the most five common species that we've identified in the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The last line of the query, with the `MOD(tree_id,11)<=8` clause, allows us
    to pick up only 80% of the records from the entire dataset. `MOD` stands for *modulo*
    and returns the remainder of the division of `tree_id` by 11\. With the less than
    or equal operator (`<=`), we're approximately extracting 80% of the entire dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With a similar approach, we can create a  `` `10_nyc_trees_xgboost.evaluation_table`
    `` table that will be used for the evaluation of our ML model. Let''s execute
    the following `CREATE TABLE` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In contrast to when we created the training table, for the `evaluation_table`
    table we're picking up only 10% of the records from the entire dataset, by applying
    a `MOD(tree_id,11)=9` filter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we''ll execute the following SQL statement in order to create a ``
    `10_nyc_trees_xgboost.classification_table` `` table that will be used to apply
    our XGBoost classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This new table is very similar to the previous ones, but thanks to the `MOD`
    function will contain the remaining 10% of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this section, we've analyzed the dataset that contains information about
    trees in New York City, applied some data quality checks to exclude empty values,
    and segmented the dataset, focusing on the five most common species. Now that
    we've completed the preparatory steps, it's time to move on and start the training
    of our BigQuery ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Training the XGBoost classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve segmented the dataset into multiple tables to support the different
    stages of the ML model life cycle, let''s focus on the training of our XGBoost
    classification model. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with training our first ML model, `xgboost_classification_model_version_1`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this BigQuery ML statement, we can see `CREATE OR REPLACE MODEL` keywords
    used to start the training of the model. These keywords are followed by the identifier
    of the ML model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After the identifier, we can notice an `OPTIONS` clause. For the `MODEL_TYPE`,
    we've chosen a `BOOSTED_TREE_CLASSIFIER` option, which allows us to build a XGBoost
    classification model. The `BOOSTER_TYPE = 'GBTREE'` clause is considered a default
    option to train XGBoost boosted tree models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In order to limit the complexity of training and the resource consumption, we've
    chosen to train only one tree in parallel with a `NUM_PARALLEL_TREE = 1` clause,
    and to stop the training after `50` iterations using `MAX_ITERATIONS`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A `HIST` parameter is suggested for large datasets in the XGBoost documentation,
    and an `EARLY_STOP = FALSE` clause is used to prevent the training phase being
    stopped after the first iteration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The last option, `AUTO_CLASS_WEIGHTS=TRUE`, is used to balance the weights—
    in the case of an unbalanced dataset—with some tree species that can occur more
    frequently than others.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This first version of the model tries to predict the species of each tree, leveraging
    only the `zip_city` code where the tree is planted and the diameter of the tree,
    `tree_dbh`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At the end of the training, we can access the ML model from the BigQuery navigation
    menu to have a look at the performance of the model. Selecting the **Evaluation**
    tab, we can see the **ROC AUC** value. In this case, the value is **0.7775**,
    as we can see in the following screenshot:![Figure 10.4 – The Evaluation metrics
    of the XGBoost classification model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16722_10_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.4 – The Evaluation metrics of the XGBoost classification model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the same **Evaluation** tab, we can also visualize the confusion matrix,
    which shows how many times the predicted value is equal to the actual one, as
    illustrated in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.5 – The Evaluation tab shows the confusion matrix for the XGBoost
    classification model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16722_10_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.5 – The Evaluation tab shows the confusion matrix for the XGBoost
    classification model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s try to improve our ML model by adding features that can be useful to
    classify the trees into different species. Let''s train the second version of
    our BigQuery ML model by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Compared to the first attempt of the previous *Step 1*, we've included additional
    features. In fact, we've added to the features the name of the borough contained
    in the `boroname` field and the `nta_name` field, which provides more specific
    information related to the position of the tree in the city.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After the execution of the SQL statement, let's access the **Evaluation** tab
    of the new model to see if we're improving its performance. Taking a look at the
    **ROC AUC** value of **0.80**, we can see a slight increase in the performance
    of our model compared to the first version.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we''ll try to add to our ML model other features related to the health
    of the tree and also to the intrusiveness of its roots, which can damage adjacent
    sidewalks, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Compared to the previous ML model, the `xgboost_classification_model_version_3`
    model includes a `health` field, which describes the health status of our tree,
    and a `sidewalk` field, which is used to specify if the roots of tree are damaging
    adjacent sidewalks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Looking at the performances of our last ML model in the **Evaluation** tab of
    the BigQuery UI, we can notice that we've achieved another increase in terms of
    the **ROC AUC**, with a value of **0.8121**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we've created different ML models by trying to use different
    features in our dataset. In the next steps, we'll use the model with the highest
    `xgboost_classification_model_version_3`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's start the evaluation stage of the XGBoost classification model on
    the evaluation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the XGBoost classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To evaluate our BigQuery ML model, we'll use a `ML.EVALUATE` function and the
    table that we've expressly created as an evaluation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following query will tell us if the model is suffering from overfitting
    or is able to also perform well on new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `SELECT` statement extracts the `roc_auc` value returned by the `ML.EVALUATE`
    function and also provides a clear description of the quality of the model that
    starts from `'POOR'` and can achieve an `'EXCELLENT'` grade, passing through some
    intermediate stages such as `'NEEDS IMPROVEMENTS'` and `'GOOD'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing the query, we can see that the score is **VERY GOOD**, as illustrated
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – The evaluation stage returns VERY GOOD for the quality of our
    BigQuery ML model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_10_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – The evaluation stage returns VERY GOOD for the quality of our
    BigQuery ML model
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've evaluated our ML model, let's see how we can apply it to other
    records to get a classification of the trees.
  prefs: []
  type: TYPE_NORMAL
- en: Using the XGBoost classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll use the ML model to classify the trees into five different
    species according to their characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test our BigQuery ML model, we''ll use a `ML.PREDICT` function on the `classification_table`
    table, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The query is composed of a `SELECT` statement that extracts the `tree_id` value,
    the actual species of the tree, the probability of each predicted species, and
    the predicted species.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, you can see the result of the query execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – The output of the query shows the actual and predicted labels'
  prefs: []
  type: TYPE_NORMAL
- en: with the related probabilities
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_10_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – The output of the query shows the actual and predicted labels
    with the related probabilities
  prefs: []
  type: TYPE_NORMAL
- en: In the two rows presented in the preceding screenshot, the trees with identifiers
    **283502** and **226929** are well classified into the **Acer platanoides** species,
    with a confidence of 61%.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've tested our BigQuery ML model, let's make some final considerations
    by comparing the results of the XGBoost classification model with the outcome
    of the logistic regression used in [*Chapter 6*](B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088),
    *Classifying Trees with Multiclass Logistic Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing business conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll use our ML model, and we'll understand how many times
    the BigQuery ML model is able to classify the trees well in the `classification_table`
    table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s execute the following query to calculate how many times the predicted
    species is congruent with the actual one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To calculate this value, we've introduced a `WHERE` clause by filtering only
    the rows where the predicted value is equal to the actual one.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the following screenshot, the `SELECT COUNT` returns a value
    of **14277** records:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – The output of the query shows how many times the classification'
  prefs: []
  type: TYPE_NORMAL
- en: model predicts the right species
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_10_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – The output of the query shows how many times the classification
    model predicts the right species
  prefs: []
  type: TYPE_NORMAL
- en: Out of a total of 27,182 rows stored in the `classification_table` table, we
    can say that our model classifies the trees into the right species in 52.52% of
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, the results of the XGBoost classification model are
    compared with the results obtained by the multiclass logistic regression, applied
    in [*Chapter 6*](B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088), *Classifying
    Trees with Multiclass Logistic Regression*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 10.9 – Comparison of the XGBoost classification model and multiclass
    logistic regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16722_10_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – Comparison of the XGBoost classification model and multiclass
    logistic regression
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the preceding table, we can say that to classify the New York City
    trees into the most common five species, the XGBoost classification model is able
    to achieve better results when compared to the multiclass logistic regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've implemented a XGBoost classification model. We've remembered
    the business scenario that was already used in [*Chapter 6*](B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088),
    *Classifying Trees with Multiclass Logistic Regression*, based on the need to
    automatically classify New York City trees. After that, we've learned the basics
    of the XGBoost boosted tree classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to build an effective model, we performed data quality checks and
    then segmented the dataset according to our needs into three tables: one to host
    training data, a second one for the evaluation stage, and a last one to apply
    our classification model.'
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase of the BigQuery ML model, we've constantly improved
    the performance of the ML model, using ROC AUC as a **key performance indicator**
    (**KPI**).
  prefs: []
  type: TYPE_NORMAL
- en: After that, we've evaluated the best ML model on a new set of records to avoid
    any overfitting, becoming more confident about the good quality of our XGBoost
    classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we've applied our BigQuery ML model to the last subset of records to
    classify the trees into species, according to their characteristics. We've discovered
    that our ML model is able to correctly classify the trees in 52.52% of cases.
    Then, we've also compared the performance of the XGBoost model with the multiclass
    logistic regression training we did in [*Chapter 6*](B16722_06_Final_ASB_ePub.xhtml#_idTextAnchor088),
    *Classifying Trees with Multiclass Logistic Regression* and noticed that XGBoost
    exceeded the multiclass logistic regression training's performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll learn about advanced **deep neural networks** (**DNNs**),
    leveraging BigQuery SQL syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Further resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**NYC Street Tree Census public dataset**: [https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tree-census](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tree-census)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGBoost home page**: [https://xgboost.ai/](https://xgboost.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGBoost documentation**: [https://xgboost.readthedocs.io/en/latest/index.html](https://xgboost.readthedocs.io/en/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CREATE MODEL` **statement for Boosted Tree models**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-boosted-tree)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML.EVALUATE` **function**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ML.PREDICT` **function**: [https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-predict)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
