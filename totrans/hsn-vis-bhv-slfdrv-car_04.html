<html><head></head><body><div><div><h1 id="_idParaDest-64"><a id="_idTextAnchor066"/><em class="italic">Chapter 3</em>: Lane Detection</h1>
			<p>This chapter will show one of the incredible things possible using computer vision in general and OpenCV in particular: lane detection. You will learn how to analyze an image and build more and more visual knowledge about it, one step after another, applying several filtering techniques, replacing noise and approximation with a better understanding of the image, until you will be able to detect where the lanes are on a straight road or on a turn, and we will apply this pipeline to a video to highlight the road.</p>
			<p>You will see that this method relies on several assumptions that might not be true in the real world, though it can be adjusted to correct for that. Hopefully, you will find this chapter quite interesting.</p>
			<p>We will cover the following topics:</p>
			<ul>
				<li>Detecting lanes in a road</li>
				<li>Color spaces</li>
				<li>Perspective correction</li>
				<li>Edge detection</li>
				<li>Thresholding</li>
				<li>Histograms</li>
				<li>The sliding window algorithm</li>
				<li>Polynomial fitting</li>
				<li>Video filtering</li>
			</ul>
			<p>By the end of this chapter, you will be able to design a pipeline that is able to detect the lanes on a road, using OpenCV.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor067"/>Technical requirements</h1>
			<p>Our lane detection pipeline requires quite a lot of code. We will explain the main concepts, and you can find the full code on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter3">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter3</a>.</p>
			<p>For the instructions and code in this chapter, you need the following:</p>
			<ul>
				<li>Python 3.7</li>
				<li>The OpenCV-Python module</li>
				<li>The NumPy module</li>
				<li>The Matplotlib module</li>
			</ul>
			<p>To identify the lanes, we need some images and a video. While it's easy to find some open source database to use for this, they are usually only available for non-commercial purposes. For this reason, in this book, we will use images and video generated by two open source projects: CARLA, a simulator useful for autonomous driving tasks, and Speed Dreams, an open source video game. All the techniques also work with real-world footage, and you are encouraged to try them on some public datasets, such as CULane or KITTI.</p>
			<p>The Code in Action videos for this chapter can be found here:</p>
			<p><a href="https://bit.ly/37pjxnO">https://bit.ly/37pjxnO</a></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor068"/>How to perform thresholding</h1>
			<p>While for a human<a id="_idIndexMarker203"/> it is easy to follow a lane, for a computer, this is not something that is so simple. One problem is that an image of the road has too much information. We need to simplify it, selecting only the parts of the image that we are interested in. We will only analyze the part of the image with the lane, but we also need to separate the lane from the rest of the image, for example, using color selection. After all, the road is typically black or dark, and lanes are usually white or yellow.</p>
			<p>In the next sections, we will analyze different color spaces, to see which one is most useful for thresholding.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor069"/>How thresholding works on different color spaces</h2>
			<p>From a practical<a id="_idIndexMarker204"/> point of view, a color space is a way to decompose the colors of an image. You are most likely comfortable with RGB, but there are others.</p>
			<p>OpenCV supports several color spaces, and as part of this pipeline, we need to choose the two best channels from a variety of color spaces. Why do we want to use two different channels? For two reasons:</p>
			<ul>
				<li>A color space that is good for white lanes might not be good for yellow ones.</li>
				<li>When there are difficult frames (for example, with shadows on the road or if the lane is discolored), one channel could be less affected than another one.</li>
			</ul>
			<p>This might not be strictly necessary for our example, as the lanes are always white, but it is definitely useful in real life.</p>
			<p>We will now see how our test image appears in different color spaces, but bear in mind that your case might be different.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor070"/>RGB/BGR</h2>
			<p>The<a id="_idIndexMarker205"/> starting<a id="_idIndexMarker206"/> point will be the following image:</p>
			<div><div><img src="img/B16322_03_01.jpg" alt="Figure 3.1 – Reference image, from Speed Dreams" width="1024" height="600"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Reference image, from Speed Dreams</p>
			<p>The image can, of course, be decomposed into three channels: red, green, and blue. As we know, OpenCV stores the image as BGR (meaning, the first byte is the blue channel, not the red channel), but conceptually, there is no difference.</p>
			<p>These are the three channels once separated:</p>
			<div><div><img src="img/B16322_03_02.jpg" alt="Figure 3.2 – BGR channels: blue, green, and red channels" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – BGR channels: blue, green, and red channels</p>
			<p>They all seem fine. We can try to separate the lane by selecting the white pixels. As the white color is (<code>255, 255, 255</code>), we could leave some margin and select the colors above 180 on the scale. To do this operation, we need to create a black image with the same size as the selected channel, then paint all the pixels that are above 180 in the original channel white:</p>
			<pre>img_threshold = np.zeros_like(channel)
img_threshold [(channel &gt;= 180)] = 255</pre>
			<p>This is<a id="_idIndexMarker207"/> how <a id="_idIndexMarker208"/>the output appears:</p>
			<div><div><img src="img/B16322_03_03.jpg" alt="Figure 3.3 – BGR channels: blue, green, and red channels, threshold above 180" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – BGR channels: blue, green, and red channels, threshold above 180</p>
			<p>They all seem good. The red channel also shows part of the car, but since we will not analyze that part of the image, it is not a problem. As the white color has the same value in the red, green, and blue channels, it is kind of expected that the lane should be visible on all three channels. This would not be true for yellow lanes, though.</p>
			<p>The value that we choose for the threshold is very important, and unfortunately, it is dependent on the colors used for the lane and on the situation of the road; light conditions and shadows will also affect it.</p>
			<p>The following figure shows a totally different threshold, 20-120:</p>
			<div><div><img src="img/B16322_03_04.jpg" alt="Figure 3.4 – BGR channels: blue, green, and red channels, threshold in the range 20-120" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – BGR channels: blue, green, and red channels, threshold in the range 20-120</p>
			<p>You can select the pixels in the 20-120 range with the following code:</p>
			<pre>img_threshold[(channel &gt;= 20) &amp; (channel &lt;= 120)] = 255</pre>
			<p>The image is <a id="_idIndexMarker209"/>probably still usable, as long as you consider that the <a id="_idIndexMarker210"/>lane is black, but it would not be recommended.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor071"/>HLS</h2>
			<p>The HLS color <a id="_idIndexMarker211"/>space divides the color into hue, lightness, and <a id="_idIndexMarker212"/>saturation. The result is sometimes surprising:</p>
			<div><div><img src="img/B16322_03_05.jpg" alt="Figure 3.5 – HLS channels: hue, lightness, and saturation" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – HLS channels: hue, lightness, and saturation</p>
			<p>The hue channel is pretty bad, noisy, and low resolution, while the lightness seems to perform well. The saturation seems to be unable to detect our lane.</p>
			<p>Let's try some thresholding:</p>
			<div><div><img src="img/B16322_03_06.jpg" alt="Figure 3.6 – HLS channels: hue, lightness, and saturation, threshold above 160" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – HLS channels: hue, lightness, and saturation, threshold above 160</p>
			<p>The <a id="_idIndexMarker213"/>threshold shows that lightness is still a good<a id="_idIndexMarker214"/> candidate.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor072"/>HSV</h2>
			<p>The HSV color<a id="_idIndexMarker215"/> space divides the color into hue, saturation, and<a id="_idIndexMarker216"/> value, and it is related to HLS. The result is therefore similar to HLS:</p>
			<div><div><img src="img/B16322_03_07.jpg" alt="Figure 3.7 – HSV channels: hue, saturation, and value" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – HSV channels: hue, saturation, and value</p>
			<p>Hue and saturation are not useful to us, but value looks fine with thresholding applied:</p>
			<div><div><img src="img/B16322_03_08.jpg" alt="Figure 3.8 – HSV channels: hue, saturation, and value, threshold above 160" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – HSV channels: hue, saturation, and value, threshold above 160</p>
			<p>As expected, the threshold of value looks good.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor073"/>LAB</h2>
			<p>The LAB (CIELAB <a id="_idIndexMarker217"/>or CIE L*a*b*) color space divides the <a id="_idIndexMarker218"/>color into L* (lightness, from black to white), a* (from green to red), and b* (from blue to yellow):</p>
			<div><div><img src="img/B16322_03_09.jpg" alt="Figure 3.9 – LAB channels: L*, a*, and b*" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – LAB channels: L*, a*, and b*</p>
			<p>L* seems fine, while a* and b* are not useful to us:</p>
			<div><div><img src="img/B16322_03_10.jpg" alt="Figure 3.10 – LAB channels: L*, a*, and b*, threshold above 160" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10 – LAB channels: L*, a*, and b*, threshold above 160</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor074"/>YCbCr</h2>
			<p>YCbCr is the<a id="_idIndexMarker219"/> last color space that we will analyze. It<a id="_idIndexMarker220"/> divides the image into Luma (Y) and two chroma components (Cb and Cr):</p>
			<div><div><img src="img/B16322_03_11.jpg" alt="Figure 3.11 – YCbCr channels: Y, Cb, and Cr" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – YCbCr channels: Y, Cb, and Cr</p>
			<p>This is the result when we apply a threshold:</p>
			<div><div><img src="img/B16322_03_12.jpg" alt="Figure 3.12 – YCbCr channels: Y, Cb, and Cr, threshold above 160" width="1101" height="215"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – YCbCr channels: Y, Cb, and Cr, threshold above 160</p>
			<p>The threshold<a id="_idIndexMarker221"/> confirms the validity of the Luma <a id="_idIndexMarker222"/>channel.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor075"/>Our choice</h2>
			<p>After some experiments, it seems that the green channel can be used for edge detection, and the L channel from the HLS space could be used as additional thresholding, so we'll stick to these. These settings should be also fine for a yellow line, while different colors might require different thresholds.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor076"/>Perspective correction</h1>
			<p>Let's take a step<a id="_idIndexMarker223"/> back and start simple. The easiest case that we can have is with a straight lane. Let's see how it looks:</p>
			<div><div><img src="img/B16322_03_13.jpg" alt="Figure 3.13 – Straight lane, from Speed Dreams" width="1024" height="600"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – Straight lane, from Speed Dreams</p>
			<p>If we were flying over the road, and watching it from a bird's eye view, the lanes would be parallel, but in the picture, they are not, because of the perspective.</p>
			<p>The perspective depends on the focal length of the lens (lenses with a shorter focal length show a stronger perspective) and the position of the camera. Once the camera is mounted on a car, the perspective is fixed, so we can take it into consideration and correct the image.</p>
			<p>OpenCV has a method to compute the perspective transformation: <code>getPerspectiveTransform()</code>.</p>
			<p>It takes two parameters, both arrays of four points, identifying the trapezoid of the perspective. One array is the source and one array is the destination. This means that the same method can be<a id="_idIndexMarker224"/> used to compute the inverse transformation, by just swapping the parameters:</p>
			<pre>perspective_correction = cv2.getPerspectiveTransform(src, dst)
perspective_correction_inv = cv2.getPerspectiveTransform(dst, src)</pre>
			<p>We need to select the area around the lanes, plus a small margin:</p>
			<div><div><img src="img/B16322_03_14.jpg" alt="Figure 3.14 – Trapezoid with the region of interest around the lanes" width="1024" height="600"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14 – Trapezoid with the region of interest around the lanes</p>
			<p>In our case, the destination is a rectangle (as we want to make it straight). <em class="italic">Figure 3.14</em> shows the green trapezoid (the <code>src</code> variable in the previous code) with the original perspective and the white rectangle (the <code>dst</code> variable in the previous code), which is the desired perspective. Please notice that for clarity, they have been drawn as overlapping, but the coordinates of the rectangle passed as a parameter are shifted, as if it was starting at <em class="italic">X</em> coordinate 0.</p>
			<p>We can now apply the perspective correction and get our bird's eye view:</p>
			<pre>cv2.warpPerspective(img, perspective_correction, warp_size, flags=cv2.INTER_LANCZOS4)</pre>
			<p>The <code>warpPerspective()</code> method accepts four parameters:</p>
			<ul>
				<li>The source image.</li>
				<li>The transformation<a id="_idIndexMarker225"/> matrix, obtained from <code>getPerspectiveTransform()</code>.</li>
				<li>The size of the output image. In our case, the width is the same as the original image, but the height is only the height of the trapezoid/rectangle.</li>
				<li>Some flags, to specify the interpolation. <code>INTER_LINEAR</code> is a common choice, but I recommend experimenting, and to give <code>INTER_LANCZOS4</code> a try.</li>
			</ul>
			<p>This is the result of warp using <code>INTER_LINEAR</code>:</p>
			<div><div><img src="img/B16322_03_15.jpg" alt="Figure 3.15 – Warped with INTER_LINEAR" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15 – Warped with INTER_LINEAR</p>
			<p>This is the result using <code>INTER_LANCZOS4</code>:</p>
			<div><div><img src="img/B16322_03_16.jpg" alt="Figure 3.16 – Warped with INTER_LANCZOS4" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.16 – Warped with INTER_LANCZOS4</p>
			<p>They are very similar, but a closer look shows that the interpolation performed with the <code>LANCZOS4</code> resampling is sharper. We will see later that at the end of the pipeline, the difference is significant.</p>
			<p>What is clear in both<a id="_idIndexMarker226"/> images is that our lines are now vertical, which intuitively could help us.</p>
			<p>We'll see in the next section how to leverage this image.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor077"/>Edge detection</h1>
			<p>The next step is detecting <a id="_idIndexMarker227"/>the edges, and we will use the green channel for that, as during our experiments, it gave good results. Please be aware that you need to experiment with the images and videos taken from the country where you plan to run the software, and with many different light conditions. Most likely, based on the color of the lines and the colors in the image, you might want to choose a different channel, possibly from another color space; you can convert the image into different color spaces using <code>cvtColor()</code>, for example:</p>
			<pre>img_hls = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HLS).astype(np.float)</pre>
			<p>We will stick to green.</p>
			<p>OpenCV has several ways to compute edge detection, and we are going to use Scharr, as it performs quite well. Scharr computes a derivative, so it detects the difference in colors in the image. We are interested in the <em class="italic">X</em> axis, and we want the result to be a 64-bit float, so our call would be like this:</p>
			<pre>edge_x = cv2.Scharr(channel, cv2.CV_64F, 1, 0)</pre>
			<p>As Scharr computes a derivative, the values can be both positive and negative. We are not interested in the sign, but only in the fact that there is an edge. So, we will take the absolute value:</p>
			<pre>edge_x = np.absolute(edge_x)</pre>
			<p>Another issue is that the values are not bounded on the 0-255 value range that we expect on a single channel image, and the values are floating points, while we need an 8-bit integer. We can fix both the issues with the following line:</p>
			<pre>edge_x = np.uint8(255 * edge_x / np.max(edge_x))</pre>
			<p>This is the result:</p>
			<div><div><img src="img/B16322_03_17.jpg" alt="Figure 3.17 – Edge detection with Scharr, scaled and with absolute values" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17 – Edge detection with Scharr, scaled and with absolute values</p>
			<p>At this point, we can apply thresholding to convert the image into black and white, to better isolate the pixels of the lanes. We need to choose the intensity of the pixels to select, and in this case, we can go for 20-120; we will select only pixels that have at least an intensity value of 20, and not more than 120:</p>
			<pre>binary = np.zeros_like(img_edge)
binary[img_edge &gt;= 20] = 255</pre>
			<p>The <code>zeros_like()</code> method creates an array full of zeros, with the same shape of the image, and the second line<a id="_idIndexMarker228"/> sets all the pixels with an intensity between 20 and 120 to 255.</p>
			<p>This is the result:</p>
			<div><div><img src="img/B16322_03_18.jpg" alt="Figure 3.18 – Result after applying a threshold of 20" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.18 – Result after applying a threshold of 20</p>
			<p>The lanes are now very visible, but there is some noise. We can reduce that by increasing the threshold:</p>
			<pre>binary[img_edge &gt;= 50] = 255</pre>
			<p>This is how the output appears:</p>
			<div><div><img src="img/B16322_03_19.jpg" alt="Figure 3.19 – Result after applying a threshold of 50" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.19 – Result after applying a threshold of 50</p>
			<p>Now, there is less noise, but we lost the lines on the top.</p>
			<p>We will now describe a technique that can help us to retain the full line without having an excessive amount <a id="_idIndexMarker229"/>of noise.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor078"/>Interpolated threshold</h2>
			<p>In practice, we don't <a id="_idIndexMarker230"/>have to choose between selecting<a id="_idIndexMarker231"/> the whole line with a lot of noise and reducing the noise while detecting only part of the line. We could apply a higher threshold to the bottom (where we have more resolution, a sharper image, and more noise) and a lower threshold on the top (there, we get less contrast, a weaker detection, and less noise, as the pixels are stretched by the perspective correction, naturally blurring them). We can just interpolate between the thresholds:</p>
			<pre>threshold_up = 15 threshold_down = 60 threshold_delta = threshold_down-threshold_up for y in range(height):  binary_line = binary[y,:]  edge_line = channel_edge[y,:]  threshold_line = threshold_up + threshold_delta * y/height   binary_line[edge_line &gt;= threshold_line] = 255</pre>
			<p>Let's see the result:</p>
			<div><div><img src="img/B16322_03_20.jpg" alt="Figure 3.20 – Result after applying an interpolated threshold, from 15 to 60" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.20 – Result after applying an interpolated threshold, from 15 to 60</p>
			<p>Now, we can have less noise at the bottom and detect weaker signals at the top. However, while a human can visually identify the lanes, for the computer, they are still just pixels in an image, so there is still work to do. But we simplified the image very much, and we are making good progress.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor079"/>Combined threshold</h2>
			<p>As we mentioned <a id="_idIndexMarker232"/>earlier, we also wanted to use the threshold on <a id="_idIndexMarker233"/>another channel, without edge detection. We chose the L channel of HLS.</p>
			<p>This is the result of thresholding above 140:</p>
			<div><div><img src="img/B16322_03_21.jpg" alt="Figure 3.21 – L channel with the threshold above 140" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.21 – L channel with the threshold above 140</p>
			<p>Not bad. Now, we can combine it with the edge:</p>
			<div><div><img src="img/B16322_03_22.jpg" alt="Figure 3.22 – Combination of the two thresholds" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.22 – Combination of the two thresholds</p>
			<p>The result is noisier, but also more robust.</p>
			<p>Before moving forward, let's introduce a picture with a turn:</p>
			<div><div><img src="img/B16322_03_23.jpg" alt="Figure 3.23 – Lane with a turn, from Speed Dreams" width="1024" height="600"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.23 – Lane with a turn, from Speed Dreams</p>
			<p>This is the threshold:</p>
			<div><div><img src="img/B16322_03_24.jpg" alt="Figure 3.24 – Lane with a turn, after the threshold" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.24 – Lane with a turn, after the threshold</p>
			<p>It still looks good, but we can see that, because of the turn, we no longer have a vertical line. In fact, at the<a id="_idIndexMarker234"/> top of the image, the lines are basically <a id="_idIndexMarker235"/>horizontal.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor080"/>Finding the lanes using histograms</h1>
			<p>How could we<a id="_idIndexMarker236"/> understand, more or less, where the lanes are? Visually, for a<a id="_idIndexMarker237"/> human, the answer is simple: the lane is a long line. But what about a computer?</p>
			<p>If we talk about vertical lines, one way could be to count the pixels that are white, on a certain column. But if we check the image with a turn, that might not work. However, if we reduce our attention to the bottom part of the image, the lines are a bit more vertical:</p>
			<div><div><img src="img/B16322_03_25.jpg" alt="Figure 3.25 – Lane with a turn, after the threshold, the bottom part" width="1022" height="38"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.25 – Lane with a turn, after the threshold, the bottom part</p>
			<p>We can now count the pixels by column:</p>
			<pre>partial_img = img[img.shape[0] // 2:, :]  # Select the bottom part
hist = np.sum(partial_img, axis=0)  # axis 0: columns direction</pre>
			<p>To save the histogram as a graph, in a file, we can use Matplotlib:</p>
			<pre>import matplotlib.pyplot as plt
 plt.plot(hist)plt.savefig(filename)plt.clf()</pre>
			<p>We obtain <a id="_idIndexMarker238"/>the <a id="_idIndexMarker239"/>following result:</p>
			<div><div><img src="img/Figure__3_26__Edited.jpg" alt="Figure 3.26 – Left: histogram of a straight lane, right: histogram of a lane with a turn" width="1168" height="414"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.26 – Left: histogram of a straight lane, right: histogram of a lane with a turn</p>
			<p>The X coordinates on the histogram represent the pixels; as our image has a resolution of 1024x600, the histogram shows 1,024 data points, with the peaks centered around the pixels where the lanes are.</p>
			<p>As we can see, in the case of a straight lane, the histogram identifies the two lines quite clearly; with a turn, the histogram is less clear (because the line makes a turn and, therefore, the white pixels are spread a bit around), but it's still usable. We can also see that in the case of a dotted line, the peek in the histogram is less pronounced, but it is still there.</p>
			<p>This looks promising!</p>
			<p>Now, we need a way to detect the two peaks. We can use <code>argmax()</code> from NumPy, which returns the index of the maximum element of an array, which is one of our peaks. However, we need two. For this, we can split the array into two halves, and select one peak on <a id="_idIndexMarker240"/>each <a id="_idIndexMarker241"/>one:</p>
			<pre>size = len(histogram)
max_index_left = np.argmax(histogram[0:size//2])
max_index_right = np.argmax(histogram[size//2:]) + size//2</pre>
			<p>Now we have the indexes, which represent the <em class="italic">X</em> coordinate of the peaks. The value itself (for example, <code>histogram[index]</code>) can be considered the confidence of having identified the lane, as more pixels mean more confidence.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor081"/>The sliding window algorithm</h1>
			<p>While we are making <a id="_idIndexMarker242"/>progress, the image still has some noise, meaning there are pixels that can reduce the precision. In addition, we only know where the line starts.</p>
			<p>The solution is to focus on the area around the line – after all, there is no reason to work on the whole warped image; we could start at the bottom of the line and proceed to "follow it." This is probably one case where an image is worth a thousand words, so this is what we want to achieve:</p>
			<div><div><img src="img/Figure_3.27_UPDATED.jpg" alt="Figure 3.27 – Top: sliding window, bottom: histogram" width="1024" height="291"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.27 – Top: sliding window, bottom: histogram</p>
			<p>On the upper part of <em class="italic">Figure 3.27</em>, each rectangle represents a window of interest. The first window on the bottom of each lane is centered on the respective peak of the histogram. Then, we need a way to "follow the line." The width of each window is dependent on the margin that we want to have, while the height depends on the number of windows that we want to have. These two numbers can be changed to reach a balance between a better detection (reducing the unwanted points and therefore the noise) and the possibility to detect more difficult turns, with a smaller radius (which will require the windows to be repositioned faster).</p>
			<p>As this algorithm requires <a id="_idIndexMarker243"/>quite some code, we will focus on the left lane for clarity, but the same computations need to also be performed for the right lane.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor082"/>Initialization</h2>
			<p>We are only interested in <a id="_idIndexMarker244"/>the pixels that have been selected by the thresholding. We can use <code>nonzero()</code> from NumPy:</p>
			<pre>non_zero = binary_warped.nonzero()
non_zero_y = np.array(non_zero[0])
non_zero_x = np.array(non_zero[1])</pre>
			<p>The <code>non_zero</code> variable will contain the coordinates of the pixels that are white, then <code>non_zero_x</code> will contain the <em class="italic">X</em> coordinates, and <code>non_zero_y</code> the <em class="italic">Y</em> coordinates. </p>
			<p>We also need to set <code>margin</code>, the movement that we are allowing to the lane (for example, half of the window width of the sliding window), and <code>min_pixels</code>, the minimum number of pixels that we want to detect to accept a new position for the sliding window. Below<a id="_idIndexMarker245"/> this threshold, we will not update it:</p>
			<pre>margin = 80 min_pixels = 50 </pre>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor083"/>Coordinates of the sliding windows</h2>
			<p>The <code>left_x</code> variable<a id="_idIndexMarker246"/> will contain the position of the left lane, and we need to initialize it with the value obtained from the histogram.</p>
			<p>After setting the stage, we can now cycle through all the windows, and the variable that we will use as the index is <code>idx_window</code>. The <em class="italic">X</em> range is computed from the last position, adding the margin:</p>
			<pre>win_x_left_min = left_x - margin
win_x_left_max = left_x + margin</pre>
			<p>The <em class="italic">Y</em> range is determined by the index of the window that we are analyzing:</p>
			<pre>win_y_top = img_height - idx_window * window_height win_y_bottom = win_y_top + window_height</pre>
			<p>Now, we need to select the pixels that are white (from <code>non_zero_x</code> and <code>non_zero_y</code>) and constrained in the window that we are analyzing. </p>
			<p>The NumPy array can be filtered using overloaded operators. To count all the <em class="italic">Y</em> coordinates that are above <code>win_y_bottom</code>, we can, therefore, simply use the following expression:</p>
			<pre>non_zero_y &gt;= win_y_bottom</pre>
			<p>The result is an array with <code>True</code> in the pixels selected and <code>False</code> on the other ones.</p>
			<p>But what we need is pixels between <code>win_y_top</code> and <code>win_y_bottom</code>:</p>
			<pre>(non_zero_y &gt;= win_y_bottom) &amp; (non_zero_y &lt; win_y_top)</pre>
			<p>We also need the <em class="italic">X</em> coordinates, which must be between <code>win_x_left_min</code> and<code> win_x_left_max</code>. As we need to just count the points, we can add a <code>nonzero()</code> call:</p>
			<pre>non_zero_left = ((non_zero_y &gt;= win_y_bottom) &amp;                 (non_zero_y &lt; win_y_top) &amp;                  (non_zero_x &gt;= win_x_left_min) &amp; 
                 (non_zero_x &lt; win_x_left_max)).nonzero()[0]</pre>
			<p>We need to select the first element because our array is inside another array of one single element.</p>
			<p>We will also keep all these values in a variable, to draw the line above the lane later:</p>
			<pre>left_lane_indexes.append(non_zero_left)</pre>
			<p>Now, we just need to <a id="_idIndexMarker247"/>update the left lane position with the average of the positions, but only if there are enough points:</p>
			<pre>if len(non_zero_left) &gt; min_pixels:
    left_x = np.int(np.mean(non_zero_x[non_zero_left]))</pre>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor084"/>Polynomial fitting</h2>
			<p>Now, we have <a id="_idIndexMarker248"/>potentially selected thousands of points, but we need to make sense of them and obtain a line. For this, we can use <code>polyfit()</code>, a method that can approximate a series of points with a polynomial of the specified degree; a second-degree polynomial will be enough for us:</p>
			<pre>x_coords = non_zero_x[left_lane_indexes]
y_coords = non_zero_y[left_lane_indexes]
left_fit = np.polynomial.polynomial.polyfit(y_coords, x_coords, 2)</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Please notice that <code>polyfit()</code> accepts the parameters in the order <code>(X, Y)</code>, while we provide them in the order <code>(Y, X)</code>. We do so because by mathematical convention, in a polynomial, <em class="italic">X</em> is known and <em class="italic">Y</em> is computed based on <em class="italic">X</em> (for example, <em class="italic">Y = X^2 + 3*X+5</em>). However, we know <em class="italic">Y</em> and we need to compute <em class="italic">X</em>, so we need to provide them in the opposite order.</p>
			<p>We are almost done.</p>
			<p>The <em class="italic">Y</em> coordinates are simply a range:</p>
			<pre>ploty = np.array([float(x) for x in range(binary_warped.shape[0])])</pre>
			<p>Then, we need to compute <em class="italic">X</em> from <em class="italic">Y</em>, using the generic formula for a polynomial of the second degree (reversed on <em class="italic">X</em> and <em class="italic">Y</em>): </p>
			<p><em class="italic">x = Ay^2 + By + C;</em></p>
			<p>This is the code:</p>
			<pre>Left_fitx = left_fit[2] * ploty ** 2 + left_fit[1] * ploty + left_fit[0]</pre>
			<p>This is where we are now:</p>
			<div><div><img src="img/B16322_03_28.jpg" alt="Figure 3.28 – Lanes drawn on the warped image" width="1024" height="115"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.28 – Lanes drawn on the warped image</p>
			<p>We can now <a id="_idIndexMarker249"/>call <code>perspectiveTransform()</code> with the inverse perspective transformation to move the pixels to their position in the image. This is the final result:</p>
			<div><div><img src="img/B16322_03_29.jpg" alt="Figure 3.29 – Lane detected on the image" width="1024" height="600"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.29 – Lane detected on the image</p>
			<p>Congratulations! It has not been particularly easy, but you can now detect a lane on a frame, under the correct conditions. Unfortunately, not all the frames will be good enough for<a id="_idIndexMarker250"/> this. Let's see in the next section how we can use the temporal evolution of the video stream to filter the data and improve the precision.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor085"/>Enhancing a video</h1>
			<p>Analyzing a video stream <a id="_idIndexMarker251"/>in real time can be a challenge from a computational point of view, but usually, it offers the possibility to improve precision, as we can build on knowledge from the previous frames and filter the result.</p>
			<p>We will now see two techniques that can be used to detect lanes with better precision when working with video streams.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor086"/>Partial histogram</h2>
			<p>If we assume that we<a id="_idIndexMarker252"/> correctly detected a lane in the previous few frames, then the lane on the current frame should be in a similar position. This assumption is affected by the speed of the car and the frame rate of the camera: the faster the car, the more the lane could change. Conversely, the faster the camera, the less the lane could have moved between two frames. In a real self-driving car, both these values are known, so they can be taken into consideration if required.</p>
			<p>From a practical point of view, this means we can limit the part of the histogram that we analyze, to avoid false detections, analyzing only some histogram pixels (for example, 30) around the<a id="_idIndexMarker253"/> average of some of the previous frames.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor087"/>Rolling average</h1>
			<p>The main result of <a id="_idIndexMarker254"/>our detection is the three values of the polynomial fit, for each lane. Following the same principle of the previous section, we can deduce that they cannot change much between frames, so we could consider the average of some of the previous frames, to reduce noise.</p>
			<p>There is a technique<a id="_idIndexMarker255"/> called the <strong class="bold">exponentially weighted moving average</strong> (or rolling average), which can be used to easily compute an approximate average on some of the last values of a stream of values.</p>
			<p>Given <code>beta</code>, a parameter greater than zero and typically close to one, the moving average can be computed like this:</p>
			<pre>moving_average = beta * prev_average + (1-beta)*new_value</pre>
			<p>As an indication, the number of frames that most affect the average is given by the following:</p>
			<pre>1 / (1 - beta)</pre>
			<p>So, <code>beta = 0.9</code> would average 10 frames, and <code>beta = 0.95</code> would average 20 frames.</p>
			<p>This concludes the chapter. I invite you to check the full code on GitHub and to play around with it. You can find some real-life footage and try to identify the lanes there.</p>
			<p>And don't forget to <a id="_idIndexMarker256"/>apply the <strong class="bold">camera calibration</strong>, if you can.</p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor088"/>Summary</h1>
			<p>In this chapter, we built a nice pipeline to detect lanes. First, we analyzed different color spaces, such as RGB, HLS, and HSV, to see which channels would be more useful to detect lanes. Then, we used perspective correction, with <code>getPerspectiveTransform()</code>, to obtain a <em class="italic">bird's eye view</em> and make parallel lines on the road also look parallel on the image we analyzed.</p>
			<p>We used edge detection with <code>Scharr()</code> to detect edges and make our analysis more robust than using only a color threshold, and we combined the two. We then computed a histogram to detect where the lanes start, and we used the "sliding window" technique to "follow" the lane in the image.</p>
			<p>Then, we used <code>polyfit()</code> to fit a second-order polynomial on the pixels detected, making sense of them, and we used the coefficients returned by the function to generate our curve, after having applied reverse perspective correction on them. Finally, we discussed two techniques that can be applied to a video stream to improve the precision: partial histogram and rolling average.</p>
			<p>Using all these techniques together, you can now build a pipeline that can detect the lanes on a road.</p>
			<p>In the next chapter, we will introduce deep learning and neural networks, powerful tools that we can use to accomplish even more complex computer vision tasks.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor089"/>Questions</h1>
			<ol>
				<li>Can you name some color spaces, other than RGB?</li>
				<li>Why do we apply perspective correction?</li>
				<li>How can we detect where the lane starts?</li>
				<li>Which technique can you use to <em class="italic">follow the lane</em> in the image?</li>
				<li>If you have many points forming more or less a lane, how can you convert them into a line?</li>
				<li>Which function can you use for edge detection?</li>
				<li>What can you use to compute the average of the last <em class="italic">N</em> positions of the lane?</li>
			</ol>
		</div>
	</div>



  </body></html>