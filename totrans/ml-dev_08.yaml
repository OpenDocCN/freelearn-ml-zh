- en: Recent Models and Developments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we have explored a large number of training mechanisms
    for machine learning models, starting with simple pass-through mechanisms, such
    as the well-known feedforward neural networks. Then we looked at a more complex
    and reality-bound mechanism, accepting a determined sequence of inputs as the
    training input, with **Recurrent Neural Networks** (**RNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to take a look at two recent players that incorporate other aspects
    of the real world. In the first case, we will have not only a single network optimizing
    its model, but also another participant, and they will both improve each other's
    results. This is the case of **Generative Adversarial Networks** (**GANs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second case, we will talk about a different kind of model, which will
    try to determine the optimal set of steps to maximize a reward: **reinforcement
    learning**.'
  prefs: []
  type: TYPE_NORMAL
- en: GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs are a new kind of unsupervised learning model, one of the very few disrupting
    models of the last decade. They have two models competing with and improving each
    other throughout the iterations.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture was originally based on supervised learning and game theory,
    and its main objective is to basically learn to generate realistic samples from
    an original dataset of elements of the same class.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s worth noting that the amount of research on GANs is increasing at an
    almost exponential rate, as depicted in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35e6af3d-756a-470f-a9d2-0f7fb907fa3f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: The GAN Zoo (https://github.com/hindupuravinash/the-gan-zoo)'
  prefs: []
  type: TYPE_NORMAL
- en: Types of GAN applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs allow new applications to produce new samples from a previous set of samples,
    including completing missing information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we depict a number of samples created with the **LSGAN
    architecture** on five scene datasets from LSUN, including a kitchen, church,
    dining room, and conference room:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6289b53f-de4e-4cd8-9116-52c6e6dd56e0.png)'
  prefs: []
  type: TYPE_IMG
- en: LSGAN created models
  prefs: []
  type: TYPE_NORMAL
- en: Another really interesting example is class-conditional image sampling using
    the **Plug and Play Generative Network (PPGN)** to fill in 100 x 100 missing pixels
    in a real 227 x 227 image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot compares PPGN variations and the equivalent Photoshop
    image completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b3e6c6c-b1df-4a67-9afa-a8869db14d78.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PPGN infilling example
  prefs: []
  type: TYPE_NORMAL
- en: 'The PPGN can also generate images synthetically at a high resolution (227 x
    227) for the volcano class. Not only are many images nearly photo-realistic, but
    the samples within this class are diverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e1b41d5-17e8-4dac-9aa0-fc13daa0b726.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PPGN generated volcano samples
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot illustrates the process vector arithmetic for visual
    concepts. It allows the use of operands representing objects in the image and
    can add or remove them, moving in a feature space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/360b4d4c-7ea5-4df5-b87d-f2d8f081c991.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector arithmetic operations for the feature space
  prefs: []
  type: TYPE_NORMAL
- en: Discriminative and generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the concept of adversarial networks, we will first define the
    two models that interact in a typical GAN setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generator**: This is tasked with taking in a sample from a standard random
    distribution (for example, a sample from an n-dimensional Gaussian) and producing
    a point that looks sort of like it could come from the same distribution as *XX*. It
    could be said that the generator wants to fool the discriminator to output 1\.
    Mathematically, it learns a function that maps the input data (*x*) to some desired
    output class label (*y*). In probabilistic terms, it learns the conditional distribution *P(y|x)* of
    the input data. It discriminates between two (or more) different classes of data—for
    example, a convolutional neural network that is trained to output 1 given an image
    of a human face and 0 otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discriminator**: This is tasked with discriminating between samples from
    the true data and the artificial data generated by the generator. Each model will
    try to beat the other (the generator''s objective is to fool the discriminator
    and the discriminator''s objective is to not be fooled by the generator):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d5b720c1-e3b6-4fea-9c47-d20cd6587eee.png)'
  prefs: []
  type: TYPE_IMG
- en: Training process for a GAN
  prefs: []
  type: TYPE_NORMAL
- en: More formally, the generator tries to learn the joint probability of the input
    data and labels simultaneously, that is, *P(x, y)*. So, it can be used for something
    else as well, such as creating likely new *(x, y)* samples. It doesn't know anything
    about classes of data. Instead, its purpose is to generate new data that fits
    the distribution of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: In the general case, both the generator and the discriminator are neural networks
    and they are trained in an alternating manner. Each of their objectives can be
    expressed as a loss function that we can optimize via gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: The final result is that both models improve themselves in tandem; the generator
    produces better images, and the discriminator gets better at determining whether
    the generated sample is fake. In practice, the final result is a model that produces
    really good and realistic new samples (for example, random pictures of a natural
    environment).
  prefs: []
  type: TYPE_NORMAL
- en: '**Summarizing, the main takeaways from GANs are:**'
  prefs: []
  type: TYPE_NORMAL
- en: GANs are generative models that use supervised learning to approximate an intractable
    cost function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs can simulate many cost functions, including the one used for maximum likelihood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the Nash equilibrium in high-dimensional, continuous, non-convex games
    is an important open research problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs are a key ingredient of PPGNs, which are able to generate compelling high-resolution
    samples from diverse image classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a field that has resurfaced recently, and it has become
    more popular in the fields of control, finding the solutions to games and situational
    problems, where a number of steps have to be implemented to solve a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'A formal definition of reinforcement learning is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Reinforcement learning is the problem faced by an agent that must learn behavior
    through trial-and-error interactions with a dynamic environment.” (Kaelbling et
    al. 1996).'
  prefs: []
  type: TYPE_NORMAL
- en: In order to have a reference frame for the type of problem we want to solve,
    we will start by going back to a mathematical concept developed in the 1950s,
    called the **Markov decision process**.
  prefs: []
  type: TYPE_NORMAL
- en: Markov decision process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before explaining reinforcement learning techniques, we will explain the type
    of problem we will attack with them.
  prefs: []
  type: TYPE_NORMAL
- en: When talking about reinforcement learning, we want to optimize the problem of
    a Markov decision process. It consists of a mathematical model that aids decision
    making in situations where the outcomes are in part random, and in part under
    the control of an agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main elements of this model are an **Agent**, an **Environment**, and a
    **State**, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2335ec23-3edc-413c-98a7-c9c96d4bedda.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified scheme of a reinforcement learning process
  prefs: []
  type: TYPE_NORMAL
- en: The agent can perform certain actions (such as moving the paddle left or right).
    These actions can sometimes result in a reward *r[t]*, which can be positive or
    negative (such as an increase or decrease in the score). Actions change the environment
    and can lead to a new state *s[t+1]*, where the agent can perform another action *a[t+1]*.
    The set of states, actions, and rewards, together with the rules for transitioning
    from one state to another, make up a Markov decision process.
  prefs: []
  type: TYPE_NORMAL
- en: Decision elements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the problem, let''s situate ourselves in the problem solving
    environment and look at the main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: The set of states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The action to take is to go from one place to another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward function is the value represented by the edge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The policy is the way to complete the task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discount factor, which determines the importance of future rewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main difference with traditional forms of supervised and unsupervised learning
    is the time taken to calculate the reward, which in reinforcement learning is
    not instantaneous; it comes after a set of steps.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the next state depends on the current state and the decision maker's action,
    and the state is not dependent on all the previous states (it doesn't have memory),
    thus it complies with the Markov property.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this is a Markov decision process, the probability of state *s[t+1]* depends
    only on the current state *s[t]* and action *a[t]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a39fd08-d2bc-4440-a22c-85a754637b0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Unrolled reinforcement mechanism
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the whole process is to generate a policy *P,* that maximizes rewards.
    The training samples are tuples, *<s, a, r>*.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the Markov process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reinforcement learning is an iterative interaction between an agent and the
    environment. The following occurs at each timestep:'
  prefs: []
  type: TYPE_NORMAL
- en: The process is in a state and the decision-maker may choose any action that
    is available in that state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process responds at the next timestep by randomly moving into a new state
    and giving the decision-maker a corresponding reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability that the process moves into its new state is influenced by the
    chosen action in the form of a state transition function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Basic RL techniques: Q-learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most well-known reinforcement learning techniques, and the one we
    will be implementing in our example, is **Q-learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning can be used to find an optimal action for any given state in a finite
    Markov decision process. Q-learning tries to maximize the value of the Q-function
    that represents the maximum discounted future reward when we perform action *a* in
    state *s*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we know the Q-function, the optimal action *a* in state *s* is the one
    with the highest Q-value. We can then define a policy π*(s),* that gives us the
    optimal action in any state, expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af84694d-ce12-4ccd-9df8-c4ea163fe2c9.png)'
  prefs: []
  type: TYPE_IMG
- en: We can define the Q-function for a transition point (*s[t], a[t], r[t], s[t+1]*)
    in terms of the Q-function at the next point (*s[t+1]*, *a[t+1]*, *r[t+1]*, *s[t+2]*),
    similar to what we did with the total discounted future reward. This equation
    is known as the **Bellman equation for Q-learning:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0153a9f1-4dc4-4b03-8b8a-108c8090f0c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In practice, we  can think of the Q-function as a lookup table (called a **Q-table**)
    where the states (denoted by *s*) are rows and the actions (denoted by *a*) are
    columns, and the elements (denoted by *Q(s, a)*) are the rewards that you get
    if you are in the state given by the row and take the action given by the column.
    The best action to take at any state is the one with the highest reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You will realize that the algorithm is basically doing stochastic gradient descent
    on the Bellman equation, backpropagating the reward through the state space (or
    episode) and averaging over many trials (or epochs). Here, `α` is the learning
    rate that determines how much of the difference between the previous Q-value and
    the discounted new maximum Q-value should be incorporated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent this process with the following flowchart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/deb6d7a0-4377-43e1-bfea-b9b75915f19a.png)'
  prefs: []
  type: TYPE_IMG
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bellman, Richard, *A Markovian decision process.* Journal of Mathematics and
    Mechanics (1957): 679-684.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaelbling, Leslie Pack, Michael L. Littman, and Andrew W. Moore, *Reinforcement
    learning: A survey.* Journal of artificial intelligence research 4 (1996): 237-285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow, Ian, et al., *Generative adversarial nets, a**dvances in neural
    information processing systems,* 2014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford, Alec, Luke Metz, and Soumith Chintala, *Unsupervised representation
    learning with deep convolutional generative adversarial networks.* arXiv preprint
    arXiv:1511.06434 (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isola, Phillip, et al., *Image-to-image translation with conditional adversarial
    networks*, arXiv preprint arXiv:1611.07004 (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao, Xudong, et al., *Least squares generative adversarial networks.* arXiv
    preprint ArXiv:1611.04076 (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eghbal-Zadeh, Hamid, and Gerhard Widmer, *Likelihood Estimation for Generative
    Adversarial Networks.* arXiv preprint arXiv:1707.07530 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen, Anh, et al., *Plug & play generative networks: Conditional iterative
    generation of images in latent space*. arXiv preprint arXiv:1612.00005 (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have reviewed two of the most important and innovative architectures
    that have appeared in recent. Every day, new generative and reinforcement models
    are applied in innovative ways, whether to generate feasible new elements from
    a selection of previously known classes or even to win against professional players
    in strategy games.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will provide precise instructions so you can use and
    modify the code provided to better understand the different concepts you have
    acquired throughout the book.
  prefs: []
  type: TYPE_NORMAL
