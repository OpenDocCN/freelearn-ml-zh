- en: Recent Models and Developments
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 近期模型和发展
- en: In the previous chapters, we have explored a large number of training mechanisms
    for machine learning models, starting with simple pass-through mechanisms, such
    as the well-known feedforward neural networks. Then we looked at a more complex
    and reality-bound mechanism, accepting a determined sequence of inputs as the
    training input, with **Recurrent Neural Networks** (**RNNs**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们探讨了机器学习模型的大量训练机制，从简单的传递机制开始，如众所周知的前馈神经网络。然后我们查看了一个更复杂且与现实世界相关的机制，接受一个确定的输入序列作为训练输入，即**循环神经网络**（**RNNs**）。
- en: Now it's time to take a look at two recent players that incorporate other aspects
    of the real world. In the first case, we will have not only a single network optimizing
    its model, but also another participant, and they will both improve each other's
    results. This is the case of **Generative Adversarial Networks** (**GANs**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看两个最近结合了现实世界其他方面的参与者了。在第一种情况下，我们不仅有一个优化其模型的单一网络，还有一个参与者，他们将相互改进各自的结果。这是**生成对抗网络**（**GANs**）的情况。
- en: 'In the second case, we will talk about a different kind of model, which will
    try to determine the optimal set of steps to maximize a reward: **reinforcement
    learning**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，我们将讨论一种不同类型的模型，它将尝试确定最大化奖励的最佳步骤集：**强化学习**。
- en: GANs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANs
- en: GANs are a new kind of unsupervised learning model, one of the very few disrupting
    models of the last decade. They have two models competing with and improving each
    other throughout the iterations.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是一种新的无监督学习模型，是过去十年中为数不多的颠覆性模型之一。它们有两个模型在迭代过程中相互竞争和改进。
- en: This architecture was originally based on supervised learning and game theory,
    and its main objective is to basically learn to generate realistic samples from
    an original dataset of elements of the same class.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构最初基于监督学习和博弈论，其主要目标是基本学会从同一类别的原始数据集中生成真实样本。
- en: 'It''s worth noting that the amount of research on GANs is increasing at an
    almost exponential rate, as depicted in the following graph:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，关于GAN的研究数量几乎呈指数级增长，如下面的图表所示：
- en: '![](img/35e6af3d-756a-470f-a9d2-0f7fb907fa3f.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35e6af3d-756a-470f-a9d2-0f7fb907fa3f.jpg）'
- en: 'Source: The GAN Zoo (https://github.com/hindupuravinash/the-gan-zoo)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：GAN动物园（https://github.com/hindupuravinash/the-gan-zoo）
- en: Types of GAN applications
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN应用类型
- en: GANs allow new applications to produce new samples from a previous set of samples,
    including completing missing information.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: GANs允许从先前的一组样本中产生新的样本，包括完成缺失信息。
- en: 'In the following screenshot, we depict a number of samples created with the **LSGAN
    architecture** on five scene datasets from LSUN, including a kitchen, church,
    dining room, and conference room:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图中，我们展示了使用**LSGAN架构**在LSUN的五个场景数据集上创建的多个样本，包括厨房、教堂、餐厅和会议室：
- en: '![](img/6289b53f-de4e-4cd8-9116-52c6e6dd56e0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6289b53f-de4e-4cd8-9116-52c6e6dd56e0.png)'
- en: LSGAN created models
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LSGAN创建的模型
- en: Another really interesting example is class-conditional image sampling using
    the **Plug and Play Generative Network (PPGN)** to fill in 100 x 100 missing pixels
    in a real 227 x 227 image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常有趣的例子是使用**即插即用生成网络（PPGN**）进行类条件图像采样，以填充真实227 x 227图像中的100 x 100缺失像素。
- en: 'The following screenshot compares PPGN variations and the equivalent Photoshop
    image completion:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图比较了PPGN变体和等效的Photoshop图像完成效果：
- en: '![](img/8b3e6c6c-b1df-4a67-9afa-a8869db14d78.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b3e6c6c-b1df-4a67-9afa-a8869db14d78.jpg）'
- en: PPGN infilling example
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PPGN填充示例
- en: 'The PPGN can also generate images synthetically at a high resolution (227 x
    227) for the volcano class. Not only are many images nearly photo-realistic, but
    the samples within this class are diverse:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PPGN还可以为火山类别以高分辨率（227 x 227）合成图像。不仅许多图像几乎达到照片级真实感，而且该类别的样本种类繁多：
- en: '![](img/7e1b41d5-17e8-4dac-9aa0-fc13daa0b726.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e1b41d5-17e8-4dac-9aa0-fc13daa0b726.jpg)'
- en: PPGN generated volcano samples
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PPGN生成的火山样本
- en: 'The following screenshot illustrates the process vector arithmetic for visual
    concepts. It allows the use of operands representing objects in the image and
    can add or remove them, moving in a feature space:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图说明了视觉概念的向量算术过程。它允许使用代表图像中对象的操作数，并可以在特征空间中添加或删除它们：
- en: '![](img/360b4d4c-7ea5-4df5-b87d-f2d8f081c991.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/360b4d4c-7ea5-4df5-b87d-f2d8f081c991.png）'
- en: Vector arithmetic operations for the feature space
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 特征空间的向量算术操作
- en: Discriminative and generative models
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别性和生成性模型
- en: 'To understand the concept of adversarial networks, we will first define the
    two models that interact in a typical GAN setup:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解对抗网络的概念，我们首先定义在典型的GAN设置中相互作用的两个模型：
- en: '**Generator**: This is tasked with taking in a sample from a standard random
    distribution (for example, a sample from an n-dimensional Gaussian) and producing
    a point that looks sort of like it could come from the same distribution as *XX*. It
    could be said that the generator wants to fool the discriminator to output 1\.
    Mathematically, it learns a function that maps the input data (*x*) to some desired
    output class label (*y*). In probabilistic terms, it learns the conditional distribution *P(y|x)* of
    the input data. It discriminates between two (or more) different classes of data—for
    example, a convolutional neural network that is trained to output 1 given an image
    of a human face and 0 otherwise.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器**：这个模型的任务是接受来自标准随机分布（例如，来自n维高斯分布的样本）的样本，并产生一个看起来可能来自与*XX*相同分布的点。可以说，生成器想要欺骗判别器输出1。从数学上讲，它学习一个将输入数据(*x*)映射到某些期望输出类别标签(*y*)的函数。在概率论术语中，它学习输入数据的条件分布*P(y|x*)。它区分两个（或更多）不同的数据类别——例如，一个训练后输出1的卷积神经网络，给定人脸图像时输出1，否则输出0。'
- en: '**Discriminator**: This is tasked with discriminating between samples from
    the true data and the artificial data generated by the generator. Each model will
    try to beat the other (the generator''s objective is to fool the discriminator
    and the discriminator''s objective is to not be fooled by the generator):'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**判别器**：这个模型的任务是区分来自真实数据和生成器生成的虚假数据。每个模型都会试图打败对方（生成器的目标是欺骗判别器，判别器的目标是不要被生成器欺骗）：'
- en: '![](img/d5b720c1-e3b6-4fea-9c47-d20cd6587eee.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d5b720c1-e3b6-4fea-9c47-d20cd6587eee.png)'
- en: Training process for a GAN
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的训练过程
- en: More formally, the generator tries to learn the joint probability of the input
    data and labels simultaneously, that is, *P(x, y)*. So, it can be used for something
    else as well, such as creating likely new *(x, y)* samples. It doesn't know anything
    about classes of data. Instead, its purpose is to generate new data that fits
    the distribution of the training data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，生成器试图同时学习输入数据和标签的联合概率，即*P(x, y)*。因此，它也可以用于其他目的，例如创建可能的新*(x, y)*样本。它对数据的类别一无所知。相反，它的目的是生成符合训练数据分布的新数据。
- en: In the general case, both the generator and the discriminator are neural networks
    and they are trained in an alternating manner. Each of their objectives can be
    expressed as a loss function that we can optimize via gradient descent.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般情况下，生成器和判别器都是神经网络，并且它们交替训练。它们各自的目标可以表示为一个我们可以通过梯度下降进行优化的损失函数。
- en: The final result is that both models improve themselves in tandem; the generator
    produces better images, and the discriminator gets better at determining whether
    the generated sample is fake. In practice, the final result is a model that produces
    really good and realistic new samples (for example, random pictures of a natural
    environment).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是两个模型同时改进；生成器产生更好的图像，判别器在判断生成的样本是否为假样本方面变得更好。在实践中，最终结果是产生真正优秀和逼真的新样本（例如，自然环境的随机图片）的模型。
- en: '**Summarizing, the main takeaways from GANs are:**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结，GANs的主要收获是：**'
- en: GANs are generative models that use supervised learning to approximate an intractable
    cost function
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs是使用监督学习来近似不可处理的成本函数的生成模型
- en: GANs can simulate many cost functions, including the one used for maximum likelihood
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs可以模拟许多成本函数，包括用于最大似然估计的那个
- en: Finding the Nash equilibrium in high-dimensional, continuous, non-convex games
    is an important open research problem
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高维、连续、非凸游戏中找到纳什均衡是一个重要的未解研究问题
- en: GANs are a key ingredient of PPGNs, which are able to generate compelling high-resolution
    samples from diverse image classes
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs是PPGNs的关键组成部分，能够从不同的图像类别生成引人注目的高分辨率样本
- en: Reinforcement learning
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning is a field that has resurfaced recently, and it has become
    more popular in the fields of control, finding the solutions to games and situational
    problems, where a number of steps have to be implemented to solve a problem.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一个最近重新出现的领域，它在控制、寻找游戏和情境问题的解决方案等领域变得更加流行，在这些领域中，需要执行多个步骤来解决一个问题。
- en: 'A formal definition of reinforcement learning is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个正式定义如下：
- en: '"Reinforcement learning is the problem faced by an agent that must learn behavior
    through trial-and-error interactions with a dynamic environment.” (Kaelbling et
    al. 1996).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: “强化学习是代理必须通过与动态环境进行试错交互来学习行为所面临的问题。”（Kaelbling 等人，1996年）。
- en: In order to have a reference frame for the type of problem we want to solve,
    we will start by going back to a mathematical concept developed in the 1950s,
    called the **Markov decision process**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有一个参考框架来解决问题类型，我们将首先回到20世纪50年代发展起来的一个数学概念，称为**马尔可夫决策过程**。
- en: Markov decision process
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: Before explaining reinforcement learning techniques, we will explain the type
    of problem we will attack with them.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释强化学习技术之前，我们将解释我们将用它们解决的问题类型。
- en: When talking about reinforcement learning, we want to optimize the problem of
    a Markov decision process. It consists of a mathematical model that aids decision
    making in situations where the outcomes are in part random, and in part under
    the control of an agent.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈论强化学习时，我们想要优化马尔可夫决策过程的问题。它由一个数学模型组成，有助于在部分结果随机、部分受代理控制的决策情境中进行决策。
- en: 'The main elements of this model are an **Agent**, an **Environment**, and a
    **State**, as shown in the following diagram:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的主要元素是一个**代理**、一个**环境**和一个**状态**，如下面的图所示：
- en: '![](img/2335ec23-3edc-413c-98a7-c9c96d4bedda.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2335ec23-3edc-413c-98a7-c9c96d4bedda.png)'
- en: Simplified scheme of a reinforcement learning process
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习过程的简化方案
- en: The agent can perform certain actions (such as moving the paddle left or right).
    These actions can sometimes result in a reward *r[t]*, which can be positive or
    negative (such as an increase or decrease in the score). Actions change the environment
    and can lead to a new state *s[t+1]*, where the agent can perform another action *a[t+1]*.
    The set of states, actions, and rewards, together with the rules for transitioning
    from one state to another, make up a Markov decision process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以执行某些动作（例如将桨向左或向右移动）。这些动作有时会导致奖励 *r[t]*，这可以是正的或负的（例如得分的增加或减少）。动作会改变环境，并可能导致新的状态
    *s[t+1]*，其中代理可以执行另一个动作 *a[t+1]*。状态、动作和奖励的集合，以及从一个状态转换到另一个状态的规则，共同构成了马尔可夫决策过程。
- en: Decision elements
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策元素
- en: 'To understand the problem, let''s situate ourselves in the problem solving
    environment and look at the main elements:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个问题，让我们将自己置于问题解决环境中，并查看主要元素：
- en: The set of states
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态的集合
- en: The action to take is to go from one place to another
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要采取的动作是从一个地方移动到另一个地方
- en: The reward function is the value represented by the edge
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励函数是表示边的值
- en: The policy is the way to complete the task
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略是完成任务的方式
- en: A discount factor, which determines the importance of future rewards
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 折扣因子，它决定了未来奖励的重要性
- en: The main difference with traditional forms of supervised and unsupervised learning
    is the time taken to calculate the reward, which in reinforcement learning is
    not instantaneous; it comes after a set of steps.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的监督学习和无监督学习形式相比，主要区别在于计算奖励所需的时间，在强化学习中不是瞬时的；它是在一系列步骤之后到来的。
- en: Thus, the next state depends on the current state and the decision maker's action,
    and the state is not dependent on all the previous states (it doesn't have memory),
    thus it complies with the Markov property.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，下一个状态取决于当前状态和决策者的动作，状态不依赖于所有先前状态（它没有记忆），因此它符合马尔可夫属性。
- en: 'Since this is a Markov decision process, the probability of state *s[t+1]* depends
    only on the current state *s[t]* and action *a[t]*:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个马尔可夫决策过程，状态 *s[t+1]* 的概率只取决于当前状态 *s[t]* 和动作 *a[t]*：
- en: '![](img/3a39fd08-d2bc-4440-a22c-85a754637b0d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a39fd08-d2bc-4440-a22c-85a754637b0d.png)'
- en: Unrolled reinforcement mechanism
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 展开式强化机制
- en: The goal of the whole process is to generate a policy *P,* that maximizes rewards.
    The training samples are tuples, *<s, a, r>*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程的目标是生成一个策略 *P*，以最大化奖励。训练样本是元组，*<s, a, r>*。
- en: Optimizing the Markov process
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化马尔可夫过程
- en: 'Reinforcement learning is an iterative interaction between an agent and the
    environment. The following occurs at each timestep:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是代理与环境之间的迭代交互。在每个时间步发生以下情况：
- en: The process is in a state and the decision-maker may choose any action that
    is available in that state
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处于某个状态时，决策者可以选择在该状态下可用的任何动作
- en: The process responds at the next timestep by randomly moving into a new state
    and giving the decision-maker a corresponding reward
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下一个时间步，过程通过随机移动到新状态并给予决策者相应的奖励来响应
- en: The probability that the process moves into its new state is influenced by the
    chosen action in the form of a state transition function
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过程进入其新状态的概率受所选动作的影响，这种影响以状态转换函数的形式出现
- en: 'Basic RL techniques: Q-learning'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本RL技术：Q学习
- en: One of the most well-known reinforcement learning techniques, and the one we
    will be implementing in our example, is **Q-learning**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的强化学习技术之一，也是我们将在示例中实现的技术，是**Q学习**。
- en: Q-learning can be used to find an optimal action for any given state in a finite
    Markov decision process. Q-learning tries to maximize the value of the Q-function
    that represents the maximum discounted future reward when we perform action *a* in
    state *s*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习可用于在有限马尔可夫决策过程中找到任何给定状态的最佳动作。Q学习试图最大化表示在状态*s*执行动作*a*时最大折现未来奖励的Q函数的值。
- en: 'Once we know the Q-function, the optimal action *a* in state *s* is the one
    with the highest Q-value. We can then define a policy π*(s),* that gives us the
    optimal action in any state, expressed as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道了Q函数，状态*s*中的最佳动作*a*就是具有最高Q值的动作。然后我们可以定义一个策略π*(s)*，它给出任何状态的最佳动作，如下所示：
- en: '![](img/af84694d-ce12-4ccd-9df8-c4ea163fe2c9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af84694d-ce12-4ccd-9df8-c4ea163fe2c9.png)'
- en: We can define the Q-function for a transition point (*s[t], a[t], r[t], s[t+1]*)
    in terms of the Q-function at the next point (*s[t+1]*, *a[t+1]*, *r[t+1]*, *s[t+2]*),
    similar to what we did with the total discounted future reward. This equation
    is known as the **Bellman equation for Q-learning:**
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用下一个点的Q函数（*s[t+1]*, *a[t+1]*, *r[t+1]*, *s[t+2]*)来定义转换点（*s[t]*, *a[t]*,
    *r[t]*, *s[t+1]*）的Q函数，类似于我们处理总折现未来奖励的方式。这个方程被称为**Q学习的贝尔曼方程**：
- en: '![](img/0153a9f1-4dc4-4b03-8b8a-108c8090f0c3.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0153a9f1-4dc4-4b03-8b8a-108c8090f0c3.png)'
- en: 'In practice, we  can think of the Q-function as a lookup table (called a **Q-table**)
    where the states (denoted by *s*) are rows and the actions (denoted by *a*) are
    columns, and the elements (denoted by *Q(s, a)*) are the rewards that you get
    if you are in the state given by the row and take the action given by the column.
    The best action to take at any state is the one with the highest reward:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们可以将Q函数视为一个查找表（称为**Q表**），其中状态（用*s*表示）是行，动作（用*a*表示）是列，而元素（用*Q(s, a)*表示）是在给定行的状态下采取给定列的动作所获得的奖励。在任何状态下采取的最佳动作是具有最高奖励的动作：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will realize that the algorithm is basically doing stochastic gradient descent
    on the Bellman equation, backpropagating the reward through the state space (or
    episode) and averaging over many trials (or epochs). Here, `α` is the learning
    rate that determines how much of the difference between the previous Q-value and
    the discounted new maximum Q-value should be incorporated.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你会意识到该算法基本上是在贝尔曼方程上进行随机梯度下降，通过状态空间（或剧集）回传奖励，并在许多试验（或时代）上平均。在这里，`α`是学习率，它决定了先前Q值和折现后的新最大Q值之间差异的多少应该被纳入。
- en: 'We can represent this process with the following flowchart:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下流程图来表示这个过程：
- en: '![](img/deb6d7a0-4377-43e1-bfea-b9b75915f19a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/deb6d7a0-4377-43e1-bfea-b9b75915f19a.png)'
- en: References
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bellman, Richard, *A Markovian decision process.* Journal of Mathematics and
    Mechanics (1957): 679-684.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝尔曼，理查德，*马尔可夫决策过程*。数学与力学杂志（1957年）：679-684。
- en: 'Kaelbling, Leslie Pack, Michael L. Littman, and Andrew W. Moore, *Reinforcement
    learning: A survey.* Journal of artificial intelligence research 4 (1996): 237-285.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凯尔布林，莱斯利·帕克，迈克尔·L·利特曼，和安德鲁·W·摩尔，*强化学习：综述*。人工智能研究杂志4（1996年）：237-285。
- en: Goodfellow, Ian, et al., *Generative adversarial nets, a**dvances in neural
    information processing systems,* 2014
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 古德费洛，伊恩，等人，*生成对抗网络，神经信息处理系统进展*，2014年
- en: Radford, Alec, Luke Metz, and Soumith Chintala, *Unsupervised representation
    learning with deep convolutional generative adversarial networks.* arXiv preprint
    arXiv:1511.06434 (2015).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉德福德，亚历克，卢克·梅茨，和索乌米特·奇塔拉，*使用深度卷积生成对抗网络进行无监督表示学习*。arXiv预印本arXiv:1511.06434（2015年）。
- en: Isola, Phillip, et al., *Image-to-image translation with conditional adversarial
    networks*, arXiv preprint arXiv:1611.07004 (2016).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伊索拉，菲利普，等人，*条件对抗网络进行图像到图像的翻译*。arXiv预印本arXiv:1611.07004（2016年）。
- en: Mao, Xudong, et al., *Least squares generative adversarial networks.* arXiv
    preprint ArXiv:1611.04076 (2016).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao, Xudong, et al., *最小二乘生成对抗网络*。arXiv预印本 ArXiv:1611.04076 (2016)。
- en: Eghbal-Zadeh, Hamid, and Gerhard Widmer, *Likelihood Estimation for Generative
    Adversarial Networks.* arXiv preprint arXiv:1707.07530 (2017).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eghbal-Zadeh, Hamid, 和 Gerhard Widmer, *生成对抗网络的似然估计*。arXiv预印本 arXiv:1707.07530
    (2017)。
- en: 'Nguyen, Anh, et al., *Plug & play generative networks: Conditional iterative
    generation of images in latent space*. arXiv preprint arXiv:1612.00005 (2016).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen, Anh, et al., *即插即用生成网络：潜在空间中条件迭代生成图像*。arXiv预印本 arXiv:1612.00005 (2016)。
- en: Summary
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have reviewed two of the most important and innovative architectures
    that have appeared in recent. Every day, new generative and reinforcement models
    are applied in innovative ways, whether to generate feasible new elements from
    a selection of previously known classes or even to win against professional players
    in strategy games.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了最近出现的最重要和最具创新性的两种架构。每天都有新的生成和强化模型以创新的方式被应用，无论是从已知类别的选择中生成可行的新元素，甚至是在策略游戏中战胜专业玩家。
- en: In the next chapter, we will provide precise instructions so you can use and
    modify the code provided to better understand the different concepts you have
    acquired throughout the book.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将提供精确的说明，以便您可以使用和修改提供的代码，更好地理解您在整本书中学到的不同概念。
