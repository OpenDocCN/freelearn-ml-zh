<html><head></head><body><div class="calibre1" title="Chapter&#xA0;12.&#xA0;Processing Video Sequences"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title"><a id="ch12" class="calibre6"/>Chapter 12. Processing Video Sequences</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div class="calibre1"><ul class="itemizedlist"><li class="listitem">Reading video sequences</li><li class="listitem">Processing the video frames</li><li class="listitem">Writing video sequences</li><li class="listitem">Extracting the foreground objects in a video</li></ul></div><div class="calibre1" title="Introduction"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch12lvl1sec71" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">Video signals constitute a rich source of visual information. They are made of a sequence of images, called <span><strong class="calibre15">frames</strong></span>, that are taken at regular time intervals (specified as the <span><strong class="calibre15">frame rate</strong></span>, generally expressed in frames per second) and show a scene in motion. With the advent of powerful computers, it is now possible to perform advanced visual analysis on video sequences-sometimes at rates close to, or even faster than, the actual video frame rate. This chapter will show you how to read, process, and store video sequences.</p><p class="calibre8">We will see that once the individual frames of a video sequence have been extracted, the different image processing functions presented in this book can be applied to each of them. In addition, we will also look at algorithms that perform a temporal analysis of the video sequence, comparing adjacent frames and accumulating image statistics over time in order to extract foreground objects.</p></div></div>
<div class="calibre1" title="Reading video sequences"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch12lvl1sec72" class="calibre6"/>Reading video sequences</h1></div></div></div><p class="calibre8">In order to process a video sequence, we need to be able to read each of its frames. OpenCV has put in place an easy-to-use framework that can help us perform frame extraction from video files or even from USB or IP cameras. This recipe shows you how to use it.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec215" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Basically, all you need to do in order to read the frames of a video sequence is create an instance of the <code class="literal">cv::VideoCapture</code> class. You then create a loop that will extract and read each video frame. Here is a basic main function that displays the frames of a video sequence:</p><pre class="programlisting">    int main() 
    { 
      // Open the video file 
      cv::VideoCapture capture("bike.avi"); 
      // check if video successfully opened 
      if (!capture.isOpened()) 
        return 1; 
 
      // Get the frame rate 
      double rate= capture.get(CV_CAP_PROP_FPS); 
 
      bool stop(false); 
      cv::Mat frame;    // current video frame 
      cv::namedWindow("Extracted Frame"); 
 
      // Delay between each frame in ms 
      // corresponds to video frame rate 
      int delay= 1000/rate; 
 
      // for all frames in video 
      while (!stop) { 
 
        // read next frame if any 
        if (!capture.read(frame)) 
          break; 
 
        cv::imshow("Extracted Frame",frame); 
 
        // introduce a delay 
        // or press key to stop 
        if (cv::waitKey(delay)&gt;=0) 
          stop= true; 
      } 
 
      // Close the video file. 
      // Not required since called by destructor 
      capture.release(); 
      return 0; 
    } 
</pre><p class="calibre8">A window will appear on which the video will play as shown in the following screenshot:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_12_001.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec216" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">To open a video, you simply need to specify the video filename. This can be done by providing the name of the file in the constructor of the <code class="literal">cv::VideoCapture</code> object. It is also possible to use the <code class="literal">open</code> method if the <code class="literal">cv::VideoCapture</code> object has already been created. Once the video is successfully opened (this can be verified through the <code class="literal">isOpened</code> method), it is possible to start frame extraction. It is also possible to query the <code class="literal">cv::VideoCapture</code> object for information associated with the video file by using its <code class="literal">get</code> method with the appropriate flag. In the preceding example, we obtained the frame rate using the <code class="literal">CV_CAP_PROP_FPS</code> flag. Since it is a generic function, it always returns a double even if another type would be expected in some cases. For example, the total number of frames in the video file would be obtained (as an integer) as follows:</p><pre class="programlisting">    long t= static_cast&lt;long&gt;( capture.get(CV_CAP_PROP_FRAME_COUNT)); 
</pre><p class="calibre8">Have a look at the different flags that are available in the OpenCV documentation in order to find out what information can be obtained from the video.</p><p class="calibre8">There is also a <code class="literal">set</code> method that allows you to input parameters into the <code class="literal">cv::VideoCapture</code> instance. For example, you can request to move to a specific frame using the <code class="literal">CV_CAP_PROP_POS_FRAMES</code> flag:</p><pre class="programlisting">    // goto frame 100 
    double position= 100.0; 
    capture.set(CV_CAP_PROP_POS_FRAMES, position); 
</pre><p class="calibre8">You can also specify the position in milliseconds using <code class="literal">CV_CAP_PROP_POS_MSEC</code>, or you can specify the relative position inside the video using <code class="literal">CV_CAP_PROP_POS_AVI_RATIO</code> (with <code class="literal">0.0</code> corresponding to the beginning of the video and <code class="literal">1.0</code> to the end). The method returns <code class="literal">true</code> if the requested parameter setting is successful. Note that the possibility to get or set a particular video parameter largely depends on the codec that is used to compress and store the video sequence. If you are unsuccessful with some parameters, that could be simply due to the specific codec you are using.</p><p class="calibre8">Once the captured video is successfully opened, the frames can be sequentially obtained by repetitively calling the <code class="literal">read</code> method, as we did in the example of the previous section. One can equivalently call the overloaded reading operator:</p><pre class="programlisting">    capture &gt;&gt; frame; 
</pre><p class="calibre8">It is also possible to call the two basic methods:</p><pre class="programlisting">    capture.grab(); 
    capture.retrieve(frame); 
</pre><p class="calibre8">Also note how, in our example, we introduced a delay in displaying each frame. This is done using the <code class="literal">cv::waitKey</code> function. Here, we set the delay at a value that corresponds to the input video frame rate (if <code class="literal">fps</code> is the number of frames per second, then <code class="literal">1/fps</code> is the delay between two frames in milliseconds). You can obviously change this value to display the video at a slower or faster speed. However, if you are going to display the video frames, it is important that you insert such a delay if you want to make sure that the window has sufficient time to refresh (since it is a process of low priority, it will never refresh if the CPU is too busy). The <code class="literal">cv::waitKey</code> function also allows us to interrupt the reading process by pressing any key. In this case, the function returns the ASCII code of the key that is pressed. Note that, if the delay specified to the <code class="literal">cv::waitKey</code> function is <code class="literal">0</code>, then it will wait indefinitely for the user to press a key. This is very useful if someone wants to trace a process by examining the results frame by frame.</p><p class="calibre8">The final statement calls the <code class="literal">release</code> method, which will close the video file. However, this call is not required since <code class="literal">release</code> is also called by the <code class="literal">cv::VideoCapture</code> destructor.</p><p class="calibre8">It is important to note that in order to open the specified video file, your computer must have the corresponding codec installed; otherwise, <code class="literal">cv::VideoCapture</code> will not be able to decode the input file. Normally, if you are able to open your video file with a video player on your machine (such as Windows Media Player), then OpenCV should also be able to read this file.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec217" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">You can also read the video stream produced by a camera that is connected to your computer (a USB camera, for example). In this case, you simply specify an ID number (an integer) instead of a filename to the <code class="literal">open</code> function. Specifying <code class="literal">0</code> for the ID will open the default installed camera. In this case, the role of the <code class="literal">cv::waitKey</code> function that stops the processing becomes essential, since the video stream from the camera will be infinitely read.</p><p class="calibre8">Finally, it is also possible to load a video from the Web. In this case, all you have to do is provide the correct address, for example:</p><pre class="programlisting">    cv::VideoCapture capture("http://www.laganiere.name/bike.avi"); 
</pre></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec218" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The <span><em class="calibre16">Writing video sequences</em></span> recipe in this chapter has more information on video codecs.</li><li class="listitem">The <a href="http://ffmpeg.org/">
http://ffmpeg.org/
</a> website presents a complete open source and cross-platform solution for audio/video reading, recording, converting, and streaming. The OpenCV classes that manipulate video files are built on top of this library.</li></ul></div></div></div>
<div class="calibre1" title="Processing the video frames"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch12lvl1sec73" class="calibre6"/>Processing the video frames</h1></div></div></div><p class="calibre8">In this recipe, our objective is to apply some processing functions to each of the frames of a video sequence. We will do this by encapsulating the OpenCV video capture framework into our own class. Among other things, this class will allow us to specify a function that will be called each time a new frame is extracted.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec219" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">What we want is to be able to specify a processing function (a <span><strong class="calibre15">callback function</strong></span>) that will be called for each frame of a video sequence. This function can be defined as receiving a <code class="literal">cv::Mat</code> instance and outputting a processed frame. Therefore, in our framework, the processing function must have the following signature to be a valid callback:</p><pre class="programlisting">    void processFrame(cv::Mat&amp; img, cv::Mat&amp; out); 
</pre><p class="calibre8">As an example of such a processing function, consider the following simple function that computes the Canny edges of an input image:</p><pre class="programlisting">    void canny(cv::Mat&amp; img, cv::Mat&amp; out) { 
      // Convert to gray 
      if (img.channels()==3) 
        cv::cvtColor(img,out, cv::COLOR_BGR2GRAY); 
      // Compute Canny edges 
      cv::Canny(out,out,100,200); 
      // Invert the image 
      cv::threshold(out,out,128,255,cv::THRESH_BINARY_INV); 
    } 
</pre><p class="calibre8">Our <code class="literal">VideoProcessor</code> class encapsulates all aspects of a video-processing task. Using this class, the procedure will be to create a class instance, specify an input video file, attach the callback function to it, and then start the process. Programmatically, these steps are accomplished using our proposed class, as follows:</p><pre class="programlisting">      // Create instance 
      VideoProcessor processor; 
      // Open video file 
      processor.setInput("bike.avi"); 
      // Declare a window to display the video 
      processor.displayInput("Current Frame"); 
      processor.displayOutput("Output Frame"); 
      // Play the video at the original frame rate 
      processor.setDelay(1000./processor.getFrameRate()); 
      // Set the frame processor callback function 
      processor.setFrameProcessor(canny); 
      // Start the process 
      processor.run(); 
</pre><p class="calibre8">If this code is run, then two windows will play the input video and the output result at the original frame rate (a consequence of the delay introduced by the <code class="literal">setDelay</code> method). For example, considering the input video for which a frame is shown in the previous recipe, the output window will look as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_12_002.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec220" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">As we did in other recipes, our objective was to create a class that encapsulates the common functionalities of a video-processing algorithm. As one might expect, the class includes several member variables that control the different aspects of the video frame processing:</p><pre class="programlisting">    class VideoProcessor { 
 
      private: 
 
       // the OpenCV video capture object 
       cv::VideoCapture capture; 
       // the callback function to be called  
       // for the processing of each frame 
       void (*process)(cv::Mat&amp;, cv::Mat&amp;); 
       // a bool to determine if the  
       // process callback will be called 
       bool callIt; 
       // Input display window name 
       std::string windowNameInput; 
       // Output display window name 
       std::string windowNameOutput; 
       // delay between each frame processing 
       int delay; 
       // number of processed frames  
       long fnumber; 
       // stop at this frame number 
       long frameToStop; 
       // to stop the processing 
       bool stop; 
</pre><p class="calibre8">The first member variable is the <code class="literal">cv::VideoCapture</code> object. The second attribute is the <code class="literal">process</code> function pointer that will point to the callback function. This function can be specified using the corresponding setter method:</p><pre class="programlisting">      // set the callback function that 
      // will be called for each frame 
      void setFrameProcessor(void (*frameProcessingCallback)
                             (cv::Mat&amp;, cv::Mat&amp;)) { 
 
        process= frameProcessingCallback; 
      } 
</pre><p class="calibre8">The following method opens the video file:</p><pre class="programlisting">      //set the name of the video file 
      bool setInput(std::string filename) { 
 
        fnumber= 0; 
        // In case a resource was already  
        // associated with the VideoCapture instance 
        capture.release(); 
        // Open the video file 
        return capture.open(filename); 
      } 
</pre><p class="calibre8">It is generally interesting to display the frames as they are processed. Therefore, two methods are used to create the display windows:</p><pre class="programlisting">      // to display the input frames 
      void displayInput(std::string wn) { 
 
        windowNameInput= wn; 
        cv::namedWindow(windowNameInput); 
      } 
   
      // to display the processed frames 
      void displayOutput(std::string wn) { 
        windowNameOutput= wn; 
        cv::namedWindow(windowNameOutput); 
      } 
</pre><p class="calibre8">The main method, called <code class="literal">run</code>, is the one that contains the frame extraction loop:</p><pre class="programlisting">    // to grab (and process) the frames of the sequence 
    void run() { 
      // current frame 
      cv::Mat frame; 
      //output frame 
      cv::Mat output; 
 
      // if no capture device has been set 
      if (!isOpened()) 
        return; 
 
        stop= false; 
      while (!isStopped()) { 
   
        // read next frame if any 
        if (!readNextFrame(frame)) 
          break; 
  
        // display input frame 
        if (windowNameInput.length()!=0)  
          cv::imshow(windowNameInput,frame); 
 
         // calling the process function 
        if (callIt) { 
 
          //process the frame 
          process(frame, output); 
          //increment frame number 
          fnumber++; 
 
        } 
        else { 
          // no processing 
          output= frame; 
        } 
 
        // display output frame 
        if (windowNameOutput.length()!=0) 
          cv::imshow(windowNameOutput,output); 
          // introduce a delay 
          if (delay&gt;=0 &amp;&amp; cv::waitKey(delay)&gt;=0) 
            stopIt(); 
 
          // check if we should stop 
          if (frameToStop&gt;=0 &amp;&amp; getFrameNumber()==frameToStop) 
            stopIt(); 
         } 
     } 
 
    // Stop the processing 
    void stopIt() { 
      stop= true; 
    } 
 
    // Is the process stopped? 
    bool isStopped() { 
      return stop; 
    } 
 
    // Is a capture device opened? 
    bool isOpened() { 
      capture.isOpened(); 
    } 
 
    // set a delay between each frame 
    // 0 means wait at each frame 
    // negative means no delay 
    void setDelay(int d) { 
      delay= d; 
    } 
</pre><p class="calibre8">This method uses a <code class="literal">private</code> method that reads the frames:</p><pre class="programlisting">    // to get the next frame  
    // could be: video file or camera 
    bool readNextFrame(cv::Mat&amp; frame) { 
      return capture.read(frame); 
    } 
</pre><p class="calibre8">The <code class="literal">run</code> method proceeds by first calling the read method of the <code class="literal">cv::VideoCapture</code> class. There is then a series of operations that are executed, but before each of them is invoked, a check is made to determine whether it has been requested. The input window is displayed only if an input window name has been specified (using the <code class="literal">displayInput</code> method); the callback function is called only if one has been specified (using the <code class="literal">setFrameProcessor</code> method). The output window is displayed only if an output window name has been defined (using <code class="literal">displayOutput</code>); a delay is introduced only if one has been specified (using the <code class="literal">setDelay</code> method). Finally, the current frame number is checked if a stop frame has been defined (using the <code class="literal">stopAtFrameNo</code> method).</p><p class="calibre8">One might also wish to simply open and play the video file (without calling the callback function). Therefore, we have two methods that specify whether or not we want the callback function to be called:</p><pre class="programlisting">    // process callback to be called 
    void callProcess() { 
      callIt= true; 
    } 
 
    // do not call process callback 
    void dontCallProcess() { 
      callIt= false; 
    } 
</pre><p class="calibre8">Finally, the class also offers the possibility to stop at a certain frame number:</p><pre class="programlisting">    void stopAtFrameNo(long frame) { 
      frameToStop= frame; 
    } 
 
    // return the frame number of the next frame 
    long getFrameNumber() { 
      // get info of from the capture device 
      long fnumber= static_cast&lt;long&gt;(capture.get(CV_CAP_PROP_POS_FRAMES)); 
      return fnumber;  
    } 
</pre><p class="calibre8">The class also contains a number of getter and setter methods that are basically just a wrapper over the general <code class="literal">set</code> and <code class="literal">get</code> methods of the <code class="literal">cv::VideoCapture</code> framework.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec221" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">Our <code class="literal">VideoProcessor</code> class is there to facilitate the deployment of a video-processing module. A few additional refinements can be made to it.</p><div class="calibre1" title="Processing a sequence of images"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch12lvl3sec49" class="calibre6"/>Processing a sequence of images</h3></div></div></div><p class="calibre8">Sometimes, the input sequence is made of a series of images that are individually stored in distinct files. Our class can be easily modified to accommodate such input. You just need to add a member variable that will hold a vector of image filenames and its corresponding iterator:</p><pre class="programlisting">    // vector of image filename to be used as input 
    std::vector&lt;std::string&gt; images; 
    // image vector iterator 
    std::vector&lt;std::string&gt;::const_iterator itImg; 
</pre><p class="calibre8">A new <code class="literal">setInput</code> method is used to specify the filenames to be read:</p><pre class="programlisting">    // set the vector of input images 
    bool setInput(const std::vector&lt;std::string&gt;&amp; imgs) { 
      fnumber= 0; 
      // In case a resource was already  
      // associated with the VideoCapture instance 
      capture.release(); 
 
      // the input will be this vector of images 
      images= imgs; 
      itImg= images.begin(); 
      return true; 
    } 
</pre><p class="calibre8">The <code class="literal">isOpened</code> method becomes as follows:</p><pre class="programlisting">    // Is a capture device opened? 
    bool isOpened() { 
      return capture.isOpened() || !images.empty(); 
    } 
</pre><p class="calibre8">The last method that needs to be modified is the private <code class="literal">readNextFrame</code> method that will read from the video or from the vector of filenames, depending on the input that has been specified. The test is that if the vector of image filenames is not empty, then that is because the input is an image sequence. The call to <code class="literal">setInput</code> with a video filename clears this vector:</p><pre class="programlisting">    // to get the next frame  
    // could be: video file; camera; vector of images 
    bool readNextFrame(cv::Mat&amp; frame) { 
 
      if (images.size()==0) 
        return capture.read(frame); 
 
      else { 
        if (itImg != images.end()) { 
          frame= cv::imread(*itImg); 
          itImg++; 
          return frame.data != 0; 
        } else 
 
          return false; 
      } 
    } 
</pre></div><div class="calibre1" title="Using a frame processor class"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch12lvl3sec50" class="calibre6"/>Using a frame processor class</h3></div></div></div><p class="calibre8">In an object-oriented context, it might make more sense to use a frame processing class instead of a frame processing function. Indeed, a class would give the programmer much more flexibility in the definition of a video-processing algorithm. We can, therefore, define an interface that any class that wishes to be used inside the <code class="literal">VideoProcessor</code> will need to implement:</p><pre class="programlisting">    // The frame processor interface 
    class FrameProcessor { 
      public: 
      // processing method 
      virtual void process(cv:: Mat &amp;input, cv:: Mat &amp;output)= 0; 
    }; 
</pre><p class="calibre8">A setter method allows you to input a <code class="literal">FrameProcessor</code> instance to the <code class="literal">VideoProcessor</code> framework and assign it to the added <code class="literal">FrameProcessor</code> member variable that is defined as a pointer to a <code class="literal">FrameProcessor</code> object:</p><pre class="programlisting">    // set the instance of the class that  
    // implements the FrameProcessor interface 
    void setFrameProcessor(FrameProcessor* frameProcessorPtr) { 
      // invalidate callback function 
      process= 0; 
       // this is the frame processor instance  
       // that will be called 
       frameProcessor= frameProcessorPtr; 
       callProcess(); 
    } 
</pre><p class="calibre8">When a frame <code class="literal">processor</code> class instance is specified, it invalidates any frame processing function that could have been set previously. The same obviously applies if a frame processing function is specified instead. The <code class="literal">while</code> loop of the <code class="literal">run</code> method is modified to take into account this modification:</p><pre class="programlisting">    while (!isStopped()) { 
 
      // read next frame if any 
      if (!readNextFrame(frame)) 
        break; 
 
      // display input frame 
      if (windowNameInput.length()!=0) 
        cv::imshow(windowNameInput,frame); 
 
      //** calling the process function or method ** 
      if (callIt) { 
 
        // process the frame 
        if (process) // if call back function 
          process(frame, output); 
        else if (frameProcessor)  
          // if class interface instance 
          frameProcessor-&gt;process(frame,output); 
        // increment frame number 
        fnumber++; 
      } 
      else { 
        output= frame; 
      } 
      // display output frame 
      if (windowNameOutput.length()!=0) 
        cv::imshow(windowNameOutput,output); 
      // introduce a delay 
      if (delay&gt;=0 &amp;&amp; cv::waitKey(delay)&gt;=0) 
        stopIt(); 
      // check if we should stop 
      if (frameToStop&gt;=0 &amp;&amp; getFrameNumber()==frameToStop) 
        stopIt(); 
    } 
</pre></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec222" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The <span><em class="calibre16">Tracking feature points in a video</em></span> recipe of <a href="ch13.html" title="Chapter 13. Tracking Visual Motion">
Chapter 13
</a>, <span><em class="calibre16">Tracking Visual Motion</em></span>, gives you an example of how to use the <code class="literal">FrameProcessor</code> class interface</li><li class="listitem">The GitHub project at <a href="https://github.com/asolis/vivaVideo">
https://github.com/asolis/vivaVideo
</a> presents a more sophisticated framework for processing video with multithreading in OpenCV</li></ul></div></div></div>
<div class="calibre1" title="Writing video sequences"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch12lvl1sec74" class="calibre6"/>Writing video sequences</h1></div></div></div><p class="calibre8">In the previous recipes, we learned how to read a video file and extract its frames. This recipe will show you how to write frames and, therefore, create a video file. This will allow us to complete the typical video-processing chain: reading an input video stream, processing its frames, and then storing the results in a new video file.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec223" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Writing video files in OpenCV is done using the <code class="literal">cv::VideoWriter</code> class. An instance is constructed by specifying the filename, the frame rate at which the generated video should play, the size of each frame, and whether or not the video will be created in color:</p><pre class="programlisting">    writer.open(outputFile,     // filename 
                codec,          // codec to be used  
                framerate,      // frame rate of the video 
                frameSize,      // frame size 
                isColor);       // color video? 
</pre><p class="calibre8">In addition, you must specify the way you want the video data to be saved. This is the <code class="literal">codec</code> argument; this will be discussed at the end of this recipe.</p><p class="calibre8">Once the video file is opened, frames can be added to it by repetitively calling the <code class="literal">write</code> method:</p><pre class="programlisting">    writer.write(frame);   // add the frame to the video file 
</pre><p class="calibre8">Using the <code class="literal">cv::VideoWriter</code> class, our <code class="literal">VideoProcessor</code> class introduced in the previous recipe can easily be expanded in order to give it the ability to write video files. A simple program that will read a video, process it, and write the result to a video file would then be written as follows:</p><pre class="programlisting">    // Create instance 
    
    VideoProcessor processor; 
 
    // Open video file 
    processor.setInput("bike.avi"); 
    processor.setFrameProcessor(canny); 
    processor.setOutput("bikeOut.avi"); 
    // Start the process 
    processor.run(); 
</pre><p class="calibre8">Proceeding as we did in the preceding recipe, we also want to give the user the possibility to write the frames as individual images. In our framework, we adopt a naming convention that consists of a prefix name followed by a number made of a given number of digits. This number is automatically incremented as frames are saved. Then, to save the output result as a series of images, you would swap the preceding statement with this one:</p><pre class="programlisting">    processor.setOutput("bikeOut",  //prefix 
                        ".jpg",     // extension 
                        3,          // number of digits 
                        0);         // starting index 
</pre><p class="calibre8">Using the specified number of digits, this call will create the <code class="literal">bikeOut000.jpg</code>, <code class="literal">bikeOut001.jpg</code>, and <code class="literal">bikeOut002.jpg</code> files, and so on.</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec224" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">Let's now describe how to modify our <code class="literal">VideoProcessor</code> class in order to give it the ability to write video files. First, a <code class="literal">cv::VideoWriter</code> variable member must be added to our class (plus a few other attributes):</p><pre class="programlisting">    class VideoProcessor { 
 
      private: 
 
      // the OpenCV video writer object 
      cv::VideoWriter writer; 
      // output filename 
      std::string outputFile; 
      // current index for output images 
      int currentIndex; 
      // number of digits in output image filename 
      int digits; 
      // extension of output images 
      std::string extension; 
</pre><p class="calibre8">An extra method is used to specify (and open) the output video file:</p><pre class="programlisting">    // set the output video file 
    // by default the same parameters than  
    // input video will be used 
    bool setOutput(const std::string &amp;filename, int codec=0,          
                   double framerate=0.0, bool isColor=true) { 
 
      outputFile= filename; 
      extension.clear(); 
  
      if (framerate==0.0) 
        framerate= getFrameRate(); // same as input 
 
      char c[4]; 
      // use same codec as input 
      if (codec==0) {  
        codec= getCodec(c); 
      } 
 
      // Open output video 
      return writer.open(outputFile,      // filename 
                         codec,           // codec to be used  
                         framerate,       // frame rate of the video 
                         getFrameSize(),  // frame size 
                         isColor);        // color video? 
    } 
</pre><p class="calibre8">A private method, called the <code class="literal">writeNextFrame</code> method, handles the frame writing procedure (in a video file or as a series of images):</p><pre class="programlisting">    // to write the output frame  
    // could be: video file or images 
    void writeNextFrame(cv::Mat&amp; frame) { 
      if (extension.length()) { // then we write images 
 
        std::stringstream ss; 
        // compose the output filename 
        ss &lt;&lt; outputFile &lt;&lt; std::setfill('0')  
           &lt;&lt; std::setw(digits) &lt;&lt; currentIndex++ &lt;&lt; extension; 
        cv::imwrite(ss.str(),frame); 
     
      } else { 
        // then write to video file 
        writer.write(frame); 
      } 
    } 
</pre><p class="calibre8">For the case where the output is made of individual image files, we need an additional setter method:</p><pre class="programlisting">    // set the output as a series of image files 
    // extension must be ".jpg", ".bmp" 
    bool setOutput(const std::string &amp;filename, // prefix 
                   const std::string &amp;ext,      // image file extension 
                   int numberOfDigits=3,        // number of digits 
                   int startIndex=0) {          // start index 
 
      // number of digits must be positive 
      if (numberOfDigits&lt;0) 
        return false; 
 
      // filenames and their common extension 
      outputFile= filename; 
      extension= ext; 
 
      // number of digits in the file numbering scheme 
      digits= numberOfDigits; 
      // start numbering at this index 
      currentIndex= startIndex; 
 
      return true; 
    } 
</pre><p class="calibre8">Finally, a new step is then added to the video capture loop of the <code class="literal">run</code> method:</p><pre class="programlisting">  while (!isStopped()) { 
 
    // read next frame if any 
    if (!readNextFrame(frame)) 
      break; 
 
    // display input frame 
    if (windowNameInput.length()!=0) 
      cv::imshow(windowNameInput,frame); 
 
    // calling the process function or method 
    if (callIt) { 
 
      // process the frame 
      if (process) 
        process(frame, output); 
      else if (frameProcessor) 
        frameProcessor-&gt;process(frame,output); 
      // increment frame number 
      fnumber++; 
    }  else { 
      output= frame; 
    } 
 
    //** write output sequence ** 
    if (outputFile.length()!=0) 
      writeNextFrame(output); 
    // display output frame 
    if (windowNameOutput.length()!=0) 
      cv::imshow(windowNameOutput,output); 
    // introduce a delay 
    if (delay&gt;=0 &amp;&amp; cv::waitKey(delay)&gt;=0) 
      stopIt(); 
 
    // check if we should stop 
    if (frameToStop&gt;=0 &amp;&amp; getFrameNumber()==frameToStop) 
      stopIt(); 
    } 
  } 
</pre></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec225" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">When a video is written to a file, it is saved using a codec. A <span><strong class="calibre15">codec</strong></span> is a software module that is capable of encoding and decoding video streams. The codec defines both the format of the file and the compression scheme that is used to store the information. Obviously, a video that has been encoded using a given codec must be decoded with the same codec. For this reason, four-character codes have been introduced to uniquely identify codecs. This way, when a software tool needs to write a video file, it determines the codec to be used by reading the specified four-character code.</p><div class="calibre1" title="The codec four-character code"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch12lvl3sec51" class="calibre6"/>The codec four-character code</h3></div></div></div><p class="calibre8">As the name suggests, the four-character code is made up of four ASCII characters that can also be converted into an integer by appending them together. Using the <code class="literal">cv::CAP_PROP_FOURCC</code> flag of the <code class="literal">get</code> method of an opened <code class="literal">cv::VideoCapture</code> instance, you can obtain the code of an opened video file. We can define a method in our <code class="literal">VideoProcessor</code> class to return the four-character code of an input video:</p><pre class="programlisting">    // get the codec of input video 
    int getCodec(char codec[4]) { 
  
      // undefined for vector of images 
      if (images.size()!=0) return -1; 
      union { // data structure for the 4-char code 
        nt value; 
        char code[4]; 
      } returned; 
 
      // get the code 
      returned.value= static_cast&lt;int&gt;(capture.get(cv::CAP_PROP_FOURCC)); 
      // get the 4 characters 
      codec[0]= returned.code[0]; 
      codec[1]= returned.code[1]; 
      codec[2]= returned.code[2]; 
      codec[3]= returned.code[3]; 
 
      // return the int value corresponding to the code 
      return returned.value; 
    } 
</pre><p class="calibre8">The <code class="literal">get</code> method always returns a <code class="literal">double</code> value that is then casted into an <code class="literal">integer</code>. This integer represents the code from which the four characters can be extracted using a <code class="literal">union</code> data structure. If we open our test video sequence, then we have the following statements:</p><pre class="programlisting">    char codec[4]; 
    processor.getCodec(codec); 
    std::cout &lt;&lt; "Codec: " &lt;&lt; codec[0] &lt;&lt; codec[1] 
              &lt;&lt; codec[2] &lt;&lt; codec[3] &lt;&lt; std::endl; 
</pre><p class="calibre8">From the preceding statements, we obtain, for our example, the following:</p><pre class="programlisting">    Codec : XVID 
</pre><p class="calibre8">When a video file is written, the codec must be specified using its four-character code. This is the second parameter in the <code class="literal">open</code> method of the <code class="literal">cv::VideoWriter</code> class. You can use, for example, the same one as the input video (this is the default option in our <code class="literal">setOutput</code> method). You can also pass the value <code class="literal">-1</code> and the method will pop up a window that will ask you to select one codec from the list of available codecs. The list you will see in this window corresponds to the list of installed codecs on your machine. The code of the selected codec is then automatically sent to the <code class="literal">open</code> method.</p></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec226" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The <a href="https://www.xvid.com/">
https://www.xvid.com/
</a> website offers you an open source video codec library based on the MPEG-4 standard for video compression. <span><strong class="calibre15">Xvid</strong></span> also has a competitor called <span><strong class="calibre15">DivX</strong></span>, which offers proprietary but free codec and software tools.</li></ul></div></div></div>
<div class="calibre1" title="Extracting the foreground objects in a video"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h1 class="title1"><a id="ch12lvl1sec75" class="calibre6"/>Extracting the foreground objects in a video</h1></div></div></div><p class="calibre8">This chapter is about reading, writing, and processing video sequences. The objective is to be able to analyze a complete video sequence. As an example, in this recipe, you will learn how to perform temporal analysis of a sequence in order to extract the moving foreground objects. Indeed, when a fixed camera observes a scene, the background remains mostly unchanged. In this case, the interesting elements are the moving objects that evolve inside this scene. In order to extract these foreground objects, we need to build a model of the background, and then compare this model with a current frame in order to detect any foreground objects. This is what we will do in this recipe. Foreground extraction is a fundamental step in intelligent surveillance applications.</p><p class="calibre8">If we had an image of the background of the scene (that is, a frame that contains no foreground objects) at our disposal, then it would be easy to extract the foreground of a current frame through a simple image difference:</p><pre class="programlisting">    // compute difference between current image and background 
    cv::absdiff(backgroundImage,currentImage,foreground); 
</pre><p class="calibre8">Each pixel for which this difference is high enough would then be declared as a foreground pixel. However, most of the time, this background image is not readily available. Indeed, it could be difficult to guarantee that no foreground objects are present in a given image, and in busy scenes, such situations might rarely occur. Moreover, the background scene often evolves over time because, for instance, the lighting condition changes (for example, from sunrise to sunset) or because new objects are added or removed from the background.</p><p class="calibre8">Therefore, it is necessary to dynamically build a model of the background scene. This can be done by observing the scene for a period of time. If we assume that most often, the background is visible at each pixel location, then it could be a good strategy to simply compute the average of all of the observations. However, this is not feasible for a number of reasons. First, this would require a large number of images to be stored before computing the background. Second, while we are accumulating images to compute our average image, no foreground extraction is done. This solution also raises the problem of when and how many images should be accumulated to compute an acceptable background model. In addition, the images where a given pixel is observing a foreground object would have an impact on the computation of the average background.</p><p class="calibre8">A better strategy is to dynamically build the background model by regularly updating it. This can be accomplished by computing what is called a <span><strong class="calibre15">running average</strong></span> (also called <span><strong class="calibre15">moving average</strong></span>). This is a way to compute the average value of a temporal signal that takes into account the latest received values. If <code class="literal">p<sub class="calibre21">t</sub></code> is the pixel value at a given time <code class="literal">t</code> and <code class="literal">μ<sub class="calibre21">t-1</sub></code> is the current average value, then this average is updated using the following formula:</p><p class="calibre8">

</p><div class="mediaobject"><img alt="Extracting the foreground objects in a video" src="graphics/B05388_12_04.jpg" class="calibre17"/></div><p class="calibre8">

</p><p class="calibre8">The <code class="literal">α</code> parameter is called the <span><strong class="calibre15">learning rate</strong></span>, and it defines the influence of the current value over the currently estimated average. The larger this value is, the faster the running average will adapt to changes in the observed values but, at the same time, slowly moving objects will tend to disappear in the background when the learning rate is set too high. In fact, the appropriate learning rate largely depends on the dynamic of the scene. To build a background model, one just has to compute a running average for every pixel of the incoming frames. The decision to declare a foreground pixel is then simply based on the difference between the current image and the background model.</p><div class="calibre1" title="How to do it..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec227" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Let's build a class that will learn a background model using a moving average and that will extract foreground objects by subtraction. The required attributes are as follows:</p><pre class="programlisting">    class BGFGSegmentor : public FrameProcessor { 
      cv::Mat gray;          // current gray-level image 
      cv::Mat background;    // accumulated background 
      cv::Mat backImage;     // current background image 
      cv::Mat foreground;    // foreground image 
      // learning rate in background accumulation 
      double learningRate; 
      int threshold;         // threshold for foreground extraction 
</pre><p class="calibre8">The main process consists of comparing the current frame with the background model and then updating this model:</p><pre class="programlisting">    // processing method 
    void process(cv:: Mat &amp;frame, cv:: Mat &amp;output) { 
      // convert to gray-level image 
      cv::cvtColor(frame, gray, cv::COLOR_BGR2GRAY); 
      // initialize background to 1st frame 
      if (background.empty()) 
        gray.convertTo(background, CV_32F); 
      // convert background to 8U 
      background.convertTo(backImage,CV_8U); 
 
      // compute difference between image and background 
      cv::absdiff(backImage,gray,foreground); 
      // apply threshold to foreground image         
      cv::threshold(foreground,output,threshold, 
                    255,cv::THRESH_BINARY_INV); 
 
      // accumulate background 
      cv::accumulateWeighted(gray, background,  
                             // alpha*gray + (1-alpha)*background 
                             learningRate,  // alpha 
                             output);       // mask 
    } 
</pre><p class="calibre8">Using our video-processing framework, the foreground extraction program will be built as follows:</p><pre class="programlisting">    int main() { 
      // Create video procesor instance 
      VideoProcessor processor; 
   
      // Create background/foreground segmentor 
       BGFGSegmentor segmentor; 
       segmentor.setThreshold(25); 
 
      // Open video file 
      processor.setInput("bike.avi"); 
 
      // Set frame processor 
      processor.setFrameProcessor(&amp;segmentor); 
 
      // Declare a window to display the video 
      processor.displayOutput("Extracted Foreground"); 
 
      // Play the video at the original frame rate 
      processor.setDelay(1000./processor.getFrameRate()); 
 
      // Start the process 
      processor.run(); 
    } 
</pre><p class="calibre8">One of the resulting binary foreground images that will be displayed is as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="How to do it..." src="graphics/image_12_004.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div class="calibre1" title="How it works..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec228" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">Computing the running average of an image is easily accomplished through the <code class="literal">cv::accumulateWeighted</code> function that applies the running average formula to each pixel of the image. Note that the resulting image must be a floating point image. This is why we had to convert the background model into a background image before comparing it with the current frame. A simple thresholded absolute difference (computed by <code class="literal">cv::absdiff</code> followed by <code class="literal">cv::threshold</code>) extracts the foreground image. Note that we then used the foreground image as a mask to <code class="literal">cv::accumulateWeighted</code> in order to avoid updating pixels declared as foreground. This works because our foreground image is defined as being <code class="literal">false</code> (that is, <code class="literal">0</code>) at foreground pixels (which also explains why the foreground objects are displayed as black pixels in the resulting image).</p><p class="calibre8">Finally, it should be noted that, for simplicity, the background model that is built by our program is based on the gray-level version of the extracted frames. Maintaining a color background would require the computation of a running average in some color space. As it is often the case with parametric vision algorithms, the main difficulty in the presented approach is to determine the appropriate value for the threshold that would give good results for a given video.</p></div><div class="calibre1" title="There's more..."><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec229" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The preceding, simple method to extract foreground objects in a scene works well for simple scenes that show a relatively stable background. However, in many situations, the background scene might fluctuate in certain areas between different values, thus causing frequent false foreground detections. These might be due to, for example, a moving background object (for example, tree leaves) or a glaring effect (for example, on the surface of water). Casted shadows also pose a problem since they are often detected as part of a moving object. In order to cope with these problems, more sophisticated background modeling methods have been introduced.</p><div class="calibre1" title="The Mixture of Gaussian method"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h3 class="title3"><a id="ch12lvl3sec52" class="calibre6"/>The Mixture of Gaussian method</h3></div></div></div><p class="calibre8">One of these algorithms is the <span><strong class="calibre15">Mixture of Gaussian</strong></span> method. It proceeds in a way that is similar to the method presented in this recipe, but adds a number of improvements.</p><p class="calibre8">First, the method maintains more than one model per pixel (that is, more than one running average). This way, if a background pixel fluctuates between, let's say, two values, two running averages are then stored. A new pixel value will be declared as the foreground only if it does not belong to any of the most frequently observed models. The number of models used is a parameter of the method, and a typical value is <code class="literal">5</code>.</p><p class="calibre8">Second, not only is the running average maintained for each model, but also for the running variance. This is computed as follows:</p><p class="calibre8">
</p><div class="mediaobject"><img alt="The Mixture of Gaussian method" src="graphics/B05388_12_05.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">These computed averages and variances are used to build a Gaussian model from which the probability of a given pixel value belonging to the background can be estimated. This makes it easier to determine an appropriate threshold since it is now expressed as a probability rather than an absolute difference. Consequently, in areas where the background values have larger fluctuations, a greater difference will be required to declare a foreground object.</p><p class="calibre8">Finally, this is an adaptive model, that is when a given Gaussian model is not hit sufficiently often, it is excluded from being part of the background model. Reciprocally, when a pixel value is found to be outside the currently maintained background models (that is, it is a foreground pixel), a new Gaussian model is created. If, in the future, this new model frequently receives pixels, then it becomes associated with the background.</p><p class="calibre8">This more sophisticated algorithm is obviously more complex to implement than our simple background/foreground segmentor. Fortunately, an OpenCV implementation exists, called <code class="literal">cv::bgsegm::createBackgroundSubtractorMOG</code>, and is defined as a subclass of the more general <code class="literal">cv::BackgroundSubtractor</code> class. When used with its default parameter, this class is very easy to use:</p><pre class="programlisting">    int main(){ 
      // Open the video file 
      cv::VideoCapture capture("bike.avi"); 
      // check if video successfully opened 
      if (!capture.isOpened()) 
        return 0; 
 
      // current video frame 
      cv::Mat frame; 
      // foreground binary image 
      cv::Mat foreground; 
      // background image 
      cv::Mat background; 
      cv::namedWindow("Extracted Foreground"); 
 
      // The Mixture of Gaussian object 
      // used with all default parameters 
      cv::Ptr&lt;cv::BackgroundSubtractor&gt; ptrMOG =
                      cv::bgsegm::createBackgroundSubtractorMOG(); 
      bool stop(false); 
      // for all frames in video 
      while (!stop) { 
        // read next frame if any 
        if (!capture.read(frame)) 
          break; 
 
        // update the background 
        // and return the foreground 
         ptrMOG-&gt;apply(frame,foreground,0.01); 
 
        // Complement the image 
        cv::threshold(foreground,foreground,128, 
                      255,cv::THRESH_BINARY_INV); 
        //show foreground and background 
        cv::imshow("Extracted Foreground",foreground); 
 
        // introduce a delay 
        // or press key to stop 
        if (cv::waitKey(10)&gt;=0) 
          stop= true; 
      } 
    } 
</pre><p class="calibre8">As you can see, it is just a matter of creating the class instance and calling the method that simultaneously updates the background and returns the foreground image (the extra parameter being the learning rate). Also note that the background model is computed in color here. The method implemented in OpenCV also includes a mechanism to reject shadows by checking whether the observed pixel variation is simply caused by a local change in brightness (if so, then it is probably due to a shadow) or whether it also includes some change in chromaticity.</p><p class="calibre8">A second implementation is also available and is simply called <code class="literal">cv::BackgroundSubtractorMOG2</code>. One of the improvements is that the number of appropriate Gaussian models per pixel to be used is now determined dynamically. You can use this in place of the previous one in the preceding example. You should run these different methods on a number of videos in order to appreciate their respective performances. In general, you will observe that <code class="literal">cv::BackgroundSubtractorMOG2</code> is much faster.</p></div></div><div class="calibre1" title="See also"><div class="calibre1"><div class="calibre1"><div class="calibre1"><h2 class="title2"><a id="ch12lvl2sec230" class="calibre6"/>See also</h2></div></div></div><div class="calibre1"><ul class="itemizedlist"><li class="listitem">The article by <span><em class="calibre16">C. Stauffer</em></span> and <span><em class="calibre16">W.E.L. Grimso</em></span>n, <span><em class="calibre16">Adaptive Background Mixture Models for Real-Time Tracking</em></span>, in <span><em class="calibre16">Conf. on Computer Vision and Pattern Recognition</em></span>, 1999, gives you a more complete description of the Mixture of Gaussian algorithm</li></ul></div></div></div></body></html>