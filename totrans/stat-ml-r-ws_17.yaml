- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the Bayesian inference framework, covering
    its core components and implementation details. Bayesian inference introduces
    a useful framework that provides an educated guess on the predictions of the target
    outcome as well as quantified uncertainty estimates. Starting from a prior distribution
    that embeds domain expertise, the Bayesian inference approach allows us to continuously
    learn updated information from the data and update the posterior distribution
    to form a more realistic view of the underlying parameters.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have grasped essential skills when working
    with the Bayesian inference framework. You will learn the core theory behind Bayes‚Äô
    theorem and its use in the Bayesian linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Bayesian statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diving deeper into Bayesian inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full Bayesian inference procedure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian linear regression with a categorical variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the code in this chapter, you will need to have the latest versions
    of the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ggplot2`, 3.4.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ggridges`, 0.5.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rjags`, 4.13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`coda`, 0.19.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the versions mentioned along with the packages in the preceding
    list are the latest ones while I am writing this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: All the code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_14/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_14/working.R).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Bayesian statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bayesian approach to statistics and **machine learning** (**ML**) provides
    a logical, transparent, and interpretable framework. This is a uniform framework
    that can build problem-specific models for both statistical inference and prediction.
    In particular, Bayesian inference offers a method to figure out unknown or unobservable
    quantities given known facts (observed data), employing probability to describe
    the uncertainty over the possible values of unknown quantities‚Äînamely, random
    variables of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Using Bayesian statistics, we are able to express our prior assumption about
    unknown quantities and adjust this based on the observed data. It provides the
    Bayesian versions of common statistical procedures such as hypothesis testing
    and linear regression, covered in *Chapters 11*, *Statistics estimation*, and
    *12*, *Linear Regression in R*. Compared to the frequentist approach, which we
    have adopted in all the models covered so far, the Bayesian approach additionally
    allows us to construct problem-specific models that can make the best use of the
    data in a continuous learning fashion (via the Bayesian posterior update, to be
    covered in the following section).
  prefs: []
  type: TYPE_NORMAL
- en: For example, the unknown quantities correspond to the parameters we are trying
    to estimate in a linear or logistic regression model. Instead of treating them
    as fixed quantities and using the principle of maximum likelihood to estimate
    their values, the Bayesian approach treats them as moving variables with their
    respective probability distribution of possible values.
  prefs: []
  type: TYPE_NORMAL
- en: Let us get a first glimpse of the famous Bayesian theorem.
  prefs: []
  type: TYPE_NORMAL
- en: A first look into the Bayesian theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Bayesian theorem describes the relationship between conditional probabilities
    of statistical quantities. In the context of linear regression, we would treat
    the parameter Œ≤ as a random variable instead of fixed quantities as we did with
    linear regression in [*Chapter 12*](B18680_12.xhtml#_idTextAnchor258). For example,
    in a simple linear regression model y = Œ≤x, instead of obtaining the single best
    parameter Œ≤¬†* by minimizing the **ordinary least squares** (**OLS**) given the
    available data (x, y), we would instead treat Œ≤ as a random variable that follows
    a specific distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Doing this involves two distributions about Œ≤, the parameter of interest, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first distribution is the **prior distribution** P(Œ≤), which corresponds
    to a subjective distribution we assign to Œ≤ before we observe any data. This distribution
    encapsulates our prior belief about the probabilities of possible values of Œ≤
    before we observe any actual data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second distribution is the **posterior distribution** P(Œ≤|x, y), which corresponds
    to the updated belief about this distribution after we observe the data. This
    is the distribution we want to estimate through the update. Such an update is
    necessary in order to conform our prior belief to what we actually observe in
    reality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naturally, we would hope that the posterior distribution P(Œ≤|x, y) stays closer
    to what the data reflects as the training size gets large, and correspondingly
    stays further away from the prior belief. Here, the data refers to (x, y). To
    proceed with the update, the data would enter as what we call the likelihood function
    P(y|x, Œ≤), also referred to as the **generative model**. That is, P(y|x, Œ≤) represents
    the likelihood (similar to probability, although in an unnormalized way) of observing
    the target y given the input feature x and parameter value Œ≤, where we have treated
    Œ≤ as a specific value instead of a random variable. In other words, we would first
    sample from the distribution P(Œ≤) to obtain a concrete value of Œ≤, and then follow
    the specific observation model to obtain the actual data point y given the input
    feature x. For example, we would assume a normal distribution for the observation
    model if the errors are assumed to follow a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to compile these three quantities together via the following
    Bayesian theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: P(Œ≤ | x, y) = ¬†P(y|x, Œ≤)P(Œ≤)¬†_¬†P(y|x)
  prefs: []
  type: TYPE_NORMAL
- en: Here, P(y|x) is referred to as the evidence, which acts as a normalizing constant
    to ensure that the posterior distribution is a valid probability distribution,
    meaning each probability is non-negative and sums (or integrates) to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14**.1* illustrates the Bayesian theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 ‚Äì Illustrating the Bayesian theorem that calculates the posterior
    distribution ‚ÄãP‚Äã(<?AID d835?><?AID df37?>‚Ää|‚Ääx,‚Äây)‚Äã‚Äã](img/B18680_14_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 ‚Äì Illustrating the Bayesian theorem that calculates the posterior
    distribution P(ùú∑ | x, y)
  prefs: []
  type: TYPE_NORMAL
- en: Note that the prior distribution P(Œ≤) in Bayesian linear regression could be
    chosen to model our prior belief about the parameter Œ≤, which is something not
    available when using the frequentist framework in OLS-based linear regression.
    In practice, we often go with a normal prior distribution, but it could be any
    distribution that captures the prior belief about the probabilities of possible
    values of Œ≤ before we observe any data. The Bayesian framework thus allows us
    to incorporate the prior knowledge into the modeling in a principled manner. For
    example, if we believe that all features should have similar effects, we can then
    configure the prior distributions of the coefficients to be centered around the
    same value.
  prefs: []
  type: TYPE_NORMAL
- en: Since the parameter Œ≤ follows a posterior distribution P(Œ≤ | x, y), the resulting
    prediction y¬†* given a new input data x¬†* will not be a single number, as in the
    case of the frequentist approach. Instead, we will obtain a series of possible
    values of y¬†*, which follows a posterior predictive distribution P( y¬†*| x¬†*,
    Œ≤). That is, the prediction y¬†* is treated as a random variable due to the randomness
    in the parameter Œ≤. We can then use this distribution to understand the uncertainty
    in the resulting predictions. For example, if the posterior predictive distribution
    P( y¬†*| x¬†*, Œ≤) is wide, the resulting predictions, which are sampled from P(
    y¬†*| x¬†*, Œ≤), contain a higher degree of uncertainty. On the other hand, if the
    distribution is narrow, the resulting predictions are more concentrated and thus
    more confident. The posterior distribution P(Œ≤ | x, y) will also continue to evolve
    as new data becomes available.
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces more on the generative model.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the generative model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Bayesian inference, the generative model specifies the probability distribution
    that governs how the data is generated. For example, when the available target
    data is binary, we could assume it is generated following a Bernoulli distribution
    with a parameter p that represents the probability of success. To get a list of
    binary outcomes, we would first assign a probability value to p and then use this
    Bernoulli distribution to generate binary labels by repeated sampling from this
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through an exercise to understand the generative process.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.1 ‚Äì Generating binary outcomes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will generate a list of binary outcomes based on a Bernoulli
    distribution. This involves comparing a random sample from a uniform distribution
    valued between `0` and `1` to the preset probability of success. Follow the next
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with generating a random number in the range of `0` and `1` using
    a uniform distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Again, remember to set the random seed for reproducibility purposes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we compare the number to a preset probability of `0.2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This completes the generation of a single binary number. Now, let us expand
    it to 10 numbers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With the help of the following code, we‚Äôll generate 10 binary numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we used a `for` loop to repeatedly generate a uniform random number, compare
    it with the preset probability of success, and then store the result in a vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert the vector to numbers `0` and `1`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we have observed 1 instance of success out of 10 draws, despite a
    20% probability of success. As the sample size increases, we would expect the
    empirical probability of success (10% in this case) to be close to the theoretical
    value (20%).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It turns out that this generative model corresponds to a binomial process or
    a binomial distribution, which allows us to generate binary outcomes in one shot.
    Specifically, we can use the `rbinom``()` function to simulate data from a binomial
    distribution, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, `n` is the number of samples to be generated from the generative model,
    `size` is the number of trials to run, and `prob` is the underlying probability
    of success valued between `0.0` and `1.0`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are essentially working with a known parameter p, which is the
    probability of success. In practice, this would be an unknown parameter, something
    we are interested in estimating from the data. Bayesian inference would allow
    us to do that, with the assistance of a prior distribution and the likelihood
    function, as introduced in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding prior distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A prior distribution, an essential component of Bayesian inference, represents
    the prior knowledge or belief about the underlying parameter before we observe
    the actual data. It essentially specifies the probability distribution of the
    parameters based on domain-specific preference or expertise. If we have a valid
    reason to believe that certain values of the parameters are more likely, we can
    choose a prior distribution that reflects this preference.
  prefs: []
  type: TYPE_NORMAL
- en: The prior distribution, denoted as P(Œ≤), treats Œ≤ as a random variable and specifies
    its probability distribution. That is, it tells us which values of Œ≤ are more
    likely than others. In our running example on Bayesian linear regression, the
    prior distribution for Œ≤ is often chosen to be a multivariate Gaussian distribution.
    This is mainly for mathematical convenience, as the Gaussian distribution has
    nice properties that make it easier to work with. However, if we have no prior
    preference, a uniform distribution (which gives the same probability to all possible
    choices and is thus uninformed) could be a good candidate.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we can also use the prior to impose a form of regularization on the
    model. By choosing a prior distribution that favors smaller values of the parameters
    (such as a Gaussian distribution centered at `0`), we can discourage the model
    from finding solutions with large coefficients, thus preventing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we randomly generate 10 samples of P(Œ≤) following
    a uniform distribution between `0` and `0.2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When we model the probability of success p ‚àà [0,1] as a random variable, we
    often assign a beta distribution as the prior. For example, in the following code
    snippet, we generate 1,000 samples of p from the beta distribution p ‚àº Beta(35,55)
    using the `rbeta()` function, followed by showing its density plot after converting
    it to a DataFrame format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code will generate the output shown in *Figure 14**.2*.
    It shows a prior concentration of the probability of success close to `0.4`, within
    the range of `0` and `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 ‚Äì Visualizing the density plot of the prior distribution](img/B18680_14_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 ‚Äì Visualizing the density plot of the prior distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'The Beta(a, b) distribution is defined on the interval from `0` to `1`, thus
    providing a natural and flexible prior to the probability random variable. We
    can tune the `Beta` shape parameters a and b to produce alternative prior models.
    In the following code snippet, we compare the origina`l` `B``e``t``a``(``35``,``55``)`
    prior distributions with two alternatives: `B``e``t``a``(``1``,` `1``)` and `B``e``t``a``(``100``,`
    `100``)`. We then plot all three prior distributions together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the output shown in *Figure 14**.3*. Each prior
    distribution has a different preference region. For example, distribution B is
    close to a uniform distribution and has no specific preference, while distribution
    C places a strong preference for `0.5`, as indicated by the peak value around
    `0.5` and a narrow spread:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 ‚Äì Visualizing three different prior distributions](img/B18680_14_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 ‚Äì Visualizing three different prior distributions
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we‚Äôll introduce you to the likelihood function.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the likelihood function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The likelihood function describes how likely the observed data is, given a set
    of fixed model parameters. In a parametric model (a model that assumes a certain
    set of parameters), the likelihood is the probability of the observed data as
    a function of the parameters. The specific form of the likelihood function depends
    on the distribution (more specifically, the observation model) assumed for the
    data. For example, if we assume the data follows a normal distribution, the likelihood
    function would take the form of a normal probability density function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at a concrete example. Suppose we are developing a simple linear
    regression model with standard normal errors, expressed as y = Œ≤x + œµ, where œµ
    ‚àº N(0,1). Here, we have ignored the intercept term and only considered the slope.
    For a specific data point ( x¬†i, y¬†i), we can express the likelihood l¬†i as the
    probability evaluated at the probability density function of the error term:'
  prefs: []
  type: TYPE_NORMAL
- en: l¬†i = ¬†1¬†_¬†‚àö¬†_¬†2œÄ¬†¬† e¬†‚àí(y¬†i‚àíŒ≤x¬†i)¬†2¬†_¬†2
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the dataset consists of a total of n input-output pairs, the joint likelihood
    of all data points can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: L = Œ†¬†i=1¬†n¬† l¬†i = (¬†1¬†_¬†‚àö¬†_¬†2œÄ¬†¬†)¬†n e¬†‚àí‚àë¬†i=1¬†n¬†¬†(y¬†i‚àíŒ≤x¬†i)¬†2¬†_¬†2
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, we would often work with the log-likelihood after introducing
    the log transformation, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: logL = log(¬†1¬†_¬†‚àö¬†_¬†2œÄ¬†¬†)¬†n e¬†‚àí‚àë¬†i=1¬†n¬†¬†(y¬†i‚àíŒ≤x¬†i)¬†2¬†_¬†2¬† = ‚àí 0.5nlog2œÄ ‚àí 0.5‚àë¬†i=1¬†n¬†(y¬†i
    ‚àí Œ≤ x¬†i)¬†2
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to the objective function used in OLS-based linear regression, we
    find that the exponent term ‚àë¬†i=1¬†n¬†¬†(y¬†i ‚àí Œ≤ x¬†i)¬†2 is exactly the sum of squared
    errors. When using the maximum likelihood estimation procedure, these two different
    objective functions become equivalent to each other. In other words, we have the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: logL ‚âà ‚àí ‚àë¬†i=1¬†n¬†(y¬†i ‚àí Œ≤ x¬†i)¬†2
  prefs: []
  type: TYPE_NORMAL
- en: Here, we ignored the constant term (¬†1¬†_¬†‚àö¬†_¬†2œÄ¬†)¬†n at the last step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us go through an example of how to calculate the joint likelihood of a
    set of observed data. In the following code listing, we create a list of data
    points in `x` and `y` and a simple linear regression model with coefficient `b`
    to generate the predicted values in `y_pred`, along with the residual terms in
    residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then plug in the closed-form expression for the joint likelihood and
    calculate the total log-likelihood, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us look at another example of the binomial model. As discussed earlier,
    when the underlying parameter p represents the probability of success, we can
    use the `rbinom()` function to obtain the probability of observing a certain outcome
    (number of successes) in a total number of draws and with a specific probability
    of success. In the following code snippet, we first create a vector of probabilities
    to indicate different probabilities of success and calculate the corresponding
    likelihood in a total of 1,000 trials of sampling from a binomial model Bin(p,
    10), where 10 is the total number of draws. Lastly, we visualize all likelihood
    functions via a stacked density plot using the `geom_density_ridges()` function
    from the `ggridges` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 ‚Äì Visualizing the stacked density plot as the likelihood functions
    of different sampling from the binomial distribution](img/B18680_14_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 ‚Äì Visualizing the stacked density plot as the likelihood functions
    of different sampling from the binomial distribution
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces the posterior model.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the posterior model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A posterior distribution represents what we know about unknown parameters after
    observing the available data. It combines the prior beliefs, expressed via the
    prior distribution, with the evidence presented by the data, expressed in the
    likelihood function, to form a new distribution over the possible parameter values,
    which can be either discrete or continuous.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on Bayes‚Äô theorem, the posterior distribution is proportional to the
    product of the prior distribution and the likelihood function:'
  prefs: []
  type: TYPE_NORMAL
- en: P(Œ≤ | x, y) ‚àù P(y|x, Œ≤)P(Œ≤)
  prefs: []
  type: TYPE_NORMAL
- en: Note that we do not need to know the evidence term in the denominator when solving
    the optimal value of the parameter Œ≤, since P(y|x) is totally independent of Œ≤.
    As we gather more data and update our beliefs, the posterior distribution would
    often become more sharply peaked around the true parameter value, indicating an
    increased level of confidence.
  prefs: []
  type: TYPE_NORMAL
- en: However, when we need to know P(y|x) in order to calculate P(Œ≤ | x, y), the
    task is not so straightforward since a closed-form solution may not be available,
    or the parameters are multi-dimensional and prohibit a direct calculation of nested
    integration. In such cases, we would often resort to numerical methods such as
    **Markov chain Monte Carlo** (**MCMC**) or approximate methods such as variational
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through an exercise to understand the overall inference process. We
    will use the `rjags` package to do the calculations based on our running example.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.2 ‚Äì Obtaining the posterior distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use the `rjags` package to perform Bayesian inference,
    including specifying the model architecture and obtaining the posterior distribution
    for the underlying parameter. Follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with defining the likelihood function as a binomial distribution (using
    `dbin`) with parameters `p` for the probability of success and `n` for the total
    number of samples. We will also, define the prior distribution for `p` as a beta
    distribution (using `dbeta`) with parameters `a` and `b`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we specify `textConnection(bayes_model)` to pass the model specification
    string to `jags`. The data argument is a list specifying the observed data and
    the parameters for the prior distribution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we draw samples from the posterior distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the `coda.samples()` function is used to run MCMC simulations and draw
    samples of the parameter. The `n.iter` argument specifies the number of iterations
    for the MCMC simulation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we plot the posterior:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the preceding code will result in the output shown in *Figure 14**.5*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.5 ‚Äì Visualizing the posterior distribution for the underlying parameter
    (probability of success)](img/B18680_14_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 ‚Äì Visualizing the posterior distribution for the underlying parameter
    (probability of success)
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we‚Äôll dive deeper into Bayesian inference, starting by
    introducing the normal-normal model, a commonly used type of model in Bayesian
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper into Bayesian inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian inference is a statistical method that makes use of conditional probability
    to update the prior beliefs about the parameters of a statistical model given
    the observed data. The output of Bayesian inference is a posterior distribution,
    which is a probability distribution that represents our updated beliefs about
    the parameter after observing the data.
  prefs: []
  type: TYPE_NORMAL
- en: When calculating the exact posterior distribution is difficult, we would often
    resort to MCMC, which is a technique for estimating the distribution of a random
    variable. It‚Äôs a method commonly used to generate samples from the posterior distribution
    in Bayesian inference, especially when the dimensionality of the model parameters
    is high, making an analytical solution intractable.
  prefs: []
  type: TYPE_NORMAL
- en: The following section introduces the normal-normal model and uses MCMC to estimate
    its posterior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the normal-normal model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The normal-normal model is another foundational model in Bayesian inference.
    It refers to the case when the likelihood is normally distributed and the prior
    is also normally distributed, both following a bell-shaped curve. This type of
    model is often used in Bayesian statistics as it has a closed-form solution for
    the posterior distribution, which also happens to be normally distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Let us look at a concrete exercise of the normal-normal model using MCMC-based
    Bayesian inference.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.3 ‚Äì Working with the normal-normal model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will define a normal likelihood function whose mean follows
    a normal prior and whose standard deviation follows a uniform prior. We will then
    use `rjags` to obtain the posterior estimates for the mean and the standard deviation.
    Follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs start with simulating 100 data points that follow a normal distribution
    with a true mean of `2` and a true standard deviation of `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we specify the model architecture, including a normal likelihood function
    for the data, a normal prior distribution for the mean variable, and a uniform
    prior for the standard deviation variable. The normal prior is parameterized by
    `0` and `0.1` for the mean and standard deviation, respectively, and the uniform
    prior ranges from `0` to `10`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding model states that each observation in our data (`y[i]`) follows
    a normal distribution with mean `mu` and precision, `prec`, (defined as the reciprocal
    of the variance, hence `prec <- pow(sigma, -2)`). The mean `mu` follows a normal
    distribution with mean `0` and precision close to `0`, which corresponds to a
    relatively large variance and, therefore, a weak prior belief about `mu`. The
    standard deviation, `sigma`, follows a uniform distribution from `0` to `10`,
    which expresses complete uncertainty about its value between these two bounds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, compile the model in `jags` and burn in the Markov chain, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the `burn-in` period is a number of initial iterations that we discard
    when performing Bayesian inference, under the assumption that the resulting chain
    may not have converged during this period. The idea is to let the Markov chain
    burn in until it reaches a distribution that is stable and reflective of the posterior
    distribution we are interested in. In this case, we would discard the first 1,000
    iterations of the Markov chain, and these are not used in the subsequent analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here, we generate samples from the posterior distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the `summary()` function provides useful summary statistics for the posterior
    samples of `mu` and `sigma`, including their means, medians, and credible intervals.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, let‚Äôs visualize the results by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the preceding line of code will generate the output shown in *Figure
    14**.6*, which shows trace plots and density plots for the posterior samples of
    `mu` and `sigma`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.6 ‚Äì Visualizing the trace plots and density plots for the posterior
    samples](img/B18680_14_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.6 ‚Äì Visualizing the trace plots and density plots for the posterior
    samples
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss more on the MCMC.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MCMC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Markov chain is a mathematical model that transits from one state to another
    within a finite or countable number of possible states. It is a sequence of random
    variables where the future state depends only on the present state and not on
    the sequence of events before it. This property is known as the **Markov property**,
    or **memorylessness**.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of statistical inference, we can sample from complex probability
    distributions and create models of sequence data via Monte Carlo simulations,
    and thus the term MCMC. MCMC algorithms construct a Markov chain of parameter
    values where the stationary distribution of the chain is the posterior distribution
    of the underlying parameters. The chain is generated by iteratively proposing
    new parameter values and accepting or rejecting these candidate values based on
    a preset rule, ensuring that the samples converge to the posterior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us analyze the details of the previous MCMC chain. In the following code
    snippet, we convert the chain to a DataFrame and print its first few rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'These are MCMC samples generated to approximate the posterior distribution
    for `mu` and `sigma`, respectively. Each sample depends on the previous sample
    only and is unrelated to other prior samples. We can plot these samples in a line
    plot called a trace plot, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code generates the output shown in *Figure 14**.7*. A
    trace plot is a commonly used diagnostic tool in MCMC. It is a graphical representation
    of the values of the samples at each iteration or step of the MCMC algorithm.
    In this case, the trace plot shows no obvious trend, suggesting that both chains
    have converged stably:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 ‚Äì Visualizing the trace plots of MCMC chains](img/B18680_14_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.7 ‚Äì Visualizing the trace plots of MCMC chains
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us observe the first 100 samples using `ggplot()` for `mu`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code generates the output shown in *Figure 14**.8*. We
    can see that the sampler moves to another region after sufficiently exploring
    the previous region:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8 ‚Äì Visualizing the first 100 iterations of the mu chain](img/B18680_14_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.8 ‚Äì Visualizing the first 100 iterations of the mu chain
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can also display the density plot only by setting `trace = FALSE`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the output shown in *Figure 14**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9 ‚Äì Visualizing the density plot of both chains](img/B18680_14_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.9 ‚Äì Visualizing the density plot of both chains
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we would often run multiple chains in MCMC. This allows us to check
    whether all chains converge to the same distribution, which is a critical step
    to ensure that MCMC sampling is done correctly. Since each chain in MCMC starts
    at a different initial point, running multiple chains could help check whether
    the posterior distribution is dependent on the starting values. When all chains
    converge to the same distribution regardless of the initial points, we have a
    higher level of confidence in the stability and robustness of the MCMC process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet runs MCMC over four chains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the trace plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in *Figure 14**.10*, all four chains have more or less converged to
    a stable state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.10 ‚Äì Visualizing the density plot of both chains](img/B18680_14_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.10 ‚Äì Visualizing the density plot of both chains
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can check the summary of the MCMC samples, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will cover the full Bayesian inference procedure, including
    quantifying the posterior uncertainty and making predictions based on the posterior
    distribution of the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The full Bayesian inference procedure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full Bayesian inference starts by specifying the model architecture, including
    the prior distribution for unknown (unobserved) parameters and the likelihood
    function that determines how the data is generated. We can then perform MCMC to
    infer the posterior distribution of these parameters given the observed dataset.
    Finally, we can use the posterior distribution to either quantify the uncertainty
    about these parameters or make predictions for new input data with quantified
    uncertainty about the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The following exercise illustrates this process using the `mtcars` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.4 ‚Äì Performing full Bayesian inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will perform Bayesian linear regression with a single
    feature and two unknown parameters: `intercept` and `slope`. The model looks at
    the relationship between car weight (`wt`) and horsepower (`hp`) in the `mtcars`
    dataset. Follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify a Bayesian inference model where each target `wt` is modeled as a realization
    of a random variable following a normal distribution. The `mean` parameter is
    a linear combination of two parameters (`a` for `intercept` and `b` for `slope`)
    with the corresponding input feature. Both `a` and `b` follow a normal distribution,
    and the `variance` parameter follows a uniform distribution. The code is illustrated
    in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the model with three chains and control the random seed for model reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run a burn-in period of `three` iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'MCMC samples from the posterior distribution of the parameters, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a trace plot of the MCMC samples to assess convergence, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the preceding code generates the output shown in *Figure 14**.11*,
    suggesting a decent convergence for all three parameters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.11 ‚Äì Visualizing the trace plots of all three parameters](img/B18680_14_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.11 ‚Äì Visualizing the trace plots of all three parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the mean of the posterior distributions for parameters `a` and `b`,
    and use these mean values to make point predictions and plot the prediction line
    on a scatterplot of the data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we use the mean of the posterior distribution as the parameter value
    to make a point prediction for each input feature. The commands generate the output
    shown in *Figure 14**.12*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.12 ‚Äì Making points predictions using the mean value of the posterior
    distributions for each parameter](img/B18680_14_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.12 ‚Äì Making points predictions using the mean value of the posterior
    distributions for each parameter
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that Bayesian linear regression offers quantified uncertainty
    compared to frequentist linear regression covered in previous chapters. Such uncertainty
    comes in the form of credible intervals, which differ from the confidence interval.
    Specifically, we obtain the credible interval by treating the parameter as a random
    variable and the data as a fixed quantity, which makes more sense as we only get
    to observe the data once in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the `a` and `b` using the `HPDinterval()` function and plot the confidence
    intervals in the histogram, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the commands generates the output shown in *Figure 14**.13*. Here,
    we calculate the 95% credible interval by default:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.13 ‚Äì Visualizing the posterior distribution and credible interval
    of the model parameters](img/B18680_14_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.13 ‚Äì Visualizing the posterior distribution and credible interval
    of the model parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, make posterior predictions for a new input value, 120, and plot the
    posterior predictive distribution based on the list of posterior samples for the
    parameters, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the commands generates the output shown in *Figure 14**.14*. This posterior
    predictive distribution captures the model‚Äôs uncertainty about the prediction:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.14 ‚Äì Visualizing the posterior predictive distribution for a new
    input feature](img/B18680_14_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.14 ‚Äì Visualizing the posterior predictive distribution for a new input
    feature
  prefs: []
  type: TYPE_NORMAL
- en: The next section covers a Bayesian linear regression model using a categorical
    input variable.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian linear regression with a categorical variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the predictor is categorical, such as a binary feature, we would set one
    parameter for each corresponding category. The following exercise demonstrates
    such an example.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 14.5 ‚Äì Performing Bayesian inference with a categorical variable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will examine the relationship between `am` (automatic
    or manual transmission, a categorical variable) and `mpg` (miles per gallon, a
    continuous variable). We will define the mean of the normal likelihood for `mpg`
    as a function of `am`, with a different mean `mu[i]` for each level of `am`. We‚Äôll
    also give `mu` a normal prior and standard deviation `s` a uniform prior. Follow
    the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the aforementioned model architecture, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the model, generate the posterior samples, and show the convergence
    plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the preceding code generates the output shown in *Figure 14**.15*,
    suggesting a decent convergence of all model parameters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.15 ‚Äì Visualizing the convergence plots](img/B18680_14_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.15 ‚Äì Visualizing the convergence plots
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the distribution of the dataset along with the mean estimate of parameters
    for both levels of the categorical variable, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will generate the output shown in *Figure 14**.16*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 14.16 ‚Äì Visualizing the distribution and the mean estimates of the
    dataset](img/B18680_14_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.16 ‚Äì Visualizing the distribution and the mean estimates of the dataset
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides a comprehensive introduction to Bayesian statistics, beginning
    with an exploration of the fundamental Bayes‚Äô theorem. We delved into its components,
    starting with understanding the generative model, which helps us simulate data
    and examine how changes in parameters affect the data generation process.
  prefs: []
  type: TYPE_NORMAL
- en: We then focused on understanding the prior distribution, an essential part of
    Bayesian statistics that represents our prior knowledge about an uncertain parameter.
    This was followed by an introduction to the likelihood function, a statistical
    function that determines how likely it is for a set of observations to occur given
    specific parameter values.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we introduced the concept of the posterior model. This combines our prior
    distribution and likelihood to give a new probability distribution that represents
    updated beliefs after having seen the data. We also explored more complex models,
    such as the normal-normal model, wherein both the likelihood and the prior are
    normally distributed. We further investigated the mechanics of Bayesian inference
    through the MCMC method, a powerful tool for estimating the distribution of parameters
    and making predictions. A detailed walk-through of the full Bayesian inference
    procedure accompanied this.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we discussed Bayesian linear regression with a categorical variable,
    which extends the methodology to models that include categorical predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You‚Äôve successfully navigated to the end of this book, a testament
    to your dedication and effort. This journey, hopefully enriching for you and certainly
    so for me, marks a significant milestone in your ongoing exploration of the dynamic
    field of statistics and ML. I‚Äôm honored that you chose this book as a companion
    in this voyage, and I trust it has laid a solid foundation for your future pursuits.
  prefs: []
  type: TYPE_NORMAL
