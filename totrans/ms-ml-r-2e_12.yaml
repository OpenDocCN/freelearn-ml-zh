- en: Time Series and Causality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"An economist is an expert who will know tomorrow why the things he predicted
    yesterday didn''t happen today."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Laurence J. Peter'
  prefs: []
  type: TYPE_NORMAL
- en: A univariate time series is where the measurements are collected over a standard
    measure of time, which could be by the minute, hour, day, week, or month. What
    makes the time series problematic over the other data is that the order of the
    observations probably matters. This dependency of order can cause the standard
    analysis methods to produce an unnecessarily high bias or variance.
  prefs: []
  type: TYPE_NORMAL
- en: It seems that there is a paucity of literature on machine learning and time
    series data. This is unfortunate as so much of real-world data involves a time
    component. Furthermore, time series analysis can be quite complicated and tricky.
    I would say that if you haven't seen a time series analysis done incorrectly,
    you haven't been looking close enough.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect involving time series that is often neglected is causality. Yes,
    we don't want to confuse correlation with causation, but in time series analysis,
    one can apply the technique of Granger causality in order to determine if causality,
    statistically speaking, exists.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will apply time series/econometric techniques to identify
    univariate forecast models, vector autoregression models, and finally, Granger
    causality. After completing the chapter, you may not be a complete master of the
    time series analysis, but you will know enough to perform an effective analysis
    and understand the fundamental issues to consider when building time series models
    and creating predictive models (forecasts).
  prefs: []
  type: TYPE_NORMAL
- en: Univariate time series analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will focus on two methods to analyze and forecast a single time series:
    **exponential smoothing** and **autoregressive integrated moving average** (**ARIMA**)
    models. We will start by looking at exponential smoothing models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like moving average models, exponential smoothing models use weights for past
    observations. But unlike moving average models, the more recent the observation
    the more weight it is given relative to the later ones. There are three possible
    smoothing parameters to estimate: the overall smoothing parameter, a trend parameter,
    and smoothing parameter. If no trend or seasonality is present, then these parameters
    become null.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The smoothing parameter produces a forecast with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Yt+1 = α(Yt) + (1 – α)Yt-1 + (1-α)2Yt-2 +…, where 0 < α ≤ 1*'
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, *Y[t]* is the value at the time T, and alpha (*α*) is the
    smoothing parameter. Algorithms optimize the alpha (and other parameters) by minimizing
    the errors, for example, **sum of squared error** (**SSE**) or **mean squared
    error** (**MSE**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The forecast equation along with trend and seasonality equations, if applicable,
    will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The forecast, where *A* is the preceding smoothing equation and *h* is the number
    of forecast periods, *Y[t+h] = A + hB[t] + S[t]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trend equation,  *B[t] = β(A[t] – A[t-1]) + (1 – β)B[t-1]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The seasonality, where *m* is the number of seasonal periods, *S[t] = Ω(Y[t]
    – A[t-1] – B[t-1]) + (1 - Ω)S[t-m]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This equation is referred to as the **Holt-Winters Method**. The forecast equation
    is additive in nature with the trend as linear. The method also allows the inclusion
    of a dampened trend and multiplicative seasonality, where the seasonality proportionally
    increases or decreases over time. In my experience, the Holt-Winter's Method provides
    the best forecasts, even better than the ARIMA models. I have come to this conclusion
    on having to update long-term forecasts for hundreds of time series based on monthly
    data, and in roughly 90 percent of the cases, Holt-Winters produced minimal forecast
    error. Additionally, you don't have to worry about the assumption of stationarity
    as in an ARIMA model. Stationarity is where the time series has a constant mean,
    variance, and correlation between all the time periods. Having said this, it is
    still important to understand the ARIMA models as there will be situations where
    they have the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the autoregressive model, the value of *Y* at time *T* is a linear
    function of the prior values of *Y*. The formula for an autoregressive lag-1 model *AR(1)*,
    is *Yt = constant + ΦYt-1 + Et*. The critical assumptions for the model are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Et* denotes the errors that are identically and independently distributed
    with a mean zero and constant variance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The errors are independent of *Yt*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yt, Yt-1, Yt-n...* is stationary, which means that the absolute value of *Φ*
    is less than one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With a stationary time series, you can examine **Autocorrelation Function**
    (**ACF**). The ACF of a stationary series gives correlations between *Yt* and
    *Yt-h* for *h = 1, 2...n*. Let''s use R to create an *AR(1)* series and plot it.
    In doing so, we will also look at the capabilities of the `ggfortify` package,
    which acts as a wrapper around `ggplot2` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s examine `ACF`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **ACF** plot shows the correlations exponentially decreasing as the **Lag**
    increases. The dotted blue lines indicate the confidence bands of a significant
    correlation. Any line that extends above the high or below the low band is considered
    significant. In addition to ACF, one should also examine **Partial Autocorrelation
    Function** (**PACF**). PACF is a conditional correlation, which means that the
    correlation between *Yt* and *Yt-h* is conditional on the observations that come
    between the two. One way to intuitively understand this is to think of a linear
    regression model and its coefficients. Let''s assume that you have *Y = B0 + B1X1*
    versus *Y = B0 + B1X1 + B2X2*. The relationship of *X* to *Y* in the first model
    is linear with a coefficient, but in the second model, the coefficient will be
    different because of the relationship between *Y* and *X2* now being accounted
    for as well. Note that in the following `PACF` plot, the partial autocorrelation
    value at lag-1 is identical to the autocorrelation value at lag-1, as this is
    not a conditional correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: We can safely make the assumption that the series is stationary from the appearance
    of the preceding time series plot. We'll look at a couple of statistical tests
    in the practical exercise to ensure that the data is stationary, but most of the
    time, the eyeball test is sufficient. If the data is not stationary, then it is
    possible to detrend the data by taking its differences. This is the Integrated
    (I) in ARIMA. After differencing, the new series becomes *ΔYt = Yt - Yt-1*. One
    should expect a first-order difference to achieve stationarity, but on some occasions,
    a second-order difference may be necessary. An ARIMA model with *AR(1)* and *I(1)*
    would be annotated as (1,1,0).
  prefs: []
  type: TYPE_NORMAL
- en: 'The MA stands for moving average. This is not the simple moving average as
    the 50-day moving average of a stock price, it''s rather a coefficient that is
    applied to the errors. The errors are, of course, identically and independently
    distributed with a mean zero and constant variance. The formula for an *MA(1)*
    model is *Yt = constant + Et + ΘEt-1*. As we did with the *AR(1)* model, we can
    build an *MA(1)* in R, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `ACF` and `PACF` plots are a bit different from the *AR(1)* model. Note
    that there are some rules of thumb while looking at the plots in order to determine
    whether the model has AR and/or MA terms. They can be a bit subjective; so I will
    leave it to you to learn these heuristics, but trust R to identify the proper
    model. In the following plots, we will see a significant correlation at lag-1
    and two significant partial correlations at lag-1 and lag-2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding figure is the `ACF` plot, and now, we will see the `PACF` plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_11_06.png)'
  prefs: []
  type: TYPE_IMG
- en: With the ARIMA models, it is possible to incorporate seasonality, including
    the autoregressive, integrated, and moving average terms. The nonseasonal ARIMA
    model notation is commonly (p,d,q). With seasonal ARIMA, assume that the data
    is monthly, then the notation would be (p,d,q) x (P,D,Q)12, with the '12' in the
    notation taking the monthly seasonality into account. In the packages that we
    will use, R will automatically identify whether the seasonality should be included;
    if so, the optimal terms will be included as well.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Granger causality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine you are asked a question such as, "What is the relationship between
    the amount of new prescriptions and total prescriptions for medicine X?". You
    know that these are measured monthly, so what could you do to understand that
    relationship, given that people believe that new scripts will drive up total scripts.
    Or, how about testing the hypothesis that commodity prices, in particular copper,
    is a leading indicator of stock market prices in the US? Well, with two sets of
    time series data, *x* and *y*, Granger causality is a method that attempts to
    determine whether one series is likely to influence a change in the other. This
    is done by taking different lags of one series and using this to model the change
    in the second series. To accomplish this, we will create two models that will
    predict *y*, one with only the past values of *y* (`Ω`) and the other with the
    past values of *y* and *x* (`π`). The models are as follows, where *k* is the
    number of lags in the time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The RSS are then compared and `F-test` is used to determine whether the nested
    model `(Ω)` is adequate enough to explain the future values of *y* or whether
    the full model (`π`) is better. `F-test` is used to test the following null and
    alternate hypotheses:'
  prefs: []
  type: TYPE_NORMAL
- en: '*H0*: `αi = 0` for each *i* `∊[1,k]`, no Granger causality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*H1*: `αi ≠ 0` for at least one *i* `∊[1,k]`, Granger causality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentially, we are trying to determine whether we can say that statistically,
    *x* provides more information about the future values of *y* than the past values
    of *y* alone. In this definition, it is clear that we are not trying to prove
    actual causation; only that the two values are related by some phenomenon. Along
    these lines, we must also run this model in reverse in order to verify that *y*
    does not provide information about the future values of *x*. If we find that this
    is the case, it is likely that there is some exogenous variable, say *Z*, that
    needs to be controlled or would possibly be a better candidate for the Granger
    causation. To avoid spurious results, the method should be applied to a stationary
    time series. Note that research papers are available that discuss the techniques
    nonlinear models use, but this is outside of the scope for this book; however,
    we will examine it from a non-stationary standpoint. There is an excellent introductory
    paper that revolves around the age-old conundrum of the chicken and the egg (Thurman,
    1988).
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of different ways to identify the proper lag structure. Naturally,
    one can use brute force and ignorance to test all the reasonable lags, one at
    a time. One may have a rational intuition based on domain expertise or perhaps
    prior research that exists to guide the lag selection. If not, then **Vector Autoregression**
    (**VAR**) can be applied to identify the lag structure with the lowest information
    criterion, such as **Aikake's Information Criterion** (**AIC**) or **Final Prediction
    Error** (**FPE**). For simplicity, here is the notation for the VAR models with
    two variables, and this incorporates only one lag for each variable. This notation
    can be extended for as many variables and lags as appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = constant[1] + B[11]Y[t-1] + B[12]Y[t-1] + e[1]*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X = constant[1] + B2[1]Y[t-1] + B2[2]Y[t-1] + e2*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In R, this process is quite simple to implement as we will see in the following
    practical problem.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The planet isn't going anywhere. We are! We're goin' away.
  prefs: []
  type: TYPE_NORMAL
- en: '- Philosopher and comedian, George Carlin'
  prefs: []
  type: TYPE_NORMAL
- en: Climate change is happening. It always has and will, but the big question, at
    least from a political and economic standpoint, is I the climate change man-made?
    I will use this chapter to put econometric time series modeling to the test to
    try and learn whether carbon emissions cause, statistically speaking, climate
    change, and in particular, rising temperatures. Personally, I would like to take
    a neutral stance on the issue, always keeping in mind the tenets that Mr. Carlin
    left for us in his teachings on the subject.
  prefs: []
  type: TYPE_NORMAL
- en: The first order of business is to find and gather the data. For temperature,
    I chose the **HadCRUT4** annual median temperature time series, which is probably
    the gold standard. This data is compiled by a cooperative effort of the Climate
    Research Unit of the University of East Anglia and the Hadley Centre at the UK's
    Meteorological Office. A full discussion of how the data is compiled and modeled
    is available at [http://www.metoffice.gov.uk/hadobs/index.html](http://www.metoffice.gov.uk/hadobs/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The data that we will use is provided as an annual anomaly, which is calculated
    as the difference of the median annual surface temperature for a given time period versus
    the average of the reference years (1961-1990). The annual surface temperature
    is an ensemble of the temperatures collected globally and blended from the **CRUTEM4**
    surface air temperature and **HadSST3** sea-surface datasets. This data has come
    under attack as biased and unreliable: [http://www.telegraph.co.uk/comment/11561629/Top-scientists-start-to-examine-fiddled-global-warming-figures.html](http://www.telegraph.co.uk/comment/11561629/Top-scientists-start-to-examine-fiddled-global-warming-figures.html).
    This is way outside of our scope of effort here, so we must accept and utilize
    this data as it is. I''ve pulled the data from 1919 March, 1958 through 2013 to
    match our CO2 data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Global CO2 emission estimates can be found at the **Carbon Dioxide Information
    Analysis Center** (**CDIAC**) of the US Department of Energy at the following
    website:  [http://cdiac.ornl.gov/](http://cdiac.ornl.gov/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve placed the data in a `.csv` file (`climate.csv`) for you to download
    and store in your working directory: [https://github.com/datameister66/data/](https://github.com/datameister66/data/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load it and examine the structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will put this in a time series structure, specifying the start
    and end years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With our data loaded and put in time series structures, we can now begin to
    understand and further prepare it for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two packages are required for this effort, so ensure they are installed on
    your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start out with plots of the two time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_08-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It appears that CO2 levels really started to increase after World War II and
    a rapid rise in temperature anomalies in the mid-1970s. There does not appear
    to be any obvious outliers, and variation over time appears constant. Using the
    standard procedure, we can see that the two series are highly correlated, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed earlier, this is nothing to jump for joy about as it proves absolutely
    nothing. We will look for the structure by plotting `ACF` and `PACF` for both
    series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_09-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This code gives us the `PACF` plot for temperature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This code gives us the `ACF` plot for `CO2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_11-2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This code gives us the `PACF` plot for `CO2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_12-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With the slowly decaying ACF patterns and rapidly decaying PACF patterns, we
    can assume that these series are both autoregressive, although temp appears to
    have some significant MA terms. Next, let''s have a look at **Cross Correlation
    Function** (**CCF**). Note that we put our *x* before our *y* in the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_11_13-1.png)'
  prefs: []
  type: TYPE_IMG
- en: '**CCF** shows us the correlation between the temperature and lags of CO2\.
     If the negative lags of the *x* variable have a high correlation, we can say
    that *x* leads *y*. If the positive lags of *x* have a high correlation, we say
    that *x* lags *y*. Here, we can see that CO2 is both a leading and lagging variable.
    For our analysis, it is encouraging that we see the former, but odd that we see
    the latter. We will see during the VAR and Granger causality analysis whether
    this will matter or not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we need to test whether the data is stationary. We can prove
    this with the **Augmented Dickey-Fuller** (**ADF**) test available in the `tseries`
    package, using the `adf.test()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For both series, we have insignificant `p-values`, so we cannot reject the null
    and conclude that they are not stationary.
  prefs: []
  type: TYPE_NORMAL
- en: Having explored the data, let's begin the modeling process, starting with the
    application of univariate techniques to the temperature anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the modeling and evaluation step, we will focus on three tasks. The first
    is to produce a univariate forecast model applied to just the surface temperature.
    The second is to develop a regression model of the surface temperature based on
    itself and CO2 levels, using that output to inform our work on whether CO2 levels Granger
    cause the surface temperature anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Univariate time series forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With this task, the objective is to produce a univariate forecast for the surface
    temperature, focusing on choosing either a Holt linear trend model or an ARIMA
    model. We will train the models and determine their predictive accuracy on an
    out-of-time test set, just like we''ve done in other learning endeavors. The following
    code creates the temperature subset and then the train and test sets, starting
    after WWII:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To build our smoothing model, we will use the `holt()` function found in the
    `forecast` package. We will build two models, one with and one without a damped
    trend. In this function, we will need to specify the time series, number of forecast
    periods as *h = ...*, method to select the initial state values, either `"optimal"`
    or `"simple"`, and whether we want a damped trend. Specifying `"optimal"`, the
    algorithm will find optimal initial starting values along with the smoothing parameters,
    while `"simple"` calculates starting values using the first few observations.
    Now, in the `forecast` package, you can use the `ets()` function, which will find
    all the optimal parameters. However, in our case, let''s stick with `holt()` so
    that we can compare methods. Let''s try the `holt` model without a damped trend,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the `forecast` and see how well it performed out of sample with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_14-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at the plot, it seems that this forecast is showing a slight linear
    uptrend. Let''s have a go by including the `damped` trend, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_15-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, in univariate analysis, we develop an `ARIMA` model, using `auto.arima()`,
    which is also from the `forecast` package. There are many options that you can
    specify in the function, or you can just include your time series data and it
    will find the best `ARIMA` fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The abbreviated output shows that the model selected is an MA = 1, I = 1, or
    `ARIMA(0,1,1)` with drift (equivalent to an intercept term). We can examine the
    plot of its performance on the `test` data in the same fashion as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_16-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is very similar to the `holt` method with no damped trend. We can score
    each model to find the one that provides the lowest error, mean absolute percentage
    error (MAPE), with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The forecast error is slightly less with the ARIMA 0,1,1 versus the `holt` methods,
    and clearly, the damped trend model performed the worst.
  prefs: []
  type: TYPE_NORMAL
- en: With the statistical and visual evidence, it seems that the best choice for
    a univariate forecast model is the ARIMA model. Interestingly, in the first edition
    using annual data, the Holt method with a damped trend had the best accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we've completed the building of a univariate forecast model for the
    surface temperature anomalies, and now we will move on to the next task of seeing
    if CO2 levels cause these anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the causality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, this is where I think the rubber meets the road and we will
    separate causality from mere correlation, well, statistically speaking anyway.
    This is not the first time that this technique has been applied to the problem.
    Triacca (2005) found no evidence to suggest that atmospheric CO2 Granger caused
    the surface temperature anomalies. On the other hand, Kodra (2010) concluded that
    there is a causal relationship, but put forth the caveat that their data was not
    stationary even after a second-order differencing. While this effort will not
    settle the debate, it will hopefully inspire you to apply the methodology in your
    personal endeavors. The topic at hand certainly provides an effective training
    ground to demonstrate the Granger causality.
  prefs: []
  type: TYPE_NORMAL
- en: Our plan here is to first demonstrate spurious linear regression where the residuals
    suffer from autocorrelation, also known as serial correlation. Then, we will examine
    two different approaches to Granger causality. The first will be the traditional
    methods, where both series are stationary. Then, we will look at the method demonstrated
    by Toda and Yamamoto (1995), which applies the methodology to the raw data or,
    as it is sometimes called, the "levels".
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get started with the spurious regression then, which I have seen implemented
    in the real world far too often. Here we simply build a linear model and examine
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Notice how everything is significant, and we have an adjusted R-squared of 0.7\.
    OK, they are highly correlated but this is all meaningless as discussed by Granger
    and Newbold (1974). Again, I have seen results like these presented in meetings
    with many people with advanced degrees, and I had to be the bad guy and challenge
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the serial correlation, starting with a time series plot of the
    residuals, which produce a clear pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_11_17-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we create an ACF plot showing significant autocorrelation out to lag 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_11_18-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can test for autocorrelation by performing the `Durbin-Watson test`. The
    null hypothesis in the test is no autocorrelation exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: From examining the plots, it comes as no surprise that we can safely reject
    the null hypothesis of no autocorrelation. The simple way to deal with autocorrelation
    is to incorporate lagged variables of the dependent time series and/or to make
    all the data stationary. We will do that next using vector autoregression to identify
    the appropriate lag structure to incorporate in our causality efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Vector autoregression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve seen in the preceding section that temperature is stationary and CO2
    requires a first order difference. Another simple way to show this is with the
    `forecast` package''s  `ndiffs()` function. It provides an output that spells
    out the minimum number of differences needed to make the data stationary. In the
    function, you can specify which test out of the three available ones you would
    like to use: **Kwiatkowski, Philips, Schmidt & Shin** (**KPSS**), **Augmented
    Dickey Fuller** (**ADF**), or **Philips-Peron** (**PP**). I will use ADF in the
    following code, which has a null hypothesis that the data is not stationary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that both require a first order difference to become stationary. To
    get started, we will create the difference. Then, we will complete the traditional
    approach, where both series are stationary. Let''s also load our packages for
    this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now a matter of determining the optimal lag structure based on the information
    criteria using vector autoregression. This is done with the `VARselect` function
    in the `vars` package. You only need to specify the data and number of lags in
    the model using `lag.max = x` in the function. Let''s use a maximum of 12 lags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We called the information criteria using `lag$selection`. Four different criteria
    are provided including **AIC**, **Hannan-Quinn Criterion** (**HQ**), **Schwarz-Bayes
    Criterion** (**SC**), and **FPE**. Note that AIC and SC are covered in [Chapter
    2](e29f4d81-7287-41b9-9f2b-0baeffb00a9c.xhtml), *Linear Regression - The Blocking
    and Tackling of Machine Learning*, so I will not go over the criterion formulas
    or differences here. If you want to see the actual results for each lag, you can
    use `lag$criteria`. We can see that `AIC` and `FPE` have selected lag 5 and HQ
    and SC lag 1 as the optimal structure to a `VAR` model. It seems to make sense
    that the 5-year lag is the one to use. We will create that model using the `var()`
    function. I''ll let you try it with lag 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary results are quite lengthy as it builds two separate models and
    would take up probably two whole pages. What I provide is the abbreviated output
    showing the results with temperature as the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The model was significant with a resulting adjusted R-square of 0.35.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did in the previous section, we should check for serial correlation.
    Here, the `VAR` package provides the `serial.test()` function for multivariate
    autocorrelation. It offers several different tests, but let''s focus on ` Portmanteau
    Test`, and also please note that the DW test is for univariate series only. The
    null hypothesis is that autocorrelations are zero and the alternate is that they
    are not zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: With `p-value` at `0.3481`, we do not have evidence to reject the null and can
    say that the residuals are not autocorrelated. What does the test say with 1 lag?
  prefs: []
  type: TYPE_NORMAL
- en: 'To do the Granger causality tests in R, you can use either the `lmtest` package
    and the `Grangertest()` function or the `causality()` function in the `vars` package.
    I''ll demonstrate the technique using `causality()`. It is very easy as you just
    need to create two objects, one for `x` causing `y` and one for `y` causing `x`, utilizing
    the `fit1` object previously created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now just a simple matter to call the Granger test results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `p-value` value for CO2 differences of Granger causing temperature is `0.05908` and
    not significant in the other direction. So what does all this mean? The first
    thing we can say is that Y does not cause X. As for X causing Y, we cannot reject
    the null at the 0.05 significance level and therefore conclude that X does not
    Granger cause Y.  However, is that the relevant conclusion here? Remember, the
    p-value evaluates how likely the effect is if the null hypothesis is true. Also,
    remember that the test was never designed to be some binary Yay or Nay.  If this
    was a controlled experiment, it would be unlikely for us to hesitate to say we
    had insufficient evidence to reject the null, like the **Food and Drug Administration**
    (**FDS**) would do for a phase 3 clinical trial.  Since this study is based on
    observational data, I believe we can say that it is highly probable that "CO2
    emissions Granger cause Surface Temperature Anomalies". But, there is a lot of
    room for criticism on that conclusion. I mentioned upfront the controversy around
    the quality of the data.  The thing that still concerns me is what year to start
    the analysis.  I chose 1945 because it looked about right; you could say I applied
    *proc eyeball* in SAS terminology.  What year is chosen has a dramatic impact
    on the analysis, changing the lag structure and also leading to insignificant
    `p-values`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we still need to model the original CO2 levels using the alternative
    Granger causality technique. The process to find the correct number of lags is
    the same as before, except we do not need to make the data stationary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try the lag 6 structure and see whether we can achieve significance,
    remembering to add one extra lag to account for the integrated series. A discussion
    on the technique and why it needs to be done is available at [http://davegiles.blogspot.de/2011/04/testing-for-granger-causality.html](http://davegiles.blogspot.de/2011/04/testing-for-granger-causality.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now, to determine Granger causality for X causing Y, you conduct a Wald test,
    where the coefficients of X and only X are 0 in the equation to predict Y, remembering
    to not include the extra coefficients that account for integration in the test.
  prefs: []
  type: TYPE_NORMAL
- en: The Wald test in R is available in the `aod` package we've already loaded. We
    need to specify the coefficients of the full model, its variance-covariance matrix,
    and the coefficients of the causative variable.
  prefs: []
  type: TYPE_NORMAL
- en: The coefficients for Temp that we need to test in the VAR object consist of
    a range of even numbers from 2 to 12, while the coefficients for CO2 are odd from
    1 to 11\. Instead of using c(2, 4, 6, and so on) in our function, let's create
    an object with base R's `seq()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s see how CO2 does Granger causing temperature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to run the `wald` test, described in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'How about that? We are close to the magical 0.05 `p-value`. Let''s test the
    other direction causality with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing to show here is how to use a vector autoregression to produce
    a forecast. A `predict` function is available, so let''s `autoplot()` it for a
    25-year period and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/image_11_19.png)'
  prefs: []
  type: TYPE_IMG
- en: It seems dark days lie ahead, perhaps, to coin a phrase, "Winter is Coming" from
    the popular TV series Game of Thrones. Fine by me as my investment and savings
    plan for a long time has consisted of canned goods and ammunition. What else am
    I supposed to do, ride a horse to work? I'll do that the day Al Gore does. In
    the meantime, I am going to work on my suntan.
  prefs: []
  type: TYPE_NORMAL
- en: If nothing else, I hope it has stimulated your thinking on how to apply the
    technique to your own real-world problems or maybe even to examine the climate
    change data in more detail. There should be a high bar when it comes to demonstrating
    causality, and Granger causality is a great tool for assisting in that endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the goal was to discuss how important the element of time is
    in the field of machine learning and analytics, to identify the common traps when
    analyzing the time series, and demonstrate the techniques and methods to work
    around these traps. We explored both the univariate and bivariate time series
    analyses for global temperature anomalies and human carbon dioxide emissions.
    Additionally, we looked at Granger causality to determine whether we can say,
    statistically speaking, that atmospheric CO2 levels cause surface temperature
    anomalies. We discovered that the p-values are higher than 0.05 but less than
    0.10 for Granger causality from CO2 to temperature. It does show that Granger
    causality is an effective tool in investigating causality in machine learning
    problems. In the next chapter, we will shift gears and take a look at how to apply
    learning methods to textual data.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, keep in mind that in time series analysis, we just skimmed the
    surface. I encourage you to explore other techniques around changepoint detection,
    decomposition of time series, nonlinear forecasting, and many others. Although
    not usually considered part of the machine learning toolbox, I believe you will
    find it an invaluable addition to yours.
  prefs: []
  type: TYPE_NORMAL
