- en: Predictive Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Predictive modeling is a process that is using advanced statistics and probability
    algorithms to predict outcomes, based on a pretrained and built model or function.
    These algorithms can be groups in a family of algorithms based on the outcome
    of the predicted variable. The outcome is usually the forecasted value that explains
    the future behavior. Several variables or input data consist of a mathematical
    function, also called the model (hence also data modeling), and these input data
    are trying to explain or predict the outcome. To better understand predictive
    modeling, the chapter will consist of the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced predictive algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and using predictive solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing prediction with R Services in SQL Server database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The focus in this chapter will be on delivering insight into understanding
    how predictive modeling can be used in SQL Server 2016/2017, using R on your typical
    business problem. In the enterprise environment, a business problem can be defined
    in a very broad aspect. For example, in medicine, a typical problem that predictive
    modeling can help understand and solve is, will the change of the ingredient A
    and B for the medicine C, help cure the disease? Furthermore, in the metallurgic
    industry, can we simulate how an anti-corrosion coating paint will age through
    time—or in retails, how can a customer select a better product in a store based
    on their needs or behavior? One can say, our everyday life is intertwined with
    predictions and forecast. Usually, every logistical problem all of us are facing
    is a simple question on a potentially very relevant topic: if I leave home for
    work 5 minutes later, will this affect my driving time if I take one shortcut
    and so on and so forth. Literally, we can say, our everyday decisions are the
    sum of all actions we take with a given output.'
  prefs: []
  type: TYPE_NORMAL
- en: Data modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data modeling is a process where we try to find a function (or the so-called
    model) with a set of independent variables or input data. Just like in data warehousing,
    where modeling is referring to establishing the conceptual framework based on
    the physical data structure and with the help of ORM or UML (even CRC) diagrams
    one explores the structures in data the same is seen with exploring the structures
    when doing predictive analysis. In case of the latter, data modeling is exploring
    the structures (or relations) between two or more variables. These relations can
    be presented as a function and are essentially stored as a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start modeling, we will use some of the Microsoft data available at the
    following GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Microsoft/sql-server-samples/tree/master/samples/features/machine-learning-services/python/getting-started/rental-prediction](https://github.com/Microsoft/sql-server-samples/tree/master/samples/features/machine-learning-services/python/getting-started/rental-prediction)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do not get confused at this Python example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Downloading this database will download the `TutorialDB.bak` file, which you
    simply restore to your SQL Server instance, where R in-database is installed.
    This database is included as part of the accompanying code that comes with this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part of modeling data is to set up the understanding of how predictions at
    a later phase will work. Therefore, in this phase, we will create an understanding
    of the variables and their relation to each other. Create restore from the downloaded
    file and run the following restore from the backup T-SQL command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can simply use the `RESTORE` command in SSMS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.gif)'
  prefs: []
  type: TYPE_IMG
- en: You will now have the database restored and the `dbo.rental_data` table at your
    use. For now, this will be enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the dataset ready, we can now start modeling the data by exploring and
    understanding the variables and the relations among them. This quick exploration
    can be performed in SQL Operation Studio (link to download: [https://docs.microsoft.com/en-us/sql/sql-operations-studio/download](https://docs.microsoft.com/en-us/sql/sql-operations-studio/download)),
    where we will use a simple query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides the standard table view of the results, this will also give a nice
    chart viewer, where a simple graphical representation of variables will give you
    better insights into the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'But without the general understanding of descriptive statistics, we will not
    continue. So, using the `rxSummary` function from the `RevoScaleR` package will
    give the desired results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the results as a simple descriptive statistics table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Exploring the uni- and bi-variate statistics was part of the previous [Chapter
    5](part0081.html#2D7TI0-e3f81285367248f4bbc6431bcd4f926d),* RevoScaleR Package,*
    but here we will focus more on bi- and multi-variate statistics. Before we begin,
    let''s explore the correlations some more. Based on exploring the variable names
    and descriptive statistics, common sense will tell us that during the holidays,
    the rental count should be higher. Checking this can be done using the correlation
    coefficient. The following is a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give you the idea of the bi-variate relationship of `0.332`. This
    is a weak correlation but a positive one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.gif)'
  prefs: []
  type: TYPE_IMG
- en: This simply means that if the `RentalCount` variable gets higher, the number
    of holidays also increases. This indeed makes sense, since if more holidays are
    coming, more rentals are expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can continue exploring and searching for the correlations by combining
    each of the variables. This is similar to making a CROSS JOIN, but there are easier
    ways to do this. One is, of course, by using common sense and selecting the meaningful
    correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And we get the following results as shown in the figure below. Interpretation
    and understanding of the results is of high importance. So, the holiday time is
    by far the most correlative variable with rental count variable. Neither the day
    of the week, nor the year, play any significant role. There is a very tiny, yet
    negative correlation of `-0.110` between `Month` and `RentalCount`, which can
    be understood as higher months might have lower rental counts and vice versa.
    Since this correlation is so weak, it is meaningless to make a fuss over of this
    particular correlation (even if it makes or does not make any sense):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, one can explore the distribution of the values within each variable
    by plotting the boxplots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00093.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The second way is to plot the diagram of correlations between the variables.
    One way to do it is to invoke the `corrplot` R library, which gives you a very
    powerful and useful visualization of correlation. I tend to create the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Code copied and slightly changed from the corrplot lattice documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This procedure can be directly implemented and used in SSRS or in Power BI
    suit or Excel; the visual is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In a single graph, a trained eye will immediately see the correlations and their
    statistical significance. So, the `0.33 RentalCount` and `Holiday` is visible
    here, but also the `RentalCount` and `Snow` is of `0.19` positive correlation.
    But if we want to explore the behavior of the values dispersion (variance), we
    can also include the analysis of variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are working with large datasets or XDF data formats, `RevoScaleR` package
    also comes equipped with functions that compute and calculate correlation matrixes.
    Here is an R code using `rxCovCor` (or, alternatively, one can use `rxCor` or
    `rxCov`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the same results as all the previous calculation of correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.gif)'
  prefs: []
  type: TYPE_IMG
- en: This output also has the ability to see the standard deviations, means and sum
    of weights, but the best part is that it stores the results in a data frame, which
    can be easily imported or used with other T-SQL tables. The results can be invoked
    using `allCov$CovCor` (R language stores the results as an object of lists and
    each list can be retrieved by using a dollar sign `$` and referencing the name
    of the list—in this case, `CovCor`).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we want to further investigate our so-far highest correlation between
    the `RentalCount` and `Holiday`, **Analysis Of Variance **(**ANOVA**) will be
    the appropriate method. We will compare two groups (or levels) of variable `Holiday`
    (`0` is not a holiday while `1 ` is a holiday) and whether there is a difference
    between the rental counts. By doing so, calculating F-statistics and its significance
    will tell us the ratio of between-group variance to within-group variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the T-SQL code with R code for statistical calculation of ANOVA,
    the output result is created in such manner that it returns the F-Statistic and
    statistical significance. The following figure shows results returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00096.gif)'
  prefs: []
  type: TYPE_IMG
- en: Results tell us that the F-Statistic is statistically significant—even though
    it is small—and this means that the means are most likely not equal (and we would
    be, in this case, rejecting the null hypothesis). To find where the difference
    lies, `TukeyHDS` test would give us further information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to illustrate the difference, since we will not go into the details, we
    can use the `stripchart` visualization of the difference between the holiday distribution
    of the rentals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'With the R code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The distribution of the cases can tell us that on holidays an average of `400`
    or higher rental counts are made, whereas on a normal day, there is a huge density
    of counts between `10` and `50`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When determining which features (variables) are good for further analysis and
    predictive algorithms, we can use the Decrease Gini mean calculation. One of the
    Gini mean functions is available in `randomForest` package, so, let''s call the
    function and see which variables are to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'With T-SQL code we are returning the decreased Gini coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.gif)'
  prefs: []
  type: TYPE_IMG
- en: '`Gini` coefficient can also be represented visually as a scatter plot, so that
    the user can immediately determine which variables contribute most to the model.
    For the sake of brevity, the code for this graph is included in the code but not
    in the book.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00099.gif)'
  prefs: []
  type: TYPE_IMG
- en: One can now determine, which of the following variables play any role or contribute
    gain in the model. The **MeanDecreaseGini** was drawn as `varImpPlot(fit_RF)`.
    Technically, this is how one can determine which variables or input parameters
    have the least or most impact, but each of these techniques will give you some
    of the aspects—what can be good in the model, and what may not. Comparing the
    `Holiday` variable in the correlation matrix and mean decrease plot, you can see
    that it gives different methods and different results. Most significant are the
    ones where particular variables do not play any importance whatsoever through
    several different methods.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced predictive algorithms and analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have examined the data preparation and data exploration functions
    available in the `RevoScaleR` package. Besides these functions, predicting classification
    or regression problems can also be done, especially when dealing with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will mention only few of these. The complete list is available online ([https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/revoscaler](https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/revoscaler))
    and some of the points are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rxLinMod`: This is used for building and predicting a linear model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rxLogit`: This is used for building and predicting the logistic regression
    model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rxGlm`: This is used for creating a generalized linear model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rxDTree`: This is used for creating a classification or regression tree'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rxBTrees`: This is used for building a classification or regression decision
    forest—that is using a stochastic gradient boosting algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rxDForest`: This is used for building a classification or regression decision
    forest model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rxNaiveBayes`: This is used for building a Naive Bayes classification model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these algorithms are part of a family of supervised algorithms, where the
    only unsupervised (or undirected) algorithm available in `RevoScaleR` package
    is `rxKMeans`, which is used for dealing with clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same dataset as we did earlier, we plug in and start using `rxLinMod`
    and `rxGlm` for demonstrating how this can be used within T-SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Both will give you the predicted values based on the inputted dataset, along
    with the newly predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00100.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'A curious eye will tell you that the predicted values are far-off from the
    original values. So, the prediction formula in both cases was trying to predict
    variable `RentalCount` based on variables: `Year`, `Month`, `Day`, `WeekDay`,
    `Snow`, and `Holiday`. The formula is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Comparing the variables `RentalCount_Pred` and `RentalCount` will show the difference/offset
    from the real and predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding sample, you can also compare the results of all the three
    algorithms. If you run comparison with all three datasets, observation by observation,
    you can see immediately which algorithms performed best:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, the yellow bar represents the original values, and by far the best hit is
    the decision trees algorithm, given the upper formula and understanding the insights
    of the data. The graph just represents a randomly taken observation. This can
    also be achieved by calculating the accuracy or measure that calculates how much
    deviation had accrued from the original values.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and using predictive solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing the in-database solution and creating it for continuous development
    (and also deployment), several aspects should be taken into consideration. First
    of all, the environment where data scientists will be working. You might give
    them a powerful, standalone server or even allocate proper seats in the cloud.
    They will need it, especially when training the model. This is extremely important,
    as you don't want to have your highly-paid statisticians and mathematicians wait
    for the models to compute and generate. So, enabling the route to a highly scalable
    CPU and RAM powerful computations is a must. Second to this, you have to get the
    data there. Whether it's on cloud or on premises, getting data there (and later
    also, back) should not be overlooked, as this might also be the point where you
    will lose precious time. And, lastly, having the environment set with proper settings,
    environment variables, packages, and all paths to proprietary software enabled
    is also of importance.
  prefs: []
  type: TYPE_NORMAL
- en: '`RevoScaleR` package comes equipped with a function for easily switching the
    computational environments. We will now invoke a simple command in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: By doing this, you can set the local computational environment (that is, the
    client's machine) or the server side, where—in this case—a standalone R Server
    would reside. With a simple function call, the computational context (or simple
    environment) is switched, as course, keeping in mind that all the data resides
    on both sides (so that you avoid the unneeded data transferring) and that all
    the server environment variables are set correctly.
  prefs: []
  type: TYPE_NORMAL
- en: For training the model, several good practices can be chosen. Splitting the
    data for training, testing or training, and testing and validating are several
    practices. Also, a very good practice is to test the percentage of training/testing/validating
    datasets. You might get a 50/50 or 60/40 or 70/30 percentage, but usually you
    carry out and decide this when mining the data. After that, you should also consider
    validation of the data; several aspects are available, from **leave one out**
    (**LOO**) or 10-folds or 5-folds for choosing the validation data for validating
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Not going into the topic too deep, and to make this demo simpler, we can decide
    to do a 70/30 percentage on the spot. Since we have the pleasure of the T-SQL
    database here, we can select and store the training subset in a table, or create
    a view, or decide which 70% we want to take.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This also heavily depends on your business model. The first approach simply
    takes 70% of the data from the original dataset, whereas the second select statement
    takes roughly 70% of the original data, but makes a split based on the year of
    the rental. This might have a crucial impact on how the model will behave and
    also how you want the decision to be affected by this, especially the business
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this is cleared and covered, a best practice is to store the trained model
    in the tables for faster predictions. We will now create a table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Since the `set.seed` is defined, you will always get the same subset, wherever
    you run this code. If you want to get different results, you should comment it
    out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the sampling is done again, based on the problem you are predicting, you
    need to define your prediction formula. In this case, I am using a formula converter
    to create a proper formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Creating a formula through a procedure, making it not hard coded, is also a
    very useful approach, especially in the corporate environment where data scientists
    will set up the pool of independent variables and later the data engineer would
    choose which to include, prior to pushing the data to compute the model and deploy
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The process of bi-variate and multi-variate statistics can also give the data
    engineers and stewards better insights and understanding of what and how the data
    is operating and correlating, and that there are no unwanted correlations or variables
    that just do not function.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this cleared up, we can set up and build the procedures that will run
    the model training and have the models stored in the database. Due to the space
    limits of this chapter, I will only show the creation of one procedure; the rest
    of the procedures can be found in the accompanying chapter materials:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The procedure for random forest in T-SQL would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: I have added something extra to the procedure, such that every time the model
    is trained, an extra is added. This is accuracy, which will also give the data
    engineer and stewards in the later phases a good insight into deciding which model
    outperforms the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can simply run the procedure as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will populate the destination table where the models are kept. The results
    should be stored in the table `[dbo].[Rental_data_models]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.gif)'
  prefs: []
  type: TYPE_IMG
- en: Once this is done, you need to have the evaluation procedure set as well that
    will help determine which model functions the best. However, this part can be
    done using Power BI, or reporting services, or simply just R.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is part of R code that can be included in your visualization tool for
    easier comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The observed values should tell you which model is performing best. Once you
    have done this, you can choose the model and see how the predictions can be done.
  prefs: []
  type: TYPE_NORMAL
- en: Performing predictions with R Services in the SQL Server database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calling stored procedures is the easiest way to organize your code and start
    predicting right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, only a sample will be shown here of how to create a stored procedure
    to predict new datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, you can start predicting using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And, as a result, you will get a predicted value for the variables of `Year`,
    `Month`, `Day`, `WeekDay`, `Holiday`, and `Snow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00103.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Intentionally, the field `OrigPredictedCount` was set to `0`, but the new predicted
    value is the value of `278.996` and that is based on the input variables. While
    checking how the model learned, it is best to also check the original value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that there is no values in month`= 5`, so the model must have it learned
    from other values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00104.gif)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have covered the supervised predictive algorithms, let's quickly
    jump into the cluster—part of the functions that `RevoScaleR` package supports
    as the only undirected algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the example of how to create a simple clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output, which is the presentation of the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00105.gif)'
  prefs: []
  type: TYPE_IMG
- en: To explore the clustering and play with different number of clusters, one thing
    for sure would be to use R code directly or to create a report for exploring the
    characteristics of clusters using Power BI, Excel, or SSRS.
  prefs: []
  type: TYPE_NORMAL
- en: Adding some additional information on cluster centers, statistics as `withinSS`,
    `betweenSS`, `totSS`, and others will also help us to understand the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Scree plot is also an additional and very nice presentation of choosing the
    correct number of clusters. Adding such graphics into a report will also help
    the user choose the right number of clusters and help them understand what and
    how clusters are formed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scree plot R code is used for determining where elbow is happening and does
    whether it has the right number of clusters; if so, three clusters would be an
    optimum number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'On this plot we can see where the elbow is being created and we can determine
    that the best solution is three clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00106.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Putting everything together into a report (SSRS report) makes exploring even
    better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The user can change the number of clusters by selecting the desired number
    and the report will change accordingly. The report is based on three additional
    procedures that export the graphs based on the inputted number of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Running this in SSMS will give you a var binary string, but adding the result
    of this procedure as an image in SSRS or Power BI/Excel will yield a plot derived
    from R.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a nice visualization to your exploratory project upon building a predictive
    analytics system is definitely a very nice wrap-up for business and end users,
    as well as for the data wranglers and engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered the extensible functionalities of the `RevoScaleR`
    package to deliver fast and good predictions based on the explored datasets. In
    the previous chapter *Statistical learning with RevoScaleR package*, we have covered
    data exploration, preparation and simple and bi-variate statistics. This chapter
    showed how `RevoScaleR` package was designed to work with large datasets (that
    overcome the limitations of RAM and single CPU), enabling spill to disk and multi
    threading. The same procedures can be used as well in database instances of R,
    for delivering the predictions to your business and data residing in the database.
    We have covered this aspect as well, exploring different algorithms and comparing
    the solutions. Once you have your model selected, you may want to use the `PREDICT`
    clause. which is a new feature in SQL Server 2017 with a slightly altered architecture.
    Please note that currently (at the time of writing this chapter) the model size
    can not exceed 100 MB if you want to use `PREDICT` clause. Currently, only `RevoScaleR`
    and `MicrosoftML` packages are supported to use this clause, and not even all
    `RevoScaleR` (and MicrosoftML) algorithms are supported—currently supported are
    `rxLinMod`, `rxLogit`, `rxBTrees`, `rxDtree`, `rxdForest`. However, this real-time
    scoring with `PREDICT` clause will definitely develop and evolve in the next release
    of SQL Server.
  prefs: []
  type: TYPE_NORMAL
- en: We need to predict a classification or regression problem. The majority of the
    problems can be supported using `RevoScaleR` package and many of these algorithms
    were also empowered by a new set of additional classifiers available in the `MicrosoftML`
    package. Exploring both packages will give your decision-making a much-needed
    boost. Also, storing serialized models into the database is an optimal way of
    storing and calling trained models (functions) that can be retrained by adding
    a simple logic implementation using SQL Server agents or triggers.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 07](part0102.html#318PC0-e3f81285367248f4bbc6431bcd4f926d), *Operationalizing
    R Code*, you will learn how to operationalize your model and solution and explore
    different ways how to do it and some good practices.
  prefs: []
  type: TYPE_NORMAL
