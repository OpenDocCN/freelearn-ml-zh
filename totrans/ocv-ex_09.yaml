- en: Chapter 9. Learning Object Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about video surveillance, background modeling,
    and morphological image processing. We discussed how we can use different morphological
    operators to apply cool visual effects to input images. In this chapter, we will
    learn how to track an object in a live video. We will discuss the different characteristics
    of an object that can be used to track it. We will also learn about different
    methods and techniques used for object tracking. Object tracking is used extensively
    in robotics, self-driving cars, vehicle tracking, player tracking in sports, video
    compression, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will learn:'
  prefs: []
  type: TYPE_NORMAL
- en: How to track colored objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build an interactive object tracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a corner detector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect good features to track
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build an optical flow-based feature tracker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking objects of a specific color
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build a good object tracker, we need to understand what characteristics
    can be used to make our tracking robust and accurate. So, let's take a baby step
    in this direction, and see how we can use colorspaces to come up with a good visual
    tracker. One thing to keep in mind is that the color information is sensitive
    to lighting conditions. In real-world applications, you need to do some preprocessing
    to take care of this. But for now, let's assume that somebody else is doing this
    and we are getting clean color images.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different colorspaces and picking up a good one will depend on
    what people use for different applications. While RGB is the native representation
    on the computer screen, it's not necessarily ideal for humans. When it comes to
    humans, we give names to colors that are based on their hue. This is why **HSV**
    (**Hue Saturation Value**) is probably one of the most informative colorspaces.
    It closely aligns with how we perceive colors. Hue refers to the color spectrum,
    saturation refers to the intensity of a particular color, and value refers to
    the brightness of that pixel. This is actually represented in a cylindrical format.
    You can refer to a simple explanation about this at [http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html](http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html).
    We can take the pixels of an image to the HSV space and then use colorspace distances
    and threshold in this space thresholding to track a given object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following frame in the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tracking objects of a specific color](img/B04283_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you run it through the colorspace filter and track the object, you will
    see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tracking objects of a specific color](img/B04283_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see here, our tracker recognizes a particular object in the video
    based on its color characteristics. In order to use this tracker, we need to know
    the color distribution of our target object. The following code is used to track
    a colored object that selects only pixels that have a certain given hue. The code
    is well commented, so read the explanation mentioned previously for each line
    to see what''s happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Building an interactive object tracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A colorspace-based tracker gives us the freedom to track a colored object, but
    we are also constrained to a predefined color. What if we just want to randomly
    pick an object? How do we build an object tracker that can learn the characteristics
    of the selected object and track it automatically? This is where the CAMShift
    algorithm, which stands for **Continuously Adaptive Meanshift**, comes into the
    picture. It's basically an improved version of the Meanshift algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of Meanshift is actually nice and simple. Let's say we select a
    region of interest, and we want our object tracker to track that object. In this
    region, we select a bunch of points based on the color histogram and compute the
    centroid of spatial points. If the centroid lies at the center of this region,
    we know that the object hasn't moved. But if the centroid is not at the center
    of this region, then we know that the object is moving in some direction. The
    movement of the centroid controls the direction in which the object is moving.
    So, we move the bounding box of the object to a new location so that the new centroid
    becomes the center of this bounding box. Hence, this algorithm is called Meanshift
    because the mean (that is, the centroid) is shifting. This way, we keep ourselves
    updated with the current location of the object.
  prefs: []
  type: TYPE_NORMAL
- en: However, the problem with Meanshift is that the size of the bounding box is
    not allowed to change. When you move the object away from the camera, the object
    will appear smaller to the human eye, but Meanshift will not take this into account.
    The size of the bounding box will remain the same throughout the tracking session.
    Hence, we need to use CAMShift. The advantage of CAMShift is that it can adapt
    the size of the bounding box to the size of the object. Along with this, it can
    also keep track of the orientation of the object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following figure in which the object is highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an interactive object tracker](img/B04283_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have selected the object, the algorithm computes the histogram
    backprojection and extracts all the information. What is histogram backprojection?
    It''s just a way of identifying how well the image fits into our histogram model.
    We compute the histogram model of a particular thing, and then use this model
    to find that thing in an image. Let''s move the object and see how it gets tracked:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an interactive object tracker](img/B04283_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looks like the object is getting tracked fairly well. Let''s change the orientation,
    and check whether the tracking is maintained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an interactive object tracker](img/B04283_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the bounding ellipse has changed its location as well as its
    orientation. Let''s change the perspective of the object, and see whether it''s
    still able to track it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building an interactive object tracker](img/B04283_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We are still good! The bounding ellipse has changed the aspect ratio to reflect
    the fact that the object looks skewed now (because of the perspective transformation).
    Let''s take a look at the user interface functionality in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function basically captures the coordinates of the rectangle that were
    selected in the window. The user just needs to click on them and drag them with
    the mouse. There are a set of inbuilt functions in OpenCV that help us detect
    these different mouse events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code used to perform object tracking based on CAMShift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the HSV image waiting to be processed at this point. Let''s go
    ahead and see how we can use our thresholds to process this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see here, we use the HSV image to compute the histogram of the region.
    We use our thresholds to locate the required color in the HSV spectrum and then
    filter out the image based on that. Let''s go ahead and see how we can compute
    the histogram backprojection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to display the results. Using the rotated rectangle, let''s
    draw an ellipse around our region of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Detecting points using the Harris corner detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Corner detection is a technique used to detect *interest points* in the image.
    These interest points are also called *feature points* or simply *features* in
    Computer Vision terminology. A corner is basically an intersection of two edges.
    An *interest point* is basically something that can be uniquely detected in an
    image. A corner is a particular case of an interest point. These interest points
    help us characterize an image. These points are used extensively in applications,
    such as object tracking, image classification, visual search, and so on. Since
    we know that the corners are *interesting*, let's see how we can detect them.
  prefs: []
  type: TYPE_NORMAL
- en: In Computer Vision, there is a popular corner detection technique called the
    Harris corner detector. We construct a 2 x 2 matrix based on partial derivatives
    of the grayscale image, and then analyze the eigenvalues. Now what does this mean?
    Well, let's dissect it so that we can understand it better. Let's consider a small
    patch in the image. Our goal is to check whether this patch has a corner in it.
    So, we consider all the neighboring patches and compute the intensity difference
    between our patch and all those neighboring patches. If the difference is high
    in all directions, then we know that our patch has a corner in it. This is actually
    an oversimplification of the actual algorithm, but it covers the gist. If you
    want to understand the underlying mathematical details, you can take a look at
    the original paper by Harris and Stephens at [http://www.bmva.org/bmvc/1988/avc-88-023.pdf](http://www.bmva.org/bmvc/1988/avc-88-023.pdf).
    A corner point is a point where both the eigenvalues would have large values.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the Harris corner detector, it will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting points using the Harris corner detector](img/B04283_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the green circles on the TV remote are the detected corners.
    This will change based on the parameters you choose for the detector. If you modify
    the parameters, you can see that more points might get detected. If you make it
    strict, then you might not be able to detect soft corners. Let''s take a look
    at the following code to detect Harris corners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We converted the image to grayscale and detected corners using our parameters.
    You can find the complete code in the `.cpp` files. These parameters play an important
    role in the number of points that will be detected. You can check out the OpenCV
    documentation of the Harris corner detector at [http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=cornerharris#void
    cornerHarris(InputArray src, OutputArray dst, int blockSize, int ksize, double
    k, int borderType](http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=cornerharris#voidcornerHarris%28InputArraysrc,OutputArraydst,intblockSize,intksize,doublek,intborderType%29)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have all the information that we need. Let''s go ahead and draw circles
    around our corners to display the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this code takes a `blockSize` input argument. Depending on the
    size you choose, the performance will vary. Start with a value of `4` and play
    around with it to see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: Shi-Tomasi Corner Detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Harris corner detector performs well in many cases, but it can still be
    improved. Around six years after the original paper by Harris and Stephens, Shi-Tomasi
    came up with something better and they called it *Good Features To Track*. You
    can read the original paper at: [http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf).
    They used a different scoring function to improve the overall quality. Using this
    method, we can find the *N* strongest corners in the given image. This is very
    useful when we don''t want to use every single corner to extract information from
    the image. As discussed earlier, a good interest point detector is very useful
    in applications, such as object tracking, object recognition, image search, and
    so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you apply the Shi-Tomasi corner detector to an image, you will see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shi-Tomasi Corner Detector](img/B04283_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see here, all the important points in the frame are captured. Let''s
    take a look at the following code to track these features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We extracted the frame and used `goodFeaturesToTrack` to detect the corners.
    It''s important to understand that the number of corners detected will depend
    on our choice of parameters. You can find a detailed explanation at [http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack](http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack).
    Let''s go ahead and draw circles on these points to display the output image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This program takes a `numCorners` input argument. This value indicates the maximum
    number of corners you want to track. Start with a value of `100` and play around
    with it to see what happens. If you increase this value, you will see more feature
    points getting detected.
  prefs: []
  type: TYPE_NORMAL
- en: Feature-based tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature-based tracking refers to tracking individual feature points across successive
    frames in the video. The advantage here is that we don't have to detect feature
    points in every single frame. We can just detect them once and keep tracking them
    after that. This is more efficient as compared to running the detector on every
    frame. We use a technique called optical flow to track these features. Optical
    flow is one of the most popular techniques in Computer Vision. We choose a bunch
    of feature points, and track them through the video stream. When we detect the
    feature points, we compute the displacement vectors and show the motion of those
    keypoints between consecutive frames. These vectors are called motion vectors.
  prefs: []
  type: TYPE_NORMAL
- en: A motion vector for a particular point is just a directional line that indicates
    where that point has moved as compared to the previous frame. Different methods
    are used to detect these motion vectors. The two most popular algorithms are the
    Lucas-Kanade method and Farneback algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The Lucas-Kanade method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Lucas-Kanade method is used for sparse optical flow tracking. By sparse,
    we mean that the number of feature points is relatively low. You can refer to
    their original paper at [http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf](http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf).
    We start the process by extracting the feature points. For each feature point,
    we create 3 x 3 patches with the feature point at the center. We assume that all
    the points within each patch will have a similar motion. We can adjust the size
    of this window, depending on the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: For each feature point in the current frame, we take the surrounding 3 x 3 patch
    as our reference point. For this patch, we take a look at its neighborhood in
    the previous frame to get the best match. This neighborhood is usually bigger
    than 3 x 3 because we want to get the patch that's closest to the patch under
    consideration. Now, the path from the center pixel of the matched patch in the
    previous frame to the center pixel of the patch under consideration in the current
    frame will become the motion vector. We do this for all the feature points, and
    extract all the motion vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the following frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas-Kanade method](img/B04283_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to add some points that we want to track. Just go ahead and click on
    a bunch of points on this window with your mouse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas-Kanade method](img/B04283_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If I move to a different position, you will see that the points are still being
    tracked correctly within a small margin of error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas-Kanade method](img/B04283_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s add a lot of points and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas-Kanade method](img/B04283_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, it will keep tracking those points. However, you will notice
    that some of the points will be dropped in between because of factors, such as
    prominence, speed of the movement, and so on. If you want to play around with
    it, you can just keep adding more points to it. You can also allow the user to
    select a region of interest in the input video. You can then extract feature points
    from this region of interest and track the object by drawing the bounding box.
    It will be a fun exercise!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code used to perform Lucas-Kanade-based tracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the current image and the previous image to compute the optical flow
    information. Needless to say that the quality of the output will depend on the
    parameters you have chosen. You can find more details about the parameters at
    [http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk](http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk).
    To increase the quality and robustness, we need to filter out the points that
    are very close to each other because they do not add the new information. Let''s
    go ahead and do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the tracking points. The next step is to refine the location of
    these points. What exactly does "refine" mean in this context? To increase the
    speed of computation, there is some level of quantization involved. In layman''s
    terms, you can think of it as "rounding off". Now that we have the approximate
    region, we can refine the location of the point within that region to get a more
    accurate outcome. Let''s go ahead and do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The Farneback algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gunnar Farneback proposed this optical flow algorithm and it's used for dense
    tracking. Dense tracking is used extensively in robotics, augmented reality, 3D
    mapping, and so on. You can check out the original paper at [http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf).
    The Lucas-Kanade method is a sparse technique, which means that we only need to
    process some pixels in the entire image. The Farneback algorithm, on the other
    hand, is a dense technique that requires us to process all the pixels in the given
    image. So, obviously, there is a trade-off here. Dense techniques are more accurate,
    but they are slower. Sparse techniques are less accurate, but they are faster.
    For real-time applications, people tend to prefer sparse techniques. For applications
    where time and complexity is not a factor, people prefer dense techniques to extract
    finer details.
  prefs: []
  type: TYPE_NORMAL
- en: In his paper, Farneback describes a method for dense optical flow estimation
    based on polynomial expansion for two frames. Our goal is to estimate the motion
    between these two frames, and it's basically a three-step process. In the first
    step, each neighborhood in both the frames is approximated by polynomials. In
    this case, we are only interested in quadratic polynomials. The next step is to
    construct a new signal by global displacement. Now that each neighborhood is approximated
    by a polynomial, we need to see what happens if this polynomial undergoes an ideal
    translation. The last step is to compute the global displacement by equating the
    coefficients in the yields of these quadratic polynomials.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how is this feasible? If you think about it, we are assuming that an entire
    signal is a single polynomial and there is a global translation relating the two
    signals. This is not a realistic scenario. So, what are we looking for? Well,
    our goal is to find out whether these errors are small enough so that we can build
    a useful algorithm that can track the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following static image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Farneback algorithm](img/B04283_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If I move sideways, you can see that the motion vectors point in the horizontal
    direction. They simply track the movement of my head:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Farneback algorithm](img/B04283_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If I move away from the webcam, you can see that the motion vectors point in
    a direction that is perpendicular to the image plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Farneback algorithm](img/B04283_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is the code used to perform optical flow-based tracking using the Farneback
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we use the Farneback algorithm to compute the optical flow
    vectors. The `calcOpticalFlowFarneback` input parameters are important when it
    comes to the quality of tracking. You can find the details about these parameters
    at [http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html](http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html).
    Let''s go ahead and draw those vectors on the output image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We used a function called `drawOpticalFlow` to draw these optical flow vectors.
    These vectors indicate the direction of the motion. Let''s take a look at the
    function to see how we can draw these vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about object tracking. We learned how to use the
    HSV colorspace to track colored objects. We discussed clustering techniques used
    for object tracking and how we can build an interactive object tracker using the
    CAMShift algorithm. We learned about corner detectors and how to track corners
    in a live video. We discussed how to track features in a video using optical flow.
    We also learned the concepts behind Lucas-Kanade and Farneback algorithms and
    implemented them as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss segmentation algorithms and see how we
    can use them for text recognition.
  prefs: []
  type: TYPE_NORMAL
