- en: '*Chapter 1*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*'
- en: Introduction to Clustering Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类方法简介
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Describe the uses of clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述聚类的用途
- en: Perform the k-means algorithm using built-in R libraries
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置的R库执行k-means算法
- en: Perform the k-medoids algorithm using built-in R libraries
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置的R库执行k-medoids算法
- en: Determine the optimum number of clusters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定最佳聚类数量
- en: In this chapter, we will have a look at the concept of clustering and some basic
    clustering algorithms.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨聚类的概念和一些基本的聚类算法。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: 'The 21st century is the digital century, where every person on every rung of
    the economic ladder is using digital devices and producing data in digital format
    at an unprecedented rate. 90% of data generated in the last 10 years was generated
    in the last 2 years. This is an exponential rate of growth, where the amount of
    data is increasing by 10 times every 2 years. This trend is expected to continue
    for the foreseeable future:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 21世纪是数字世纪，在这个世纪里，经济阶梯上的每个人都在以前所未有的速度使用数字设备并以数字格式产生数据。在过去10年中产生的90%的数据是在过去两年中产生的。这是一个指数增长的趋势，数据量每两年增加10倍。预计这种趋势将在可预见的未来继续：
- en: '![Figure 1.1: Increase in data year on year'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1：年度数据增长'
- en: '](img/C12628_01_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/C12628_01_01.jpg)'
- en: 'Figure 1.1: The increase in digital data year on year'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.1：年度数字数据增长
- en: But this data is not just stored in hard drives; it's being used to make lives
    better. For example, Google uses the data it has to serve you better results,
    and Netflix uses the data it has to serve you better movie recommendations. In
    fact, their decision to make their hit show *House of Cards* was based on analytics.
    IBM is using the medical data it has to create an artificially intelligent doctor
    and to detect cancerous tumors from x-ray images.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些数据不仅仅存储在硬盘上；它们正在被用来让生活变得更好。例如，谷歌使用它拥有的数据为你提供更好的搜索结果，Netflix使用它拥有的数据为你提供更好的电影推荐。事实上，他们制作热门剧集《纸牌屋》的决定是基于数据分析的。IBM正在使用它拥有的医疗数据创建一个人工智能医生，并从X光图像中检测癌细胞。
- en: 'To process this amount of data with computers and come up with relevant results,
    a particular class of algorithms is used. These algorithms are collectively known
    as machine learning algorithms. Machine learning is divided into two parts, depending
    on the type of data that is being used: one is called **supervised** learning
    and the other is called **unsupervised learning**.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用计算机处理如此大量的数据并得出相关结果，使用特定类别的算法。这些算法统称为机器学习算法。机器学习根据所使用的数据类型分为两部分：一种称为**监督学习**，另一种称为**无监督学习**。
- en: Supervised learning is done when we get labeled data. For example, say we get
    1,000 images of x-rays from a hospital that are labeled as normal or fractured.
    We can use this data to train a machine learning model to predict whether an x-ray
    image shows a fractured bone or not.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们获得标记数据时，进行监督学习。例如，假设我们从一家医院获得了1,000张标记为正常或骨折的X光片。我们可以使用这些数据来训练一个机器学习模型，以预测X光片是否显示骨折的骨头。
- en: Unsupervised learning is when we just have raw data and are expected to come
    up with insights without any labels. We have the ability to understand the data
    and recognize patterns in it without explicitly being told what patterns to identify.
    By the end of this book, you're going to be aware of all of the major types of
    unsupervised learning algorithms. In this book, we're going to be using the R
    programming language for demonstration, but the algorithms are the same for all
    languages.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是在我们只有原始数据并期望在没有标签的情况下得出见解时进行的。我们有能力理解数据并识别其中的模式，而无需明确告知要识别哪些模式。到本书结束时，你将了解所有主要类型的无监督学习算法。在本书中，我们将使用R编程语言进行演示，但算法对所有语言都是相同的。
- en: In this chapter, we're going to study the most basic type of unsupervised learning,
    **clustering**. At first, we're going to study what clustering is, its types,
    and how to create clusters with any type of dataset. Then we're going to study
    how each type of clustering works, looking at their advantages and disadvantages.
    At the end, we're going to learn when to use which type of clustering.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究最基本的无监督学习类型，**聚类**。首先，我们将研究聚类是什么，它的类型，以及如何使用任何类型的数据集创建聚类。然后我们将研究每种聚类类型的工作原理，查看它们的优缺点。最后，我们将学习何时使用哪种类型的聚类。
- en: Introduction to Clustering
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类简介
- en: 'Clustering is a set of methods or algorithms that are used to find natural
    groupings according to predefined properties of variables in a dataset. The Merriam-Webster
    dictionary defines a cluster as "a number of similar things that occur together."
    Clustering in unsupervised learning is exactly what it means in the traditional
    sense. For example, how do you identify a bunch of grapes from far away? You have
    an intuitive sense without looking closely at the bunch whether the grapes are
    connected to each other or not. Clustering is just like that. An example of clustering
    is presented here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一组用于根据数据集中变量的预定义属性找到自然分组的方法或算法。Merriam-Webster词典将聚类定义为“一起发生的一组相似事物。”无监督学习中的聚类在传统意义上正是这个意思。例如，你如何从远处识别一串葡萄？你不需要仔细看这串葡萄就能直观地感觉到葡萄是否相互连接。聚类就是这样。下面提供了一个聚类的例子：
- en: '![Figure 1.2: Representation of two clusters in a dataset'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2：数据集中两个聚类的表示'
- en: '](img/C12628_01_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_02.jpg)'
- en: 'Figure 1.2: A representation of two clusters in a dataset'
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.2：数据集中两个聚类的表示
- en: 'In the preceding graph, the data points have two properties: cholesterol and
    blood pressure. The data points are classified into two clusters, or two bunches,
    according to the **Euclidean** distance between them. One cluster contains people
    who are clearly at high risk of heart disease and the other cluster contains people
    who are at low risk of heart disease. There can be more than two clusters, too,
    as in the following example:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，数据点有两个属性：胆固醇和血压。根据它们之间的**欧几里得**距离，数据点被分类为两个聚类，或两个串。一个聚类包含明显有心脏病高风险的人，另一个聚类包含心脏病风险低的人。也可以有超过两个聚类，如下面的例子所示：
- en: '![Figure 1.3: Representation of three clusters in a dataset'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.3：数据集中三个聚类的表示'
- en: '](img/C12628_01_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_03.jpg)'
- en: 'Figure 1.3: A representation of three clusters in a dataset'
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.3：数据集中三个聚类的表示
- en: In the preceding graph, there are three clusters. One additional group of people
    has high blood pressure but with low cholesterol. This group may or may not have
    a risk of heart disease. In further sections, clustering will be illustrated on
    real datasets in which the x and y coordinates denote actual quantities.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，有三个聚类。一个额外的人群有高血压但胆固醇低。这个群体可能或可能没有心脏病风险。在接下来的章节中，将在实际数据集上展示聚类，其中x和y坐标表示实际数量。
- en: Uses of Clustering
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类的应用
- en: Like all methods of unsupervised learning, clustering is mostly used when we
    don't have labeled data – data with predefined classes – for training our models.
    Clustering uses various properties, such as Euclidean distance and **Manhattan**
    **distance**, to find patterns in the data and classify them according to similarities
    in their properties without having any labels for training. So, clustering has
    many use cases in fields where labeled data is unavailable or we want to find
    patterns that are not defined by labels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有无监督学习方法一样，聚类通常在我们没有标记数据——即具有预定义类别的数据——用于训练模型时使用。聚类使用各种属性，如欧几里得距离和**曼哈顿****距离**，来寻找数据中的模式并将它们根据其属性的相似性进行分类，而不需要任何用于训练的标签。因此，聚类在标记数据不可用或我们想要找到由标签未定义的模式的应用领域中有许多用例。
- en: 'The following are some applications of clustering:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些聚类的应用：
- en: '**Exploratory data analysis**: When we have unlabeled data, we often do clustering
    to explore the underlying structure and categories of the dataset. For example,
    a retail store might want to explore how many different segments of customers
    they have, based on purchase history.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索性数据分析**：当我们有未标记的数据时，我们通常会进行聚类以探索数据集的潜在结构和类别。例如，一家零售店可能想要根据购买历史来探索他们有多少不同的客户细分市场。'
- en: '**Generating training data**: Sometimes, after processing unlabeled data with
    clustering methods, it can be labeled for further training with supervised learning
    algorithms. For example, two different classes that are unlabeled might form two
    entirely different clusters, and using their clusters, we can label data for further
    supervised learning algorithms that are more efficient in real-time classification
    than our unsupervised learning algorithms.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成训练数据**：有时，在用聚类方法处理未标记数据后，它可以被标记以进一步使用监督学习算法进行训练。例如，两个未标记的不同类别可能形成两个完全不同的聚类，我们可以使用它们的聚类来为更有效的实时分类的监督学习算法标记数据，这些算法比我们的无监督学习算法更有效。'
- en: '**Recommender systems**: With the help of clustering, we can find the properties
    of similar items and use these properties to make recommendations. For example,
    an e-commerce website, after finding customers in the same clusters, can recommend
    items to customers in that cluster based upon the items bought by other customers
    in that cluster.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐系统**：借助聚类，我们可以找到相似物品的特性，并利用这些特性进行推荐。例如，一个电子商务网站，在找到同一集群的客户后，可以根据该集群中其他客户购买的物品向该集群中的客户推荐商品。'
- en: '**Natural language processing**: Clustering can be used for the grouping of
    similar words, texts, articles, or tweets, without labeled data. For example,
    you might want to group articles on the same topic automatically.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言处理**：聚类可以用于对相似词语、文本、文章或推文的分组，无需标签数据。例如，你可能希望自动将同一主题的文章分组。'
- en: '**Anomaly detection**: You can use clustering to find outliers. We''re going
    to learn about this in *Chapter 6*, *Anomaly Detection*. Anomaly detection can
    also be used in cases where we have unbalanced classes in data, such as in the
    case of the detection of fraudulent credit card transactions.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：你可以使用聚类来找到异常值。我们将在第 6 章 *异常检测* 中学习这一点。异常检测也可以用于数据中存在不平衡类别的情况，例如在欺诈信用卡交易检测中。'
- en: Introduction to the Iris Dataset
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 爱丽丝数据集简介
- en: In this chapter, we're going to use the Iris flowers dataset in exercises to
    learn how to classify three species of Iris flowers (Versicolor, Setosa, and Virginica)
    without using labels. This dataset is built-in to R and is very good for learning
    about the implementation of clustering techniques.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用爱丽丝花朵数据集进行练习，学习如何在不使用标签的情况下对三种爱丽丝花朵（Versicolor、Setosa 和 Virginica）进行分类。这个数据集是
    R 内置的，非常适合学习聚类技术的实现。
- en: Note that in our exercise dataset, we have final labels for the flowers. We're
    going to compare clustering results with those labels. We choose this dataset
    just to demonstrate that the results of clustering make sense. In the case of
    datasets such as the wholesale customer dataset (covered later in the book), where
    we don't have final labels, the results of clustering cannot be objectively verified
    and therefore might lead to misguided conclusions. That's the kind of use case
    where clustering is used in real life when we don't have final labels for the
    dataset. This point will be clearer once you have done both the exercises and
    activities.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在我们的练习数据集中，我们为花朵有最终标签。我们将比较聚类结果与这些标签。我们选择这个数据集只是为了展示聚类结果是有意义的。在数据集如批发客户数据集（本书后面将介绍）的情况下，我们没有最终标签，聚类结果无法客观验证，因此可能会导致错误的结论。这就是在实际生活中没有数据集的最终标签时使用聚类的用例。一旦你完成了这两个练习和活动，这一点将更加清晰。
- en: 'Exercise 1: Exploring the Iris Dataset'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 1：探索爱丽丝数据集
- en: 'In this exercise, we''re going to learn how to use the Iris dataset in R. Assuming
    you already have R installed in your system, let''s proceed:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将学习如何在 R 中使用爱丽丝数据集。假设你已经在系统中安装了 R，让我们继续：
- en: 'Load the Iris dataset into a variable as follows:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式将爱丽丝数据集加载到变量中：
- en: '[PRE0]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that our Iris data is in the `iris_data` variable, we can have a look at
    its first few rows by using the `head` function in R:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们的爱丽丝数据已经存储在 `iris_data` 变量中，我们可以使用 R 中的 `head` 函数查看其前几行：
- en: '[PRE1]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.4: First six rows of the Iris dataset'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.4：爱丽丝数据集的前六行'
- en: '](img/C12628_01_04.jpg)'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/C12628_01_04.jpg](img/C12628_01_04.jpg)'
- en: 'Figure 1.4: The first six rows of the Iris dataset'
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.4：爱丽丝数据集的前六行
- en: We can see our dataset has five columns. We're mostly going to use two columns
    for ease of visualization in plots of two dimensions.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的数据集有五个列。我们将主要使用两个列来简化二维图的可视化。
- en: Types of Clustering
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类类型
- en: 'As stated previously, clustering algorithms find natural groupings in data.
    There are many ways in which we can find natural groupings in data. The following
    are the methods that we''re going to study in this chapter:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，聚类算法可以在数据中找到自然分组。我们可以以多种方式在数据中找到自然分组。以下是我们将在本章中研究的方法：
- en: k-means clustering
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means 聚类
- en: k-medoids clustering
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-medoids 聚类
- en: 'Once the concepts related to the basic types of clustering are clear, we will
    have a look at other types of clustering, which are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦基本聚类类型的概念清晰，我们将探讨其他类型的聚类，如下所示：
- en: k-modes
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-modes
- en: Density-based clustering
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于密度的聚类
- en: Agglomerative hierarchical clustering
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类层次聚类
- en: Divisive clustering
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分裂聚类
- en: Introduction to k-means Clustering
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means聚类简介
- en: 'K-means clustering is one of the most basic types of unsupervised learning
    algorithm. This algorithm finds natural groupings in accordance with a predefined
    similarity or distance measure. The distance measure can be any of the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类是最基本的无监督学习算法之一。这个算法根据预定义的相似度或距离度量找到自然分组。距离度量可以是以下任何一种：
- en: Euclidean distance
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: Manhattan distance
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曼哈顿距离
- en: Cosine distance
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦距离
- en: Hamming distance
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汉明距离
- en: To understand what a distance measure does, take the example of a bunch of pens.
    You have 12 pens. Six of them are blue, and six red. Six of them are ball point
    pens and six are ink pens. If you were to use ink color as a similarity measure,
    then six blue pens and six red pens will be in different clusters. The six blue
    pens can be ink or ball point here; there's no restriction on that. But if you
    were to use the type of pen as the similarity measure, then the six ink pens and
    six ball point pens would be in different clusters. Now it doesn't matter whether
    the pens in each cluster are of the same color or not.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解距离度量做什么，以一束笔为例。你有12支笔。其中6支是蓝色的，6支是红色的。其中6支是圆珠笔，6支是墨水笔。如果你要用墨水颜色作为相似度度量，那么6支蓝色笔和6支红色笔将位于不同的簇中。这里的6支蓝色笔可以是墨水笔或圆珠笔，没有限制。但如果你要用笔的类型作为相似度度量，那么6支墨水笔和6支圆珠笔将位于不同的簇中。现在，每个簇中的笔是否颜色相同并不重要。
- en: Euclidean Distance
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'Euclidean distance is the straight-line distance between any two points. Calculation
    of this distance in two dimensions can be thought of an extension of the Pythagorean
    theorem, which you might have studied in school. But Euclidean distance can be
    calculated between two points in any n-dimensional space, not just a two-dimensional
    space. The Euclidean distance between any two points is the square root of the
    sum of squares of differences between the coordinates. An example of the calculation
    of Euclidean distance is presented here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离是任意两点之间的直线距离。在二维空间中计算这个距离可以被视为你在学校可能学过的勾股定理的扩展。但欧几里得距离可以在任何n维空间中计算，而不仅仅是二维空间。任意两点之间的欧几里得距离是它们坐标差的平方和的平方根。这里展示了欧几里得距离计算的例子：
- en: '![Figure 1.5: Representation of Euclidean distance calculation'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.5：欧几里得距离计算的表示'
- en: '](img/C12628_01_05.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/C12628_01_05.jpg]'
- en: 'Figure 1.5: Representation of Euclidean distance calculation'
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.5：欧几里得距离计算的表示
- en: In k-means clustering, Euclidean distance is used. One disadvantage of using
    Euclidean distance is that it loses its meaning when the dimensionality of data
    is very high. This is related to a phenomenon known as the curse of dimensionality.
    When datasets possess many dimensions, they can be harder to work with, since
    distances between all points can become extremely high, and the distances are
    difficult to interpret and visualize.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means聚类中，使用欧几里得距离。使用欧几里得距离的一个缺点是，当数据的维度非常高时，它就失去了意义。这与一个被称为维度诅咒的现象有关。当数据集具有许多维度时，它们可能更难处理，因为所有点之间的距离都可能变得极高，而这些距离难以解释和可视化。
- en: So, when the dimensionality of data is very high, either we reduce its dimensions
    with principal component analysis, which we're going to study in *Chapter 4*,
    *Dimension Reduction*, or we use cosine similarity.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当数据的维度非常高时，我们要么通过主成分分析来降低其维度，我们将在*第4章*，*降维*中学习到这一方法，要么使用余弦相似度。
- en: Manhattan Distance
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 曼哈顿距离
- en: 'By definition, Manhattan distance is the distance between two points measured
    along a right angle to the axes:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，曼哈顿距离是沿着与坐标轴成直角测量的两点之间的距离：
- en: '![Figure 1.6: Representation of Manhattan distance'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.6：曼哈顿距离的表示'
- en: '](img/C12628_01_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/C12628_01_06.jpg]'
- en: 'Figure 1.6: Representation of Manhattan distance'
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.6：曼哈顿距离的表示
- en: The length of the diagonal line is the Euclidean distance between the two points.
    Manhattan distance is simply the sum of the absolute value of the differences
    between two coordinates. So, the main difference between Euclidean distance and
    Manhattan distance is that with Euclidean distance, we square the distances between
    coordinates and then take the root of the sum, but in Manhattan distance, we directly
    take the sum of the absolute value of the differences between coordinates.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对角线的长度是两点之间的欧几里得距离。曼哈顿距离简单地说就是两个坐标之间差的绝对值的总和。因此，欧几里得距离和曼哈顿距离的主要区别在于，在欧几里得距离中，我们平方坐标之间的距离，然后取和的根，但在曼哈顿距离中，我们直接取坐标之间差的绝对值的总和。
- en: Cosine Distance
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 余弦距离
- en: 'Cosine similarity between any two points is defined as the cosine of the angle
    between any two points with the origin as its vertex. It can be calculated by
    dividing the dot product of any two vectors by the product of the magnitudes of
    the vectors:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 任意两点之间的余弦相似度定义为以原点为顶点的任意两点之间的角度的余弦值。它可以通过将任意两个向量的点积除以向量的模的乘积来计算：
- en: '![Figure 1.7: Representation of cosine similarity and cosine distance'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.7：余弦相似度和余弦距离的表示'
- en: '](img/C12628_01_07.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_07.jpg)'
- en: 'Figure 1.7: Representation of cosine similarity and cosine distance'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.7：余弦相似度和余弦距离的表示
- en: Cosine distance is defined as (1-cosine similarity).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦距离定义为（1-余弦相似度）。
- en: Cosine distance varies from 0 to 2, whereas cosine similarity varies between
    -1 to 1\. Always remember that cosine similarity is one minus the value of of
    the cosine distance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦距离的范围是0到2，而余弦相似度的范围在-1到1之间。始终记住，余弦相似度是余弦距离的值的倒数。
- en: The Hamming Distance
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汉明距离
- en: 'The Hamming distance is a special type of distance that is used for categorical
    variables. Given two points of equal dimensions, the Hamming distance is defined
    as the number of coordinates differing from one another. For example, let''s take
    two points, (0, 1, 1) and (0, 1, 0). Only one value, which is the last value,
    is different between these two variables. As such, the Hamming distance between
    them is 1:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 汉明距离是一种特殊的距离，用于分类变量。给定两个维度相等的点，汉明距离定义为彼此不同的坐标的数量。例如，让我们取两个点（0，1，1）和（0，1，0）。这两个变量之间只有一个值不同，即最后一个值。因此，它们之间的汉明距离是1：
- en: '![Figure 1.8: Representation of the Hamming distance'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.8：汉明距离的表示'
- en: '](img/C12628_01_08.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_08.jpg)'
- en: 'Figure 1.8: Representation of the Hamming distance'
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.8：汉明距离的表示
- en: k-means Clustering Algorithm
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means聚类算法
- en: 'K-means clustering is used to find clusters in a dataset of similar points
    when we have unlabeled data. In this chapter, we''re going to use the Iris flowers
    dataset. This dataset contains information about the length and breadth of sepals
    and petals of flowers of different species. With the help of unsupervised learning,
    we''re going to learn how to differentiate between them without knowing which
    properties belong to which species. The following is the scatter plot of our dataset:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类用于在未标记的数据集的相似点数据集中找到簇。在本章中，我们将使用鸢尾花数据集。这个数据集包含了不同物种的花瓣长度和宽度的信息。借助无监督学习，我们将学习如何在不了解哪些属性属于哪个物种的情况下区分它们。以下是我们数据集的散点图：
- en: '![Figure 1.9: Scatter plot of the Iris flowers dataset'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.9：鸢尾花数据集的散点图'
- en: '](img/C12628_01_09.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_09.jpg)'
- en: 'Figure 1.9: A scatter plot of the Iris flowers dataset'
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.9：鸢尾花数据集的散点图
- en: 'This is the scatter plot of two variables of the Iris flower dataset: sepal
    length and sepal width.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是鸢尾花数据集两个变量的散点图：花瓣长度和花瓣宽度。
- en: If we were to identify the clusters in the preceding dataset according to the
    distance between the points, we would choose clusters that look like bunches of
    grapes hanging from a tree. You can see that there are two major bunches (one
    in the top left and the other being the remaining points). The k-means algorithm
    identifies these "bunches" of grapes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要根据点之间的距离来识别前一个数据集中的簇，我们会选择看起来像挂在树上的葡萄串的簇。你可以看到有两个主要的大串（一个在左上角，另一个是剩余的点）。k-means算法识别这些“葡萄串”。
- en: 'The following figure shows the same scatter plot, but with the three different
    species of Iris shown in different colors. These species are taken from the ''species''
    column of the original dataset, and are as follows: Iris setosa (shown in green),
    Iris versicolor (shown in red), and Iris virginica (shown in blue). We''re going
    to see whether we can determine these species by forming our own classifications
    using clustering:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了相同的散点图，但用不同颜色显示了三种不同的鸢尾花品种。这些品种来自原始数据集的“species”列，具体如下：鸢尾花setosa（用绿色表示），鸢尾花versicolor（用红色表示），和鸢尾花virginica（用蓝色表示）。我们将通过形成自己的分类来尝试确定这些品种，使用聚类：
- en: '![Figure 1.10: Scatter plot showing different species of Iris flowers dataset'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.10：展示不同品种的鸢尾花数据集的散点图'
- en: '](img/C12628_01_10.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_10.jpg)'
- en: 'Figure 1.10: A scatter plot showing different species of Iris flowers dataset'
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.10：展示不同品种的鸢尾花数据集的散点图
- en: 'Here is a photo of Iris setosa, which is represented in green in the preceding
    scatter plot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是鸢尾花setosa的照片，在先前的散点图中用绿色表示：
- en: '![Figure 1.11: Iris setosa'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.11：鸢尾花setosa'
- en: '](img/C12628_01_11.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_11.jpg)'
- en: 'Figure 1.11: Iris setosa'
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.11：鸢尾花
- en: 'The following is a photo of Iris versicolor, which is represented in red in
    the preceding scatter plot:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的照片是鸢尾花versicolor，在先前的散点图中用红色表示：
- en: '![Figure 1.12: Iris versicolor'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.12：鸢尾花变种'
- en: '](img/C12628_01_12.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_12.jpg)'
- en: 'Figure 1.12: Iris versicolor'
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.12：鸢尾花versicolor
- en: 'Here is a photo of Iris virginica, which is represented in blue in the preceding
    scatter plot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是鸢尾花virginica的照片，在先前的散点图中用蓝色表示：
- en: '![Figure 1.13: Iris virginica'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.13：鸢尾花virginica'
- en: '](img/C12628_01_13.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_13.jpg)'
- en: 'Figure 1.13: Iris virginica'
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.13：鸢尾花virginica
- en: Steps to Implement k-means Clustering
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现k-means聚类的步骤
- en: 'As we saw in the scatter plot in figure 1.9, each data point represents a flower.
    We''re going to find clusters that will identify these species. To do this type
    of clustering, we''re going to use k-means clustering, where k is the number of
    clusters we want. The following are the steps to perform k-means clustering, which,
    for simplicity of understanding, we''re going to demonstrate with two clusters.
    We will build up to using three clusters later, in order to try and match the
    actual species groupings:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在图1.9中的散点图中看到的，每个数据点代表一朵花。我们将找到可以识别这些品种的簇。为了进行这种聚类，我们将使用k-means聚类，其中k是我们想要的簇数。以下执行k-means聚类的步骤，为了理解简单，我们将用两个簇来演示。我们将在稍后使用三个簇，以尝试匹配实际的物种分组：
- en: Choose any two random coordinates, k1 and k2, on the scatter plot as initial
    cluster centers.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在散点图上选择任意两个随机坐标，k1和k2，作为初始簇中心。
- en: Calculate the distance of each data point in the scatter plot from coordinates
    k1 and k2.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算散点图中每个数据点与坐标k1和k2的距离。
- en: Assign each data point to a cluster based on whether it is closer to k1 or k2
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据数据点与k1或k2的接近程度，将每个数据点分配到簇中。
- en: Find the mean coordinates of all points in each cluster and update the values
    of k1 and k2 to those coordinates respectively.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到每个簇中所有点的平均坐标，并将k1和k2的值分别更新到这些坐标。
- en: Start again from *Step 2* until the coordinates of k1 and k2 stop moving significantly,
    or after a certain pre-determined number of iterations of the process.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*步骤2*重新开始，直到k1和k2的坐标停止显著移动，或者经过一定预定的迭代次数后。
- en: We're going to demonstrate the preceding algorithm with graphs and code.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用图表和代码演示先前的算法。
- en: 'Exercise 2: Implementing k-means Clustering on the Iris Dataset'
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2：在鸢尾花数据集上实现k-means聚类
- en: 'In this exercise, we will implement k-means clustering step by step:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将逐步实现k-means聚类：
- en: 'Load the built-in Iris dataset in the `iris_data` variable:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`iris_data`变量中加载内置的鸢尾花数据集：
- en: '[PRE2]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Set the color for different species for representation on the scatter plot.
    This will help us see how the three different species are split between our initial
    two groupings:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置不同物种在散点图上的颜色，以便表示。这将帮助我们看到三种不同的物种是如何在我们最初的两组分类之间分割的：
- en: '[PRE3]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Choose any two random clusters'' centers to start with:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择任意两个随机簇的中心开始：
- en: '[PRE4]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can try changing the points and see how it affects the final clusters.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以尝试改变点，看看它如何影响最终的簇。
- en: 'Plot the scatter plot along with the centers you chose in the previous step.
    Pass the length and width of the sepals of the iris flowers, along with the color,
    to the `plot` function in the first line, and then pass x and y coordinates of
    both the centers to the `points()` function. Here, `pch` is for selecting the
    type of representation of the center of the clusters – in this case, 4 is a cross
    and 5 is a diamond:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制散点图，并包含您在上一步骤中选择的中心。在第一行中，将鸢尾花花瓣的长度和宽度以及颜色传递给`plot`函数，然后将中心和点的x和y坐标传递给`points()`函数。在这里，`pch`用于选择聚类中心的表示类型——在这种情况下，4代表一个十字，5代表一个菱形：
- en: '[PRE5]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.14: Scatter plot of the chosen cluster centers'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图1.14：所选聚类中心点的散点图'
- en: '](img/C12628_01_14.jpg)'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12628_01_14.jpg)'
- en: 'Figure 1.14: A scatter plot of the chosen cluster centers'
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.14：所选聚类中心点的散点图
- en: 'Choose the number of iterations you want. The number of iterations should be
    such that the centers stop changing significantly after each iteration. In our
    case, six iterations are sufficient:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您想要的迭代次数。迭代次数应使得每次迭代后中心的变化不再显著。在我们的例子中，六次迭代就足够了：
- en: '[PRE6]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Initialize the variable that will keep track of the number of iterations in
    the loop:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化将跟踪循环中迭代次数的变量：
- en: '[PRE7]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Start the `while` loop to find the final cluster centers:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始`while`循环以找到最终的聚类中心：
- en: '[PRE8]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Calculate the distance of each point from the current cluster centers, which
    is *Step 2* in the algorithm. We''re calculating the Euclidean distance here using
    the `sqrt` function:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个点到当前聚类中心的距离，这是算法中的*第二步*。我们在这里使用`sqrt`函数计算欧几里得距离：
- en: '[PRE9]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Assign each point to the cluster whose center it is closest to, which is *Step
    3* of the algorithm:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个点分配到其最近的中心所在的聚类，这是算法的*第三步*：
- en: '[PRE10]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Calculate new cluster centers by calculating the mean `x` and `y` coordinates
    of the point in each cluster (*step 4* in the algorithm) with the `mean()` function
    in R:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算每个聚类中点的平均`x`和`y`坐标来计算新的聚类中心（算法中的*第四步*），使用R中的`mean()`函数：
- en: '[PRE11]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Update the variable that is keeping the count of iterations for us to effectively
    carry out *step 5* of the algorithm:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新变量以记录迭代的次数，以便有效地执行算法的*第五步*：
- en: '[PRE12]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we''re going to overwrite the species colors with new colors to demonstrate
    the two clusters. So, there will only be two colors on our next scatter plot –
    one color for cluster 1, and one color for cluster 2:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将用新的颜色覆盖物种颜色以展示两个聚类。因此，我们的下一个散点图上只有两种颜色——一种颜色代表聚类1，另一种颜色代表聚类2：
- en: '[PRE13]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Plot the new scatter plot, which contains clusters along with cluster centers:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制包含聚类及其中心的新的散点图：
- en: '[PRE14]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output will be as follows:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.15: Scatter plot representing each cluster with a different color'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.15：用不同颜色表示每个聚类的散点图'
- en: '](img/C12628_01_15.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_15.jpg)'
- en: 'Figure 1.15: A scatter plot representing each cluster with a different color'
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.15：用不同颜色表示每个聚类的散点图
- en: Notice how setosa (which used to be green) has been grouped in the left cluster,
    while most of the virginica flowers (which were blue) have been grouped into the
    right cluster. The versicolor flowers (which were red) have been split between
    the two new clusters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到setosa（以前是绿色）已被分组在左侧聚类中，而大多数virginica花朵（以前是蓝色）已被分组在右侧聚类中。versicolor花朵（以前是红色）被分在两个新的聚类之间。
- en: You have successfully implemented the k-means clustering algorithm to identify
    two groups of flowers based on their sepal size. Notice how the position of centers
    has changed after running the algorithm.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您已成功实现k-means聚类算法，根据花瓣大小识别两组花朵。注意算法运行后中心位置的变化。
- en: In the following activity, we are going to increase the number of clusters to
    three to see whether we can group the flowers correctly into their three different
    species.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下活动中，我们将增加聚类的数量到三个，以查看我们是否可以将花朵正确地分组到三种不同的物种中。
- en: 'Activity 1: k-means Clustering with Three Clusters'
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动1：使用三个聚类的k-means聚类
- en: 'Write an R program to perform k-means clustering on the Iris dataset using
    three clusters. In this activity, we''re going to perform the following steps:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个R程序，使用三个聚类对鸢尾花数据集进行k-means聚类。在这个活动中，我们将执行以下步骤：
- en: Choose any three random coordinates, k1, k2, and k3, on the plot as centers.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图上选择任意三个随机坐标，k1、k2和k3，作为中心。
- en: Calculate the distance of each data point from k1, k2, and k3.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个数据点到k1、k2和k3的距离。
- en: Classify each point by the cluster whose center it is closest to.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过找到最近的聚类中心来对每个点进行分类。
- en: Find the mean coordinates of all points in the respective clusters and update
    the values of k1, k2, and k3 to those values.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到各自聚类中所有点的平均坐标，并将 k1、k2 和 k3 的值更新到这些值。
- en: Start again from *Step 2* until the coordinates of k1, k2, and k3 stop moving
    significantly, or after 10 iterations of the process.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *步骤 2* 重新开始，直到 k1、k2 和 k3 的坐标停止显著移动，或者经过 10 次迭代过程后。
- en: 'The outcome of this activity will be a chart with three clusters, as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的结果将是一个包含三个聚类的图表，如下所示：
- en: '![Figure 1.16: Expected scatter plot for the given cluster centers'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.16：给定聚类中心的预期散点图](img/C12628_01_16.jpg)'
- en: '](img/C12628_01_16.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/C12628_01_16.jpg](img/C12628_01_16.jpg)'
- en: 'Figure 1.16: The expected scatter plot for the given cluster centers'
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.16：给定聚类中心的预期散点图
- en: You can compare your chart to Figure 1.10 to see how well the clusters match
    the actual species classifications.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将您的图表与图 1.10 进行比较，以查看聚类与实际物种分类的匹配程度。
- en: Note
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 198.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可在第 198 页找到。
- en: Introduction to k-means Clustering with Built-In Functions
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用内置函数的 k-means 聚类介绍
- en: 'In this section, we''re going to use some built-in libraries of R to perform
    k-means clustering instead of writing custom code, which is lengthy and prone
    to bugs and errors. Using pre-built libraries instead of writing our own code
    has other advantages, too:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 R 的某些内置库来执行 k-means 聚类，而不是编写冗长且容易出错的自定义代码。使用预构建库而不是编写自己的代码也有其他优点：
- en: Library functions are computationally efficient, as thousands of man hours have
    gone into the development of those functions.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库函数在计算上效率很高，因为这些函数的开发投入了数千人的工时。
- en: Library functions are almost bug-free as they've been tested by thousands of
    people in almost all practically-usable scenarios.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库函数几乎无错误，因为它们在几乎所有实际可用的场景中都被数千人测试过。
- en: Using libraries saves time, as you don't have to invest time in writing your
    own code.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用库可以节省时间，因为您不必花费时间编写自己的代码。
- en: k-means Clustering with Three Clusters
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用三个聚类的 k-means 聚类
- en: In the previous activity, we performed k-means clustering with three clusters
    by writing our own code. In this section, we're going to achieve a similar result
    with the help of pre-built R libraries.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个活动中，我们通过编写自己的代码执行了具有三个聚类的 k-means 聚类。在本节中，我们将借助预构建的 R 库实现类似的结果。
- en: 'At first, we''re going to start with a distribution of three types of flowers
    in our dataset, as represented in the following graph:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将在数据集中以三种类型的花的分布开始，如下所示：
- en: '![Figure 1.17: Graph representing three species of iris in three colors'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.17：用三种颜色表示三种鸢尾花物种的图](img/C12628_01_17.jpg)'
- en: '](img/C12628_01_17.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/C12628_01_17.jpg](img/C12628_01_17.jpg)'
- en: 'Figure 1.17: A graph representing three species of iris in three colors'
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.17：用三种颜色表示三种鸢尾花物种的图
- en: In the preceding plot, setosa is represented in blue, virginica in gray, and
    versicolor in pink.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，setosa 用蓝色表示，virginica 用灰色表示，versicolor 用粉色表示。
- en: With this dataset, we're going to perform k-means clustering and see whether
    the built-in algorithm is able to find a pattern on its own to classify these
    three species of iris using their sepal sizes. This time, we're going to use just
    four lines of code.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个数据集，我们将执行 k-means 聚类，看看内置算法是否能够自己找到模式来根据花瓣大小对这些三种鸢尾花物种进行分类。这次，我们只需要四行代码。
- en: 'Exercise 3: k-means Clustering with R Libraries'
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 3：使用 R 库进行 k-means 聚类
- en: 'In this exercise, we''re going to learn to do k-means clustering in a much
    easier way with the pre-built libraries of R. By completing this exercise, you
    will be able to divide the three species of Iris into three separate clusters:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将学习使用 R 的预构建库以更简单的方式执行 k-means 聚类。通过完成这个练习，您将能够将三种 Iris 物种划分为三个单独的聚类：
- en: 'We put the first two columns of the iris dataset, sepal length and sepal width,
    in the `iris_data` variable:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将鸢尾花数据集的前两列，即花瓣长度和花瓣宽度，放入 `iris_data` 变量中：
- en: '[PRE15]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We find the k-means cluster centers and the cluster to which each point belongs,
    and store it all in the `km.res` variable. Here, in the `kmeans`, function we
    enter the dataset as the first parameter, and the number of clusters we want as
    the second parameter:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们找到 k-means 聚类中心以及每个点所属的聚类，并将所有这些存储在 `km.res` 变量中。在这里，在 `kmeans` 函数中，我们将数据集作为第一个参数输入，我们想要的聚类数量作为第二个参数：
- en: '[PRE16]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The k-means function has many input variables, which can be altered to get different
    final outputs. You can find out more about them here in the documentation at [https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/kmeans](https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/kmeans).
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: k-means函数有许多输入变量，可以调整以获得不同的最终输出。你可以在[https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/kmeans](https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/kmeans)的文档中了解更多信息。
- en: 'Install the `factoextra` library as follows:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按以下步骤安装`factoextra`库：
- en: '[PRE17]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We import the `factoextra` library for visualization of the clusters we just
    created. `Factoextra` is an R package that is used for plotting multivariate data:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入`factoextra`库来可视化我们刚刚创建的簇。`Factoextra`是一个R包，用于绘制多元数据：
- en: '[PRE18]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Generate the plot of the clusters. Here, we need to enter the results of k-means
    as the first parameter. In `data`, we need to enter the data on which clustering
    was done. In `pallete`, we''re selecting the type of the geometry of points, and
    in `ggtheme`, we''re selecting the theme of the output plot:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成簇的图。在这里，我们需要将k-means的结果作为第一个参数输入。在`data`中，我们需要输入聚类所用的数据。在`pallete`中，我们选择点的几何形状类型，在`ggtheme`中，我们选择输出图的样式：
- en: '[PRE19]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output will be as follows:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 1.18: Three species of Iris have been clustered into three clusters'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.18：三种鸢尾花已经被聚成三个簇'
- en: '](img/C12628_01_18.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_18.jpg)'
- en: 'Figure 1.18: Three species of Iris have been clustered into three clusters'
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.18：三种鸢尾花已经被聚成三个簇
- en: Here, if you compare Figure 1.18 to Figure 1.17, you will see that we have classified
    all three species almost correctly. The clusters we've generated don't exactly
    match the species shown in figure 1.18, but we've come very close considering
    the limitations of only using sepal length and width to classify them.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，如果你将图1.18与图1.17进行比较，你会看到我们几乎正确地分类了所有三种物种。我们生成的簇与图1.18中显示的物种不完全匹配，但考虑到仅使用花瓣长度和宽度进行分类的限制，我们已经非常接近了。
- en: You can see from this example that clustering would've been a very useful way
    of categorizing the irises if we didn't already know their species. You will come
    across many examples of datasets where you don't have labeled categories, but
    are able to use clustering to form your own groupings.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子中，你可以看到，如果我们不知道它们的物种，聚类将是一个非常有用的对鸢尾花进行分类的方法。你将遇到许多数据集的例子，在这些数据集中，你没有标记的类别，但能够使用聚类来形成自己的分组。
- en: Introduction to Market Segmentation
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 市场细分简介
- en: 'Market segmentation is dividing customers into different segments based on
    common characteristics. The following are the uses of customer segmentation:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 市场细分是根据共同特征将客户划分为不同的细分市场。以下是一些客户细分的用途：
- en: Increasing customer conversion and retention
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高客户转化率和留存率
- en: Developing new products for a particular segment by identifying it and its needs
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过识别特定细分市场和其需求来开发新产品
- en: Improving brand communication with a particular segment
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特定细分市场改善品牌沟通
- en: Identifying gaps in marketing strategy and making new marketing strategies to
    increase sales
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别营销策略中的差距并制定新的营销策略以增加销售额
- en: 'Exercise 4: Exploring the Wholesale Customer Dataset'
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习4：探索批发客户数据集
- en: In this exercise, we will have a look at the data in the wholesale customer
    dataset.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将查看批发客户数据集中的数据。
- en: Note
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For all the exercises and activities where we are importing an external CSV
    or image files, go to **RStudio**-> **Session**-> **Set Working Directory**->
    **To Source File Location**. You can see in the console that the path is set automatically.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有需要导入外部CSV或图像文件的外部练习和活动，请转到**RStudio**-> **会话**-> **设置工作目录**-> **到源文件位置**。你可以在控制台中看到路径已自动设置。
- en: To download the CSV file, go to [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Exercise04/wholesale_customers_data.csv](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Exercise04/wholesale_customers_data.csv).
    Click on `wholesale_customers_data.csv`.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要下载CSV文件，请访问[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Exercise04/wholesale_customers_data.csv](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Exercise04/wholesale_customers_data.csv)。点击`wholesale_customers_data.csv`。
- en: Note
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from the UCI Machine Learning Repository. You can find
    the dataset at [https://archive.ics.uci.edu/ml/machine-learning-databases/00292/](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/).
    We have downloaded the file and saved it at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Exercise04/wholesale_customers_data.csv.](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Exercise04/wholesale_customers_data.csv.
    )
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此数据集来自UCI机器学习仓库。您可以在以下网址找到数据集：[https://archive.ics.uci.edu/ml/machine-learning-databases/00292/](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/)。我们已经下载了文件并将其保存在[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Exercise04/wholesale_customers_data.csv.](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Exercise04/wholesale_customers_data.csv.)
- en: 'Save it to the folder in which you have installed R. Now, to load it in R,
    use the following function:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其保存到您安装R的文件夹中。现在，要在R中加载它，请使用以下函数：
- en: '[PRE20]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we may have a look at the different columns and rows in this dataset by
    using the following function in R:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过使用以下R函数来查看这个数据集的不同列和行：
- en: '[PRE21]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 1.19: Columns of the wholesale customer dataset'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.19：批发客户数据集的列]'
- en: '](img/C12628_01_19.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/C12628_01_19.jpg]'
- en: 'Figure 1.19: Columns of the wholesale customer dataset'
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.19：批发客户数据集的列
- en: These six rows show the first six rows of annual spending in monetary units
    by category of product.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这六行显示了按产品类别划分的年度货币消费的前六行。
- en: 'Activity 2: Customer Segmentation with k-means'
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动二：使用k-means进行客户细分
- en: 'For this activity, we''re going to use the wholesale customer dataset from
    the UCI Machine Learning Repository. It''s available at: [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv).
    We''re going to identify customers belonging to different market segments who
    like to spend on different types of goods with clustering. Try k-means clustering
    for values of k from 2 to 6\.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个活动，我们将使用来自UCI机器学习仓库的批发客户数据集。它可在以下网址找到：[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv)。我们将通过聚类来识别属于不同市场细分、喜欢购买不同类型商品的客户。尝试使用k-means聚类，k的值为2到6。
- en: Note
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from the UCI Machine Learning Repository. You can find
    the dataset at [https://archive.ics.uci.edu/ml/machine-learning-databases/00292/](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/).
    We have downloaded the file and saved it at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv.](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv.
    )
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集来自UCI机器学习仓库。您可以在以下网址找到数据集：[https://archive.ics.uci.edu/ml/machine-learning-databases/00292/](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/)。我们已经下载了文件并将其保存在[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv.](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv.)
- en: 'These steps will help you complete the activity:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将帮助您完成活动：
- en: 'Read data downloaded from the UCI Machine Learning Repository into a variable.
    The data can be found at: [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv).'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将从UCI机器学习仓库下载的数据读入一个变量。数据可在以下网址找到：[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity02/wholesale_customers_data.csv).
- en: Select only two columns, Grocery and Frozen, for easy visualization of clusters.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只选择两列，即杂货和冷冻食品，以便于可视化聚类。
- en: As in *Step 2* of *Exercise 4*, *Exploring the Wholesale Customer Dataset*,
    change the value for the number of clusters to 2 and generate the cluster centers.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同*第4步*中*练习4*，*探索批发客户数据集*的*第2步*，将聚类数量设置为2并生成聚类中心。
- en: Plot the graph as in *Step 4* in *Exercise 4*, *Exploring the Wholesale Customer
    Dataset*.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照第4步中的*练习4*，*探索批发客户数据集*绘制图表。
- en: Save the graph you generate.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存您生成的图表。
- en: Repeat *Steps 3*, *4*, and *5* by changing value for the number of clusters
    to 3, 4, 5, and 6.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过改变聚类数量的值，重复*步骤3*、*步骤4*和*步骤5*，将聚类数量设置为3、4、5和6。
- en: Decide which value for the number of clusters best classifies the dataset.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定哪个聚类数量的值最适合对数据进行分类。
- en: 'The output will be chart of six clusters as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是一个包含六个聚类的图表，如下所示：
- en: '![Figure 1.20: Expected chart for six clusters'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.20：预期六个聚类的图表'
- en: '](img/C12628_01_20.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_20.jpg)'
- en: 'Figure 1.20: The expected chart for six clusters'
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.20：预期六个聚类的图表
- en: Note
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 201.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第201页找到。
- en: Introduction to k-medoids Clustering
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-medoids聚类简介
- en: k-medoids is another type of clustering algorithm that can be used to find natural
    groupings in a dataset. k-medoids clustering is very similar to k-means clustering,
    except for a few differences. The k-medoids clustering algorithm has a slightly
    different optimization function than k-means. In this section, we're going to
    study k-medoids clustering.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: k-medoids是另一种聚类算法，可用于在数据集中找到自然分组。k-medoids聚类与k-means聚类非常相似，除了几个不同之处。k-medoids聚类算法的优化函数与k-means略有不同。在本节中，我们将研究k-medoids聚类。
- en: The k-medoids Clustering Algorithm
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-medoids聚类算法
- en: 'There are many different types of algorithms to perform k-medoids clustering,
    the simplest and most efficient of which is **Partitioning Around Medoids**, or
    PAM for short. In PAM, we do the following steps to find cluster centers:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同类型的算法可以执行k-medoids聚类，其中最简单且效率最高的算法是**基于聚类中心的划分**，简称PAM。在PAM中，我们执行以下步骤来找到聚类中心：
- en: Choose k data points from the scatter plot as starting points for cluster centers.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从散点图中选择k个数据点作为聚类中心的起始点。
- en: Calculate their distance from all the points in the scatter plot.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算它们与散点图中所有点的距离。
- en: Classify each point into the cluster whose center it is closest to.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个点分类到其最近的中心所在的聚类中。
- en: Select a new point in each cluster that minimizes the sum of distances of all
    points in that cluster from itself.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个聚类中选择一个新点，该点使该聚类中所有点与该点的距离之和最小。
- en: Repeat *Step 2* until the centers stop changing.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*，直到中心不再改变。
- en: You can see that the PAM algorithm is identical to the k-means clustering algorithm,
    except for *Step 1* and *Step 4*. For most practical purposes, k-medoids clustering
    gives almost identical results to k-means clustering. But in some special cases
    where we have outliers in a dataset, k-medoids clustering is preferred as it's
    more robust to outliers. More about when to use which type of clustering and their
    differences will be studied in later sections.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，PAM算法与k-means聚类算法相同，除了*步骤1*和*步骤4*。对于大多数实际应用，k-medoids聚类几乎给出了与k-means聚类相同的结果。但在某些特殊情况下，如果数据集中有异常值，k-medoids聚类更受欢迎，因为它对异常值更稳健。关于何时使用哪种类型的聚类及其差异将在后面的章节中研究。
- en: k-medoids Clustering Code
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-medoids聚类代码
- en: In this section, we're going to use the same Iris flowers dataset that we used
    in the last two sections and compare to see whether the results are visibly different
    from the ones we got last time. Instead of writing code to perform each step of
    the k-medoids algorithm, we're directly going to use libraries of R to do PAM
    clustering.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用与上一节相同的鸢尾花数据集，并将其与上次的结果进行比较，以查看结果是否与上次的结果有明显的不同。我们将直接使用R的库来执行PAM聚类，而不是编写代码来执行k-medoids算法的每个步骤。
- en: 'Exercise 5: Implementing k-medoid Clustering'
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习5：实现k-medoid聚类
- en: 'In this exercise, we''re going to perform k-medoids with R''s pre-built libraries:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用R的预建库来执行k-medoids聚类：
- en: 'Store the first two columns of the iris dataset in the `iris_data` variable:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将鸢尾花数据集的前两列存储在`iris_data`变量中：
- en: '[PRE22]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Install the `cluster` package:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`cluster`包：
- en: '[PRE23]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Import the `cluster` package:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`cluster`包：
- en: '[PRE24]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Store the PAM clustering results in the `km.res` variable:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将PAM聚类结果存储在`km.res`变量中：
- en: '[PRE25]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Import the `factoextra` library:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`factoextra`库：
- en: '[PRE26]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Plot the PAM clustering results in a graph:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图中绘制PAM聚类结果：
- en: '[PRE27]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 1.21: Results of k-medoids clustering'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.21：k-medoids聚类结果'
- en: '](img/C12628_01_21.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_21.jpg)'
- en: 'Figure 1.21: Results of k-medoids clustering'
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.21：k-medoids聚类结果
- en: The results of k-medoids clustering are not very different from those of the
    k-means clustering we did in the previous section.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: k-medoids聚类的结果与我们在上一节中执行的k-means聚类结果没有很大差异。
- en: 'So, we can see that the preceding PAM algorithm classifies our dataset into
    three clusters that are similar to the clusters we got with k-means clustering.
    If we plot the results of both types of clustering side by side, we can clearly
    see how similar they are:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到，前面的PAM算法将我们的数据集划分为三个与k-means聚类得到的簇相似的簇。如果我们将这两种聚类的结果并排绘制，我们可以清楚地看到它们是多么相似：
- en: '![Figure 1.22: k-medoids clustering versus k-means clustering results'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.22：k-medoids聚类与k-means聚类结果'
- en: '](img/C12628_01_22.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_22.jpg)'
- en: 'Figure 1.22: The results of k-medoids clustering versus k-means clustering'
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.22：k-medoids聚类与k-means聚类的结果
- en: In the preceding graphs, observe how the centers of both k-means and k-medoids
    clustering are so close to each other, but centers for k-medoids clustering are
    directly overlapping on points already in the data while the centers for k-means
    clustering are not.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，观察k-means和k-medoids聚类的中心是如何如此接近的，但k-medoids聚类的中心直接重叠在数据中的点上，而k-means聚类的中心则不是。
- en: k-means Clustering versus k-medoids Clustering
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means聚类与k-medoids聚类
- en: 'Now that we''ve studied both k-means and k-medoids clustering, which are almost
    identical to each other, we''re going to study the differences between them and
    when to use which type of clustering:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经研究了k-means和k-medoids聚类，它们几乎相同，我们将研究它们之间的差异以及何时使用哪种类型的聚类：
- en: 'Computational complexity: Of the two methods, k-medoids clustering is computationally
    more expensive. When our dataset is too large (>10,000 points) and we want to
    save computation time, we''ll prefer k-means clustering over k-medoids clustering.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算复杂度：在这两种方法中，k-medoids聚类的计算成本更高。当我们的数据集太大（>10,000个点）且我们想要节省计算时间时，我们将更倾向于选择k-means聚类而不是k-medoids聚类。
- en: Note
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Whether your dataset is large or not is entirely dependent on the computation
    power available. As computation gets cheaper over time, what is considered a large
    dataset will change in the future.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集的大小完全取决于可用的计算能力。随着时间的推移，计算成本越来越低，因此被认为是大数据集的标准将在未来发生变化。
- en: 'Presence of outliers: k-means clustering is more sensitive to outliers than
    k-medoids. Cluster center positions can shift significantly due to the presence
    of outliers in the dataset, so we use k-medoids clustering when we have to build
    clusters resilient to outliers.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值的存在：与k-medoids聚类相比，k-means聚类对异常值更敏感。由于数据集中存在异常值，簇中心的定位可能会发生显著变化，因此当我们需要构建对异常值有弹性的簇时，我们使用k-medoids聚类。
- en: 'Cluster centers: Both the k-means and k-medoids algorithms find cluster centers
    in different ways. The center of a k-medoids cluster is always a data point in
    the dataset. The center of a k-means cluster does not need to be a data point
    in the dataset.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇中心：k-means和k-medoids算法以不同的方式找到簇中心。k-medoids簇的中心始终是数据集中的数据点。k-means簇的中心不需要是数据集中的数据点。
- en: 'Activity 3: Performing Customer Segmentation with k-medoids Clustering'
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动三：使用k-medoids聚类进行客户细分
- en: Use the Wholesale customer dataset to perform both k-means and k-medoids clustering,
    and then compare the results. Read the data downloaded from the UCI machine learning
    repository into a variable. The data can be found at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Data/wholesale_customers_data.csv](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Data/wholesale_customers_data.csv).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批发客户数据集进行k-means和k-medoids聚类，然后比较结果。将从UCI机器学习仓库下载的数据读入一个变量。数据可以在[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Data/wholesale_customers_data.csv](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Data/wholesale_customers_data.csv)找到。
- en: Note
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from the UCI Machine Learning Repository. You can find
    the dataset at [https://archive.ics.uci.edu/ml/machine-learning-databases/00292/](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/).
    We have downloaded the file and saved it at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity03/wholesale_customers_data.csv.](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity03/wholesale_customers_data.csv.
    )
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集来自UCI机器学习仓库。您可以在[https://archive.ics.uci.edu/ml/machine-learning-databases/00292/](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/)找到数据集。我们已经下载了文件并将其保存在[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity03/wholesale_customers_data.csv.](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity03/wholesale_customers_data.csv.
    )
- en: 'These steps will help you complete the activity:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将帮助您完成活动：
- en: Select only two columns, Grocery and Frozen, for easy two-dimensional visualization
    of clusters.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅选择两列，杂货和冷冻，以便于进行簇的二维可视化。
- en: Use k-medoids clustering to plot a graph showing four clusters for this data.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 k-medoids 聚类来绘制显示四个簇的图表。
- en: Use k-means clustering to plot a four-cluster graph.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 k-means 聚类来绘制四簇图表。
- en: Compare the two graphs to comment on how the results of the two methods differ.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较两个图表，以评论两种方法的结果差异。
- en: 'The outcome will be a k-means plot of the clusters as follows:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一个 k-means 簇的图表，如下所示：
- en: '![Figure 1.23: Expected k-means plot of the cluster'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.23：簇的预期 k-means 图'
- en: '](img/C12628_01_23.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12628_01_23.jpg)'
- en: 'Figure 1.23: The expected k-means plot of the cluster'
  id: totrans-306
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.23：簇的预期 k-means 图
- en: Note
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 206.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 206 页找到。
- en: Deciding the Optimal Number of Clusters
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决定簇的最佳数量
- en: Until now, we've been working on the iris flowers dataset, in which we know
    how many categories of flowers there are, and we chose to divide our dataset into
    three clusters based on this knowledge. But in unsupervised learning, our primary
    task is to work with data about which we don't have any information, such as how
    many natural clusters or categories there are in a dataset. Also, clustering can
    be a form of exploratory data analysis too, in which case, you won't have much
    information about the data. And sometimes, when the data has more than two dimensions,
    it becomes hard to visualize and find out the number of clusters manually. So,
    how do we find the optimal number of clusters in these scenarios? In this section,
    we're going to learn techniques to get the optimal value of the number of clusters.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理鸢尾花数据集，其中我们知道有多少种类的花，并且我们根据这个知识将我们的数据集划分为三个簇。但在无监督学习中，我们的主要任务是处理关于我们没有任何信息的数据，例如数据集中有多少自然簇或类别。此外，聚类也可以是一种探索性数据分析的形式，在这种情况下，你不会对数据有太多了解。有时，当数据有超过两个维度时，可视化变得困难，手动找出簇的数量也变得困难。那么，在这些情况下，我们如何找到簇的最佳数量呢？在本节中，我们将学习获取簇数量最佳值的技术。
- en: Types of Clustering Metrics
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类度量类型
- en: 'There is more than one way of determining the optimal number of clusters in
    unsupervised learning. The following are the ones that we''re going to study in
    this chapter:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，确定簇的最佳数量有多种方法。以下是我们将在本章中研究的方法：
- en: The Silhouette score
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮廓分数
- en: The Elbow method / WSS
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肘部方法 / WSS
- en: The Gap statistic
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gap 统计量
- en: Silhouette Score
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 轮廓分数
- en: 'The silhouette score or average silhouette score calculation is used to quantify
    the quality of clusters achieved by a clustering algorithm. Let''s take a point,
    a, in a cluster, x:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓分数或平均轮廓分数的计算用于量化聚类算法获得的簇的质量。让我们以簇 x 中的一个点 a 为例：
- en: 'Calculate the average distance between point a and all the points in cluster
    x (denoted by **dxa**):![Figure 1.24: Calculating the average distance between
    point a and all the points of cluster x'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算点 a 与簇 x 中所有点的平均距离（用 **dxa** 表示）：![图 1.24：计算点 a 与簇 x 中所有点的平均距离
- en: '](img/C12628_01_24.jpg)'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12628_01_24.jpg)'
- en: 'Figure 1.24: Calculating the average distance between point a and all the points
    of cluster x'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.24：计算点 a 与簇 x 中所有点的平均距离
- en: 'Calculate the average distance between point a and all the points in another
    cluster nearest to a (**dya**):![Figure 1.25: Calculating the average distance
    between point a and all the points near cluster x'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算点 a 与另一个簇中离 a 最近的簇中所有点的平均距离（**dya**）![图 1.25：计算点 a 与簇 x 中所有近点的平均距离
- en: '](img/C12628_01_25.jpg)'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12628_01_25.jpg)'
- en: 'Figure 1.25: Calculating the average distance between point a and all the points
    near cluster x'
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.25：计算点 a 与簇 x 中所有点的平均距离
- en: Calculate the silhouette score for that point by dividing the difference of
    the result of *Step 1* from the result of *Step 2* by the max of the result of
    *Step 1* and *Step 2* ((dya-dxa)/max(dxa,dya)).
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将 *步骤 1* 的结果与 *步骤 2* 的结果之差除以 *步骤 1* 和 *步骤 2* 的最大值来计算该点的轮廓分数（(dya-dxa)/max(dxa,dya)）。
- en: Repeat the first three steps for all points in the cluster.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对簇中的所有点重复前三个步骤。
- en: 'After getting the silhouette score for every point in the cluster, the average
    of all those scores is the silhouette score of that cluster:![Figure 1.26: Calculating
    the silhouette score'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在得到聚类中每个点的轮廓得分后，所有这些得分的平均值是该聚类的轮廓得分：![图 1.26：计算轮廓得分
- en: '](img/C12628_01_26.jpg)'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/C12628_01_26.jpg)'
- en: 'Figure 1.26: Calculating the silhouette score'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.26：计算轮廓得分
- en: Repeat the preceding steps for all the clusters in the dataset.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集中的所有聚类重复前面的步骤。
- en: 'After getting the silhouette score for all the clusters in the dataset, the
    average of all those scores is the silhouette score of that dataset:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在得到数据集中所有聚类的轮廓得分后，所有这些得分的平均值是该数据集的轮廓得分：
- en: '![Figure 1.27: Calculating the average silhouette score'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.27：计算平均轮廓得分'
- en: '](img/C12628_01_27.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/C12628_01_27.jpg)'
- en: 'Figure 1.27: Calculating the average silhouette score'
  id: totrans-333
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.27：计算平均轮廓得分
- en: The silhouette score ranges between 1 and -1\. If the silhouette score of a
    cluster is low (between 0 and -1), it means that the cluster is spread out or
    the distance between the points of that cluster is high. If the silhouette score
    of a cluster is high (close to 1), it means that the clusters are well defined
    and the distance between the points of a cluster is low and their distance from
    points of other clusters is high. So, the ideal silhouette score is near 1.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓得分介于 1 和 -1 之间。如果一个聚类的轮廓得分低（介于 0 和 -1 之间），这意味着该聚类分布较广或该聚类中点的距离较高。如果一个聚类的轮廓得分高（接近
    1），这意味着聚类定义良好，聚类中点的距离较低，而与其他聚类中点的距离较高。因此，理想的轮廓得分接近 1。
- en: Understanding the preceding algorithm is important for forming an understanding
    of silhouette scores, but it's not important for learning how to implement it.
    So, we're going to learn how to do silhouette analysis in R using some pre-built
    libraries.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 理解前面的算法对于形成对轮廓得分的理解很重要，但它对于学习如何实现它并不重要。因此，我们将学习如何使用一些预构建的库在 R 中进行轮廓分析。
- en: 'Exercise 6: Calculating the Silhouette Score'
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 6：计算轮廓得分
- en: 'In this exercise, we''re going to learn how to calculate the silhouette score
    of a dataset with a fixed number of clusters:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将学习如何计算具有固定聚类数量的数据集的轮廓得分：
- en: 'Put the first two columns of the iris dataset, sepal length and sepal width,
    in the `iris_data` variable:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Iris 数据集的前两列，即花瓣长度和花瓣宽度，放入 `iris_data` 变量中：
- en: '[PRE28]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Import the `cluster` library to perform k-means clustering:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `cluster` 库以执行 k-means 聚类：
- en: '[PRE29]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Store the k-means clusters in the `km.res` variable:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 k-means 聚类存储在 `km.res` 变量中：
- en: '[PRE30]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Store the pair-wise distance matrix for all data points in the `pair_dis` variable:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有数据点的成对距离矩阵存储在 `pair_dis` 变量中：
- en: '[PRE31]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Calculate the silhouette score for each point in the dataset:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据集中每个点的轮廓得分：
- en: '[PRE32]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Plot the silhouette score plot:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制轮廓得分图：
- en: '[PRE33]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Fig 1.28: The silhouette score for each point in every cluster is represented
    by a single bar'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.28：每个聚类中每个点的轮廓得分用一个单独的条形表示'
- en: '](img/C12628_01_28.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/C12628_01_28.jpg)'
- en: 'Fig 1.28: The silhouette score for each point in every cluster is represented
    by a single bar'
  id: totrans-353
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.28：每个聚类中每个点的轮廓得分用一个单独的条形表示
- en: The preceding plot gives the average silhouette score of the dataset as 0.45\.
    It also shows the average silhouette score cluster-wise and point-wise.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了数据集的平均轮廓得分为 0.45。它还显示了按聚类和按点计算的平均轮廓得分。
- en: In the preceding exercise, we calculated the silhouette score for three clusters.
    But for deciding how many clusters to have, we'll have to calculate the silhouette
    score for multiple clusters in the dataset. In the next exercise, we're going
    to learn how to do this with the help of a library called `factoextra` in R.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的练习中，我们计算了三个聚类的轮廓得分。但为了决定有多少个聚类，我们必须计算数据集中多个聚类的轮廓得分。在下一个练习中，我们将学习如何使用 R 中的
    `factoextra` 库来完成这项工作。
- en: 'Exercise 7: Identifying the Optimum Number of Clusters'
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 7：确定最佳聚类数量
- en: 'In this exercise, we''re going to identify the optimal number of clusters by
    calculating the silhouette score on various values of k in one line of code with
    the help of an R library:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过在一行代码中使用 R 库来计算 k 的各种值，以确定最佳聚类数量：
- en: 'Put first two columns, sepal length and sepal width, of the Iris data set in
    the `iris_data` variable:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Iris 数据集的前两列，即花瓣长度和花瓣宽度，放入 `iris_data` 变量中：
- en: '[PRE34]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Import the `factoextra` library:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `factoextra` 库：
- en: '[PRE35]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Plot a graph of the silhouette score versus number of clusters (up to 20):'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制轮廓分数与簇数量（最多20个）的图表：
- en: '[PRE36]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In the second argument, you may change k-means to k-medoids or any other type
    of clustering. The `k.max` variable is the max number of clusters up to which
    the score is to be calculated. In the method argument of the function, you can
    enter three types of clustering metrics to be included. All three of them are
    discussed in this chapter.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第二个参数中，你可以将k-means改为k-medoids或任何其他类型的聚类。`k.max`变量是要计算的簇的最大数量。在函数的方法参数中，你可以输入要包含的三种聚类度量类型。所有这三种度量在本章中都有讨论。
- en: 'The output is as follows:'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Fig 1.29: Number of clusters versus average silhouette score'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.29：簇数量与平均轮廓分数的关系]'
- en: '](img/C12628_01_29.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/C12628_01_29.jpg](img/C12628_01_29.jpg)'
- en: 'Fig 1.29: The number of clusters versus average silhouette score'
  id: totrans-369
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.29：簇数量与平均轮廓分数的关系
- en: From the preceding graph, you select a value of k that has the highest score;
    that is, 2\. Two is the optimal number of clusters as per the silhouette score.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，你选择一个具有最高分数的k值；即2。根据轮廓分数，2是最佳簇数量。
- en: WSS/Elbow Method
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WSS/肘部方法
- en: 'To identify a cluster in a dataset, we try to minimize the distance between
    points in a cluster, and the **Within-Sum-of-Squares** (WSS) method measures exactly
    that. The WSS score is the sum of the squares of the distances of all points within
    a cluster. In this method, we perform the following steps:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在数据集中识别簇，我们尝试最小化簇内点之间的距离，而**内部平方和**（WSS）方法正是衡量这一点。WSS分数是簇内所有点距离平方的总和。在此方法中，我们执行以下步骤：
- en: Calculate clusters by using different values of k.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用不同的k值计算簇。
- en: 'For every value of k, calculate WSS using the following formula:![Figure 1.30:
    The formula to calculate WSS where p is the total number of dimensions of the
    data](img/C12628_01_30.jpg)'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个k值，使用以下公式计算WSS：![图1.30：计算WSS的公式，其中p是数据的总维度数](img/C12628_01_30.jpg)
- en: 'Figure 1.30: The formula to calculate WSS where p is the total number of dimensions
    of the data'
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.30：计算WSS的公式，其中p是数据的总维度数
- en: 'This formula is illustrated here:'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此公式在此处展示：
- en: '![Figure 1.31: Distances relative to two points, but WSS measures sums of all
    distances relative to all points within every cluster](img/C12628_01_31.jpg)'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图1.31：相对于两个点的距离，但WSS衡量的是相对于每个簇内所有点的所有距离的总和](img/C12628_01_31.jpg)'
- en: 'Figure 1.31: Illustration of WSS score'
  id: totrans-378
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.31：WSS分数的说明
- en: Note
  id: totrans-379
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Figure 1.31 illustrates WSS relative to two points, but in reality WSS measures
    sums of all distances relative to all points. within every cluster.
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1.31说明了相对于两个点的WSS，但现实中WSS衡量的是相对于每个簇内所有点的所有距离的总和。
- en: Plot number of clusters k versus WSS score.
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制簇数量k与WSS分数的关系图。
- en: Identify the k value after which the WSS score doesn't decrease significantly
    and choose this k as the ideal number of clusters. This point is also known as
    the elbow of the graph, hence the name "**elbow method**".
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定WSS分数不再显著下降的k值，并选择这个k作为理想的簇数量。这个点也被称为图表的“肘部”，因此得名“**肘部方法**”。
- en: In the following exercise, we're going to learn how to identify the ideal number
    of clusters with the help of the `factoextra` library.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将学习如何借助`factoextra`库来确定理想簇的数量。
- en: 'Exercise 8: Using WSS to Determine the Number of Clusters'
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习8：使用WSS确定簇的数量
- en: In this exercise, we will see how we can use WSS to determine the number of
    clusters. Perform the following steps.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将看到如何使用WSS来确定簇的数量。执行以下步骤。
- en: 'Put the first two columns, sepal length and sepal width, of the Iris data set
    in the `iris_data` variable:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Iris数据集的前两列，即花瓣长度和花瓣宽度，放入`iris_data`变量中：
- en: '[PRE37]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Import the `factoextra` library:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`factoextra`库：
- en: '[PRE38]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Plot a graph of WSS versus number of clusters (up to 20):'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制WSS与簇数量（最多20个）的图表：
- en: '[PRE39]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Fig 1.32: WSS versus number of clusters'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.32：WSS与簇数量的关系]'
- en: '](img/C12628_01_32.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/C12628_01_32.jpg](img/C12628_01_32.jpg)'
- en: 'Fig 1.32: WSS versus number of clusters'
  id: totrans-395
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.32：WSS与簇数量的关系
- en: In the preceding graph, we can choose the elbow of the graph as k=3, as the
    value of WSS starts dropping more slowly after k=3\. Choosing the elbow of the
    graph is always a subjective choice and there could be times where you could choose
    k=4 or k=2 instead of k=3, but with this graph, it's clear that k>5 are inappropriate
    values for k as they are not the elbow of the graph, which is where the graph's
    slope changes sharply.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们可以选择图表的肘部作为 k=3，因为当 k=3 后，WSS 的值开始下降得更慢。选择图表的肘部始终是一个主观的选择，有时你可能选择
    k=4 或 k=2 而不是 k=3，但在这个图表中，很明显 k>5 的值不合适，因为它们不是图表的肘部，图表的斜率在这里急剧变化。
- en: The Gap Statistic
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 间隙统计
- en: The Gap statistic is one of the most effective methods of finding the optimal
    number of clusters in a dataset. It is applicable to any type of clustering method.
    The Gap statistic is calculated by comparing the WSS value for the clusters generated
    on our observed dataset versus a reference dataset in which there are no apparent
    clusters. The reference dataset is a uniform distribution of data points between
    the minimum and maximum values of our dataset on which we want to calculate the
    Gap statistic.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 间隙统计是寻找数据集中最佳聚类数量的最有效方法之一。它适用于任何类型的聚类方法。间隙统计是通过比较在观测数据集上生成的聚类与在参考数据集上生成的聚类（其中没有明显的聚类）的
    WSS 值来计算的。参考数据集是在我们想要计算间隙统计的观测数据集的最小值和最大值之间均匀分布的数据点。
- en: So, in short, the Gap statistic measures the WSS values for both observed and
    random datasets and finds the deviation of the observed dataset from the random
    dataset. To find the ideal number of clusters, we choose a value of k that gives
    us the maximum value of the Gap statistic. The mathematical details of how these
    deviations are measured are beyond the scope of this book. In the next exercise,
    we're going to learn how to calculate the Gap statistic with the help of the `factoviz`
    library.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，简而言之，间隙统计测量观测和随机数据集的 WSS 值，并找出观测数据集与随机数据集的偏差。为了找到理想的聚类数量，我们选择一个使间隙统计值最大的
    k 值。这些偏差如何测量的数学细节超出了本书的范围。在下一项练习中，我们将学习如何使用 `factoviz` 库计算间隙统计。
- en: 'Here is a reference dataset:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个参考数据集：
- en: '![Figure 1.33: Reference dataset'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.33：参考数据集'
- en: '](img/C12628_01_33.jpg)'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 C12628_01_33.jpg](img/C12628_01_33.jpg)'
- en: 'Figure 1.33: The reference dataset'
  id: totrans-403
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.33：参考数据集
- en: 'The following is the observed dataset:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为观测数据集：
- en: '![Figure 1.34: Observed dataset'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.34：观测数据集'
- en: '](img/C12628_01_34.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 C12628_01_34.jpg](img/C12628_01_34.jpg)'
- en: 'Figure 1.34: The observed dataset'
  id: totrans-407
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.34：观测数据集
- en: 'Exercise 9: Calculating the Ideal Number of Clusters with the Gap Statistic'
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 9：使用间隙统计计算理想的聚类数量
- en: 'In this exercise, we will calculate the ideal number of clusters using the
    Gap statistic:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用间隙统计计算理想的聚类数量：
- en: 'Put the first two columns, sepal length and sepal width, of the Iris data set
    in the `iris_data` variable as follows:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Iris 数据集的前两列，即花瓣长度和花瓣宽度，放入 `iris_data` 变量中，如下所示：
- en: '[PRE40]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Import the `factoextra` library as follows:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按以下方式导入 `factoextra` 库：
- en: '[PRE41]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Plot the graph of Gap statistics versus number of clusters (up to 20):'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制间隙统计与聚类数量（最多 20）的图表：
- en: '[PRE42]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Fig 1.35: Gap statistic versus number of clusters'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.35：间隙统计与聚类数量'
- en: '](img/C12628_01_35.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 C12628_01_35.jpg](img/C12628_01_35.jpg)'
- en: 'Fig 1.35: Gap statistics versus number of clusters'
  id: totrans-418
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.35：间隙统计与聚类数量
- en: As we can see in the preceding graph, the highest value of the Gap statistic
    is for k=3\. Hence, the ideal number of clusters in the iris dataset is three.
    Three is also the number of species in the dataset, indicating that the gap statistic
    has enabled us to reach a correct conclusion.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，间隙统计的最高值对应于 k=3。因此，iris 数据集中的理想聚类数量是三个。三个也是数据集中的物种数量，这表明间隙统计使我们得出了正确的结论。
- en: 'Activity 4: Finding the Ideal Number of Market Segments'
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 4：寻找理想的市场细分数量
- en: 'Find the optimal number of clusters in the wholesale customers dataset with
    all three of the preceding methods:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述三种方法在批发客户数据集中找到最佳聚类数量：
- en: Note
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset is taken from the UCI Machine Learning Repository. You can find
    the dataset at [https://archive.ics.uci.edu/ml/machine-learning-databases/00292/](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/).
    We have downloaded the file and saved it at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity04/wholesale_customers_data.csv.](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity04/wholesale_customers_data.csv.
    )
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集来自UCI机器学习仓库。您可以在[https://archive.ics.uci.edu/ml/machine-learning-databases/00292/](https://archive.ics.uci.edu/ml/machine-learning-databases/00292/)找到数据集。我们已经下载了文件并将其保存在[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity04/wholesale_customers_data.csv.](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-R/tree/master/Lesson01/Activity04/wholesale_customers_data.csv.
    )
- en: Load columns 5 to 6 of the wholesale customers dataset in a variable.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个变量中加载批发客户数据集的第5到6列。
- en: Calculate the optimal number of clusters for k-means clustering with the silhouette
    score.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用轮廓分数计算k-means聚类的最佳聚类数量。
- en: Calculate the optimal number of clusters for k-means clustering with the WSS
    score.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用WSS分数计算k-means聚类的最佳聚类数量。
- en: Calculate the optimal number of clusters for k-means clustering with the Gap
    statistic.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Gap统计量计算k-means聚类的最佳聚类数量。
- en: The outcome will be three graphs representing the optimal number of clusters
    with the silhouette score, with the WSS score, and with the Gap statistic.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是三个图表，分别表示最佳聚类数量与轮廓分数、WSS分数和Gap统计量。
- en: Note
  id: totrans-429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 208.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第208页找到。
- en: As we have seen, each method will give a different value for the optimal number
    of clusters. Sometimes, the results won't make sense, as you saw in the case of
    the Gap statistic, which gave the optimal number of clusters as one, which would
    mean that clustering shouldn't be done on this dataset and all data points should
    be in a single cluster.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们所看到的，每种方法都会给出最佳聚类数量的不同值。有时，结果可能没有意义，就像你在Gap统计量的例子中看到的那样，它给出的最佳聚类数量为一，这意味着不应该在这个数据集上进行聚类，所有数据点都应该在一个单独的聚类中。 '
- en: All the points in a given cluster will have similar properties. Interpretation
    of those properties is left to domain experts. And there's almost never a right
    answer for the right number of clusters in unsupervised learning.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 给定聚类中的所有点将具有相似的性质。对这些性质的解读留给领域专家。在无监督学习中，几乎永远没有一个正确的答案来确定正确的聚类数量。
- en: Summary
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Congratulations! You have completed the first chapter of this book. If you've
    understood everything we've studied until now, you now know more about unsupervised
    learning than most people who claim to know data science. The k-means clustering
    algorithm is so fundamental to unsupervised learning that many people equate k-means
    clustering with unsupervised learning.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经完成了这本书的第一章。如果你理解了我们至今所学的所有内容，你现在对无监督学习的了解比大多数声称了解数据科学的人都要多。k-means聚类算法对无监督学习来说如此基础，以至于许多人把k-means聚类和无监督学习等同起来。
- en: In this chapter, you not only learned about k-means clustering and its uses,
    but also k-medoids clustering, along with various clustering metrics and their
    uses. So, now you have a top-tier understanding of k-means and k-medoid clustering
    algorithms.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你不仅学习了k-means聚类及其应用，还学习了k-medoids聚类，以及各种聚类指标及其应用。因此，现在你对k-means和k-medoid聚类算法有了顶级理解。
- en: In the next chapter, we're going to have a look at some of the lesser-known
    clustering algorithms and their uses.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨一些不太为人所知的聚类算法及其应用。
