- en: '*Chapter 5*: Advanced Model Building – Part I'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we begin the transition from basic to advanced model building
    through the introduction of the nuanced issues and choices that a data scientist
    considers when building enterprise-grade models. We will discuss data splitting
    options, compare modeling algorithms, present a two-stage grid-search strategy
    for hyperparameter optimization, introduce H2O AutoML for automatically fitting
    multiple algorithms to data, and further investigate feature engineering tactics
    to extract as much information as possible from the data. We will introduce H2O
    Flow, a menu-based UI that is included with H2O, which is useful for monitoring
    the health of the H2O cluster and enables interactive data and model investigations.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the entire process, we will illustrate these advanced model-building
    concepts using the Lending Club problem that was introduced in [*Chapter 3*](B16721_03_Final_SK_ePub.xhtml#_idTextAnchor042),
    *Fundamental Workflow – Data to Deployable Model*. By the end of this chapter,
    you will be able to build an enterprise-scale, optimized predictive model using
    one or more supervised learning algorithms available within H2O. After that, all
    that is left is to review the model and deploy it into production.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data for validation or cross-validation and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithm considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model optimization with grid search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H2O AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering options
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging H2O Flow to enhance your IDE workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together – algorithms, feature engineering, grid search, and
    AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are introducing the code and datasets in this chapter for the first time.
    At this point, if you have not set up your H2O environment, please refer to [*Appendix*](B16721_Appendix_Final_SK_ePub.xhtml#_idTextAnchor268)
    *– Alternative Methods to Launch H2O Clusters,* to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data for validation or cross-validation and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Splitting data into training, validation, and test sets is the accepted standard
    for model building when the size of the data is sufficiently large. The idea behind
    validation is simple: most algorithms naturally overfit on training data. Here,
    overfitting means that some of what is being modeled are actual idiosyncrasies
    of that specific dataset (for instance, noise) rather than representative of the
    population as a whole. So, how do you correct this? Well, you can do it by creating
    a holdout sample, called a validation set, which is scored against during the
    model-building process to determine whether what is being modeled is a signal
    or noise. This enables things such as hyperparameter tuning, model regularization,
    early stopping, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: The test dataset is an additional holdout that is used at the end of model building
    to determine true model performance. Having holdout test data is critical for
    any model build. In fact, it is so critical that you should neither trust nor
    deploy a model that has not been measured against a test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative to the train-validate-test split is to use a train-test split
    with k-fold cross-validation on the training data. Here is how that works:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the training data into k-folds, where, in our example, k is 5\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a model with one of the folds playing the role of validation data and the
    other four folds being combined into training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this so that each fold is used as validation once.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This yields five models, each validated on a different subset of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates this concept nicely:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Illustration of 5-fold cross-validation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.1_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Illustration of 5-fold cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: The k-fold cross-validation approach was originally developed for small data
    to allow the model to see more data in training. This comes at the cost of higher
    computational expenses. For many data scientists, k-fold cross-validation is used
    regardless of the data size.
  prefs: []
  type: TYPE_NORMAL
- en: Model Overfitting and Data Splitting
  prefs: []
  type: TYPE_NORMAL
- en: The concept of model overfitting is critical. By definition, overfit models
    do not generalize well. If you are using a train-validate-test approach and building
    many models on the same validation set, it is likely that the leading model is
    overfit on the validation data. This likelihood increases as the number of models
    increases. Measuring the leading models against a holdout test set is the best
    indication of actual performance after deployment.
  prefs: []
  type: TYPE_NORMAL
- en: We can minimize any overfit-to-validation issues by ensuring each model is built
    on its own randomly selected train-validate partition. This could occur naturally
    in k-fold cross-validation if each model is built on a different partitioning
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting thing happens with data science competitions that have multiple
    entries (in the hundreds or thousands) that are tested against a blind holdout
    test dataset. It has been shown that leading models commonly overfit on the test
    data. So, what should you do in such a situation? The obvious answer is to have
    an additional holdout set, such as a meta-test set, that can be used to fairly
    evaluate how well these models would generalize after deployment.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will demonstrate both approaches using the Lending Club
    dataset. The following code begins in the *Model training* section of [*Chapter
    3*](B16721_03_Final_SK_ePub.xhtml#_idTextAnchor042), *Fundamental Workflow – Data
    to Deployable Model*, specifically in *step 3* of *Fundamental Workflow*.
  prefs: []
  type: TYPE_NORMAL
- en: Train, validate, and test set splits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We split the data into three parts: 60% for training, 20% for validation, and
    20% for final testing, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is straightforward. Optionally, we set `seed` for the reproducibility
    of the data splits. The `ratios` parameter only requires the training and validation
    proportions, and the test split is obtained by subtraction from one. The `destination_frames`
    option allows us to name the resulting data objects, which is not required but
    will make their identification in H2O Flow easier.
  prefs: []
  type: TYPE_NORMAL
- en: Train and test splits for k-fold cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We could also split the data into two parts: 80% for training and 20% for testing.
    This can be done using a k-fold cross-validation approach, as the following code
    demonstrates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How to Set a Seed
  prefs: []
  type: TYPE_NORMAL
- en: Random numbers in current computing are not random at all, but deterministic.
    **Pseudo**-**random number generators** (**PRNGs**) are complicated mathematical
    functions that return a fixed sequence of values given a specific seed. If the
    seed is omitted, the computer will set the seed automatically – typically, from
    the system clock. This seed value is often reported in logs. Setting the seed
    in code allows the analysis to be explicitly reproducible.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will turn our attention to choosing a modeling algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will address the question of how a data scientist should
    decide which of the many machine learning and statistical algorithms should be
    chosen to solve a particular problem. We assume some prior familiarity with statistical
    and machine learning models such as logistic regression, decision trees, random
    forests, and gradient boosting models.
  prefs: []
  type: TYPE_NORMAL
- en: As outlined in [*Chapter 4*](B16721_04_Final_SK_ePub.xhtml#_idTextAnchor064),
    *H2O Model Building at Scale – Capability Articulation* H2O provides multiple
    supervised and unsupervised learning algorithms that can be used to build models.
    For example, in the case of a binary classification problem, a data scientist
    could choose a parametric GLM model (logistic regression); semiparametric GAM;
    nonparametric tree-based approaches such as **Random Forest**, **GBM**, **XGBoost**,
    or **RuleFit**; models from the machine learning community such as **Support Vector
    Machines** (**SVMs**) or **Deep Learning Neural Networks**; or the simple **Naïve
    Bayes Classifier**. To complicate things even further, any subset of these algorithms
    could be combined into one predictive model using **Stacked Ensembles** (which
    is a method for combining multiple highly predictive models into a single model;
    we will discuss this in the *H2O AutoML* section). So, what is a data scientist
    to do?
  prefs: []
  type: TYPE_NORMAL
- en: A Note on RuleFit
  prefs: []
  type: TYPE_NORMAL
- en: The RuleFit algorithm is actually a penalized linear model. Here, we list it
    with tree-based models because the rules are extracted from a large population
    of randomly created decision trees. Rule selection and model regularization occur
    via LASSO. The intent is to combine the interpretability of linear models and
    explicit rules with the flexibility and predictive power of tree-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: If the only criterion for model selection is pure predictive power, a data scientist
    could simply try everything and pick the model that performs best on a test dataset.
    Let's call this the *Kaggle solution*, named after the popular Kaggle data science
    competitions. Kaggle competitions result in algorithms and modeling approaches
    being pressure tested over multiple problems and datasets. Insights discovered
    during these competitions have found their way into real-world data science practices.
  prefs: []
  type: TYPE_NORMAL
- en: However, in an enterprise setting, it is rare for predictive power to be the
    only consideration for algorithm selection. Model transparency could be another.
    As an oversimplification, parametric models that are inherently interpretable
    (GLM) might be less predictive than nonparametric models. Nonparametric models
    such as random forest, GBM, XGBoost, and deep learning neural networks are black
    boxes that are difficult to interpret but frequently produce superior predictions.
    (Note that the GAM and RuleFit algorithms combine model transparency with predictions
    that often rival black-box methods.)
  prefs: []
  type: TYPE_NORMAL
- en: In addition to pure modeling criteria, there are business and implementation
    considerations that come into play in modeling and deployment decisions. We will
    cover these, in more detail, in the later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the remaining part of this section, we will give a high-level overview of
    decision trees, random forest, and gradient boosting models. We will illustrate
    the Lending Club data while concentrating on two specific boosting implementations:
    H2O GBM and XGBoost.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm Popularity in the Industry
  prefs: []
  type: TYPE_NORMAL
- en: Our collective experience of working with scores of customers spanning multiple
    industries leads to the following general observations. First, classification
    problems are more prevalent than regression problems by a wide margin. Second,
    when choosing an algorithm, the gold standard for interpretable classification
    problems remains logistic regression (GLM). The most frequent nonparametric algorithm
    choice is some form of gradient boosting, currently the GBM, XGBoost, or LightGBM
    implementations. The popularity of gradient boosting has been helped by its frequent
    appearance (either alone or in an ensemble) high up on the Kaggle leaderboards.
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the heart of every random forest or GBM implementation is the concept of
    a decision tree. A decision tree can be used for either *classification*, where
    observations are assigned to discrete groups, or *regression*, where observations
    are a numerical outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Observation assignment is made through *conditional control statements* that
    form a tree-like structure. The general decision tree algorithm can be described
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Search through all the candidate predictors, identifying the variable split
    that yields the greatest predictive power.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each newly created branch, repeat the variable splitting process from *step
    1*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue until the stopping criteria are met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The functions used for splitting include information entropy and the Gini coefficient.
    Let's illustrate them using entropy. In information theory, the entropy of a random
    variable is the average level of uncertainty in the variable's outcomes. A pure
    or homogeneous classification tree node will have an entropy of zero. At each
    candidate split, we calculate the entropy and choose the split with the lowest
    entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, we could continue splitting until all nodes are pure, but that
    would yield an extremely overfit tree. Instead, we utilize stopping criteria such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum number of observations that is needed at each node after splitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reduction in entropy is not enough based on a selected cutoff value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum depth of the tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To illustrate, let's suppose we are building a decision tree to model the probability
    of surviving the sinking of the Titanic in 1912\. Our data includes name, gender,
    age, the class of passage booked, the price of the tickets, the location of the
    cabin or berth, the city where the passenger boarded, any traveling companions,
    and more. The resulting decision tree can be found in the diagram that follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first split increases the predictive power the most (by reducing entropy
    the most):'
  prefs: []
  type: TYPE_NORMAL
- en: Is the subject Male? If yes, the next split is created by the `Age < 18` rule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For males older than 18, the survival probability for this terminal or *leaf*
    node is 17%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For males under 18, one more split is needed: `3rd Class`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For males in the 3rd class who are under 18, the survival probability is 14%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For males in the 1st and 2nd classes who are under 18, the survival probability
    is 44%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tree on the `Male=Yes` branch stops splitting at these leaf nodes because
    one or more stopping criteria have been met.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A similar process for the `Male=No` branch proceeds. Note that according to
    this model, non-`3rd Class` females have a survival probability of 95%. For `3rd
    Class` female passengers, survival probabilities depend on where they boarded,
    resulting in either a 38% or 70% survival probability leaf node. The decision
    tree model supports the *women and children first* ethos for emergencies at sea:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – A decision tree modeling the survival probabilities on the Titanic'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.2_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – A decision tree modeling the survival probabilities on the Titanic
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees have some clear advantages. Their layout is simple to comprehend,
    and their interpretation, as we have just demonstrated, is straightforward. The
    algorithm trains and scores quickly. Decision trees are robust when it comes to
    nonlinear relationships, feature distributions, correlated features, and missing
    values. On the other hand, they do not model linear relationships efficiently.
    They have high variance, meaning, in part, that trees are easily overfitted. Perhaps
    their greatest drawback is that individual decision trees don't predict particularly
    well, which is an issue that was first raised by the original developers of the
    decision tree methodology.
  prefs: []
  type: TYPE_NORMAL
- en: To remedy the poor predictive properties of decision trees, algorithms based
    on ensembles of individual trees have been developed. In general, the objective
    of ensemble methods is to create a *strong learner* by combining information across
    multiple *weak learners* (in our case, decision trees). The adaptation of two
    ensemble methods, bagging and boosting, to trees has resulted in random forest
    and gradient boosting algorithms, respectively. We will review each of these ensemble
    methods and their implementations in H2O next.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bagging** (which is short for *bootstrap aggregating*) is an ensemble method
    that fits models to bootstrapped samples of the data and averages across them.
    **Bootstrapping** is a resampling method that samples from the data rows with
    replacement. This creates randomness in the row (or observation) space. Random
    forest is a bagging method for decision trees that adds randomness to the column
    (or variable) space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The random forest algorithm can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a deep tree based on randomly selected rows of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each split, only evaluate a random subset of variables to split on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat this many times, creating a *forest* as a collection of all the trees.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the average across all trees in the forest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: H2O includes two implementations of random forest, **Distributed Random Forest**
    (**DRF**) and **Extremely Randomized Trees** (**XRT**). In the following sections,
    we will summarize these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Random Forest (DRF)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DRF is the default random forest implementation in H2O. The highlights of this
    algorithm are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Each tree in a DRF is built in parallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The splitting rule is created by choosing the most discriminative threshold
    among a random subset of candidate features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extremely Randomized Trees (XRT)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The XRT algorithm adds additional randomness to the splitting-rule process.
    This has the effect of reducing model variance at the cost of (slightly) increased
    bias. XRT is enabled by setting `histogram_type="Random"`:'
  prefs: []
  type: TYPE_NORMAL
- en: Each tree in an XRT is built in parallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rather than finding the most discriminative threshold, this algorithm will create
    thresholds at random for each candidate variable. The best of this set is picked
    as the splitting rule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hyperparameters for both random forest implementations are shared.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The random forest methods in H2O require the following hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of trees to be built, `ntrees` (this defaults to 50).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum tree depth, `max_depth` (this defaults to 20). Note that too large
    a value can result in overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum number of observations per leaf, `min_rows` (this defaults to 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional hyperparameters are available for tuning the random forest model.
    You can find them at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html).
    A grid search can aid the process of hyperparameter selection and model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boosting is an ensemble method that combines models sequentially, with each
    new model built on the residuals of the previous model. Boosted trees are based
    on a sequence of relatively shallow decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'The boosted trees algorithm can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start by building a shallow decision tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a shallow decision tree to the residuals of the previous tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply the residual tree by a shrinkage parameter (or the learning rate).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* and *3* until the stopping criteria are met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Building on the residuals makes the algorithm concentrate on areas where the
    model is not predicting well. The process is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – The H2O GBM algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.3_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – The H2O GBM algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'The GBM approach results in highly predictive models, but care must be taken
    to avoid overfitting. H2O includes two versions of gradient boosting: H2O GBM
    and XGBoost. In the following sections, we will summarize these algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: H2O GBM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The H2O GBM implementation follows the original algorithm, as described in
    the book, *The Elements of Statistical Learning by* *Jerome H. Friedman, Robert
    Tibshirani, and Trevor Hastie*, with modifications to improve performance on large
    and complex data. We can summarize this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Each tree in a GBM is built in parallel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical variables can be split into groups instead of just using Boolean
    splits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared histograms are used to calculate cut points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H2O uses a greedy search of histogram bins, optimizing the improvement in squared
    error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important advantage of this implementation is that H2O GBM naturally handles
    high-cardinality categorical variables (that is, categorical variables with a
    lot of categories).
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XGBoost is very similar to classic GBM, with the main difference being the inclusion
    of a penalty term for the number of variables. Mathematically, this means it contains
    regularization terms in the cost function. Trees are grown in *breadth* rather
    than *depth*.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular GBM approach is LightGBM. The LightGBM algorithm builds trees
    as deep as necessary by repeatedly splitting the one leaf that gives the biggest
    gain. Unlike XGBoost, trees are grown in *depth* rather than *breadth*. In theory,
    LightGBM is optimized for sparse data. While H2O does not implement LightGBM directly,
    it provides a method for emulating the LightGBM approach using a set of options
    within XGBoost (such as setting `tree_method="hist"` and `grow_policy="lossguide"`).
    For more details, please refer to [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.html).
  prefs: []
  type: TYPE_NORMAL
- en: Boosting hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All boosting methods in H2O require the following hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of trees to be built, `ntrees` (the default is 50).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum tree depth, `max_depth` (the default is 6).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shrinkage parameter or learning rate, `learn_rate` (the default is 0.3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simply adding trees to boosting approaches without further restrictions can
    lead to overfitting. A grid search can aid in the process of hyperparameter tuning.
    Additional hyperparameters for boosting will be introduced in the *Model optimization
    with grid search* section.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Returning to the Lending Club data, now we are ready to build baseline models
    for each algorithm we are considering. By baseline, we mean models that have been
    fitted with settings at reasonable or default values. This will be the starting
    point in model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in [*Chapter 3*](B16721_03_Final_SK_ePub.xhtml#_idTextAnchor042),
    *Fundamental Workflow – Data to Deployable Model*, we start with the `bad_loan`
    response and the same set of predictors for all models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we remove the `bad_loan` response variable and the `issue_d`
    raw date variable from the predictors. Recall that `issue_d` was used to create
    two features, `issue_d_month` and `issue_d_year`, which are included in the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we fit a baseline H2O GBM model using a train-validate-test split, followed
    by a baseline XGBoost model using 5-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline GBM train-validate-test model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first model we fit is a default H2O GBM, trained on the 60%–20% training-validation
    split with the following default settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `model_id` parameter in the `gbm.train` command is optional and used
    to label the model object for identification in H2O Flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will investigate model diagnostics and explainability, in greater depth,
    in [*Chapter 7*](B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127), *Understanding
    ML Models*. Here, we are only using a couple of those commands to aid in comparing
    the gradient boosting algorithms. To begin with, we visualize the performance
    of the baseline GBM model across all splits using the `model_performance` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `%matplotlib` command allows figures to be displayed in a Jupyter notebook.
    This is only required once and is not needed outside of Jupyter. The first ROC
    curve is for the train split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – The ROC curve for the GBM train split'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.4_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – The ROC curve for the GBM train split
  prefs: []
  type: TYPE_NORMAL
- en: 'The second ROC curve for the validation split uses similar code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – The ROC curve for the GBM validation split'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.5_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – The ROC curve for the GBM validation split
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROC curve for the test split uses similar code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – The ROC curve for the GBM test split'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.6_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – The ROC curve for the GBM test split
  prefs: []
  type: TYPE_NORMAL
- en: 'To extract the AUC for these splits, we enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The code block and results, as produced in the Jupyter notebook, are shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – The GBM model performance results from the Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.7_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – The GBM model performance results from the Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, the train and validation performance values are stored in the
    model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return a dictionary, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – AUC from the GBM model object in the Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.8_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – AUC from the GBM model object in the Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: These results show that the baseline GBM model is overfitting on the training
    data. This is not a surprise.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a quick look at model interpretation, which we will cover in more
    depth in [*Chapter 7*](B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127), *Understanding
    ML Models*. The variable importance plot ranks variables in terms of relative
    importance in predicting bad loans. Relative importance for a variable is determined
    by checking whether that variable was used to split on and calculating the decrease
    in squared error across all trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to produce a variable importance plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – The baseline GBM variable importance plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.9_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – The baseline GBM variable importance plot
  prefs: []
  type: TYPE_NORMAL
- en: The resulting variable importance plot, as shown in *Figure 5.9*, shows that
    the address state, which is a high-cardinality categorical variable with 50 levels
    corresponding to the states in the United States, is by far the most important
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Baseline XGBoost cross-validated model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s build our baseline XGBoost model using 5-fold cross-validation and the
    train-test split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `nfolds` sets the number of folds, `seed` is optional
    and included here for instructional purposes, and `model_id` is an optional identifier
    for use in H2O Flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get AUC for the train and cross-validation sets directly from the model
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – The XGBoost model train and cross-validation performance results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.10_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – The XGBoost model train and cross-validation performance results
  prefs: []
  type: TYPE_NORMAL
- en: 'The test set AUC requires that we include the test data to be scored against:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – The XGBoost model test performance results from the Jupyter
    notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.11_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 – The XGBoost model test performance results from the Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily combine these results into a single dictionary using a little
    Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This Python code block produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – XGBoost model performance as a dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.12_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12 – XGBoost model performance as a dictionary
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the AUC values confirm that the baseline model is overfit on the training
    data and is far too optimistic. The fact that the cross-validation and test AUC
    values are in the same ballpark is comforting, as it means the cross-validation
    procedure is more accurately reflecting what you might see in the out-of-sample
    test data. This is an important check and might not always be the case, especially
    when the training and test splits cover different time periods. Next, let''s consider
    a variable importance plot for the baseline XGBoost model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – A baseline XGBoost variable importance plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.13_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.13 – A baseline XGBoost variable importance plot
  prefs: []
  type: TYPE_NORMAL
- en: A comparison of the variable importance plots for the GBM and XGBoost baseline
    models demonstrates some of the differences between these two boosting algorithms.
    Additionally, it introduces us to a more nuanced discussion of how to choose an
    algorithm given the multiple options under consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the most important variable in the H2O GBM model is `addr_state`,
    a high-cardinality categorical variable (with approximately 50 levels corresponding
    to the states in the United States). XGBoost defaults to the one-hot encoding
    of categorical variable levels. One-hot encoding represents each level of a categorical
    variable with a numeric variable containing 1 for the rows in that level and 0
    otherwise. The one-hot encoding of a categorical variable with 50 levels such
    as `addr_state` results in 50 new, relatively sparse variables corresponding to
    each state. In the XGBoost variable importance plot, states appear individually
    and far lower in importance, such as `addr_state_FL`, `addr_state_CA`, and `addr_state_NV`
    in the previous plot.
  prefs: []
  type: TYPE_NORMAL
- en: A data scientist could always address this issue with feature engineering approaches
    such as target encoding. Target encoding, which we will revisit in more detail
    later, is a method for replacing levels of categorical variables with representative
    numeric values. If target encoding is implemented, then the choice between XGBoost
    and H2O GBM might come down to pure performance. On the other hand, if target
    encoding is not an option, then H2O GBM should be the boosting algorithm choice.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, XGBoost requires target encoding, while H2O GBM gives the data
    scientist the option of modeling the high-cardinality categorical variables directly
    or by using a target-encoded version of those variables. This is a nice illustration
    of the interaction between algorithms, feature engineering choices, and potentially
    other factors such as business, compliance, or regulatory considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will turn our attention to improving our baseline models by using grid
    search to find the hyperparameter settings for model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization with grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing an algorithm for building a predictive model is not enough. Many algorithms
    have hyperparameters whose values have a direct impact on the predictive power
    of the model. So, how do you choose values for your hyperparameters?
  prefs: []
  type: TYPE_NORMAL
- en: A brute-force method would create a grid of all possible values and search over
    them. This approach is computationally expensive, takes an inordinate amount of
    time, and ultimately, yields results that are not much better than what we could
    achieve by other means. We have outlined a strategy for grid search that has proved
    effective in building optimized models while running in a reasonable amount of
    time.
  prefs: []
  type: TYPE_NORMAL
- en: The general strategy entails, first, tuning a few key parameters using a **Cartesian**
    grid search. These key parameters are those we expect to have the biggest impact
    on the results. Then, we fine-tune other parameters using a random grid search.
    This two-stage approach allows us to hone in on the computationally expensive
    parameters first.
  prefs: []
  type: TYPE_NORMAL
- en: 'From our experience with gradient boosting methods across many datasets from
    many domains, our strategy follows these principles:'
  prefs: []
  type: TYPE_NORMAL
- en: The optimal value for maximum allowed tree depth (`max_depth`) is heavily data-
    and problem-specific. Deeper trees, especially at depths greater than 10, can
    take significantly longer to train. In the interest of time, it is a good idea
    to, first, narrow the approximate depth down to a small range of values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We increase the number of trees (`ntrees`) until the validation set error starts
    increasing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Very low learning rates (`learn_rate`) are universally recommended. This generally
    yields better accuracy but requires more trees and additional computation time.
    A clever alternative is to start with a relatively high learning rate (say 0.05
    or 0.02) and iteratively shrink it by using `learn_rate_annealing`. For example,
    setting `learn_rate=0.02` and `learn_rate_annealing=0.995` speeds up convergence
    significantly without sacrificing much accuracy. This is very useful for hyperparameter
    searches. For even faster scans, values of 0.05 and 0.99 can be tried instead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sampling rows and columns using `sample_rate` and `col_sample_rate`, respectively,
    reduces the validation and test set error rates and improves generalization. A
    good starting point for most datasets is between 70% and 80% sampling on both
    rows and columns (rates of between 0.7 and 0.8). Optionally, the column sampling
    rate per tree parameter (`col_sample_rate_per_tree`) can be set. It is multiplicative
    with `col_sample_rate`. For example, setting both parameters to 0.9 results in
    an overall 81% of columns being considered for the split.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Early stopping using `stopping_rounds`, `stopping_metric`, and `stopping_tolerance`
    can make grid search more efficient. For our needs, we can use 5, AUC, and 1e-4
    as good starting points. This means that if the validation AUC doesn't improve
    by more than 0.0001 after 5 iterations, the computation will end.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To improve the predictive accuracy of highly imbalanced classification datasets,
    the `sample_rate_per_class` parameter can be set. This implements stratified row
    sampling based on the specific response class. The parameter values are entered
    as an array of ratios, one per response class, in lexicographic order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most of the other options only have a relatively small impact on model performance.
    That said, they might be worth tuning with a random hyperparameter search.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will build an optimized H2O GBM model for the Lending Club data and
    compare the results with the baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – a Cartesian grid search to focus on the best tree depth
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The optimal `max_depth` parameter value is very specific to the use case and
    data being modeled. Additionally, it has a profound impact on the model training
    time. In other words, large tree-depth values require significantly more computation
    than smaller values. First, we will focus on good candidate `max_depth` values
    using a quick Cartesian grid search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we use early stopping in conjunction with learning rate annealing to
    speed up convergence and efficiently tune the `max_depth` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We follow our strategy by defining an excessive number of trees with early
    stopping enabled. We use learning rate annealing, as shown in the following code
    block, to shrink the `learn_rate` and sample 80% of the rows and columns. Also,
    we score every 10 trees in order to make the early stopping reproducible. For
    model builds with large amounts of data, we might want to score every 100 or 1,000
    trees:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Setting the score_tree_interval Parameter
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scoring trees during a model grid search is essentially a waste of compute resources,
    as it requires more time to arrive at an optimum solution. However, it is required
    to make the early stopping process reproducible. We want to set the value high
    enough to ensure reproducibility but also not waste compute cycles. This is, largely,
    data- and problem-specific. The value of 10 that we used earlier is perhaps too
    aggressive even for this problem; 100 might have been more appropriate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we define the grid and set the search criteria to Cartesian:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we fit the grid, as shown in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To display the grid search results based on descending values of AUC, we use
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Tuning the maximum tree depth parameter value'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.14_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14 – Tuning the maximum tree depth parameter value
  prefs: []
  type: TYPE_NORMAL
- en: For this data and the H2O GBM algorithm, the `max_depth` values of 2 to 6 appear
    to give the best results. Next, we will search in the range of 2 to 6 and tune
    any additional parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – a random grid search to tune other parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have focused on a good range for the maximum tree depth, we can
    set up our tuning hyperparameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The `min_split_improvement` parameter attempts to reduce overfitting in the
    GBM and XGBoost models by demanding that each split does not lead to worse error
    measures. We will try four different settings of that parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following search criteria, we limit our runtime to 5 minutes for illustrative
    purposes. Additionally, we limit the number of models built to 10\. Depending
    on your use case, you might want to increase the runtime substantially or just
    exclude these options altogether:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we set up our final grid parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'And we fit our final grid, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Further Documentation
  prefs: []
  type: TYPE_NORMAL
- en: There are several additional hyperparameters available that are listed in the
    H2O documentation at [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.html).
  prefs: []
  type: TYPE_NORMAL
- en: Further details on grid search can be found at [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html#grid-search-in-python](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/grid-search.html#grid-search-in-python).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we train the model. Note that the `max_runtime_secs` setting, as follows,
    overrides the value set in `search_criteria_tune`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'After 3 minutes or less, we look at the results of our grid search sorted by
    `AUC`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – The grid search results for GBM model optimization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.15_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.15 – The grid search results for GBM model optimization
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Strategy Results
  prefs: []
  type: TYPE_NORMAL
- en: This exercise shows the importance of hyperparameter tuning. Although we constrained
    this optimization by only searching for 3 minutes and producing 10 models, 9 out
    of 10 outperformed the baseline GBM model with default values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily select the best model based on the previous leaderboard and extract
    its AUC performance values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'This Python code block produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – The performance of the best optimized GBM model from the grid
    search'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.16_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.16 – The performance of the best optimized GBM model from the grid
    search
  prefs: []
  type: TYPE_NORMAL
- en: Our grid search strategy is a tremendous way to fine-tune the hyperparameters
    of a machine learning model. Next, we will explore AutoML in H2O.
  prefs: []
  type: TYPE_NORMAL
- en: H2O AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most efficient method of model building and tuning utilizes H2O AutoML.
    AutoML builds models from multiple algorithms while implementing appropriate grid
    search and model optimization based on the model type. The user can specify constraints
    such as compute time limits or limits on the number of models created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some features of AutoML include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: AutoML trains a random grid of GLMs, GBMs, and DNNs using a carefully chosen
    hyperparameter space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Individual models are tuned using a validation set or with cross-validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two stacked ensemble models are trained by default: *All Models* and a lightweight
    *Best of Family* ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoML returns a sorted leaderboard of all models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any model can be easily promoted to production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacked ensembles** are highly predictive models that usually appear at the
    top of leaderboards. Similar to the other ensemble approaches that we introduced
    earlier (such as bagging and boosting), we stack works by combining information
    from multiple predictive models into one. Unlike bagging and boosting, which rely
    on weak learners as the component models, stacking works by optimally combining
    a diverse set of strongly predictive models. The *All Models* stacked ensemble
    is created by combining the entire list of models investigated in an AutoML run.
    The *Best of Family* ensemble contains, at most, six component models. Its performance
    is usually comparable to the All Models ensemble, but being less complex, it is
    typically better suited for production. (For more information on stacked ensembles,
    see [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training models using AutoML is relatively straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: The AutoML Runtime Parameter Choices
  prefs: []
  type: TYPE_NORMAL
- en: Our values for the `max_runtime_secs_per_model` and `max_models` parameters
    allow us to quickly screen multiple model types while restricting overall runtime.
    This is neither optimal nor recommended and is used in tutorial or classroom settings
    to demonstrate AutoML. Instead, you can set the overall `max_runtime_secs` parameter
    to an explicit value. The default is 3,600 (that is, 1 hour).
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O AutoML trains the following algorithms (in order):'
  prefs: []
  type: TYPE_NORMAL
- en: Three XGBoost GBMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A grid of GLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A DRF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Five H2O GBMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep neural net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An extremely randomized forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random grids of XGBoost GBMs, H2O GBMs, and deep neural nets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two stacked ensemble models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there is not enough time to complete all of these algorithms, some can be
    omitted from the leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: The AutoML leaderboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The AutoML object contains a leaderboard of models along with their cross-validated
    model performance. You can create a leaderboard for a specific dataset by specifying
    the `leaderboard_frame` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'The models are ranked by a metric whose default is based on the problem type:'
  prefs: []
  type: TYPE_NORMAL
- en: For regression, this is deviance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For binary classification, AUC is the default metric.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For multiclass classification, we use the mean per-class error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional metrics such as Logloss are provided for convenience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we print out the leaderboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.17 – The AutoML leaderboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.17_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.17 – The AutoML leaderboard
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the stacked ensemble models outperform all the individual models
    on the leaderboard. Any of these models can be selected for further investigation
    and potential deployment. Next, we will show you how to select the top model.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the top model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `aml.leader` object contains the best model from the leaderboard, including
    details for both training and cross-validated data. We use the following code
    to print the AUC values for training, cross-validation, and testing data for the
    best model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – The performance of the best model from the AutoML leaderboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.18_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.18 – The performance of the best model from the AutoML leaderboard
  prefs: []
  type: TYPE_NORMAL
- en: Examining a selected model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In practice, the leading model might not be the one you end up putting into
    production. As alluded to earlier, other considerations such as the modeling type,
    regulatory or compliance requirements, internal business preferences, and the
    likelihood of model approval could play a role in determining which model to use.
  prefs: []
  type: TYPE_NORMAL
- en: Other Reasons to Use a Leaderboard
  prefs: []
  type: TYPE_NORMAL
- en: The most obvious reason for using AutoML and exploring its leaderboard is to
    find the top model and put that into production. As mentioned earlier, that might
    not be allowed. Let's consider a scenario where I am only able to put a GLM into
    production. So, why bother fitting other models using AutoML? One answer is that
    the best model gives me a practical ceiling that I can also report. *GLM has an
    AUC of 0.69905, while the best possible model yielded 0.71336*.
  prefs: []
  type: TYPE_NORMAL
- en: In a business context, I should always be able to translate performance differences
    into terms of cost reduction or increased profit. In other words, the AUC difference
    translated into dollars and cents is "the cost of doing business" or "how much
    money is being left on the table" by using the selected model instead of the best.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we demonstrate how to select any model from the leaderboard. The top
    individual (non-ensemble) model is in third position. We select this model with
    the following code and examine its performance in terms of AUC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – The performance of the selected model from the AutoML leaderboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.19_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.19 – The performance of the selected model from the AutoML leaderboard
  prefs: []
  type: TYPE_NORMAL
- en: Once a model object has been selected via AutoML, all the model diagnostics
    and explainability procedures, which we will cover in [*Chapter 7*](B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127),
    *Understanding ML Models*, will be available.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will demonstrate how feature engineering can lead to better
    predictive models. Second only to data cleaning, typically, feature engineering
    is the most time-consuming of all tasks involved in the modeling process. It can
    also be the "secret sauce" that makes for a great predictive model.
  prefs: []
  type: TYPE_NORMAL
- en: So, what does feature engineering mean? Put simply, it is how to extract information
    from raw data into a form that is both usable by the modeling algorithm and interpretable
    for the problem at hand. For example, a date or date-time object might be represented
    in data as a string or a number (for example, Unix time is the number of seconds
    since 00:00:00 UTC on January 1, 1970). Presented with such features, an algorithm
    is liable to treat dates as levels of a categorical variable or a continuous numeric
    value. Neither of these forms is very helpful. However, embedded in this raw data
    is information about not only the day, the month, and the year, but the day of
    the week, the weekend or weekday, seasons, holidays, and more. If the object contains
    time, then you could also produce the hour of the day, the time of day (for example,
    morning, afternoon, evening, or night), and more.
  prefs: []
  type: TYPE_NORMAL
- en: Which features to create depends largely on the use case. Even in the best of
    circumstances, most engineered features may not be selected by a model algorithm.
    Subject-matter expertise and an understanding of the context of the problem play
    a major role in engineering good features. For example, the debt-to-income ratio
    used in lending divides how much a customer owes per month by their monthly income.
    This engineered feature has proven so predictive in risk modeling that it has
    been given its own name and abbreviation, DTI.
  prefs: []
  type: TYPE_NORMAL
- en: Subject-Matter Expertise and Feature Engineering
  prefs: []
  type: TYPE_NORMAL
- en: One of our colleagues, an accomplished data scientist, multiple Kaggle grandmaster,
    and a Ph.D., once commented that he did not enjoy FinTech data science competitions
    because "there is more Fin than Tech in them." By this, he meant, at least in
    part, that those problems put a premium on subject-matter insights that he had
    no experience of.
  prefs: []
  type: TYPE_NORMAL
- en: Another great example of feature engineering is **natural language processing**
    (**NLP**) in the context of predictive modeling. NLP attempts to represent words,
    word meanings, and sentences as numeric values that can be incorporated naturally
    into machine learning algorithms. TF-IDF and word embeddings (word2vec) are two
    such approaches. We will cover these in more detail in the *Modeling in Sparkling
    Water* section of [*Chapter 6*](B16721_06_Final_SK_ePub.xhtml#_idTextAnchor106),
    *Advanced Model Building – Part II*, and in the detailed Lending Club analysis
    within [*Chapter 8*](B16721_08_Final_SK_ePub.xhtml#_idTextAnchor137), *Putting
    It All Together*.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this section, we will investigate target encoding in depth.
    Target encoding is one of the most common and impactful feature engineering options
    available. We will illustrate its use in the **Lending Club model**. In [*Chapter
    8*](B16721_08_Final_SK_ePub.xhtml#_idTextAnchor137), *Putting It All Together*,
    we will implement additional feature engineering recipes to improve the predictive
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Target encoding replaces categorical levels with a numeric value representing
    some function of the target variable, such as the mean. The following diagram
    illustrates mean target encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Mean target encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.20_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.20 – Mean target encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach is simple: replace the categorical feature levels (**A**, **B**,
    and **C**) with their respective means (**0.75**, **0.66**, and **1.00**).'
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding is a clever idea and, in spirit, is analogous to the random
    effects found in statistical random and mixed effect models. In fact, for certain
    simple cases, you can prove that mean target encoding actually yields the empirical
    Bayes estimates of the random effects. What this means is that the intent behind
    target encoding is based on sound principles.
  prefs: []
  type: TYPE_NORMAL
- en: However, target encoding uses a function of the target as an input to predict
    the target. This is the very definition of data leakage. Data leakage leads to
    overly optimistic models that do not generalize well and are, at best, misleading
    in practice. H2O implements target encoding using carefully constructed cross-validation
    procedures. Essentially, this eliminates data leakage by calculating the target-encoded
    value for each row based on other folds of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Random Effects
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical structure underlying the estimation of random effects in statistical
    models does not suffer from data leakage concerns in the same way that target
    encoding does. This is because the information in the target variable is partitioned,
    and the portion used to estimate the random effects is separate from the portion
    used to estimate the other model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the **H2O-3 Target Encoding Estimator** to replace categorical values
    with a mean of the target variable. We tune target encoding via the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting `data_leakage_handling` to `k-fold` controls data leakage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding random `noise` to the target average helps to prevent overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We adjust for categories with small group sizes through `blending`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any categorical levels with fewer observations will result in an unreliable
    (high variance) target-encoded mean. A blended average consisting of a weighted
    average of the group's target value and the global target value can improve this
    estimate. By setting `blending=True`, the target mean will be weighted based on
    the sample size of the categorical level.
  prefs: []
  type: TYPE_NORMAL
- en: When blending is enabled, the `smoothing` parameter controls the rate of transition
    between the level's posterior probability and the prior probability (with a default
    value of 20). The `inflection_point` parameter represents half of the sample size
    for which we completely trust the estimate. The default value is 10.
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding the Lending Club data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To determine whether a categorical variable would benefit from target encoding,
    first, create a table for the variable, which has been sorted from most frequent
    to least frequent. To do this efficiently, we will define a Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: Note that the preceding code requires the Python pandas package to be available
    since the `as_data_frame` call outputs the table in a pandas format.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, consider the `purpose` variable, which records the purpose of the loan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Levels of the purpose variable'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.21_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.21 – Levels of the purpose variable
  prefs: []
  type: TYPE_NORMAL
- en: Note the high concentration of loans for debt consolidation (46%) and the sizable
    numbers for both credit cards (13%) and other (11%), with the remaining 30% captured
    among 11 other loan purposes. One option for this data would be to collapse the
    categories into fewer levels and leave the `purpose` variable as a categorical
    variable. This might make sense if the categories could be collapsed in a coherent
    manner. A better option uses mean target encoding to represent all levels without
    overfitting those with small percentages in the tail. Blending will also be enabled
    here, although the amount of smoothing it provides might not be impactful. The
    `renewable_energy` category has 75 observations, which, in most cases, is sufficient
    to reliably estimate a mean even though the percentage is very small.
  prefs: []
  type: TYPE_NORMAL
- en: 'A second variable to consider is `addr_state`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'The first few rows are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – The top ten states by count'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.22_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.22 – The top ten states by count
  prefs: []
  type: TYPE_NORMAL
- en: 'And the last few rows are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – The last seven states by count'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.23_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.23 – The last seven states by count
  prefs: []
  type: TYPE_NORMAL
- en: 'High-cardinality categorical variables such as `addr_state` are prime candidates
    for target encoding. The distribution of records is also highly skewed, with the
    top 4 levels accounting for, approximately, 40% of the data. Blending will be
    especially important because the raw counts for states in the tail are extremely
    small:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the target encoding estimator and specifying the columns
    to encode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `k_fold` strategy requires a fold column, which is created as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train a target encoding model by setting the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a new target-encoded train and test set, explicitly setting the
    noise level on the test set to `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, check the results of target encoding by looking at histograms of the
    target-encoded variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This yields the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – The target-encoded loan purpose variable'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.24_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.24 – The target-encoded loan purpose variable
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code produces a histogram for the `addr_state_te` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – The target-encoded address state variable'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.25_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.25 – The target-encoded address state variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the target-encoded variables to the predictor list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, remove the source columns, using a list comprehension for efficiency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we create other features, our predictor list will change. In order to keep
    track of these steps, it is wise to update a copy of the `predictors` list rather
    than the original:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Additionally, we rename our datasets using the target-encoded values for convenience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How Much Should You Tune the Target Encoding Model?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note that we used the same target encoding parameters for transforming two
    different variables. So, why not encode variables individually with custom parameter
    settings for each? In our situation, we did not need to. The only parameter values
    to vary are those that determine the amount of blending: `inflection_point` and
    `smoothing`. For the `purpose` variable, blending is not really needed because
    sample sizes are large enough to yield accurate means. On the other hand, the
    `addr_state` variable would greatly benefit from blending. Therefore, we set the
    parameters to values that are reasonable for `addr_state`. These will, essentially,
    be ignored by `purpose`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In situations where the outputs of one model are inputs for another, always
    bear in mind that what matters is the effect that varying parameter settings in
    the input model have on the final model's predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s refit our model with these new features using AutoML and print the leaderboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.26 – The AutoML leaderboard after target encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.26_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.26 – The AutoML leaderboard after target encoding
  prefs: []
  type: TYPE_NORMAL
- en: The best individual (non-ensemble) model is a GBM, whose performance (**0.704491**)
    is only slightly better than the best GBM (**0.703838**) prior to target encoding.
    People often ask, is this tiny performance gain worth the effort of target encoding?
    That question misses the point entirely. Recall that H2O GBM naturally handles
    high-cardinality categorical variables well, so the fact that performance is equivalent
    should come as no surprise.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is the right question to ask? Let''s look at variable importance and compare
    the variables before and after target encoding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before target encoding, the high-cardinality variables are among the most important:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.27 – Variable importance for the H2O GBM model before target encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.27_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.27 – Variable importance for the H2O GBM model before target encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'After target encoding, the importance of these categorical variables has changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28 – Variable importance for the H2O GBM model after target encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.28_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.28 – Variable importance for the H2O GBM model after target encoding
  prefs: []
  type: TYPE_NORMAL
- en: The effect of target encoding for the GBM model has less to do with overall
    model performance than with the impact and interpretation of those variables.
    Target encoding `purpose` has only slightly changed its importance, from *third*
    place to *fifth* place. Target encoding `addr_state` has decreased its impact
    substantially, from *first* place to *seventh* place. This impact difference also
    leads to an interpretability difference. The former model primarily splits on
    state, in essence implying a different loan default model per state (with implications
    that might have to be explained to a regulator). In the latter model, the effect
    of the state is adjusted in a very similar manner to random effects in a statistical
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The data scientist has the option of choosing which scenario makes the most
    sense for their situation. An additional benefit of target encoding `addr_state`
    is the blending feature, which, in production, will generalize better for states
    with low counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the best XGBoost target-encoded model from the leaderboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.29 – The XGBoost model variable importance plot after target encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.29_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.29 – The XGBoost model variable importance plot after target encoding
  prefs: []
  type: TYPE_NORMAL
- en: Both `purpose` and `address_state` have entered the top 10 in positions almost
    identical to the GBM model. Target encoding categorical variables is more important
    in XGBoost models than in GBM models. Other considerations being equal, some feature
    engineering steps may be influenced by the algorithm chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Other feature engineering options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple ways in which to categorize feature engineering options,
    and there are almost an infinite number of approaches you could take, depending
    on the problem. For some high-level categorizations, we can think of the following
    rough hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algebraic transformers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add, subtract, multiply, or divide numeric columns to create new interaction
    features.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use simple mathematical functions such as log, exp, power, roots, and trigonometric
    functions
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cluster-based transformers: Use k-means or other unsupervised algorithms to
    create clusters. Then, do the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measure the distance of a numeric observation to a specified cluster.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider each cluster as a level of a categorical variable and target encode
    to clusters.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numeric to categorical transformations: Often, binning into deciles or using
    histograms and then taking the mean within each bin produces good predictive features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Categorical to numeric transformations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-hot or indicator value encoding.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Target encoding.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numeric summary encoding: This is similar to target encoding except you are
    summarizing one of the numeric predictor columns rather than the target variable;
    for example, the mean temperature per state.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Weight of evidence**: This is only used for binary classification problems.
    The weight of evidence is the natural log of the ratio of successes over failures
    (good over bad and ones over zeros):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B16721_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Dimension reduction transformations: Truncated eigenvalue or singular value
    decomposition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a data scientist, you can combine multiples of these components into a reasonable
    feature for a particular problem at hand. We will revisit some of these recipes
    in our complete analysis of the Lending Club data, which can be found in [*Chapter
    8*](B16721_08_Final_SK_ePub.xhtml#_idTextAnchor137), *Putting It All Together*.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging H2O Flow to enhance your IDE workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'H2O Flow is a web-based UI available wherever an H2O cluster is running. Flow
    is interactive, allowing users to do everything including import data, build models,
    investigate models, and put models into production. While incredibly easy to use,
    our experience is that most data scientists (authors included) prefer coding in
    Python or R to menu-driven interactive interfaces. This section is written for
    those data scientists: why use Flow when I am a coder?'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring** the state of the H2O cluster and the jobs that are being run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive investigation** of the data, models, model diagnostics, and more
    where interactivity is an asset rather than an annoyance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting to Flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By default, Flow is started on port 54321 of the H2O server as the cluster
    is launched (this port is configurable at startup). Enter `Error! Hyperlink reference
    not valid.` into your browser to open Flow. The Flow UI is straightforward and
    self-explanatory, with helpful instructions and videos:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.30 – The H2O Flow UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.30_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.30 – The H2O Flow UI
  prefs: []
  type: TYPE_NORMAL
- en: First, let's consider Flow's monitoring capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring with Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under the **Admin** menu in Flow, the top three options are **Jobs**, **Cluster
    Status**, and **Water Meter**. These are central to the monitoring capabilities
    of Flow, and we will review each of them individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Flow **Admin** menu is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.31 – The monitoring options using the Flow Admin menu'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.31_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.31 – The monitoring options using the Flow Admin menu
  prefs: []
  type: TYPE_NORMAL
- en: We start by monitoring jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring jobs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `getJobs` in the Flow command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.32 – Listing the job options using the Flow Admin menu'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.32_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.32 – Listing the job options using the Flow Admin menu
  prefs: []
  type: TYPE_NORMAL
- en: We continue by monitoring health.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring H2O cluster health
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `getCloud` command. This monitors the health of the cluster and is one
    of the first places to check whether H2O does not appear to be working correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.33 – Monitoring the cluster status using the Flow Admin menu'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.33_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.33 – Monitoring the cluster status using the Flow Admin menu
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will monitor CPU usage.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring CPU usage live
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Water Meter** tool is a useful monitor of CPU usage. It shows a bar per
    CPU with colors corresponding to the activity status of each CPU. Rather than
    watching a black progress bar grow across the cell of a Jupyter notebook, the
    Water Meter is much more informative. Also, it illustrates, in real time, how
    well a particular computation is distributed among the available compute resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.34 – The H2O Flow Water Meter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.34_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.34 – The H2O Flow Water Meter
  prefs: []
  type: TYPE_NORMAL
- en: We can also monitor grid search.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring grid search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: H2O Flow allows you to interactively monitor individual model builds, but it
    is especially useful when executing multiple jobs like a grid search or AutoML
    creates. These can be monitored live upon launch and reviewed during runtime and
    after completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in our grid search strategy was to evaluate model depth. While
    the model is running, we can open Flow and list jobs. The job named `gbm_depth_grid`
    is running. Clicking on the name opens the running job, allowing us to view more
    details or cancel the job. These actions are not readily available from within
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.35 – Grid search job monitoring within Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.35_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.35 – Grid search job monitoring within Flow
  prefs: []
  type: TYPE_NORMAL
- en: 'Selecting the **View** button at any time opens the grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.36 – Grid search results within Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.36_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.36 – Grid search results within Flow
  prefs: []
  type: TYPE_NORMAL
- en: The subsequent selection of any of the individual grid models opens an interactive
    model view, which we will discuss in more detail in the next section and in [*Chapter
    7*](B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127), *Understanding ML Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring AutoML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Monitoring AutoML jobs is similar. First, search for the AutoML build job in
    the job listings and select the model''s name link:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.37 – Selecting the AutoML build job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.37_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.37 – Selecting the AutoML build job
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the AutoML build is in process, you can monitor the progress live or click
    on **View** to watch the leaderboard as the models are built:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.38 – Viewing the AutoML build job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.38_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.38 – Viewing the AutoML build job
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Flow is Great for Monitoring Leaderboards'
  prefs: []
  type: TYPE_NORMAL
- en: The interactive leaderboard is a great way to monitor AutoML jobs in real time.
    This is especially true for AutoML runs that are not constrained to finish quickly
    but plan to run for multiple hours as models are built. Again, all that is available
    in Python is a progress bar that can seem very slow if you cannot see the actual
    work on the server (Figure 5.39).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.39 – The AutoML leaderboard in Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.39_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.39 – The AutoML leaderboard in Flow
  prefs: []
  type: TYPE_NORMAL
- en: The selection of any individual AutoML model opens an interactive model view.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive investigations with Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, interactivity in Flow is quite useful for the live
    monitoring of running jobs. In addition, Flow makes exploring data before modeling
    and evaluating candidate models after model build more convenient than coding
    in Python. The only potential downside to menu-driven exploration is when reproducibility
    is at a premium and documentation of the whole modeling process is required. We
    will explore this topic in more detail when we discuss H2O AutoDoc capabilities
    in [*Chapter 7*](B16721_07_Final_SK_ePub.xhtml#_idTextAnchor127), *Understanding
    ML Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive data exploration in Flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Data** menu, select **List All Frames**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.40 – Listing data frames in Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.40_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.40 – Listing data frames in Flow
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **LendingClubClean.hex** link to pull up the data summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.41 – The Lending Club data in Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.41_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.41 – The Lending Club data in Flow
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on the `purpose` column link produces a summary plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.42 – The loan purpose data column in Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.42_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.42 – The loan purpose data column in Flow
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, clicking on `Inspect` and then `domain` will yield a summary table similar
    to the one that we created in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.43 – A table of the loan purpose data in Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.43_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.43 – A table of the loan purpose data in Flow
  prefs: []
  type: TYPE_NORMAL
- en: Model exploration in Flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Selecting any model in Flow, whether through the **List All Models** option
    in the **Model** menu item, from a grid search, or the AutoML leaderboard, yields
    a model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.44 – A GBM model summary from AutoML in Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.44_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.44 – A GBM model summary from AutoML in Flow
  prefs: []
  type: TYPE_NORMAL
- en: 'The layout of the model summary makes it very easy to explore. ROC curves and
    AUC values for training and validation sets are displayed by default. Variable
    importance plots are also readily available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.45 – GBM variable importance in Flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.45_B16721.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.45 – GBM variable importance in Flow
  prefs: []
  type: TYPE_NORMAL
- en: Generally, getting results immediately through the model summary is more convenient
    than doing the equivalent from the Python client.
  prefs: []
  type: TYPE_NORMAL
- en: Best Practices in Flow
  prefs: []
  type: TYPE_NORMAL
- en: If you are coding in Python, we strongly suggest using Flow solely as a monitoring
    platform and read-only tool. That is the approach we use in our own work. Code
    should contain all the steps that import data, create features, fit models, deploy
    models, and more. This allows you to repeat any analysis and is a prerequisite
    for reproducibility. Code is often less convenient for investigative and interactive
    steps. Reserve those for Flow.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together – algorithms, feature engineering, grid search, and
    AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The H2O AutoML implementation is simple yet powerful, so why would we ever need
    grid search? In fact, for a lot of real-world enterprise use cases, any of the
    top candidates in an AutoML leaderboard would be great models to put into production.
    This is especially true of the stacked ensemble models produced by AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: However, our coverage of grid search was not just to satisfy intellectual curiosity.
    A more involved process, which we will outline next, uses AutoML followed by a
    customized grid search to discover and fine-tune model performance.
  prefs: []
  type: TYPE_NORMAL
- en: An enhanced AutoML procedure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start by running AutoML on your data to create a baseline leaderboard. You can
    investigate leading models, gain an understanding of the runtimes required to
    fit algorithms to your data, and more, which may inform future AutoML parameter
    choices and expectations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second stage is feature engineering. While developing new features, repeat
    AutoML runs as desired to check the impact of engineering and see what other insights
    might be gained from diagnostics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After completion of the feature engineering stage, use AutoML to create a final
    leaderboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a model from the leaderboard as a candidate for production. If you select
    an ensemble model, you are done. There is very little you can do to improve upon
    the performance of a stacked ensemble.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you choose an individual model, say a GBM or DRF, use the parameters of that
    model as a guide for further grid searching, employing the general strategy outlined
    in this chapter. It is possible to further fine-tune a candidate model using additional
    grid search.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This enhanced AutoML procedure might be overkill for a lot of problems. If you
    are in a business that has a practice of quickly building and deploying models,
    especially one that updates or replaces models frequently, then this approach
    might literally be more effort than it is worth. The advantages of a model built
    on recent data often outweigh the gains made by using these extra modeling steps.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you are in an industry where the model review and due diligence
    process is long and involved, where the models that are put into production tend
    to stay in production for a long time, or you are working on a model that is high
    risk in any way (for example, one that directly impacts peoples' lives rather
    than just what ad they will see on a website), then this more involved procedure
    might well be worth the extra effort. We have used it successfully in multiple
    real-world use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have considered different options for splitting data, explored,
    in some depth, powerful and popular algorithms such as gradient boosting and random
    forest, learned how to optimize model hyperparameters using a two-stage grid search
    strategy, utilized AutoML to efficiently fit multiple models, and further investigated
    options for feature engineering, including a deep dive into target encoding. Additionally,
    we saw how Flow can be used to monitor the H2O system and investigate data and
    models interactively. You now have most of the tools required to build effective
    enterprise-scale predictive models using the H2O platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we are not finished with our advanced modeling topics. In [*Chapter
    6*](B16721_06_Final_SK_ePub.xhtml#_idTextAnchor106), *Advanced Model Building
    – Part II*, we will discuss best practices for data acquisition, look in more
    depth at checkpointing and refitting models, and show you how to ensure reproducibility.
    Additionally, we will thoroughly consider two more hands-on examples: the first
    demonstrating Sparkling Water pipelines for efficiently integrating Spark capabilities
    with H2O modeling, and the second introducing isolation forests, an unsupervised
    learning algorithm for anomaly detection in H2O.'
  prefs: []
  type: TYPE_NORMAL
