- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing the Human in the Loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Active ML promises more efficient ML by intelligently selecting the most informative
    samples for labeling by human oracles. However, the success of these human-in-the-loop
    systems depends on effective interface design and workflow management. In this
    chapter, we will cover best practices for optimizing the human role in active
    ML. First, we will explore interactive system design, discussing how to create
    labeling interfaces that enable efficient and accurate annotations. Next, we will
    provide an extensive overview of the leading human-in-the-loop frameworks for
    managing the labeling pipeline. We will then turn to handling model-label disagreements
    through adjudication and quality control. After that, we will discuss strategies
    for recruiting qualified labelers and managing them effectively. Finally, we will
    examine techniques for evaluating and ensuring high-quality annotations and properly
    balanced datasets. By the end of this chapter, you will have the skills to build
    optimized human-in-the-loop systems that fully leverage the symbiosis between
    humans and AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing interactive learning systems and workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling model-label disagreements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effectively managing human-in-the-loop systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring annotation quality and dataset balance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using the `huggingface` package, so you’ll need
    to install it, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Plus, you will need the following imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Designing interactive learning systems and workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The effectiveness of a human-in-the-loop system depends heavily on how well
    the labeling interface and workflow are designed. Even with advanced active ML
    algorithms selecting the most useful data points, poor interface design can cripple
    the labeling process. Without intuitive controls, informative queries, and efficient
    workflows adapted to humans, annotation quality and speed will suffer.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover best practices for optimizing the human experience
    when interacting with active ML systems. Following these guidelines will enable
    you to create intuitive labeling pipelines, minimize ambiguity, and streamline
    the labeling process as much as possible. We will also discuss strategies for
    integrating active ML queries, collecting labeler feedback, and combining expert
    and crowd labelers. By focusing on human-centered design, you can develop interactive
    systems that maximize the utility of human input for your models.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, we’ll provide definitions for the terms mentioned previously as they
    will be the main focus of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **labeling interface** is a user interface through which human annotators
    provide labels for data samples, such as *Roboflow*, *Encord*, and *LabelBox*,
    to name a few. It includes the visual presentation of each sample, as well as
    the controls and mechanisms for entering or selecting the desired labels. For
    example, an object detection labeling interface may display an image and provide
    tools to draw bounding boxes around objects and select class labels for each box.
    The annotator can then label cars, pedestrians, animals, and other objects appearing
    in the image using the interface controls. This can be seen in *Figure 3**.1*,
    where the bounding boxes have been drawn around the dogs using the *dog* class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Example of a labeling interface where the annotator is drawing
    bounding boxes around the dogs](img/B21789_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Example of a labeling interface where the annotator is drawing
    bounding boxes around the dogs
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk about workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **workflow** is the end-to-end sequence of steps that’s followed by the annotator
    to complete the labeling task. It encompasses the full life cycle of a labeling
    job, from receiving the samples to be labeled, interacting with the interface
    to apply labels, submitting the completed annotations, and potentially handling
    exceptions or errors, as depicted in *Figure 3**.2*. Optimizing the workflow involves
    streamlining these steps so that annotators can complete labeling efficiently.
    For an image classification task, the workflow may proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The annotator logs in to the labeling system and receives a batch of images
    to label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The annotator is shown the first image and uses the interface to apply labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The annotator submits the labeled image and moves on to the next image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After labeling the batch, the annotator submits the job, which triggers a reviewing
    stage for a quality check to be performed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A reviewer checks the labeled images for accuracy and consistency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any errors are found, the images with incorrect annotations are returned
    to the annotator, at which point they need to re-label those images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once all the images pass the quality checks, the job is marked as complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Illustration of the labeling workflow](img/B21789_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Illustration of the labeling workflow
  prefs: []
  type: TYPE_NORMAL
- en: In short, the labeling interface focuses on the specific moment of interacting
    with an individual sample and applying labels. The workflow looks more holistically
    at the overall process and how to smoothly guide annotators through their labeling
    work. An effective human-in-the-loop system needs to design both aspects carefully
    around the human user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know what a labeling interface and a workflow are, we understand
    that they can greatly impact the efficiency, accuracy, and overall quality of
    annotations in an active ML system. When designing interactive systems, there
    are several key considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intuitive and efficient interfaces**: The labeling interface should be intuitive
    and easy to use. When choosing the labeling interface that you want to use for
    a project, keep in mind that the UI the labelers will use has to be simple and
    efficient. For example, is it easy to draw a bounding box or a polygon around
    the objects of interest in a computer vision annotation project? Are there features
    to speed up the labeling process, such as using a pre-trained model such as Meta’s
    **Segment Anything Model** (**SAM**) ([https://segment-anything.com/](https://segment-anything.com/)),
    which can segment any object in images and pre-label the objects on the image?
    A good example of this is the Smart Polygon feature provided by the labeling platform
    Roboflow, which allows users to automatically label objects with polygons or bounding
    boxes, as shown in *Figure 3**.3*. We will discuss Roboflow later in this chapter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.3 - Roboflow’s Smart Polygon feature using SAM to automatically
    label objects on images, as demonstrated at https://blog.roboflow.com/automated-polygon-labeling-computer-vision/](img/B21789_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 - Roboflow’s Smart Polygon feature using SAM to automatically label
    objects on images, as demonstrated at [https://blog.roboflow.com/automated-polygon-labeling-computer-vision/](https://blog.roboflow.com/automated-polygon-labeling-computer-vision/)
  prefs: []
  type: TYPE_NORMAL
- en: '**Onboarding resources**: Minimal training should be required for labelers
    to use the labeling platform. To minimize the amount of training needed, an effective
    approach is to provide an onboarding document initially written by someone knowledgeable
    about using the labeling interface. This document can then be edited and updated
    by the labelers themselves as they learn how to overcome obstacles. This way,
    the training for labelers becomes less exhaustive as they can utilize the knowledge
    gained by each labeler and pass on what they''ve learned to new members joining
    the team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ontologies – naming convention**: An important consideration when designing
    a workflow for an annotation project is to carefully choose the names of the classes
    used for annotation. Having multiple spellings or words for the same class is
    a common issue. For example, in a project that aims to classify different types
    of pets, if a labeler decides to use the breed of the dog (for example, Australian
    Shepherd) instead of the class *dog*, it can cause issues later on. Fixing these
    issues is not always easy and can be time-consuming and expensive. Therefore,
    it is essential to select a labeling platform that allows the use of a labeling
    ontology. A **labeling ontology** is a structured vocabulary that provides a common
    understanding of how to label data, defines the classes, and outlines the relationships
    between different classes, especially in cases where nested structures exist.
    The ontology should also support the correction of typos or incorrect naming.
    In such cases, the names of the classes should be modifiable and update all objects
    labeled using that class. Additionally, the ontologies should be shareable across
    annotation projects to ensure consistency throughout. The ontology should support
    annotations of different styles, such as bounding boxes, polygons, classifications,
    and more. For example, the labeling platform Encord has a flexible labeling ontology
    feature, as depicted in *Figure 3**.4*. We will discuss Encord later in this chapter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Encord’s labeling ontology](img/B21789_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Encord’s labeling ontology
  prefs: []
  type: TYPE_NORMAL
- en: '**Informative queries**: The samples that are presented to labelers should
    provide sufficient context and information to ensure clarity in the labeling task.
    This may involve providing complete documents or images instead of just extracts.
    Queries should be designed to minimize any potential ambiguity. For example, when
    using active ML to select the most informative frames for labeling from unlabeled
    videos, it is crucial to carefully organize the workflow to ensure that labelers
    are aware of the specific video they are labeling. In this case, the solution
    would be to ensure that the selected frames are separated by video and sorted
    before being sent to the labeling platform. The frames should then be presented
    to the labelers as a video with *jumps* due to the absence of non-selected frames.
    However, it is important to note that these missing frames do not remove the overall
    context entirely. The labeling platform Encord offers a solution for cases like
    this one with their feature called *image sequences*. Their image sequences format
    presents groups of images as a video to the labelers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated workflows**: Labeling tasks should be automated as much as possible.
    The process of creating a task should involve minimal human intervention. One
    effective method for achieving this automation is by implementing a script that
    can run model inference on the unlabeled data pool, then use these predictions
    in the active ML sampling, particularly when utilizing uncertainty sampling, and,
    finally, send the selected data samples to the labeling platform and assign them
    to labelers and reviewers based on their availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labeler feedback**: Allowing labelers to provide feedback or ask questions
    on difficult or ambiguous samples enables the annotation quality to improve over
    time. Therefore, the labeling platform should include a commenting and chatting
    system, which would allow labelers to help each other or seek guidance from field
    experts and reviewers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By focusing on these aspects, you can create labeling systems that are adapted
    to human strengths and limitations. Well-designed interfaces and workflows result
    in more accurate, consistent, and efficient annotations, which are essential for
    the success of an ML project. Now, let’s explore the current labeling platforms
    that you can consider.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring human-in-the-loop labeling tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Human-in-the-loop labeling frameworks** are critical for enabling effective
    collaboration between humans and ML systems. In this section, we will explore
    some of the leading human-in-the-loop labeling tools for active ML.'
  prefs: []
  type: TYPE_NORMAL
- en: We will look at how these frameworks allow humans to provide annotations, verify
    predictions, adjust model confidence thresholds, and guide model training through
    interfaces and workflows optimized for human-AI collaboration. Key capabilities
    provided by human-in-the-loop frameworks include annotation-assisted active ML,
    human verification of predictions, confidence calibration, and model interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: The labeling tools we will examine include Snorkel AI, Prodigy, Encord, Roboflow,
    and others. We will walk through examples of how these tools can be leveraged
    to build applied active learning systems with effective human guidance. The strengths
    and weaknesses of different approaches will be discussed. By the end of this section,
    you will have a solid understanding of how to determine the right human-in-the-loop
    framework for your ML project based on your use case needs and constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Common labeling platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many labeling platforms offer a range of features for data labeling, including
    AI-assisted labeling, active ML, collaboration tools, and quality control tools.
    However, they vary in terms of pricing, model training, deployment capabilities,
    data management and curation tools, and model explainability features. For instance,
    when we examine six of the most frequently used labeling platforms, as depicted
    in *Figure 3**.5*, we can observe these distinctions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Comparison table of six of the most common labeling platforms](img/B21789_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Comparison table of six of the most common labeling platforms
  prefs: []
  type: TYPE_NORMAL
- en: Overall, all of the labeling platforms mentioned previously offer a variety
    of advantages and disadvantages. It is important to choose a platform that is
    well suited to the specific needs and requirements of the ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: If you are seeking a platform with a wide array of features, including AI-assisted
    labeling, active learning, collaboration tools, and quality control, then Snorkel
    AI ([https://snorkel.ai/](https://snorkel.ai/)), Encord ([https://encord.com/](https://encord.com/)),
    LabelBox ([https://labelbox.com/](https://labelbox.com/)), and Dataloop ([https://dataloop.ai/](https://dataloop.ai/))
    may be suitable options for you. On the other hand, if you require a platform
    specifically designed for **natural language processing** (**NLP**) tasks, then
    Prodigy ([https://prodi.gy/](https://prodi.gy/)) might be a good choice.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll examine how to handle situations where the model and the human disagree.
  prefs: []
  type: TYPE_NORMAL
- en: Handling model-label disagreements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Disagreements between model predictions and human labels are inevitable. In
    this section, we will study how to identify and resolve conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: Programmatically identifying mismatches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To identify discrepancies between the model’s predictions and the human-annotated
    labels, we can write some simple Python code that highlights the mismatches for
    review.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the example of an NLP sentiment classifier. This type of classifier
    is designed to analyze and understand the sentiment or emotions expressed in text.
    By examining the words, phrases, and context used in a given piece of text, an
    NLP sentiment classifier can determine whether the sentiment is positive, negative,
    or neutral. First, we will use the `sentiment-analysis` model from Huggingface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The model correctly classifies these two sentences. Now, we want to study what
    flags a mismatch between a labeled dataset and the model’s predictions for further
    review. So, we will download a labeled dataset from `huggingface` called *imdb*.
    This dataset is a large movie review dataset that’s used for binary sentiment
    classification. We can load this dataset with the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For testing purposes, we’ll only use a few samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can take a better look at this dataset by looking at the last item in the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have a field called `text`, which provides a review of the movie, and
    a field called `label`, which classifies whether the sentiment is positive or
    negative. In this case, it is positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s gather the model’s predictions on these five samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to find out if we have mismatches with the original annotations from the
    dataset, we must define `x_true` and `y_true`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `x_true` is an array of the reviews and `y_true` is an array of labels.
    We can compare these to the model’s predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, data points where the model and human disagree have been returned. These
    would be selected for additional review in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Manual review of conflicts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After sampling the mismatched cases, we can do a manual review. Here are some
    example scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: The model predicts *dog* but the human labeled it as *cat*. On review, the photo
    quality was poor and it was a dog. This is a human error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model predicts *negative* sentiment but the text was confidently *positive*
    according to the reviewer. This indicates a weakness in the model and needs to
    be fixed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A manual review of model predictions provides valuable insights into the errors
    made by both the model and human labelers. One key strength is its ability to
    identify **systematic biases** in the training data, as well as cases where the
    model fails in ways that humans would not. However, a manual review is time-consuming
    and limited by human subjectivity and oversight. Typically, only a small subset
    of cases is reviewed, which may not uncover all weaknesses in the model. While
    a manual review serves as a useful debugging tool during model development, conducting
    large-scale reviews is often impractical. In such cases, alternative techniques
    such as active learning cycles may be necessary to further improve the model’s
    robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to utilize mismatched sampling is by including the mismatched samples
    in the active ML pool for re-labeling. This allows for a better understanding
    of confusing cases and enables the model to be trained to handle such cases more
    effectively. This iterative process of continuously adding and re-labeling data
    helps fine-tune the model without the need for manual reviewing.
  prefs: []
  type: TYPE_NORMAL
- en: By systematically identifying, understanding, and resolving model-label disagreements,
    the system improves over time. The key is to maintain human oversight in the process.
    In the next section, we will talk about how to manage human-in-the-loop systems.
  prefs: []
  type: TYPE_NORMAL
- en: Effectively managing human-in-the-loop systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting high-quality annotations requires finding, vetting, supporting, and
    retaining effective labelers. It is crucial to build an appropriate labeling team
    that meets the requirements of the ML project.
  prefs: []
  type: TYPE_NORMAL
- en: The first option is to establish an internal labeling team. This involves hiring
    full-time employees to label data, which enables close management and training.
    Cultivating domain expertise is easier when done internally. However, there are
    drawbacks to this, such as higher costs and turnover. This option is only suitable
    for large, ongoing labeling requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to crowdsource labeling tasks using platforms such as ScaleAI,
    which allow labeling tasks to be distributed to a large, on-demand workforce.
    This option provides flexibility and lower costs, but it can lack domain expertise.
    Quality control becomes challenging when working with anonymous crowd workers.
  prefs: []
  type: TYPE_NORMAL
- en: You could use third-party labeling services, such as Innovatiana, which specializes
    in providing trained annotators for ML projects. This option leverages existing
    labeling teams and workflows. However, it can be more costly than crowdsourcing
    and challenging to manage.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, hybrid options are also doable. For example, you could use a mixed strategy
    that combines third-party labelers with internal reviewers. The optimal approach
    depends on budget, timelines, data sensitivity, field expertise, and project scope.
    A combination of sources provides flexibility. The key is instituting strong training,
    validation, and monitoring to get the quality needed from any labeling resource.
  prefs: []
  type: TYPE_NORMAL
- en: '*The next question here is how to manage the labeling* *team efficiently.*'
  prefs: []
  type: TYPE_NORMAL
- en: Managing a labeling team for maximum efficiency requires setting clear guidelines
    so that labelers understand expectations and the big-picture goals. As mentioned
    earlier in this chapter, workflows should be structured to optimize labelers’
    time, automating where possible and minimizing redundant tasks. Providing good
    tools is also key.
  prefs: []
  type: TYPE_NORMAL
- en: 'When feasible, give labelers access to dashboards and metrics so that they
    can see the growing dataset. This keeps them engaged in the process. For example,
    in *Figure 3**.6*, we can see that in the Encord platform, the labelers can see
    how much they have labeled and how much they have left:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Encord’s Instance label tasks status for an annotation project](img/B21789_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Encord’s Instance label tasks status for an annotation project
  prefs: []
  type: TYPE_NORMAL
- en: Open communication channels allow labelers to easily discuss ambiguities, ask
    questions, and provide feedback. Make yourself accessible as a resource. Review
    labeling speed, accuracy, and costs to identify opportunities to improve productivity
    through enhancements to tools, training, or workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Effective management also requires holding regular meetings to discuss progress,
    issues, and feedback. Designate senior labelers to help train and support newer
    members. Break large projects into stages with milestones to maintain focus. Highlight
    top performers and celebrate wins to motivate the team. Address underperformance
    through coaching and training.
  prefs: []
  type: TYPE_NORMAL
- en: With strong training, optimized workflows, communication, and performance management,
    a labeling team can work efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Another issue to be aware of and to address is that annotators can introduce
    biases based on their personal experiences, cultural backgrounds, or misunderstandings
    of the task at hand. This can lead to biased datasets where certain perspectives
    or characteristics are overrepresented or underrepresented, thus affecting the
    fairness and accuracy of ML models trained on these datasets. Biased annotations
    can potentially lead to AI models that perpetuate or even amplify these biases.
    This is particularly concerning in sensitive applications such as facial recognition,
    sentiment analysis, and predictive policing, where biased data can lead to unfair
    or discriminatory outcomes. As ML evolves, there’s a growing emphasis on developing
    general-purpose foundational models that significantly reduce the reliance on
    extensive human labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A general-purpose foundational model is a versatile AI system that’s been trained
    on vast amounts of data that can be adapted or fine-tuned to perform a wide range
    of tasks across different domains without the need for task-specific training
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: An example of such innovation is SAM when it is used as a feature on labeling
    platforms to help accelerate labeling, which embodies the capability to understand
    and segment various objects in images or videos without the need for explicit,
    detailed human annotation for every new object type. This not only streamlines
    the development process by requiring less manual labeling but also aims to mitigate
    bias by relying on generalized learning capabilities that can adapt to diverse
    scenarios without inheriting the specific biases of a small group of human annotators.
    However, the design and training of these foundational models still necessitate
    careful consideration of the data they’re trained on and the potential biases
    inherent in those datasets, highlighting the ongoing importance of fairness and
    ethical considerations in the field of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Even with an efficient labeling team, can we ensure consistent quality of annotations
    over time? In the following section, we will explore this issue and discuss methods
    for ensuring the ongoing quality of annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring annotation quality and dataset balance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Maintaining high annotation quality and target class balance requires diligent
    management. In this section, we’ll look at some techniques that can help assure
    labeling quality.
  prefs: []
  type: TYPE_NORMAL
- en: Assess annotator skills
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is highly recommended that annotators undergo thorough training sessions
    and complete qualification tests before they can work independently. This ensures
    that they have a solid foundation of knowledge and understanding in their respective
    tasks. These performance metrics can be visualized in the labeling platform when
    the reviewers accept or reject annotations. If a labeler has many rejected annotations,
    it is necessary to ensure that they understand the task and assess what help can
    be provided to them.
  prefs: []
  type: TYPE_NORMAL
- en: It is advisable to periodically assess the labeler’s skills by providing control
    samples for evaluation purposes. This ongoing evaluation helps maintain the quality
    and consistency of their work over time.
  prefs: []
  type: TYPE_NORMAL
- en: For example, designing datasets with known labels and asking the labelers to
    label these evaluation sets can be a good way to check if the task is well understood.
    Then, we can assess the accuracy of the annotations using a simple Python script.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must define some dummy annotations that have been made by a labeler
    and some real annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must calculate the accuracy and kappa score using the `sklearn` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This technique is a simple and easy way to implement a basic assessment of the
    annotator’s skills. Aim for an accuracy of above 90% and a kappa score of above
    0.80 and then we can investigate poor agreements.
  prefs: []
  type: TYPE_NORMAL
- en: Use multiple annotators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your budget allows, you can assign each data point to multiple annotators
    to identify conflicts. These conflicts can then be resolved through consensus
    or by an expert reviewer.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, with sentiment analytics labeling, we have our dummy annotations
    for three labelers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a pandas DataFrame with the labels from the three labelers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can take the majority vote as a real label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This method can be expensive because the labelers work on the same data, but
    it can ultimately result in more accurate annotations. Its feasibility depends
    on the priorities of the ML project, as well as the budget and organization of
    the labeling team. For instance, if the labeling team consists of junior labelers
    who are new to the field, this method may be a suitable choice.
  prefs: []
  type: TYPE_NORMAL
- en: Balanced sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To prevent imbalanced datasets, we can actively sample minority classes at higher
    rates during data collection.
  prefs: []
  type: TYPE_NORMAL
- en: When collecting a dataset, it is important to monitor the distribution of labels
    across classes and adjust sampling rates accordingly. Without intervention, datasets
    often end up skewed toward majority classes due to their natural higher frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some ways to actively sample minority classes at higher rates:'
  prefs: []
  type: TYPE_NORMAL
- en: Employing active ML approaches such as **uncertainty sampling** can bias selection
    toward rare cases. Indeed, uncertainty sampling actively selects the data points
    that the current model is least certain about for labeling. These tend to be edge
    cases and rare examples, rather than the common cases the model has already seen
    many examples of. Since, by definition, minority classes occur less frequently,
    the model is naturally more uncertain about these classes. So, uncertainty sampling
    will tend to pick more examples from the under-represented classes for labeling
    to improve the model’s understanding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Checking label distributions periodically during data collection is important.
    If minority classes are underrepresented, it is recommended to selectively sample
    more data points with those labels. This can be achieved by sampling the data
    from the unlabeled data pool using a pre-trained model that can identify the unrepresented
    classes. To ensure higher representation, the sampling strategy should be set
    to select specific classes with a higher ratio. For example, let’s reuse the *imdb*
    dataset from Hugging Face:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For testing purposes, we assume that the dataset is unlabeled and that the
    labels attached to it are from the model’s predictions. So, our goal is to sample
    the under-represented class. Let’s assume class `0` is under-represented and we
    want to over-sample it. First, we must take the training dataset as our dummy
    unlabeled data pool and convert it into a pandas `DataFrame`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must get the number of data points for each label:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must calculate the number of samples to sample for each label, assuming
    we want to sample 1,000 samples and we want 80% of these samples to belong to
    class `0` and 20% to class `1`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, we sampled with the correct ratio and can, in theory, add these samples
    to our labeling queue next. By setting a higher sampling ratio for class `0` from
    the unlabeled data, we selectively oversample the minority class when getting
    new labeled data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The key is closely tracking the evolving label distribution and steering sampling
    toward under-represented classes. This prevents highly imbalanced datasets that
    fail to provide sufficient examples for minority classes. The result is higher-quality,
    more balanced training data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored strategies for effectively incorporating human input into
    active ML systems. We discussed how to design workflows that enable efficient
    collaboration between humans and AI models. Leading open source frameworks for
    human-in-the-loop learning were reviewed, including their capabilities for annotation,
    verification, and active learning.
  prefs: []
  type: TYPE_NORMAL
- en: Handling model-label disagreements is a key challenge in human-AI systems. Techniques
    such as manually reviewing conflicts and active learning cycles help identify
    and resolve mismatches. Carefully managing the human annotation workforce is also
    critical as it covers recruiters, training, quality control, and tooling.
  prefs: []
  type: TYPE_NORMAL
- en: A major focus was ensuring high-quality balanced datasets while using methods
    such as qualification exams, inter-annotator metrics such as the accuracy or the
    Kappa score, consensus evaluations, and targeted sampling. By implementing robust
    processes around collaboration, conflict resolution, annotator management, and
    data labeling quality, the usefulness of human input in the loop can be maximized.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will shift our focus to applying active ML approaches
    specifically for computer vision tasks such as image classification, semantic
    segmentation, and object detection.
  prefs: []
  type: TYPE_NORMAL
