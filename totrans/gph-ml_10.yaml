- en: 'Chapter 7: Text Analytics and Natural Language Processing Using Graphs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, a vast amount of information is available in the form of text in terms
    of natural written language. The very same book you are reading right now is one
    such example. The news you read every morning, the tweets or the Facebook posts
    you sent/read earlier, the reports you write for a school assignment, the emails
    we write continuously – these are all examples of information we exchange via
    written documents and text. It is undoubtedly the most common way of indirect
    interaction, as opposed to direct interaction such as talking or gesticulating.
    It is, therefore, crucial to be able to leverage such kinds of information and
    extract insights from documents and texts.
  prefs: []
  type: TYPE_NORMAL
- en: The vast amount of information present nowadays in this form has determined
    the great development and recent advances in the field of **natural language processing**
    (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will show you how to process natural language texts and
    review some basic models that allow us to structure text information. Using the
    information that's been extracted from a corpus of documents, we will show you
    how to create networks that can be analyzed using some of the techniques we have
    seen in previous chapters. In particular, using a tagged corpus we will show you
    how to develop both supervised (classification models to classify documents in
    pre-determined topics) and unsupervised (community detection to discover new topics)
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Providing a quick overview of a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the main concepts and tools used in NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating graphs from a corpus of documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a document topic classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using *Python 3.8* for all our exercises. The following is a list
    of Python libraries that you must install for this chapter using pip. To do this,
    run, for example, `pip install networkx==2.4` on the command line and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: All the code files relevant to this chapter are available at [https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter07](https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter07).
  prefs: []
  type: TYPE_NORMAL
- en: Providing a quick overview of a dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To show you how to process a corpus of documents with the aim of extracting
    relevant information, we will be using a dataset derived from a well-known benchmark
    in the field of NLP: the so-called **Reuters-21578**. The original dataset includes
    a set of 21,578 news articles that were published in the financial Reuters newswire
    in 1987, which were assembled and indexed in categories. The original dataset
    has a very skewed distribution, with some categories appearing only in the training
    set or in the test set. For this reason, we will use a modified version, known
    as **ApteMod**, also referred to as *Reuters-21578 Distribution 1.0*, that has
    a smaller skew distribution and consistent labels between the training and test
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Even though these articles are a bit outdated, the dataset has been used in
    a plethora of papers on NLP and still represents a dataset that's often used for
    benchmarking algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, Reuters-21578 contains enough documents for interesting post-processing
    and insights. A corpus with a larger number of documents can easily be found nowadays
    (see, for instance, [https://github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets)
    for an overview of the most common ones), but they may require larger storage
    and computational power so that they can be processed. In [*Chapter 9*](B16069_09_Final_JM_ePub.xhtml#_idTextAnchor141)*,
    Building a Data-Driven, Graph-Powered Application*, we will show you some of the
    tools and libraries that can be used to scale out your application and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each document of the Reuters-21578 dataset is provided with a set of labels
    that represent its content. This makes it a perfect benchmark for testing both
    supervised and unsupervised algorithms. The Reuters-21578 dataset can easily be
    downloaded using the `nltk` library (which is a very useful library for post-processing
    documents):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you will see from inspecting the `corpus` DataFrame, the IDs are in the
    form `training/{ID}` and `test/{ID}`, which makes it clear which documents should
    be used for training and for testing. To start, let''s list all the topics and
    see how many documents there are per topic using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The Reuters-21578 dataset includes 90 different topics with a significant degree
    of unbalance between classes, with almost 37% of the documents in the `most common`
    category and only 0.01% in each of the five least common categories. As you can
    see from inspecting the text, some of the documents have some newline characters
    embedded, which can easily be removed in the first text cleaning stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have loaded the data in memory, we can start analyzing it. In the
    next subsection, we will show you some of the main tools that can be used for
    dealing with unstructured text data. They will help you extract structured information
    so that it can be used with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the main concepts and tools used in NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When processing documents, the first analytical step is certainly to infer the
    document language. Most analytical engines that are used in NLP tasks are, in
    fact, trained on documents in a specific language and should only be used for
    such a language. Some attempts to build cross-language models (see, for instance,
    multi-lingual embeddings such as [https://fasttext.cc/docs/en/aligned-vectors.html](https://fasttext.cc/docs/en/aligned-vectors.html)
    and [https://github.com/google-research/bert/blob/master/multilingual.md](https://github.com/google-research/bert/blob/master/multilingual.md))
    have recently gained increasing popularity, although they still represent a small
    portion of NLP models. Therefore, it is very common to first infer the language
    so that you can use the correct downstream analytical NLP pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: You can use different methods to infer the language. One very simple yet effective
    approach relies on looking for the most common words of a language (the so-called
    `stopwords`, such as `the`, `and`, `be`, `to`, `of`, and so on) and building a
    score based on their frequencies. Its precision, however, tends to be limited
    to short text and does not make use of the word's positioning and context. On
    the other hand, Python has many libraries that use more elaborated logic, allowing
    us to infer the language in a more precise manner. Some such libraries are `fasttext`,
    `polyglot`, and `langdetect`, to name just a few.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we will use `fasttext` in the following code, which can be integrated
    with very few lines and provides support for more than 150 languages. The language
    can be inferred for all documents using the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you will see in the output, there seem to be documents in languages other
    than English. Indeed, these documents are often either very short or have a strange
    structure, which means they're not actual news articles. When documents represent
    text that a human would read and label as news, the model is generally rather
    precise and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have inferred the language, we can continue with the language-dependent
    steps of the analytical pipeline. For the following tasks, we will be using `spaCy`,
    which is an extremely powerful library that allows us to embed state-of-the-art
    NLP models with very few lines of code. After installing the library with `pip
    install spaCy`, language-specific models can be integrated by simply installing
    them using the `spaCy` download utility. For instance, the following command can
    be used to download and install the English model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we should have the language models for English ready to use. Let''s see
    which information it can provide. Using spaCy is extremely simple and, using just
    one line of code, can embed the computation as a very rich set of information.
    Let''s start by applying the model to one of the documents in the Reuters corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: SUBROTO SAYS INDONESIA SUPPORTS TIN PACT EXTENSION
  prefs: []
  type: TYPE_NORMAL
- en: Mines and Energy Minister Subroto confirmed Indonesian support for an extension
    of the sixth **International Tin Agreement** (**ITA**), but said a new pact was
    not necessary. Asked by Reuters to clarify his statement on Monday in which he
    said the pact should be allowed to lapse, Subroto said Indonesia was ready to
    back extension of the ITA. "We can support extension of the sixth agreement,"
    he said. "But a seventh accord we believe to be unnecessary." The sixth ITA will
    expire at the end of June unless a two-thirds majority of members vote for an
    extension.
  prefs: []
  type: TYPE_NORMAL
- en: '`spacy` can easily be applied just by loading the model and applying it to
    the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `parsed` object, which is returned by `spacy`, has several fields due to
    many models being combined into a single pipeline. These provide a different level
    of text structuring. Let''s examine them one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`spacy` generally works fairly well. However, please note that, depending on
    the context, a bit of model tuning or rule modification might be necessary. For
    instance, when you''re dealing with short texts that contain slang, emoticons,
    links, and hashtags, a better choice for text segmentation and tokenization may
    be `TweetTokenizer`, which is included in the `nltk` library. Depending on the
    context, we encourage you to explore other possible segmentations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the document returned by `spacy`, the sentence segmentation can be found
    in the `sents` attribute of the `parsed` object. Each sentence can be iterated
    over its token by simply using the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Each token is a spaCy `Span` object that has attributes that specify the type
    of token and further characterization that's introduced by the other models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`DET`) is usually followed by a noun, and so on. When using spaCy, the information
    about PoS tagging is usually stored in the `label_` attribute of the `Span` object.
    The types of tags that are available can be found at [https://spacy.io/models/en](https://spacy.io/models/en).
    Conversely, you can get a human-readable value for a given type using the `spacy.explain`
    function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ents` attribute of the `parsed` object. spaCy also provides some utilities
    to nicely visualize the entities in a text using the `displacy` module:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Example of the spaCy output for the NER engine'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – Example of the spaCy output for the NER engine
  prefs: []
  type: TYPE_NORMAL
- en: '`spacy` can be used to build a syntactic tree that can be navigated to identify
    relationships between the tokens. As we will see shortly, this information can
    be crucial when building knowledge graphs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Example of a syntactic dependency tree provided by spaCy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_07_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Example of a syntactic dependency tree provided by spaCy
  prefs: []
  type: TYPE_NORMAL
- en: '`Span` object via the `lemma_` attribute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As shown in the preceding diagram, spaCy pipelines can be easily integrated
    to process the entire corpus and store the results in our `corpus` DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This DataFrame represents the structured information of the documents. This
    will be the base of all our subsequent analysis. In the next section, we will
    show you how to build graphs while using such information.
  prefs: []
  type: TYPE_NORMAL
- en: Creating graphs from a corpus of documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will use the information we extracted in the previous section
    using the different text engines to build networks that relate the different information.
    In particular, we will focus on two kinds of graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge-based graphs**, where we will use the semantic meaning of sentences
    to infer relationships between the different entities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bipartite graphs**, where we will be connecting the documents to the entities
    that appear in the text. We will then project the bipartite graph into a homogeneous
    graph, which will be made up of either document or entity nodes only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Knowledge graphs are very interesting as they not only relate entities but
    also provide a direction and a meaning to the relationship. For instance, let''s
    take a look at the following relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: I (->) buy (->) a book
  prefs: []
  type: TYPE_NORMAL
- en: 'This is substantially different from the following relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: I (->) sell (->) a book
  prefs: []
  type: TYPE_NORMAL
- en: Besides the kind of relationship (buying or selling), it is also important to
    have a direction, where the subject and object are not treated symmetrically,
    but where there is a difference between who is performing the action and who is
    the target of such an action.
  prefs: []
  type: TYPE_NORMAL
- en: So, to create a knowledge graph, we need a function that can identify the **Subject-Verb-Object**
    (**SVO**) triplet for each sentence. This function can then be applied to all
    the sentences in the corpus; then, all the triplets can be aggregated to generate
    the corresponding graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SVO extractor can be implemented on top of the enrichment provided by spaCy
    models. Indeed, the tagging provided by the dependency tree parser can be very
    helpful for separating main sentences and their subordinates, as well as identifying
    the SOV triplets. The business logic may need to consider a few special cases
    (such as conjunctions, negations, and preposition handling), but this can be encoded
    with a set of rules. Moreover, these rules may also change, depending on the specific
    use case, with slight variations to be tuned by the user. A base implementation
    of such rules can be found at [https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py](https://github.com/NSchrading/intro-spacy-nlp/blob/master/subject_object_extraction.py).
    These have been slightly adopted for our scope and are included in the GitHub
    repository provided with this book. Using this helper function, we can compute
    all the triplets in the corpus and store them in our `corpus` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The type of the connection (determined by the sentence''s main predicate) is
    stored in the `edge` column. The first 10 most common relationships can be shown
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The most common edge types correspond to very basic predicates. Indeed, together
    with very general verbs (such as be, have, tell, and give), we can also find predicates
    that are more related to a financial context (such as buy, sell, and make). Using
    all these edges, we can now create our knowledge-based graph using the `networkx`
    utility function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'By filtering the edge DataFrame and creating a subnetwork using this information,
    we can analyze specific relationship types, such as the `lend` edge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the subgraph based on the *lend* relationships.
    As we can see, it already provides interesting economical insights, such as the
    economic relationships between countries, such as Venezuela-Ecuador and US-Sudan:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Example of a portion of the knowledge graph for the edges related
    the lending relationships'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_07_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Example of a portion of the knowledge graph for the edges related
    the lending relationships
  prefs: []
  type: TYPE_NORMAL
- en: You can play around with the preceding code by filtering the graph based on
    other relationships. We definitely encourage you to do so, in order to unveil
    further interesting insights from the knowledge graphs we just created. In the
    next section, we will show you another method that allows us to encode the information
    that's been extracted from the text into a graph structure. In doing so, we will
    also make use of a particular type of graph that we introduced in [*Chapter 1*](B16069_01_Final_JM_ePub.xhtml#_idTextAnchor014)*,*
    *Bipartite Graphs*.
  prefs: []
  type: TYPE_NORMAL
- en: Bipartite document/entity graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Knowledge graphs can unveil and query aggregated information over entities.
    However, other graph representations are also possible and can be useful in other
    situations. For example, when you want to cluster documents semantically, the
    knowledge graph may not be the best data structure to use and analyze. Knowledge
    graphs are also not very effective at finding indirect relationships, such as
    identifying competitors, similar products, and so on, that do not often occur
    in the same sentence, but that often occur in the same document.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these limitations, we will encode the information present in the
    document in the form of a **bipartite graph**. For each document, we will extract
    the entities that are most relevant and connect a node, representing the document,
    with all the nodes representing the relevant entities in such a document. Each
    node may have multiple relationships: by definition, each document connects multiple
    entities. By contract, an entity can be referenced in multiple documents. As we
    will see, cross-referencing can be used to create a measure of similarity between
    entities and documents. This similarity can also be used for projecting the bipartite
    graph into one particular set of nodes – either the document nodes or the entity
    nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To this aim, to build our bipartite graph, we need to extract the relevant
    entities of a document. The term *relevant entity* is clearly fuzzy and broad.
    In the current context, we will consider a relevant entity to be either a named
    entity (such as an organization, person, or location recognized by the NER engine)
    or a keyword; that is, a word (or a composition of words) that identifies and
    generally describes the document and its content. For instance, the suitable keywords
    for this book may be "graph," "network," "machine learning," "supervised model,"
    and "unsupervised model." Many algorithms exist that extract keywords from a document.
    One very simple way to do this is based on the so-called TF-IDF score, which is
    based on building a score for each token (or group of tokens, often referred to
    as *grams*) that is proportional to the word count in the document (the **Term
    Frequency**, or **TF**) and to the inverse of the frequency of that word in a
    given corpus (the **Inverse Document Frequency**, or **IDF**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16067_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B16067_07_002.png) represents the count of word ![](img/B16067_07_003.png)
    in document ![](img/B16067_07_004.png), ![](img/B16067_07_005.png) represents
    the number of documents in the corpus, and ![](img/B16067_07_006.png) is the document
    where the word ![](img/B16067_07_007.png) appears. Therefore, the TF-IDF score
    promotes words that are repeated many times in the document, penalizing words
    that are common and therefore might not be very representative for a document.
    There are also more sophisticated algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'One method that is quite powerful and worth mentioning in the context of this
    book is indeed `gensim`, which can be used in a straightforward manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the score represents the centrality, which represents the importance
    of a given token. As you can see, some composite tokens may also occur, such as
    `japanese electronics`. Keyword extraction can be implemented to compute the keywords
    for the entire corpus, thus storing the information in our `corpus` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides the keywords, to build the bipartite graph, we also need to parse the
    named entities that were extracted by the NER engine, and then encode the information
    in a similar data format as the one that was used for the keywords. This can be
    done using a few utility functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'With these functions, parsing the `spacy` tags can be done with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `entities` DataFrame can easily be merged with the `corpus` DataFrame using
    the `pd.concat` function, thus placing all the information in a single data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have all the ingredients for our bipartite graph, we can create
    the edge list by looping over all the documents-entity or document-keyword pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the edge list has been created, we can produce the bipartite graph using
    `networkx` APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can look at an overview of our graph by using `nx.info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next subsection, we will project the bipartite graph in either of the
    two sets of nodes: entities or documents. This will allow us to explore the difference
    between the two graphs and cluster both the terms and documents using the unsupervised
    techniques described in [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)*,
    Supervised Graph Learning*. Then, we will return to the bipartite graph to show
    an example of supervised classification, which we''ll do by leveraging the network
    information of the bipartite graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: Entity-entity graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will start by projecting our graph into the set of entity nodes. `networkx`
    provides a special submodule for dealing with bipartite graphs, `networkx.algorithms.bipartite`,
    where a number of algorithms have already been implemented. In particular, the
    `networkx.algorithms.bipartite.projection` submodule provides a number of utility
    functions to project bipartite graphs on a subset of nodes. Before performing
    projection, we must extract the nodes relative to a particular set (either documents
    or entities) using the "bipartite" property we created when we generated the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The graph projection basically creates a new graph with the set of selected
    nodes. Edges are places between the nodes based on whether two nodes have neighbors
    in common. The basic `projected_graph` function creates such a network with unweighted
    edges. However, it is usually more informative to have edges weighted based on
    the number of common neighbors. The `projection` module provides different functions
    based on how the weights are computed. In the next section, we will use `overlap_weighted_projected_graph`,
    where the edge weight is computed using the Jaccard similarity based on common
    neighbors. However, we encourage you to also explore the other options that, depending
    on your use case and context, may best suit your aims.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware of dimensions – filtering the graph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There is another point of caution you should be aware of when dealing with
    projections: the dimension of the projected graph. In certain cases, like the
    one we are considering here, projection may create an extremely large number of
    edges, which makes the graph hard to analyze. In our use case, following the logic
    we used to create our network, a document node is connected to at least 10 keywords,
    plus a few entities. In the resulting entity-entity graph, all these entities
    will be connected to each other as they share at least one common neighbor (the
    document that contains them). Therefore, we will only be generating around ![](img/B16067_07_008.png)
    edges for one document. If we multiply this number for the number of documents,
    ![](img/B16067_07_009.png), we will end up with several edges that, despite the
    small use case, already become almost intractable, since there''s a few million
    edges. Although this surely represents a conservative upper bound (as some of
    the co-occurrence between entities will be common in many documents and therefore
    not repeated), it provides an order of magnitude of the complexity that you might
    expect. Therefore, we encourage you to proceed with caution before projecting
    your bipartite graph, depending on the topology of the underlying network and
    the size of your graph. One trick to reduce this complexity and make the projection
    feasible is to only consider entity nodes that have a certain degree. Most of
    the complexity arises from the presence of entities that appear only once or a
    few times, but still generate *cliques* within the graph. Such entities are not
    very informative for capturing patterns and providing insights. Besides, they
    are possibly strongly affected by statistical variability. On the other hand,
    we should focus on strong correlations that are supported by larger occurrences
    and provide more reliable statistical results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we will only consider entity nodes with a certain degree. To this
    aim, we will generate the filtered bipartite subgraph, which excludes nodes with
    low degree values, namely smaller than 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This subgraph can now be projected without generating a graph with an excessive
    number of edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the dimension of the graph with the `networkx` function of `nx.info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Despite the filters we''ve applied, the number of edges and the average node
    degree are still quite large. The following graph shows the distribution of the
    degree and of the edge weights, where we can observe one peak in the degree distribution
    at fairly low values, with a fat tail toward large degree values. Also, the edge
    weight shows a similar behavior, with a peak at rather low values and fat right
    tails. These distributions suggest the presence of several small communities,
    namely cliques, which are connected to each other via some central nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Degree and weight distribution for the entity-entity network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_07_04(Merged).jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – Degree and weight distribution for the entity-entity network
  prefs: []
  type: TYPE_NORMAL
- en: 'The distribution of the edge weights also suggests that a second filter could
    be applied. The filter on the entity degree that we applied previously on the
    bipartite graph allowed us to filter out rare entities that only appeared in a
    few documents. However, the resulting graph could also be affected by the opposite
    problem: popular entities may be connected just because they tend to appear often
    in documents, even if there is not an interesting causal connection between them.
    Consider the US and Microsoft. They are almost surely connected, as it is extremely
    likely that there will be at least one or a few documents where they both appear.
    However, if there is not a strong and causal connection between them, it is very
    unlikely that the Jaccard similarity will be large. Considering only the edges
    with the largest weights allows you to focus on the most relevant and possibly
    stable relationships. The edge weight distribution shown in the preceding graph
    suggests that a suitable threshold could be `0.05`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Such a threshold reduces the number of edges significantly, making it feasible
    to analyze the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 7.5 – Degree Distribution (Left) and Edge Weight Distribution (Right)
    for the resulting graph, after filtering based on the edge weight](img/B16069_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Degree Distribution (Left) and Edge Weight Distribution (Right)
    for the resulting graph, after filtering based on the edge weight
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows the distribution of the node degree and edge weights
    for the filtered graph. The distribution for the edge weights corresponds to the
    right tail of the distribution shown in *Figure 7.4*. The relationship that the
    degree distribution has with *Figure 7.4* is less obvious, and it shows the peak
    for the nodes that have a degree around 10, as opposed to the peak shown in *Figure
    7.4*, which was observed in the low range, at around 100.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the graph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using Gephi we can provide an overview of the overall network, which is shown
    in *Figure 7.6*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Entity-entity network highlighting the presence of multiple
    small subcommunities](img/B16069_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Entity-entity network highlighting the presence of multiple small
    subcommunities
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain some further insights on the topology of the network, we will also
    compute some global measures, such as the average shortest path, clustering coefficient,
    and global efficiency. Although the graph has five different connected components,
    the largest one almost entirely accounts for the whole graph, including 2,254
    out of 2,265 nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The global properties of the largest components can be found with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The shortest path and global efficiency may require a few minutes of computation.
    This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the magnitude of these metrics (with a shortest path of about 5 and
    a clustering coefficient around 0.2), together with the degree distribution shown
    previously, we can see that the network has multiple communities of a limited
    size. Other interesting local properties, such as degree, page rank, and betweenness
    centralities distributions, are shown in the following graph, which shows how
    all these measures tend to correlate and connect to each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Relationships and distribution between the degree, page rank,
    and betweenness centrality measures](img/B16069_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Relationships and distribution between the degree, page rank, and
    betweenness centrality measures
  prefs: []
  type: TYPE_NORMAL
- en: After providing a description in terms of loca;/global measures, as well as
    a general visualization of the network, we will apply some of the techniques we
    have seen in the previous chapters to identify some insights and information within
    the network. We will do this using the unsupervised techniques described in [*Chapter
    4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064)*, Supervised Graph Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by using the Louvain community detection algorithms, which, by
    optimizing their modularity, aim to identify the best partitions of the nodes
    in disjoint communities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note that the results might vary between runs because of random seeds. However,
    a similar partition, with a distribution of cluster memberships similar to the
    one shown in the following graph, should emerge. We generally observe about 30
    communities, with the larger ones containing around 130-150 documents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Distribution of the size of the detected communities](img/B16069_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Distribution of the size of the detected communities
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.9* shows a close-up of one of the communities, where we can identify
    a particular topic/argument. On the left, beside the entity nodes, we can also
    see the document nodes, thus uncovering the structure of the related bipartite
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Close-up for one of the communities we''ve identified](img/B16069_07_09(Merged).jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Close-up for one of the communities we've identified
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064),
    *Supervised Graph Learning*, we can extract insightful information about the topology
    and similarity between entities by using node embeddings. In particular, we can
    use Node2Vec, which, by feeding a randomly generated walk to a skip-gram model,
    can project the nodes into a vector space, where close-by nodes are mapped to
    nearby points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the vector space of embeddings, we can apply traditional clustering algorithms,
    such as *GaussianMixture*, *K-means*, and *DB-scan*. As we did in the previous
    chapters, we can also project the embeddings into a 2D plane using t-SNE to visualize
    clusters and communities. Besides giving us another option to identify clusters/communities
    within the graph, Node2Vec can also be used to provide similarity between words,
    as traditionally done by `turkey`," which provides semantically similar words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Although these two approaches, Node2Vec and Word2Vec, share some methodological
    similarities, the two embedding schemes come from different types of information:
    Word2Vec is built directly from the text and encloses relationships at the sentence
    level, while Node2Vec encodes a description that acts more at the document level,
    since it comes from the bipartite entity-document graph.'
  prefs: []
  type: TYPE_NORMAL
- en: Document-document graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s project the bipartite graph into the set of document nodes to create
    a document-document network we can analyze. In a similar way to when we created
    an entity-entity network, we will use the `overlap_weighted_projected_graph` function
    to obtain a weighted graph that can be filtered to reduce the number of significant
    edges. Indeed, the topology of the network and the business logic used to build
    the bipartite graph do not favor clique creation, as we saw for the entity-entity
    graph: two nodes will only be connected when they share at least one keyword,
    organization, location, or person. This is certainly possible, but not extremely
    likely, within groups of 10-15 nodes, as observed for the entities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did previously, we can easily build our network with the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the distribution of the degree and the edge weight.
    This can help us decide on the value of the threshold to be used to filter out
    the edges. Interestingly, the node degree distribution shows a clear peak toward
    large values compared to the degree distribution observed for the entity-entity
    graph. This suggests the presence of a number of *supernodes* (that is, nodes
    with rather large degrees) that are highly connected. Also, the edge weight distribution
    shows the Jaccard index''s tendency to attain values close to 1, which are much
    larger than the ones we observed in the entity-entity graph. These two observations
    highlight a profound difference between the two networks: whereas the entity-entity
    graph is characterized by many tightly connected communities (namely cliques),
    the document-document graph is characterized by a rather tight connection among
    nodes with a large degree (which constitutes the core) versus a periphery of weakly
    connected or disconnected nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Degree Distribution and Edge Weight Distribution for the projection
    of the bipartite graph into the document-document network](img/B16069_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Degree Distribution and Edge Weight Distribution for the projection
    of the bipartite graph into the document-document network
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be convenient to store all the edges in a DataFrame so that we can plot
    them and then use them to filter and, thus, create a subgraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'By looking at the preceding diagram, it seems reasonable to set a threshold
    value of `0.6` for the edge weight, thus allowing us to generate a more tractable
    network using the `edge_subgraph` function of `networkx`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the resulting distribution for the degree and for
    the edge weight for the reduced graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Degree Distribution and Edge Weight Distribution for the document-document
    filtered network](img/B16069_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Degree Distribution and Edge Weight Distribution for the document-document
    filtered network
  prefs: []
  type: TYPE_NORMAL
- en: 'The substantial difference in topology of the document-document graph with
    respect to the entity-entity graph can also be clearly seen in the following diagram,
    which shows a full network visualization. As anticipated by the distributions,
    the document-document network is characterized by a core network and several weekly
    connected satellites. These satellites represent all the documents that share
    none or a few keywords or entity common occurrences. The number of disconnected
    documents is quite large and accounts for almost 50% of the total:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – (Left) Representation of the document-document filtered network,
    highlighting the presence of a core and a periphery. (Right) Close-up of the core,
    with some subcommunities embedded. The node size is proportional to the node degree'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_07_12(Merged).jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.12 – (Left) Representation of the document-document filtered network,
    highlighting the presence of a core and a periphery. (Right) Close-up of the core,
    with some subcommunities embedded. The node size is proportional to the node degree
  prefs: []
  type: TYPE_NORMAL
- en: 'It may be worthwhile extracting the connected components for this network using
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following graph, we can see the distribution for the connected component
    sizes. Here, we can clearly see the presence of a few very large clusters (the
    cores), together with a large number of disconnected or very small components
    (the periphery or satellites). This structure is strikingly different from the
    one we observed for the entity-entity graph, where all the nodes were generated
    by a very large, connected cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Distribution of the connected component sizes, highlighting
    the presence of many small-sized communities (representing the periphery) and
    a few large communities (representing the core)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_07_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.13 – Distribution of the connected component sizes, highlighting the
    presence of many small-sized communities (representing the periphery) and a few
    large communities (representing the core)
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be interesting to investigate the structure of the core components further.
    We can extract the subgraph composed of the largest components of the network
    from the full graph with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect the properties of the core network using `nx.info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The left panel in *Figure 7.12* shows a Gephi visualization of the core. As
    we can see, the core is composed of a few communities, along with nodes with fairly
    large degrees strongly connected to each other.
  prefs: []
  type: TYPE_NORMAL
- en: As we did for the entity-entity network, we can process the network to identify
    communities embedded in the graph. However, different from what we did previously,
    the document-document graph now provides a mean for judging the clustering using
    the document labels. Indeed, we expect documents belonging to the same topic to
    be close and connected to each other. Moreover, as we will see shortly, this will
    also allow us to identify similarities among topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s start by extracting the candidate communities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will extract the topic mixture within each community to see whether
    there is a homogeneity (all the documents belonging to the same class) or some
    correlation between topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '`normalizedCommunityTopics` is a DataFrame that, for each community (row in
    the DataFrame), provides the topic mixture (in percentage) of the different topics
    (along the column axis). To quantify the heterogeneity of the topic mixture within
    the clusters/communities, we must compute the Shannon entropy of each community:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16067_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B16067_07_011.png) represents the entropy of the cluster, ![](img/B16067_07_012.png),
    and ![](img/B16067_07_013.png) corresponds to the percentage of topic ![](img/B16067_07_014.png)
    in community ![](img/B16067_07_015.png). We must compute the empirical Shannon
    entropy for all communities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the entropy distribution across all communities.
    Most communities have zero or very low entropy, thus suggesting that the documents
    that belong to the same class (label) tend to cluster together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Entropy distribution of the topic mixture in each community'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16069_07_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.14 – Entropy distribution of the topic mixture in each community
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if most of the communities show zero or low variability around topics,
    it is interesting to investigate whether there is a relationship between topics,
    when communities show some heterogeneity. Namely, we compute the correlation between
    topic distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'These can then be represented and visualized using a topic-topic network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The left-hand side of the following diagram shows the full graph representation
    for the topics network. As observed for the document-document network, the topic-topic
    graph shows a structure organized in a periphery of disconnected nodes and a strongly
    connected core. The right-hand side of the following diagram shows a close-up
    of the core network. This indicates a correlation that is supported by a semantic
    meaning, with the topics related to commodities tightly connected to each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – (Left) Topic-topic correlation graph, organized with a periphery-core
    structure. (Right) Close-up of the core of the network](img/B16069_07_15(Merged).jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – (Left) Topic-topic correlation graph, organized with a periphery-core
    structure. (Right) Close-up of the core of the network
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we analyzed the different types of networks that arise when
    analyzing documents and, more generally, text sources. To do so, we used global
    and local properties to statistically describe the networks, as well as some unsupervised
    algorithms, which allowed us to unveil some structure within the graph. In the
    next section, we will show you how to leverage these graph structures when building
    a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Building a document topic classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To show you how to leverage a graph structure, we will focus on using the topological
    information and the connections between the entities provided by the bipartite
    entity-document graph to train multi-label classifiers. This will help us predict
    the document topics. To do this, we will analyze two different approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A shallow machine-learning approach**, where we will use the embeddings we
    extracted from the bipartite network to train *traditional* classifiers, such
    as a RandomForest classifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A more integrated and differentiable approach** based on using a graphical
    neural network that''s been applied to heterogeneous graphs (such as the bipartite
    graph).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s consider the first 10 topics, which we have enough documentation on
    to train and evaluate our models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block produces the following output. This shows the names
    of the topics, all of which we will focus on in the following analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'When training topic classifiers, we must restrict our focus to only those documents
    that belong to such labels. The filtered corpus can easily be obtained by using
    the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have extracted and structured the dataset, we are ready to start
    training our topic models and evaluating their performance. In the next section,
    we will start by creating a simple model using shallow learning methods so that
    we can increase the complexity of the model by using graph neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Shallow learning methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start by implementing a shallow approach for the topic classification
    tasks by leveraging the network''s information. We will show you how to do this
    so that you can customize even further, depending on your use case:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will compute the embeddings by using `Node2Vec` on the bipartite
    graph. Filtered document-document networks are characterized by a periphery with
    many nodes that are disconnected, so they would not benefit from topological information.
    On the other hand, the unfiltered document-document network will have many edges,
    which makes the scalability of the approach an issue. Therefore, using the bipartite
    graph is crucial in order to efficiently leverage the topological information
    and the connection between entities and documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the `dimension` embedding, as well as our `window`, which is used for
    generating the walks, are hyperparameters that must be optimized via cross-validation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To make this computationally efficient, a set of embeddings can be computed
    beforehand, saved to disk, and then be used in the optimization process. This
    would work based on the assumption that we are in a *semi-supervised* setting
    or in a *transductive* task, where we have connection information about the entire
    dataset, apart from their labels, at training time. Later in this chapter, we
    will outline another approach, based on graph neural networks, that provides an
    inductive framework for integrating topology when training classifiers. Let''s
    store the embeddings in a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we can choose and loop different values for `dimension` and `window`.
    Some possible choices are 10, 20, and 30 for both variables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These embeddings can be integrated into a scikit-learn `transformer` so that
    they can be used in a grid search cross-validation process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To build a modeling training pipeline, we will split our corpus into training
    and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will also build functions to conveniently extract features and labels:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can instantiate the modeling pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define the parameter space, as well as the configuration, for the cross-validated
    grid search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s train our topic model by using the `fit` method of the sklearn
    API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Great! You have just created your topic model, which leverages the graph''s
    information. Once the best model has been identified, we can use this model on
    the test dataset to evaluate its performance. To do so, we must define the following
    helper function, which allows us to obtain a set of predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `sklearn` functionalities, we can promptly look at the performance of
    the trained classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'This provides the following output, which shows the overall performance measure
    that''s received by the F1-score. This is around 0.6 – 0.8, depending on how unbalanced
    classes are accounted for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: You can play around with the types and hyperparameters of the analytical pipeline,
    vary the models, and experiment with different values when you're encoding the
    embeddings. As we mentioned previously, the preceding approach is clearly transductive
    since it uses an embedding that's been trained on the entire dataset. This is
    a common situation in semi-supervised tasks, where the labeled information is
    only present in a small subset of points, and the task is to infer the labels
    for all the unknown samples. In the next subsection, we will outline how to build
    an inductive classifier using graph neural networks. These can be used when the
    test samples are not known at training time.
  prefs: []
  type: TYPE_NORMAL
- en: Graph neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's describe a neural network-based approach that natively integrates
    and makes use of the graph structure. Graph neural networks were introduced in
    [*Chapter 3*](B16069_03_Final_JM_ePub.xhtml#_idTextAnchor046), *Unsupervised Graph
    Learning*, and [*Chapter 4*](B16069_04_Final_JM_ePub.xhtml#_idTextAnchor064),
    *Supervised Graph Learning*. However, here, we will show you how to apply this
    framework to heterogeneous graphs; that is, graphs where there is more than one
    type of node. Each node type might have a different set of features and the training
    might target only one specific node type over the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach we will show here will make use of `stellargraph` and the `GraphSAGE`
    algorithms, which we described previously. These methods also support the use
    of features for each node, instead of just relying on the topology of the graph.
    If you do not have any node features, the one-hot node representation can be used
    in its place, as shown in [*Chapter 6*](B16069_06_Final_JM_ePub.xhtml#_idTextAnchor100),
    *Social Network Graphs*. However, here, to make things more general, we will produce
    a set of node features based on the TF-IDF score (which we saw earlier) for each
    entity and keyword. Here, we will show you a step-by-step guide that will help
    you train and evaluate a model, based on graph neural networks, for predicting
    document topic classification:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by computing the TF-IDF score for each document. `sklearn` already
    provides some functionalities that allow us to easily compute the TF-IDF scores
    from a corpus of documents. The `TfidfVectorizer` `sklearn` class already comes
    with a `tokenizer` embedded. However, since we already have a tokenized and lemmatized
    version that we extracted with `spacy`, we can also provide an implementation
    of a custom tokenizer that leverages on spaCy processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This can be used in `TfidfVectorizer`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To make the approach truly inductive, we will only train the TF-IDF for the
    training set. This will only be applied to the test set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For our convenience, the two TF-IDF representations (for the training and test
    sets) can now be stacked together into a single data structure representing the
    features for the document nodes for the whole graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Beside the feature information for document nodes, we will also build a simple
    feature vector for entities, based on the one-hot encoding representation of the
    entity type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have all the information we need to create an instance of a `StellarGraph`.
    We will do this by merging the information of the node features, both for documents
    and for entities, with the connections provided by the `edges` DataFrame. We should
    only filter out some of the edges/nodes so that we only include the documents
    that belong to the targeted topics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we have created our `StellarGraph`. We can inspect the network,
    similar to what we did for `networkx`, with the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following overview:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `StellarGraph` description is actually very informative. Besides, `StellarGraph`
    also natively handles different types of nodes and edges and provides out-of-the-box
    segmented statistics for each node/edge type.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You may have noted that the graph we just created includes both training and
    test data. To truly test the performance of an inductive approach and avoid information
    from being linked between the train and test sets, we need to create a subgraph
    that only contains the data available at training time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The considered subgraph contains 16,927 nodes and 62,454 edges, compared to
    the 23,998 nodes and 86,849 edges in the entire graph.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we only have the data and the network available at training time,
    we can build our machine learning model on top of it. To do so, we will split
    the data into train, validation, and test data. For training, we will only use
    10% of the data, which resembles a semi-supervised task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can start to build our graph neural network model using `stellargraph`
    and the `keras` API. First, we will create a generator able to produce the samples
    that will feed the neural network. Note that, since we are dealing with a heterogeneous
    graph, we need a generator that will sample examples from nodes that only belong
    to specific class. Here, we will be using the `HinSAGENodeGenerator` class, which
    generalizes the node generator we used for the homogeneous graph into heterogeneous
    graphs, allowing us to specify the node type we want to target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using this object, we can create a generator for the train and validation datasets:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can create our GraphSAGE model. As we did for the generator, we need
    to use a model that can handle heterogenous graphs. Here, `HinSAGE` will be used
    in place of `GraphSAGE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that in the final dense layer, we use a *sigmoid* activation function
    instead of a *softmax* activation function, since the problem at hand is a multi-class,
    multi-label task. Thus, a document may belong to more than one class, and the
    sigmoid activation function seems a more sensible choice in this context. As usual,
    we will compile our Keras model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will train the neural network model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure.7.16 – (Top) Train and validation accuracy versus the number of epochs.
    (Bottom) Binary cross-entropy loss for the training and validation dataset versus
    the number of epochs](img/B16069_07_016.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure.7.16 – (Top) Train and validation accuracy versus the number of epochs.
    (Bottom) Binary cross-entropy loss for the training and validation dataset versus
    the number of epochs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding graph shows the plots of the evolution of the train and validation
    losses and accuracy versus the number of epochs. As we can see, the train and
    validation accuracy increase consistently, up to around 30 epochs. Here, the accuracy
    of the validation set settle to a *plateau*, whereas the training accuracy continues
    to increase, indicating a tendency for overfitting. Thus, stopping training at
    around 50 seems a rather legitimate choice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the model has been trained, we can test its performance on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should provide the following values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that because of the unbalanced label distribution, accuracy may not be
    the best choice for assessing performances. Besides, a value of 0.5 is generally
    used for thresholding, so providing label assignment may also be sub-optimal in
    unbalanced settings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To identify the best threshold to be used to classify the documents, we will
    compute the prediction over all the test samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will compute the F1-score with a macro average (where the F1-score
    for the single classes are averaged) for different threshold choices:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As shown in the following graph, a threshold value of 0.2 seems to be the best
    choice as it achieves the best performance:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Macro-averaged F1-score versus the threshold used for labeling'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16069_07_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 7.17 – Macro-averaged F1-score versus the threshold used for labeling
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using a threshold value of 0.2, we can extract the classification report for
    the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, we have trained a graph neural network model and assessed its
    performance. Now, let''s apply this model to a set of unobserved data – the data
    that we left out at the very beginning – and represent the true test data in an
    inductive setting. To do this, we need to instantiate a new generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the graph we''ve taken as an input from `HinSAGENodeGenerator` is
    now the entire graph (in place of the filtered one we used previously), which
    contains both training and test documents. Using this class, we can create a generator
    that only samples from the test nodes, filtering out the ones that do not belong
    to one of our main selected topics:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model can then be evaluated over these samples, and the labels are predicted
    using the threshold we identified earlier; that is, 0.2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can extract the performance of the inductive test dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following table:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Compared to the shallow learning method, we can see that we have achieved a
    substantial improvement in performance that's between 5-10%.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how to process unstructured information and how
    to represent such information by using graphs. Starting from a well-known benchmark
    dataset, the Reuters-21578 dataset, we applied standard NLP engines to tag and
    structure textual information. Then, we used these high-level features to create
    different types of networks: knowledge-based networks, bipartite networks, and
    projections for a subset of nodes, as well as a network relating the dataset topics.
    These different graphs have also allowed us to use the tools we presented in previous
    chapters to extract insights from the network representation.'
  prefs: []
  type: TYPE_NORMAL
- en: We used local and global properties to show you how these quantities can represent
    and describe structurally different types of networks. We then used unsupervised
    techniques to identify semantic communities and cluster documents that belong
    to similar subjects/topics. Finally, we used the labeled information provided
    in a dataset to train supervised multi-class multi-label classifiers, which also
    leveraged the topology of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we applied supervised techniques to a heterogeneous graph, where two
    different node types are present: documents and entities. In this setting, we
    showed you how to implement both transductive and inductive approaches by using
    shallow learning and graph neural networks, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at another domain where graph analytics can
    be efficiently used to extract insights and/or create machine learning models
    that leverage network topology: transactional data. The next use case will also
    allow you to generalize the bipartite graph concepts that were introduced in this
    chapter to another level: tripartite graphs.'
  prefs: []
  type: TYPE_NORMAL
