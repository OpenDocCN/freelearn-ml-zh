["```py\nimport cv2 as cv\nfrom feature_matching import FeatureMatching\n\ndef main():\n    capture = cv.VideoCapture(0)\n    assert capture.isOpened(), \"Cannot connect to camera\"\n```", "```py\n capture.set(cv.CAP_PROP_FPS, 10)\n capture.set(cv.CAP_PROP_FRAME_WIDTH, 640)\n capture.set(cv.CAP_PROP_FRAME_HEIGHT, 480)\n```", "```py\nmatching = FeatureMatching(train_image='train.png')\n```", "```py\n    for success, frame in iter(capture.read, (False, None)):\n        cv.imshow(\"frame\", frame)\n        match_succsess, img_warped, img_flann = matching.match(frame)\n```", "```py\n        if match_succsess:\n            cv.imshow(\"res\", img_warped)\n            cv.imshow(\"flann\", img_flann)\n        if cv.waitKey(1) & 0xff == 27:\n            break\n```", "```py\nclass FeatureMatching: \n     def __init__(self, train_image: str = \"train.png\") -> None:\n```", "```py\nself.f_extractor = cv.xfeatures2d_SURF.create(hessianThreshold=400)\n```", "```py\nself.img_obj = cv.imread(train_image, cv.CV_8UC1)\nassert self.img_obj is not None, f\"Could not find train image {train_image}\"\n```", "```py\nself.sh_train = self.img_obj.shape[:2]\n```", "```py\nself.key_train, self.desc_train = \\\n    self.f_extractor.detectAndCompute(self.img_obj, None)\n```", "```py\nindex_params = {\"algorithm\": 0, \"trees\": 5}\nsearch_params = {\"checks\": 50}\nself.flann = cv.FlannBasedMatcher(index_params, search_params)\n```", "```py\nself.last_hinv = np.zeros((3, 3))\nself.max_error_hinv = 50.\nself.num_frames_no_success = 0\nself.max_frames_no_success = 5\n```", "```py\nself.f_extractor = cv2.xfeatures2d_SURF.create(hessianThreshold=400)\n```", "```py\nkey_query = self.f_extractor.detect(img_query)\n```", "```py\nimg_keypoints = cv2.drawKeypoints(img_query, key_query, None,\n     (255, 0, 0), 4)\ncv2.imshow(\"keypoints\",img_keypoints) \n```", "```py\nkey_query, desc_query = self.f_extractor.compute(img_query, key_query)\n```", "```py\nkey_query, desc_query = self.f_extractor.detectAndCompute (img_query, None)\n```", "```py\ngood_matches = self.match_features(desc_query)\n```", "```py\ndef match_features(self, desc_frame: np.ndarray) -> List[cv2.DMatch]:\n        matches = self.flann.knnMatch(self.desc_train, desc_frame, k=2)\n```", "```py\n# discard bad matches, ratio test as per Lowe's paper\ngood_matches = [ x[0] for x in matches \n    if x[0].distance < 0.7 * x[1].distance]\n```", "```py\nreturn good_matches \n```", "```py\ndef draw_good_matches(img1: np.ndarray,\n                      kp1: Sequence[cv2.KeyPoint],\n                      img2: np.ndarray,\n                      kp2: Sequence[cv2.KeyPoint],\n                      matches: Sequence[cv2.DMatch]) -> np.ndarray:\n```", "```py\nrows1, cols1 = img1.shape[:2]\nrows2, cols2 = img2.shape[:2]\nout = np.zeros((max([rows1, rows2]), cols1 + cols2, 3), dtype='uint8')\n```", "```py\nout[:rows1, :cols1, :] = img1[..., None]\nout[:rows2, cols1:cols1 + cols2, :] = img2[..., None]\n```", "```py\nfor m in matches:\n    c1 = tuple(map(int,kp1[m.queryIdx].pt))\n    c2 = tuple(map(int,kp2[m.trainIdx].pt))\n    c2 = c2[0]+cols1,c2[1]\n```", "```py\nradius = 4\nBLUE = (255, 0, 0)\nthickness = 1\n# Draw a small circle at both co-ordinates\ncv2.circle(out, c1, radius, BLUE, thickness)\ncv2.circle(out, c2, radius, BLUE, thickness)\n\n# Draw a line in between the two points\ncv2.line(out, c1, c2, BLUE, thickness)\n```", "```py\nreturn out\n```", "```py\ncv2.imshow('imgFlann', draw_good_matches(self.img_train, \n     self.key_train, img_query, key_query, good_matches))\n```", "```py\ntrain_points = [self.key_train[good_match.queryIdx].pt\n                for good_match in good_matches]\nquery_points = [key_query[good_match.trainIdx].pt\n                for good_match in good_matches]\n```", "```py\ndef detect_corner_points(src_points: Sequence[Point],\n                         dst_points: Sequence[Point],\n                         sh_src: Tuple[int, int]) -> np.ndarray:\n```", "```py\nH, _ = cv2.findHomography(np.array(src_points), np.array(dst_points), cv2.RANSAC)\n```", "```py\nif H is None:\n    raise Outlier(\"Homography not found\")\n```", "```py\nheight, width = sh_src\nsrc_corners = np.array([(0, 0), (width, 0),\n                        (width, height),\n                        (0, height)], dtype=np.float32)\n```", "```py\nreturn cv2.perspectiveTransform(src_corners[None, :, :], H)[0]\n```", "```py\ndst_corners = detect_corner_points(\n                train_points, query_points, self.sh_train)\n```", "```py\ndst_corners[:, 0] += self.sh_train[1]\ncv2.polylines(\n    img_flann,\n    [dst_corners.astype(np.int)],\n    isClosed=True,\n    color=(0,255,0),\n    thickness=3)\n```", "```py\nHinv = cv2.linalg.inverse(H) \n```", "```py\n@staticmethod\ndef scale_and_offset(points: Sequence[Point],\n                     source_size: Tuple[int, int],\n                     dst_size: Tuple[int, int],\n                     factor: float = 0.5) -> List[Point]:\n    dst_size = np.array(dst_size)\n    scale = 1 / np.array(source_size) * dst_size * factor\n    bias = dst_size * (1 - factor) / 2\n    return [tuple(np.array(pt) * scale + bias) for pt in points]\n```", "```py\ntrain_points_scaled = self.scale_and_offset(\n    train_points, self.sh_train, sh_query)\n```", "```py\nHinv, _ = cv2.findHomography(\n    np.array(query_points), np.array(train_points_scaled), cv2.RANSAC)\n```", "```py\nimg_warp = cv2.warpPerspective(img_query, Hinv, (sh_query[1], sh_query[0]))\n```", "```py\ndef match(self, frame):\n    # create a working copy (grayscale) of the frame\n    # and store its shape for convenience\n    img_query = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    sh_query = img_query.shape # rows,cols \n```", "```py\nkey_query, desc_query = self.f_extractor.detectAndCompute(\n    img_query, None)\ngood_matches = self.match_features(desc_query)\ntrain_points = [self.key_train[good_match.queryIdx].pt\n                for good_match in good_matches]\nquery_points = [key_query[good_match.trainIdx].pt\n                for good_match in good_matches]\n```", "```py\ntry:\n    # early outlier detection and rejection\n    if len(good_matches) < 4:\n        raise Outlier(\"Too few matches\")\n```", "```py\ndst_corners = detect_corner_points(\n    train_points, query_points, self.sh_train)\n```", "```py\nif np.any((dst_corners < -20) | (dst_corners > np.array(sh_query) + 20)):\n    raise Outlier(\"Out of image\")\n```", "```py\nfor prev, nxt in zip(dst_corners, np.roll(\n        dst_corners, -1, axis=0)):\n    area += (prev[0] * nxt[1] - prev[1] * nxt[0]) / 2.\n```", "```py\nif not np.prod(sh_query) / 16\\. < area < np.prod(sh_query) / 2.:\n    raise Outlier(\"Area is unreasonably small or large\")\n```", "```py\ntrain_points_scaled = self.scale_and_offset(\n    train_points, self.sh_train, sh_query)\nHinv, _ = cv2.findHomography(\n    np.array(query_points), np.array(train_points_scaled), cv2.RANSAC)\n```", "```py\nsimilar = np.linalg.norm(\nHinv - self.last_hinv) < self.max_error_hinv\nrecent = self.num_frames_no_success < self.max_frames_no_success\nif recent and not similar:\n raise Outlier(\"Not similar transformation\")\n```", "```py\nexcept Outlier as e:\n    print(f\"Outlier:{e}\")\n    self.num_frames_no_success += 1\n    return False, None, None\n```", "```py\nelse:\n    # reset counters and update Hinv\n    self.num_frames_no_success = 0\n    self.last_h = Hinv\n```", "```py\nimg_warped = cv2.warpPerspective(\n    img_query, Hinv, (sh_query[1], sh_query[0]))\n```", "```py\nimg_flann = draw_good_matches(\n    self.img_obj,\n    self.key_train,\n    img_query,\n    key_query,\n    good_matches)\n# adjust x-coordinate (col) of corner points so that they can be drawn\n# next to the train image (add self.sh_train[1])\ndst_corners[:, 0] += self.sh_train[1]\ncv2.polylines(\n    img_flann,\n    [dst_corners.astype(np.int)],\n    isClosed=True,\n    color=(0,255,0),\n    thickness=3)\nreturn True, img_warped, img_flann\n```"]