- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLOps Governance with Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the rapidly evolving digital era, the successful implementation of **machine
    learning** (**ML**) solutions is not just about creating sophisticated models
    that can predict outcomes accurately for complex use cases. While this is undoubtedly
    essential, the proficient management and governance of **artificial intelligence**
    (**AI**)/**ML operations** (**MLOps**) is equally important. This is especially
    important in an enterprise setting where companies have to ensure they adhere
    to several internal policies and regulatory compliance requirements. This chapter
    delves into different MLOps governance components and how you can utilize features
    available within Google Cloud to implement them.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps governance revolves around instituting a structured approach to managing
    and optimizing the various moving parts of ML operations. It encompasses the processes,
    tools, and guidelines that ensure the smooth functioning of ML projects, all while
    complying with required policies and regulations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover MLOps governance on Google Cloud, with a focus
    on the following key areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MLOps governance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case studies of MLOps governance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing MLOps governance on Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, our goal is to equip you with a comprehensive understanding
    of MLOps governance, its implementation on Google Cloud, and its critical role
    in maintaining successful, scalable, and compliant ML operations.
  prefs: []
  type: TYPE_NORMAL
- en: What is MLOps governance and what are its key components?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLOps refers to the discipline that combines ML, data science, and DevOps principles
    to manage the life cycle of ML models efficiently. The goal of MLOps is to create
    a streamlined pipeline for developing, deploying, and maintaining ML models, ensuring
    that these models provide reliable and consistent results. However, the implementation
    and management of such a practice require a governing framework to ensure adherence
    to best practices and standards. This governing framework is what we refer to
    as MLOps governance.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps governance is an essential, yet often overlooked, aspect of implementing
    and managing ML models within an organization. It encapsulates a comprehensive
    set of rules, procedures, and guidelines aimed at overseeing the ML models throughout
    their life cycle. This governance plays a pivotal role in ensuring that the MLOps
    pipeline operates smoothly and ethically, mitigating any risks associated with
    ML model deployment and usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary focus of MLOps governance is to create a reliable, transparent,
    and accountable ML system within an organization. This involves overseeing aspects
    such as data handling, model development, model deployment, model monitoring,
    and model auditing and can be broken down into two key facets: **data governance**
    and **model governance**.'
  prefs: []
  type: TYPE_NORMAL
- en: Data governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML models are only as good as the data they are trained on. In MLOps governance,
    data handling refers to the governance of how data is collected, stored, processed,
    and used. It entails ensuring the quality and relevance of data, preserving data
    privacy, and complying with relevant regulations. This guarantees that the data
    that’s used for model training is not only of high quality but also ethically
    sourced and used.
  prefs: []
  type: TYPE_NORMAL
- en: Model governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model governance comprises the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model deployment management**: Overseeing model deployment involves ensuring
    that the model is correctly integrated into the organization’s system and that
    it operates as expected. It also involves checking that the model doesn’t inadvertently
    cause any harmful outcomes, such as biased results or privacy violations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model auditing**: MLOps governance ensures that there is a systematic review
    of the ML models in terms of their performance, ethical implications, and overall
    impact on the organization. Model auditing is essential to maintain transparency
    and accountability, particularly in scenarios where the model’s predictions significantly
    influence business decisions or user experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model monitoring**: Once the model has been deployed, MLOps governance requires
    that it be monitored continuously for any changes in its performance. This includes
    tracking the model’s accuracy, detecting data drift, and making sure the model
    continues to deliver reliable predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLOps governance is not a one-size-fits-all practice; it needs to be tailored
    to the specific needs and circumstances of each organization. This might involve
    customizing the governance based on the nature of the data being handled, the
    type of ML models being used, the specific applications of these models, and the
    broader regulatory landscape.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, MLOps governance is a critical component of any organization that
    employs ML models. By establishing robust MLOps governance, organizations can
    ensure that their MLOps practices are not just effective but also ethical, transparent,
    and compliant.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise scenarios that highlight the importance of MLOps governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the importance of MLOps governance, let’s go through some real-world
    scenarios that highlight this.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 1 – limiting bias in AI solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a financial services firm deploying a suite of ML models to predict
    credit risk. A large firm in the finance sector would have an array of internal
    policies around the data access, usage, and risk assessment of predictive models
    that its ML solutions will need to adhere to. This could range from limits on
    what data can be used for such purposes to who can access the model’s outputs.
    It would also be obligated to follow several regulatory requirements, such as
    preventing bias against protected classes in its decision-making models. For example,
    a bank would need to ensure that its decision-making process around loan approval
    is not biased based on race or gender. Even if the regulators can’t decipher the
    underlying ML models, they can conduct statistical analysis to detect whether
    there is a significant correlation between loan approvals and factors such as
    race. If a bank is found to be biased in its decision-making, besides being hit
    with substantial penalties by the regulators, it would also have a major public
    relations disaster on its hands. So the bank needs to build checks and balances
    in their ML development life cycle to flag any such issues in their models under
    development and prevent such models from ever reaching production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 2 – the need to constantly monitor shifts in feature distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a scenario where an online e-commerce giant makes extensive use of
    AI to provide personalized recommendations to its retail customers. It has a set
    of models that seem to be working well in production. Now, the retailer is making
    a big marketing push to acquire users in additional regions that have been underrepresented
    in its customer base so far. As the influx of customers from new regions starts
    to grow, the retailer’s business development team notices a sharp decline in its
    click-through rate and revenue per user session based on the new monthly sales
    analytics report. When its analysts dig into the possible causes, they realize
    that the age distribution of users from the new regions is significantly different
    from the age distribution of the customers from the existing regions. This type
    of shift in feature/data distribution is known as **data drift** in MLOps parlance
    and can have a significant impact on user experience and, ultimately, the company’s
    bottom line. Although we are considering a hypothetical scenario where the company’s
    expansion into additional regions is causing a shift in data, this can happen
    due to several different scenarios, including, but not limited to, a shift in
    marketing strategy, a change in the economy, and a change in product offerings.
  prefs: []
  type: TYPE_NORMAL
- en: So, it’s important to have checks in place to catch such material changes in
    inference input data early so that the data science team can mitigate its impact
    by either building newer models with more recent data or building more targeted
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 3 – the need to monitor costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With great power comes great responsibility. Just like any other scalable technology
    in the cloud, there is a possibility of your team running up a huge bill if the
    resources are not planned properly, and proper budgets and limits are not set
    in the Google Cloud projects as safeguards. Consider a situation where a data
    scientist spins up a Vertex AI Workbench environment with an expensive GPU attached
    to the node for a quick experiment but then forgets to shut down the machine.
    Another similar scenario would be where someone tries to schedule an MLOps pipeline
    to run once a month with an extremely large GPU cluster but mistakenly configures
    it to run once a day, thereby making the cost 30x what it should have been. One
    or two such mistakes by themselves might not break the bank for a typical mid-size
    company but you can imagine how such costs can quickly add up, especially in large,
    distributed teams where no single person has full context of whether a training
    job running on a $10k/month cluster for last 3 days is an actual experiment being
    tracked or whether it’s just a mistake. So, it’s extremely important to set up
    cost management policies and, more importantly, automated controls that would
    limit the usage of specific resources on **Google Cloud** **Platform** (**GCP**).
  prefs: []
  type: TYPE_NORMAL
- en: Scenario 4 – monitoring how the training data is sourced
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although data controls at the source of the data would primarily be handled
    by the data owners, AI product/solution leaders need to be cognizant of where
    they are sourcing their data from. If the data that’s being used to train the
    models is later discovered to be unlicensed or coming from sources with questionable
    data quality, it can lead to a significant amount of wasted resources, both in
    terms of infrastructure cost and personnel overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the different tools and features available within Vertex
    AI to help you implement MLOps governance across your ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Tools in Vertex AI that can help with governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vertex AI offers several tools to help with ML solution governance and monitoring
    that you can utilize to implement and track your organization’s standard governance
    policies and more generic governance best practices. Please keep in mind that
    for many of the governance policies, especially the ones around security and cost
    management, you will need to use tools outside of Vertex AI. For example, to set
    up monthly cost limits and budgets, you will need to use GCP’s native billing
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through the details of the different tools within Vertex AI that
    can be used as part of MLOps governance processes:'
  prefs: []
  type: TYPE_NORMAL
- en: Model Registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vertex AI Model Registry provides a centralized, organized, and secure location
    for managing all ML models within an organization. This facilitates seamless and
    efficient ML operations, from development and validation to deployment and monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Vertex AI Model Registry](img/B17792_11_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Vertex AI Model Registry
  prefs: []
  type: TYPE_NORMAL
- en: Acting as a central hub for managing your ML models’ life cycles, Vertex AI
    Model Registry offers a bird’s-eye view of your models, thus enabling a more organized
    and efficient method of tracking and training new model versions. It serves as
    an access point from where you can deploy your preferred model version to an endpoint,
    either directly or by employing aliases.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Model Registry extends its support to custom models across all AutoML
    data types – be it text, tabular data, images, or videos. Moreover, it can incorporate
    BigQuery ML models, which means that if you have models that have undergone training
    via BigQuery ML, you can easily register them within Vertex AI Model Registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigating to the model version details page, you’re provided with numerous
    options: you can evaluate a model, deploy it to an endpoint, set up batch prediction,
    and inspect specific details related to the model. With its user-friendly and
    streamlined interface, Vertex AI Model Registry simplifies how you can manage
    and deploy your optimal models to a production environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore how Vertex AI Model Registry contributes to ML governance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Centralized repository for models**: Model Registry provides a single location
    where all models in the organization are stored. This centralized repository makes
    it easy for data scientists, ML engineers, and DevOps teams to store, access,
    and manage models. It also fosters cross-functional visibility and collaboration,
    which are essential elements in maintaining a robust governance framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version control and model lineage**: Every time a new model is trained or
    an existing model is updated, a new version is created in Model Registry. It maintains
    a history of all versions of a model, enabling easy tracking and comparison of
    different versions and ensuring that any updates or modifications are adequately
    logged and accounted for:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Vertex AI Model Registry (version view)](img/B17792_11_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Vertex AI Model Registry (version view)
  prefs: []
  type: TYPE_NORMAL
- en: '**Model metadata management**: In combination with the Metadata Store, it can
    help record the model’s lineage, providing information about the datasets, model
    parameters, and training pipelines that are used to build each version of the
    model. This lineage information is invaluable for auditing and compliance purposes,
    a critical aspect of ML governance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model validation and testing**: Before a model is deployed into production,
    it needs to be validated and tested to ensure it meets the requisite performance
    metrics. Model Registry supports this by integrating with Vertex AI’s model evaluation
    tools. These tools can compare different model versions and validate them against
    predefined metrics, ensuring that only accurate and reliable models are deployed.
    You can view detailed information about your models, including performance metrics,
    directly from the model details page:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Vertex AI model evaluation](img/B17792_11_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Vertex AI model evaluation
  prefs: []
  type: TYPE_NORMAL
- en: '**Integration with other Vertex AI services**: Model Registry integrates seamlessly
    with other Vertex AI services, including training pipelines and prediction services.
    Vertex AI Model Registry allows you to easily deploy your models to an endpoint
    with a few clicks or a few lines of code for real-time predictions. Integration
    with BigQuery allows you to register BQML models into Vertex AI Model Registry
    so that you can track all your models in one place. This integration facilitates
    end-to-end MLOps governance, allowing for efficient, consistent, and controlled
    ML operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's look at the Metadata Store.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vertex AI Metadata Store provides a robust, scalable system for tracking and
    managing all metadata associated with your ML workflows. Metadata, in this context,
    refers to information about the data used, the details of model training runs,
    the parameters used in these runs, the metrics generated, the artifacts created,
    and much more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Vertex AI Metadata Store model lineage](img/B17792_11_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Vertex AI Metadata Store model lineage
  prefs: []
  type: TYPE_NORMAL
- en: 'By systematically collecting and organizing this metadata, Vertex AI Metadata
    Store enables comprehensive tracking of the entire ML life cycle, facilitating
    effective ML governance. Here’s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Traceability**: One of the key features of Vertex AI Metadata Store is its
    ability to provide end-to-end traceability for tracked ML workflows. For every
    model built, it can trace back the lineage of the data and the steps taken during
    preprocessing, feature engineering, model training, validation, and deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model experimentation and comparison**: Vertex AI Metadata Store allows you
    to track and compare different model versions, parameters, and metrics. This aids
    in governance by ensuring that the development and selection of models are systematic
    and transparent, making it easier to replicate and audit your processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency and standardization**: By using Vertex AI Metadata Store, organizations
    can standardize metadata across different ML workflows. This promotes consistency
    in how ML workflows are executed and tracked, making it easier to apply governance
    policies and procedures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliance and regulatory adherence**: In industries such as healthcare or
    finance, ML models must comply with strict regulatory requirements. Vertex AI
    Metadata Store aids in this compliance by providing a detailed lineage of the
    ML model that can link the final trained model to the source of data and showcase
    that model development best practices were followed and proper evaluation criteria
    were satisfied before the model was deployed in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reproducibility**: Vertex AI Metadata Store also plays a significant role
    in ensuring the reproducibility of ML experiments, a crucial aspect of ML governance.
    By keeping track of all elements of an experiment, including data, configurations,
    parameters, and results, it ensures that the experiment can be reliably reproduced
    in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaboration and communication**: Metadata Store can foster better collaboration
    and communication within teams. With the comprehensive tracking of ML workflows,
    team members can understand what others are doing, promoting transparency and
    effective collaboration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vertex AI Metadata Store serves as a comprehensive repository for the metadata
    associated with your ML operations, presented as a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Vertex AI Metadata Store](img/B17792_11_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Vertex AI Metadata Store
  prefs: []
  type: TYPE_NORMAL
- en: Within this graph-based metadata framework, both artifacts and executions form
    the nodes, while events serve as the connecting edges that designate artifacts
    as the inputs or outputs of specific executions. *Contexts* denote logical subgroups,
    encompassing select sets of artifacts and executions for ease of reference.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Metadata Store permits the application of metadata as key-value pairs
    to the artifacts, executions, and contexts. For instance, a trained model could
    carry metadata that provides details about the training framework used, performance
    indicators such as accuracy, precision, and recall, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: To fully grasp shifts in the performance of your ML system, a thorough analysis
    of metadata produced by your ML workflows and the genealogy of its artifacts is
    mandatory. The lineage of an artifact encases all elements contributing to its
    origination, along with subsequent artifacts and metadata originating from this
    root artifact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for example, the lineage of a model, which could comprise the following
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: The datasets that were utilized for model training, testing, and evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hyperparameters that were employed during the training process of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific code base, which is instrumental in training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata that was accrued from the training and evaluation stages, such
    as the accuracy of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The artifacts that were derived from this parent model, such as batch prediction
    results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Vertex ML Meta data system arranges resources in a hierarchical structure,
    necessitating that all resources belong to a Metadata Store. Therefore, establishing
    a MetadataStore is a prerequisite for creating Metadata resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s delve into the key concepts and terminology that’s used in Vertex ML
    Metadata, which forms the basis for organizing resources and components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MetadataStore**: This forms the top-tier container for metadata resources.
    A MetadataStore is region-specific and linked to a unique Google Cloud project.
    Conventionally, organizations employ one shared MetadataStore per project to manage
    metadata resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata resources**: Vertex ML Metadata presents a graph-like data model
    to embody metadata originating from and utilized by ML workflows. The chief concepts
    under this model are artifacts, executions, events, and contexts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifact**: In the context of an ML workflow, an artifact is a distinct entity
    or data fragment generated or consumed. It could be datasets, models, input files,
    training logs, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context**: A context is leveraged to group artifacts and executions under
    one searchable and typed category. It can be used to denote sets of metadata.
    For instance, a run of an ML pipeline could be designated as a context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate, contexts can encapsulate the following metadata sets:'
  prefs: []
  type: TYPE_NORMAL
- en: A single run of a Vertex AI Pipelines pipeline, where the context represents
    the run and each execution symbolizes a step in the ML pipeline. This demonstrates
    how artifacts, executions, and context meld into Vertex ML Metadata’s graph data
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An experiment run from a Jupyter Notebook. Here, the context could symbolize
    the notebook, and each execution could denote a cell within that notebook:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event**: An event is the term that’s used to describe the connection between
    artifacts and executions. Each artifact can be generated by an execution and consumed
    by others. Events aid in establishing the lineage of artifacts in ML workflows
    by chaining together artifacts and executions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execution**: An execution is a log of a single step in the ML workflow, generally
    annotated with its runtime parameters. Examples of executions include model training,
    model evaluation, model deployment, data validation, and data ingestion.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata Schema**: A MetadataSchema provides a schema for specific types
    of artifacts, executions, or contexts. These schemas are employed to validate
    the key-value pairs at the time of creation of the corresponding Metadata resources.
    The schema validation only scrutinizes matching fields between the resource and
    the MetadataSchema. These types of schemas are depicted using OpenAPI schema objects
    and are generally described using YAML.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercise – using Vertex AI Metadata Store to track ML model development
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please refer to the accompanying notebook, *Chp11_Metadata_Store.ipynb*, [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_Metadata_Store.ipynb](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_Metadata_Store.ipynb),
    which walks you through the exercise to create a Metadata Store to store artifacts
    from a Vertex AI Pipeline run.
  prefs: []
  type: TYPE_NORMAL
- en: Let's talk about the Feature Store next.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google Cloud’s Vertex AI Feature Store is a managed service that allows data
    scientists and ML engineers to create, manage, and share ML features. The service
    helps accelerate the process of turning raw data into ML models, ensuring the
    models are built with high-quality data that is both reliable and consistent.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Vertex AI Feature Store primarily streamlines the model development process,
    it also plays a significant role in supporting ML governance. Let’s delve deeper
    into how this service assists with various facets of ML governance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data management and traceability**: A key aspect of ML governance is ensuring
    that the data that’s used for developing ML models is accurate, relevant, and
    traceable. Vertex AI Feature Store facilitates this by maintaining metadata about
    model lineage. This level of traceability makes it possible to audit the entire
    data pipeline effectively, thus promoting transparency and accountability in ML
    operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency**: Consistency in the data used for training and serving
    models is essential for ML governance. Discrepancies can lead to skewed results,
    negatively impacting the model’s performance and reliability. Vertex AI Feature
    Store provides unified storage for both training and online serving, ensuring
    that the same data features are used across these stages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality monitoring**: Maintaining the quality of data is another important
    aspect of ML governance. Poor data quality can lead to biased or inaccurate model
    predictions. Vertex AI Feature Store helps manage this by providing functionalities
    to monitor and validate the data ingested into the feature store. It can help
    identify anomalies or changes in data distribution over time, allowing timely
    intervention and rectification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data versioning and reproducibility**: In the context of ML governance, managing
    different versions of features is essential to track changes over time and enable
    reusability. Vertex AI Feature Store automatically tracks data updates and supports
    point-in-time lookup, which helps with consistency in training experiments and
    model reproducibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy and security**: ML governance also involves ensuring that data privacy
    and security regulations are adhered to. Vertex AI Feature Store is built on Google
    Cloud’s robust security model, ensuring that sensitive data is encrypted both
    at rest and in transit. With Google Cloud’s **Identity and Access Management**
    (**IAM**), organizations can also enforce fine-grained access controls to the
    feature store, ensuring that only authorized individuals have access to sensitive
    data features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are best practices when using Feature Store:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Modeling features for multiple entities**: There can be scenarios where some
    features apply to more than one type of entity. Consider, for instance, a computed
    value that tracks the clicks on a product by a user. Such a feature jointly characterizes
    the product-user duo. In these cases, it’s advisable to form a new entity type
    such as **product-user** to group the shared features. Entity IDs can be formed
    by combining the IDs of the individual entities involved, given that the IDs are
    strings. These collectively formed entity types are known as composite entity
    types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulating access with IAM policies**: IAM roles and policies provide a powerful
    way to govern access across multiple teams with diverse needs. For example, you
    might have ML researchers, data scientists, DevOps, and site reliability engineers
    who all need to access the same feature store, but the extent of their access
    can vary. Resource-level IAM policies can be employed to control access to a specific
    feature store or entity type. This allows for each role or persona within your
    organization to have a predefined IAM role tailored to the specific level of access
    required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizing batch ingestion with resource monitoring and tuning**: Batch ingestion
    jobs can intensify the CPU utilization of your feature store, thereby affecting
    online serving performance. To strike a balance, consider starting with one worker
    for every 10 online serving nodes and then monitoring the CPU usage during ingestion.
    The number of workers can be adjusted for future batch ingestion jobs based on
    your monitoring results to optimize throughput and CPU usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managing historical data with the disableOnlineServing field**: During the
    process of backfilling – that is, ingesting historical feature values – you can
    disable online serving, which effectively bypasses any modifications to the online
    store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adopting autoscaling for cost optimization**: For users facing frequent fluctuations
    in load, autoscaling can help in cost optimization. This enables Vertex AI Feature
    Store to auto-adjust the number of nodes according to CPU utilization. However,
    it’s worth noting that autoscaling might not be the best solution for managing
    sudden surges in traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing online serving nodes for real-time serving performance**: It’s essential
    to test the performance of your online serving nodes to ensure the real-time performance
    of your feature store. This can be accomplished by benchmarking parameters such
    as QPS, latency, and API. Remember to run these tests from the same region, use
    the gRPC API in the SDK for better performance, and conduct long-duration tests
    for more accurate metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batchReadFeatureValues` or `exportFeatureValues` request. This ensures the
    request runs a query over a subset of available feature data, which can result
    in significant savings on offline storage usage costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercise – using Vertex AI Feature Store to catalog and monitor features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please refer to the accompanying notebook, *Chp11_feature_store.ipynb*, [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_feature_store.ipynb](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_feature_store.ipynb),
    which walks you through the exercise of enabling model monitoring in Vertex AI
  prefs: []
  type: TYPE_NORMAL
- en: We'll discuss Kubeflow Pipelines in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This topic is covered in detail in [*Chapter 10*](B17792_10.xhtml#_idTextAnchor136),
    *Vertex AI Deployment and* *Automation Tools*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vertex AI Pipelines is designed to help manage and orchestrate ML workflows,
    and it plays a significant role in ML governance. By providing a platform for
    building, deploying, and managing ML workflows, this tool enables organizations
    to implement effective governance processes for their ML operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining and reusing ML pipelines**: Vertex AI Pipelines and Kubeflow Pipelines
    support defining pipelines as a series of componentized steps. These steps can
    encapsulate data preprocessing, model training, evaluation, deployment, and more.
    By defining these steps, you can enforce best practices, ensure that every step
    of the pipeline is traceable, and guarantee that all models are developed consistently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reuse of pipelines and components across multiple workflows is another significant
    advantage. This allows for standardization across different ML projects, which
    is a crucial aspect of ML governance. Standardization not only promotes code and
    process reuse but also reduces the risk of errors and ensures consistency in how
    ML models are built and deployed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Versioning and experiment tracking**: Both Vertex AI Pipelines and Kubeflow
    Pipelines offer capabilities for versioning and experiment tracking. With ML model
    versioning, different versions of models can be managed, and older versions can
    be rolled back when necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment tracking is also critical for governance. It provides visibility
    into how different model parameters and datasets impact the performance of a model.
    The ability to record and compare experiments also facilitates auditability, allowing
    you to understand the decision-making process behind each model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated and reproducible pipelines**: Automating ML workflows ensures that
    all steps are executed consistently and reliably, which is an essential aspect
    of ML governance. Both Vertex AI Pipelines and Kubeflow Pipelines allow for the
    creation of automated pipelines, which means each step in the ML process is reproducible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducibility is an often-understated aspect of ML governance. Reproducible
    pipelines mean you can track the data, code, configurations, and results at every
    step of the pipeline, which is crucial for debugging and auditing purposes. This
    is particularly important when your models need to comply with certain regulations
    that require transparent and explainable model development processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other Google Cloud services**: Vertex AI Pipelines and Kubeflow
    Pipelines are designed to work seamlessly with other Google Cloud services, such
    as BigQuery for data management, Cloud Storage for storing models and data, and
    AI Platform for model deployment. This integration makes it easier to implement
    governance processes across your entire ML workflow. For example, you can ensure
    data privacy and security by using BigQuery’s data governance features, or you
    can manage access control and monitor model performance using the capabilities
    of AI Platform. Vertex AI Pipelines and Kubeflow Pipelines offer various features
    that support ML governance, including pipeline definition and reuse, versioning,
    experiment tracking, automation, reproducibility, and integration with other Google
    Cloud services. By leveraging these features, organizations can effectively manage
    their ML operations, ensure compliance with best practices and regulations, and
    create a transparent, accountable, and efficient ML workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's talk about Monitoring in detail!
  prefs: []
  type: TYPE_NORMAL
- en: Model Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vertex AI Monitoring plays a critical role in the MLOps governance process
    by offering tools for the real-time monitoring and management of ML models. It
    enables organizations to establish transparency, accountability, and reliability
    in their ML processes. Here’s an overview of how Vertex AI Monitoring helps with
    ML governance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model monitoring**: Vertex AI Monitoring offers automated monitoring of models
    deployed in production. This means the system tracks the model’s performance continuously,
    identifying any potential drift in the data and degradation in the model’s performance.
    If the model’s performance dips below a predefined threshold, it alerts the appropriate
    stakeholders. This continuous monitoring is vital for maintaining the model’s
    accuracy and relevance, which are fundamental aspects of MLOps governance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data skew and drift detection**: One of the main features of Vertex AI Monitoring
    is its ability to detect data skew and drift. Data skew is the difference between
    the data used for training a model and the data used for serving predictions.
    Drift, on the other hand, is the change in data over time. Both can lead to a
    decline in the model’s performance. Vertex AI Monitoring automatically detects
    these discrepancies and provides timely alerts, allowing for rapid remediation.
    Ensuring the consistency and reliability of data aligns with the principle of
    data governance, a critical component of MLOps governance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated alerts**: Automated alerts from Vertex AI Monitoring provide an
    early warning system for any potential issues with the models in production. Timely
    alerts ensure that any problems are identified and remediated promptly, preventing
    any long-term impact on the model’s performance or the business operations. This
    feature is vital for risk management, a crucial aspect of MLOps governance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other Google Cloud tools**: Vertex AI Monitoring seamlessly
    integrates with other Google Cloud tools such as Cloud Logging and Cloud Monitoring.
    This allows you to create comprehensive dashboards for visualizing your ML model’s
    health and performance, and to receive alerts for any detected issues. These features
    enable more robust monitoring and troubleshooting capabilities, improving the
    overall governance of ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at the details of how a monitoring solution calculates training-serving
    skew and prediction drift.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vertex AI Monitoring uses **TensorFlow Data Validation** (**TFDV**) to detect
    training-serving skew and prediction drift by calculating distributions and distance
    scores. The process involves two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the baseline statistical distribution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the context of Vertex AI Monitoring, skew detection and drift detection
    hinge critically on the accurate definition of a baseline statistical distribution.
    The distinction between the baselines for these two facets lies in the data used
    to compute them:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Skew detection**: The baseline is derived from the statistical distribution
    of the feature values present in the training data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Drift detection**: Conversely, for drift detection, the baseline is formulated
    from the statistical distribution of the observed feature values from the recent
    production data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The process of calculating these distributions unfolds as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Categorical features**: The distribution for categorical features is determined
    by computing the quantity or proportion of occurrences for each potential value
    of the feature.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numerical features**: When dealing with numerical features, Vertex AI Monitoring
    segregates the entire range of possible feature values into uniform intervals.
    Subsequently, the number or percentage of feature values residing within each
    interval is computed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that the baseline is initially set at the time of creating
    a model monitoring job and is subject to recalculation only if there are updates
    to the training dataset allocated for the job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculating the statistical distribution of recent feature values seen in production
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The process initiates by contrasting the distribution of the most recent feature
    values, observed in a production environment, with a baseline distribution, through
    the computation of a distance score. Different methods are utilized for different
    types of features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Categorical features**: The L-infinity distance method is employed to compute
    the distance score'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numerical features**: The Jensen-Shannon divergence method is used to calculate
    the distance score'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When the computed distance score exceeds a predefined threshold, indicating
    a significant disparity between the two statistical distributions, Vertex AI Monitoring
    identifies and flags the inconsistency, labeling it as skew or drift.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following are best practices for utilizing Vertex AI Monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction request sampling rate**: To enhance cost efficiency, a prediction
    request sampling rate can be configured. This feature enables monitoring a portion
    of the production inputs to a model instead of the entire dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring frequency**: It’s possible to define the frequency at which the
    recently logged inputs of a deployed model are scrutinized for skew or drift.
    This frequency, also known as the monitoring window size, dictates the time frame
    of logged data evaluated in each monitoring run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerting thresholds**: You can set alerting thresholds for each feature that
    is monitored. If the statistical distance between the input feature distribution
    and its respective baseline surpasses this threshold, an alert is generated. By
    default, both categorical and numerical features are monitored, each with a threshold
    value of 0.3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shared configuration parameters across multiple models**: An online prediction
    endpoint can host more than one model. When skew or drift detection is enabled
    on an endpoint, certain configuration parameters, including detection type, monitoring
    frequency, and the fraction of input requests monitored, are shared across all
    models hosted on that endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model-specific configuration parameters**: Apart from the shared parameters,
    it is also possible to specify different values for other configuration parameters
    for each model. This flexibility allows you to tailor the monitoring settings
    according to the unique needs and behavior of each model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercise – [notebook] using Vertex AI Monitoring features to track the performance
    of deployed models in production environments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please refer to the accompanying notebook, `Chp11_Model_Monitoring.ipynb`, [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_Model_Monitoring.ipynb](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter11/Chp11_Model_Monitoring.ipynb),
    which walks you through the exercise of enabling model monitoring in Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at billing monitoring in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Billing monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**GCP** offers a suite of robust billing and cost management tools that can
    play a crucial role in MLOps governance. These tools provide fine-grained visibility
    into how resources are being utilized, helping organizations effectively manage
    costs associated with their ML workflows. Here’s how:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Budgets and alerts**: GCP’s budget and alerts feature allows organizations
    to establish custom budgets for their GCP projects or billing accounts, and configure
    alerts when the actual spending exceeds the defined thresholds. This tool is instrumental
    in tracking and controlling the costs associated with training, deploying, and
    running ML models. When integrated into the MLOps governance framework, it ensures
    that the expenses related to ML workflows do not exceed their allocated budgets,
    preventing cost overruns and promoting financial responsibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detailed billing reports**: GCP’s detailed billing reports offer insights
    into the specific costs associated with each service. For instance, an organization
    can view detailed reports about the expenses incurred for services such as Vertex
    AI, Cloud Storage, BigQuery, and Compute Engine. These reports allow organizations
    to understand which ML workflows or components are more cost-intensive and need
    optimization. This granular visibility is essential for cost governance in MLOps,
    enabling organizations to strategically plan their resource usage and manage costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Billing export to BigQuery**: GCP allows you to export detailed billing data
    to BigQuery, Google’s highly scalable and cost-effective data warehouse. This
    feature enables organizations to analyze their GCP billing data programmatically
    and build custom dashboards using data visualization tools such as Data Studio.
    With this, MLOps teams can better understand and manage the costs associated with
    various ML projects, and identify opportunities for savings and optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost management tools**: GCP’s cost management tools, such as the Pricing
    Calculator and the **Total Cost of Ownership** (**TCO**) tool, help organizations
    forecast their cloud expenses and compare them with the costs of running the same
    infrastructure on-premises or on other cloud platforms. These tools are especially
    valuable in the planning and budgeting stages of ML projects, enabling MLOps teams
    to make more informed decisions about resource allocation and cost optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud Functions for automating cost controls**: GCP’s serverless execution
    environment, Cloud Functions, can be used to create functions that automatically
    stop or start services based on custom logic. For example, you can write a function
    that automatically stops a Compute Engine instance when it’s not being used, thereby
    saving costs. This level of automated cost control can be invaluable in managing
    the costs associated with running ML models, a crucial aspect of MLOps governance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since billing and budget monitoring is a much broader topic than Vertex AI,
    it is outside the scope of this book, but you can refer to the GCP Billing documentation
    ([https://cloud.google.com/billing/docs/how-to](https://cloud.google.com/billing/docs/how-to))
    to dive deeper into the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went over the fundamentals of MLOps governance, detailing
    its key role in maintaining ML systems’ efficiency, accuracy, and reliability.
    To emphasize the importance of MLOps governance in real-world scenarios, we explored
    case studies from various sectors, showcasing how this governance model can dramatically
    impact the success of AI/ML implementations.
  prefs: []
  type: TYPE_NORMAL
- en: As we dove deeper into the topic, we clarified the core components of MLOps
    governance – data governance and model governance – offering an overview of their
    function and necessity within the ML model life cycle. Additionally, we went through
    some real-world scenarios that effectively underscored the relevance and importance
    of MLOps governance.
  prefs: []
  type: TYPE_NORMAL
- en: On the technical side, we enumerated and discussed several tools available within
    Vertex AI that aid in ML solution governance and monitoring. We touched upon the
    functionalities of Model Registry, Metadata Store, Feature Store, Vertex AI Pipelines,
    Model Monitoring, and GCP’s cost management tools. Through their combined use,
    we illustrated how you can establish robust, transparent, and compliant ML operations.
  prefs: []
  type: TYPE_NORMAL
- en: We supplemented this chapter with examples and exercises on implementing ML
    governance using Vertex AI to cement these concepts. These practical exercises
    offered hands-on experience with Vertex AI’s Model Registry, Metadata Store, and
    Model Monitoring functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section of this book, *Part 3*, *Prebuilt/Turnkey ML Solutions Available
    in GCP*, we will cover different out-of-the-box ML models and solutions such as
    GenAI/LLM models, Document AI, Vision APIs, and NLP APIs, which you can utilize
    to build ML solutions for different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'GCP Vertex AI Metadata Store documentation: [https://cloud.google.com/vertex-ai/docs/ml-metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata)'
  prefs: []
  type: TYPE_NORMAL
- en: 'GCP Vertex AI billing and budgeting features: [https://cloud.google.com/billing/docs/how-to](https://cloud.google.com/billing/docs/how-to)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Practitioner’s guide to MLOps: [https://cloud.google.com/resources/mlops-whitepaper](https://cloud.google.com/resources/mlops-whitepaper)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Prebuilt/Turnkey ML Solutions Available in GCP'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will learn about some of the most commonly used prebuilt ML
    solution offerings available in Google Cloud. Many of these solutions are ready
    to use and can be integrated with real-world use cases in no time. Most importantly,
    this part also covers the recently launched generative AI offerings within Vertex
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B17792_12.xhtml#_idTextAnchor173), *Vertex AI – Generative AI
    Tools*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B17792_13.xhtml#_idTextAnchor194), *Document AI – an End-to-End
    Solution for Processing Documents*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B17792_14.xhtml#_idTextAnchor203), *ML APIs for Vision, NLP,
    and Speech*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
