- en: Artificial Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Artificial neural networks try to mimic the way the human brain works. They
    are used to solve a number of difficult problems, such as understanding written
    or spoken language, identifying objects in an image, or driving a car.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn the basics of how an artificial neural network works, look at
    the steps and mathematical calculations needed to train it, and have a general
    view of complex neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the perceptron – the simplest type of neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a deep network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the backpropagation algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To complete this chapter, you will need to download the `transfusion.xlsx` file
    from the GitHub repository at [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09)[.](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09)
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the perceptron – the simplest type of neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks are inspired by the human brain' more specifically, by the neuron
    cells that compose it. Actually, since there have been major advances in neuroscience
    since the first artificial neuron was designed, it would be better to say that
    they are inspired by what was known about the brain some years ago.
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptron was the first attempt to build an artificial neural network
    (Frank Rosenblatt, 1959). It was actually a model of a single neuron, with multiple
    inputs and one output. The value at the output is calculated as the weighted sum
    of the inputs and these weights are adjusted iteratively. This simple implementation
    has many disadvantages and limitations, so it was later replaced by the multilayer
    perceptron. The most basic model of this artificial neural network has the structure
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bdc8e1b-7880-4957-97dc-3eaa548f0d74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The input and output layers are taken from the perceptron, but a hidden layer
    of nodes is now added. Each node in this layer acts in practice as a neuron. To
    understand how the inputs and outputs of each neuron work and how information
    is sent through the network, we need to know the details of how each neuron is
    built. A schematic view of an artificial neuron could be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e343766d-3cc8-4b5f-b2bb-9f6ff1a0e948.png)'
  prefs: []
  type: TYPE_IMG
- en: The combination function calculates the resulting input as the sum of the inputs
    weighted by w[i]. The activation function calculates the output using this input.
    The output range is usually limited to [0;1], using different functions. It is
    often the case that the neuron transmits the signal only if the input value is
    above a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: How does an artificial neural network learn? A training dataset is used, for
    which the outputs are known. The input values are fed into the network, the predicted
    output is compared to the real output, and the w[i] weights are adjusted iteratively
    at each step. This means that a neural network is a supervised learning model.
  prefs: []
  type: TYPE_NORMAL
- en: The more complex the problem, the larger the number of training samples needed
    to adjust the weights. We will also see that the number of hidden layers and neurons
    are also adjusted depending on the problem. Fine tuning these parameters is a
    complex problem, almost a field of study in itself.
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks are useful since they can model any mathematical
    function. So, even if the relationship between the input values is unknown, we
    can use a network to reproduce it and make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Since the training process can be complicated and the number of parameters that
    are adjusted at training time is large, it is often difficult to understand why
    an artificial neural network correctly predicts a given value. The explainability
    of the artificial intelligence models, based on neural networks, is also an extensively
    studied problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some applications of neural networks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Image analysis—faces, objects, colors, expressions, and gestures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sound analysis—voices, speech to text, and sentiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification—email spam, fraud in document content, and sentiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware failures—predictive and/or diagnostic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health risks and/or diagnostics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer or employee churn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see how training works in practice, following an example.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use a public dataset from the Blood Transfusion Service Center in Hsin-Chu
    City, Taiwan (*Knowledge discovery on RFM model using Bernoulli sequence*, by
    Yeh, I-Cheng, Yang, King-Jang, and Ting, Tao-Ming, Expert Systems with Applications,
    2008). The set contains information about blood donors, summarized in five variables:'
  prefs: []
  type: TYPE_NORMAL
- en: R (Recency – months since last donation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F (Frequency – total number of donations)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: M (Monetary – total blood donated in c.c.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T (Time – months since first donation)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A binary variable representing whether they donated blood in March 2007 (one
    stands for donating blood; zero stands for not donating blood)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We would like to prove how well an artificial neural network can learn from
    the first four of the preceding features, and predict the target, variable five.
    Follow these steps to reproduce and learn about the calculations already shown
    in the `transfusion.xlsx` file:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the `transfusion.xlsx` file into Excel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the worksheet named `transfusion`*,* you will find the input data. It should
    look something like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7967366a-93c2-410f-ad15-6d600228ff6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the data is not presented in any particular order, we can use the first
    500 entries to train the neural network. Open a new worksheet and rename it `training1`
    (remember that we are repeating the steps to create the worksheets already present
    in the file, so that you can compare your results).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a set of variables like the one you see in the following screenshot.
    If you use the same cells, it will be easier to follow the next steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/69450203-d090-42ce-92f0-2302834382e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we build an artificial neural network with four inputs (the four features
    in the input data) and one hidden layer containing two neurons, we need eight
    weight parameters: `w[11]`, `w[12]`, `w[13]`, and `w[14]` for hidden neuron one,
    and `w[2][1]`, `w[21]`, `w[23]` and `w[24]` for hidden neuron two. The remaining
    parameters will be explained later.'
  prefs: []
  type: TYPE_NORMAL
- en: From the worksheet named `transfusion`*,* copy the first 500 data rows (excluding
    the header).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `training1` worksheet click on cell *B22.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paste the copied cells.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You now have a table containing the input values, called x[1], x[2], x[3], [and]
    x[4], plus the output binary value, y. Column *#* in the table just shows the
    row number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The combination function of the hidden neuron *j* is the weighted sum of the
    inputs, as shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61e5a896-c544-4479-92ce-70b8a57ad7b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our example, *N=4*, which gives us the next two expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7b2f35d-7a34-4e15-84a2-f6aaf583b23f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/2e2aefae-4e9f-4556-acbb-d2ec47513588.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Taking into account these expressions, write the following formula in cell
    *G22*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*=$E$3*B22+$E$4*C22+$E$5*D22+$E$6*E22*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In cell *H22*, write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*=$E$7*B22+$E$8*C22+$E$9*D22+$E$10*E22*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy these expressions down to the rest of the cells in columns G and H. The
    simplest and most commonly used activation function is the following sigmoid function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0f5b8a1e-0a46-446d-a652-b3fb87820ca5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our example, x is the combination function calculated for each hidden neuron
    and each entry used for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61804567-fd35-481b-8e62-d2f33bb64a3d.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/8d789415-0d7b-46c7-bc99-fbb0f31af1d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Define cell I22 as *=1/(1+EXP(-G22))* and cell J22 as *=1/(1+EXP(-H22)).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy these formulas down to the rest of the rows in columns I and J. The last
    calculation is the neural network output, which is a weighted sum of the outputs
    from the hidden neurons, plus a constant value that acts as a threshold; if the
    total input is less that this value, the output is zero and the network does not
    activate. This can be expressed by the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/342955f2-5ac0-4bcc-ac54-baca8e6f59a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can then write the following in cell *K22*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*=$E$11+$E$12*I22+$E$13*J22*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the formula down to the rest of the cells in column K. Since *E11*, *E12*,
    and *E13* are the cells we have saved for theta 1, theta 2 and theta 3 respectively.
    We use all the defined weights and parameters in our calculations, but we don''t
    have values for them. Training a neural network implies finding the values for
    these parameters that make the output as close as the target value as possible,
    for example, for each combination of *x[1]*, *x**[2]*, *x[3]*,and *x[4]*, the
    difference between the value of *Output* and the value of *y* should be the minimum
    possible. We need to calculate three values then: the output error (Output-y),
    the squared error (Error²), and the sum of the squared errors, which is the value
    to minimize.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The function we are minimizing, the sum of the squared errors, is only one possible
    **loss function***.* There are other functions that are used to compare the output
    of the neural network with the training value. Studying when to apply each function
    is shown in more advanced machine learning books.
  prefs: []
  type: TYPE_NORMAL
- en: Define cell L22 as *=K22-F22.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the formula down to the rest of the rows in column L.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define cell M22 as *=L22^2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the formula down to the rest of the rows in column M.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define cell E15 as *=SUM(M22:M521).* This is the sum of the squared errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can now use Excel''s Solver to set values to w[11], w[12], w[13],w[14],
    w[2][1], w[21], w[23], w[24], θ[o], θ[1], and θ[2], while minimizing the sum of
    the squared errors:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to Data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Solver.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fill in the details as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6c5c74e7-bfe4-4416-9cc2-d95d14f046be.png)'
  prefs: []
  type: TYPE_IMG
- en: The objective is E15, where we store the sum of squared errors, and the variable
    cells E3 to E13.
  prefs: []
  type: TYPE_NORMAL
- en: Click on Solve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The optimal result is shown in the following table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Parameters** | **Values** |'
  prefs: []
  type: TYPE_TB
- en: '| w11 | -3.915205816 |'
  prefs: []
  type: TYPE_TB
- en: '| w12 | 0.055009315 |'
  prefs: []
  type: TYPE_TB
- en: '| w13 | 0.016855755 |'
  prefs: []
  type: TYPE_TB
- en: '| w14 | -0.301397506 |'
  prefs: []
  type: TYPE_TB
- en: '| w21 | -0.016701972 |'
  prefs: []
  type: TYPE_TB
- en: '| w22 | 0.451221978 |'
  prefs: []
  type: TYPE_TB
- en: '| w23 | -0.001645853 |'
  prefs: []
  type: TYPE_TB
- en: '| w24 | -0.011395209 |'
  prefs: []
  type: TYPE_TB
- en: '| theta0 | -0.349977457 |'
  prefs: []
  type: TYPE_TB
- en: '| theta1 | 0.247932886 |'
  prefs: []
  type: TYPE_TB
- en: '| theta2 | 1.256803829 |'
  prefs: []
  type: TYPE_TB
- en: '| **Square error** | **77.02669809** |'
  prefs: []
  type: TYPE_TB
- en: The results may vary depending on the type of regression used in Solver and
    on the initial values. The gradient descent algorithm search (explained in the
    *Understanding the backpropagation algorithm* section) might be trapped in a local
    minimum that has a larger value than the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Define cell N22 as *=round(K22)* to convert the output of the neural network
    to binary values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Comparing the predicted and linear values, you can build the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '|  |  | Real |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Predicted | 1 | 32 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 0 | 86 | 360 |'
  prefs: []
  type: TYPE_TB
- en: Use the confusion matrix to measure the accuracy of the neural network training.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you are satisfied with the training, you can use the values obtained for
    the parameters to **predict** the y value for the rest of the data (which was
    never used in the training, and can then be used to test the network output).
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to predict the target variable using the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Make a copy of the worksheet named `training1`. Name the new worksheet `test1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delete the range of cells *B22:F521.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the last 248 rows in the worksheet named `transfusion` to the new worksheet,
    starting on cell B22.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All calculations should work and you should be able to see the results of using
    the test data as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have now developed a simple exercise that shows how an artificial neural
    network learns from input data. The calculations we made are the base of the **backpropagation**
    algorithm, which is explained in detail in the last section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Building a deep network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our example of artificial neural network is very simple and only contains one
    hidden layer. Can we add more layers? Of course we can! The next step in complexity
    could be something similar to the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c050b30-e1ed-4643-aa73-191c25058bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: We added a new hidden layer with two neurons, but we could add more layers and
    more neurons per layer. The architecture of a network depends on the specific
    use we give it. Multilayer artificial neural networks are often known as **deep
    neural networks***.*
  prefs: []
  type: TYPE_NORMAL
- en: The output of a deep network is calculated in analogy with the single layer
    one, considering all inputs to each neuron, the activation function, and the addition
    of all the inputs to the output neuron. Looking at the preceding diagram, it is
    clear that each layer in the network is affected by the previous one. It is usually
    the case that, in order to solve complex problems, each layer learns a specific
    set of characteristics. For example, when identifying an image, the first layer
    could train on colors, the second on shapes, the third on objects, and so on,
    increasing in complexity as we advance toward the output.
  prefs: []
  type: TYPE_NORMAL
- en: As we add more neurons to the network, there are more parameters we need to
    adjust. The way this is done in practice will become clear in the following section,
    where the backpropagation algorithm is described.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the backpropagation algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two phases in the training process of a deep neural network: forward
    and back propagation. We have seen the forward phase in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the weighted sum of the inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f2c41a91-087f-4c24-9267-6ffa5e890830.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Apply the activation function to the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/16c6e136-e030-4cd2-a071-df9461aca7dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Find different activation functions in the suggested reading at the end of the
    chapter. The sigmoid function is the most common and is easier to use, but not
    the only one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the output by adding all the results from the last layer (N neurons):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d5f5294b-af9c-4713-85b2-41757d612f22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After the forward phase, we calculate the error as the difference between the
    output and the known target value: *Error = (Output-y)².*'
  prefs: []
  type: TYPE_NORMAL
- en: All weights are assigned random values at the beginning of the forward phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output, and therefore the error, are functions of the weights *w[i]* and
    *θ[i]*. This means that we could go backward from the error and see how a small
    variation in each weight affects the result. This is expressed in mathematical
    terms as the derivative or gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08aca1b3-5c40-4fa9-b81c-6537eb05be9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This equation measures the change in the error every time we change w[1] by
    a small amount. We actually apply an activation function inside each neuron, so
    the change in error turns into the following equation (known as the chain rule):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c02d2573-640e-4923-b633-c129c4ca12aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We want to change all weights values in the direction that decreases the error.
    This is the reason why the optimization method is called **gradient descent**.
    If we imagine the error as a function of two weights (there are more than two,
    of course, but we human beings have a hard time thinking beyond three dimensions!),
    we can picture this optimization as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5ef8f3f-cfa8-4b61-aaf1-05b5f915d86f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When are the weights adjusted? There are three methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Online**: With each new training sample, all weights are recalculated. This
    is very time-consuming and could lead to problems if the dataset has too many
    outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch**: The weights are calculated for the whole training dataset, calculating
    the accumulated error and using it to correct them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic**: The batch mode is used taking small samples of the training
    data. This speeds up the whole process and makes the method more robust against
    local optimal values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are now familiar with how artificial neural networks are built and how their
    output is calculated. It is generally impractical to perform these calculations
    as the size of the network grows, as often happens for all practical and useful
    implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have studied the basic principles of how artificial neural networks are built
    and how they learn from the input data. Even if the actual method, in practice,
    for using neural networks is different than what we have done in our example,
    our approach is useful in order to understand the details and to go beyond the
    idea that neural networks are mysterious black boxes that magically solve problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how we can use pre-built machine learning models
    available in Azure, connecting them to Excel to solve the problems we have presented
    up to now.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the results of the perceptron test, build the confusion matrix and evaluate
    the quality of the prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There is one important step that is missing in the binary classification problem
    that we solved with our artificial neural network, which might improve the result
    if we implement it. What did we miss? Hint: build an histogram of the binary variable
    that indicates whether there was a blood donation in March 2007.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Brief Introduction to Neural Networks* by David Kriesel*,* available online
    at [http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-1col-dkrieselcom.pdf](http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-1col-dkrieselcom.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural Networks and Deep Learning* by Michael A. Nielsen*,* available online
    at [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning: Using Algorithms to Make Machines Think*, [https://opensourceforu.com/2017/12/deep-learning-using-algorithms-to-make-machines-think/](https://opensourceforu.com/2017/12/deep-learning-using-algorithms-to-make-machines-think/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
