<html><head></head><body>
  <div id="_idContainer202" class="Basic-Text-Frame">
    <h1 class="chapterNumber">7</h1>
    <h1 id="_idParaDest-183" class="chapterTitle">Visualizing Convolutional Neural Networks</h1>
    <p class="normal">Up to this point, we have only dealt with tabular data and, briefly, text data, in <em class="chapterRef">Chapter 5</em>, <em class="italic">Local Model-Agnostic Interpretation Methods</em>. This chapter will exclusively explore interpretation methods that work with images and, in particular, with the <strong class="keyWord">Convolutional Neural Network</strong> (<strong class="keyWord">CNN</strong>) models that<a id="_idIndexMarker682"/> train image classifiers. Typically, deep learning models are regarded as the epitome of black box models. However, one of the benefits of a CNN is how easily it lends itself to visualization, so we can not only visualize outcomes but also every step of the learning process with <strong class="keyWord">activations</strong>. The possibility of interpreting these steps is rare among so-called black box models. Once we have grasped how CNNs learn, we will study how to use state-of-the-art gradient-based attribution methods, such as <em class="italic">saliency maps</em> and <em class="italic">Grad-CAM</em> to debug class attribution. Lastly, we will extend our attribution debugging know-how with perturbation-based attribution methods such as <em class="italic">occlusion sensitivity</em> and <code class="inlineCode">KernelSHAP</code>.</p>
    <p class="normal">These are the main topics we are going to cover:</p>
    <ul>
      <li class="bulletList">Assessing the CNN classifier with traditional interpretation methods</li>
      <li class="bulletList">Visualizing the learning process with an activation-based method</li>
      <li class="bulletList">Evaluating misclassifications with gradient-based attribution methods</li>
      <li class="bulletList">Understanding classifications with perturbation-based attribution methods</li>
    </ul>
    <h1 id="_idParaDest-184" class="heading-1">Technical requirements</h1>
    <p class="normal">This chapter’s example uses the <code class="inlineCode">mldatasets</code>, <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, <code class="inlineCode">sklearn</code>, <code class="inlineCode">tqdm</code>, <code class="inlineCode">torch</code>,<code class="inlineCode"> torchvision</code>,<code class="inlineCode"> pytorch-lightning</code>,<code class="inlineCode"> efficientnet-pytorch</code>,<code class="inlineCode"> torchinfo</code>, <code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, and <code class="inlineCode">captum</code> libraries. Instructions on how to install all of these libraries are in the <em class="italic">Preface</em>. </p>
    <div class="note">
      <p class="normal">The code for this chapter is located here: <a href="https://packt.link/qzUvD"><span class="url">https://packt.link/qzUvD</span></a>.</p>
    </div>
    <h1 id="_idParaDest-185" class="heading-1">The mission</h1>
    <p class="normal">Over two billion tons of waste is produced annually globally, and it’s expected to grow to over 3.5 billion tons by 2050. The alarming rise in global waste production and the need for effective waste management systems have become increasingly critical in recent years. Over half of all household trash in high-income countries is recyclable, with 20% in lower-income countries and rising. Currently, most waste ends up in landfills or incinerated, contributing to environmental pollution and climate change. This is avoidable, considering that, globally, a significant portion of all recyclable materials is not recycled. </p>
    <p class="normal">Assuming recyclable waste is collected, it can still be hard and costly to sort it. Previously, waste classification technologies included:</p>
    <ul>
      <li class="bulletList">Separating materials by size with rotating cylindrical screens with holes (“trommel screens”)</li>
      <li class="bulletList">Separating ferrous and non-ferrous metals with magnetic forces and magnetic fields (“eddy current separators”)</li>
      <li class="bulletList">Separating by weight with air</li>
      <li class="bulletList">Separating by density with water (“sink-float separation”)</li>
      <li class="bulletList">Manual sorting performed by humans</li>
    </ul>
    <p class="normal">Implementing all of these techniques effectively can be challenging, even for a large, wealthy, urban municipality. To tackle this challenge, <strong class="keyWord">smart recycling systems</strong> have emerged, leveraging computer vision and AI to classify waste efficiently and accurately.</p>
    <p class="normal">The development of smart recycling systems can be traced back to the early 2010s when researchers and innovators started exploring the potential of computer vision and AI to improve waste management processes. They first developed basic image recognition algorithms, utilizing features such as color, shape, and texture to identify waste materials. These systems were primarily used in research settings with limited commercial applications. As machine learning and AI became more advanced, smart recycling systems underwent significant improvements. CNNs and other deep learning techniques enabled these systems to learn from vast amounts of data and improve their waste classification accuracy. Additionally, the integration of AI-driven robotics allowed for automated sorting and handling of waste materials, increasing efficiency in recycling plants.</p>
    <p class="normal">Costs are significantly lower than a decade ago for cameras, robots, and even chips that run deep learning models in low-latency, high-volume scenarios, making state-of-the-art smart recycling systems accessible to even smaller and poorer municipal waste management departments. One of these municipalities in Brazil is looking to revamp their 20-year-old recycling plant made up of a patchwork of machines with a collective sorting accuracy of only 70%. Human sorting can only partially compensate for the difference, leading to inevitable pollution and contamination issues. The Brazilian municipality want to replace the current system with a single conveyor belt that sorts waste efficiently from 12 different categories into bins with a series of robots.</p>
    <p class="normal">They purchased the conveyor belt, industrial robots, and cameras. Then, they paid an AI consultancy company to develop a model to classify the recyclables. Still, they wanted models of different sizes because they weren’t sure how quickly these would run on the hardware they had. </p>
    <p class="normal">As requested, the consultancy returned with models of various sizes between 4 and 64 million parameters. The largest model (b7) is over six times slower than the smallest one (b0). Still, the largest model has a significantly higher validation F1 score at 96% (F1 val), as opposed to approximately 90% for the smallest one:</p>
    <figure class="mediaobject"> <img src="../Images/B18406_07_01.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.1: F1 scores for models delivered by the AI consultancy company</p>
    <p class="normal">The municipal leadership was delighted with the results but also surprised because the consultants asked for no domain knowledge or data to train the models, which made them very skeptical. They asked their recycling plant workers to test the models with a batch of recyclables. They got a 25% misclassification rate with that one batch.</p>
    <p class="normal">To seek a second opinion and an honest evaluation of the model, the municipality has approached another AI consultancy firm – yours! </p>
    <p class="normal">The first order of business was to assemble a test dataset that was more realistic of the edge cases that the recycling plant workers found among the misclassifications. Your colleague obtained F1 scores with the test dataset between 62% and 66% (F1 test). Next, they have asked you to understand what’s causing those misclassifications.</p>
    <h1 id="_idParaDest-186" class="heading-1">The approach</h1>
    <p class="normal">No single interpretation method is perfect, and even the best scenario can only tell you one part of the story. Therefore, you have decided to, first, assess the model’s predictive performance using traditional interpretation methods, including the following:</p>
    <ul>
      <li class="bulletList">ROC curves and ROC-AUC</li>
      <li class="bulletList">Confusion matrices and some metrics derived from them, such as accuracy, precision, recall, and F1</li>
    </ul>
    <p class="normal">Then, you’ll examine the model using an activation-based method:</p>
    <ul>
      <li class="bulletList">Intermediate activation</li>
    </ul>
    <p class="normal">This is followed by evaluating decisions with three gradient-based methods:</p>
    <ul>
      <li class="bulletList">Saliency maps</li>
      <li class="bulletList">Grad-CAM</li>
      <li class="bulletList">Integrated gradients</li>
    </ul>
    <p class="normal">And a backpropagation-based method:</p>
    <ul>
      <li class="bulletList">DeepLIFT</li>
    </ul>
    <p class="normal">This is followed by three perturbation-based methods:</p>
    <ul>
      <li class="bulletList">Occlusion sensitivity</li>
      <li class="bulletList">Feature ablation</li>
      <li class="bulletList">Shapley value sampling</li>
    </ul>
    <p class="normal">I hope that you understand why the model is not performing as it should and how to fix it by the end of this process. You can also leverage the many plots and visualizations you will produce to communicate this story to the municipality’s executives.</p>
    <h1 id="_idParaDest-187" class="heading-1">Preparations</h1>
    <p class="normal">You will find <a id="_idIndexMarker683"/>most of the code for this example here: <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/07/GarbageClassifier.ipynb"><span class="url">https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/07/GarbageClassifier.ipynb</span></a></p>
    <h2 id="_idParaDest-188" class="heading-2">Loading the libraries</h2>
    <p class="normal">To run <a id="_idIndexMarker684"/>this example, you need to install the following libraries:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">torchvision</code> to load the dataset</li>
      <li class="bulletList"><code class="inlineCode">mldatasets</code>, <code class="inlineCode">pandas</code>, <code class="inlineCode">numpy</code>, and <code class="inlineCode">sklearn</code> (scikit-learn) to manipulate the dataset</li>
      <li class="bulletList"><code class="inlineCode">torch</code>, <code class="inlineCode">pytorch-lightning</code>, <code class="inlineCode">efficientnet-pytorch</code>, and <code class="inlineCode">torchinfo</code> to predict with the models and show info about the models</li>
      <li class="bulletList"><code class="inlineCode">matplotlib</code>, <code class="inlineCode">seaborn</code>, <code class="inlineCode">cv2</code>, <code class="inlineCode">tqdm</code>, and <code class="inlineCode">captum</code> to make and visualize the interpretations</li>
    </ul>
    <p class="normal">You should load all of them first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> os, gc
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> mldatasets
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> OneHotEncoder
<span class="hljs-keyword">import</span> torchvision
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> pytorch_lightning <span class="hljs-keyword">as</span> pl
<span class="hljs-keyword">import</span> efficientnet_pytorch
<span class="hljs-keyword">from</span> torchinfo <span class="hljs-keyword">import</span> summary
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> matplotlib.cm <span class="hljs-keyword">import</span> ScalarMappable 
<span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> LinearSegmentedColormap
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">import</span> cv2
<span class="hljs-keyword">from</span> tqdm.notebook <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">from</span> captum <span class="hljs-keyword">import</span> attr
</code></pre>
    <p class="normal">Next, we will load and prepare the data.</p>
    <h2 id="_idParaDest-189" class="heading-2">Understanding and preparing the data</h2>
    <p class="normal">The<a id="_idIndexMarker685"/> data used to train the model is publicly available at Kaggle (<a href="https://www.kaggle.com/datasets/mostafaabla/garbage-classification"><span class="url">https://www.kaggle.com/datasets/mostafaabla/garbage-classification</span></a>). It’s called “Garbage Classification” and is a compilation of several different online sources, including web scraping. It has already been split into training and test datasets and also comes with an additional smaller test dataset taken from Wikimedia Commons that your colleague used to test the models. These test images come in a slightly higher resolution too.</p>
    <p class="normal">We <a id="_idIndexMarker686"/>download the data from a ZIP file like this: </p>
    <pre class="programlisting code"><code class="hljs-code">dataset_file = <span class="hljs-string">"</span><span class="hljs-string">garbage_dataset_sample"</span>
dataset_url = <span class="hljs-string">f"https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/raw/main/datasets/</span><span class="hljs-subst">{dataset_file}</span><span class="hljs-string">.zip"</span>
torchvision.datasets.utils.download_url(dataset_url, <span class="hljs-string">"."</span>)
torchvision.datasets.utils.extract_archive(<span class="hljs-string">f"</span><span class="hljs-subst">{dataset_file}</span><span class="hljs-string">.zip"</span>,\
                                           remove_finished=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">It will also extract the ZIP file into four folders corresponding to the three datasets and the larger resolution test dataset. Please note that <code class="inlineCode">garbage_dataset_sample</code> has only a fraction of the training and validation datasets. If you want to download the full datasets, then use <code class="inlineCode">dataset_file = "garbage_dataset"</code>. It won’t impact the size of the test dataset either way. Next, we can initialize the transformation and loading of the datasets like this:</p>
    <pre class="programlisting code"><code class="hljs-code">X_train, norm_mean = (<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>)
norm_std  = (<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>)
transform = torchvision.transforms.Compose(
    [
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(norm_mean, norm_std),
    ]
)
train_data = torchvision.datasets.ImageFolder(
    <span class="hljs-string">f"</span><span class="hljs-subst">{dataset_file}</span><span class="hljs-string">/train"</span>, transform
)
val_data = torchvision.datasets.ImageFolder(
    <span class="hljs-string">f"</span><span class="hljs-subst">{dataset_file}</span><span class="hljs-string">/validation"</span>, transform
)
test_data = torchvision.datasets.ImageFolder(
    <span class="hljs-string">f"</span><span class="hljs-subst">{dataset_file}</span><span class="hljs-string">/test"</span>, transform
)
test_400_data = torchvision.datasets.ImageFolder(
    <span class="hljs-string">f"</span><span class="hljs-subst">{dataset_file}</span><span class="hljs-string">/test_400"</span>, transform
)
</code></pre>
    <p class="normal">What<a id="_idIndexMarker687"/> the above code does is compose a series of standard transforms such as normalization and converting images to tensors. Then, it instantiates PyTorch datasets corresponding to each folder – that is, one for the training, validation, and test datasets, as well as the larger resolution test dataset (<code class="inlineCode">test_400_data</code>). These datasets also include transforms. That way, each time an image is loaded from one of the datasets, it is automatically transformed. We can verify that the shapes of the datasets match our expectations with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">f"# Training Samples:    \t</span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(train_data)}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"# Validation Samples:  \t</span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(val_data)}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"# Test Samples:        \t</span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(test_data)}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Sample Dimension:      \t</span><span class="hljs-subst">{test_data[</span><span class="hljs-number">0</span><span class="hljs-subst">][</span><span class="hljs-number">0</span><span class="hljs-subst">].shape}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"="</span>*<span class="hljs-number">50</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"# Test 400 Samples:    \t</span><span class="hljs-subst">{</span><span class="hljs-built_in">len</span><span class="hljs-subst">(test_400_data)}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"# 400 Sample Dimension:\t</span><span class="hljs-subst">{test_400_data[</span><span class="hljs-number">0</span><span class="hljs-subst">][</span><span class="hljs-number">0</span><span class="hljs-subst">].shape}</span><span class="hljs-string">"</span>)
</code></pre>
    <p class="normal">The preceding code outputs the number of images in each dataset and the dimensions of the images in the datasets. You can tell that there are over 3,700 training images, 900 validation images, and 120 test images of 3 x 224 x 224 dimensions. The first number corresponds to the channels (red, green, and blue) and the following two to the width and height in pixels, which is what the model uses for inference. The Test 400 dataset is the same as the Test dataset, the except images have a larger height and width. We won’t need the Test 400 dataset for inference, so it’s Okay that it doesn’t meet the model’s dimension requirements:</p>
    <pre class="programlisting con"><code class="hljs-con"># Training Samples:        3724
# Validation Samples:      931
# Test Samples:            120
Sample Dimension:          torch.Size([3, 224, 224])
==================================================
# Test 400 Samples:        120 
# 400 Sample Dimension:    torch.Size([3, 400, 400])
</code></pre>
    <h3 id="_idParaDest-190" class="heading-3">Data preparation</h3>
    <p class="normal">If you <a id="_idIndexMarker688"/>print<code class="inlineCode">(test_data[0])</code>, you’ll notice that it will first output a tensor with the image and then a single integer, which we call a scalar. This integer is a number between 0 and 11, which corresponds to the labels used. For quick reference, these are the 12 labels:</p>
    <pre class="programlisting code"><code class="hljs-code">labels_l = [<span class="hljs-string">'battery'</span>, <span class="hljs-string">'biological'</span>, <span class="hljs-string">'</span><span class="hljs-string">brown-glass'</span>, <span class="hljs-string">'cardboard'</span>,\
            <span class="hljs-string">'clothes'</span>, <span class="hljs-string">'green-glass'</span>, <span class="hljs-string">'metal'</span>, <span class="hljs-string">'paper'</span>, <span class="hljs-string">'plastic'</span>,\
            <span class="hljs-string">'shoes'</span>, <span class="hljs-string">'trash'</span>, <span class="hljs-string">'white-glass'</span>]
</code></pre>
    <p class="normal">Interpreting often involves taking single samples and extracting them from the dataset to later perform inference with the model. To that end, it’s important to get familiar with extracting any image from the dataset, say the very first sample from the test dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">tensor, label = test_400_data[<span class="hljs-number">0</span>]
img = mldatasets.tensor_to_img(tensor, norm_std, norm_mean)
plt.figure(figsize=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))
plt.title(labels_l[label], fontsize=<span class="hljs-number">16</span>)
plt.imshow(img)
plt.show()
</code></pre>
    <p class="normal">In the <a id="_idIndexMarker689"/>preceding snippet, we are taking the first sample (<code class="inlineCode">0</code>) from the higher resolution version of the test dataset (<code class="inlineCode">test_400_data</code>) and extracting the <code class="inlineCode">tensor</code> and <code class="inlineCode">label</code> portion from it. Then, we are using the convenience function <code class="inlineCode">tensor_to_img</code> to convert the PyTorch tensor to a <code class="inlineCode">numpy</code> array but also reversing the standardization that had been previously performed on the tensor. Then, we plot the image with <code class="inlineCode">matplotlib</code>'s <code class="inlineCode">imshow</code> and use the <code class="inlineCode">labels_l</code> list to convert the label into a string, which we print in the title. The result can be seen in <em class="italic">Figure 7.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_02.png" alt="A picture containing diagram  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.2: A test sample for a recyclable alkaline battery</p>
    <p class="normal">Another <a id="_idIndexMarker690"/>preprocessing step we will need to perform is the <strong class="keyWord">One-Hot Encoding</strong> (<strong class="keyWord">OHE</strong>) of the <code class="inlineCode">y</code> labels because we will need the OHE form to evaluate the model’s predictive performance. Once we initialize the <code class="inlineCode">OneHotEncoder</code>, we will need to <code class="inlineCode">fit</code> it to the test labels (<code class="inlineCode">y_test</code>) in array format. But first, we will need to put the test labels into a list (<code class="inlineCode">y_test</code>). We can do the same with the validation labels because these will also be useful for easy evaluation:</p>
    <pre class="programlisting code"><code class="hljs-code">y_test = np.array([l <span class="hljs-keyword">for</span> _, l <span class="hljs-keyword">in</span> test_data])
y_val = np.array([l <span class="hljs-keyword">for</span> _, l <span class="hljs-keyword">in</span> val_data])
ohe = OneHotEncoder(sparse=<span class="hljs-literal">False</span>).\
              fit(np.array(y_test).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">Also, for the sake of reproducibility, always initialize your random seeds like this:</p>
    <pre class="programlisting code"><code class="hljs-code">rand = <span class="hljs-number">42</span>
os.environ[<span class="hljs-string">'PYTHONHASHSEED'</span>]=<span class="hljs-built_in">str</span>(rand)
np.random.seed(rand)
random.seed(rand)
device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)
<span class="hljs-keyword">if</span> device == <span class="hljs-string">'cuda'</span>:
    torch.cuda.manual_seed(rand)
<span class="hljs-keyword">else</span>:
    torch.manual_seed(rand)
</code></pre>
    <p class="normal">It is acknowledged that determinism is very difficult with deep learning and is often session-, platform-, and architecture-dependent. If you are using an <strong class="keyWord">NVIDIA GPU</strong>, you can attempt to use PyTorch to avoid nondeterministic algorithms with the command <code class="inlineCode">torch.use_deterministic_algorithms(True)</code>. It’s not a guarantee, but it will throw an error when the operation that you are attempting can’t be accomplished deterministically. If it succeeds, it will be much slower. It’s only worth it if you need to make model outcomes identical – for instance, for scientific research or regulatory compliance. For further details about reproducibility and PyTorch, look here: <a href="https://pytorch.org/docs/stable/notes/randomness.html"><span class="url">https://pytorch.org/docs/stable/notes/randomness.html</span></a>.</p>
    <h3 id="_idParaDest-191" class="heading-3">Inspect data</h3>
    <p class="normal">Now, let’s take <a id="_idIndexMarker691"/>a peek at what images are in our datasets. We know that the training and validation datasets are very similar, so we will start with the validation dataset. We can iterate every class in <code class="inlineCode">labels_l</code> and randomly select a single one from the validation dataset with <code class="inlineCode">np.random.choice</code>. We place each image on a 4 × 3 grid with the class label above it:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.subplots(figsize=(<span class="hljs-number">14</span>,<span class="hljs-number">10</span>))
<span class="hljs-keyword">for</span> c, category <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels_l):
    plt.subplot(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, c+<span class="hljs-number">1</span>)
    plt.title(labels_l[c], fontsize=<span class="hljs-number">12</span>)
    idx = np.random.choice(np.where(y_test==c)[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
    im = mldatasets.tensor_to_img(test_data[idx][<span class="hljs-number">0</span>], norm_std,\
                                  norm_mean)
    plt.imshow(im, interpolation=<span class="hljs-string">'spline16'</span>)
    plt.axis(<span class="hljs-string">"off"</span>)
plt.show()
</code></pre>
    <p class="normal">The preceding code generates <em class="italic">Figure 7.3</em>. You can tell that there is significant pixelation around the edges of the items; some items appear much darker than others, and some of the pictures are from odd angles:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_03.png" alt="Graphical user interface  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 7.3: A random sample of the validation dataset</p>
    <p class="normal">Let’s now do<a id="_idIndexMarker692"/> the same for the test dataset to compare it to the validation/training datasets. We can use the same code as before, except we replace <code class="inlineCode">y_val</code> with <code class="inlineCode">y_test</code>, and <code class="inlineCode">val_data</code> with <code class="inlineCode">test_data.</code> The resulting code generates <em class="italic">Figure 7.4</em>. You can tell that the test set has less pixelated and more consistently lit items, mostly from the top- and side-facing angles:</p>
    <figure class="mediaobject"> <img src="../Images/B18406_07_04.png" alt="A picture containing text, different, various  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.4: A random sample of the test dataset</p>
    <p class="normal">We won’t need to<a id="_idIndexMarker693"/> train a CNN in this chapter. Thankfully, it has been provided to us by the client.</p>
    <h3 id="_idParaDest-192" class="heading-3">The CNN models</h3>
    <p class="normal">The<a id="_idIndexMarker694"/> models trained by the other consultancy company are fine-tuned EfficientNet models. In other words, the AI consultancy company took a previously trained model with the EfficientNet architecture and trained it further with the garbage classification dataset. This technique is called <strong class="keyWord">transfer learning</strong> because it allows a model to utilize previously learned knowledge from a large dataset (in this case, a million images from the ImageNet database) and apply it to new tasks with smaller datasets. The advantage is it significantly reduces training time and computational resources while maintaining high performance because it has already learned to extract useful features from images, which can be a valuable starting point for a new task and only needs to adapt to the specific task at hand.</p>
    <p class="normal">It makes sense<a id="_idIndexMarker695"/> that they chose EfficientNet. After all, EfficientNet is a family of CNNs introduced by Google AI researchers in 2019. The key innovation of EfficientNet is its compound scaling method, which enables the model to achieve higher accuracy and efficiency than other CNNs. In addition, it is based on the observation that different dimensions of the model, such as width, depth, and resolution, contribute to the overall performance in a balanced way. The EfficientNet architecture is built upon a baseline model called EfficientNet-B0. A compound scaling method is employed to create larger and more powerful versions of the baseline model, which simultaneously scales up the width, depth, and resolution of the network. This results in a series of models, EfficientNet-B1 to EfficientNet-B7, with increasing capacity and performance. The largest model, EfficientNet-B7, has achieved state-of-the-art performance on several benchmarks, such as ImageNet.</p>
    <h3 id="_idParaDest-193" class="heading-3">Load the CNN model</h3>
    <p class="normal">Before we can load the model, we must define the class for EfficientLite – a class that inherits from PyTorch Lightning’s <code class="inlineCode">pl.LightningModule</code>. This class is designed to create a custom model based on the EfficientNet architecture, train it, and perform inference. We only need it for the latter, which is why we have also adapted it to include a <code class="inlineCode">predict()</code> function – much like scikit-learn models do for the convenience of being able to use similar evaluation functions to would with these models:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="code-highlight"><strong class="hljs-title-slc">EfficientLite</strong></span>(pl.LightningModule):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, lr: </span><span class="hljs-built_in">float</span><span class="hljs-params">, num_class: </span><span class="hljs-built_in">int</span><span class="hljs-params">,\</span>
<span class="hljs-params">                 pretrained=</span><span class="hljs-string">"efficientnet-b0"</span><span class="hljs-params">, *args, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__()
        self.save_hyperparameters()
        self.model = efficientnet_pytorch.EfficientNet.\
                                      from_pretrained(pretrained)
        in_features = self.model._fc.in_features
        self.model._fc = torch.nn.Linear(in_features, num_class)
    <span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):
        <span class="hljs-keyword">return</span> self.model(x)
    <span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self, dataset</span>):
        self.model.<span class="hljs-built_in">eval</span>()
        device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available()\
                              <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)
        <span class="hljs-keyword">with</span> torch.no_grad():
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(dataset, np.ndarray):
                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(dataset.shape) == <span class="hljs-number">3</span>:
                    dataset = np.expand_dims(dataset, axis=<span class="hljs-number">0</span>)
                dataset = [(x,<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> dataset]
            loader = torch.utils.data.DataLoader(dataset,\
                                                 batch_size=<span class="hljs-number">32</span>)
            probs = <span class="hljs-literal">None</span>
        <span class="hljs-keyword">for</span> X_batch, _ <span class="hljs-keyword">in</span> tqdm(loader):
            X_batch = X_batch.to(device, dtype=torch.float32)
            logits_batch =  self.model(X_batch)
            probs_batch = torch.nn.functional.softmax(logits_batch,\
                                       dim=<span class="hljs-number">1</span>).cpu().detach().numpy()
            <span class="hljs-keyword">if</span> probs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                probs = np.concatenate((probs, probs_batch))
            <span class="hljs-keyword">else</span>:
                probs = probs_batch
            clear_gpu_cache()
        <span class="hljs-keyword">return</span> probs
</code></pre>
    <p class="normal">You will notice <a id="_idIndexMarker696"/>that the class has three functions:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">__init__</code>: This is the constructor for the <code class="inlineCode">EfficientLite</code> class. It initializes the model by loading a pretrained EfficientNet model using the <code class="inlineCode">efficientnet_pytorch.EfficientNet.from_pretrained()</code> method. It then replaces the last fully connected layer (<code class="inlineCode">_fc</code>) with a new <code class="inlineCode">torch.nn.Linear</code> layer that has the same number of input features but a different number of output features equal to the number of classes (<code class="inlineCode">num_class</code>).</li>
      <li class="bulletList"><code class="inlineCode">forward</code>: This method defines the forward pass of the model. It takes an input tensor <code class="inlineCode">x</code> and passes it through the model, returning the output.</li>
      <li class="bulletList"><code class="inlineCode">predict</code>: This method takes a dataset and performs inference using the trained model. It first sets the model to evaluation mode (<code class="inlineCode">self.model.eval()</code>). The input dataset is converted into a DataLoader object with a batch size of 32. The method iterates over the DataLoader, processing each batch of data and computing probabilities using the softmax function. The <code class="inlineCode">clear_gpu_cache()</code> function is called after each iteration to release unused GPU memory. Finally, the method returns the computed probabilities as a <code class="inlineCode">numpy</code> array.</li>
    </ul>
    <p class="normal">If you are using a CUDA-enabled GPU, there’s a utility function called <code class="inlineCode">clear_gpu_cache()</code>, which is run every time there’s a GPU-intensive operation. Depending on how powerful your GPU is, you may need to run it more often. Feel free to use another convenience function, <code class="inlineCode">print_gpu_mem_used()</code>, to check how much GPU memory is utilized at any given moment or to print the entire summary with <code class="inlineCode">print(torch.cuda.memory_summary())</code>. The next code downloads the pre-trained EfficientNet <a id="_idIndexMarker697"/>model, loads the model weights to EfficientLite, and prepares the model for inference. Lastly, it prints a summary:</p>
    <pre class="programlisting code"><code class="hljs-code">model_weights_file = <span class="code-highlight"><strong class="hljs-string-slc">"garbage-finetuned-efficientnet-b4"</strong></span>
model_url = <span class="hljs-string">f"https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/raw/main/models/</span><span class="hljs-subst">{model_weights_file}</span><span class="hljs-string">.ckpt"</span>
torchvision.datasets.utils.download_url(model_url, <span class="hljs-string">"."</span>)
garbage_mdl = EfficientLite.load_from_checkpoint(
    <span class="hljs-string">f"</span><span class="hljs-subst">{model_weights_file}</span><span class="hljs-string">.ckpt"</span>
)
garbage_mdl = garbage_mdl.to(device).<span class="hljs-built_in">eval</span>()
<span class="hljs-built_in">print</span>(summary(garbage_mdl))
</code></pre>
    <p class="normal">The code is pretty straightforward but what’s important to note is that we are choosing the b4 model for this chapter, which is in between b0 and b7 in terms of size, speed, and accuracy. You can change the last digit according to your hardware’s abilities, but it might change some of the outcomes of this chapter’s code. The preceding snippet outputs the following summary:</p>
    <pre class="programlisting con"><code class="hljs-con">=======================================================================<br/>Layer (type:depth-idx) Param # =======================================================================<br/>EfficientLite -- 
├─EfficientNet: 1-1 – 
│ └─Conv2dStaticSamePadding: 2-1 1,296 
│ │ └─ZeroPad2d: 3-1 – 
│ └─BatchNorm2d: 2-2 96 
│ └─ModuleList: 2-3 – 
│ │ └─MBConvBlock: 3-2 2,940 
│ │ └─MBConvBlock: 3-3 1,206 
│ │ └─MBConvBlock: 3-4 11,878 
│ │ └─MBConvBlock: 3-5 18,120 
│ │ └─MBConvBlock: 3-6 18,120 
│ │ └─MBConvBlock: 3-7 18,120 
│ │ └─MBConvBlock: 3-8 25,848 
│ │ └─MBConvBlock: 3-9 57,246 
│ │ └─MBConvBlock: 3-10 57,246 
│ │ └─MBConvBlock: 3-11 57,246 
│ │ └─MBConvBlock: 3-12 70,798 
│ │ └─MBConvBlock: 3-13 197,820 
│ │ └─MBConvBlock: 3-14 197,820 
│ │ └─MBConvBlock: 3-15 197,820 
│ │ └─MBConvBlock: 3-16 197,820 
│ │ └─MBConvBlock: 3-17 197,820 
│ │ └─MBConvBlock: 3-18 240,924 
│ │ └─MBConvBlock: 3-19 413,160 
│ │ └─MBConvBlock: 3-20 413,160 
│ │ └─MBConvBlock: 3-21 413,160 
│ │ └─MBConvBlock: 3-22 413,160 
│ │ └─MBConvBlock: 3-23 413,160 
│ │ └─MBConvBlock: 3-24 520,904 
│ │ └─MBConvBlock: 3-25 1,159,332 
│ │ └─MBConvBlock: 3-26 1,159,332 
│ │ └─MBConvBlock: 3-27 1,159,332 
│ │ └─MBConvBlock: 3-28 1,159,332 
│ │ └─MBConvBlock: 3-29 1,159,332 
│ │ └─MBConvBlock: 3-30 1,159,332 
│ │ └─MBConvBlock: 3-31 1,159,332 
│ │ └─MBConvBlock: 3-32 1,420,804 
│ │ └─MBConvBlock: 3-33 3,049,200 
│ └─Conv2dStaticSamePadding: 2-4 802,816 
│ │ └─Identity: 3-34 – 
│ └─BatchNorm2d: 2-5 3,584 
│ └─AdaptiveAvgPool2d: 2-6 – 
│ └─Dropout: 2-7 – 
│ └─Linear: 2-8 21,516 
│ └─MemoryEfficientSwish: 2-9 
======================================================================
Total params: 17,570,132 
Trainable params: 17,570,132 
Non-trainable params: 0 ======================================================================
</code></pre>
    <p class="normal">It has<a id="_idIndexMarker698"/> pretty much everything we need to know about the model. It has two custom convolutional layers (<code class="inlineCode">Conv2dStaticSamePadding</code>), each followed by a batch normalization layer (<code class="inlineCode">BatchNorm2d</code>) and 32 <code class="inlineCode">MBConvBlock</code> modules.</p>
    <p class="normal">The network also has a memory-efficient implementation of the Swish activation function (<code class="inlineCode">MemoryEfficientSwish</code>), which, like all activation functions, introduces non-linearity into the model. It’s smooth and non-monotonic, which helps it converge more quickly while learning more complex and nuanced patterns. It also has a global average pooling operation (<code class="inlineCode">AdaptiveAvgPool2d</code>), which reduces the spatial dimensions of the feature maps. It then has a first <code class="inlineCode">Dropout</code> layer for regularization, followed by a fully connected layer (<code class="inlineCode">Linear</code>) that takes it from 1792 nodes to 12. Dropout prevents overfitting by making a fraction of the neurons inactive in each update cycle. If you want to see more <a id="_idIndexMarker699"/>details of how the output shape of each layer gets reduced between one layer and another, enter the <code class="inlineCode">input_size</code> into the summary – like <code class="inlineCode">summary(garbage_mdl, input_size=(64, 3, 224, 224))</code> – because the network was designed with a batch size of 64 in mind. Don’t worry if none of these terms sound familiar to you. We will revisit them later.</p>
    <h2 id="_idParaDest-194" class="heading-2">Assessing the CNN classifier with traditional interpretation methods</h2>
    <p class="normal">We will first <a id="_idIndexMarker700"/>evaluate the model using the<a id="_idIndexMarker701"/> validation dataset with the <code class="inlineCode">evaluate_multiclass_mdl</code> function. The arguments include the model (<code class="inlineCode">garbage_mdl</code>), our validation data (<code class="inlineCode">val</code>_<code class="inlineCode">data</code>), as well as the class names (<code class="inlineCode">labels_l</code>) and the encoder (<code class="inlineCode">ohe</code>). Lastly, we won’t plot the ROC curves (<code class="inlineCode">plot_roc=False</code>). This function returns the predicted labels and probabilities, which we can store in variables for later use:</p>
    <pre class="programlisting code"><code class="hljs-code">y_val_pred, y_val_prob = mldatasets.evaluate_multiclass_mdl(
    garbage_mdl, val_data,\
    class_l=labels_l, ohe=ohe, plot_roc=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">The preceding code generates both <em class="italic">Figure 7.5</em> with a confusion matrix and <em class="italic">Figure 7.6</em> with performance metrics for each class:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_05.png" alt="Graphical user interface, text, application, email  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.5: The confusion matrix for the validation dataset</p>
    <p class="normal">Even <a id="_idIndexMarker702"/>though <a id="_idIndexMarker703"/>the confusion matrix in <em class="italic">Figure 7.5</em> seems to suggest a perfect classification, once you see the precision and recall breakdown in <em class="italic">Figure 7.6</em>, you can tell that the model had issues with metal, plastic, and white glass:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_06.png" alt="A picture containing text, receipt  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.6: The classification report for the validation dataset</p>
    <p class="normal">You can<a id="_idIndexMarker704"/> expect a model to always reach <code class="inlineCode">100%</code> training<a id="_idIndexMarker705"/> accuracy if you train it for enough epochs using optimal hyperparameters. A near-perfect validation accuracy is harder to achieve, depending on how different these two are. We know that the validation dataset is simply a sample of images from the same collection, so it’s not particularly surprising that 94.7% was achieved.</p>
    <p class="normal">Now, let’s repeat the same code snippet for the test dataset. This time, we want to see the ROC curves (<code class="inlineCode">plot_roc=True</code>) but only the averages, and not on a class-by-class basis (<code class="inlineCode">plot_roc_class=False</code>) because there are only four pictures per class. Given the small number of samples, we can display the numbers in the confusion matrix rather than percentages (<code class="inlineCode">pct_matrix=False</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">y_test_pred, y_test_prob = mldatasets.evaluate_multiclass_mdl(
    garbage_mdl, test_data,\
    class_l=labels_l, ohe=ohe,\
    plot_roc=<span class="hljs-literal">True</span>, plot_roc_class=<span class="hljs-literal">False</span>, pct_matrix=<span class="hljs-literal">False</span>
)
</code></pre>
    <p class="normal">The preceding code snippet <a id="_idIndexMarker706"/>generated the ROC curve in <em class="italic">Figure 7.7</em>, the confusion matrix in <em class="italic">Figure 7.8</em>, and <a id="_idIndexMarker707"/>the classification report in <em class="italic">Figure 7.9</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_07.png" alt="Chart, line chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.7: The ROC curve for the test dataset</p>
    <p class="normal">The test ROC plot (<em class="italic">Figure 7.7</em>) shows the macro-average and micro-average ROC curves. The difference in both of these is in how they are calculated. Macro metrics are computed<a id="_idIndexMarker708"/> for each class independently<a id="_idIndexMarker709"/> and then averaged, treating each differently, whereas micro-averages factor in the contribution or representation of each class; generally, micro-averages are more reliable.</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_08.png" alt="Chart, scatter chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.8: The confusion matrix for the test dataset</p>
    <p class="normal">If we take a look at the confusion matrix in <em class="italic">Figure 7.8</em>, we can tell that only biological, green glass, and shoes are getting 10-out-of-10 classifications. However, a lot of items are being misclassified as biologicals and shoes. On the other hand, many items are more often than not<a id="_idIndexMarker710"/> misclassified, such as metal, paper, and plastic. Many <a id="_idIndexMarker711"/>of them are similar in shape or color, so you could understand how that would happen, but how does a piece of metal get confused with white glass, or paper with a battery?</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_09.png" alt="Table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.9: The predictive performance metrics for the test dataset</p>
    <p class="normal">When classification models are discussed in a business setting, stakeholders are often only interested in one number: accuracy. It’s easy to let this drive the discussion, but there’s much more nuance to it. For instance, the disappointing test accuracy (68.3%) could mean many things. It could mean that six classes are getting perfect classification, and all others are not, or that 12 classes are getting only half misclassified. There are many possibilities of what could be going on.</p>
    <p class="normal">In any case, when dealing with a multiclass classification problem, even an accuracy below 50% might not be as bad as it seems. Consider that the <strong class="keyWord">no information rate</strong> represents the accuracy that can be achieved by a naive model that always predicts the most frequent class in the dataset. It serves as a benchmark to ensure that the developed model is providing<a id="_idIndexMarker712"/> insights beyond this simplistic approach. And <a id="_idIndexMarker713"/>with 12 evenly split, the <strong class="keyWord">no information rate</strong> is likely to be around 8.33% (100%/12 classes), so 68% is still orders of magnitude higher than that. In fact, there is less of a leap to 100%! To a machine learning practitioner, this means that if we judge solely based on test accuracy results, the model is still learning something of value that can be improved upon.</p>
    <p class="normal">In any case, the predictive performance metrics in <em class="italic">Figure 7.9</em> for the test dataset are consistent with what we saw in the confusion matrix. Biological gets high recall but low precision and metal, paper, plastic, and trash are low for both.</p>
    <h3 id="_idParaDest-195" class="heading-3">Determining what misclassifications to focus on</h3>
    <p class="normal">We have<a id="_idIndexMarker714"/> already noticed some exciting misclassifications we can focus on:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Metal false positives</strong>: 16 out <a id="_idIndexMarker715"/>of the 120 samples in the test dataset were misclassified as metal. That’s 42% of all misclassifications! What is it about metal that renders it so easily confused with other garbage according to the model?</li>
      <li class="bulletList"><strong class="keyWord">Plastic false negatives</strong>: 70% of <a id="_idIndexMarker716"/>all true plastic samples were misclassified. Thus, plastics had the lowest recall of any material besides trash. It’s easy to tell why trash would be so difficult to classify because it’s exceedingly diverse but not plastic.</li>
    </ul>
    <p class="normal">We should also examine some true positives to contrast these misclassifications. Namely, batteries because they get many false positives as metals and plastic, and white glass because it gets false negatives as metals 30% of the time. Since there are so many metal false positives, we should narrow them down to just those that are for batteries.</p>
    <p class="normal">To visualize the tasks ahead, we can create a DataFrame (<code class="inlineCode">preds_df</code>) with the true labels (<code class="inlineCode">y_true</code>) in one column and predicted labels in another (<code class="inlineCode">y_pred</code>). And to understand how certain the models are of these predictions, we can create another DataFrame with the probabilities (<code class="inlineCode">probs_df</code>). We can generate column totals for these probabilities to sort the columns according to which category the model is most certain about across all samples. Then, we can concatenate our predictions DataFrame with the first 12 columns from our probabilities DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code">preds_df = pd.DataFrame({<span class="hljs-string">'y_true'</span>:[labels_l[o] <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> y_test],\
                         <span class="hljs-string">'y_pred'</span>:y_test_pred})
probs_df = pd.DataFrame(y_test_prob*<span class="hljs-number">100</span>).<span class="hljs-built_in">round</span>(<span class="hljs-number">1</span>)
probs_df.loc[<span class="hljs-string">'</span><span class="hljs-string">Total'</span>]= probs_df.<span class="hljs-built_in">sum</span>().<span class="hljs-built_in">round</span>(<span class="hljs-number">1</span>)
probs_df.columns = labels_l
probs_df = probs_df.sort_values(<span class="hljs-string">'Total'</span>, axis=<span class="hljs-number">1</span>, ascending=<span class="hljs-literal">False</span>)
probs_df.drop([<span class="hljs-string">'Total'</span>], axis=<span class="hljs-number">0</span>, inplace=<span class="hljs-literal">True</span>)
probs_final_df = probs_df.iloc[:,<span class="hljs-number">0</span>:<span class="hljs-number">12</span>]
preds_probs_df = pd.concat([preds_df, probs_final_df], axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Let’s now output <a id="_idIndexMarker717"/>the DataFrame with color coding for the prediction instances we are interested in assessing. On one hand, we have the metal false positives and, on the other, the plastic false negatives. But we also have the true positives for battery and white glass. Lastly, we have bolded all probabilities over 50% and hidden all probabilities of 0% so that it’s easier to spot any predictions with high probabilities:</p>
    <pre class="programlisting code"><code class="hljs-code">num_cols_l = <span class="hljs-built_in">list</span>(preds_probs_df.columns[<span class="hljs-number">2</span>:])
num_fmt_dict = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(num_cols_l, [<span class="hljs-string">"{:,.1f}%"</span>]*<span class="hljs-built_in">len</span>(num_cols_l)))
preds_probs_df[
    (preds_probs_df.y_true!=preds_probs_df.y_pred)
    | (preds_probs_df.y_true.isin([<span class="hljs-string">'</span><span class="hljs-string">battery'</span>, <span class="hljs-string">'white-glass'</span>]))
].style.<span class="hljs-built_in">format</span>(num_fmt_dict).apply(
    <span class="hljs-keyword">lambda</span> x: [<span class="hljs-string">'background: lightgreen'</span> <span class="hljs-keyword">if</span> (x[<span class="hljs-number">0</span>] == x[<span class="hljs-number">1</span>])\
                <span class="hljs-keyword">else</span> <span class="hljs-string">''</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x], axis=<span class="hljs-number">1</span>
).apply(
    <span class="hljs-keyword">lambda</span> x: [<span class="hljs-string">'background: orange'</span> <span class="hljs-keyword">if</span> (x[<span class="hljs-number">0</span>] != x[<span class="hljs-number">1</span>] <span class="hljs-keyword">and</span>\
                x[<span class="hljs-number">1</span>] == <span class="hljs-string">'</span><span class="hljs-string">metal'</span> <span class="hljs-keyword">and</span> x[<span class="hljs-number">0</span>] == <span class="hljs-string">'battery'</span>)\
                <span class="hljs-keyword">else</span> <span class="hljs-string">''</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x], axis=<span class="hljs-number">1</span>
).apply(
    <span class="hljs-keyword">lambda</span> x: [<span class="hljs-string">'</span><span class="hljs-string">background: yellow'</span> <span class="hljs-keyword">if</span> (x[<span class="hljs-number">0</span>] != x[<span class="hljs-number">1</span>] <span class="hljs-keyword">and</span>\
                                        x[<span class="hljs-number">0</span>] == <span class="hljs-string">'plastic'</span>)\
                <span class="hljs-keyword">else</span> <span class="hljs-string">''</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x], axis=<span class="hljs-number">1</span>
).apply(
    <span class="hljs-keyword">lambda</span> x: [<span class="hljs-string">'font-weight: bold'</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(i, <span class="hljs-built_in">float</span>)\
                                                 <span class="hljs-keyword">and</span> i &gt;= <span class="hljs-number">50</span>\
                <span class="hljs-keyword">else</span> <span class="hljs-string">''</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x], axis=<span class="hljs-number">1</span>
).apply(
    <span class="hljs-keyword">lambda</span> x: [<span class="hljs-string">'color:transparent'</span> <span class="hljs-keyword">if</span> i == <span class="hljs-number">0.0</span>\
                <span class="hljs-keyword">else</span> <span class="hljs-string">''</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> x], axis=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">The preceding code snippet produces <em class="italic">Figure 7.10</em>. We can tell by the highlights which are the <a id="_idIndexMarker718"/>metal false positives and the plastic false negatives, as well as which would be the true positives: #0-6 for battery, and #110-113 and #117-119 for white glass:</p>
    <p class="packt_figref"><img src="../Images/B18406_07_10.png" alt="Table  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.10: Table with all 38 misclassifications in the test dataset, selected true positives, and their true and predicted labels, as well as their predicted probabilities</p>
    <p class="normal">We can easily store the indexes for these instances in lists with the following code. That way, for <a id="_idIndexMarker719"/>future reference, we can iterate through these lists to assess individual predictions or subset arrays with them to perform interpretation tasks for the entire group. As you can tell, we have lists for all four groups:</p>
    <pre class="programlisting code"><code class="hljs-code">plastic_FN_idxs = preds_df[
    (preds_df[<span class="hljs-string">'y_true'</span>] !=preds_df[<span class="hljs-string">'y_pred'</span>])
    &amp; (preds_df[<span class="hljs-string">'y_true'</span>] == <span class="hljs-string">'plastic'</span>)
].index.to_list()
metal_FP_idxs = preds_df[
    (preds_df[<span class="hljs-string">'y_true'</span>] != preds_df[<span class="hljs-string">'y_pred'</span>])
    &amp; (preds_df[<span class="hljs-string">'y_pred'</span>] == <span class="hljs-string">'metal'</span>)
    &amp; (preds_df[<span class="hljs-string">'y_true'</span>] == <span class="hljs-string">'battery'</span>)
].index.to_list()
battery_TP_idxs = preds_df[
    (preds_df[<span class="hljs-string">'y_true'</span>] ==preds_df[<span class="hljs-string">'y_pred'</span>])
    &amp; (preds_df[<span class="hljs-string">'y_true'</span>] == <span class="hljs-string">'battery'</span>)
].index.to_list()
wglass_TP_idxs = preds_df[
    (preds_df[<span class="hljs-string">'y_true'</span>] == preds_df[<span class="hljs-string">'y_pred'</span>])
    &amp; (preds_df[<span class="hljs-string">'y_true'</span>] == <span class="hljs-string">'white-glass'</span>)
].index.to_list()
</code></pre>
    <p class="normal">Now that we have all our data preprocessed, the model is fully loaded and lists the groups of predictions to debug. Now we can move forward. Let the interpretation begin!</p>
    <h1 id="_idParaDest-196" class="heading-1">Visualizing the learning process with activation-based methods</h1>
    <p class="normal">Before we <a id="_idIndexMarker720"/>get into discussing activations, layers, filters, neurons, gradients, convolutions, kernels, and all the fantastic elements that make up a CNN, let’s first briefly revisit the mechanics of a CNN and one in particular.</p>
    <p class="normal">The convolution layer is the essential building block of a CNN, which is a sequential neural network. It convolves the input <a id="_idIndexMarker721"/>with <strong class="keyWord">learnable filters</strong>, which are relatively small but are applied across the entire width, height, and depth at specific distances or <strong class="keyWord">strides</strong>. Each filter produces a two-dimensional <strong class="keyWord">activation map</strong> (also known as a <strong class="keyWord">feature map</strong>). It’s called an activation map because it denotes positions of activations in the images – in other words, where specific “features” are located. In this context, a feature is an abstract spatial representation that, downstream in the process, is reflected in the learned weights of fully connected (<strong class="keyWord">linear</strong>) layers. For instance, in the garbage CNN case, the first convolutional layer has 48 filters with a 3 × 3 kernel, a 2 × 2 stride, and static padding, which ensure that the output maps maintain the same size as the inputs. Filters are template matching because they end up activating areas of the activation map when certain patterns are found in the input image.</p>
    <p class="normal">But before<a id="_idIndexMarker722"/> we get to our fully connected layers, we have to reduce the dimensions of our filters until they have a workable size. For instance, if we flattened the output of our first convolution (48 × 112 × 112), we would have over 602,000 features. I think we can all agree that that would be too much to feed into a fully connected layer. Even if we used enough neurons to handle this workload, we probably wouldn’t have captured enough spatial representations for the neural network to make sense of the images. For this reason, convolutional layers are often paired with pooling layers, which downsample the input – in other words, they reduce the dimensionality of the data. In this case, there’s an adaptive average pooling layer (<code class="inlineCode">AdaptiveAvgPool2d</code>) that performs an average across all the channels as well as many pooling layers within the <strong class="keyWord">Mobile Inverted Bottleneck Convolution Blocks</strong> (<code class="inlineCode">MBConvBlock</code>).</p>
    <p class="normal">Incidentally, <code class="inlineCode">MBConvBlock</code>, <code class="inlineCode">Conv2dStaticSamePadding</code>, and <code class="inlineCode">BatchNorm2d</code> are the building blocks of the EfficientNet architecture. These components work together to create a highly efficient and accurate convolutional neural network:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">MBConvBlock</code>: Mobile inverted bottleneck convolution blocks that form the core of the EfficientNet architecture. In traditional convolutional layers, filters are applied across all input channels simultaneously, resulting in a high number of computations, but <code class="inlineCode">MBConvBlocks</code> divide the process into two steps: first, they apply depthwise convolutions that handle each input channel separately, and then use pointwise (1 x 1) convolutions to combine the information from different channels. For this reason, inside the <code class="inlineCode">MBConvBlock</code> modules for B0, there are three convolutional layers: a depthwise convolution, a pointwise (1 x 1) convolution (called project convolution), and another pointwise (1 x 1) convolution (called expand convolution) in some blocks. However, the first block only contains two convolutional layers (depthwise and project convolutions) because it doesn’t have an expand convolution. For B4, the architecture is similar except more convolutions are stacked in each block and there are twice as many <code class="inlineCode">MBConvBlocks</code>. Naturally, B7 has many more blocks and convolutional layers. For B4, there are a total of 158 convolutional operations between the 32 <code class="inlineCode">MBConvBlocks</code>.</li>
      <li class="bulletList"><code class="inlineCode">Conv2dStaticSamePadding</code>: Unlike traditional convolutional layers (such as <code class="inlineCode">Conv2d</code>), these don’t reduce the dimensions. It ensures the input and output feature maps have the same spatial dimensions.</li>
      <li class="bulletList"><code class="inlineCode">BatchNorm2d</code>: Batch normalization layers that help stabilize and accelerate training by normalizing the input features, which helps keep the distribution of the input features consistent during training.</li>
    </ul>
    <p class="normal">Once the <a id="_idIndexMarker723"/>over 230 convolutional and pooling operations are performed, we are left with a flattened output of a more workable size: 1,792 features, which the fully connected layer converts into 12, which, leveraging <strong class="keyWord">softmax</strong> activation, outputs probabilities between 0 and 1 for each of the classes. In the garbage CNN, there is a <strong class="keyWord">dropout</strong> layer involved to help regularize the training. We can ignore this entirely because, for inference, they are ignored.</p>
    <p class="normal">If this wasn’t entirely clear, don’t fret! The sections that follow will demonstrate visually through activations, gradients, and perturbations how the network probably learned or did not learn image representations.</p>
    <h2 id="_idParaDest-197" class="heading-2">Intermediate activations</h2>
    <p class="normal">For<a id="_idIndexMarker724"/> inference, the image goes through the network’s input and the prediction comes out through the output traversing every single layer. However, one of the advantages of having a sequential and layered architecture is that we can extract any layer’s output and not just the final layer. The <strong class="keyWord">intermediate activations</strong> are<a id="_idIndexMarker725"/> simply the outputs of any of the convolution or pooling layers. They are activation maps because, after an activation function has been applied, the brighter spots map to the image’s features. In this case, the model used ReLU on all convolutional layers, so that is what activates the spots. We are only interested in the convolutional layers’ intermediate activations because the pooling layers are simply downsampled versions of these ones. Why not see the higher-resolution version instead?</p>
    <p class="normal">As the <a id="_idIndexMarker726"/>filters become smaller in width and height, the learned representations will be larger. In other words, the first convolutional layer may be about details such as texture, the following one about edges, and the last one about shapes. We must then flatten the convolutional layers’ output to feed it to the multilayer perceptron that takes over from then on.</p>
    <p class="normal">What we will do now <a id="_idIndexMarker727"/>is extract activations for some of the convolutional layers. In B4, there are 158, so we can’t do all of them! To this end, we will obtain the first level of layers with <code class="inlineCode">model.children()</code>, and iterate across them. We will append the two <code class="inlineCode">Conv2dStaticSamePadding</code> layers from this top level into a <code class="inlineCode">conv_layers</code> list. But we will also go deeper, appending the first convolutional layer for the first six <code class="inlineCode">MBConvBlock</code> layers in the <code class="inlineCode">ModuleList</code> layers. In the end, we should have eight convolutional layers – the six in the middle belonging to Mobile Inverted Bottleneck Convolution blocks:</p>
    <pre class="programlisting code"><code class="hljs-code">conv_layers = []
model_children = <span class="hljs-built_in">list</span>(garbage_mdl.model.children())
<span class="hljs-keyword">for</span> model_child <span class="hljs-keyword">in</span> model_children:
    <span class="hljs-keyword">if</span> (<span class="hljs-built_in">type</span>(model_child) ==\
                efficientnet_pytorch.utils.Conv2dStaticSamePadding):
        conv_layers.append(model_child)
    <span class="hljs-keyword">elif</span> (<span class="hljs-built_in">type</span>(model_child) == torch.nn.modules.container.ModuleList):
        module_children = <span class="hljs-built_in">list</span>(model_child.children())
        module_convs = []
        <span class="hljs-keyword">for</span> module_child <span class="hljs-keyword">in</span> module_children:
            module_convs.append(<span class="hljs-built_in">list</span>(module_child.children())[<span class="hljs-number">0</span>])
        conv_layers.extend(module_convs[:<span class="hljs-number">6</span>])
<span class="hljs-built_in">print</span>(conv_layers)
</code></pre>
    <p class="normal">Before we iterate across all of them producing activation maps for each convolutional layer, let’s do it for a single filter and layer:</p>
    <pre class="programlisting code"><code class="hljs-code">idx = battery_TP_idxs[<span class="hljs-number">0</span>]
tensor = test_data[idx][<span class="hljs-number">0</span>][<span class="hljs-literal">None</span>, :].to(device)
label = y_test[idx]
method = attr.LayerActivation(garbage_mdl, conv_layers[layer]
attribution = method.attribute(tensor).detach().cpu().numpy()
<span class="hljs-built_in">print</span>(attribution.shape)
</code></pre>
    <p class="normal">The preceding snippet extracts the <code class="inlineCode">tensor</code> for the first battery true positive (<code class="inlineCode">battery_TP_idxs[0]</code>). Then, it initializes the <code class="inlineCode">LayerActivation</code> attribution method with the model (<code class="inlineCode">garbage_mdl</code>) and the first convolutional layer (<code class="inlineCode">conv_layers[0]</code>). Using the <code class="inlineCode">attribute</code> function, it creates an <code class="inlineCode">attribution</code> with this method. For the shape of the attribution, we should get <code class="inlineCode">(1, 48, 112, 112)</code>. The tensor was for a single image, so it makes sense that <a id="_idIndexMarker728"/>the first number is a one. The next number corresponds to the number of filters, followed by the <a id="_idIndexMarker729"/>width and height dimensions of each filter. Regardless of the kind of attribution, the numbers inside each attribution relate to how a pixel in the input is seen by the model. Interpretation varies according to the method. However, generally, it is interpreted that higher numbers mean more of an impact on the outcome, but attributions may also have negative numbers, which mean the opposite.</p>
    <p class="normal">Let’s visualize the first filter, but before we do that, we must decide what colormap to use. A colormap will determine what colors to assign to different numbers as a gradient. For instance, the following colormap has white for <code class="inlineCode">0</code> (<code class="inlineCode">#ffffff</code> in hexadecimal), a medium gray for <code class="inlineCode">0.25</code>, and black (<code class="inlineCode">#000000</code> in hexadecimal) for <code class="inlineCode">1</code> with a gradient between these colors:</p>
    <pre class="programlisting code"><code class="hljs-code">cbinary_cmap = LinearSegmentedColormap.from_list(<span class="hljs-string">'custom binary'</span>,
                                                 [(<span class="hljs-number">0</span>, <span class="hljs-string">'#ffffff'</span>),
                                                  (<span class="hljs-number">0.25</span>, <span class="hljs-string">'#777777'</span>),
                                                  (<span class="hljs-number">1</span>, <span class="hljs-string">'#000000'</span>)])
</code></pre>
    <p class="normal">You can also use any of the named colormaps from <a href="https://matplotlib.org/stable/tutorials/colors/colormaps.html"><span class="url">https://matplotlib.org/stable/tutorials/colors/colormaps.html</span></a>, rather than using your own. Next, let’s plot the attribution for the first filter like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">filter</span> = <span class="hljs-number">0</span>
filter_attr = attribution[<span class="hljs-number">0</span>,<span class="hljs-built_in">filter</span>]
filter_attr = mldatasets.apply_cmap(filter_attr, cbinary_cmap, <span class="hljs-string">'positive'</span>)
y_true = labels_l[label]
y_pred = y_test_pred[idx]
fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">6</span>))
fig.suptitle(<span class="hljs-string">f"Actual label: </span><span class="hljs-subst">{y_true}</span><span class="hljs-string">, Predicted: </span><span class="hljs-subst">{y_pred}</span><span class="hljs-string">"</span>, fontsize=<span class="hljs-number">16</span>)
ax.set_title(
    <span class="hljs-string">f"(</span><span class="hljs-subst">{method.get_name()}</span><span class="hljs-string"> Attribution for Filter #</span><span class="hljs-subst">{</span><span class="hljs-built_in">filter</span><span class="hljs-subst">+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string"> for\</span>
<span class="hljs-string">        Convolutional Layer #</span><span class="hljs-subst">{layer+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string">)"</span>,
    fontsize=<span class="hljs-number">12</span>
)
ax.imshow(filter_attr)
ax.grid(<span class="hljs-literal">False</span>)
fig.colorbar(
    ScalarMappable(norm=<span class="hljs-string">'linear'</span>, cmap=cbinary_cmap),
    ax=ax,
    orientation=<span class="hljs-string">"vertical"</span>
)
plt.show()
</code></pre>
    <p class="normal">The code saves the first filter for the first image from the attribution to <code class="inlineCode">filter_attr</code> and then leverages the <code class="inlineCode">apply_cmap</code> utility function to convert the attribution array to a normalized <a id="_idIndexMarker730"/>colormap, which<a id="_idIndexMarker731"/> is then plotted with <code class="inlineCode">imshow</code>. The rest of the code plots the title with the actual and predicted label for the image and a <code class="inlineCode">colorbar</code> with the colormap. The snippet will output <em class="italic">Figure 7.11</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_11.png" alt="Diagram  Description automatically generated with low confidence"/></figure>
    <p class="packt_figref">Figure 7.11: Intermediate activation map for the first filter for the first convolutional layer for the first true positive battery sample</p>
    <p class="normal">As you can tell in <em class="italic">Figure 7.11</em>, it seems like the intermediate activations for the first filter are finding the edges of the battery and the most prominent text.</p>
    <p class="normal">Next, we will iterate across all computational layers and every battery and visualize attributions for each one. Now, some of these attribution operations can be computationally expensive, so it’s important to clear the GPU cache (<code class="inlineCode">clear_gpu_cache()</code>) in between them:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> l, layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(conv_layers):
    layer = conv_layers[l]
    method = attr.LayerActivation(garbage_mdl, layer)
    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> battery_TP_idxs:
        orig_img = mldatasets.tensor_to_img(test_400_data[idx][<span class="hljs-number">0</span>],\
                                            norm_std, norm_mean,\
                                            to_numpy=<span class="hljs-literal">True</span>)
        tensor = test_data[idx][<span class="hljs-number">0</span>][<span class="hljs-literal">None</span>, :].to(device)
        label = <span class="hljs-built_in">int</span>(y_test[idx])
        attribution = method.attribute(tensor).detach().cpu().numpy()
        viz_img =  mldatasets.<span class="code-highlight"><strong class="hljs-slc">create_attribution_grid</strong></span>(attribution,\
                            cmap=<span class="hljs-string">'copper'</span>, cmap_norm=<span class="hljs-string">'positive'</span>)
        y_true = labels_l[label]
        y_pred = y_test_pred[idx]
        probs_s = probs_df.loc[idx]
        name = method.get_name()
        title = <span class="hljs-string">f'CNN Layer #</span><span class="hljs-subst">{l+</span><span class="hljs-number">1</span><span class="hljs-subst">}</span><span class="hljs-string"> </span><span class="hljs-subst">{name}</span><span class="hljs-string"> Attributions for Sample #</span><span class="hljs-subst">{idx}</span><span class="hljs-string">'</span>
        mldatasets.<span class="code-highlight"><strong class="hljs-slc">compare_img_pred_viz</strong></span>(orig_img, viz_img, y_true,\
                                        y_pred, probs_s, title=title)
    clear_gpu_cache()
</code></pre>
    <p class="normal">The<a id="_idIndexMarker732"/> preceding snippet should <a id="_idIndexMarker733"/>look fairly familiar. Where it’s different is that it’s placing every attribution map for every filter in a grid (<code class="inlineCode">viz_img</code>) with <code class="inlineCode">create_attribition_grid</code>. It could just then display it with <code class="inlineCode">plt.imshow</code> as before, but instead, we will leverage a utility function called <code class="inlineCode">compare_img_pred_viz</code> to visualize the attribution(s) side by side with the original image (<code class="inlineCode">orig_img</code>). It also takes the sample’s actual label (<code class="inlineCode">y_true</code>) and predicted label (<code class="inlineCode">y_pred</code>). Optionally, we can provide a <code class="inlineCode">pandas</code> series with the probabilities for this prediction (<code class="inlineCode">probs_s</code>) and a <code class="inlineCode">title</code>. It generates 56 images in total, including <em class="italic">Figures 7.12</em>,<em class="italic"> 7.13</em>, and <em class="italic">7.14</em>.</p>
    <p class="normal">As you can tell from <em class="italic">Figure 7.12</em>, the first convolutional layer seems to be picking up on the battery’s letters as well as its contours:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_12.png" alt="Graphical user interface  Description automatically generated with low confidence"/></figure>
    <p class="packt_figref">Figure 7.12: Intermediate activations for the first convolutional layer for battery #4</p>
    <p class="normal">However, <em class="italic">Figure 7.13</em> shows<a id="_idIndexMarker734"/> how, by <a id="_idIndexMarker735"/>the fourth convolutional layer, the network understands a battery’s contours better:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_13.png" alt="Graphical user interface  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.13: Intermediate activations for the fourth convolutional layer for battery #4</p>
    <p class="normal">The<a id="_idIndexMarker736"/> last convolutional <a id="_idIndexMarker737"/>layer in <em class="italic">Figure 7.14</em> is impossible to interpret because there are 1,792 filters that are 7 pixels wide and high, but rest assured, there are some very high-level features encoded in those tiny maps:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_14.png" alt="A picture containing text  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.14: Intermediate activations for the last convolutional layer for battery #4</p>
    <p class="normal">Extracting <a id="_idIndexMarker738"/>intermediate<a id="_idIndexMarker739"/> activations can provide you with some insight on a sample-by-sample basis. In other words, it’s a <strong class="keyWord">local model interpretation method</strong>. It’s <a id="_idIndexMarker740"/>by no means the only layerwise-attribution method. Captum has more than ten layer attribution methods: <a href="https://github.com/pytorch/captum#about-captum"><span class="url">https://github.com/pytorch/captum#about-captum</span></a>.</p>
    <h1 id="_idParaDest-198" class="heading-1">Evaluating misclassifications with gradient-based attribution methods</h1>
    <p class="normal"><strong class="keyWord">Gradient-based methods</strong> calculate <strong class="keyWord">attribution maps</strong> for each classification with both forward and background <a id="_idIndexMarker741"/>passes through the CNN. As <a id="_idIndexMarker742"/>the name suggests, these methods leverage the gradients in the backward pass to compute the <a id="_idIndexMarker743"/>attribution maps. All of these methods are local interpretation methods because they only derive a single interpretation per sample. Incidentally, attributions in this context mean that we are attributing the predicted labels to areas of an image. They are often<a id="_idIndexMarker744"/> called <strong class="keyWord">sensitivity maps</strong> in academic literature, too.</p>
    <p class="normal">To get<a id="_idIndexMarker745"/> started, we will first need to create an array with all of our misclassification samples (<code class="inlineCode">X_misclass</code>) from the test dataset (<code class="inlineCode">test_data</code>) using the combined indexes for all of our misclassifications of interest (<code class="inlineCode">misclass_idxs</code>). Since there aren’t that many misclassifications, we are loading a single batch of them (<code class="inlineCode">next</code>): </p>
    <pre class="programlisting code"><code class="hljs-code">misclass_idxs = metal_FP_idxs + plastic_FN_idxs[-<span class="hljs-number">4</span>:]
misclass_data = torch.utils.data.Subset(test_data, misclass_idxs)
misclass_loader = torch.utils.data.DataLoader(misclass_data,\
                                              batch_size = <span class="hljs-number">32</span>)
X_misclass, y_misclass = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(misclass_loader))
X_misclass, y_misclass = X_misclass.to(device), y_misclass.to(device)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker746"/>next step is to create a utility function we can reuse to obtain the attribution maps for any method. Optionally, we can smooth the map with a method called <code class="inlineCode">NoiseTunnel</code> (<a href="https://github.com/pytorch/captum#getting-started"><span class="url">https://github.com/pytorch/captum#getting-started</span></a>). We will cover this method in more detail later:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_attribution_maps</span>(<span class="code-highlight"><strong class="hljs-params-slc">method</strong></span><span class="hljs-params">, model, device,X,y=</span><span class="hljs-literal">None</span><span class="hljs-params">,\</span>
                         <span class="hljs-params">init_args={}, nt_type=</span><span class="hljs-literal">None</span><span class="hljs-params">, nt_samples=</span><span class="hljs-number">10</span><span class="hljs-params">,\</span>
<span class="hljs-params">                         stdevs=</span><span class="hljs-number">0.2</span><span class="hljs-params">, **kwargs</span>):
    attr_maps_size = <span class="hljs-built_in">tuple</span>([<span class="hljs-number">0</span>] + <span class="hljs-built_in">list</span>(X.shape[<span class="hljs-number">1</span>:]))
    attr_maps = torch.empty(attr_maps_size).to(device)
    <span class="code-highlight"><strong class="hljs-slc">attr_method</strong></span> = <span class="code-highlight"><strong class="hljs-slc">method</strong></span>(model, **init_args)
    <span class="hljs-keyword">if</span> nt_type <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        noise_tunnel = attr.NoiseTunnel(attr_method)
        nt_attr_maps = torch.empty(attr_maps_size).to(device)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(X))):
        X_i = X[i].unsqueeze(<span class="hljs-number">0</span>).requires_grad_()
        model.zero_grad()
        extra_args = {**kwargs}
        <span class="hljs-keyword">if</span> y <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            y_i = y[i].squeeze_()
            extra_args.update({<span class="hljs-string">"target"</span>:y_i})
      
        attr_map = <span class="code-highlight"><strong class="hljs-slc">attr_method.attribute</strong></span>(X_i, **extra_args)
        attr_maps = torch.cat([attr_maps, attr_map])
        <span class="hljs-keyword">if</span> nt_type <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            model.zero_grad()
            nt_attr_map = noise_tunnel.attribute(
                X_i, nt_type=nt_type, nt_samples=nt_samples,\
                stdevs=stdevs, nt_samples_batch_size=<span class="hljs-number">1</span>, **extra_args)
            nt_attr_maps = torch.cat([nt_attr_maps, nt_attr_map])
        clear_gpu_cache()
    <span class="hljs-keyword">if</span> nt_type <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">return</span> attr_maps, nt_attr_maps
    <span class="hljs-keyword">return</span> attr_maps
</code></pre>
    <p class="normal">The <a id="_idIndexMarker747"/>preceding<a id="_idIndexMarker748"/> code can create attribution maps for any Captum method for a given model and device. To that end, it takes tensors for the images, <code class="inlineCode">X</code>, and their corresponding labels, <code class="inlineCode">y</code>. The labels are optional and only needed if the attribution method is targeted – most methods are. Most attribution methods (<code class="inlineCode">attr_method</code>) are initialized with only the model, but some require some additional arguments (<code class="inlineCode">init_args</code>). Where they tend to have the most arguments is when the attribution is generated with the <code class="inlineCode">attribute</code> function, which is why we have the <code class="inlineCode">**kwargs</code> collect additional arguments in the <code class="inlineCode">get_attribution_maps</code> function and place them in this call.</p>
    <p class="normal">One important thing to note is that, in this function, we iterate across all the samples in the <code class="inlineCode">X</code> tensor and create the attribute maps for each one independently. This is often unnecessary because the attribute methods are all equipped to process a batch at once. However, there’s a risk the hardware can’t handle an entire batch, and at the time of this writing, very few methods come with an <code class="inlineCode">internal_batch_size</code> argument, which can limit how many samples are processed at a time. What we are doing here is essentially equivalent to setting this number to <code class="inlineCode">1</code> every single time in an effort to ensure that we don’t run into memory issues. However, if you have powerful hardware, you can rewrite the function to process the <code class="inlineCode">X</code> and <code class="inlineCode">y</code> tensors directly.</p>
    <p class="normal">Next, we will perform our first gradient-based attribution method.</p>
    <h2 id="_idParaDest-199" class="heading-2">Saliency maps</h2>
    <p class="normal"><strong class="keyWord">Saliency maps</strong> rely <a id="_idIndexMarker749"/>on the absolute<a id="_idIndexMarker750"/> value of gradients. The intuition is that it will find the pixels in the image that can be perturbed the least so that the output changes the most with these values. It <a id="_idIndexMarker751"/>doesn’t perform perturbations, so it doesn’t validate the hypothesis, and the use of absolute values prevents it from finding other evidence to the contrary.</p>
    <p class="normal">This first saliency map<a id="_idIndexMarker752"/> method was groundbreaking at the time and has inspired a bunch of different methods. It’s typically nicknamed “vanilla” to distinguish it from other saliency maps.</p>
    <p class="normal">Generating saliency maps for all of our misclassified samples is relatively simple with our <code class="inlineCode">get_attribution_maps</code> function. All you need is the Captum attribution method (<code class="inlineCode">attr.Saliency</code>), model (<code class="inlineCode">garbage_mdl</code>), device, and the tensors for the misclassified samples (<code class="inlineCode">X_misclass</code> and <code class="inlineCode">y_misclass</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">saliency_maps = get_attribution_maps(attr.Saliency, garbage_mdl,\
                                     device, X_misclass, y_misclass)
</code></pre>
    <p class="normal">We can plot the output of one of these saliency maps, the fifth one, side by side with the sample image to provide context. Matplotlib can do this easily with a <code class="inlineCode">subplots</code> grid. We will make a 1 × 3 grid and place the sample image in the first spot, its saliency heatmap in the second, and one overlayed over the other in the third. As we have done with previous attribution maps, we can use <code class="inlineCode">tensor_to_img</code> to convert the images to <code class="inlineCode">numpy</code> arrays while also applying a colormap to the attribution. It uses the jet colormap (<code class="inlineCode">cmap='jet'</code>) by default to make the salient areas appear more striking:</p>
    <pre class="programlisting code"><code class="hljs-code">pos = <span class="hljs-number">4</span>
orig_img = mldatasets.tensor_to_img(X_misclass[pos], norm_std,\
                                    norm_mean, to_numpy=<span class="hljs-literal">True</span>)
attr_map = mldatasets.tensor_to_img(
    saliency_maps[pos], to_numpy=<span class="hljs-literal">True</span>,\
    cmap_norm=<span class="hljs-string">'positive'</span>
)
fig, axs = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">5</span>))
axs[<span class="hljs-number">0</span>].imshow(orig_img)
axs[<span class="hljs-number">0</span>].grid(<span class="hljs-literal">None</span>)
axs[<span class="hljs-number">0</span>].set_title(<span class="hljs-string">"Original Image"</span>)
axs[<span class="hljs-number">1</span>].imshow(attr_map)
axs[<span class="hljs-number">1</span>].grid(<span class="hljs-literal">None</span>)
axs[<span class="hljs-number">1</span>].set_title(<span class="hljs-string">"Saliency Heatmap"</span>)
axs[<span class="hljs-number">2</span>].imshow(np.mean(orig_img, axis=<span class="hljs-number">2</span>), cmap=<span class="hljs-string">"gray"</span>)
axs[<span class="hljs-number">2</span>].imshow(attr_map, alpha=<span class="hljs-number">0.6</span>)
axs[<span class="hljs-number">2</span>].grid(<span class="hljs-literal">None</span>)
axs[<span class="hljs-number">2</span>].set_title(<span class="hljs-string">"Saliency Overlayed"</span>)
idx = misclass_idxs[pos]
y_true = labels_l[<span class="hljs-built_in">int</span>(y_test[idx])]
y_pred = y_test_pred[idx]
plt.suptitle(<span class="hljs-string">f"Actual label: </span><span class="hljs-subst">{y_true}</span><span class="hljs-string">, Predicted: </span><span class="hljs-subst">{y_pred}</span><span class="hljs-string">"</span>)
plt.show()
</code></pre>
    <p class="normal">The <a id="_idIndexMarker753"/>preceding code generates the plot in <em class="italic">Figure 7.15</em>:</p>
    <p class="packt_figref"><img src="../Images/B18406_07_15.png" alt="Chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.15: Saliency maps for plastic misclassified as biological waste</p>
    <p class="normal">The sample image in <em class="italic">Figure 7.15</em> appears to be shredded plastic, but the prediction is for biological waste. The vanilla saliency map attributes that prediction mostly to the smoother duller areas of the plastic. It appears that the lack of specular highlights has thrown the model off, but typically, older broken pieces of plastic lose their shine.</p>
    <div class="note">
      <p class="normal">Specular highlights are bright spots of light that appear on the surface of an object when it reflects light. They are often the direct reflections of a light source and are more pronounced on shiny or glossy surfaces, such as metal, glass, or water.</p>
    </div>
    <h2 id="_idParaDest-200" class="heading-2">Guided Grad-CAM</h2>
    <p class="normal">To discuss <strong class="keyWord">guided Grad-CAM</strong>, we first<a id="_idIndexMarker754"/> ought<a id="_idIndexMarker755"/> to discuss <strong class="keyWord">CAM</strong>, which stands for <strong class="keyWord">Class Activation Map</strong>. The way CAM works is that it removes all but the last fully connected layers, and it replaces the last <strong class="keyWord">MaxPooling</strong> layer <a id="_idIndexMarker756"/>with a <strong class="keyWord">Global Average Pooling</strong> (<strong class="keyWord">GAP</strong>) layer. A GAP layer <a id="_idIndexMarker757"/>calculates the average value of each feature map, reducing it to a single value per map, while a MaxPooling layer downsizes feature maps by selecting the maximum value from a set of values in a local region of the map. For instance, in this case:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">The last convolutional layer outputs a tensor that is <code class="inlineCode">1792</code> × <code class="inlineCode">7</code> × <code class="inlineCode">7</code>.</li>
      <li class="numberedList">GAP reduces dimensions by merely averaging the last two dimensions of this tensor, producing a <code class="inlineCode">1792</code> × <code class="inlineCode">1</code> × <code class="inlineCode">1</code> tensor.</li>
      <li class="numberedList">It then feeds this to a fully connected layer with 12 neurons corresponding to each class.</li>
      <li class="numberedList">Once you retrain a CAM model and pass a sample image through the CAM model, it takes the weights from the last layer (a <code class="inlineCode">1792</code> × <code class="inlineCode">12</code> tensor) and extracts the values corresponding to the predicted class (a <code class="inlineCode">1792</code> × <code class="inlineCode">1</code> tensor).</li>
      <li class="numberedList">Then, you calculate the dot product of the last convolutional layer’s output (<code class="inlineCode">1792</code> × <code class="inlineCode">7</code> × <code class="inlineCode">7</code>) with the weight tensor (<code class="inlineCode">1792</code> x <code class="inlineCode">1</code>).</li>
      <li class="numberedList">This weighted sum will end with a <code class="inlineCode">1</code> × <code class="inlineCode">7</code> × <code class="inlineCode">7</code> tensor.</li>
      <li class="numberedList">With bilinear interpolation to stretch it out to <code class="inlineCode">1</code> × <code class="inlineCode">224</code> × <code class="inlineCode">224</code>, this becomes an upsampled class activation map. When you upsample data, you increase its dimensions.</li>
    </ol>
    <p class="normal">The intuition behind CAM is that CNNs inherently retain spatial details in convolutional layers but they are, sadly, lost in fully connected layers. In fact, each filter in the last convolutional layer represents visual patterns at different spatial locations. Once weighted, they represent the most salient regions in the entire image. However, to apply CAM, you must radically modify a model and retrain it, and some models don’t lend themselves easily to this.</p>
    <p class="normal">As the name suggests, Grad-CAM is a similar concept but lacks the modifying and retraining hassle, and uses gradients instead – specifically, those of the class score (prior to softmax) concerning the convolutional layer’s activation maps. GAP is performed on these gradients to <a id="_idIndexMarker758"/>obtain <strong class="keyWord">neuron importance weights</strong>. Then, we compute a weighted linear combination of activation maps with these weights, followed by a ReLU. The ReLU is very important because it ensures locating features that only positively influence the outcome. Like CAM, it is upsampled with bilinear interpolation to match the dimensions of the image.</p>
    <p class="normal">Grad-CAM does <a id="_idIndexMarker759"/>have some shortcomings too, such as failing to identify multiple occurrences or the entirety of the object represented by the predicted class. Like CAM, the resolution of the activation maps may be limited by the final convolutional layer’s dimensions, hence the upsampling.</p>
    <p class="normal">For these<a id="_idIndexMarker760"/> reasons, we are using <strong class="keyWord">guided Grad-CAM</strong> instead. Guided Grad-CAM is a combination of Grad-CAM and guided backpropagation. Guided backpropagation is another visualization method that computes the gradients of the target class with respect to the input image, but it modifies the backpropagation process to only propagate positive gradients for positive activations. This results in a higher-resolution, more detailed visualization. This is achieved by performing an element-wise multiplication of the Grad-CAM heatmap (upsampled to the input image resolution) with the guided backpropagation result. The output is a visualization that emphasizes the most relevant features in the image for the given class, with higher spatial detail than Grad-CAM alone.</p>
    <p class="normal">Generating Grad-CAM attribution maps for all of our misclassified samples can be done with our <code class="inlineCode">get_attribution_maps</code> function. All you need is the Captum attribution method (<code class="inlineCode">attr.GuidedGradCam</code>), model (<code class="inlineCode">garbage_mdl</code>), device, and the tensors for the misclassified samples (<code class="inlineCode">X_misclass</code> and <code class="inlineCode">y_misclass</code>), and, within the method initialization arguments, a layer for which Grad-CAM attributions are computed:</p>
    <pre class="programlisting code"><code class="hljs-code">gradcam_maps = get_attribution_maps(
    attr.GuidedGradCam, garbage_mdl, device, X_misclass,\
    y_misclass, init_args={<span class="hljs-string">'layer'</span>:conv_layers[<span class="hljs-number">3</span>]}
)
</code></pre>
    <p class="normal">Notice that we aren’t using the last layer (which can be indexed with <code class="inlineCode">7</code> or <code class="inlineCode">-1</code>) but the fourth one (<code class="inlineCode">3</code>). This is just to keep things interesting, but we can change it. Next, let’s plot the attributions just as we have before. Nearly the same code is used except <code class="inlineCode">saliency_maps</code> is replaced by <code class="inlineCode">gradcam_maps</code>. The output is depicted in <em class="italic">Figure 7.16</em>.</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_16.png" alt="Chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.16: Guided Grad-CAM heatmaps for plastic misclassified as biological waste</p>
    <p class="normal">As you<a id="_idIndexMarker761"/> can <a id="_idIndexMarker762"/>observe in <em class="italic">Figure 7.16</em>, similar smooth matte areas are highlighted as with the saliency attribution maps, that except guided Grad-CAM yields a few bright areas and edges.</p>
    <p class="normal">Take all of this with a grain of salt. There is still a lot of ongoing debate in the CNN interpretation domain. And researchers are still coming up with new and better methods, and even techniques that are nearly perfect for most use cases still have flaws. Regarding CAM-like methods, there are <a id="_idIndexMarker763"/>many newer ones, such as <strong class="keyWord">Score-CAM</strong>, <strong class="keyWord">Ablation-CAM</strong>, and <strong class="keyWord">Eigen-CAM</strong>, which <a id="_idIndexMarker764"/>provide similar functionality but don’t rely<a id="_idIndexMarker765"/> on gradients, which can be unstable and, therefore, occasionally unreliable. We won’t discuss them here because, of course, they aren’t gradient-based! But it’s essential to note that it doesn’t hurt to try different methods to see what works for your use case.</p>
    <h2 id="_idParaDest-201" class="heading-2">Integrated gradients</h2>
    <p class="normal"><strong class="keyWord">Integrated gradients</strong> (<strong class="keyWord">IG</strong>), also <a id="_idIndexMarker766"/>known<a id="_idIndexMarker767"/> as <strong class="keyWord">path-integrated gradients</strong>, is a<a id="_idIndexMarker768"/> technique that is not exclusive to CNNs. You can apply it to any neural network architecture because it computes the gradients of the output with respect to the inputs averaged all along a path between a <strong class="keyWord">baseline</strong> and the actual input. It is agnostic to the presence of convolutional layers. However, it requires the definition of a baseline, which is supposed to convey a lack of signal, like a uniformly colored image. In practice, for CNNs in particular, this is what a zero baseline represents, which, for every pixel, would usually mean a completely black image. Also, although the name suggests <a id="_idIndexMarker769"/>the use of <strong class="keyWord">path integrals</strong>, integrals aren’t computed but approximated, with summation in sufficiently small intervals for a certain number of steps. For a CNN, this means it makes variations of the input image progressively darker or lighter until it becomes the baseline corresponding to the predefined number of steps. It then feeds these variations to the CNN, computes the gradients for each one, and averages them. The IG is the dot product of the image times the gradient averages.</p>
    <p class="normal">Like<a id="_idIndexMarker770"/> Shapley values, IG is grounded in solid mathematical theory. In this case, it’s the <strong class="keyWord">fundamental theorem of calculus for line integrals</strong>. The mathematical proof of the IG method ensures that the attributions of all the features add up to the difference between the model’s prediction on the input data and its prediction on the baseline input. In addition to this property, which they call <strong class="keyWord">completeness</strong>, there is linearity preservation, symmetry preservation, and sensitivity. We won’t describe each of these properties here. However, it’s important to note that some interpretation methods satisfy notable mathematical properties, while others demonstrate their effectiveness in practical terms.</p>
    <p class="normal">In addition to IG, we will also leverage <code class="inlineCode">NoiseTunnel</code> to perform small random perturbations on the sample image – in other words, to add noise. It creates different noisy versions of the same sample image multiple times and then computes the attribution method for each. It then averages these attributions, potentially making the attribution maps much smoother, which is why this <a id="_idIndexMarker771"/>method is called <strong class="keyWord">SmoothGrad</strong>.</p>
    <p class="normal">But wait, you may ask: Shouldn’t it be a perturbation-based method then?! We’ve already dealt with several perturbation-based methods before in this book, from SHAP to anchors, and something they have in common is that they perturb the input to measure the effect on the output. SmoothGrad doesn’t measure the impact on the outputs. It only helps yield a more robust attribution map because the mean attribution of perturbed inputs should make for more trustworthy attribution maps. We perform cross-validation to evaluate machine learning models for the same reason: the average metrics performed on different test datasets with slightly different distributions make for better metrics.</p>
    <p class="normal">For IG, we will use very similar code as we did for Saliency, except we will add several arguments related to <code class="inlineCode">NoiseTunnel</code>, such as the type of noise tunnel (<code class="inlineCode">nt_type='smoothgrad'</code>), the sample variations to produce (<code class="inlineCode">nt_samples=20</code>), and an amount of random noise to add to each one in standard deviations (<code class="inlineCode">stdevs=0.2</code>). We will find that the more permuted samples to generate, the better, up to a point, and then it doesn’t <a id="_idIndexMarker772"/>have much effect. However, there is such a thing as too much noise, and if you use too little, there won’t be any effect:</p>
    <pre class="programlisting code"><code class="hljs-code">ig_maps, smooth_ig_maps = get_attribution_maps(
    attr.IntegratedGradients, garbage_mdl, device, X_misclass,\
    y_misclass, nt_type=<span class="hljs-string">'smoothgrad'</span>, nt_samples=<span class="hljs-number">20</span>, stdevs=<span class="hljs-number">0.2</span>
)
</code></pre>
    <p class="normal">We can also <a id="_idIndexMarker773"/>optionally define the number of steps for the IG (<code class="inlineCode">n_steps</code>). It’s set to <code class="inlineCode">50</code> by default, and we can also modify the baselines, which is a tensor of zeros by default. As we’ve done with Grad-CAM, we can plot our first sample image side by side with the IG map, but this time, we will modify the code to plot the SmoothGrad integrated gradients (<code class="inlineCode">smooth_ig_maps</code>) in the third position, like this:</p>
    <pre class="programlisting code"><code class="hljs-code">nt_attr_map = mldatasets.tensor_to_img(
    smooth_ig_maps[pos], to_numpy=<span class="hljs-literal">True</span>, cmap_norm=<span class="hljs-string">'positive'</span>
)
axs[<span class="hljs-number">2</span>].imshow(nt_attr_map)
axs[<span class="hljs-number">2</span>].grid(<span class="hljs-literal">None</span>)
axs[<span class="hljs-number">2</span>].set_title(<span class="hljs-string">"SmoothGrad Integrated Gradients"</span>)
</code></pre>
    <p class="normal">The modified snippet outputs <em class="italic">Figure 7.17</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_17.png" alt="Chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.17: Integrated gradient heatmaps for plastic misclassified as biological waste</p>
    <p class="normal">The areas in the IG heatmap in <em class="italic">Figure 7.17</em> coincide with many of the regions spotted by the saliency and guided Grad-CAM maps. However, there are more clusters of strong attributions in the bright yellow areas as well as in brownish shadowed areas, which is consistent with how some foods look when they are disposed of (like banana peels and rotten leafy greens). On the other hand, the bright orange and green areas aren’t.</p>
    <p class="normal">As for the <a id="_idIndexMarker774"/>SmoothGrad IG heatmap, it is <a id="_idIndexMarker775"/>striking how different this map is compared to the non-smooth IG heatmap. This is not always the case; often, it’s just a smoother version. What likely happened was that the <code class="inlineCode">0.2</code> noise distorted the attributions a bit too much, or that 20 perturbed samples weren’t enough. However, it’s tough to tell because it’s also possible that SmoothGrad more accurately depicts the real story.</p>
    <p class="normal">We won’t do this now, but you can visually “tune” the <code class="inlineCode">stdevs</code> and <code class="inlineCode">nt_samples</code> parameters. You can try it with less noise and more samples, using a series of combinations, such as <code class="inlineCode">0.1</code> and <code class="inlineCode">80</code>, and <code class="inlineCode">0.15</code> and <code class="inlineCode">40</code>, trying to figure out whether you see a commonality between them. The one you go with is the one that most clearly depicts this consistent story. One of the shortcomings of SmoothGrad is having to define optimal parameters. Incidentally, IG also has the same issue with defining the baselines and number of steps (<code class="inlineCode">n_steps</code>). The default baseline won’t work in cases where the input image is too large or small, so it must be changed, and the authors of the IG paper suggest that 20-300 steps will approximate the integral within 5%.</p>
    <h2 id="_idParaDest-202" class="heading-2">Bonus method: DeepLIFT</h2>
    <p class="normal">IG has its <a id="_idIndexMarker776"/>detractors, who have made similar<a id="_idIndexMarker777"/> methods that avoid using gradients, such as <strong class="keyWord">DeepLIFT</strong>. IG can be sensitive to zero-valued gradients and discontinuities with gradients, which can lead to misleading attributions. But these point to general disadvantages shared by all gradient-based methods. For this reason, we are introducing the <strong class="keyWord">Deep Learning Important FeaTures</strong> algorithm (<strong class="keyWord">DeepLIFT</strong>). It’s neither a gradient-based nor a perturbation-based method. It’s a backpropagation-based approach!</p>
    <p class="normal">In this section, we will contrast it with IG. Like IG and Shapley values, DeepLIFT was designed for <strong class="keyWord">completeness</strong>, and as such, complies with remarkable mathematical properties. In addition to that, like IG, DeepLIFT can also be applied to various deep learning architectures, including CNNs <a id="_idIndexMarker778"/>and <strong class="keyWord">recurrent neural networks</strong> (<strong class="keyWord">RNNs</strong>), making it versatile for different use cases.</p>
    <p class="normal">DeepLIFT works by <a id="_idIndexMarker779"/>decomposing the output prediction of the model into contributions from each input feature, using the concept of “difference-from-reference.” It backpropagates these contributions through the network layers to assign an importance score to each input feature.</p>
    <p class="normal">More<a id="_idIndexMarker780"/> specifically, like IG, it uses a baseline that represents no specific information about any class. However, it then calculates the difference in the activations of each neuron between the input and the baseline, and it backpropagates these differences through the network, calculating each neuron’s contribution to the output prediction. Then we sum the contributions for each input feature to obtain its importance score (attribution).</p>
    <p class="normal">It’s advantages <a id="_idIndexMarker781"/>over IG are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Reference-based</strong>: Unlike gradient-based methods such as IG, DeepLIFT explicitly compares the input to a reference input, making the attributions more interpretable and meaningful.</li>
      <li class="bulletList"><strong class="keyWord">Non-linear interactions</strong>: DeepLIFT considers the non-linear interactions between neurons when computing attributions. It captures these interactions by considering the multipliers (the change in output due to the change in input) in each layer of the neural network.</li>
      <li class="bulletList"><strong class="keyWord">Stability</strong>: DeepLIFT is more stable than gradient-based methods, as it is less sensitive to small changes in the input, providing more consistent attributions. So, using a SmoothGrad is unnecessary on DeepLIFT attributions although highly recommended for gradient-based methods.</li>
    </ul>
    <p class="normal">Overall, DeepLIFT provides a more interpretable, stable, and comprehensive approach to attributions, making it a valuable tool for understanding and explaining deep learning models.</p>
    <p class="normal">Next, we will create DeepLIFT attribution maps in a similar fashion as we have done the others:</p>
    <pre class="programlisting code"><code class="hljs-code">deeplift_maps = get_attribution_maps(attr.DeepLift, garbage_mdl,\
                                     device, X_misclass, y_misclass)
</code></pre>
    <p class="normal">To plot an attribution map, nearly the same code as with Grad-CAM is used, except <code class="inlineCode">gradcam_maps</code> is replaced by <code class="inlineCode">deeplift_maps</code>. The output is depicted in <em class="italic">Figure 7.18</em>.</p>
    <p class="packt_figref"><img src="../Images/B18406_07_18.png" alt="Chart  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.18: DeepLIFT heatmaps for plastic misclassified as biological waste</p>
    <p class="normal">The<a id="_idIndexMarker782"/> attributions of <em class="italic">Figure 7.18 </em>are not as noisy as in IG. But<a id="_idIndexMarker783"/> they also seem to cluster around some dull yellows and dark areas in the shadows; it also points toward dull greens near the top-right corner.</p>
    <h2 id="_idParaDest-203" class="heading-2">Tying it all together</h2>
    <p class="normal">Now, we<a id="_idIndexMarker784"/> will take everything that we have learned about gradient-based attribution methods and use it to understand the reasons for all the chosen misclassifications (the plastic false negatives and metal false positives). As we did with intermediate activation maps, we can leverage the <code class="inlineCode">compare_img_pred_viz</code> function to place the higher-resolution sample image side by side with four attribution maps: saliency, Grad-CAM, SmoothGrad IG, and DeepLift. To this end, we first have to iterate all the misclassifications’ positions and indexes and extract all the maps. Note that we are using <code class="inlineCode">overlay_bg</code> in the <code class="inlineCode">tensor_to_img</code> function to produce a new image overlaying the original image with the heatmap for each. Lastly, we concatenate the four attribution outputs into a single image (<code class="inlineCode">viz_img</code>). Just as we have done before, we extract the actual label (<code class="inlineCode">y_true</code>), predicted label (<code class="inlineCode">y_pred</code>), and <code class="inlineCode">pandas</code> series with the probabilities (<code class="inlineCode">probs_s</code>) to add some context to the plot we will produce. The <code class="inlineCode">for</code> loop will produce six plots but, for brevity’s sake, we are only going to discuss three of them:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> pos, idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(misclass_idxs):
    orig_img = mldatasets.tensor_to_img(test_400_data[idx][<span class="hljs-number">0</span>],\
                                   norm_std, norm_mean, to_numpy=<span class="hljs-literal">True</span>)
    bg_img = mldatasets.tensor_to_img(test_data[idx][<span class="hljs-number">0</span>],\
                                   norm_std, norm_mean, to_numpy=<span class="hljs-literal">True</span>)
    map1 = mldatasets.tensor_to_img(
        saliency_maps[pos], to_numpy=<span class="hljs-literal">True</span>,\
        cmap_norm=<span class="hljs-string">'positive'</span>, overlay_bg=bg_img
    )
    map2 = mldatasets.tensor_to_img(
        smooth_ig_maps[pos],to_numpy=<span class="hljs-literal">True</span>,\
        cmap_norm=<span class="hljs-string">'positive'</span>, overlay_bg=bg_img
    )
    map3 = mldatasets.tensor_to_img(
        gradcam_maps[pos], to_numpy=<span class="hljs-literal">True</span>,\
        cmap_norm=<span class="hljs-string">'positive'</span>, overlay_bg=bg_img
    )
    map4 = mldatasets.tensor_to_img(
        deeplift_maps[pos], to_numpy=<span class="hljs-literal">True</span>,\
        cmap_norm=<span class="hljs-string">'positive'</span>, overlay_bg=bg_img
    )
    viz_img = cv2.vconcat([
        cv2.hconcat([map1, map2]),
        cv2.hconcat([map3, map4])
    ])
    label = <span class="hljs-built_in">int</span>(y_test[idx])
    y_true = labels_l[label]
    y_pred = y_test_pred[idx]
    probs_s = probs_df.loc[idx]
    title = <span class="hljs-string">'Gradient-Based Attr for Misclassification Sample #{}'</span>.\
                                                           <span class="hljs-built_in">format</span>(idx)
    mldatasets.compare_img_pred_viz(orig_img, viz_img, y_true,\
                                    y_pred, probs_s, title=title)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker785"/> preceding code generates <em class="italic">Figures 7.19</em> to<em class="italic"> 7.21</em>. It’s important to note that in all generated plots, we can observe saliency attributions at the top left, SmoothGrad IG at the top right, guided Grad-CAM at the bottom left, and DeepLIFT at the bottom right:</p>
    <p class="packt_figref"><img src="../Images/B18406_07_19.png" alt="Graphical user interface  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.19: Gradient-based attributions for battery as metal misclassification #8</p>
    <p class="normal">In <em class="italic">Figure 7.19</em>, there’s a <a id="_idIndexMarker786"/>lack of consistency between all four attribution methods. The saliency attribution maps show that all the center parts of the batteries are seen as metal surfaces, in addition to the white parts of the cardboard container. On the other hand, SmoothGrad IG zeros in on the white cardboard and Grad-CAM on the blue cardboard almost exclusively. Lastly, DeepLIFT is much more sparse, only pointing to some parts of the white cardboard.</p>
    <p class="normal">In <em class="italic">Figure 7.20</em>, the attributions are much more consistent than in <em class="italic">Figure 7.19</em>. Matte white areas are clearly confusing the model. This makes sense considering that the plastic in the training data was mostly single pieces of empty plastic containers – including white milk jugs. However, people do recycle toys, plastic tools like spatulas, and other plastic objects. Interestingly enough, although all attribution methods were salient around white and light-yellow surfaces, SmoothGrad IG also highlights some edges, like one of the ducks’ hats and another one’s collar:</p>
    <p class="packt_figref"><img src="../Images/B18406_07_20.png" alt="Graphical user interface, application  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.20: Gradient-based attributions for plastic misclassification #86</p>
    <p class="normal">To continue <a id="_idIndexMarker787"/>with the recycling toys theme, how do LEGO bricks get misclassified as batteries? See <em class="italic">Figure 7.21</em> for an interpretation:</p>
    <p class="packt_figref"><img src="../Images/B18406_07_21.png" alt="Diagram  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.21: Gradient-based attributions for plastic misclassification #89</p>
    <p class="normal"><em class="italic">Figure 7.21</em> shows<a id="_idIndexMarker788"/> how, among all the attribution methods, it’s mostly the yellow and green bricks (and to a lesser degree, light blue) that are to blame for the misclassification because these are popular colors among battery manufacturers, as attested by the training data. Also, the flat surface in between the studs got the most attributions since these resemble the contacts in batteries and, more specifically, 9-volt square batteries. As with the other examples, saliency was the most noisy method. However, this time, guided Grad-CAM was the least noisy. It was also more salient on the edges than on surfaces, unlike the others.</p>
    <p class="normal">We will next try to discover what the model learned about batteries (in addition to white glass) through perturbation-based attribution methods performed on true positives.</p>
    <h1 id="_idParaDest-204" class="heading-1">Understanding classifications with perturbation-based attribution methods</h1>
    <p class="normal">Perturbation-based methods<a id="_idIndexMarker789"/> have already been covered to a great extent in this book so far. So many of the methods we have covered, including SHAP, LIME, anchors, and even permutation feature importance, employ perturbation-based strategies. The intuition behind them is that if you remove, alter, or <a id="_idIndexMarker790"/>mask features in your input data and then make predictions with them, you’ll be able to attribute the difference between the new predictions and the original<a id="_idIndexMarker791"/> predictions to the changes you made in the input. These strategies can be leveraged in both global and local interpretation methods.</p>
    <p class="normal">We will now do the same as we did with the misclassification samples, but to the chosen true positives, and gather four of each class in a single tensor (<code class="inlineCode">X_correctcls</code>):</p>
    <pre class="programlisting code"><code class="hljs-code">correctcls_idxs = wglass_TP_idxs[:<span class="hljs-number">4</span>] + battery_TP_idxs[:<span class="hljs-number">4</span>] 
correctcls_data = torch.utils.data.Subset(test_data, correctcls_idxs)
correctcls_loader = torch.utils.data.DataLoader(correctcls_data,\
                                                batch_size = <span class="hljs-number">32</span>)
X_correctcls, y_correctcls = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(correctcls_loader))
X_correctcls, y_correctcls = X_correctcls.to(device),\
                             y_correctcls.to(device)
</code></pre>
    <p class="normal">One of the more complicated aspects of performing permutation methods on images is that there are not just a few dozen features but many thousands to permute. Picture this: 224 x 224 equals 50,176 pixels, and if we want to measure how a change in each pixel independently affects the outcome, we’ll need to make at least 20 permuted samples for each pixel. So, over a million! For this reason, several permutation methods accept masks to determine which blocks of pixels to permute at once. If we group them in blocks of 32 x 32 pixels, this means we’ll have only 49 blocks in total to permute. However, although it will speed up the attribution methods, we’ll miss out on the effects on smaller sets of pixels the larger the block.</p>
    <p class="normal">We can use many methods to create masks, such as using a segmentation algorithm to break up the images into intuitive blocks based on surfaces and edges. Segmentation is done per image, so the number and placement of segments will vary on an image-to-image basis. There are many methods with scikit-learn’s image segmentation library (<code class="inlineCode">skimage.segmentation</code>): <a href="https://scikit-image.org/docs/stable/api/skimage.segmentation.html"><span class="url">https://scikit-image.org/docs/stable/api/skimage.segmentation.html</span></a>. However, we are going to keep things simple and create one mask for all 224 x 224 images with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">feature_mask = torch.zeros(<span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>).<span class="hljs-built_in">int</span>().to(device)
counter = <span class="hljs-number">0</span>
strides = <span class="hljs-number">16</span>
<span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">224</span>, strides):
    <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">224</span>, strides):
        feature_mask[:, row:row+strides, col:col+strides] = counter
        counter += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">What the <a id="_idIndexMarker792"/>preceding code does is initialize a tensor of zeros the size of the model’s input. It’s easier to <a id="_idIndexMarker793"/>conceptualize this tensor as an empty image. Then it moves across strides that are 16 pixels wide and high, from the top-left corner of the image to the bottom right. As it moves across, it sets the values with consecutive numbers with the <code class="inlineCode">counter</code>. What you end up with is a tensor with all values filled with numbers between 0 and 195, and, if you visualized it as an image, it would be a diagonal gradient from black at the top left to light gray at the bottom right. What’s important to note is that each block with the same value is treated as if it were the same pixel by the attribution method.</p>
    <p class="normal">Before we move forward, let’s discuss baselines. In Captum attribution methods, as in other libraries for that matter, the default baseline is a tensor of zeros, which is usually equivalent to a black image when images are made up of floating-point numbers between 0 and 1. However, in our case, we are standardizing our input tensors so the model doesn’t see tensors with a minimum of 0 but a mean of 0! Therefore, for our garbage model, a tensor of zeros corresponds to a medium gray image, not a black image. For gradient-based methods, there’s nothing inherently wrong with a gray image baseline because there are likely a number of steps between it and the input image. However, perturbation-based methods can be particularly sensitive to having baselines that are too close to the input image because if you replace parts of the input image with the baseline, the model won’t tell the difference!</p>
    <p class="normal">For our garbage model’s case, a black image is made up of tensors of <code class="inlineCode">-2.1179</code> because one of the transformations performed to standardize the input tensors was <code class="inlineCode">(x-0.485)/0.229</code>, which happens to equal approximately <code class="inlineCode">-2.1179</code>, when <code class="inlineCode">x=0</code>. You can also calculate the tensors when <code class="inlineCode">x=1</code>; it converts to <code class="inlineCode">2.64</code> for a white image. That being said, there’s no harm in assuming that somewhere in our true positive samples, there’s at least one pixel that has the lowest value and another with the highest, so we will just use <code class="inlineCode">max()</code> and <code class="inlineCode">min()</code> to create both light and dark baselines:</p>
    <pre class="programlisting code"><code class="hljs-code">baseline_light = <span class="hljs-built_in">float</span>(X_correctcls.<span class="hljs-built_in">max</span>().detach().cpu())
baseline_dark = <span class="hljs-built_in">float</span>(X_correctcls.<span class="hljs-built_in">min</span>().detach().cpu())
</code></pre>
    <p class="normal">We will use only one baseline for all but one perturbation method but feel free to switch them around. Now, on to creating attribution maps for each method!</p>
    <h2 id="_idParaDest-205" class="heading-2">Feature ablation</h2>
    <p class="normal"><strong class="keyWord">Feature ablation</strong> is a<a id="_idIndexMarker794"/> relatively <a id="_idIndexMarker795"/>simple method. What it does is occlude portions of the sample input image <a id="_idIndexMarker796"/>by replacing it with the baseline, which is, by default, zero. The goal is to understand the importance of each input feature (or feature group) in making a prediction by observing the effect of altering it.</p>
    <p class="normal">Here’s how feature ablation works:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><strong class="keyWord">Obtain the original prediction</strong>: First, the model’s prediction for the original input is obtained. This serves as a baseline for comparing the effect of perturbing the input features.</li>
      <li class="numberedList"><strong class="keyWord">Perturb the input feature</strong>: Next, for each input feature (or feature group as set by the feature mask), it is replaced with the baselines value. This creates an “ablated” version of the input.</li>
      <li class="numberedList"><strong class="keyWord">Obtain the prediction for the perturbed input</strong>: The model’s prediction is calculated for the ablated input.</li>
      <li class="numberedList"><strong class="keyWord">Compute the attribution</strong>: The difference in the model’s predictions between the original input and the ablated input is calculated. This difference is attributed to the altered feature, indicating its importance in the prediction.</li>
    </ol>
    <p class="normal">Feature ablation is a simple and intuitive approach to understanding the importance of input features in a model’s prediction. However, it has some limitations. It assumes that features are independent and may not accurately capture the effects of interactions between features. Additionally, it can be computationally expensive for models with a large number of input features or complex input structures. Despite these limitations, feature ablation is a valuable tool for understanding and interpreting model behavior.</p>
    <p class="normal">To generate the attribution maps, we will use the <code class="inlineCode">get_attribution_maps</code> function as we have before, and enter the additional arguments for the <code class="inlineCode">feature_mask</code> and <code class="inlineCode">baselines</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">ablation_maps = get_attribution_maps(
    attr.FeatureAblation,garbage_mdl,\
    device,X_correctcls,y_correctcls,\
    feature_mask=feature_mask,\
    baselines=baseline_dark
)
</code></pre>
    <p class="normal">To <a id="_idIndexMarker797"/>plot an example of the attribution map, you<a id="_idIndexMarker798"/> can copy the same code that we used for saliency, except <code class="inlineCode">saliency_maps</code> is replaced by <code class="inlineCode">ablation_maps</code>, and we are using the second image in the <code class="inlineCode">occlusion_maps</code> array, like this:</p>
    <pre class="programlisting code"><code class="hljs-code">pos = <span class="hljs-number">2</span>
orig_img = mldatasets.tensor_to_img(X_correctcls[pos], norm_std,\
                                    norm_mean, to_numpy=<span class="hljs-literal">True</span>)
attr_map = mldatasets.tensor_to_img(occlusion_maps[pos],to_numpy=<span class="hljs-literal">True</span>,\
                         cmap_norm=<span class="hljs-string">'positive'</span>)
</code></pre>
    <p class="normal">The modified snippet outputs <em class="italic">Figure 7.22</em>:</p>
    <p class="packt_figref"><img src="../Images/B18406_07_22.png" alt="Graphical user interface  Description automatically generated with medium confidence"/></p>
    <p class="packt_figref">Figure 7.22: Feature ablation maps for a white glass true positive from the test dataset</p>
    <p class="normal">In <em class="italic">Figure 7.22</em>, the feature groups in the bottom of the bowl of the wine glass appear to be most important because their absence makes the biggest difference in the outcome, but other portions of the glass are also salient to a lesser degree, except the stem of the wine glass. It makes sense because a wine glass without a stem is still a glass-like container.</p>
    <p class="normal">Next, we will discuss a similar method that will be able to show us attributions with greater detail.</p>
    <h2 id="_idParaDest-206" class="heading-2">Occlusion sensitivity</h2>
    <p class="normal"><strong class="keyWord">Occlusion sensitivity</strong> is<a id="_idIndexMarker799"/> very similar to feature ablation because it also replaces portions of the image with a baseline. However, unlike feature ablation, it doesn’t use a feature mask to group pixels<a id="_idIndexMarker800"/> together. Instead, it groups contiguous features automatically with a sliding window and strides. In this process, it creates many overlapping regions. When this happens, it averages the output differences to compute the attribution for each pixel.</p>
    <p class="normal">In this scenario, besides overlapping regions and their corresponding averages, occlusion sensitivity and feature ablation are identical. In fact, if we used both sliding windows and strides of 3 x 16 x 16, there wouldn’t be any overlapping areas and the feature grouping would be identical to those defined by our <code class="inlineCode">feature_mask</code> made up of 16 x 16 blocks.</p>
    <p class="normal">So, you may wonder, what’s the point of being familiar with both methods? The point is occlusion sensitivity is only suitable for use when a fixed grouping of contiguous features matter, like with images and perhaps other spatial data. And because of its use of strides, it can capture local dependencies and spatial relationships between features. However, although we used contiguous blocks of features, feature ablation doesn’t have to because the <code class="inlineCode">feature_mask</code> can be arranged in whichever way it makes most sense for your inputs to be segmented. This small detail makes it very versatile to other data types. Therefore, feature ablation is a more general approach that can handle various input types and model architectures, while occlusion sensitivity is specifically tailored to image data and convolutional neural networks, with a focus on spatial relationships between features.</p>
    <p class="normal">To generate the attribution maps for occlusion, we will do as before, and enter the additional arguments for the <code class="inlineCode">baselines</code>, <code class="inlineCode">sliding_window_shapes</code>, and <code class="inlineCode">strides</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">occlusion_maps = get_attribution_maps(
    attr.Occlusion, garbage_mdl,\
    device,X_correctcls,y_correctcls,\
    baselines=baseline_dark,\
    sliding_window_shapes=(<span class="hljs-number">3</span>,<span class="hljs-number">16</span>,<span class="hljs-number">16</span>),\
    strides=(<span class="hljs-number">3</span>,<span class="hljs-number">8</span>,<span class="hljs-number">8</span>)
)
</code></pre>
    <p class="normal">Please note that we are creating ample overlapping regions by setting the strides to be only 8 pixels while the sliding windows are 16 pixels. To plot an attribution, you can copy the same code <a id="_idIndexMarker801"/>that we used for feature ablation, except <code class="inlineCode">ablation_maps</code> is replaced by <code class="inlineCode">occlusion_maps</code>. The output is depicted in <em class="italic">Figure 7.23</em>:</p>
    <figure class="mediaobject"><img src="../Images/B18406_07_23.png" alt="Graphical user interface  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 7.23: Occlusion sensitivity maps for a white glass true positive from the test dataset</p>
    <p class="normal">With <em class="italic">Figure 7.23</em>, we <a id="_idIndexMarker802"/>can tell that occlusion’s attributions are eerily similar to the ablation’s attribution, except with more resolution. This resemblance shouldn’t be surprising considering how the feature mask of the former aligns with the sliding window of the latter.</p>
    <p class="normal">Whether we use blocks of non-overlapping 16 x 16 pixels or overlapping 8 x 8, the impact of their absence is measured independently to create the attributions. Therefore, both ablation and occlusion methods aren’t equipped to measure interactions between non-contiguous feature groups. This can prove to be a problem when the absence of two non-contiguous feature groups is what causes a classification to change. For instance, can a wine glass without a stem or a base still be considered a wine glass? It can certainly be considered glass, one would hope, but perhaps the model has learned the wrong relationships.</p>
    <p class="normal">Speaking of relationships, next, we will revisit an old friend: Shapley!</p>
    <h2 id="_idParaDest-207" class="heading-2">Shapley value sampling</h2>
    <p class="normal">If you<a id="_idIndexMarker803"/> recall from <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>, Shapley has provided a method <a id="_idIndexMarker804"/>that is very good at measuring and attributing the impact of coalitions of features to the outcome. Shapley does this by permuting entire coalitions of features at a time rather than permuting one feature at a time, like the two previous methods. That way, it can tease out how more than one feature or feature group interacts with one another.</p>
    <p class="normal">The code to create the attribution maps should be very familiar by now. This method uses the <code class="inlineCode">feature_mask</code> and <code class="inlineCode">baselines</code> but also the number of feature permutations tested (<code class="inlineCode">n_samples</code>). This last attribute has a huge impact on the fidelity of the method. However, it can make it notoriously computationally expensive, so we are not going to run it with the default 25 samples per permutation. Instead, we will use 5 samples to make things more manageable. However, feel free to tweak it should your hardware be able to handle it:</p>
    <pre class="programlisting code"><code class="hljs-code">svs_maps = get_attribution_maps(
    attr.ShapleyValueSampling,garbage_mdl,\
    device, X_correctcls, y_correctcls,\
    baselines=baseline_dark,\
    n_samples=<span class="hljs-number">5</span>, feature_mask=feature_mask
)
</code></pre>
    <p class="normal">To plot an attribution, you can copy the same snippet that we used for occlusion, except <code class="inlineCode">occlusion_maps</code> is replaced by <code class="inlineCode">svs_maps</code>. The output is shown in <em class="italic">Figure 7.24</em>:</p>
    <p class="packt_figref"><img src="../Images/B18406_07_24.png" alt="" role="presentation"/></p>
    <p class="packt_figref">Figure 7.24: Shapley value sampling maps for a white glass true positive from the test dataset</p>
    <p class="normal"><em class="italic">Figure 7.24</em> shows some consistent attributions, such as the most salient area being in the bottom-left corner of the wine glass bowl. Also, the base seems to be more important than the occlusion and ablation methods suggested.</p>
    <p class="normal">However, the <a id="_idIndexMarker805"/>attributions are a lot more noisy than the previous ones. This is partially because we didn’t use a sufficient number of samples to cover all the combinations of features and interactions, and partially because of the messy nature of interactions. It makes<a id="_idIndexMarker806"/> sense that attributions for a single independent feature are concentrated in a few areas, such as the bowl of a wine glass. However, interactions can rely on several parts of the image, such as the base and the rim of the wine glass. They may become important only when they appear together. More interesting is the effect of a background. For instance, if you remove portions of the background, does the wine glass no longer look like a wine glass? Perhaps the background is more important than you think, especially when dealing with a translucent material.</p>
    <h2 id="_idParaDest-208" class="heading-2">KernelSHAP</h2>
    <p class="normal">Since<a id="_idIndexMarker807"/> we are on the topic of Shapley values, let’s try <code class="inlineCode">KernelSHAP</code> from <em class="chapterRef">Chapter 4</em>, <em class="italic">Global Model-Agnostic Interpretation Methods</em>. It leverages LIME to compute Shapley values more efficiently. The <a id="_idIndexMarker808"/>Captum implementation is similar to the SHAP one except it uses linear regression and not Lasso, and it computes the kernel differently. Also, for the LIME image explainer, it is best to use meaningful feature groups (called superpixels) rather than the contiguous blocks we have used in the feature mask. The same advice persists for <code class="inlineCode">KernelSHAP</code>. However, we will keep this simple for this exercise, and also consistent for comparing with the other three permutation-based methods.</p>
    <p class="normal">We will now create the attribution maps but, this time, we will do one with light baselines and another with dark ones. Because <code class="inlineCode">KernelSHAP</code> is an approximation to Shapley sampling values and not as computationally expensive, we can set <code class="inlineCode">n_samples=300</code>. However, this won’t necessarily guarantee high fidelity because it takes a high amount of samples in <code class="inlineCode">KernelSHAP</code> to approximate what a relatively low amount of samples can do exhaustively with Shapley:</p>
    <pre class="programlisting code"><code class="hljs-code">kshap_light_maps = get_attribution_maps(attr.KernelShap, garbage_mdl,\
                                  device, X_correctcls, y_correctcls,\
                                  baselines=baseline_light,\
                                  n_samples=<span class="hljs-number">300</span>,\
                                  feature_mask=feature_mask)
kshap_dark_maps = get_attribution_maps(attr.KernelShap, garbage_mdl,\
                                  device, X_correctcls, y_correctcls,\
                                  baselines=baseline_dark,\
                                  n_samples=<span class="hljs-number">300</span>,\
                                  feature_mask=feature_mask)
</code></pre>
    <p class="normal">To plot an attribution, you can copy the same snippet that we used for Shapley, except <code class="inlineCode">svs_maps</code> is <a id="_idIndexMarker809"/>replaced by <code class="inlineCode">kshap_light_maps</code>, and we modify the code to plot the attributions with the dark baselines in the third position, like this:</p>
    <pre class="programlisting code"><code class="hljs-code">axs[<span class="hljs-number">2</span>].imshow(attr_dark_map)
axs[<span class="hljs-number">2</span>].grid(<span class="hljs-literal">None</span>)
axs[<span class="hljs-number">2</span>].set_title(<span class="hljs-string">"Kernel Shap Dark Baseline Heatmap"</span>)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker810"/> preceding snippet outputs the plots in <em class="italic">Figure 7.25</em>:</p>
    <p class="packt_figref"><img src="../Images/B18406_07_25.png" alt="A picture containing graphical user interface  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.25: KernelSHAP maps for a white glass true positive from the test dataset</p>
    <p class="normal">The two attribution maps in <em class="italic">Figure 7.25</em> are mostly not consistent with each other, but more importantly, not with the previous attributions. Sometimes, some methods have a harder time than others, or it takes some tweaking to get them to work how we expect them to.</p>
    <h2 id="_idParaDest-209" class="heading-2">Tying it all together</h2>
    <p class="normal">Now, we<a id="_idIndexMarker811"/> will take everything that we have learned about perturbation-based attribution methods and use it to understand the reasons for all the chosen true positive classifications (for both white glass and batteries). As we did before, we can leverage the <code class="inlineCode">compare_img_pred_viz</code> function to place the higher-resolution sample image side by side with the four attribution maps: feature ablation, occlusion sensitivity, Shapley, and <code class="inlineCode">KernelSHAP</code>. First, we have to iterate all the classifications’ positions and indexes and extract all the maps. Note that we are using <code class="inlineCode">overlay_bg</code> to produce a new image overlaying the original image with the heatmap for every attribution, as we did for the gradient-based section. Lastly, we concatenate the four attribution outputs into a single image (<code class="inlineCode">viz_img</code>). Just as we have done before, we extract the actual label (<code class="inlineCode">y_true</code>), predicted label (<code class="inlineCode">y_pred</code>), and <code class="inlineCode">pandas</code> series with the probabilities (<code class="inlineCode">probs_s</code>) to add some context to the plot we will produce. The <code class="inlineCode">for</code> loop will produce six plots, but we will only discuss two of them:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> pos, idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(correctcls_idxs):
    orig_img = mldatasets.tensor_to_img(test_400_data[idx][<span class="hljs-number">0</span>],\
                                        norm_std, norm_mean,\
                                        to_numpy=<span class="hljs-literal">True</span>)
    bg_img = mldatasets.tensor_to_img(test_data[idx][<span class="hljs-number">0</span>],\
                                      norm_std, norm_mean,\
                                      to_numpy=<span class="hljs-literal">True</span>)
    map1 = mldatasets.tensor_to_img(ablation_maps[pos],\
                                    to_numpy=<span class="hljs-literal">True</span>,\
                                    cmap_norm=<span class="hljs-string">'positive'</span>,\
                                    overlay_bg=bg_img)
    map2 = mldatasets.tensor_to_img(svs_maps[pos], to_numpy=<span class="hljs-literal">True</span>,\
                                    cmap_norm=<span class="hljs-string">'positive'</span>,\
                                    overlay_bg=bg_img)
    map3 = mldatasets.tensor_to_img(occlusion_maps[pos],\
                                    to_numpy=<span class="hljs-literal">True</span>,\
                                    cmap_norm=<span class="hljs-string">'positive'</span>,\
                                    overlay_bg=bg_img)
    map4 = mldatasets.tensor_to_img(kshap_dark_maps[pos],\
                                    to_numpy=<span class="hljs-literal">True</span>,\
                                    cmap_norm=<span class="hljs-string">'</span><span class="hljs-string">positive'</span>,\
                                    overlay_bg=bg_img)
    viz_img = cv2.vconcat([
            cv2.hconcat([map1, map2]),
            cv2.hconcat([map3, map4])
        ])
    label = <span class="hljs-built_in">int</span>(y_test[idx])
    y_true = labels_l[label]
    y_pred = y_test_pred[idx]
    probs_s = probs_df.loc[idx]
    title = <span class="hljs-string">'Pertubation-Based Attr for Correct classification #{}'</span>.\
                <span class="hljs-built_in">format</span>(idx)
    mldatasets.compare_img_pred_viz(orig_img, viz_img, y_true,\
                                    y_pred, probs_s, title=title)
</code></pre>
    <p class="normal">The preceding code snippet generates several explanations, including <em class="italic">Figures 7.26</em> to <em class="italic">7.28</em>. For your reference, ablation is in the top-left corner and occlusion is at the bottom left. Then, Shapley is at the top right and <code class="inlineCode">KernelSHAP</code> is at the bottom right.</p>
    <p class="normal">Overall, you <a id="_idIndexMarker812"/>can tell that ablation and occlusion are very consistent, while much less so with Shapley and <code class="inlineCode">KernelSHAP</code>. However, what Shapley and <code class="inlineCode">KernelSHAP</code> have in common is that the attributions are more spread out.</p>
    <p class="normal">In <em class="italic">Figure 7.26</em>, all attribution methods have text highlighted, as well as, at least, the left contact of the battery. This is similar to <em class="italic">Figure 7.28</em>, where the text is abundantly highlighted as well as the top contact. This suggests that, for batteries, the model has learned that text and a contact matter. As for white glass, it is less clear. All the attribution methods in <em class="italic">Figure 7.27</em> point to some of the edges of the broken vase, but not always the same edges (except for ablation and occlusion, which are consistent):</p>
    <p class="packt_figref"><img src="../Images/B18406_07_26.png" alt="Graphical user interface, application  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.26: Perturbation-based attributions for battery classification #1</p>
    <p class="normal">White <a id="_idIndexMarker813"/>glass is the hardest glass to classify of the three, and it’s not hard to tell why:</p>
    <p class="packt_figref"><img src="../Images/B18406_07_27.png" alt="Graphical user interface  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.27: Perturbation-based attributions for white glass classification #113</p>
    <p class="normal">As <a id="_idIndexMarker814"/>noted in <em class="italic">Figure 7.27</em>, and others in the test example, it’s hard for the model to distinguish white glass from the light backgrounds. It manages to classify it correctly with these examples. However, this doesn’t mean it will generalize well in other examples where glass is in shards and not as well-lit. As long as the attributions show significant influence from the background, it’s hard to trust that it can recognize glass for its specular highlights, texture, and edges alone.</p>
    <p class="packt_figref"><img src="../Images/B18406_07_28.png" alt="Graphical user interface  Description automatically generated"/></p>
    <p class="packt_figref">Figure 7.28: Perturbation-based attributions for battery classification #2</p>
    <p class="normal">For <em class="italic">Figure 7.28</em>, the background is also highlighted significantly in all attribution maps. But perhaps this is because the baseline was dark and so is the object in its entirety. If you replace an area just outside the edge of the battery with a black square, it makes sense that the model would be confused. For this reason, with permutation-based methods, it’s important to choose an appropriate baseline.</p>
    <h1 id="_idParaDest-210" class="heading-1">Mission accomplished</h1>
    <p class="normal">The mission was to provide an objective evaluation of the garbage classification model for the municipal recycling plant. The predictive performance on out-of-sample validation images was dismal! You could have stopped there, but then you would not have known how to make a better model.</p>
    <p class="normal">However, the predictive performance evaluation was instrumental in deriving specific misclassifications, as well as correct classifications, to assess using other interpretation methods. To this end, you ran a comprehensive suite of interpretation methods, including activation, gradient, perturbation, and backpropagation-based methods. The consensus between all the methods was that the model was having the following issues:</p>
    <ul>
      <li class="bulletList">Differentiating between the background and the objects</li>
      <li class="bulletList">Understanding that different objects share similar color hues</li>
      <li class="bulletList">Confounding lighting conditions, such as specular highlights as specific material characteristics, like with the wine glasses</li>
      <li class="bulletList">An inability to separate unique features of each object, such as plastic studs in LEGO bricks from battery contacts</li>
      <li class="bulletList">Being confused by objects with multiple materials, such as batteries contained in plastic and even cardboard packaging </li>
    </ul>
    <p class="normal">To address these problems, the model needed to be trained with a more varied dataset – hopefully, one that reflects the real-world conditions of the recycling plant; for instance, the expected background (on a conveyor belt), different lighting conditions, and even objects partially occluded by hands, gloves, bags, and so on. Also, they ought to add a category for miscellaneous objects that are made up of multiple materials.</p>
    <p class="normal">Once this dataset has been compiled, it is essential to leverage data augmentation to make the model even more robust to all sorts of variations: angle, brightness, contrast, saturation, and hue variants. And they won’t have to retrain the model from scratch! They can even fine-tune EfficientNet!</p>
    <h1 id="_idParaDest-211" class="heading-1">Summary</h1>
    <p class="normal">After reading this chapter, you should understand how to leverage traditional interpretation methods to more thoroughly assess predictive performance on a CNN classifier and visualize the learning process of CNNs with activation-based methods. You should also understand how to compare and contrast misclassifications and true positives with gradient-based and perturbation-based attribution methods. In the next chapter, we will study interpretation methods for NLP transformers.</p>
    <h1 id="_idParaDest-212" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList">Smilkov, D., Thorat, N., Kim, B., Viégas, F., and Wattenberg, M., 2017, <em class="italic">SmoothGrad: Removing noise by adding noise</em>. ArXiv, abs/1706.03825: <a href="https://arxiv.org/abs/1706.03825"><span class="url">https://arxiv.org/abs/1706.03825</span></a></li>
      <li class="bulletList">Sundararajan, M., Taly, A., and Yan, Q., 2017, <em class="italic">Axiomatic Attribution for Deep Networks</em>. Proceedings of Machine Learning Research, pp. 3319–3328, International Convention Centre, Sydney, Australia: <a href="https://arxiv.org/abs/1703.01365"><span class="url">https://arxiv.org/abs/1703.01365</span></a></li>
      <li class="bulletList">Zeiler, M.D., and Fergus, R., 2014, <em class="italic">Visualizing and Understanding Convolutional Networks</em>. In European conference on computer vision, pp. 818–833: <a href="https://arxiv.org/abs/1311.2901"><span class="url">https://arxiv.org/abs/1311.2901</span></a></li>
      <li class="bulletList">Shrikumar, A., Greenside, P., and Kundaje, A., 2017, <em class="italic">Learning Important Features Through Propagating Activation Differences</em>: <a href="https://arxiv.org/abs/1704.02685"><span class="url">https://arxiv.org/abs/1704.02685</span></a></li>
    </ul>
    <h1 class="heading-1">Learn more on Discord</h1>
    <p class="normal">To join the Discord community for this book – where you can share feedback, ask the author questions, and learn about new releases – follow the QR code below:</p>
    <p class="normal"><a href="Chapter_7.xhtml"><span class="url">https://packt.link/inml</span></a></p>
    <p class="normal"><img src="../Images/QR_Code107161072033138125.png" alt="" role="presentation"/></p>
  </div>
</body></html>