<html><head></head><body>
		<div id="_idContainer029">
			<h1 id="_idParaDest-30"><em class="italic"><a id="_idTextAnchor029"/>Chapter 2</em>: Architectures for Streaming and Real-Time Machine Learning</h1>
			<p>Streaming architectures are an essential component of solutions for real-time machine learning and streaming analytics. Even if you have a model or other analytics tools that can treat data in real time, update, and respond straight away, this will be of no use if there is no architecture to support your solution.</p>
			<p>The first important consideration is making sure that your models and analytics can function on each data point; there needs to be an update function and/or a predict function that can update the solution on each new observation being received by the system.</p>
			<p>Another important consideration for real-time and streaming architectures is data ingress: how to make sure that data can be received on an observation per observation basis, rather than the more traditional batch approach with daily database updates, for example. </p>
			<p>Besides that, it will be important that you understand how to make different software systems communicate. For example, data has to flow very fast from your data generating process, maybe go through a data storage solution, a data quality tool, or a security layer, and then be received by your analytics program. The analytics program will do its work and send the result back to the source, or maybe forward the treated data points to a visualization solution, an alerting system, or similar.</p>
			<p>In this chapter, you will get an introduction to architectures for streaming and real-time machine learning. The central focus of this book will remain on the analytics and machine learning part of the pipeline. The goal of this chapter is to give you enough elements to imagine and implement rough working architectures, while some of the highly-specialized parts on performance, availability, and security will be left out.</p>
			<p>This chapter covers the following topics:</p>
			<ol>
				<li>Defining your analytics as a function</li>
				<li>Understanding microservices architecture</li>
				<li>Communicating between services through APIs</li>
				<li>Demystifying the HTTP protocol</li>
				<li>Building a simple API on AWS</li>
				<li>Big data tools for real-time streaming</li>
				<li>Calling a big data environment in real time</li>
			</ol>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Technical requirements</h1>
			<p>You can find all the code for this book on GitHub at the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python">https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python</a>. If you are not yet familiar with Git and GitHub, the easiest way to download the notebooks and code samples is the following:</p>
			<ol>
				<li value="1">Go to the link of the repository.</li>
				<li>Go to the green <strong class="bold">Code</strong> button.</li>
				<li>Select <strong class="bold">Download ZIP</strong>.</li>
			</ol>
			<p>When you download the ZIP file, you unzip it in your local environment, and you will be able to access the code through your preferred Python editor.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Python environment</h2>
			<p>To follow along <a id="_idIndexMarker067"/>with this book, you can download the code in the repository and execute it using your preferred Python editor.</p>
			<p>If you are not yet familiar with Python environments, I would advise you to check out Anaconda (<a href="https://www.anaconda.com/products/individual">https://www.anaconda.com/products/individual</a>), which comes with the Jupyter Notebook and JupyterLab, which are both great for executing notebooks. It also comes with Spyder and VSCode for editing scripts and programs.</p>
			<p>If you have difficulty installing Python or the associated programs on your machine, you can check out Google Colab (<a href="https://colab.research.google.com/">https://colab.research.google.com/</a>) or Kaggle Notebooks (<a href="https://www.kaggle.com/code">https://www.kaggle.com/code</a>), which both allow you to run Python code in online notebooks for free, without any setup to do.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code in the book will generally use Colab and Kaggle Notebooks with Python version 3.7.13, and you can<a id="_idIndexMarker068"/> set up your own environment to mimic this. </p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Defining your analytics as a function</h1>
			<p>In order to get <a id="_idIndexMarker069"/>started with architecture, let's build an idea from the ground up using the different building blocks that are necessary to make this a minimal working product.</p>
			<p>The first thing that you need to have for this is an understanding of the type of real-time analytics that you want to execute.</p>
			<p>For now, let's go with the same example as in the previous chapter: a real-time business rule that prints an alert when the temperature or acidity of our production line is out of the acceptable limits.</p>
			<p>In the previous chapter, this alert was coded as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code block 2-1</p>
			<pre class="source-code">def super_simple_alert(datapoint):</pre>
			<pre class="source-code">  if datapoint['temperature'] &lt; 10:</pre>
			<pre class="source-code">    print('this is a real time alert. temp too low')</pre>
			<pre class="source-code">  if datapoint['pH'] &gt; 5.5:</pre>
			<pre class="source-code">    print('this is a real time alert. pH too high')</pre>
			<p>In the previous chapter, you used an iteration over a DataFrame to test out this code. In reality, you will always need an idea of architecture so that you can make your code actually receive data in real time from a data generating process. This building block will be covered in this chapter.</p>
			<p>In the following schematic drawing, you see a high-level architectural schema for our streaming solution:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B18335_02_01.jpg" alt="Figure 2.1 – A high-level architectural schema for a streaming solution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – A high-level architectural schema for a streaming solution</p>
			<p>In this schematic drawing, you clearly see that writing code will give you some of the key components of your solution. However, you need to build an architecture around this to make the solution come to life. The darker pieces are still missing from the example implementation.</p>
			<p>While the goal of this book is not to give a full in-depth course on architecture, you will discover some tools and building blocks here that will allow you to deliver an MVP real-time use case. To<a id="_idIndexMarker070"/> get your building blocks cleanly organized, you will need to choose an architectural structure for your solutions. Microservices are an architectural pattern that will allow you to build clean, small building blocks and have them communicate with each other.</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Understanding microservices architecture</h1>
			<p>The concept of <strong class="bold">microservices</strong> is<a id="_idIndexMarker071"/> important to understand when working on architectures. Although there are other ways to architecture software projects, microservices are quite popular for a good reason. They help teams be flexible and effective, and help to keep software flexible and clearly structured.</p>
			<p>The idea behind microservices is in the name: software is represented as many small services that operate individually. When looking at the overall architecture, each of the microservices is inside a small, <em class="italic">black box</em> with clearly defined inputs and outputs. Processes are put in place to call the right black box at the right time.</p>
			<p>Microservice architecture is loosely coupled. This means that there is no fixed communication between the different microservices. Instead, each microservice can be called, or not called, by any other services or code.</p>
			<p>If a change needs to be made to one of the microservices, the scope of the change is fairly local, thereby not affecting other microservices. As input and output are predefined, this also helps in keeping the foundational structure of the program in order, without<a id="_idIndexMarker072"/> it being fixed in any way.</p>
			<p>To allow different <a id="_idIndexMarker073"/>microservices to communicate, an often-chosen solution is to use <strong class="bold">Application Programming Interfaces</strong> (<strong class="bold">APIs</strong>). Let's deep dive into those now.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Communicating between services through APIs</h1>
			<p>A central<a id="_idIndexMarker074"/> component <a id="_idIndexMarker075"/>in microservice architectures is the use of APIs. An API is a part that allows you to connect two microservices (or other pieces of code) together.</p>
			<p>APIs are much like websites. Just like a website, an API is built behind a website-like link or an IP address. When you go to a website, the server of the website sends you the code that represents the website. Your internet browser then interprets this code and shows you a web page.</p>
			<p>When you call an API, the API will receive your request. The request triggers your code to be run on the server and generates a response that is sent back to you. If something goes wrong (maybe your request was not as expected or an error occurs), you may not receive any response, or receive an error code such as <strong class="source-inline">request not authorized</strong> or <strong class="source-inline">internal server error</strong>.</p>
			<p>The following figure shows a flow chart that covers this. A computer or user sends an HTTP request, and the API server sends back the response according to the code that runs on the API server:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B18335_02_02.jpg" alt="Figure 2.2 – A high-level architectural schema for a streaming solution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – A high-level architectural schema for a streaming solution</p>
			<p>You can call APIs with a lot of different tools. Sometimes, you can even use your internet browser, otherwise, tools such as cURL do the job on the command line. You can use tools such <a id="_idIndexMarker076"/>as <a id="_idIndexMarker077"/>Postman or Insomnia for calling APIs with a user interface. All the communication is covered in fixed rules and practices, which, together, are called the HTTP protocol, which we will explore in the next section.</p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Demystifying the HTTP protocol</h1>
			<p>Interaction <a id="_idIndexMarker078"/>between services (or websites) uses the HTTP protocol. When working with APIs and building communicating microservices, it is important to understand the basics of the HTTP protocol.</p>
			<p>The most important thing to know is how to send and format requests and responses.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>The GET request</h2>
			<p>The simplest <a id="_idIndexMarker079"/>HTTP request is the <strong class="source-inline">GET</strong> request. You use this when <a id="_idIndexMarker080"/>you need to get something from a server or a service. For example, when going to a website, your browser sends a <strong class="source-inline">GET</strong> request to the website's IP address to obtain the website's layout code.</p>
			<p>A <strong class="source-inline">GET</strong> request can simply be sent from Python using the following code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code block 2-2</p>
			<pre class="source-code">import requests</pre>
			<pre class="source-code">import json</pre>
			<pre class="source-code">response = requests.get('http://www.google.com')</pre>
			<pre class="source-code">print(response.status_code)</pre>
			<pre class="source-code">print(response.text)</pre>
			<p>This code uses the <strong class="source-inline">requests</strong> library in Python to send a <strong class="source-inline">GET</strong> request to the Google home page. This is technically the same process as going to your internet browser and going to the Google home page. You'll obtain all the code that is needed for your web browser to show you the Google home page. Although many of you are very familiar with the look of the Google home page in your browser, it is much less recognizable in this code response. It is important to understand that it is actually exactly the same thing, just in a different format. </p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>The POST request</h2>
			<p>The <strong class="source-inline">POST</strong> request <a id="_idIndexMarker081"/>is another<a id="_idIndexMarker082"/> request that you'll encounter very often. It allows you to send some data with your request. This is often necessary, especially in analytics APIs, as the analytics are likely to happen on this data. By adding the data in the body of the <strong class="source-inline">POST</strong> request, you make sure that your analytics code received your data.</p>
			<p>The syntax in Python will be something like the following code block. For now, this code doesn't work as you have not built a server that is able to do something with this data. However, just keep in mind that the <strong class="source-inline">POST</strong> request allows you to send your data point to an API with the goal of obtaining a response:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code block 2-3</p>
			<pre class="source-code">import requests</pre>
			<pre class="source-code">import json</pre>
			<pre class="source-code">data = {'temperature': 10, 'pH': 5.5}</pre>
			<pre class="source-code">response = requests.post('http://www.example.com',data=data)</pre>
			<pre class="source-code">print(response.status_code)</pre>
			<pre class="source-code">print(response.text)</pre>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>JSON format for communication between systems</h2>
			<p>The most<a id="_idIndexMarker083"/> common format <a id="_idIndexMarker084"/>for interaction between services is the <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) format. It is a data type that very strongly resembles the dictionary format in Python. In effect, it is a key-value object that is surrounded by accolades.</p>
			<p>An example of a JSON payload is as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code block 2-4</p>
			<pre class="source-code">{</pre>
			<pre class="source-code">     'name': 'your name',</pre>
			<pre class="source-code">     'address': 'your address',</pre>
			<pre class="source-code">     'age': 'your age'</pre>
			<pre class="source-code">}</pre>
			<p>This data format is fairly easy to understand and very commonly used. It is, therefore, important to understand how it works. You'll see its use later on in the chapter as well.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>RESTful APIs</h2>
			<p>While API<a id="_idIndexMarker085"/> development is out of scope for this book, it will be useful to have some pointers <a id="_idIndexMarker086"/>and best practices. The most used API structure is the <strong class="bold">Representational State Transfer</strong> (<strong class="bold">REST</strong>) API.</p>
			<p>The REST API works just like other APIs, but it follows a certain set of style rules that make it recognizable as a REST API, also called the RESTful API.</p>
			<p>There are six guiding constraints in REST APIs:</p>
			<ul>
				<li>Client-server architecture</li>
				<li>Statelessness</li>
				<li>Cacheability</li>
				<li>Layered system</li>
				<li>Code on demand (optional)</li>
				<li>Uniform interface</li>
			</ul>
			<p>If you want to go further on this, some further reading resources are provided at the end of the chapter. Now that we have learned about the HTTP protocol, let's build an API on <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>).</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Building a simple API on AWS</h1>
			<p>In order<a id="_idIndexMarker087"/> to<a id="_idIndexMarker088"/> do something practical, let's build a super simple API on AWS. This will allow you to understand how different services can communicate together. It can also serve as a good testing environment for putting the examples in the rest of the book to the test.</p>
			<p>You will use the following components of the AWS framework.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>API Gateway in AWS</h2>
			<p>This is an <a id="_idIndexMarker089"/>AWS service<a id="_idIndexMarker090"/> that handles API requests for you. You specify the type of request that you expect to receive, and you specify the action that should be taken upon reception of a request. When building an API using API Gateway, this will automatically generate an IP address or link to which you can send your API requests.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Lambda in AWS</h2>
			<p>Lambda is a<a id="_idIndexMarker091"/> serverless execution environment<a id="_idIndexMarker092"/> for code. This means that you can write Python code, plug it to the API Gateway, and not think about how to set up servers, firewalls, and all that. This is great for decoupling systems, and it is fast enough for many real-time systems.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Data-generating process on a local machine</h2>
			<p>As the last <a id="_idIndexMarker093"/>component, you<a id="_idIndexMarker094"/> will build a separate data-generating process in Python. You can execute this code in a notebook. Every time a new data point is generated, the code will call the API with the analytics service and reply with an alert if needed.</p>
			<p>A schematic overview of this architecture can be seen in the following figure:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B18335_02_03.jpg" alt="Figure 2.3 – Detailed architecture schema for AWS&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Detailed architecture schema for AWS</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Implementing the example</h2>
			<p>In order to<a id="_idIndexMarker095"/> implement the example, we will use the following step-by-step instructions. If you have an AWS account, you can skip <em class="italic">Step 0</em>.</p>
			<h3>Step 0 – Creating an account on AWS</h3>
			<p>If you do not yet <a id="_idIndexMarker096"/>have an account on AWS, it is easy to create one. You will have to set it up with a credit card, but the services that we will use here all have a free tier. As long as you shut down the resources at the end of your test, you are unlikely to incur any fees. However, be careful, because mistakes happen, and if you use a lot of resources on AWS, you will end up paying.</p>
			<p>To set up an account, you can simply follow the steps on <a href="http://aws.amazon.com">aws.amazon.com</a>.</p>
			<h3>Step 1 – Setting up a Lambda function</h3>
			<p>Upon receipt <a id="_idIndexMarker097"/>of<a id="_idIndexMarker098"/> the <strong class="source-inline">POST</strong> request, a Lambda function has to be called to execute our alert and send back the response.</p>
			<p>Go to <strong class="bold">Lambda</strong> in the <strong class="bold">Services</strong> menu and click on <strong class="bold">Create function</strong>. You will see the following screen:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B18335_02_04.jpg" alt="Figure 2.4 – Creating a Lambda function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Creating a Lambda function</p>
			<p>Make sure to <a id="_idIndexMarker099"/>select <strong class="bold">Python</strong> and to <a id="_idIndexMarker100"/>give the appropriate name to your function.</p>
			<p>When you have finished creating the function, it is time to code it. You can use the following code for this:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code block 2-5</p>
			<pre class="source-code">import json</pre>
			<pre class="source-code">def super_simple_alert(datapoint):    </pre>
			<pre class="source-code">    answer = ''</pre>
			<pre class="source-code">    if datapoint['temperature'] &lt; 10:</pre>
			<pre class="source-code">        answer += 'temp too low ' </pre>
			<pre class="source-code">    if datapoint['pH'] &gt; 5.5:</pre>
			<pre class="source-code">        answer += 'pH too high '</pre>
			<pre class="source-code">    if answer == '':</pre>
			<pre class="source-code">        answer = 'all good'</pre>
			<pre class="source-code">    return answer</pre>
			<pre class="source-code">def lambda_handler(event, context):</pre>
			<pre class="source-code">    answer = super_simple_alert(event)</pre>
			<pre class="source-code">    return {</pre>
			<pre class="source-code">        'statusCode': 200,</pre>
			<pre class="source-code">        'body': json.dumps({'status': answer}),</pre>
			<pre class="source-code">    }</pre>
			<p>This code has<a id="_idIndexMarker101"/> two<a id="_idIndexMarker102"/> functions. The <strong class="source-inline">super_simple_alert</strong> function takes a <strong class="source-inline">datapoint</strong> and returns an answer (an alarm in string format). The <strong class="source-inline">lambda_handler</strong> function is the code that deals with the incoming API calls. The event contains the <strong class="source-inline">datapoint</strong>, so the event is passed to the <strong class="source-inline">super_simple_alert</strong> function in order to analyze whether an alert should be launched. This is stored in the <strong class="source-inline">answer</strong> variable. Finally, the <strong class="source-inline">lambda_handler</strong> function returns a Python dictionary with the status code <strong class="source-inline">200</strong> and a body that contains the answer.</p>
			<p>The window should now look as follows:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B18335_02_05.jpg" alt="Figure 2.5 – The Lambda function window&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – The Lambda function window</p>
			<h3>Step 2 – Set up API Gateway</h3>
			<p>As a first <a id="_idIndexMarker103"/>step, let's <a id="_idIndexMarker104"/>set up API Gateway to receive a <strong class="source-inline">POST</strong> request. The <strong class="source-inline">POST</strong> request will contain a body in which there is JSON that has a value for temperature and pH, just like in the alerting example.</p>
			<p>To set up API Gateway, you have to go to the <strong class="bold">API Gateway</strong> menu, which is accessible through the <strong class="bold">Services</strong> menu. The <strong class="bold">Management</strong> console looks as follows:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B18335_02_06.jpg" alt="Figure 2.6 – The AWS Management console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – The AWS Management console</p>
			<p>You <a id="_idIndexMarker105"/>should <a id="_idIndexMarker106"/>end up on the <strong class="bold">API Gateway</strong> menu, which looks as follows:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B18335_02_07.jpg" alt="Figure 2.7 – The API Gateway menu&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – The API Gateway menu</p>
			<p>When you <a id="_idIndexMarker107"/>are in the <strong class="bold">API Gateway</strong> menu, you<a id="_idIndexMarker108"/> can go to <strong class="bold">Create API</strong> to set up your first API.</p>
			<p>Inside <strong class="bold">Create API</strong>, do the following steps:</p>
			<ol>
				<li value="1">Select <strong class="bold">REST API</strong>.</li>
				<li>Choose the <strong class="bold">REST</strong> protocol.</li>
				<li>Build the API as a new API.</li>
				<li>Create an API name, for example, <strong class="source-inline">streamingAPI</strong>.</li>
			</ol>
			<p>You will obtain an empty API configuration menu, as follows:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B18335_02_08.jpg" alt="Figure 2.8 – Adding a method in API Gateway&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – Adding a method in API Gateway</p>
			<p>We want<a id="_idIndexMarker109"/> to<a id="_idIndexMarker110"/> add a <strong class="source-inline">POST</strong> method, so go to <strong class="bold">Actions</strong> | <strong class="bold">Create Method</strong>. Select <strong class="bold">POST</strong> in the dropdown and click on the small <strong class="bold">v</strong> character next to the word <strong class="bold">POST</strong> to create the <strong class="source-inline">POST</strong> method. The following menu will appear for setting up the <strong class="source-inline">POST</strong> method:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B18335_02_09.jpg" alt="Figure 2.9 – The POST setup&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – The POST setup</p>
			<h3>Step 3 – Deploy the API</h3>
			<p>Still in the API<a id="_idIndexMarker111"/> Gateway<a id="_idIndexMarker112"/> menu, click on <strong class="bold">Actions</strong> | <strong class="bold">Deploy API</strong>. You can create a new stage called <strong class="source-inline">test</strong> to deploy to. You can use the default setup for this stage, but it is important to take the URL that is on top here to be able to call your API from your data generation process. You will need to set the settings as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B18335_02_010.jpg" alt="Figure 2.10 – More details for the API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – More details for the API</p>
			<h3>Step 4 – Calling your API from another Python environment</h3>
			<p>Now, you <a id="_idIndexMarker113"/>can <a id="_idIndexMarker114"/>call <a id="_idIndexMarker115"/>your API from another Python environment, such as a notebook on your own computer, or from a Google Colab notebook.</p>
			<p>You can use the following code to do that:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code block 2-6</p>
			<pre class="source-code">import requests</pre>
			<pre class="source-code">import json</pre>
			<pre class="source-code">data = {'temperature': 8, 'pH': 4}</pre>
			<pre class="source-code">response = requests.post('YOUR_URL', data = json.dumps(data))</pre>
			<pre class="source-code">print(json.loads(response.text))</pre>
			<p>You will obtain the following answer:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code block 2-7</p>
			<pre class="source-code">{'statusCode': 200, 'body': '{"status": "temp too low "}'}</pre>
			<p>Now, you<a id="_idIndexMarker116"/> can<a id="_idIndexMarker117"/> imagine <a id="_idIndexMarker118"/>how a real-time data-generating process would simply call the API at each new data point and alerts would be generated right away!</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>More architectural considerations</h2>
			<p>Although this is a<a id="_idIndexMarker119"/> great first try at building an API, you should be aware that there is much more to think about when you want to build this in a reliable and secure way. There is a reason that data science and software engineering are different jobs, and it takes time to learn all the skills necessary to manage an API from A to Z. In general, this will not be asked of a data scientist.</p>
			<p>Some of the things that were not covered in this example are as follows:</p>
			<ul>
				<li>Performance: scaling, load balancing, and latency</li>
				<li>DDoS attacks</li>
				<li>Security and hacking</li>
				<li>Financial aspects of API invocation</li>
				<li>Dependency on a cloud provider versus being cloud provider agnostic</li>
			</ul>
			<p>At the end of<a id="_idIndexMarker120"/> the chapter, there are some resources for further reading, which you can check out.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Other AWS services and other services in general that have the same functionality</h2>
			<p>The current<a id="_idIndexMarker121"/> example used API Gateway and a Lambda function to build an API. The advantages of this method are the easiness of access and setup, which makes it great as a method to present in this book. However, you should be aware that there are many other tools and technologies for building APIs. </p>
			<p>AWS is one of the most used cloud providers, and most things that can be done on AWS can be done on the other cloud providers' platforms as well. Examples of other big players are Google's GCP and Microsoft's Azure. Even on AWS, there are many alternatives.</p>
			<p>You can also build APIs in local environments. When doing this, you'll again have a large choice of tools and providers. Now that you have seen how to build an API using standard programming in Python and using a microservices approach, you will next see some alternatives using the big data environment. Big data environments generally have a steeper learning curve and may often be made for a specific use case, but they can be very powerful and absolutely necessary when working with high volume and velocity.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Big data tools for real time streaming</h1>
			<p>There<a id="_idIndexMarker122"/> are many <a id="_idIndexMarker123"/>big data tools that do<a id="_idIndexMarker124"/> real<a id="_idIndexMarker125"/>-time streaming analytics. They can be great alternatives for <em class="italic">regular</em> real-time systems, especially when volumes are large and high speeds are required.</p>
			<p>As a reminder, the term <strong class="bold">big data</strong> is generally used to regroup tools that solve problems that are too complex to fit in memory The problems solved have three core characteristics: volume, variety, and velocity.</p>
			<p>Big data tools are generally known for doing a lot of work in parallel computing. When writing non-optimized, regular Python code, the code will often pass data points one by one. Big data solutions solve this by treating data points in parallel on multiple servers. This approach makes big data tools faster whenever there is a lot of data, but slower when there is little data (due to the overhead of managing the different workers).</p>
			<p>Big data tools are often relatively specific; they should only be used for use cases that have vast amounts of data. It does not make sense to start working on big data tools for every problem at hand.</p>
			<p>Numerous such <a id="_idIndexMarker126"/>solutions<a id="_idIndexMarker127"/> are <a id="_idIndexMarker128"/>made for working with streaming<a id="_idIndexMarker129"/> data. Let's have a look at some commonly used tools:</p>
			<ul>
				<li><strong class="bold">Spark Streaming</strong>: Spark Streaming is<a id="_idIndexMarker130"/> an addition to Spark, one of the main tools for big data nowadays. Spark Streaming can be plugged into sources such as Kafka, Flume, and Amazon Kinesis, thereby making streaming data accessible in a Spark environment.</li>
				<li><strong class="bold">Apache Kafka</strong>: Kafka is an <a id="_idIndexMarker131"/>open source tool managed by Apache. It is a framework that is made for delivering real-time data feeds. It is used by many companies to deliver data pipelines and streaming analytics. Even some cloud providers have integrated Kafka into their solutions.</li>
				<li><strong class="bold">Apache Flume</strong>: Apache <a id="_idIndexMarker132"/>Flume is another open source tool managed by Apache, which also focuses on streaming data. Flume is specifically used for treating large amounts of log data in a big data environment.</li>
				<li><strong class="bold">Apache Beam</strong>: Another tool in <a id="_idIndexMarker133"/>the Apache streaming family is Apache Beam. This tool can handle both batch and streaming data. It is best known for building ETL and data processing pipelines.</li>
				<li><strong class="bold">Apache Storm</strong>: Apache Storm is <a id="_idIndexMarker134"/>a stream processing computation framework that allows doing distributed computation. It is used to process data streams with Hadoop in real time.</li>
				<li><strong class="bold">Apache NiFi</strong>: Apache NiFi is<a id="_idIndexMarker135"/> a tool that focuses on ETL. It gives its users the possibility to automate and manage data flows between systems. It can work together with Kafka.</li>
				<li><strong class="bold">Google Cloud DataFlow</strong>: Google<a id="_idIndexMarker136"/> Cloud DataFlow is a tool proposed by Google Cloud Platform. It is developed specifically for tackling streaming use cases. It allows users to execute Apache Beam pipelines in a fully managed service.</li>
				<li><strong class="bold">Amazon Kinesis</strong>: Amazon Kinesis<a id="_idIndexMarker137"/> is strongly based on open source Apache Kafka, which was discussed earlier. The advantage of using Kinesis over Kafka is that it comes with a lot of things that are managed for you, whereas if you use Kafka directly, you spend more effort on managing the service. Of course, in return, you must use the AWS platform to access it.</li>
				<li><strong class="bold">Azure Stream Analytics</strong>: Azure<a id="_idIndexMarker138"/> Stream Analytics is the main streaming analytics service proposed on Microsoft's cloud platform, Azure. It is a real-time analytics service that is based on Trill.</li>
				<li><strong class="bold">IBM Streams</strong>: IBM Streams<a id="_idIndexMarker139"/> is a streaming analytics<a id="_idIndexMarker140"/> tool <a id="_idIndexMarker141"/>that is <a id="_idIndexMarker142"/>proposed <a id="_idIndexMarker143"/>on the IBM cloud. Just like Kinesis, it is based on the open source Kafka project.</li>
			</ul>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Calling a big data environment in real time</h2>
			<p>If your real-time <a id="_idIndexMarker144"/>analytics service is <a id="_idIndexMarker145"/>managed by a big data or specific streaming tool, you cannot always follow the API method for connecting your real-time process to your analytics process.</p>
			<p>In most cases, you'll need to look into the documentation of the tool of your choice and make sure that you understand how to make the connections work. At this point, you are often going to need a specialized profile to work with you, as this level of architecture and data engineering is generally considered out of scope for most data scientists.</p>
			<p>A general difference between the microservice system and the big data system is that in a microservice approach, we are generally considering that there must be a response coming from the API that is taken into account by the calling service.</p>
			<p>In big data environments, it is much more common for a service such as a website to send data to a big data environment but not need a response. You could imagine a website that writes out every interaction by a user to a fixed location as JSON files. The big data streaming tool is then plugged onto this data storage location to read in the data in a streaming fashion and converts this into an analysis, a visualization, or something else.</p>
			<p>Let's build a minimal example that will show how to do this:</p>
			<ol>
				<li value="1">First, create a JSON file called <strong class="source-inline">example.json</strong>, in which you write only the following data:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code block 2-8</p>
			<p class="source-code">{'value':'hello'}</p>
			<ol>
				<li value="2">You<a id="_idIndexMarker146"/> can <a id="_idIndexMarker147"/>now write a very short piece of Spark Streaming code that reads this data in a streaming way:<p class="source-code">from pyspark.sql import SparkSession</p><p class="source-code">from pyspark.sql.types import *</p><p class="source-code">spark = SparkSession \</p><p class="source-code">    .builder \</p><p class="source-code">    .appName("quickexample") \</p><p class="source-code">    .getOrCreate()</p><p class="source-code">schema = StructType([ StructField("value", StringType(), True) ])</p><p class="source-code">streamingDF = (</p><p class="source-code">  spark</p><p class="source-code">    .readStream</p><p class="source-code">    .schema(schema)</p><p class="source-code">    .json('example.json')</p><p class="source-code">)</p><p class="source-code">display(streamingDF)</p></li>
			</ol>
			<p>In short, this code starts by creating a <strong class="source-inline">spark</strong> session. Once the session is created, a schema is defined for the <strong class="source-inline">example.json</strong> file. As it has only one key (called <strong class="source-inline">value</strong>), the schema is quite short. The data type for the value is <strong class="source-inline">string</strong>.</p>
			<p>You then see that the data is imported using the <strong class="source-inline">.readStream</strong> method, which actually does a lot of the heavy lifting in streaming for you. If you'd like to go further with this example, you could write all kinds of analytical Spark functions using the <strong class="source-inline">streamingDF</strong> library and<a id="_idIndexMarker148"/> you will <a id="_idIndexMarker149"/>have<a id="_idIndexMarker150"/> streaming analytics using the well-known big data tool <strong class="bold">PySpark</strong>.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Summary</h1>
			<p>In this chapter, you have started to discover the field of architecture. You have built your own API on AWS, and you have seen the basic foundation of communication between systems. You should now understand that data is key in communication between systems and that good communication between systems is essential for delivering value through analytics.</p>
			<p>This is especially true in the case of real-time and streaming analytics. The high speed and often large size of data can easily pose problems if architectural bottlenecks are not identified early enough in the project.</p>
			<p>There are other topics that you must remember to take into account, including security, availability, and compliance. Those topics are best left to someone who makes it their full-time responsibility to take care of such data architecture problems.</p>
			<p>In the following chapter, we'll go back to the core of this book, as you'll discover how to build analytics use cases on streaming data.</p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/>Further reading</h1>
			<ul>
				<li><em class="italic">Microservices Architecture</em>: <a href="https://cloud.google.com/learn/what-is-microservices-architecture">https://cloud.google.com/learn/what-is-microservices-architecture</a></li>
				<li>API: <a href="https://www.redhat.com/en/topics/api/what-are-application-programming-interfaces">https://www.redhat.com/en/topics/api/what-are-application-programming-interfaces</a></li>
				<li>HTTP: <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP">https://developer.mozilla.org/en-US/docs/Web/HTTP</a></li>
				<li><em class="italic">Top 10 real-time data streaming tools</em>: <a href="https://ipspecialist.net/top-10-real-time-data-streaming-tools/">https://ipspecialist.net/top-10-real-time-data-streaming-tools/</a></li>
				<li>Spark Streaming: <a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">https://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li>
				<li>Kafka: <a href="https://kafka.apache.org/">https://kafka.apache.org/</a></li>
				<li>Flume: <a href="https://flume.apache.org/">https://flume.apache.org/</a></li>
				<li>Beam: <a href="https://beam.apache.org/">https://beam.apache.org/</a></li>
				<li>Storm: <a href="https://storm.apache.org/">https://storm.apache.org/</a></li>
				<li>NiFi: <a href="https://nifi.apache.org/">https://nifi.apache.org/</a></li>
				<li>Google Cloud Dataflow: <a href="https://cloud.google.com/dataflow">https://cloud.google.com/dataflow</a></li>
				<li>Amazon Kinesis: <a href="https://aws.amazon.com/kinesis/">https://aws.amazon.com/kinesis/</a></li>
				<li>Azure Stream Analytics: <a href="https://azure.microsoft.com/en-us/services/stream-analytics/">https://azure.microsoft.com/en-us/services/stream-analytics/</a></li>
				<li>IBM Streams: <a href="https://www.ibm.com/docs/en/streams">https://www.ibm.com/docs/en/streams</a></li>
				<li><em class="italic">Capturing Web Page Scroll Progress with Amazon Kinesis</em>, by AWS: <a href="https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/kinesis-examples-capturing-page-scrolling.html">https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/kinesis-examples-capturing-page-scrolling.html</a></li>
			</ul>
		</div>
	</body></html>