<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06" class="calibre6"/>Chapter 6. Filtering the Images</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Filtering images using low-pass filters</li><li class="listitem">Downsampling images with filters</li><li class="listitem">Filtering images using a median filter</li><li class="listitem">Applying directional filters to detect edges</li><li class="listitem">Computing the Laplacian of an image</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch06lvl1sec40" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">Filtering is one of the fundamental tasks in signal and image processing. It is a process aimed at selectively extracting certain aspects of an image that are considered to convey important information in the context of a given application. Filtering removes noise in images, extracts interesting visual features, allows image resampling, and so on. It finds its roots in the general <strong class="calibre15">Signals and Systems</strong> theory. We will not cover this theory in detail here. However, this chapter will present some of the important concepts related to filtering and will show you how filters can be used in image-processing applications. But first, let's begin with a brief explanation of the concept of frequency domain analysis.</p><p class="calibre8">When we look at an image, we observe different gray-levels (or colors) patterns laid out over it. Images differ from each other because they have different gray-level distributions. However, there is another point of view under which an image can be analyzed. We can look at the gray-level variations that are present in an image. Some images contain large areas of almost constant intensity (for example, a blue sky), while in other images, the gray-level intensities vary rapidly over the image (for example, a busy scene crowded with many small objects).</p><p class="calibre8">Therefore, observing the frequency of these variations in an image constitutes another way of characterizing an image. This point of view is referred to as the <strong class="calibre15">frequency domain</strong>, while characterizing an image by observing its gray-level distribution is referred to as the <strong class="calibre15">spatial domain</strong>.</p><p class="calibre8">The frequency domain analysis decomposes an image into its frequency content from the lowest to the highest frequencies. Areas where the image intensities vary slowly contain only low frequencies, while high frequencies are generated by rapid changes in intensities. Several well-known transformations exist, such as the <strong class="calibre15">Fourier transform</strong> or the <strong class="calibre15">Cosine transform</strong>, which can be used to explicitly show the frequency content of an image. Note that since an image is a two-dimensional entity, it is made of both vertical frequencies (variations in the vertical directions) and horizontal frequencies (variations in the horizontal directions).</p><p class="calibre8">Under the frequency domain analysis framework, a <strong class="calibre15">filter</strong> is an operation that amplifies certain bands of frequencies of an image (or leaves them unchanged) while blocking (or reducing) other image frequency bands. A low-pass filter is, for instance, a filter that eliminates the high-frequency components of an image; and reciprocally, a high-pass filter eliminates the low-frequency components. This chapter will present some filters that are frequently used in image processing and will explain their effect when applied on an image.</p></div></div>
<div><div><div><div><h1 class="title1"><a id="ch06lvl1sec41" class="calibre6"/>Filtering images using low-pass filters</h1></div></div></div><p class="calibre8">In this first recipe, we will present some very basic low-pass filters. In the introductory section of this chapter, we learned that the objective of such filters is to reduce the amplitude of the image variations. One simple way to achieve this goal is to replace each pixel with the average value of the pixels around it. By doing this, the rapid intensity variations will be smoothed out and thus replaced by more gradual transitions.</p><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec120" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The objective of the <code class="literal">cv::blur</code> function is to smooth an image by replacing each pixel with the average pixel value computed over a rectangular neighborhood. This low-pass filter is applied as follows:</p><pre class="programlisting">cv::blur(image,result, cv::Size(5,5)); // size of the filter 
</pre><p class="calibre8">This kind of filter is also called a <strong class="calibre15">box filter</strong>. Here, we applied it by using a <code class="literal">5x5</code> filter in order to make the filter's effect more visible. Our original image is the following:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_001.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The result of the filter being applied on the preceding image is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_002.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">In some cases, it might be desirable to give more importance to the closer pixels in the neighborhood of a pixel. Therefore, it is possible to compute a weighted average in which nearby pixels are assigned a larger weight than ones that are further away. This can be achieved by using a weighted scheme that follows a <strong class="calibre15">Gaussian function</strong> (a "bell-shaped" function). The <code class="literal">cv::GaussianBlur</code> function applies such a filter and it is called as follows:</p><pre class="programlisting">cv::GaussianBlur(image, result,  
                 cv::Size(5,5), // size of the filter 
                 1.5);          // parameter controlling 
                                // the shape of the Gaussian
</pre><p class="calibre8">The result is then the following image:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_003.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec121" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">A filter is said to be linear if its application corresponds to replacing a pixel with a weighted sum of neighboring pixels. This is the case of the mean filter in which a pixel is replaced by the sum of all pixels in a rectangular neighborhood divided by the size of this neighborhood (to get the average value). This is like multiplying each neighboring pixel by <code class="literal">1</code> over the total number of pixels and summing all of these values. The different weights of a filter can be represented using a matrix that shows the multiplying factors associated with each pixel position in the considered neighborhood.</p><p class="calibre8">The central element of the matrix corresponds to the pixel on which the filter is currently applied. Such a matrix is sometimes called a <strong class="calibre15">kernel</strong> or a <strong class="calibre15">mask</strong>. For a <code class="literal">3x3</code> mean filter, the corresponding kernel would be as follows:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_06_29.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The <code class="literal">cv::boxFilter</code> function filters an image with a square kernel made of many <code class="literal">1</code>s only. It is similar to the mean filter but without dividing the result by the number of coefficients.</p><p class="calibre8">Applying a linear filter then corresponds to moving a kernel over each pixel of an image and multiplying each corresponding pixel by its associated weight. Mathematically, this operation is called a <strong class="calibre15">convolution</strong> and can formally be written as follows:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_06_24-300x43.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The preceding double summation aligns the current pixel at <code class="literal">(x,y)</code> with the center of the kernel, which is assumed to be at coordinate <code class="literal">(0,0)</code>.</p><p class="calibre8">Looking at the output images produced in this recipe, it can be observed that the net effect of a low-pass filter is to blur or smooth the image. This is not surprising since this filter attenuates the high-frequency components that correspond to the rapid variations visible on an object's edge.</p><p class="calibre8">In the case of a Gaussian filter, the weight associated with a pixel is proportional to its distance from the central pixel. Recall that the 1D Gaussian function has the following form:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_06_25-300x88.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The normalizing coefficient <code class="literal">A</code> is chosen so that the area under the Gaussian curve equals one. The σ (sigma) value controls the width of the resulting Gaussian function. The greater this value is, the flatter the function will be. For example, if we compute the coefficients of the 1D Gaussian filter for the interval <code class="literal">[-4, 0, 4]</code> with σ = 0.5, we obtain the following coefficients:</p><pre class="programlisting">[0.0 0.0 0.00026 0.10645 0.78657 0.10645 0.00026 0.0 0.0]</pre><p class="calibre8">For σ=1.5, these coefficients are as follows:</p><pre class="programlisting">[0.0076 0.03608 0.1096 0.2135 0.2667 0.2135 0.1096 0.0361 0.0076 ]</pre><p class="calibre8">Note that these values were obtained by calling the <code class="literal">cv::getGaussianKernel</code> function with the appropriate σ value:</p><pre class="programlisting">    cv::Mat gauss= cv::getGaussianKernel(9, sigma,CV_32F); 
</pre><p class="calibre8">The shape of the Gaussian curve for these two σ values is shown in the following figure. The symmetrical bell shape of the Gaussian function makes it a good choice for filtering:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_06_006.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">As it can be observed, pixels farther from the center have a lower weight, which makes the pixel-to-pixel transitions smoother. This contrasts with the flat mean filter where pixels far away can cause sudden changes in the current mean value. In terms of frequencies, this implies that the mean filter does not remove all the high frequency components.</p><p class="calibre8">To apply a 2D Gaussian filter on an image, one can simply apply a 1D Gaussian filter on the image lines first (to filter the horizontal frequencies), followed by the application of another 1D Gaussian filter on the image columns (to filter the vertical frequencies). This is possible because the Gaussian filter is a <strong class="calibre15">separable filter</strong> (that is, the 2D kernel can be decomposed into two 1D filters). The <code class="literal">cv::sepFilter2D</code> function can be used to apply a general separable filter. It is also possible to directly apply a 2D kernel using the <code class="literal">cv::filter2D</code> function. In general, separable filters are faster to compute than non-separable ones because they require less multiplication operations.</p><p class="calibre8">With OpenCV, the Gaussian filter to be applied to an image is specified by providing both the number of coefficients (the third parameter, which is an odd number) and the value of σ (the fourth parameter) to <code class="literal">cv::GaussianBlur</code>. You can also simply set the value of σ and let OpenCV determine the appropriate number of coefficients (you then input a value of <code class="literal">0</code> for the filter size). The opposite is also possible, where you input a size and a value of <code class="literal">0</code> for σ. The σ value that best fits the given size will be determined.</p></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec122" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">Downsampling images with filters</em> recipe explains how to reduce the size of an image using low-pass filters.</li><li class="listitem">The <em class="calibre16">There's more...</em> section of the <em class="calibre16">Scanning an image with neighbor access recipe</em> in <a href="ch02.html" title="Chapter 2. Manipulating Pixels">Chapter 2</a>, <em class="calibre16">Manipulating Pixels</em>, introduces the <code class="literal">cv::filter2D</code> function. This function lets you apply a linear filter to an image by inputting the kernel of your choice.</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch06lvl1sec42" class="calibre6"/>Downsampling images with filters</h1></div></div></div><p class="calibre8">Images often need to be resized (resampled). The process of reducing the size of an image is often called <strong class="calibre15">downsampling</strong>, while increasing its size is <strong class="calibre15">upsampling</strong>. The challenge in performing these operations is to ensure that the visual quality of the image is preserved as much as possible. To accomplish this objective, low-pass filters are often used; this recipe explains why.</p><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec123" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">You might think that you can reduce the size of an image by simply eliminating some of the columns and rows of the image. Unfortunately, the resulting image will not look very nice. The following figure illustrates this fact by showing you a test image that is reduced by a factor of <code class="literal">4</code> with respect to its original size by simply keeping <code class="literal">1</code> of every <code class="literal">4</code> columns and rows.</p><p class="calibre8">Note that to make the defects in this image more apparent, we zoom in on the image by displaying it with pixels that are four times larger:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_007.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Clearly, one can see that the image quality has degraded. For example, the oblique edges of the castle's roof in the original image now appear as a staircase on the reduced image. Other jagged distortions are also visible on the textured parts of the image (the brick walls, for instance).</p><p class="calibre8">These undesirable artifacts are caused by a phenomenon called <strong class="calibre15">spatial aliasing</strong> that occurs when you try to include high-frequency components in an image that is too small to contain them. Indeed, smaller images (that is, images with fewer pixels) cannot represent fine textures and sharp edges as nicely as higher resolution images (think of the difference between high-definition TV versus older TV technology). Since fine details in an image correspond to high frequencies, we need to remove these higher frequency components in an image before reducing its size.</p><p class="calibre8">We learned in the previous recipe that this can be done through a low-pass filter. Consequently, to reduce the size of an image by four without adding annoying artifacts, you must first apply a low-pass filter to the original image before throwing away columns and rows. This is how you would do this using OpenCV:</p><pre class="programlisting">    // first remove high frequency component 
    cv::GaussianBlur(image,image,cv::Size(11,11),2.0); 
    // keep only 1 of every 4 pixels 
    cv::Mat reduced(image.rows/4,image.cols/4,CV_8U); 
    for (int i=0; i&lt;reduced.rows; i++) 
      for (int j=0; j&lt;reduced.cols; j++) 
        reduced.at&lt;uchar&gt;(i,j)= image.at&lt;uchar&gt;(i*4,j*4); 
</pre><p class="calibre8">The resulting image (also displayed with pixel of four times the normal size) is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_008.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Of course, some of the fine details of the image have been lost, but globally, the visual quality of the image is better preserved than in the previous case (looking at this image from far away should convince you of the relative good quality of the image).</p></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec124" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">In order to avoid undesirable aliasing effect, an image must always be low-pass filtered before reducing its size. As we explained previously, the role of the low-pass filter is to eliminate the high-frequency components that cannot be represented in the reduced image. The formal theory demonstrating this fact is well established and is often referred to as the <strong class="calibre15">Nyquist-Shannon theorem</strong>. In substance, the theory tells us that if you downsample an image by two, then the bandwidth of the representable frequencies is also reduced by two.</p><p class="calibre8">A special OpenCV function performs image reduction using this principle. This is the <code class="literal">cv::pyrDown</code> function:</p><pre class="programlisting">    cv::Mat reducedImage;            // to contain reduced image 
    cv::pyrDown(image,reducedImage); // reduce image size by half 
</pre><p class="calibre8">The preceding function uses a <code class="literal">5x5</code> Gaussian filter to low-pass the image before reducing it by a factor of two. The reciprocal <code class="literal">cv::pyrUp</code> function that doubles the size of an image also exists. It is interesting to note that in this case, the upsampling is done by inserting the 0 values between every two columns and rows and then by applying the same <code class="literal">5x5</code> Gaussian filter (but with the coefficients multiplied by four) on the expanded image. Obviously, if you downsize an image and then upsize it, you will not recover the exact original image. What was lost during the downsizing process cannot be recovered. These two functions are used to create image pyramids. This is a data structure made of stacked versions of an image at different sizes built for efficient multi-scale image analysis. The resulting image is as follows:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_06_009.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Here, each level is two times smaller than the previous level, but the reduction factor can be less, and not necessarily an integer (for example, <code class="literal">1.2</code>). For example, if you want to efficiently detect an object in an image, the detection can be first accomplished on the small image at the top of the pyramid, and as you locate the object of interest, you can refine the search by moving to the lower levels of the pyramid that contains the higher resolution versions of the image.</p><p class="calibre8">Note that there is also a more general <code class="literal">cv::resize</code> function that allows you to specify the size you want for the resulting image. You simply call it by specifying a new size that could be smaller or larger than the original image:</p><pre class="programlisting">    cv::Mat resizedImage;                 // to contain resized image 
    cv::resize(image, resizedImage,
               cv::Size(image.cols/4,image.rows/4)); // 1/4 resizing 
</pre><p class="calibre8">It is also possible to specify resizing in terms of scale factors. In this case, an empty size instance is given as an argument followed by the desired scale factors:</p><pre class="programlisting">    cv::resize(image, resizedImage,  
               cv::Size(), 1.0/4.0, 1.0/4.0); // 1/4 resizing 
</pre><p class="calibre8">A final parameter allows you to select the interpolation method that is to be used in the resampling process. This is discussed in the following section.</p></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec125" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">When an image is resized by a factional factor, it becomes necessary to perform some pixel interpolation in order to produce new pixel values at locations that fall in between the existing ones. General image remapping, as discussed in the <em class="calibre16">Remapping an image</em> recipe in 
<a href="ch02.html" title="Chapter 2. Manipulating Pixels">Chapter 2</a>
, <em class="calibre16">Manipulating Pixels</em>, is another situation where pixel interpolation is required.</p><div><div><div><div><h3 class="title3"><a id="ch06lvl3sec26" class="calibre6"/>Interpolating pixel values</h3></div></div></div><p class="calibre8">The most basic approach to perform interpolation is to use a nearest neighbor strategy. The new grid of pixels that must be produced is placed on top of the existing image, and each new pixel is assigned the value of its closest pixel in the original image. In the case of image upsampling (that is, when using a new grid denser than the original one), this implies that more than one pixel of the new grid will receive its value from the same original pixel. For example, resizing the reduced image of the previous section by four using nearest neighbor interpolation has been done as follows:</p><pre class="programlisting">    cv::resize(reduced, newImage, cv::Size(), 3, 3, cv::INTER_NEAREST); 
</pre><p class="calibre8">In this case, the interpolation corresponds to simply increasing the size of each pixel by four. A better approach consists of interpolating a new pixel value by combining the values of several neighboring pixels. Hence, we can linearly interpolate a pixel value by considering the four pixels around it, as illustrated by the following figure:</p><p class="calibre8">
</p><div><img alt="Interpolating pixel values" src="img/image_06_010.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This is done by first vertically interpolating two pixel values to the left- and right-hand side of the added pixel. Then, these two interpolated pixels (drawn in gray in the preceding figure) are used to horizontally interpolate the pixel value at the desired location. This bilinear interpolation scheme is the default approach used by <code class="literal">cv::resize</code> (that can also be explicitly specified by the <code class="literal">cv::INTER_LINEAR</code> flag):</p><pre class="programlisting">    cv::resize(reduced, newImage, cv::Size(), 4, 4, cv::INTER_LINEAR); 
</pre><p class="calibre8">The following is the result:</p><p class="calibre8">
</p><div><img alt="Interpolating pixel values" src="img/image_06_011.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">There are also other approaches that can produce superior results. With <strong class="calibre15">bicubic interpolation</strong>, a neighborhood of <code class="literal">4x4</code> pixels is considered to perform the interpolation. However, since the approach uses more pixels (<code class="literal">16</code>) and implies the computation of cubic terms, it is slower to compute than bilinear interpolation.</p></div></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec126" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">There's more...</em> section of the <em class="calibre16">Scanning an image with neighbor access</em> recipe in <a href="ch02.html" title="Chapter 2. Manipulating Pixels">Chapter 2</a>, <em class="calibre16">Manipulating Pixels</em>, introduces the <code class="literal">cv::filter2D</code> function. This function lets you apply a linear filter to an image by inputting the kernel of your choice.</li><li class="listitem">The <em class="calibre16">Detecting scale-invariant features</em> recipe in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <em class="calibre16">Detecting Interest Points</em>, uses image pyramids to detect interest points in an image.</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch06lvl1sec43" class="calibre6"/>Filtering images using a median filter</h1></div></div></div><p class="calibre8">The first recipe of this chapter introduced the concept of linear filters. Non-linear filters also exist and can be advantageously used in image processing. One such filter is the median filter that we present in this recipe.</p><p class="calibre8">Since median filters are particularly useful in order to combat salt-and-pepper noise (or salt-only, in our case), we will use the image we created in the first recipe of 
<a href="ch02.html" title="Chapter 2. Manipulating Pixels">Chapter 2</a>
, <em class="calibre16">Manipulating Pixels</em>, which is reproduced here:</p><p class="calibre8">
</p><div><img alt="Filtering images using a median filter" src="img/image_06_012.jpg" class="calibre17"/></div><p class="calibre8">
</p><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec127" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The call to the median filtering function is done in a way that is similar to the other filters:</p><pre class="programlisting">    cv::medianBlur(image, result, 5);  
    // last parameter is size of the filter 
</pre><p class="calibre8">The resulting image is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_013.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec128" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">Since the median filter is not a linear filter, it cannot be represented by a kernel matrix, it cannot be applied through a convolution operation (that is, using the double-summation equation introduced in the first recipe of this chapter). However, it also operates on a pixel's neighborhood in order to determine the output pixel value. The pixel and its neighborhood form a set of values and, as the name suggests, the median filter will simply compute the median value of this set (the median of a set is the value at the middle position when the set is sorted). The current pixel is then replaced with this median value.</p><p class="calibre8">This explains why the filter is so efficient in eliminating the salt-and-pepper noise. Indeed, when an outlier black or white pixel is present in a given pixel neighborhood, it is never selected as the median value (it is rather the maximal or minimal value), so it is always replaced by a neighboring value.</p><p class="calibre8">In contrast, a simple mean filter would be greatly affected by such noise, as can be observed in the following image, which represents the mean filtered version of our salt-and-pepper corrupted image:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_06_014.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Clearly, the noisy pixels shifted the mean value of neighboring pixels. As a result, the noise is still visible even if it has been blurred by the mean filter.</p><p class="calibre8">The median filter also has the advantage of preserving the sharpness of the edges. However, it washes out the textures in uniform regions (for example, the trees in the background). Because of the visual impact it has on images, the median filter is often used to create special effects in photo-editing software tools. You should test it on a color image to see how it can produce cartoon-like images.</p></div></div>
<div><div><div><div><h1 class="title1"><a id="ch06lvl1sec44" class="calibre6"/>Applying directional filters to detect edges</h1></div></div></div><p class="calibre8">The first recipe of this chapter introduced the idea of linear filtering using kernel matrices. The filters that were used had the effect of blurring an image by removing or attenuating its high-frequency components. In this recipe, we will perform the opposite transformation, that is, amplifying the high-frequency content of an image. As a result, the high-pass filters introduced in this recipe will perform <strong class="calibre15">edge detection</strong>.</p><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec129" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The filter that we will use here is called the <strong class="calibre15">Sobel</strong> filter. It is said to be a directional filter, because it only affects the vertical or the horizontal image frequencies depending on which kernel of the filter is used. OpenCV has a function that applies the Sobel operator on an image. The horizontal filter is called as follows:</p><pre class="programlisting">    cv::Sobel(image,     // input 
              sobelX,    // output 
              CV_8U,     // image type 
              1, 0,      // kernel specification 
              3,         // size of the square kernel 
              0.4, 128); // scale and offset 
</pre><p class="calibre8">Vertical filtering is achieved by the following (and very similar to the horizontal filter) call:</p><pre class="programlisting">    cv::Sobel(image,     // input 
              sobelY,    // output 
              CV_8U,     // image type 
              0, 1,      // kernel specification 
              3,         // size of the square kernel 
              0.4, 128); // scale and offset 
</pre><p class="calibre8">Several integer parameters are provided to the function, and these will be explained in the next section. Note that these have been chosen to produce an 8-bit image (<code class="literal">CV_8U</code>) representation of the output.</p><p class="calibre8">The result of the horizontal <code class="literal">Sobel</code> operator is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_015.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Since, as will be seen in the next section, the kernels of the <code class="literal">Sobel</code> operator contain both positive and negative values, the result of the <code class="literal">Sobel</code> filter is generally computed in a 16-bit signed integer image (<code class="literal">CV_16S</code>). To make the results displayable as an 8-bit image, as shown in the preceding figure, we used a representation in which a zero value corresponds to gray-level <code class="literal">128</code>. Negative values are represented by darker pixels, while positive values are represented by brighter pixels. The vertical Sobel image is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_016.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">If you are familiar with photo-editing software, the preceding images might remind you of the image emboss effect, and indeed, this image transformation is generally based on the use of directional filters.</p><p class="calibre8">The two results (vertical and horizontal) can then be combined to obtain the norm of the <code class="literal">Sobel</code> filter:</p><pre class="programlisting">    // Compute norm of Sobel 
    cv::Sobel(image,sobelX,CV_16S,1,0); 
    cv::Sobel(image,sobelY,CV_16S,0,1); 
    cv::Mat sobel; 
    //compute the L1 norm 
    sobel= abs(sobelX)+abs(sobelY); 
</pre><p class="calibre8">The Sobel norm can be conveniently displayed in an image using the optional rescaling parameter of the <code class="literal">convertTo</code> method in order to obtain an image in which zero values correspond to white, and higher values are assigned darker gray shades:</p><pre class="programlisting">    // Find Sobel max value 
    double sobmin, sobmax; 
    cv::minMaxLoc(sobel,&amp;sobmin,&amp;sobmax); 
    // Conversion to 8-bit image 
    // sobelImage = -alpha*sobel + 255 
    cv::Mat sobelImage; 
    sobel.convertTo(sobelImage,CV_8U,-255./sobmax,255); 
</pre><p class="calibre8">The following image is then produced:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_017.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Looking at this image, it is now clear why this kind of operator is called an edge detector. It is then possible to threshold the image in order to obtain a binary map showing the image contours. The following snippet creates the image that follows it:</p><pre class="programlisting">    cv::threshold(sobelImage, sobelThresholded,  
                  threshold, 255, cv::THRESH_BINARY); 
</pre><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_018.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec130" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">The Sobel operator is a classic edge-detection linear filter that is based on two simple <code class="literal">3x3</code> kernels that have the following structure:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_06_30.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_06_31.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">If we view the image as a two-dimensional function, the Sobel operator can then be seen as a measure of the variation of the image in the vertical and horizontal directions. In mathematical terms, this measure is called a <strong class="calibre15">gradient</strong>, and it is defined as a 2D vector that is made from the function's first derivatives in two orthogonal directions:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_06_019-300x112.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The Sobel operator gives you an approximation of the image gradient by differencing pixels in the horizontal and vertical directions. It operates on a window around the pixel of interest in order to reduce the influence of noise. The <code class="literal">cv::Sobel</code> function computes the result of the convolution of the image with a Sobel kernel. Its complete specification is as follows:</p><pre class="programlisting">    cv::Sobel(image,         // input 
              sobel,         // output 
              image_depth,   // image type 
              xorder,yorder, // kernel specification   
              kernel_size,   // size of the square kernel 
              alpha, beta);  // scale and offset 
</pre><p class="calibre8">By using the appropriate arguments, you decide whether you wish to have the result written in an unsigned character, a signed integer, or a floating point image. Of course, if the result falls outside of the domain of the image pixel, saturation will be applied. This is where the last two parameters can be useful. Before storing the result in the image, the result can be scaled (multiplied) by <code class="literal">alpha</code> and an offset, <code class="literal">beta</code>, can be added.</p><p class="calibre8">This is how, in the previous section, we generated an image for which the Sobel value <code class="literal">0</code> was represented by the mid-gray level <code class="literal">128</code>. Each Sobel mask corresponds to a derivative in one direction. Therefore, two parameters are used to specify the kernel that will be applied, the order of the derivative in the <code class="literal">x</code>, and the <code class="literal">y</code> directions. For instance, the horizontal Sobel kernel is obtained by specifying <code class="literal">1</code> and <code class="literal">0</code> for the <code class="literal">xorder</code> and <code class="literal">yorder</code> parameters, and the vertical kernel will be generated with <code class="literal">0</code> and <code class="literal">1</code>. Other combinations are also possible, but these two are the ones that will be used most often (the case of second-order derivatives is discussed in the next recipe). Finally, it is also possible to use kernels of a size larger than <code class="literal">3x3</code>. Values <code class="literal">1</code>, <code class="literal">3</code>, <code class="literal">5</code>, and <code class="literal">7</code> are possible choices for the kernel size. A kernel of size <code class="literal">1</code> corresponds to a 1D Sobel filter (<code class="literal">1x3</code> or <code class="literal">3x1</code>). See the following <em class="calibre16">There's more...</em> section to learn why using a larger kernel might be useful.</p><p class="calibre8">Since the gradient is a 2D vector, it has a norm and a direction. The norm of the gradient vector tells you what the amplitude of the variation is, and it is normally computed as a Euclidean norm (also called <strong class="calibre15">L2 norm</strong>):</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_06_27-300x89.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">However, in image processing, this norm is often computed as the sum of the absolute values. This is called the <strong class="calibre15">L1 norm</strong>, and it gives values that are close to the L2 norm but at a lower computational cost. This is what we did in this recipe:</p><pre class="programlisting">    // compute the L1 norm 
    sobel= abs(sobelX)+abs(sobelY); 
</pre><p class="calibre8">The gradient vector always points in the direction of the steepest variation. For an image, this means that the gradient direction will be orthogonal to the edge, pointing in the darker to brighter direction. Gradient angular direction is given by the following formula:</p><p class="calibre8">Most often, for edge detection, only the norm is computed. However, if you require both the norm and the orientation, then the following OpenCV function can be used:</p><pre class="programlisting">    // Sobel must be computed in floating points 
    cv::Sobel(image,sobelX,CV_32F,1,0); 
    cv::Sobel(image,sobelY,CV_32F,0,1); 
    // Compute the L2 norm and direction of the gradient 
    cv::Mat norm, dir; 
    // Cartesian to polar transformation to get magnitude and angle 
    cv::cartToPolar(sobelX,sobelY,norm,dir); 
</pre><p class="calibre8">By default, the orientation is computed in radians. Just add <code class="literal">true</code> as an additional argument in order to have them computed in degrees.</p><p class="calibre8">A binary edge map has been obtained by applying a threshold on the gradient magnitude. Choosing the right threshold is not an obvious task. If the threshold value is too low, too many (thick) edges will be retained, while if we select a more severe (higher) threshold, then broken edges will be obtained. As an illustration of this trade-off situation, compare the preceding binary edge map with the following, which is obtained using a higher threshold value:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_06_021.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">One way to get the best of both lower and higher thresholds is to use the concept of hysteresis thresholding. This will be explained in the next chapter, where we introduce the Canny operator.</p></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec131" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">Other gradient operators also exist. We present some of them in this section. It is also possible to apply a Gaussian smoothing filter before applying a derivative filter. This makes it less sensitive to noise, as explained in this section.</p><div><div><div><div><h3 class="title3"><a id="ch06lvl3sec27" class="calibre6"/>Gradient operators</h3></div></div></div><p class="calibre8">To estimate the gradient at a pixel location, the <strong class="calibre15">Prewitt </strong>
<strong class="calibre15">operator</strong> defines the following kernels:</p><p class="calibre8">
</p><div><img alt="Gradient operators" src="img/B05388_06_32.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">
</p><div><img alt="Gradient operators" src="img/B05388_06_33.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The <strong class="calibre15">Roberts operator</strong> is based on these simple <code class="literal">2x2</code> kernels:</p><p class="calibre8">
</p><div><img alt="Gradient operators" src="img/B05388_06_34.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">
</p><div><img alt="Gradient operators" src="img/B05388_06_35.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The <strong class="calibre15">Scharr operator</strong> is preferred when more accurate estimates of the gradient orientation are required:</p><p class="calibre8">
</p><div><img alt="Gradient operators" src="img/B05388_06_36.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">
</p><div><img alt="Gradient operators" src="img/B05388_06_37.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Note that it is possible to use the Scharr kernels with the <code class="literal">cv::Sobel</code> function by calling it with the <code class="literal">CV_SCHARR</code> argument:</p><pre class="programlisting">    cv::Sobel(image,sobelX,CV_16S,1,0, CV_SCHARR); 
</pre><p class="calibre8">Or, equivalently, you can call the <code class="literal">cv::Scharr</code> function:</p><pre class="programlisting">    cv::Scharr(image,scharrX,CV_16S,1,0,3); 
</pre><p class="calibre8">All of these directional filters try to estimate the first-order derivatives of the image function. Therefore, high values are obtained at areas where large intensity variations in the filter direction are present, while flat areas produce low values. This is why filters that compute image derivatives are high-pass filters.</p></div><div><div><div><div><h3 class="title3"><a id="ch06lvl3sec28" class="calibre6"/>Gaussian derivatives</h3></div></div></div><p class="calibre8">Derivative filters are high-pass filters. As such, they tend to amplify noise and small highly-contrasted details in an image. In order to reduce the impact of these higher frequency elements, it is a good practice to first smooth the image before applying a derivative filter. You might think that this would be done in two steps, which are smoothing the image and then computing the derivative. However, a closer look at these operations reveals that it is possible to combine these two steps into one with a proper choice of the smoothing kernel. We learned previously that the convolution of an image with a filter can be expressed as a summation of terms. Interestingly, a well-known mathematical property is that the derivative of a summation of terms is equal to the summation of the terms' derivative.</p><p class="calibre8">Consequently, instead of applying the derivative on the result of the smoothing, it is possible to derive the kernel and then convolute it with the image; these two operations are then accomplished in a single pass over the pixels. Since the Gaussian kernel is continuously derivable, it represents a particularly appropriate choice. This is what is done when you call the <code class="literal">cv::sobel</code> function with different kernel sizes. The function will compute a Gaussian derivative kernel with different σ values. As an example, if we select the <code class="literal">7x7</code> Sobel filter (that is, <code class="literal">kernel_size=7</code>) in the <code class="literal">x</code> direction, the following result is obtained:</p><p class="calibre8">
</p><div><img alt="Gaussian derivatives" src="img/image_06_022.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">If you compare this image with the one shown earlier, it can be seen that many fine details have been removed, giving them more emphasis on the more significant edges. Note that we now have a band-pass filter, some higher frequencies being removed by the Gaussian filter and the lower frequencies being removed by the <code class="literal">Sobel</code> filter.</p></div></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec132" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">Detecting image contours with the Canny operator</em> recipe in <a href="ch07.html" title="Chapter 7. Extracting Lines, Contours, and Components">
Chapter 7
</a>, <em class="calibre16">Extracting Lines, Contours, and Components</em>, shows you how to obtain a binary edge map using two different threshold values</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch06lvl1sec45" class="calibre6"/>Computing the Laplacian of an image</h1></div></div></div><p class="calibre8">The Laplacian is another high-pass linear filter that is based on the computation of the image derivatives. As it will be explained, it computes second-order derivatives to measure the curvature of the image function.</p><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec133" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The OpenCV function, <code class="literal">cv::Laplacian</code>, computes the Laplacian of an image. It is very similar to the <code class="literal">cv::Sobel</code> function. In fact, it uses the same basic function, <code class="literal">cv::getDerivKernels</code>, in order to obtain its kernel matrix. The only difference is that there are no derivative order parameters since these ones are, by definition, second order derivatives.</p><p class="calibre8">For this operator, we will create a simple class that will encapsulate some useful operations related to the Laplacian. The basic attributes and methods are as follows:</p><pre class="programlisting">    class LaplacianZC { 
 
      private: 
      // laplacian 
      cv::Mat laplace; 
      // Aperture size of the laplacian kernel 
      int aperture; 
 
      public: 
 
      LaplacianZC() : aperture(3) {} 
 
      // Set the aperture size of the kernel 
      void setAperture(int a) { 
        aperture= a; 
      } 
 
      // Compute the floating point Laplacian 
      cv::Mat computeLaplacian(const cv::Mat&amp; image) { 
 
        // Compute Laplacian 
        cv::Laplacian(image,laplace,CV_32F,aperture); 
        return laplace; 
    } 
</pre><p class="calibre8">The computation of the Laplacian is done here on a floating point image. To get an image of the result, we perform a rescaling, as shown in the previous recipe. This rescaling is based on the Laplacian maximum absolute value, where value <code class="literal">0</code> is assigned gray-level <code class="literal">128</code>. A method of our class allows the following image representation to be obtained:</p><pre class="programlisting">    // Get the Laplacian result in 8-bit image 
    // zero corresponds to gray level 128 
    // if no scale is provided, then the max value will be 
    // scaled to intensity 255 
    // You must call computeLaplacian before calling this 
    cv::Mat getLaplacianImage(double scale=-1.0) { 
      if (scale&lt;0) { 
        double lapmin, lapmax; 
        // get min and max laplacian values 
        cv::minMaxLoc(laplace,&amp;lapmin,&amp;lapmax); 
        // scale the laplacian to 127 
        scale= 127/ std::max(-lapmin,lapmax); 
      } 
   
      // produce gray-level image 
      cv::Mat laplaceImage; 
      laplace.convertTo(laplaceImage,CV_8U,scale,128); 
      return laplaceImage; 
    } 
</pre><p class="calibre8">Using this class, the Laplacian image computed from a <code class="literal">7x7</code> kernel is obtained as follows:</p><pre class="programlisting">    // Compute Laplacian using LaplacianZC class 
    LaplacianZC laplacian; 
    laplacian.setAperture(7); // 7x7 laplacian 
    cv::Mat flap= laplacian.computeLaplacian(image); 
    laplace= laplacian.getLaplacianImage(); 
</pre><p class="calibre8">The resulting image is shown here:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_06_023.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec134" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">Formally, the Laplacian of a 2D function is defined as the sum of its second derivatives:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_06_28-1-300x80.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">In its simplest form, it can be approximated by the following <code class="literal">3x3</code> kernel:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/B05388_06_38.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">As for the Sobel operator, it is also possible to compute the Laplacian using larger kernels, and since this operator is even more sensitive to image noise, it is desirable to do so (unless computational efficiency is a concern). Since these larger kernels are computed using the second derivatives of the Gaussian function, the corresponding operator is often called <strong class="calibre15">Laplacian of Gaussian</strong> (<strong class="calibre15">LoG</strong>). Note that the kernel values of a Laplacian always sum up to <code class="literal">0</code>. This guarantees that the Laplacian will be zero in areas of constant intensities. Indeed, since the Laplacian measures the curvature of the image function, it should be equal to <code class="literal">0</code> on flat areas.</p><p class="calibre8">At first glance, the effect of the Laplacian might be difficult to interpret. From the definition of the kernel, it is clear that any isolated pixel value (that is, a value that's very different from its neighbors) will be amplified by the operator. This is a consequence of the operator's high sensitivity to noise. However, it is more interesting to look at the Laplacian values around an image edge. The presence of an edge in an image is the result of a rapid transition between areas of different gray-level intensities. Following the evolution of the image function along an edge (for example, caused by a transition from dark to bright), one can observe that the gray-level ascension necessarily implies a gradual transition from a positive curvature (when the intensity values start to rise) to a negative curvature (when the intensity is about to reach its high plateau). Consequently, a transition between a positive and a negative Laplacian value (or reciprocally) constitutes a good indicator of the presence of an edge. Another way to express this fact is to say that edges will be located at the zero-crossings of the Laplacian function. We will illustrate this idea by looking at the values of a Laplacian inside a small window of our test image. We select one that corresponds to an edge created by the bottom part of the roof of one of the castle's tower. A white box has been drawn in the following image to show you the exact location of this region of interest:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_06_025.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The following figure shows the numerical values (divided by <code class="literal">100</code>) of the Laplacian image (<code class="literal">7x7</code> kernel) inside the selected window:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_06_026.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">If, as illustrated, you carefully follow some of the zero-crossings of the Laplacian (located between pixels of different signs), you obtain a curve that corresponds some of the edges that is visible in the image window. In the preceding figure, we drew lines along the zero-crossings that correspond to the edge of the tower that is visible in the selected image window. This implies that, in principle, you can even detect the image edges at sub-pixel accuracy.</p><p class="calibre8">Following the zero-crossing curves in a Laplacian image is a delicate task. However, a simplified algorithm can be used to detect the approximate zero-crossing locations. This one proceeds by first thresholding the Laplacian at <code class="literal">0</code> so that a partition separating the positive and negative values is obtained. The limits between these two partitions then correspond to our zero-crossings. Therefore, we use a morphological operation to extract these contours, that is, we subtract the dilated image from the Laplacian image (this is the Beucher gradient presented in the <em class="calibre16">Applying morphological operators on gray-level images</em> recipe in 
<a href="ch05.html" title="Chapter 5. Transforming Images with Morphological Operations">Chapter 5</a>
, <em class="calibre16">Transforming Images with Morphological Operations</em>). This algorithm is implemented by the following method, which generates a binary image of zero-crossings:</p><pre class="programlisting">    // Get a binary image of the zero-crossings 
    // laplacian image should be CV_32F 
    cv::Mat getZeroCrossings(cv::Mat laplace) { 
      // threshold at 0 
      // negative values in black 
      // positive values in white 
      cv::Mat signImage; 
      cv::threshold(laplace,signImage,0,255,cv::THRESH_BINARY); 
 
      // convert the +/- image into CV_8U 
      cv::Mat binary; 
      signImage.convertTo(binary,CV_8U); 
      // dilate the binary image of +/- regions 
      cv::Mat dilated; 
      cv::dilate(binary,dilated,cv::Mat()); 
 
      // return the zero-crossing contours 
      return dilated-binary; 
    } 
</pre><p class="calibre8">The result is the following binary map:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_06_027.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">As you can see, the zero-crossings of the Laplacian detect all edges. No distinction is made between strong edges and weaker edges. We also mentioned that the Laplacian is very sensitive to noise. Also, it is interesting to note that some of the visible edges are due to compression artifacts. All these factors explain why so many edges are detected by the operator. In practice, the Laplacian is only used in conjunction with other operators to detect edges (for example, edges can be declared at zero-crossing locations of strong gradient magnitude). We will also learn in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <em class="calibre16">Detecting Interest Points</em>, that the Laplacian and other second-order operators are very useful in order to detect interest points at multiple scales.</p></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec135" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">The Laplacian is a high-pass filter but, interestingly, it is possible to approximate it by using a combination of low-pass filters. But before exploring this aspect, let's have a word about image enhancement, which is a topic we have already discussed, in 
<a href="ch02.html" title="Chapter 2. Manipulating Pixels">Chapter 2</a>
, <em class="calibre16">Manipulating Pixels</em>.</p><div><div><div><div><h3 class="title3"><a id="ch06lvl3sec29" class="calibre6"/>Enhancing the contrast of an image using the Laplacian</h3></div></div></div><p class="calibre8">The contrast of an image can be enhanced by subtracting its Laplacian from it. This is what we did in the <em class="calibre16">Scanning an image with neighbor access</em> recipe of 
<a href="ch02.html" title="Chapter 2. Manipulating Pixels">Chapter 2</a>
, <em class="calibre16">Manipulating Pixels</em>, where we introduced the kernel:</p><p class="calibre8">
</p><div><img alt="Enhancing the contrast of an image using the Laplacian" src="img/B05388_06_39.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">This is equal to 1 minus the Laplacian kernel (that is, the original image minus its Laplacian).</p></div><div><div><div><div><h3 class="title3"><a id="ch06lvl3sec30" class="calibre6"/>Difference of Gaussians</h3></div></div></div><p class="calibre8">The Gaussian filter presented in the first recipe of this chapter extracts the low frequencies of an image. We learned that the range of frequencies that are filtered by a Gaussian filter depend on the parameter σ, which controls the width of the filter. Now, if we subtract the two images that result from the filtering of an image by two Gaussian filters of different bandwidths, then the resulting image will be composed of those higher frequencies that one filter has preserved, and not the other. This operation is called <strong class="calibre15">Difference of Gaussians</strong> (<strong class="calibre15">DoG</strong>) and is computed as follows:</p><pre class="programlisting">    cv::GaussianBlur(image,gauss20,cv::Size(),2.0); 
    cv::GaussianBlur(image,gauss22,cv::Size(),2.2); 
 
    // Compute a difference of Gaussians 
    cv::subtract(gauss22, gauss20, dog, cv::Mat(), CV_32F); 
 
    // Compute the zero-crossings of DoG 
    zeros= laplacian.getZeroCrossings(dog); 
</pre><p class="calibre8">The last line of code computes the zero-crossings of the <code class="literal">DoG</code> operator. It results in the following image:</p><p class="calibre8">
</p><div><img alt="Difference of Gaussians" src="img/image_06_028.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">In fact, it can be demonstrated that with the proper choice of σ values, <code class="literal">DoG</code> operators can constitute a good approximation of LoG filters. Also, if you compute a series of difference of Gaussians from consecutive pair values in an increasing sequence of σ values, you obtain a scale-space representation of the image. This multiscale representation is useful, for example, for scale-invariant image feature detection, as will be explained in<a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <em class="calibre16">Detecting Interest Points</em>.</p></div></div><div><div><div><div><h2 class="title2"><a id="ch06lvl2sec136" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">Detecting scale-invariant features</em> recipe in <a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <em class="calibre16">Detecting Interest Points</em>, uses the Laplacian and DoG for the detection of scale-invariant features</li></ul></div></div></div></body></html>