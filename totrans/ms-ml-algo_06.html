<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Hebbian Learning and Self-Organizing Maps</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we're going to introduce the concept of Hebbian learning, based on the methods defined by the psychologist Donald Hebb. These theories immediately showed how a very simple biological law is able to describe the behavior of multiple neurons in achieving complex goals and was a pioneering strategy that linked the research activities in the fields of artificial intelligence and computational neurosciences.</p>
<p class="mce-root">In particular, we are going to discuss the following topics:</p>
<ul>
<li>The Hebb rule for a single neuron, which is a simple but biologically plausible behavioral law</li>
<li>Some variants that have been introduced to overcome a few stability problems</li>
<li>The final result achieved by a Hebbian neuron, which consists of computing the first principal component of the input dataset</li>
<li>Two neural network models (Sanger's network and Rubner-Tavan's network) that can extract a generic number of principal components</li>
<li>The concept of <strong>Self-Organizing Maps</strong> (<strong>SOMs</strong>) with a focus on the Kohonen Networks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hebb's rule</h1>
                </header>
            
            <article>
                
<p><strong>Hebb's rule</strong> has been proposed as a conjecture in 1949 by the Canadian psychologist Donald Hebb to describe the synaptic plasticity of natural neurons. A few years after its publication, this rule was confirmed by neurophysiological studies, and many research studies have shown its validity in many application, of Artificial Intelligence. Before introducing the rule, it's useful to describe the generic Hebbian neuron, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4985e161-7f26-46bd-b3bf-7fc4bb204dab.png" style="width:11.25em;height:4.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Generic Hebbian neuron with a vectorial input</div>
<p>The neuron is a simple computational unit that receives an input vector <em>x</em>, from the pre-synaptic units (other neurons or perceptive systems) and outputs a single scalar value, <em>y</em>. The internal structure of the neuron is represented by a weight vector, <em>w,</em> that models the strength of each synapse. For a single multi-dimensional input, the output is obtained as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/40a04e23-dfd5-421c-a65d-fefd03aa791a.png" style="width:5.92em;height:1.42em;"/></div>
<p>In this model, we are assuming that each input signal is encoded in the corresponding component of the vector, <em>x</em>; therefore, <em>x<sub>i</sub></em> is processed by the synaptic weight <em>w<sub>i,</sub></em> and so on. In the original version of Hebb's theory, the input vectors represent neural firing rates, which are always non-negative. This means that the synaptic weights can only be strengthened (the neuroscientific term for this phenomenon is <strong>long-term potentiation</strong> (<strong>LTP</strong>)). However, for our purposes, we assume that <em>x</em> is a real-valued vector, <span>as is <em>w</em></span>. This condition allows modeling more artificial scenarios without a loss of generality. </p>
<p>The same operation performed on a single vector holds when it's necessary to process many input samples organized in a matrix. If we have <em>N</em> m-dimensional input vectors, the formula becomes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/17fb5f3f-1296-4d5a-9250-2054a211817c.png" style="width:27.17em;height:1.67em;"/></div>
<p>The basic form of Hebb's rule in a discrete form can be expressed (for a single input) as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6b07422b-adc1-44d4-b098-d10f7539e46e.png" style="width:12.92em;height:1.58em;"/></div>
<p class="mce-root">The weight correction is hence a vector that has the same orientation of <em>x</em> and magnitude equal to <em>|x|</em> multiplied by a positive parameter, <em>η</em>, which is called the learning rate and the corresponding output, <em>y</em> (which can have either a positive or a negative sign). The sense <span>of <em>Δw</em> is determined by the sign of <em>y</em>; therefore, under the assumption that <em>x</em> and <em>y</em> are real values,</span> two different scenarios arise from this rule:</p>
<ul>
<li>If <em>x<sub>i</sub></em> &gt; 0 (&lt; 0) and <em>y</em> &gt; 0 (&lt; 0), <em>w<sub>i</sub></em> is strengthened</li>
<li>If <em>x<sub>i</sub></em> &gt; 0 (&lt; 0) and <em>y</em> &lt; 0 (&gt; 0), <em>w<sub>i</sub></em> is weakened</li>
</ul>
<p>It's easy to understand this behavior considering two-dimensional vectors:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/172d792a-ded9-46e4-8f33-ef7367a00529.png" style="width:25.08em;height:1.58em;"/></div>
<p>Therefore, if the initial angle <em>α</em> between <em>w</em> and <em>x</em> is less than 90°, <em>w</em> will have the same orientation of <em>x</em> and viceversa if <span><em>α</em> is greater than 90°. In the following diagram, there's a schematic representation of this process:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ba5db290-9b0f-434f-87fd-9837830b0738.png" style="width:38.75em;height:21.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Vectorial analysis of Hebb's rule</div>
<p>It's possible to simulate this behavior using a very simple Python snippet. Let's start with a scenario where <span><em>α</em> is less than 90° and 50 iterations:</span></p>
<pre>import numpy as np<br/><br/>w = np.array([1.0, 0.2])<br/>x = np.array([0.1, 0.5])<br/>alpha = 0.0<br/><br/>for i in range(50):<br/>    y = np.dot(w, x.T)<br/>    w += x*y<br/>    alpha = np.arccos(np.dot(w, x.T) / (np.linalg.norm(w) * np.linalg.norm(x)))<br/><br/>print(w)<br/>[  8028.48942243  40137.64711215]<br/><br/>print(alpha * 180.0 / np.pi)<br/>0.00131766983584</pre>
<p>As expected, the final angle, <span><em>α,</em> is close to zero and <em>w</em> has the same orientation and sense of <em>x</em>. We can now repeat the experiment with <em>α</em> greater than 90° (we change only the value of <em>w</em> because the procedure is the same):</span></p>
<pre>w = np.array([1.0, -1.0])<br/><br/>...<br/><br/>print(w)<br/>[-16053.97884486 -80275.89422431]<br/><br/>print(alpha * 180.0 / np.pi)<br/>179.999176456</pre>
<p>In this case, the final <span>angle,</span> <span><em>α,</em> is about 180° and, of course, <em>w</em> has the opposite sense with respect to <em>x</em>.</span></p>
<p>The scientist S. Löwel expressed this concept with the famous sentence:</p>
<div class="packt_quote">"<em>Neurons that fire together wire together</em>"</div>
<p>We can re-express this concept (adapting it to a machine learning scenario) by saying that the main assumption of this approach is based on the idea that when pre- and post-synaptic units are coherent (their signals have the same sign), the connection between neurons becomes stronger and stronger. On the other side, if they are discordant, the corresponding synaptic weight is decreased. For the sake of precision, if <em>x</em> is a spiking rate, it should be represented as a real function <em>x(t)</em> as well as <em>y(t)</em>. According to the original Hebbian theory, the discrete equation must be replaced by a differential equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c7eb6471-fb14-428f-bfa3-bf53dff22bc1.png" style="width:5.50em;height:2.67em;"/></div>
<p>If <em>x(t)</em> and <em>y(t)</em> have the same fire rate, the synaptic weight is strengthened proportionally to the product of both rates. If instead there's a relatively long delay between the pre-synaptic activity <em>x(t)</em> and the post-synaptic one <em>y(t)</em>, the corresponding weight is weakened. This is a more biologically plausible explanation of the relation <em>fire together → wire together</em>. </p>
<p>However, even if the theory has a strong neurophysiological basis, some modifications are necessary. In fact, it's easy to understand that the resulting system is always unstable. If two inputs are repeatedly applied (both real values and firing rates), the norm of the vector, w, grows indefinitely and this isn't a plausible assumption for a biological system. In fact, if we consider a discrete iteration step, we have the following equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/118b6d4e-b823-409d-878f-23b62659bfd5.png" style="width:57.42em;height:2.17em;"/></div>
<p>The previous output, <em>y<sub>k,</sub></em> is always multiplied by a factor greater than <em>1</em> (except in the case of null input), therefore it grows without a bound. As <em>y = w · x</em>, this condition implies that the magnitude of <em>w</em> increases (or remains constant if the magnitude of <em>x</em> is null) at each iteration (a more rigorous proof can be easily obtained considering the original differential equation).</p>
<p>Such a situation is not only biologically unacceptable, but it's also necessary to properly manage it in machine learning problems in order to avoid a numerical overflow after a few iterations. In the next paragraph, we're going to discuss some common methods to overcome this issue. For now, we can continue our analysis without introducing a correction factor.</p>
<p>Let's now consider a dataset, <em>X</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/aaf67da3-266b-44a7-8ecd-796b0f33f4b8.png" style="width:21.58em;height:1.58em;"/></div>
<p>We can apply the rule iteratively to all elements, but it's easier (and more useful) to average the weight modifications over the input samples (the index now refers to the whole specific vector, not to the single components):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/750fa24e-dfdb-40fd-8a83-5db4126319a6.png" style="width:33.33em;height:3.75em;"/></div>
<p>In the previous formula, <em>C</em> is the input correlation matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fc8af2b6-bb65-4941-83b5-a2d0f51b8b90.png" style="width:35.17em;height:8.08em;"/></div>
<p>For our purposes, however, it's useful to consider a slightly different Hebbian rule based on a threshold <em>θ</em> for the input vector (there's also a biological reason that justifies this choice, but it's beyond the scope of this book; the reader who is interested can find it in <em>Theoretical Neuroscience</em>,<em> </em><em>Dayan P.</em>, <em>Abbott F. L.</em>, <em>The MIT Press</em>).</p>
<p>It's easy to understand that in the original theory where <em>x(t)</em> and <em>y(t)</em> are firing rates, this modification allows a phenomenon opposite to LTP and called <strong>long-term depression</strong> (<strong>LTD</strong>). In fact, when <em>x(t) &lt; </em><span><em>θ</em> and <em>y(t)</em> is positive, the product <em>(x(t) - θ)y(t)</em> is negative and the synaptic weight is weakened.</span></p>
<p>If we set <em>θ = 〈x〉 ≈ E[X]</em>, we can derive an expression very similar to the previous one, but based on the input covariance matrix (unbiased through the Bessel's correction):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/55550e5b-8d33-4cb3-9a9b-d8ccb94e8a17.png" style="width:68.08em;height:10.92em;"/></div>
<p>For obvious reasons, this variant of the original Hebb's rule is called the <strong>covariance rule</strong>.</p>
<div class="packt_infobox">It's also possible to use the <strong><span>Maximum Likelihood Estimation</span></strong> (<strong>MLE</strong>) (or biased) covariance matrix (dividing by <em>N</em>), but it's important to check which version is adopted by the mathematical package that is employed. When using NumPy, it's possible to decide the version using the <kbd>np.cov()</kbd> <span>function </span>and setting the <kbd>bias=True/False</kbd> <span>parameter </span>(the default value is <kbd>False</kbd>). However, when <em>N &gt;&gt; 1</em>, the difference between versions decreases and can often be discarded. In this book, we'll use the unbiased version. The reader who wants to see further details about the Bessel's correction can read <em>Applied Statistics</em>,<em> </em><em>Warner R.</em>, <em>SAGE Publications</em>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analysis of the covariance rule</h1>
                </header>
            
            <article>
                
<p>The covariance matrix <span><em>Σ</em> is real and symmetric. If we apply the eigendecomposition, we get (for our purposes it's more useful to keep <em>V<sup>-1</sup></em> instead of the simplified version <em>V<sup>T</sup></em>):</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1c15dfbc-e259-44ff-9b0e-76284886dfbc.png" style="width:7.00em;height:1.33em;"/></div>
<p><em>V</em> is an orthogonal matrix (thanks to the fact that <span><em>Σ</em> is symmetric</span>) containing the eigenvectors of <span><em>Σ</em> (as columns), while <em>Ω</em> is a diagonal matrix containing the eigenvalues. Let's suppose we sort both eigenvalues (<em>λ<sub>1</sub></em>, <em>λ<sub>2</sub></em>, ..., <em>λ<sub>m</sub></em>) and the corresponding eigenvectors (<em>v<sub>1</sub></em>, <em>v<sub>2</sub></em>, ..., <em>v<sub>m</sub></em>) so that:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/87a4b53a-2784-44dd-9477-31a10ae287ea.png" style="width:11.33em;height:1.42em;"/></div>
<p>Moreover, let's suppose that <em><span>λ</span></em><sub><em>1</em> </sub>is dominant over all the other eigenvalues (it's enough that <em>λ<sub>1</sub> &gt; <span>λ</span><sub>i</sub></em> with <em>i ≠ 1</em>). As the eigenvectors are orthogonal, they constitute a basis and it's possible to express the vector w, with a linear combination of the eigenvectors:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5e779900-d7e2-45b3-9f33-78ee8de613b1.png" style="width:23.83em;height:1.50em;"/></div>
<p>The vector <em>u</em> contains the coordinates in the new basis. Let's now consider the modification to the covariance rule:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1f0dcb94-6c16-46d8-8a28-6e07331f23d4.png" style="width:24.92em;height:1.67em;"/></div>
<p class="mce-root">If we apply the rule iteratively, we get a matrix polynomial:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d8998d91-ae5b-442e-b5ce-f460f2343956.png" style="width:62.50em;height:10.67em;"/></div>
<p>Exploiting the Binomial theorem and considering that<span> <em>Σ</em></span><em><sup>0</sup><span>=I</span></em>, we can get a general expression for <em>w<sup>(k)</sup></em> as a function of <em><span>w</span><sup>(0)</sup></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/153067b7-ec73-4ae7-8012-40e7920f5964.png" style="width:16.00em;height:4.83em;"/></div>
<p>Let's now rewrite the previous formula using the change of basis:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5b09938a-c336-43f8-8f18-c8fd5440a388.png" style="width:47.50em;height:4.83em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">The vector <em>u<sup>(0)</sup></em> contains the coordinates of <em>w<sup>(0)</sup></em> in the new basis; hence, <em><span>w</span><sup>(k)</sup></em> is expressed as a polynomial where the generic term is proportional to <em>VΩ<sup>i</sup><span>u</span><sup>(0)</sup></em>.</p>
<p>Let's now consider the diagonal matrix <em>Ω<sup>k</sup></em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ecf21a99-c3ee-404f-b495-1a2ac596daef.png" style="width:23.83em;height:6.25em;"/></div>
<p class="mce-root">The last step derives from the hypothesis that <em>λ</em><sub><em>1</em> </sub>is greater than any other eigenvalue and when <em>k → ∞</em>, all <em>λ<sub>i≠1</sub><sup>k</sup><sub> </sub>&lt;&lt; <span>λ</span><sub>1</sub><sup>k</sup></em>. Of course, if <em><span>λ</span><sub>i</sub><sub>≠1</sub> &gt; 1, <span>λ</span><sub>i≠1</sub></em><sup><em>k</em> </sup>will grow as well as <em><span>λ</span><sub>1</sub><sup>k </sup></em>however, the contribution of the <em>secondary</em> eigenvalues to <em>w<sup>(k)</sup></em> becomes significantly weaker when <span><em>k → ∞</em>. Just to understand the validity of this approximation, let's consider the following situation where <em>λ<sub>1</sub></em> is slightly larger that <em>λ<sub>2</sub></em>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/937e3fe5-ece3-4b89-8507-2c80a0e1aeba.png" style="width:31.33em;height:3.50em;"/></div>
<p>The result shows a very important property: not only is the approximation correct, but as we're going to show, if an eigenvalue <em>λ<sub>i</sub></em> is larger than all the other ones, the covariance rule will always converge to the corresponding eigenvector <em>v<sub>i</sub></em>. No other stable fixed points exist! </p>
<p><span>This hypothesis is no more valid if <em>λ<sub>1</sub> = λ<sub>2</sub> = ... = λ<sub>n</sub></em>. In this case, the total variance is explained equally by the direction of each eigenvector (a condition that implies a symmetry which isn't very common in real-life scenarios). This situation can also happen when working with finite-precision arithmetic, but in general, if the difference between the largest eigenvalue and the second one is less than the maximum achievable precision (for example, 32-bit floating point), it's plausible to accept the equality.</span></p>
<p><span>Of course, we assume that the dataset is not whitened, because our goal (also in the next paragraphs) is to reduce the original dimensionality considering only a subset of components with the highest total variability (the decorrelation, like in <strong>Principal Component Analysis</strong> (<strong>PCA</strong>), must be an outcome of the algorithm, not a precondition). On the other side, zero-centering the dataset could be useful, even if not really necessary for this kind of algorithm.</span></p>
<p class="mce-root">If we rewrite the expression for <em>w<sub>k</sub></em> considering this approximation, we obtain the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/573a41b5-7c5b-4984-9ed6-cafbe8c9e831.png" style="width:62.50em;height:13.17em;"/></div>
<p>As <em>a<sub>1</sub>v + a<sub>2</sub>v + ... + a<sub>k</sub>v ∝ v</em>, this result shows that, <span>when <em>k → ∞</em>,</span> <em>w<sub>k</sub></em> will become proportional to the first eigenvector of the covariance matrix <em>Σ</em> (if <em>u<sub>1</sub><sup>(0)</sup></em> is not null) and its magnitude, without normalization, will grow indefinitely<span>.</span> The spurious effect due to the other eigenvalues becomes negligible (above all, if <em>w</em> is divided by its norm, so that the length is always <em>||w|| = 1</em>) after a limited number of iterations.</p>
<p>However, before drawing our conclusions, an important condition must be added:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/885c829c-9e7c-416e-9cf5-e0ec02e1586c.png" style="width:8.17em;height:2.08em;"/></div>
<p>In fact, if <em>w(0)</em> were orthogonal to <em>v1</em>, we would get (the eigenvectors are orthogonal to each other):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/231720a5-186b-414f-8052-d8ca438252f3.png" style="width:62.50em;height:7.83em;"/></div>
<p>This important result shows how a Hebbian neuron working with the covariance rule is able to perform a PCA limited to the first component without the need for eigendecomposing <em>Σ</em>. In fact, the vector <em>w</em> (we're not considering the problem of the increasing magnitude, which can be easily managed) will rapidly converge to the orientation where the input dataset <em>X</em> has the highest variance. In <a href="8d541a43-8790-4a91-a79b-e48496f75d90.xhtml" target="_blank">Chapter 5</a>, <em>EM Algorithm and Applications,</em> we discussed the details of PCA; in the next paragraph, we're going to discuss a couple of methods to find the first N principal components using a variant of the Hebb's rule.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of covariance rule application</h1>
                </header>
            
            <article>
                
<p>Before moving on, let's simulate this behavior with a simple Python example. We first generate <kbd>1000</kbd> values sampled from a bivariate Gaussian distribution (the variance is voluntarily asymmetric) and then we apply the covariance rule to find the first principal component (<em><span>w</span></em><sup><em>(0)</em> </sup>has been chosen so not to be orthogonal to <em>v<sub>1</sub></em>):</p>
<pre>import numpy as np<br/><br/>rs = np.random.RandomState(1000)<br/>X = rs.normal(loc=1.0, scale=(20.0, 1.0), size=(1000, 2))<br/><br/>w = np.array([30.0, 3.0])<br/><br/>S = np.cov(X.T)<br/><br/>for i in range(10):<br/>    w += np.dot(S, w)<br/>    w /= np.linalg.norm(w)<br/>    <br/>w *= 50.0<br/><br/>print(np.round(w, 1))<br/>[ 50.  -0.]</pre>
<p>The algorithm is straightforward, but there are a couple of elements that we need to comment on. The first one is the normalization of vector <em>w</em> at the end of each iteration. This is one of the techniques needed to avoid the uncontrolled growth of <em>w</em>. The second <em>tricky</em> element is the final multiplication, <em>w • 50</em>. As we are multiplying by a positive scalar, the direction of <em>w</em> is not impacted, but it's easier to show the vector in the complete plot.</p>
<p>The result is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e48fbb26-9d2b-42eb-920b-060cd073f118.png" style="width:53.83em;height:29.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Application of the covariance rule. w<sub>∞</sub> becomes proportional to the first principal component</div>
<p>After a limited number of iterations, <em>w<sub>∞</sub></em> has the same orientation of the principal eigenvector which is, in this case, parallel to the <em>x</em> axes. The sense depends on the initial value <em>w<sub>0</sub></em>; however, in a PCA, this isn't an important element.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weight vector stabilization and Oja's rule</h1>
                </header>
            
            <article>
                
<p>The easiest way to stabilize the weight vector is normalizing it after each update. In this way, its length will be always kept equal to one. In fact, in this kind of neural networks we are not interested in the magnitude, but only in the direction (that remains unchanged after the normalization). However, there are two main reasons that discourage this approach:</p>
<ul>
<li>It's non-local. To normalize vector <em>w</em>, we need to know all its values and this isn't biologically plausible. A real synaptic weight model should be self-limiting, without the need to have access to external pieces of information that cannot be available.</li>
<li>The normalization must be performed after having applied the correction and hence needs a double iterative step.</li>
</ul>
<p>In many machine learning contexts, these conditions are not limiting and they can be freely adopted, but when it's necessary to work with neuroscientific models, it's better to look for other solutions. In a discrete form, we need to determine a correction term for the standard Hebb's rule:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ffdb9cc5-e9d4-4945-b336-68dd948cea74.png" style="width:19.08em;height:1.67em;"/></div>
<p>The <em>f</em> <span>function </span>can work both as a local and non-local normalizer. An example of the first type is <strong>Oja's rule</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e3bda4b6-7a4b-4f1a-9062-c7f2e2ab741c.png" style="width:11.58em;height:1.58em;"/></div>
<p>The <em>α</em> parameter is a positive number that controls the strength of the normalization. A non-rigorous proof of the stability of this rule can be obtained considering the condition:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/37414372-7223-4fdb-b431-147780383d76.png" style="width:24.75em;height:1.67em;"/></div>
<p>The second expression implies that:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/00011c8c-f57b-4ec0-bba1-b835ba71c37a.png" style="width:18.42em;height:2.75em;"/></div>
<p>Therefore, when <em>t → ∞</em>, the magnitude of the weight correction becomes close to zero and the length of the weight vector <em>w</em> will approach a finite limit value:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e9115aeb-7623-4d4e-aa59-f3fb96f7756a.png" style="width:9.25em;height:3.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sanger's network</h1>
                </header>
            
            <article>
                
<p>A <strong>Sanger's network</strong> is a neural network model for online <em>Principal Component</em> extraction proposed by T. D. Sanger in <em>Optimal Unsupervised Learning in a Single-Layer Linear Feedforward Neural Network</em>, <em>Sanger T. D.</em>, <em>Neural Networks</em>, <em>1989/2</em>. The author started with the standard version of Hebb's rule and modified it to be able to extract a variable number of principal components (<em>v<sub>1</sub>, v<sub>2</sub>, ..., v<sub>m</sub></em>) in descending order (<em>λ<sub>1</sub> &gt; λ<sub>2</sub> &gt; ... &gt; λ<sub>m</sub></em>). The resulting approach, which is a natural extension of Oja's rule, has been called the <strong>Generalized Hebbian Rule</strong> (<strong>GHA</strong>) (or Learning). The structure of the network is represented in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/922bd024-d3e8-440d-92f9-b7211b43f136.png" style="width:26.42em;height:16.67em;"/></div>
<p>The network is fed with samples extracted from an n-dimensional dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5438be6b-e7b4-4d77-a244-27166015c419.png" style="width:24.75em;height:1.83em;"/></div>
<p>The <em>m</em> output neurons are connected to the input through a weight matrix, <em>W = {w<sub>ij</sub>},</em> where the first index refers to the input components (pre-synaptic units) and the second one to the neuron. The output of the network can be easily computed with a scalar product; however, in this case, we are not interested in it, because just like for the covariance (and Oja's) rules, the principal components are extracted through the weight updates.</p>
<p>The problem that arose after the formulation of Oja's rule was about the extraction of multiple components. In fact, if we applied the original rule to the previous network, all weight vectors (the rows of <em>w</em>) would converge to the first principal component. The main idea (based on the <strong>Gram-Schmidt</strong> orthonormalization method) to overcome this limitation is based on the observation that once we have extracted the first component <em>w<sub>1</sub></em>, the second one <em>w<sub>2</sub></em> can be forced to be orthogonal to <em>w<sub>1</sub></em>, the third one <em>w<sub>3</sub></em> can be forced to be orthogonal to <em>w<sub>1</sub></em> and <em>w<sub>2</sub></em>, and so on. Consider the following representation:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/17d5ae7b-0982-409f-800d-a40557680cc4.png" style="width:31.75em;height:11.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Orthogonalization of two weight vectors</div>
<p>In this case, we are assuming that <em>w<sub>1</sub></em> is stable and <em>w<sub>2</sub><sub>0</sub></em> is another weight vector that is converging to <em>w<sub>1</sub></em>. The projection of w<sub>20</sub> onto <em>w<sub>1</sub></em> is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a8f20651-fe3c-4e2a-aca2-69f43e36661e.png" style="width:17.50em;height:3.67em;"/></div>
<p>In the previous formula, we can omit the norm if we don't need to normalize (in the network, this process is done after a complete weight update). The orthogonal component of <em>w<sub>20</sub></em> is simply obtained with a difference:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/11870193-f3f7-464f-acbb-66481e7b485f.png" style="width:14.00em;height:1.83em;"/></div>
<p>Applying this method to the original Oja's rule, we obtain a new expression for the weight update (called Sanger's rule):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9062ac15-b758-4542-a3d7-d8c33994de1b.png" style="width:21.42em;height:4.92em;"/></div>
<p class="mce-root">The rule is referred to a single input vector <em>x</em>, hence <em>x<sub>j</sub></em> is the <em>j<sup>th</sup></em> component of <em>x</em>. The first term is the classic Hebb's rule, which forces weight <em>w</em> to become parallel to the first principal component, while the second one acts in a way similar to the Gram-Schmidt orthogonalization, by subtracting a term proportional to the projection of <em>w</em> onto all the weights connected to the previous post-synaptic units and considering, at the same time, the normalization constraint provided by Oja's rule (which is proportional to the square of the output).</p>
<p class="mce-root">In fact, expanding the last term, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a681a15a-0bea-46f0-86b6-d82361d03a2f.png" style="width:54.25em;height:4.75em;"/></p>
<p>The term subtracted to each component <em>w<sub>ij</sub></em> is proportional to all the components where the index <em>j</em> is fixed and the first index is equal to <em>1, 2, ..., i</em>. This procedure doesn't produce an immediate orthogonalization but requires several iterations to converge. The proof is non-trivial, involving convex optimization and dynamic systems methods, but, it can be found in the aforementioned paper. Sanger showed that the algorithm converges always to the sorted first <em>n</em> principal components (from the largest eigenvalue to the smallest one) if the <kbd>learning_rate</kbd> <em>η(t)</em> decreases monotonically and converges to zero when <em>t → ∞</em>. Even if necessary for the formal proof, this condition can be relaxed (a stable <em>η &lt; 1</em> is normally sufficient). In our implementation, matrix <em>W</em> is normalized after each iteration, so that, at the end of the process, <em>W<sup>T</sup></em> (the weights are in the rows) is orthonormal and constitutes a basis for the eigenvector subspace. </p>
<p>In matrix form, the rule becomes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3b86aee4-3330-452a-a4e0-ced608cb83b2.png" style="width:16.50em;height:1.83em;"/></div>
<p class="mce-root">Tril(•) is a matrix function that transforms its argument into a lower-triangular matrix and the term <em>yy<sup>T</sup></em> is equal to <em>Wxx<sup>T</sup>W</em>.</p>
<p>The algorithm for a Sanger's network is as follows:</p>
<ol>
<li>Initialize <em>W<sup>(0)</sup></em> with random values. If the input dimensionality is <em>n</em> and <em>m</em> principal components must be extracted, the shape will be <em>(m × n)</em>.</li>
<li>Set a <kbd>learning_rate</kbd> <em>η</em> (for example, <kbd>0.01</kbd>).</li>
<li>Set a <kbd>threshold</kbd> <em>Thr</em> (for example, <kbd>0.001</kbd>).</li>
<li>Set a counter <em>T = 0.</em></li>
<li>While <em>||W<sup>(t)</sup> - W<sup>(t-1)</sup>||<sub>F</sub> &gt; Thr</em>:
<ol>
<li>Set <span><em>ΔW = 0</em> (same shape of <em>W</em>)</span></li>
<li>For each <em>x</em> in <em>X</em>:
<ol>
<li>Set <em>T = T + 1</em> </li>
<li>Compute <em>y = W<sup>(t)</sup>x</em> </li>
<li>Compute and accumulate <em>ΔW += <span>η(yx<sup>T</sup> - Tril(yy<sup>T</sup>)W<sup>(t)</sup></span></em></li>
</ol>
</li>
<li> Update <em><span>W</span><sup>(t+1)</sup><span> = W</span><sup>(t)</sup> + (<span>η / T)ΔW</span></em></li>
<li>Set <em>W<sup>(t+1)</sup> = <span>W</span><sup>(t+1)</sup></em><span><em> / ||W<sup>(t+1)</sup>||<sup>(rows)</sup></em> (the norm must be computed row-wise)</span></li>
</ol>
</li>
</ol>
<p>The algorithm can also be iterated a fixed number of times (like in our example), or the two stopping approaches can be used together.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of Sanger's network</h1>
                </header>
            
            <article>
                
<p>For this Python example, we consider a bidimensional zero-centered dataset <kbd>X</kbd> with 500 samples (we are using the function defined in the first chapter). After the initialization of <kbd>X</kbd>, we also compute the eigendecomposition, to be able to double-check the result:</p>
<pre>import numpy as np<br/><br/>from sklearn.datasets import make_blobs<br/><br/>X, _ = make_blobs(n_samples=500, centers=2, cluster_std=5.0, random_state=1000)<br/>Xs = zero_center(X)<br/><br/>Q = np.cov(Xs.T)<br/>eigu, eigv = np.linalg.eig(Q)<br/><br/>print(eigu)<br/>[ 24.5106037   48.99234467]<br/><br/>print(eigv)<br/>[[-0.75750566 -0.6528286 ]
 [ 0.6528286  -0.75750566]]<br/><br/>n_components = 2<br/><br/>W_sanger = np.random.normal(scale=0.5, size=(n_components, Xs.shape[1]))<br/>W_sanger /= np.linalg.norm(W_sanger, axis=1).reshape((n_components, 1))</pre>
<p>The eigenvalues are in reverse order; therefore, we expect to have a final <em>W</em> with the rows swapped. The initial condition (with the weights multiplied by 15) is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8b8617f9-5a6f-47cc-a63e-4a13b0ae3871.png" style="width:42.33em;height:41.58em;"/></div>
<p class="mce-root">Dataset with <em>W</em> initial condition, we can implement the algorithm. For simplicity, we preferred a fixed number of iterations (<kbd>5000</kbd>) with a <kbd>learning_rate</kbd> of <em>η=0.01</em>. The reader can modify the snippet to stop when the weight matrix becomes stable:</p>
<pre>learning_rate = 0.01<br/>nb_iterations = 5000<br/>t = 0.0<br/><br/>for i in range(nb_iterations):<br/>    dw = np.zeros((n_components, Xs.shape[1]))<br/>    t += 1.0<br/>    <br/>    for j in range(Xs.shape[0]):<br/>        Ysj = np.dot(W_sanger, Xs[j]).reshape((n_components, 1))<br/>        QYd = np.tril(np.dot(Ysj, Ysj.T))<br/>        dw += np.dot(Ysj, Xs[j].reshape((1, X.shape[1]))) - np.dot(QYd, W_sanger)<br/>        <br/>    W_sanger += (learning_rate / t) * dw<br/>    W_sanger /= np.linalg.norm(W_sanger, axis=1).reshape((n_components, 1))</pre>
<p>The first thing to check is the final state of <em>W</em> (we transposed the matrix to be able to compare the columns):</p>
<pre>print(W_sanger.T)<br/>[[-0.6528286  -0.75750566]
 [-0.75750566  0.6528286 ]]</pre>
<p>As expected, <em>W</em> has converged to the eigenvectors of the input correlation matrix (the sign <kbd><em>–</em></kbd> which is associated with the sense of <em>w—</em>is not important because we care only about the orientation). The second eigenvalue is the highest, so the columns are swapped. Replotting the diagram, we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ae5146f7-2565-4816-becc-d82eb2f1ee16.png" style="width:41.33em;height:41.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Final condition, w has converged to the two principal components</div>
<p>The two components are perfectly orthogonal (the final orientations can change according to the initial conditions or the random state) and <em>w<sub>0</sub></em> points in the direction of the first principal component, while <em>w<sub>1</sub></em> points in the direction of the second component. Considering this nice property, it's not necessary to check the magnitude of the eigenvalues; therefore, this algorithm can operate without eigendecomposing the input covariance matrix. Even if a formal proof is needed to explain this behavior, it's possible to understand it intuitively. Every single neuron converges to the first principal component given a full eigenvector subspace. This property is always maintained, but after the orthogonalization, the subspace is implicitly reduced by a dimension. The second neuron will always converge to the first component, which now corresponds to the global second component, and so on.</p>
<p>One of the advantages of this algorithm (and also of the next one) is that a standard PCA is normally a bulk process (even if there are batch algorithms), while a Sanger's network is an online algorithm that is trained incrementally. In general, the time performance of a Sanger's network is worse than the direct approach because of the iterations (some optimizations can be achieved using more vectorization or GPU support). On the other side, a Sanger's network is memory-saving when the number of components is less than the input dimensionality (for example, the covariance matrix for <em>n=1000</em> has <em>10<sup>6</sup></em> elements, if <em>m = 100</em>, the weight matrix has <em>10<sup>4</sup></em> elements).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rubner-Tavan's network</h1>
                </header>
            
            <article>
                
<p>In <a href="8d541a43-8790-4a91-a79b-e48496f75d90.xhtml" target="_blank">Chapter 5</a>, <em>EM Algorithm and Applications,</em> we said that any algorithm that decorrelates the input covariance matrix is performing a PCA without dimensionality reduction. Starting from this approach, Rubner, and Tavan (in the paper <em>A Self-Organizing Network for Principal-Components Analysis</em>,<em> </em><em>Rubner J.</em>, <em>Tavan P.</em>, <em>Europhysics. Letters</em>, <em>10(7)</em>, <em>1989</em>) proposed a neural model whose goal is decorrelating the output components to force the consequent decorrelation of the output covariance matrix (in lower-dimensional subspace). Assuming a zero-centered dataset and <em>E[y] = 0</em>, the output covariance matrix for <em>m</em> principal components is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2d1f7b9f-24d5-4b1a-8917-70fc3898e4c9.png" style="width:27.08em;height:8.08em;"/></div>
<p>Hence, it's possible to achieve an approximate decorrelation, forcing the terms <em>y<sub>i</sub>y<sub>j</sub></em> with <em>i ≠ j</em> to become close to zero. The main difference with a standard approach (such as whitening or vanilla PCA) is that this procedure is local, while all the standard methods operate globally, directly with the covariance matrix. The neural model proposed by the authors is shown in the following diagram (the original model was proposed for binary units, but it works quite well also for linear ones):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e3075a27-0124-44c9-9c04-7508c476e208.png" style="width:27.83em;height:24.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Rubner-Tavan network. The connections v<sub>jk</sub> are based on the anti-Hebbian rule</div>
<p>The network has <em>m</em> output units and the last <em>m-1</em> neurons have a summing node that receives the weighted output of the previous units (hierarchical lateral connections). The dynamic is simple: the first output isn't modified. The second one is forced to become decorrelated with the first one. The third one is forced to become decorrelated with both the first and the second one and so on. This procedure must be iterated a number of times because the inputs are presented one by one and the cumulative term that appears in the correlation/covariance matrix (it's always easier to zero-center the dataset and work with the correlation matrix) must be implicitly split into its addends. It's not difficult to understand that the convergence to the only stable fixed point (which has been proven to exist by the authors) needs some iterations to correct the wrong output estimations.</p>
<p>The output of the network is made up of two contributions:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a9a5f767-e0a4-46cc-9e88-3973029b93c1.png" style="width:20.00em;height:5.00em;"/></div>
<p>The notation <em>y/x<sup>(i)</sup></em> indicates the <em>i<sup>th</sup></em> element of <em>y/x</em>. The first term produces a partial output based only on the input, while the second one uses hierarchical lateral connections to correct the values and enforce the decorrelation. The internal weights <em>w<sub>ij</sub></em> are updated using the standard version of Oja's rule (this is mainly responsible for the convergence of each weight vector to the first principal component):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e5cd6989-755d-4167-a085-1fbf5cbfc5f9.png" style="width:15.50em;height:1.83em;"/></div>
<p>Instead, the external weights <em>v<sub>jk</sub></em> are updated using an anti-Hebbian rule:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5c3b2608-4927-40c2-bf13-43fb8094fd6d.png" style="width:30.42em;height:1.83em;"/></div>
<p>The previous formula can be split into two parts: the first term <em>-ηy<sub>j</sub>y<sub>k</sub></em> acts in the opposite direction of a standard version of Hebb's rule (that's why it's called anti-Hebbian) and forces the decorrelation. The second one <em>-ηy<sub>j</sub>y<sub>k</sub>v<sub>jk</sub></em> acts as a regularizer and it's analogous to Oja's rule. The term <em>-<span>η</span>y<sub>j</sub>y<sub>k</sub></em> works as a feedback signal for the Oja's rule that readapts the updates according to the new magnitude of the actual output. In fact, after modifying the lateral connections, the outputs are also forced to change and this modification impacts on the update of <em>w<sub>ij</sub></em>. When all the outputs are decorrelated, the vectors <em>w<sub>i</sub></em> are implicitly obliged to be orthogonal. It's possible to imagine an analogy with the Gram-Schmidt orthogonalization, even if in this case the relation between the extraction of different components and the decorrelation is more complex. Like for Sanger's network, this model extracts the first <em>m</em> principal components in descending order (the reason is the same that has been intuitively explained), but for a complete (non-trivial) mathematical proof, please refer to the aforementioned paper.</p>
<p>If input dimensionality is <em>n</em> and the number of components is equal to <em>m</em>, it's possible to use a lower-triangular matrix <em>V (m × m)</em> with all diagonal elements set to <em>0</em> and a standard matrix for <em>W (n × m)</em>.</p>
<p>The structure of <em>W</em> is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c226c098-cf73-4aa3-a6a5-33ea8a83fcf0.png" style="width:14.33em;height:1.83em;"/></div>
<p>Therefore, <em>w<sub>i</sub></em> is a column-vector that must converge to the corresponding eigenvector. The structure of <em>V</em> is instead:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f9e7ed01-388d-4a44-8937-86da61b19c04.png" style="width:36.58em;height:8.25em;"/></div>
<p>Using this notation, the output becomes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/891105c6-b4ac-406d-b433-f0e1d4c2223a.png" style="width:14.50em;height:2.08em;"/></div>
<p>As the output is based on recurrent lateral connections, its value must be stabilized by iterating the previous formula for a fixed number times or until the norm between two consecutive values becomes smaller than a predefined threshold. In our example, we use a fixed number of iterations equal to five. The update rules cannot be written directly in matrix notation, but it's possible to use the vectors <em>w</em><sub><em>i</em> </sub>(columns) and <em>v</em><sub><em>j</em> </sub>(rows):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0d3a2fdb-8a2d-45fb-95f0-68f1f59c729e.png" style="width:18.42em;height:4.92em;"/></div>
<p class="mce-root">In this case, <em>y<sup>(i)</sup></em> means the <em>i<sup>th</sup></em> component of <em>y</em>. The two matrices must be populated with a loop.</p>
<p>The complete Rubner-Tavan's network algorithm is (the dimensionality of <em>x</em> is <em>n</em>, the number of components is denoted with <em>m</em>):</p>
<ol>
<li>Initialize  <em>W<sup>(0)</sup></em> randomly. The shape is <em>(n × m)</em>.</li>
<li>Initialize  <em>V<sup>(0)</sup></em> randomly. The shape is <em>(m <span>× m)</span></em>.</li>
<li>Set <em><span>V</span><sup>(0)</sup><sub> </sub>= Tril(<span>V</span><sup>(0)</sup>)</em>. <em>Tril(•)</em> transforms the input argument in a lower-triangular matrix.</li>
<li>Set all diagonal components of <em>V<sup>(0)</sup></em> equal to <em>0</em>.</li>
<li>Set the <kbd>learning_rate</kbd> <em>η</em> (for example, <kbd>0.001</kbd>).</li>
<li>Set a <kbd>threshold</kbd> <em>Thr</em> (for example, <kbd>0.0001</kbd>).</li>
<li>Set a cycle counter <em>T=0</em>.</li>
</ol>
<ol start="8">
<li>Set a maximum number of iterations <kbd>max_iterations</kbd> (for example, 1000).</li>
<li>Set a number of <kbd>stabilization_cycles</kbd> (for example, <kbd>5</kbd>):
<ol>
<li>While <em>||W<sup>(t)</sup> - W<sup>(t-1)</sup>||<sub>F</sub> &gt; Thr</em> and T &lt; <kbd>max_iterations</kbd>:
<ol>
<li>Set <em>T = T + 1</em>.</li>
<li>For each <em>x</em> in <em>X</em>:
<ol>
<li>Set <em>y<sub>prev</sub></em> to zero. The shape is (<em>m, 1</em>).</li>
<li>For <em>i=1</em> to <kbd>stabilization_cycles</kbd>:
<ol>
<li><em>y = W<sup>T</sup>x + <span>V</span><span>y</span><sub>prev</sub></em>.</li>
<li><em>y<sub>prev</sub> = y</em>.</li>
</ol>
</li>
<li>Compute the updates for <em>W</em> and <em>V</em>:
<ol>
<li>Create two empty matrices <em>ΔW (<span>n × m</span>)</em> and <em>ΔV (m<span> × m)</span></em></li>
<li>for <em>t=1</em> to <em>m</em>:<br/>
<ol>
<li><em>Δw<sub>t</sub> = ηy<sup>(t)</sup>(x - y<sup>(t)</sup>w<sub>t</sub>)</em></li>
<li><em><span>Δv<sub>t</sub> = -ηy<sup>(t)</sup>(y + y<sup>(t)</sup>v<sub>t</sub>)</span></em></li>
</ol>
</li>
<li>Update <em>W</em> and <em>V</em>:
<ol>
<li><em>W<sup>(t+1)</sup> = W<sup>(t)</sup> + <span>ΔW</span></em></li>
<li><em>V<sup>(t+1)</sup> = V<sup>(t)</sup> + <span>ΔV</span></em></li>
</ol>
</li>
<li>Set <em>V = Tril(V)</em> and set all the diagonal elements to <em>0</em></li>
<li>Set <em>W<sup>(t+1)</sup><span> </span>= <span>W</span><sup>(t+1)</sup></em><span><em> / ||W<sup>(t+1)</sup>||<sup>(columns)</sup></em><span> </span>(The norm must be computed column-wise)</span></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>In this case, we have adopted both a threshold and a maximum number of iterations because this algorithms normally converges very quickly. Moreover, I suggest the reader always checks the shapes of vectors and matrices when performing dot products.</p>
<div class="packt_infobox">In this example, as well as in all the other ones, the NumPy random seed is set equal to <kbd>1000</kbd> (<kbd>np.random.seed(1000)</kbd>). Using different values (or repeating more times the experiments without resetting the seed) can lead to slightly different results (which are always coherent). </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of Rubner-Tavan's network</h1>
                </header>
            
            <article>
                
<p>For our Python example, we are going to use the same dataset already created for the Sanger's network (which is expected to be available in the variable <kbd>Xs</kbd>). Therefore, we can start setting up all the constants and variables:</p>
<pre>import numpy as np<br/><br/>n_components = 2<br/>learning_rate = 0.0001<br/>max_iterations = 1000<br/>stabilization_cycles = 5<br/>threshold = 0.00001<br/><br/>W = np.random.normal(0.0, 0.5, size=(Xs.shape[1], n_components))<br/>V = np.tril(np.random.normal(0.0, 0.01, size=(n_components, n_components)))<br/>np.fill_diagonal(V, 0.0)<br/><br/>prev_W = np.zeros((Xs.shape[1], n_components))<br/>t = 0</pre>
<p>At this point, it's possible to implement the training loop:</p>
<pre>while(np.linalg.norm(W - prev_W, ord='fro') &gt; threshold and t &lt; max_iterations):<br/>    prev_W = W.copy()<br/>    t += 1<br/>    <br/>    for i in range(Xs.shape[0]):<br/>        y_p = np.zeros((n_components, 1))<br/>        xi = np.expand_dims(Xs[i], 1)<br/>        y = None<br/><br/>        for _ in range(stabilization_cycles):<br/>            y = np.dot(W.T, xi) + np.dot(V, y_p)<br/>            y_p = y.copy()<br/>            <br/>        dW = np.zeros((Xs.shape[1], n_components))<br/>        dV = np.zeros((n_components, n_components))<br/>        <br/>        for t in range(n_components):<br/>            y2 = np.power(y[t], 2)<br/>            dW[:, t] = np.squeeze((y[t] * xi) + (y2 * np.expand_dims(W[:, t], 1)))<br/>            dV[t, :] = -np.squeeze((y[t] * y) + (y2 * np.expand_dims(V[t, :], 1)))<br/><br/>        W += (learning_rate * dW)<br/>        V += (learning_rate * dV)<br/>        <br/>        V = np.tril(V)<br/>        np.fill_diagonal(V, 0.0)<br/>        <br/>        W /= np.linalg.norm(W, axis=0).reshape((1, n_components<strong>))</strong></pre>
<p>The final <kbd>W</kbd> and the output covariance matrix are as follows:</p>
<pre>print(W)<br/>[[-0.65992841  0.75897537]
 [-0.75132849 -0.65111933]]<br/><br/>Y_comp = np.zeros((Xs.shape[0], n_components))<br/><br/>for i in range(Xs.shape[0]):<br/>        y_p = np.zeros((n_components, 1))<br/>        xi = np.expand_dims(Xs[i], 1)<br/><br/>        for _ in range(stabilization_cycles):<br/>            Y_comp[i] = np.squeeze(np.dot(W.T, xi) + np.dot(V.T, y_p))<br/>            y_p = y.copy()<br/><br/>print(np.cov(Y_comp.T))<br/>[[ 48.9901765   -0.34109965]
 [ -0.34109965  24.51072811]]</pre>
<p>As expected, the algorithm has successfully converged to the eigenvectors (in descending order) and the output covariance matrix is almost completely decorrelated (the sign of the non-diagonal elements can be either positive or negative). Rubner-Tavan's networks are generally faster than Sanger's network, thanks to the feedback signal created by the anti-Hebbian rule; however, it's important to choose the right value for the learning rate. A possible strategy is to implement a temporal decay (as done in Sanger's network) starting with a value not greater than <kbd>0.0001</kbd>. However, it's important to reduce <em>η</em> when <em>n</em> increases (for example, <span><em>η = 0.0001 / n</em>), because the normalization strength of Oja's rule on the lateral connections <em>v<sub>jk</sub></em> is often not enough to avoid over and underflows when <em>n &gt;&gt; 1</em>. I don't suggest any extra normalization on <em>V</em> (which must be carefully analyzed considering that <em>V</em> is singular) because it can slow down the process and reduce the final accuracy.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Self-organizing maps</h1>
                </header>
            
            <article>
                
<p><strong>Self-organizing maps (SOMs)</strong> have been proposed by Willshaw and Von Der Malsburg (<em>Willshaw D. J., Von Der Malsburg C., How patterned neural connections can be set up by self-organization, Proceedings of the Royal Society of London, B/194, N. 1117</em>) to model different neurobiological phenomena observed in animals. In particular, they discovered that some areas of the brain develop structures with different areas, each of them with a high sensitivity for a specific input pattern. The process behind such a behavior is quite different from what<span> we have discussed up until now</span>, because it's based on competition among neural units based on a principle called <strong>winner-takes-all</strong>. During the training period, all the units are excited with the same signal, but only one will produce the highest response. This unit is automatically candidate to become the receptive basin for that specific pattern. The particular model we are going to present has been introduced by <strong>Kohonen</strong> (in the paper <em>Kohonen T., Self-organized formation of topologically correct feature maps, Biological Cybernetics, 43/1</em>) and it's named after him.</p>
<p>The main idea is to implement a gradual winner-takes-all paradigm, to avoid the premature convergence of a neuron (as a definitive winner) and increment the level of plasticity of the network. This concept is expressed graphically in the following graph (where we are considering a linear sequence of neurons):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/57a73e57-00b6-4470-b100-4cfb3fa1ac95.png" style="width:40.67em;height:24.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Mexican-hat dynamic implemented by a Kohonen network</div>
<p>In this case, the same pattern is presented to all the neurons. At the beginning of the training process (<strong>t=0</strong>), a positive response is observed in <strong>x<sub>i-2</sub></strong> to <strong>x<sub>i+2</sub></strong> with a peak in <strong>x<sub>i</sub></strong>. The potential winner is obviously <strong>x<sub>i</sub></strong>, but all these units are potentiated according to their distance from <strong>x<sub>i</sub></strong>. In other words, the network (which is trained sequentially) is still receptive to change if other patterns produce a stronger activation. If instead <strong>x<sub>i</sub></strong> keeps on being the winner, the radius is slightly reduced, until the only potentiated unit will be <strong>x<sub>i</sub></strong>. Considering the shape of this function, this dynamic is often called <em>Mexican Hat</em>. With this approach, the network remains plastic until all the patterns have been repeatedly presented. If, for example, another pattern elicits a stronger response in <strong>x<sub>i</sub></strong>, it's important that its activation is still not too high, to allow a fast reconfiguration of the network. At the same time, the new winner will probably be a neighbor of <strong>x<sub>i</sub></strong>, which has received a partial potentiation and can easily take the place of <strong>x<sub>i</sub></strong>.</p>
<p>A <strong>Kohonen SOM</strong> (also known as Kohonen network or simply Kohonen map) is normally represented as a bidimensional map (for example, a square matrix <em>m × m</em>, or any other rectangular shape), but 3D surfaces, such as spheres or toruses are also possible (the only necessary condition is the existence of a suitable metric). In our case, we always refer to a square matrix where each cell is a receptive neuron characterized by a synaptic weight <em>w</em> with the dimensionality of the input patterns:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dedd26ae-e2c3-4165-b46c-c3b3daa5b1e4.png" style="width:23.17em;height:1.67em;"/></div>
<p>During both training and working phases, the winning unit is determined according to a similarity measure between a sample and each weight vector. The most common metric is the Euclidean; hence, if we consider a bidimensional map <em>W</em> with a shape <em>(k × p)</em> so that <em>W ∈ ℜ<sup><span>k × p × n</span></sup></em><span>,</span> the winning unit (in terms of its coordinates) is computed as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b5c7c5c2-7139-45ad-a402-832a0386cf87.png" style="width:17.33em;height:1.58em;"/></div>
<p class="mce-root">As explained before, it's important to avoid the premature convergence because the complete final configuration could be quite different from the initial one. Therefore, the training process is normally subdivided into two different stages. During the first one, whose duration is normally about 10-20% of the total number of iterations (let's call this value <em>t<sub>max</sub></em>), the correction is applied to the winning unit and its neighbors (computed by adopting a decaying radius). Instead, during the second one, the radius is set to 1.0 and the correction is applied only to the winning unit. In this way, it's possible to analyze a larger number of possible configurations, automatically selecting the one associated with the least error. The neighborhood can have different shapes; it can be square (in closed 3D maps, the boundaries don't exist anymore), or, more easily, it's possible to employ a radial basis function based on an exponentially decaying distance-weight:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7966bd27-723f-4813-87ac-1097856d8226.png" style="width:31.75em;height:3.83em;"/></div>
<p>The relative weight of each neuron is determined by the <em>σ(t)</em>. <em><span>σ</span><sub>0 </sub></em><span>function</span> is the initial radius and <em>τ</em> is a time-constant that must be considered as a hyperparameter which determines the slope of the decaying weight. Suitable values are 5-10% of the total number of iterations. Adopting a radial basis function, it's not necessary to compute an actual neighborhood because the multiplication factor <em>n(i, j)</em> becomes close to zero outside of the boundaries. A drawback is related to the computational cost, which is higher than a square neighborhood (as the function must be computed for the whole map); however, it's possible to speed up the process by precomputing all the squared distances (the numerator) and exploiting the vectorization features offered by packages such as NumPy (a single exponential is computed every time).</p>
<p>The update rule is very simple and it's based on the idea to move the winning unit synaptic weights closer to the pattern, <em>x<sub>i,</sub></em> (repeated for the whole dataset, <em>X</em>):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/76b3c98b-fae1-47ca-a47f-68a63228bea4.png" style="width:16.92em;height:1.67em;"/></div>
<p>The <em>η(t)</em> function is the learning rate, which can be fixed, but it's preferable to start with a higher value, <em><span>η</span><sub>0 </sub></em>and let it decay to a target final value, <em><span>η<sub>∞</sub></span></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/59cd23a7-45d4-465e-a887-f2d7efc46a0a.png" style="width:18.92em;height:4.25em;"/></div>
<p>In this way, the initial changes force the weights to align with the input patterns, while all the subsequent updates allow slight modifications to improve the overall accuracy. Therefore, each update is proportional to the learning rate, the neighborhood weighted distance, and the difference between each pattern and the synaptic vector. Theoretically, if <em>Δw<sub>ij</sub></em> is equal to 0.0 for the winning unit, it means that a neuron has become the attractor of a specific input pattern, and its neighbors will be receptive to noisy/altered versions. The most interesting aspect is that the complete final map will contain the attractors for all patterns which are organized to maximize the similarity between adjacent units. In this way, when a new pattern is presented, the area of neurons that maps the most similar shapes will show a higher response. For example, if the patterns are made up of handwritten digits, attractors for the digit 1 and for digit 7 will be closer than the attractor, for example, for digit 8. A malformed 1 (which could be interpreted as 7) will elicit a response that is between the first two attractors, allowing us to assign a relative probability based on the distance. As we're going to see in the example, this feature yields to a smooth transition between different variants of the same pattern class avoiding rigid boundaries that oblige a binary decision (like in a K-means clustering or in a hard classifier). </p>
<p>The complete Kohonen SOM algorithm is as follows:</p>
<ol>
<li>Randomly initialize <em>W<sup>(0)</sup></em>. The shape is <em>(k <span>× p ×</span> n)</em>.</li>
<li>Initialize <kbd>nb_iterations</kbd>, the total number of iterations, and <em>t<sub>max</sub></em> (for example, <kbd>nb_iterations</kbd> = 1000 and <span>t</span><sub>max </sub>= 150).</li>
<li>Initialize <em>τ</em> (for example, <em>τ</em> = 100<span>).</span></li>
<li>Initialize <em>η<sub>0</sub></em> and <em>η<sub>∞</sub></em><span> (for example, <em>η</em><sub><em>0</em> </sub>= 1.0 and <em>η</em><sub><em>∞</em> </sub>= 0.05).</span></li>
<li>For <kbd>t = 0</kbd> to <kbd>nb_iterations</kbd>:<br/>
<ol>
<li>If <em>t &lt; t<sub>max</sub></em>:
<ol>
<li>Compute <em><span>η(t)</span></em></li>
<li>Compute <em><span>σ(t)</span></em></li>
</ol>
</li>
<li>Otherwise:
<ol>
<li>Set <em><span>η(t) = η<sub>∞</sub></span></em></li>
<li>Set <em><span>σ(t) = σ<sub>∞</sub></span></em></li>
</ol>
</li>
<li>For each <em>x<sub>i</sub></em> in <em>X</em>:
<ol>
<li>Compute the winning unit <em>u<sup>*</sup></em> (let's assume that the coordinates are <em>i</em>, <em>j</em>)</li>
<li>Compute <em>n(i, j)</em></li>
<li>Apply the weight correction <em><span>Δw</span><sub>ij</sub><sup><span>(t)</span></sup></em> to all synaptic weights <em><span>W</span><sup>(t)</sup></em></li>
</ol>
</li>
<li>Renormalize <em><span>W</span><sup>(t)</sup><span> </span><span>= </span><span>W</span><sup>(t)</sup></em><span><em> / ||W<sup>(t)</sup>||<sup>(columns)</sup></em> (the norm must be computed column-wise)</span></li>
</ol>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of SOM</h1>
                </header>
            
            <article>
                
<p>We can now implement an SOM using the Olivetti faces dataset. As the process can be very long, in this example we limit the number of input patterns to 100 (with a 5 × 5 matrix). The reader can try with the whole dataset and a larger map.</p>
<p>The first step is loading the data, normalizing it so that all values are bounded between 0.0 and 1.0, and setting the constants:</p>
<pre>import numpy as np<br/><br/>from sklearn.datasets import fetch_olivetti_faces<br/><br/>faces = fetch_olivetti_faces(shuffle=True)<br/><br/>Xcomplete = faces['data'].astype(np.float64) / np.max(faces['data'])<br/>np.random.shuffle(Xcomplete)<br/><br/>nb_iterations = 5000<br/>nb_startup_iterations = 500<br/>pattern_length = 64 * 64<br/>pattern_width = pattern_height = 64<br/>eta0 = 1.0<br/>sigma0 = 3.0<br/>tau = 100.0<br/><br/>X = Xcomplete[0:100]<br/>matrix_side = 5</pre>
<p>At this point, we can initialize the weight matrix using a normal distribution with a small standard deviation:</p>
<pre>W = np.random.normal(0, 0.1, size=(matrix_side, matrix_side, pattern_length))</pre>
<p>Now, we need to define the functions to determine the winning unit based on the least distance:</p>
<pre>def winning_unit(xt):<br/>    distances = np.linalg.norm(W - xt, ord=2, axis=2)<br/>    max_activation_unit = np.argmax(distances)<br/>    return int(np.floor(max_activation_unit / matrix_side)), max_activation_unit % matrix_side</pre>
<p>It's also useful to define the functions <span><em>η(t)</em> and <em>σ(t)</em></span>:</p>
<pre>def eta(t):<br/>    return eta0 * np.exp(-float(t) / tau)<br/><br/>def sigma(t):<br/>    return float(sigma0) * np.exp(-float(t) / tau)</pre>
<p>As explained before, instead of computing the radial basis function for each unit, it's preferable to use a precomputed distance matrix (in this case, <span>5 × 5 × 5 × 5) containing all the possible distances between couples of units. In this way, NumPy allows a faster calculation thanks to its vectorization features:</span></p>
<pre>precomputed_distances = np.zeros((matrix_side, matrix_side, matrix_side, matrix_side))<br/><br/>for i in range(matrix_side):<br/>    for j in range(matrix_side):<br/>        for k in range(matrix_side):<br/>            for t in range(matrix_side):<br/>                precomputed_distances[i, j, k, t] = \<br/>                    np.power(float(i) - float(k), 2) + np.power(float(j) - float(t), 2)<br/><br/>def distance_matrix(xt, yt, sigmat):<br/>    dm = precomputed_distances[xt, yt, :, :]<br/>    de = 2.0 * np.power(sigmat, 2)<br/>    return np.exp(-dm / de)</pre>
<p>The <kbd>distance_matrix</kbd> function returns the value of the radial basis function for the whole map given the center point (the winning unit) <kbd>xt, yt</kbd> and the current value of <em>σ</em> <kbd>sigmat</kbd>. Now, it's possible to start the training process (in order to avoid correlations, it's preferable to shuffle the input sequence at the beginning of each iteration):</p>
<pre>sequence = np.arange(0, X.shape[0])<br/>t = 0<br/><br/>for e in range(nb_iterations):<br/>    np.random.shuffle(sequence)<br/>    t += 1<br/>    <br/>    if e &lt; nb_startup_iterations:<br/>        etat = eta(t)<br/>        sigmat = sigma(t)<br/>    else:<br/>        etat = 0.2<br/>        sigmat = 1.0<br/>        <br/>    for n in sequence:<br/>        x_sample = X[n]<br/>        <br/>        xw, yw = winning_unit(x_sample)<br/>        dm = distance_matrix(xw, yw, sigmat)<br/>        <br/>        dW = etat * np.expand_dims(dm, axis=2) * (x_sample - W)<br/>        W += dW<br/>        <br/>    W /= np.linalg.norm(W, axis=2).reshape((matrix_side, matrix_side, 1))<strong><br/></strong></pre>
<p>In this case, we have set <em><span>η</span></em><sub><em>∞</em> </sub>= <kbd>0.2</kbd> but I invite the reader to try different values and evaluate the final result. After training for <kbd>5000</kbd> epochs, we got the following weight matrix (each weight is plotted as a bidimensional array):</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c2b13744-1295-477f-a34a-6f447eb97f70.png" style="width:29.42em;height:29.58em;"/></div>
<p>As it's possible to see, the weights have converged to faces with slightly different features. In particular, looking at the shapes of the faces and the expressions, it's easy to notice the transition between different attractors (some faces are smiling, while others are more serious; some have glasses, mustaches, and beards, and so on). It's also important to consider that the matrix is larger than the minimum capacity (there are ten different individuals in the dataset). This allows mapping more patterns that cannot be easily attracted by the right neuron. For example, an individual can have pictures with and without a beard and this can lead to confusion. If the matrix is too small, it's possible to observe an instability in the convergence process, while if it's too large, it's easy to see redundancies. The right choice depends on each different dataset and on the internal variance and there's no way to define a standard criterion. A good starting point is picking a matrix whose capacity is between 2.0 and 3.0 times larger than the number of desired attractors and then increasing or reducing its size until the accuracy reaches a maximum. The last element to consider is the labeling phase. At the end of the training process, we have no knowledge about the weight distribution in terms of winning neurons, so it's necessary to process the dataset and annotate the winning unit for each pattern. In this way, it's possible to submit new patterns to get the most likely label. This process has not been shown, but it's straightforward and the reader can easily implement it for every different scenario.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have discussed Hebb's rule, showing how it can drive the computation of the first principal component of the input dataset. We have also seen that this rule is unstable because it leads to the infinite growth of the synaptic weights and how it's possible to solve this problem using normalization or Oja's rule. </p>
<p>We have introduced two different neural networks based on Hebbian learning (Sanger's and Rubner-Tavan's networks), whose internal dynamics are slightly different, which are able to extract the first <em>n</em> principal components in the right order (starting from the largest eigenvalue) without eigendecomposing the input covariance matrix.</p>
<p>Finally, we have introduced the concept of SOM and presented a model called a Kohonen network, which is able to map the input patterns onto a surface where some attractors (one per class) are placed through a competitive learning process. Such a model is able to recognize new patterns (belonging to the same distribution) by eliciting a strong response in the attractor, that is most similar to the pattern. In this way, after a labeling process, the model can be employed as a soft classifier that can easily manage noisy or altered patterns.</p>
<p>In the next chapter, we're going to discuss some important clustering algorithms, focusing on the difference (already discussed in the previous chapters) between hard and soft clustering and discussing the main techniques employed to evaluate the performance of an algorithm.</p>


            </article>

            
        </section>
    </body></html>