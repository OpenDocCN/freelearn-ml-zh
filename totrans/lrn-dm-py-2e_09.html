<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Authorship Attribution</h1>
            </header>

            <article>
                
<p><strong>Authorship analysis</strong> is a text mining task that aims to identify certain aspects about an author, based only on the content of their writings. This could include characteristics such as age, gender, or background. In the specific <strong>authorship attribution</strong> task, we aim to identify which of a set of authors wrote a particular document. This is a classic classification task. In many ways, authorship analysis tasks are performed using standard data mining methodologies, such as cross-fold validation, feature extraction, and classification algorithms.</p>
<p>In this chapter, we will use the problem of authorship attribution to piece together the parts of the data mining methodology we developed in the previous chapters. We identify the problem and discuss the background and knowledge of the problem. This lets us choose features to extract, which we will build a pipeline for achieving. We will test two different types of features: function words and character n -grams. Finally, we will perform an in-depth analysis of the results. We will work first with a dataset of books, and then a messy, real-world corpus of e-mails.</p>
<p>The topics we will cover in this chapter are as follows:</p>
<ul>
<li>Feature engineering and how feature choice differs based on application</li>
<li>Revisiting the bag-of-words model with a specific goal in mind</li>
<li>Feature types and the character n-grams model</li>
<li>Support Vector Machines</li>
<li>Cleaning up a messy dataset for data mining</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Attributing documents to authors</h1>
            </header>

            <article>
                
<p>Authorship analysis has a background in <strong>stylometry</strong>, which is the study of an author's style of writing. The concept is based on the idea that everyone learns language slightly differently, and that measuring these nuances in people's writing will enable us to tell them apart using only the content of their writing.</p>
<p>Authorship analysis has historically (pre-1990) been performed using repeatable manual analysis and statistics, which is a good indication that it could be automated with data mining. Modern authorship analysis studies are almost entirely data mining-based, although quite a significant amount of work is still done with more manually driven analysis using linguistic styles and stylometrics. Many of the advances in feature engineering today are driven by advances in stylometrics. In other words, manual analysis discovers new features, which are then codified and used as part of the data mining process.</p>
<p>A key underlying feature of stylometry is that of <strong>writer invariants</strong>, which are features that a particular author has in all of their documents, but are not shared with other authors. In practice these writer invariants do not seem to exist, as authorship styles change over time, but the use of data mining can get us close to classifiers working off this principle.</p>
<p>As a field, authorship analysis has many sub-problems, and the main ones are as follows:</p>
<ul>
<li><strong>Authorship profiling:</strong> This determines the age, gender, or other traits of the author based on the writing. For example, we can detect the first language of a person speaking English by looking for specific ways in which they speak the language.</li>
<li><strong>Authorship verification:</strong> This checks whether the author of this document also wrote the other document. This problem is what you would normally think about in a legal court setting. For instance, the suspect's writing style (content-wise) would be analyzed to see if it matched the ransom note.</li>
<li><strong>Authorship clustering:</strong> This is an extension of authorship verification, where we use cluster analysis to group documents from a big set into clusters, and each cluster is written by the same author.</li>
</ul>
<div class="packt_infobox">However, the most common form of authorship analysis study is that of <strong>authorship attribution</strong>, a classification task where we attempt to predict which of a set of authors wrote a given document.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Applications and use cases</h1>
            </header>

            <article>
                
<p>Authorship analysis has a number of <strong>use cases</strong>. Many use cases are concerned with problems such as verifying authorship, proving shared authorship/provenance, or linking social media profiles with real-world users.</p>
<p>In a historical sense, we can use authorship analysis to verify whether certain documents were indeed written by their supposed authors. Controversial authorship claims include some of Shakespeare's plays, the Federalist papers from the USA's foundation period, and other historical texts.</p>
<div class="packt_infobox">Authorship studies alone cannot prove authorship but can provide evidence for or against a given theory, such as whether a particular person wrote a given document.</div>
<p>For example, we can analyze Shakespeare's plays to determine his writing style, before testing whether a given sonnet actually does originate from him (some recent research indicates multiple authorship of some of his work).</p>
<p>A more modern use case is that of linking social network accounts. For example, a malicious online user could set up accounts on multiple online social networks. Being able to link them allows authorities to track down the user of a given account—for example if a person is harassing other online users.</p>
<p>Another example used in the past is to be a backbone to provide expert testimony in court to determine whether a given person wrote a document. For instance, the suspect could be accused of writing an e-mail harassing another person. The use of authorship analysis could determine whether it is likely that person did, in fact, write the document. Another court-based use is to settle claims of stolen authorship. For example, two authors may claim to have written a book, and authorship analysis could provide evidence on which is the more likely author.</p>
<p>Authorship analysis is not foolproof, though. A recent study found that attributing documents to authors can be made considerably harder by simply asking people, who are otherwise untrained, to hide their writing style. This study also looked at a framing exercise where people were asked to write in the style of another person. This framing of another person proved quite reliable, with the faked document commonly attributed to the person being framed.</p>
<p>Despite these issues, authorship analysis is proving useful in a growing number of areas and is an interesting data mining problem to investigate.</p>
<div class="packt_tip">Authorship attribution can be used in expert testimony, but by itself is hard to classify as hard evidence. Always check with a lawyer before using it for formal matters, such as authorship disputes.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Authorship attribution</h1>
            </header>

            <article>
                
<p><strong>Authorship attribution</strong> (as distinct from authorship <em>analysis</em>) is a classification task by which we have a set of candidate authors, a set of documents from each of those authors namely the <strong>training set</strong>, and a set of documents of unknown authorship otherwise known as the test set. If the documents of unknown authorship definitely belong to one of the candidates, we call this a closed problem, as per the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="91" src="assets/B06162_09_01.png" width="168"/></div>
<p>If we cannot be sure of that the actual author is part of the training set, we call this an open problem. This distinction isn't just specific to authorship attribution - any data mining application where the actual class may not be in the training set is considered an open problem, with the task being to find the candidate author or to select none of them. This is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="90" src="assets/B06162_09_02.png" width="272"/></div>
<p>In authorship attribution, we typically have two restrictions on the tasks. They have been listed as follows:</p>
<ul>
<li>First, we only use content information from the documents - not metadata regarding the time of writing, delivery, handwriting style, and so on. There are ways to combine models from these different types of information, but that isn't generally considered authorship attribution and is more a <strong>data fusion</strong> application.</li>
<li>The second restriction is that we don't look at the topic of the documents; instead, we look for more salient features such as word usage, punctuation, and other text-based features. The reasoning here is that a person can write on many different topics, so worrying about the topic of their writing isn't going to model their actual authorship style. Looking at topic words can also lead to <strong>overfitting</strong> on the training data—our model may train on documents from the same author and also on the same topic. For instance, if you were to model my authorship style by looking at this book, you might conclude the words <em>data mining</em> are indicative of <em>my</em> writing style when, in fact, I write on other topics as well.</li>
</ul>
<p>From here, the pipeline for performing authorship attribution looks a lot like the one we developed in <a href="ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml">Chapter 6</a><em>, Social Media Insight Using Naive Bayes</em>.</p>
<ol>
<li>First, we extract features from our text.</li>
<li>Then, we perform some feature selection on those features.</li>
<li>Finally, we train a classification algorithm to fit a model, which we can then use to predict the class (in this case, the author) of a document.</li>
</ol>
<div class="packt_infobox">There are some differences between classifying content and classifying authorship, mostly having to do with which features are used, that we will cover in this chapter. It is critical to choose features based on the application.</div>
<p>Before we delve into these issue, we will define the scope of the problem and collect some data.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Getting the data</h1>
            </header>

            <article>
                
<p>The data we will use for the first part of this chapter is a set of books from <strong>Project Gutenberg</strong> at <a href="http://www.gutenberg.org">www.gutenberg.org</a>, which is a repository of public domain literature works. The books I used for these experiments come from a variety of authors:</p>
<ul>
<li>Booth Tarkington (22 titles)</li>
<li>Charles Dickens (44 titles)</li>
<li>Edith Nesbit (10 titles)</li>
<li>Arthur Conan Doyle (51 titles)</li>
<li>Mark Twain (29 titles)</li>
<li>Sir Richard Francis Burton (11 titles)</li>
<li>Emile Gaboriau (10 titles)</li>
</ul>
<p>Overall, there are 177 documents from 7 authors, giving a significant amount of text to work with. A full list of the titles, along with download links and a script to automatically fetch them, is given in the code bundle called <span class="packt_screen">getdata.py</span>. If running the code results in significantly fewer books than above, the mirror may be down. See this website for more mirror URLs to try in the script: <a href="https://www.gutenberg.org/MIRRORS.ALL">https://www.gutenberg.org/MIRRORS.ALL</a></p>
<p>To download these books, we use the requests library to download the files into our data directory.</p>
<p>First, in a new Jupyter Notebook, set up the data directory and ensure the following code links to it:</p>
<pre>
import os <br/>import sys <br/>data_folder = os.path.join(os.path.expanduser("~"), "Data", "books")
</pre>
<p>Next, download the data bundle from the code bundle supplied by Packt. Decompress the file into this directory. The books folder should then directly contain one folder for each author. </p>
<p>After taking a look at these files, you will see that many of them are quite messy—at least from a data analysis point of view. There is a large project Gutenberg disclaimer at the start of the files. This needs to be removed before we do our analysis.</p>
<p>For example, most books begin with information such as the following:</p>
<p><em>The</em> <em>Project Gutenberg eBook of Mugby Junction, by Charles Dickens, et al, </em><em>Illustrated by Jules A.<br/>
Goodman This eBook is for the use of anyone anywhere at no cost and with</em><br/>
<em>almost no restrictions whatsoever. You may copy it, give it away or</em><br/>
<em>re-use it under the terms of the Project Gutenberg License included</em><br/>
<em>with this eBook or online at www.gutenberg.org</em><br/>
<em>Title: Mugby Junction</em><br/>
<em>Author: Charles Dickens</em><br/>
<em>Release Date: January 28, 2009 [eBook #27924]Language: English</em><br/>
<em>Character set encoding: UTF-8<br/>
***START OF THE PROJECT GUTENBERG EBOOK MUGBY JUNCTION***</em></p>
<p>After this point, the actual text of the book starts. The use of a line starting <span class="packt_screen">***START OF THE PROJECT GUTENBERG</span> is fairly consistent, and we will use that as a cue on when the text starts - anything before this line will be ignored.</p>
<p>We could alter the individual files on disk to remove this stuff. However, what happens if we were to lose our data? We would lose our changes and potentially be unable to replicate the study. For that reason, we will perform the preprocessing as we load the files—this allows us to be sure our results will be replicable (as long as the data source stays the same). The following code removes the main source of noise from the books, which is the prelude that Project Gutenberg adds to the files:</p>
<pre>
def clean_book(document):<br/>    lines = document.split("n")<br/>    start= 0<br/>    end = len(lines)<br/>    for i in range(len(lines)):<br/>        line = lines[i]<br/>        if line.startswith("*** START OF THIS PROJECT GUTENBERG"):<br/>            start = i + 1<br/>        elif line.startswith("*** END OF THIS PROJECT GUTENBERG"):<br/>            end = i - 1<br/>    return "n".join(lines[start:end])
</pre>
<div class="packt_tip">You may want to add to this function to remove other sources of noise, such as inconsistent formatting, footer information, and so on. Investigate the files to examine what issues they have.</div>
<p>We can now get our documents and classes using the following function, which loops through these folders, loads the text documents and records a number assigned to the author as the target class.</p>
<pre>
import numpy as np<br/><br/>def load_books_data(folder=data_folder):<br/>    documents = []<br/>    authors = []<br/>    subfolders = [subfolder for subfolder in os.listdir(folder)<br/>                  if os.path.isdir(os.path.join(folder, subfolder))]<br/>    for author_number, subfolder in enumerate(subfolders):<br/>        full_subfolder_path = os.path.join(folder, subfolder)<br/>        for document_name in os.listdir(full_subfolder_path):<br/>            with open(os.path.join(full_subfolder_path, document_name), errors='ignore') as inf:<br/>                documents.append(clean_book(inf.read()))<br/>                authors.append(author_number)<br/>    return documents, np.array(authors, dtype='int')
</pre>
<p>We then call this function to actually load the books:</p>
<pre>
documents, classes = load_books_data(data_folder)
</pre>
<div class="packt_infobox">This dataset fits into memory quite easily, so we can load all of the text at once. In cases where the whole dataset doesn't fit, a better solution is to extract the features from each document one-at-a-time (or in batches) and save the resulting values to a file or in-memory matrix</div>
<p>To get a gauge on the properties of the data, one of the first things I usually do is create a simple histogram of the document lengths. If the lengths are relatively consistent, this is often easier to learn from than wildly different document lengths. In this case, there is quite a large variance in document lengths. To view this, first we extract the lengths into a list:</p>
<pre>
document_lengths = [len(document) for document in documents]
</pre>
<p>Next, we plot those. Matplotlib has a <kbd>hist</kbd> function that will do this, as does Seaborn, which produces nicer looking graphs by default.</p>
<pre>
import seaborn as sns<br/>sns.distplot(document_lengths)
</pre>
<p>The resulting graph shows the variation in document lengths:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="309" src="assets/B06162_09_06.png" width="449"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using function words</h1>
            </header>

            <article>
                
<p>One of the earliest types of features, and one that still works quite well for authorship analysis, is to use function words in a bag-of-words model. Function words are words that have little meaning on their own, but are required for creating (English!) sentences. For example, the words <em>this</em> and <em>which</em> are words that are really only defined by what they do within a sentence, rather than their meaning in themselves. Contrast this with a content word such as <em>tiger</em>, which has an explicit meaning and invokes imagery of a large cat when used in a sentence.</p>
<p>The set of words that are considered function words is not always obvious. A good rule of thumb is to choose the most frequent words in usage (over all possible documents, not just ones from the same author).</p>
<div class="packt_infobox">Typically, the more frequently a word is used, the better it is for authorship analysis. In contrast, the less frequently a word is used, the better it is for content-based text mining, such as in the next chapter, where we look at the topic of different documents.</div>
<p>The graph here gives a better idea between word and frequency relationship:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="199" src="assets/B06162_09_03.png" width="504"/></div>
<p>The use of function words is less defined by the content of the document and more by the decisions made by the author. This makes them good candidates for separating the authorship traits between different users. For instance, while many Americans are particular about the different in usage between <em>that</em> and <em>which</em> in a sentence, people from other countries, such as Australia, are less concerned with the distinction. This means that some Australians will lean towards almost exclusively using one word or the other,  while others may use <em>which</em> much more.</p>
<p>This difference, combined with thousands of other nuanced differences, makes a model of authorship.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Counting function words</h1>
            </header>

            <article>
                
<p>We can count function words using the <span class="packt_screen">CountVectorizer</span> class we used in <a href="ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml">Chapter 6</a><em>, Social Media Insight Using Naive Bayes</em>. This class can be passed a vocabulary, which is the set of words it will look for. If a vocabulary is not passed (we didn't pass one in the code of <a href="ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml">Chapter 6</a><em>, Social Media Insight Using Naive Bayes</em>), then it will learn this vocabulary from the training dataset. All the words are in the training set of documents (depending on the other parameters of course).</p>
<p>First, we set up our vocabulary of function words, which is just a list containing each of them. Exactly which words are function words and which are not is up for debate. I've found the following list, from published research, to be quite good, obtained from my own research combining word lists from other researchers. Remember that the code bundle is available from Packt publishing (or the official github channel), and therefore you don't need to type this out:</p>
<pre>
function_words = ["a", "able", "aboard", "about", "above", "absent", "according" , "accordingly", "across", "after", "against","ahead", "albeit", "all", "along", "alongside", "although", "am", "amid", "amidst", "among", "amongst", "amount", "an", "and", "another", "anti", "any", "anybody", "anyone", "anything", "are", "around", "as", "aside", "astraddle", "astride", "at", "away", "bar", "barring", "be", "because", "been", "before", "behind", "being", "below", "beneath", "beside", "besides", "better", "between", "beyond", "bit", "both", "but", "by", "can", "certain", "circa", "close", "concerning", "consequently", "considering", "could", "couple", "dare", "deal", "despite", "down", "due", "during", "each", "eight", "eighth", "either", "enough", "every", "everybody", "everyone", "everything", "except", "excepting", "excluding", "failing", "few", "fewer", "fifth", "first", "five", "following", "for", "four", "fourth", "from", "front", "given", "good", "great", "had", "half", "have", "he", "heaps", "hence", "her", "hers", "herself", "him", "himself", "his", "however", "i", "if", "in", "including", "inside", "instead", "into", "is", "it", "its", "itself", "keeping", "lack", "less", "like", "little", "loads", "lots", "majority", "many", "masses", "may", "me", "might", "mine", "minority", <br/>"minus", "more", "most", "much", "must", "my", "myself", "near", "need", "neither", "nevertheless", "next", "nine", "ninth", "no", "nobody", "none", "nor", "nothing", "notwithstanding", "number", "numbers", "of", "off", "on", "once", "one", "onto", "opposite", "or", "other", "ought", "our", "ours", "ourselves", "out", "outside", "over", "part", "past", "pending", "per", "pertaining", "place", "plenty", "plethora", "plus", "quantities", "quantity", "quarter", "regarding", "remainder", "respecting", "rest", "round", "save", "saving", "second", "seven", "seventh", "several","shall", "she", "should", "similar", "since", "six", "sixth", "so", "some", "somebody", "someone", "something", "spite","such", "ten", "tenth", "than", "thanks", "that", "the", "their", "theirs", "them", "themselves", "then", "thence", "therefore", "these", "they", "third", "this", "those","though", "three", "through", "throughout", "thru", "thus", "till", "time", "to", "tons", "top", "toward", "towards", "two", "under", "underneath", "unless", "unlike", "until", "unto", "up", "upon", "us", "used", "various", "versus","via", "view", "wanting", "was", "we", "were", "what", "whatever", "when", "whenever", "where", "whereas", "wherever", "whether", "which", "whichever", "while","whilst", "who", "whoever", "whole", "whom", "whomever", "whose", "will", "with", "within", "without", "would", "yet", "you", "your", "yours", "yourself", "yourselves"]
</pre>
<p>Now, we can set up an extractor to get the counts of these function words. Note the passing of the function words list as the <kbd>vocabulary</kbd> into the <kbd>CountVectorizer</kbd> initialiser.</p>
<pre>
from sklearn.feature_extraction.text <br/>import CountVectorizer <br/>extractor = CountVectorizer(vocabulary=function_words)
</pre>
<p>For this set of function words, the frequency within these documents is very high - as you would expect. We can use the extractor instance to obtain these counts, by fitting it on the data, and then calling <kbd>transform</kbd> (or, the shortcut using <kbd>fit_transform</kbd>).</p>
<pre>
extractor.fit(documents)<br/>counts = extractor.transform(documents)
</pre>
<p>Before plotting, we normalized these counts by dividing by the relevant document lengths. The following code does this, resulting in the percentage of words accounted for by each function word:</p>
<pre>
normalized_counts = counts.T / np.array(document_lengths)
</pre>
<p>We then average these percentages across all documents:</p>
<pre>
averaged_counts = normalized_counts.mean(axis=1)
</pre>
<p>Finally we plot them using Matplotlib (Seaborn lacks easy interfaces to basic plots like this).</p>
<pre>
from matplotlib import pyplot as plt<br/>plt.plot(averaged_counts)
</pre>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="316" src="assets/B06162_09_07.png" width="460"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classifying with function words</h1>
            </header>

            <article>
                
<p> The only new thing here is the use of <strong>Support Vector Machines</strong> (<strong>SVM</strong>), which we will cover in the next section (for now, just consider it a standard classification algorithm).</p>
<p>Next, we import our classes. We import the <span class="packt_screen">SVC</span> class, an SVM for classification, as well as the other standard workflow tools we have seen before:</p>
<pre>
from sklearn.svm import SVC <br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.pipeline import Pipeline from sklearn import grid_search
</pre>
<p>SVMs take a number of parameters. As I said, we will use one blindly here, before going into detail in the next section. We then use a dictionary to set which parameters we are going to search. For the <kbd>kernel</kbd> parameter, we will try <kbd>linear</kbd> and <kbd>rbf</kbd>. For <span class="packt_screen">C</span>, we will try values of 1 and 10 (descriptions of these parameters are covered in the next section). We then create a grid search to search these parameters for the best choices:</p>
<pre>
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}<br/>svr = SVC()<br/>grid = grid_search.GridSearchCV(svr, parameters)
</pre>
<div class="packt_infobox">Gaussian kernels (such as RBF) only work for reasonably sized data sets, such as when the number of features is fewer than about 10,000.</div>
<p>Next, we set up a pipeline that takes the feature extraction step using the <kbd>CountVectorizer</kbd> (only using function words), along with our grid search using SVM. The code is as follows:</p>
<pre>
pipeline1 = Pipeline([('feature_extraction', extractor), ('clf', grid) ])
</pre>
<p>Next, apply <kbd>cross_val_score</kbd> to get our cross-validated score for this pipeline. The result is 0.811, which means we approximately get 80 percent of the predictions correct.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Support Vector Machines</h1>
            </header>

            <article>
                
<p>SVMs are classification algorithms based on a simple and intuitive idea, backed by some complex and innovative mathematics. SVMs perform classification between two classes (although we can extend it to more classes using various meta-algorithms), by simply drawing a separating line between the two (or a hyperplane in higher-dimensions). The intuitive idea is to choose the best line of separation, rather than just any specific line.</p>
<p>Suppose that our two classes can be separated by a line such that any points above the line belong to one class and any below the line belong to the other class. SVMs find this line and use it for prediction, much the same way as linear regression works. SVMs, however, find the best line for separating the dataset. In the following figure, we have three lines that separate the dataset: blue, black, and green. Which would you say is the best option?</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="194" src="assets/B06162_09_04.png" width="231"/></div>
<p>Intuitively, a person would normally choose the blue line as the best option, as this separates the data in the cleanest way. More formally, it has the maximum distance from the line to any point in each class. Finding this line of maximum separation is an optimization problem, based on finding the lines of margin with the maximum distance between them. Solving this optimisation problem is the main task of the training phase of an SVM.</p>
<div class="packt_tip">The equations to solve SVMs is outside the scope of this book, but I recommend interested readers to go through the derivations at:<br/>
<a href="http://en.wikibooks.org/wiki/Support_Vector_Machines">http://en.wikibooks.org/wiki/Support_Vector_Machines</a> for the details.<br/>
Alternatively, you can visit:<br/>
<a href="http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html">http://docs.opencv.org/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html</a></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Classifying with SVMs</h1>
            </header>

            <article>
                
<p>After training the model, we have a line of maximum margin. The classification of new samples is then simply asking the question: does it fall above the line, or below it? If it falls above the line, it is predicted as one class. If it is below the line, it is predicted as the other class.</p>
<p>For multiple classes, we create multiple SVMs—each a binary classifier. We then connect them using any one of a variety of strategies. A basic strategy is to create a one-versus-all classifier for each class, where we train using two classes—the given class and all other samples. We do this for each class and run each classifier on a new sample, choosing the best match from each of these. This process is performed automatically in most SVM implementations.</p>
<p>We saw two parameters in our previous code: <strong>C</strong> and kernel. We will cover the kernel parameter in the next section, but the <strong>C</strong> parameter is an important parameter for fitting SVMs. The <strong>C</strong> parameter relates to how much the classifier should aim to predict all training samples correctly, at the risk of overfitting. Selecting a higher <strong>C</strong> value will find a line of separation with a smaller margin, aiming to classify all training samples correctly. Choosing a lower <strong>C</strong> value will result in a line of separation with a larger margin—even if that means that some training samples are incorrectly classified. In this case, a lower <strong>C</strong> value presents a lower chance of overfitting, at the risk of choosing a generally poorer line of separation</p>
<p>One limitation with SVMs (in their basic form) is that they only separate data that is linearly separable. What happens if the data isn't? For that problem, we use kernels.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Kernels</h1>
            </header>

            <article>
                
<p>When the data cannot be separated linearly, the trick is to embed it on to a higher dimensional space. What this means, with a lot of hand-waving about the details, is to add new features to the dataset until the data is linearly separable. If you add the right kinds of features, this linear separation will always, eventually, happen.</p>
<p>The trick is that we often compute the inner-produce of the samples when finding the best line to separate the dataset. Given a function that uses the dot product, we effectively manufacture new features without having to actually define those new features. This is known as the kernel trick and is handy because we don't know what those features were going to be anyway. We now define a kernel as a function that itself is the dot product of the function of two samples from the dataset, rather than based on the samples (and the made-up features) themselves.</p>
<p>We can now compute what that dot product is (or approximate it) and then just use that.</p>
<p>There are a number of kernels in common use. The <strong>linear kernel</strong> is the most straightforward and is simply the dot product of the two sample feature vectors, the weight feature, and a bias value. There is also a <strong>polynomial kernel</strong>, which raises the dot product to a given degree (for instance, 2). Others include the <strong>Gaussian</strong> (<strong><span class="packt_screen">rbf</span></strong>) and <strong>Sigmoidal</strong> functions. In our previous code sample, we tested between the <strong>linear</strong> kernel and the <strong>rbf</strong> kernel options.</p>
<p>The end result from all this derivation is that these kernels effectively define a distance between two samples that is used in the classification of new samples in SVMs. In theory, any distance could be used, although it may not share the same characteristics that enable easy optimization of the SVM training.</p>
<p>In scikit-learn's implementation of SVMs, we can define the kernel parameter to change which kernel function is used in computations, as we saw in the previous code sample.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Character n-grams</h1>
            </header>

            <article>
                
<p>We saw how function words can be used as features to predict the author of a document. Another feature type is character n-grams. An n-gram is a sequence of <em>n</em> tokens, where <em>n</em> is a value (for text, generally between 2 and 6). Word n-grams have been used in many studies, usually relating to the topic of the documents - as per the previous chapter. However, character n-grams have proven to be of high quality for authorship attribution.</p>
<p>Character n-grams are found in text documents by representing the document as a sequence of characters. These n-grams are then extracted from this sequence and a model is trained. There are a number of different models for this, but a standard one is very similar to the bag-of-words model we have used earlier.</p>
<p>For each distinct n-gram in the training corpus, we create a feature for it. An example of an n-gram is <kbd>&lt;e t&gt;</kbd>, which is the letter e, space, and then the letter t (the angle brackets are used to denote the start and end of the n-gram and are not part of the n-gram itself). We then train our model using the frequency of each n-gram in the training documents and train the classifier using the created feature matrix.</p>
<div class="packt_infobox">Character n-grams are defined in many ways. For instance, some applications only choose within-word characters, ignoring whitespace and punctuation. Some use this information (like our implementation in this chapter) for classification. Ultimately, this is the purpose of the model, chosen by the data miner (you!).</div>
<p>A common theory for why character n-grams work is that people more typically write words they can easily say and character n-grams (at least when n is between 2 and 6) are a good approximation for <strong>phonemes</strong>—the sounds we make when saying words. In this sense, using character n-grams approximates the sounds of words, which approximates your writing style. This is a common pattern when creating new features. First, we have a theory on what concepts will impact the end result (authorship style) and then create features to approximate or measure those concepts.</p>
<p>One key feature of a character n-gram matrix is that it is sparse and increases in sparsity with higher n-values quite quickly. For an n-value of 2, approximately 75 percent of our feature matrix is zeros. For an n-value of 5, over 93 percent is zeros. This is typically less sparse than a word n-gram matrix of the same type though and shouldn't cause many issues using a classifier that is used for word-based classifications.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Extracting character n-grams</h1>
            </header>

            <article>
                
<p>We are going to use our <kbd>CountVectorizer</kbd> class to extract character n-grams. To do that, we set the analyzer parameter and specify a value for n to extract n-grams with.</p>
<p>The implementation in scikit-learn uses an n-gram range, allowing you to extract n-grams of multiple sizes at the same time. We won't delve into different n-values in this experiment, so we just set the values the same. To extract n-grams of size 3, you need to specify (3, 3) as the value for the n-gram range.</p>
<p>We can reuse the grid search from our previous code. All we need to do is specify the new feature extractor in a new pipeline and run it:</p>
<pre>
pipeline = Pipeline([('feature_extraction', CountVectorizer(analyzer='char', ngram_range=(3,3))),<br/>                     ('classifier', grid) ]<br/>scores = cross_val_score(pipeline, documents, classes, scoring='f1') <br/>print("Score: {:.3f}".format(np.mean(scores)))
</pre>
<div class="packt_infobox">There is a lot of implicit overlap between function words and character n-grams, as character sequences in function words are more likely to appear. However, the actual features are very different and character n-grams capture punctuation, a characteristic that function words do not capture. For example, a character n-gram includes the full stop at the end of a sentence, while a function word-based method would only use the preceding word itself.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">The Enron dataset</h1>
            </header>

            <article>
                
<p>Enron was one of the largest energy companies in the world in the late 1990s, reporting revenue over $100 billion. It had over 20,000 staff and—as of the year 2000—there seemed to be no indications that something was very wrong.</p>
<p>In 2001, the <em>Enron Scandal</em> occurred, where it was discovered that Enron was undertaking systematic, fraudulent accounting practices. This fraud was deliberate, wide-ranging across the company, and for significant amounts of money. After this was publicly discovered, its share price dropped from more than $90 in 2000 to less than $1 in 2001. Enron shortly filed for bankruptcy in a mess that would take more than 5 years to finally be resolved.</p>
<p>As part of the investigation into Enron, the Federal Energy Regulatory Commission in the United States made more than 600,000 e-mails publicly available. Since then, this dataset has been used for research into everything from social network analysis to fraud analysis. It is also a great dataset for authorship analysis, as we are able to extract e-mails from the sent folder of individual users. This allows us to create a dataset much larger than many previous datasets.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Accessing the Enron dataset</h1>
            </header>

            <article>
                
<p>The full set of Enron emails is available at <a href="https://www.cs.cmu.edu/~./enron/">https://www.cs.cmu.edu/~./enron/</a></p>
<div class="packt_tip">The full dataset is quite large, and provided in a compression format called gzip. If you don't have a Linux-based machine to decompress (unzip) this file, get an alternative program, such as 7-zip (<a href="http://www.7-zip.org/">http://www.7-zip.org/</a>)</div>
<p>Download the full corpus and decompress it into your data folder. By default, this will decompress into a folder called <kbd>enron_mail_20110402</kbd> which then contains a folder called <kbd>maildir</kbd>. In the Notebook, setup the data folder for the Enron dataset:</p>
<pre>
enron_data_folder = os.path.join(os.path.expanduser("~"), "Data", "enron_mail_20150507", "maildir")
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating a dataset loader</h1>
            </header>

            <article>
                
<p>As we are looking for authorship information, we only want the e-mails we can attribute to a specific author. For that reason, we will look in each user's sent folder—that is, emails they have sent. We can now create a function that will choose a couple of authors at random and return each of the emails in their sent folder. Specifically, we are looking for the payloads—that is, the content rather than the e-mails themselves. For that, we will need an e-mail parser. The code is as follows:</p>
<pre>
from email.parser <br/>import Parser p = Parser()
</pre>
<p>We will be using this later to extract the payloads from the e-mail files that are in the data folder.</p>
<p>With our data loading function, we are going to have a lot of options. Most of these ensure that our dataset is relatively balanced. Some authors will have thousands of e-mails in their sent mail, while others will have only a few dozen. We limit our search to only authors with at least 10 e-mails using <kbd>min_docs_author</kbd> and take a maximum of 100 e-mails from each author using the <kbd>max_docs_author</kbd> parameter. We also specify how many authors we want to get—10 by default using the <kbd>num_authors</kbd> parameter. </p>
<p>The function is below. Its main purpose is to loop through the authors, retrieve a number of emails for that author, and store the <strong>document</strong> and <strong>class</strong> information in some lists. We also store the mapping between an author's name and their numerical class value, which lets us retrieve that information later.</p>
<pre>
from sklearn.utils import check_random_state
</pre>
<pre>
def get_enron_corpus(num_authors=10, data_folder=enron_data_folder, min_docs_author=10,<br/>                     max_docs_author=100, random_state=None):<br/>    random_state = check_random_state(random_state)<br/>    email_addresses = sorted(os.listdir(data_folder))<br/>    # Randomly shuffle the authors. We use random_state here to get a repeatable shuffle<br/>    random_state.shuffle(email_addresses)<br/>    # Setup structures for storing information, including author information<br/>    documents = []<br/>    classes = []<br/>    author_num = 0<br/>    authors = {}  # Maps author numbers to author names<br/>    for user in email_addresses:<br/>        users_email_folder = os.path.join(data_folder, user)<br/>        mail_folders = [os.path.join(users_email_folder, subfolder)<br/>                        for subfolder in os.listdir(users_email_folder)<br/>                        if "sent" in subfolder]<br/>        try:<br/>            authored_emails = [open(os.path.join(mail_folder, email_filename),<br/>                                    encoding='cp1252').read()<br/>                               for mail_folder in mail_folders<br/>                               for email_filename in os.listdir(mail_folder)]<br/>        except IsADirectoryError:<br/>            continue<br/>        if len(authored_emails) &lt; min_docs_author:<br/>            continue<br/>        if len(authored_emails) &gt; max_docs_author:<br/>            authored_emails = authored_emails[:max_docs_author]<br/>        # Parse emails, store the content in documents and add to the classes list<br/>        contents = [p.parsestr(email)._payload for email in authored_emails]<br/>        documents.extend(contents)<br/>        classes.extend([author_num] * len(authored_emails))<br/>        authors[user] = author_num<br/>        author_num += 1<br/>        if author_num &gt;= num_authors or author_num &gt;= len(email_addresses):<br/>            break<br/>     return documents, np.array(classes), authors
</pre>
<div class="packt_infobox">It may seem odd that we sort the e-mail addresses, only to shuffle them around. The <kbd>os.listdir</kbd> function doesn't always return the same results, so we sort it first to get some stability. We then shuffle using a random state, which means our shuffling can reproduce a past result if needed.</div>
<p>Outside of this function, we can now get a dataset by making the following function call. We are going to use a random state of 14 here (as always in this book), but you can try other values or set it to none to get a random set each time the function is called:</p>
<pre>
documents, classes, authors = get_enron_corpus(data_folder=enron_data_folder, random_state=14)
</pre>
<p>If you have a look at the dataset, there is still a further preprocessing set we need to undertake. Our e-mails are quite messy, but one of the worst bits (from an authorship analysis perspective) is that these e-mails contain writings from other authors, in the form of attached replies. Take the following email, which is <kbd>documents[100]</kbd>, for instance:</p>
<p style="padding-left: 30px"><em>I would like to be on the panel but I have on a conflict on the conference</em></p>
<p style="padding-left: 30px"><em>dates. Please keep me in mind for next year.</em></p>
<p style="padding-left: 30px"><em>Mark Haedicke</em></p>
<div class="packt_infobox">Email is a notoriously messy format. Reply quoting, for instance, is sometimes (but not always) prepended with a &gt; character. Other times, the reply is embedded in the original message. If you are doing larger scale data mining with email, be sure to spend more time cleaning the data to get better results.</div>
<p>As with the books dataset, we can plot the histogram of document lengths to get a sense of the document length distributions:</p>
<pre>
document_lengths = [len(document) for document in documents]<br/>sns.distplot(document_lengths)
</pre>
<p>The result appears to show a strong grouping around shorter documents. While this is true, it also shows that some documents are very, very long. This may skew the results, particularly if some authors are prone to writing long documents. To compensate for this, one extension to this work may be to normalise document lengths to the first 500 characters before doing the training.</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="321" src="assets/B06162_09_08.png" width="466"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Putting it all together</h1>
            </header>

            <article>
                
<p>We can use the existing parameter space and the existing classifier from our previous experiments—all we need to do is refit it on our new data. By default, training in scikit-learn is done from scratch—subsequent calls to <kbd>fit()</kbd> will discard any previous information.</p>
<div class="packt_infobox">There is a class of algorithms called <em>online learning</em> that update the training with new samples and don't restart their training each time. </div>
<p>As before, we can compute our scores by using <kbd>cross_val_score</kbd> and print the results. The code is as follows:</p>
<pre>
scores = cross_val_score(pipeline, documents, classes, scoring='f1') <br/><br/>print("Score: {:.3f}".format(np.mean(scores)))
</pre>
<p>The result is 0.683, which is a reasonable result for such a messy dataset. Adding more data (such as increasing <kbd>max_docs_author</kbd> in the dataset loading) can improve these results, as will improving the quality of the data with extra cleaning.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Evaluation</h1>
            </header>

            <article>
                
<p>It is generally never a good idea to base an assessment on a single number. In the case of the f-score, it is usually more robust to <em>tricks</em> that give good scores despite not being useful. An example of this is accuracy. As we said in our previous chapter, a spam classifier could predict everything as being spam and get over 80 percent accuracy, although that solution is not useful at all. For that reason, it is usually worth going more in-depth on the results.</p>
<p>To start with, we will look at the confusion matrix, as we did in <a href="ddd1527c-d895-4519-b709-8fe9680518c1.xhtml">Chapter 8</a><em>, Beating CAPTCHAs with Neural Networks</em>. Before we can do that, we need to predict a testing set. The previous code uses <kbd>cross_val_score</kbd>, which doesn't actually give us a trained model we can use. So, we will need to refit one. To do that, we need training and testing subsets:</p>
<pre>
from sklearn.cross_validation import train_test_split training_documents, <br/><br/>testing_documents, y_train, y_test = train_test_split(documents, classes, random_state=14)
</pre>
<p>Next, we fit the pipeline to our training documents and create our predictions for the testing set:</p>
<pre>
pipeline.fit(training_documents, y_train) <br/>y_pred = pipeline.predict(testing_documents)
</pre>
<p>At this point, you might be wondering what the best combination of parameters actually was. We can extract this quite easily from our grid search object (which is the classifier step of our pipeline):</p>
<pre>
print(pipeline.named_steps['classifier'].best_params_)
</pre>
<p>The results give you all of the parameters for the classifier. However, most of the parameters are the defaults that we didn't touch. The ones we did search for were C and kernel, which were set to 1 and linear, respectively.</p>
<p>Now we can create a confusion matrix:</p>
<pre>
from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y_pred, y_test)<br/>cm = cm / cm.astype(np.float).sum(axis=1)
</pre>
<p>Next, we get our author's names, allowing us to that we can label the axis correctly. For this purpose, we use the authors dictionary that our Enron dataset loaded. The code is as follows:</p>
<pre>
sorted_authors = sorted(authors.keys(), key=lambda x:authors[x])
</pre>
<p>Finally, we show the confusion matrix using matplotlib. The only changes from the last chapter are highlighted below; just replace the letter labels with the authors from this chapter's experiments:</p>
<pre>
%matplotlib inline <br/>from matplotlib import pyplot as plt <br/>plt.figure(figsize=(10,10))<br/>plt.imshow(cm, cmap='Blues', interpolation='nearest')<br/>tick_marks = np.arange(len(sorted_authors))<br/>plt.xticks(tick_marks, sorted_authors) <br/>plt.yticks(tick_marks, sorted_authors) <br/>plt.ylabel('Actual') <br/>plt.xlabel('Predicted') <br/>plt.show()
</pre>
<p>The results are shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="282" src="assets/B06162_09_05.png" width="287"/></div>
<p>We can see that authors are predicted correctly in most cases—there is a clear diagonal line with high values. There are some large sources of error though (darker values are larger): emails from user <span class="packt_screen">rapp-b</span> are typically predicted as being from <span class="packt_screen">reitmeyer-j</span> for instance.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we looked at the text mining-based problem of authorship attribution. To perform this, we analyzed two types of features: function words and character n-grams. For function words, we were able to use the bag-of-words model—simply restricted to a set of words we chose beforehand. This gave us the frequencies of only those words. For character n-grams, we used a very similar workflow using the same class. However, we changed the analyzer to look at characters and not words. In addition, we used n-grams that are sequences of n tokens in a row—in our case characters. Word n-grams are also worth testing in some applications, as they can provide a cheap way to get the context of how a word is used.</p>
<p>For classification, we used SVMs that optimize a line of separation between the classes based on the idea of finding the maximum margin. Anything above the line is one class and anything below the line is another class. As with the other classification tasks we have considered, we have a set of samples (in this case, our documents).</p>
<p>We then used a very messy dataset, the Enron e-mails. This dataset contains lots of artifacts and other issues. This resulted in a lower accuracy than the books dataset, which was much cleaner. However, we were able to choose the correct author more than half the time, out of 10 possible authors.</p>
<p>To take the concepts in this chapter further, look for new datasets containing authorship information. For instance, can you predict the author of a blog post? What about the author of a tweet (you may be able to reuse your data from <a href="ea7ae888-e2aa-46b5-ba45-b8c685cc5fe2.xhtml" target="_blank">Chapter 6</a>, <em>Social Media Insight Using Naive Bayes</em>)?</p>
<p>In the next chapter, we consider what we can do if we don't have target classes. This is called unsupervised learning, an exploratory problem rather than a prediction problem. We also continue to deal with messy text-based datasets.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>