- en: '*Chapter 6*: AWS Services for Data Processing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about several ways of storing data in AWS.
    In this chapter, we will explore the techniques for using that data and gaining
    some insight from the data. There are use cases where you have to process your
    data or load the data to a hive data warehouse to query and analyze the data.
    If you are on AWS and your data is in S3, then you have to create a table in hive
    on AWS EMR to query them. To provide the same as a managed service, AWS has a
    product called Athena, where you have to create a data catalog and query your
    data on S3\. If you need to transform the data, then AWS Glue is the best option
    to transform and restore it to S3\. Let's imagine a use case where we need to
    stream the data and create analytical reports on that data. For such scenarios,
    we can opt for AWS Kinesis Data Streams to stream data and store it in S3\. Using
    Glue, the same data can be copied to Redshift for further analytical utilization.
    Now, let's learn about them and we will cover the following in brief.
  prefs: []
  type: TYPE_NORMAL
- en: Using Glue for designing ETL jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying S3 data using Athena
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data through AWS Kinesis Data Streams and storing it using Kinesis
    Firehose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting data from on-premises locations to AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Migrating data to AWS and extending on-premises data centers to AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing data on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can download the data used in the examples from GitHub, available here:
    [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating ETL jobs on AWS Glue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a modern data pipeline, there are multiple stages, such as Generate Data,
    Collect Data, Store Data, Perform ETL, Analyze, and Visualize. In this section,
    we will cover each of these at a high level and understand the **ETL** (**extract**,
    **transform**, **load**) part in-depth:'
  prefs: []
  type: TYPE_NORMAL
- en: Data can be generated from several devices, including mobile devices or IoT,
    weblogs, social media, transactional data, online games, and many more besides.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This huge amount of generated data can be collected by using polling services
    or through API gateways integrated with AWS Lambda to collect the data, or via
    streams such as AWS Kinesis or AWS-managed Kafka or Kinesis Firehose. If you have
    an on-premises database and you want to collect that data to AWS, then you choose
    AWS DMS for that. You can sync your on-premises data to Amazon S3, Amazon EFS,
    or Amazon FSx via AWS DataSync. AWS Snowball is used to collect/transfer data
    into and out of AWS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step involves storing data, and we have learned some of the services
    in the previous chapter, such as AWS S3, EBS, EFS, Amazon RDS, Amazon Redshift,
    and DynamoDB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we know our data storage, an ETL job can be designed to extract-transform-load
    or extract-load-transform our structured or unstructured data into our desired
    format for further analysis. For example, we can think of AWS Lambda to transform
    the data on the fly and store the transformed data into S3, or we can run a Spark
    application on an EMR cluster to transform the data and store it in S3 or Redshift
    or RDS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many services available in AWS for performing an analysis on transformed
    data; for example, Amazon EMR, Amazon Athena, Amazon Redshift, Amazon Redshift
    Spectrum, and Kinesis Analytics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the data is analyzed, you can visualize the data using AWS Quicksight to
    understand the pattern or trend. Data scientists or machine learning professionals
    would love to apply statistical analysis to understand data distribution in a
    better way. Business users use it to prepare reports. We have already learned
    various ways to present and visualize data in [*Chapter 4*](B16735_04_Final_VK_ePub.xhtml#_idTextAnchor082),
    *Understanding and Visualizing Data*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we understood from the traditional data pipeline is that ETL is all about
    coding and maintaining code on the servers to run smoothly. If the data format
    changes in any way, then the code needs to be changed, and that results in a change
    to the target schema. If the data source changes, then the code must be able to
    handle that too and it's an overhead. *Should we write code to recognize these
    changes in data sources? Should we need a system to adapt to the change and discover
    the data for us?* The answer is **AWS Glue**. Now, let's understand why AWS Glue
    is so famous.
  prefs: []
  type: TYPE_NORMAL
- en: Features of AWS Glue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AWS Glue is a completely managed serverless ETL service on AWS. It has the
    following features:'
  prefs: []
  type: TYPE_NORMAL
- en: It automatically discovers and categorizes your data by connecting to the data
    sources and generates a data catalog.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services such as Amazon Athena, Amazon Redshift, and Amazon EMR can use the
    data catalog to query the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Glue generates the ETL code, which is an extension to Spark in Python or
    Scala, which can be modified, too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It scales out automatically to match your Spark application requirements for
    running the ETL job and loading the data into the destination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS Glue has **Data Catalog**, and that''s the secret of its success. It helps
    to discover the data from data sources and lets us understand a bit about it:'
  prefs: []
  type: TYPE_NORMAL
- en: Data Catalog automatically discovers new data and extracts schema definitions.
    It detects schema changes and version tables. It detects Apache Hive-style partitions
    on Amazon S3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Catalog comes with built-in classifiers for popular data types. Custom
    classifiers can be written using **Grok expressions**. The classifiers help to
    detect the schema.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glue crawlers can be run ad hoc or in a scheduled fashion to update the metadata
    in Glue Data Catalog. Glue crawlers must be associated with an IAM role with sufficient
    access to read the data sources, such as Amazon RDS, Amazon Redshift, and Amazon
    S3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we now have a brief idea of AWS Glue, let's run the following example to
    get our hands dirty.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with AWS Glue data catalog components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will create a job to copy data from S3 to Redshift by using
    AWS Glue. All my components are created in the `us-east-1` region. Let''s start
    by creating a bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to AWS S3 Console and create a bucket. I have named the bucket `aws-glue-example-01`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on `input-data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate inside the folder and click on the `sales-records.csv` dataset. The
    data is available in the following GitHub location: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6/AWS-Glue-Demo/input-data](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6/AWS-Glue-Demo/input-data).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we have the data uploaded in the S3 bucket, let's create a VPC in which we
    will create our Redshift cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the VPC console by accessing the [https://console.aws.amazon.com/vpc/home?region=us-east-1#](https://console.aws.amazon.com/vpc/home?region=us-east-1#)
    URL and click on `AWS services`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b) `com.amazonaws.us-east-1.s3` (Gateway type)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `Select the Default VPC` (we will use this default VPC in which our Redshift
    cluster will be created)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Leave the other fields as-is and click on **Create Endpoint**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on `redshift-self`, and choose the default VPC drop-down menu. Provide
    an appropriate description of `Redshift Security Group`. Click on **Create security
    group**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the `All traffic`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b) `Custom`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) In the search field, select the same security group `(redshift-self)`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on **Save Rules**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's create our Redshift cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the Amazon Redshift console. Click on **Create Cluster** and complete
    the highlighted fields, as shown in *Figure 6.1*:![Figure 6.1 – A screenshot of
    Amazon Redshift's Create cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_06_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.1 – A screenshot of Amazon Redshift's Create cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scroll down and fill in the highlighted fields, as shown in *Figure 6.2*:![Figure
    6.2 – A screenshot of Amazon Redshift Cluster's Database configurations section
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_06_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.2 – A screenshot of Amazon Redshift Cluster's Database configurations
    section
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scroll down and change the **Additional configurations** field, as shown in
    *Figure 6.3*:![Figure 6.3 – A screenshot of Amazon Redshift Cluster's Additional
    configurations section
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_06_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.3 – A screenshot of Amazon Redshift Cluster's Additional configurations
    section
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Change the IAM permissions, too, as shown in *Figure 6.4*:![Figure 6.4 – A screenshot
    of Amazon Redshift Cluster's Cluster permissions section
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_06_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.4 – A screenshot of Amazon Redshift Cluster's Cluster permissions section
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scroll down and click on **Create Cluster**. It will take a minute or two to
    get the cluster in the available state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will create an IAM role.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the AWS IAM console and select **Roles** in the **Access Management**
    section on the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Create role** button and choose **Glue** from the services.
    Click on the **Next: permissions** button to navigate to the next page.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Search **AmazonS3FullAccess** and select. Then, search **AWSGlueServiceRole**
    and select. As we are writing our data to Redshift as part of this example, select
    **AmazonRedshiftFullAccess**. Click on **Next: Tags**, followed by the **Next:
    Review** button.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a name, `Glue-IAM-Role`, and then click on the **Create role** button.
    The role appears as shown in *Figure 6.5*:![Figure 6.5 – A screenshot of the IAM
    role
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_06_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.5 – A screenshot of the IAM role
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we have the input data source and the output data storage handy. The next
    step is to create the Glue crawler from the AWS Glue console.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select `glue-redshift-connection`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b) `Amazon Redshift`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on `redshift-glue-example`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b) `glue-dev`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) `awsuser`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) `********` (enter the value as created in *step 10*)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on `Glue-IAM-Role` for the IAM role section, and then click **Test Connection**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to `s3-glue-crawler`, and then click **Next**. In the **Specify crawler source
    type** page, leave everything as their default settings and then click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On `s3://aws-glue-example-01/input-data/sales-records.csv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`No`. Click **Next**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Glue-IAM-Role`. Then click **Next**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Run on demand`. Click **Next**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's no database created, so click on `s3-data`, click **Next**, and then
    click **Finish**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the crawler, `s3-glue-crawler`, and then click on `1` in the `s3-data`,
    has been created, as mentioned in the previous step, and a table is added. Click
    on `sales_records_csv`. You can see that the schema has been discovered now. You
    can change the data type if the inferred data type does not meet your requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this hands-on section, we learned about database tables, database connections,
    crawlers on S3, and creation of the Redshift cluster. In the next hands-on section,
    we will learn about creating ETL jobs using Glue.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with AWS Glue ETL components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will use the Data Catalog components created earlier to
    build our job. Let''s start by creating jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS Glue console and click on **Jobs** under the **ETL** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the `s3-glue-redshift`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Glue-IAM-Role` (this is the same role we created in the previous section)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Spark`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Spark 2.4, Python 3 with improved job start up times (Glue version 2.0)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave the other fields as they are and then click on **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `sales_records_csv` and click on **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Change Schema** by default and then click **Next** (at the time of
    writing this book, machine learning transformations are not supported for Glue
    2.0).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `JDBC` as the data store and `glue-redshift-connection` as the connection.
    Provide `glue-dev` as the database name (as created in the previous section) and
    then click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next comes the **Output Schema Definition** page, where you can choose the desired
    columns to be removed from the target schema. Scroll down and click on **Save
    job and edit script**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can now see the pipeline being created on the left-hand side of the screen
    and the suggested code on the right-hand side, as shown in *Figure 6.6*. You can
    modify the code based on your requirements. Click on the **Run job** button. A
    pop-up window appears, asking you to edit any details that you wish to change.
    This is optional. Then, click on the **Run job** button:![Figure 6.6 – A screenshot
    of the AWS Glue ETL job
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_06_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.6 – A screenshot of the AWS Glue ETL job
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the job is successful, navigate to Amazon Redshift and click on **Query
    editor**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the database name as `glue-dev` and then provide the username and password
    to create a connection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the `public` schema and now you can query the table to see the records,
    as shown in *Figure 6.7*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – A screenshot of Amazon Redshift''s Query editor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_06_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – A screenshot of Amazon Redshift's Query editor
  prefs: []
  type: TYPE_NORMAL
- en: We now understand how to create an ETL job using AWS Glue to copy the data from
    the S3 bucket to Amazon Redshift. We also queried the data in Amazon Redshift
    using the query editor from the UI console. It is recommended to delete the Redshift
    cluster and AWS Glue job if you have completed the steps successfully. AWS creates
    two buckets in your account to store the AWS Glue scripts and AWS Glue temporary
    results. Therefore, delete these as well to save costs. We will use the data catalog
    created on S3 data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will learn about querying the S3 data using Athena.
  prefs: []
  type: TYPE_NORMAL
- en: Querying S3 data using Athena
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Athena is a serverless service designed for querying data stored in S3\. It
    is serverless because the client doesn''t manage the servers that are used for
    computation:'
  prefs: []
  type: TYPE_NORMAL
- en: Athena uses a schema to present the results against the query on the data stored
    in S3\. You define how you want your data to appear in the form of a schema and
    Athena reads the raw data from S3 to show the results as per the defined schema.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output can be used by other services for visualization, storing, or various
    analytics purposes. The source data in S3 can be in any of the following structured,
    semi-structured, and unstructured data formats, including XML, JSON, CSV/TSV,
    AVRO, Parquet, ORC, and more. CloudTrail, ELB Logs, and VPC flow logs can also
    be stored in S3 and analyzed by Athena.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This follows the schema-on-read technique. Unlike traditional techniques, tables
    are defined in advance in a data catalog, and data is projected when it reads.
    SQL-like queries are allowed on data without transforming the source data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s understand this with the help of an example, where we will use
    **AWSDataCatlog** created in AWS Glue on the S3 data and query them using Athena:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS Athena console. Select `AWSDataCatalog` from `sampledb`
    database will be created with a table, `elb_logs`, in the AWS Glue data catalog).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `s3-data` as the database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Settings** from the top-right corner and fill in the details as shown
    in *Figure 6.8* (I have used the same bucket as in the previous example and a
    different folder):![Figure 6.8 – A screenshot of Amazon Athena's settings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_06_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.8 – A screenshot of Amazon Athena's settings
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next step is to write your query in the query editor and execute it. Once
    your execution is complete, please delete your S3 buckets and AWS Glue data catalogs.
    This will save you money.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we learned about querying S3 data using Amazon Athena through
    the AWS data catalog. You can also create your schema and query the data from
    S3\. In the next section, we will learn about Amazon Kinesis Data Streams.
  prefs: []
  type: TYPE_NORMAL
- en: Processing real-time data using Kinesis data streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kinesis is Amazon''s streaming service and can be scaled based on requirements.
    It is highly available in a region. It has a level of persistence that retains
    data for 24 hours by default or optionally up to 365 days. Kinesis data streams
    are used for large-scale data ingestion, analytics, and monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: Kinesis can be ingested by multiple producers and multiple consumers can also
    read data from the stream. Let's understand this by means of an example in real
    time. Suppose you have a producer ingesting data to a Kinesis stream and the default
    retention period is 24 hours, which means the data ingested at 05:00:00 A.M. today
    will be available in the stream until 04:59:59 A.M. tomorrow. This data won't
    be available beyond that point and ideally, this should be consumed before it
    expires or can be stored somewhere if it's critical. The retention period can
    be extended to a maximum of 365 days at an extra cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kinesis can be used for real-time analytics or dashboard visualization. Producers
    can be imagined as a piece of code pushing data into the Kinesis stream, and it
    can be an EC2 instance running the code, a Lambda function, an IoT device, on-premises
    servers, mobile applications or devices, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, the consumer can also be a piece of code running on an EC2 instance,
    Lambda function, or on-premises servers that know how to connect to a kinesis
    stream, read the data, and apply some action to the data. AWS provides triggers
    to invoke a lambda consumer as soon as data arrives at the kinesis stream.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kinesis is scalable due to its shard architecture, which is the fundamental
    throughput unit of a kinesis stream. *What is a shard?* A shard is a logical structure
    that partitions the data based on a partition key. A shard supports a writing
    capacity of *1 MB/sec* and a reading capacity of *2 MB/sec*. *1,000* `PUT` records
    per second are supported by a single shard. If you have created a stream with
    *3 shards*, then *3 MB/sec write throughput* and *6 MB/sec read throughput* can
    be achieved and this allows *3,000* `PUT` records. So, with more shards, you have
    to pay an extra amount to get higher performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data in a shard is stored via a Kinesis data record and can be a maximum
    of 1 MB. Kinesis data records are stored across the shard based on the partition
    key. It also has a sequence number. A sequence number is assigned by kinesis as
    soon as a `putRecord` or `putRecords` API operation is performed so as to uniquely
    identify a record. The partition key is specified by the producer while adding
    the data to the Kinesis data stream, and the partition key is responsible for
    segregating and routing the record to different shards in the stream to balance
    the load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two ways to encrypt the data in a kinesis stream: server-side encryption
    and client-side encryption. Client-side encryption is hard to implement and manage
    the keys because the client has to encrypt the data before putting it into the
    stream and decrypt the data after reading from the stream. With server-side encryption
    enabled via **AWS/KMS**, the data is automatically encrypted and decrypted as
    you put the data and get it from a stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Amazon Kinesis shouldn't be confused with Amazon SQS. Amazon SQS supports one
    production group and one consumption group. If your use case demands multiple
    users sending data and receiving data, then Kinesis is the solution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For decoupling and asynchronous communications, SQS is the solution, because
    the sender and receiver do not need to be aware of one another.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In SQS, there is no concept of persistence. Once the message is read, the next
    step is deletion. There's no concept of the retention time window for Amazon SQS.
    If your use case demands large-scale ingestion, then Kinesis should be used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next section, we will learn about storing the streamed data for further
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Storing and transforming real-time data using Kinesis Data Firehose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a lot of use cases demanding the data to be streamed and stored for
    future analytics purposes. To overcome such problems, you can write a kinesis
    consumer to read the Kinesis stream and store the data in S3\. This solution needs
    an instance or a machine to run the code with the required access to read from
    the stream and write to S3\. The other possible option would be to run a Lambda
    function that gets triggered on the `putRecord` or `putRecords` API made to the
    stream and reads the data from the stream to store in the S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: To make this easy, Amazon provides a separate service called Kinesis Data Firehose.
    This can easily be plugged into a Kinesis data stream and it will require essential
    IAM roles to write data into S3\. This is a fully managed service to reduce the
    load of managing servers and code. It also supports loading the streamed data
    into Amazon Redshift, Amazon Elasticsearch Service, and Splunk. Kinesis Data Firehose
    scales automatically to match the throughput of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data can be transformed via an AWS Lambda function before storing or delivering
    it to the destination. If you want to build a raw data lake with the untransformed
    data, then by enabling source record backup, you can store it in another S3 bucket
    prior to the transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of AWS/KMS, data can be encrypted following delivery to the S3
    bucket. It has to be enabled while creating the delivery stream. Data can also
    be compressed in supported formats such as GZIP, ZIP, and SNAPPY compression formats.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will learn about different AWS services used for ingesting
    data from on-premises servers to AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Different ways of ingesting data from on-premises into AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the increasing demand for data-driven use cases, managing data on on-premises
    servers is pretty tough at the moment. Taking backups is not easy when you deal
    with a huge amount of data. This data in data lakes is being used to build deep
    neural networks, to create a data warehouse to extract meaningful information
    from it, to run analytics, and to generate reports.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we look at the available options to migrate data into AWS, then it comes
    with various challenges, too. For example, if you want to send data to S3, then
    you have to write a few lines of code to send your data to AWS. You will have
    to manage the code and servers to run the code. It has to be ensured that the
    data is commuting via the HTTPS network. You need to verify whether the data transfer
    was successful. This adds complexity as well as time and effort challenges to
    the process. To avoid such scenarios, AWS provides services to match or solve
    your use cases by designing hybrid infrastructure that allows data sharing between
    the on-premises data centers and AWS. Let's learn about these in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Storage Gateway
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storage Gateway is a hybrid storage virtual appliance. It can run in three
    different modes – **File Gateway**, **Tape Gateway**, and **Volume Gateway**.
    It can be used for extension, migration, and backups of the on-premises data center
    to AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: In Tape Gateway mode, the storage gateway stores virtual tapes on S3, and when
    ejected and archived, the tapes are moved from S3 to Glacier. Active tapes are
    stored in S3 for storage and retrieval. Archived or exported tapes are stored
    in **Virtual Tape Shelf** (**VTS**) in Glacier. Virtual tapes can be created and
    can range in size from 100 GiB to 5 TiB. A total of 1 petabyte of storage can
    be configured locally and an unlimited number of tapes can be archived to Glacier.
    This is ideal for an existing backup system on tape and where there is a need
    to migrate backup data into AWS. You can decommission the physical tape hardware
    later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In File Gateway mode, the storage gateway maps files onto S3 objects, which
    can be stored using one of the available storage classes. This helps you to extend
    the data center into AWS. You can load more files to your File Gateway and these
    are stored as S3 objects. It can run on your on-premises virtual server, which
    connects to various devices using **Server Message Block (SMB)** or **Network
    File System (NFS)**. The File Gateway connects to AWS using an HTTPS public endpoint
    to store the data on S3 objects. Life cycle policies can be applied to those S3
    objects. You can easily integrate your **Active** **Directory** (**AD**) with
    File Gateway to control access to the files on the file share.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Volume Gateway mode, the storage gateway presents block storage. There are
    two ways of using this, one is **Gateway Cached**, and the other is **Gateway
    Stored**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gateway Stored** is a volume storage gateway running locally on-premises
    and it has local storage and an upload buffer. A total of 32 volumes can be created
    and each volume can be up to 16 TB in size for a total capacity of 512 TB. Primary
    data is stored on-premises and backup data is asynchronously replicated to AWS
    in the background. Volumes are made available via **Internet Small Computer Systems
    Interface (iSCSI)** for network-based servers to access. It connects to **Storage
    Gateway Endpoint** via an HTTPS public endpoint and creates EBS snapshots from
    backup data. These snapshots can be used to create standard EBS volumes. This
    option is ideal for migration to AWS or disaster recovery or business continuity.
    The local system will still use the local volume, but the EBS snapshots are in
    AWS, which can be used instead of backups. It''s not the best option for data
    center extension because you require a huge amount of local storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gateway Cached** is a volume storage gateway running locally on-premises
    and it has cache storage and an upload buffer. The difference is that the data
    that is added to the storage gateway is not local but uploaded to AWS. Primary
    data is stored in AWS. Frequently accessed data is cached locally. This is an
    ideal option for extending the on-premises data center to AWS. It connects to
    **Storage Gateway Endpoint** via an HTTPS public endpoint and creates **S3 Backed
    Volume (AWS-managed bucket)** snapshots that are stored as standard EBS snapshots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snowball, Snowball Edge, and Snowmobile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'They belong to the same product clan for the physical transfer of data between
    business operating locations and AWS. For moving a large amount of data into and
    out of AWS, you can use any of the three:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Snowball**: This can be ordered from AWS by logging a job. AWS delivers the
    device to load your data and send it back. Data in Snowball is encrypted using
    KMS. It comes with two capacity ranges, one 50 TB and the other 80 TB. It is economical
    to order one or more Snowball devices for data between 10 TB and 10 PB. The device
    can be sent to different premises. It does not have any compute capability; it
    only comes with storage capability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snowball Edge**: This is like Snowball, but it comes with both storage and
    computes capability. It has a larger capacity than Snowball. It offers fastened
    networking, such as 10 Gbps over RJ45, 10/25 Gb over SFP28, and 40/100 Gb+ over
    QSFP+ copper. This is ideal for the secure and quick transfer of terabytes to
    petabytes of data into AWS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snowmobile**: This is a portable data center within a shipping container
    on a truck. This allows you to move exabytes of data from on-premises to AWS.
    If your data size exceeds 10 PB, then Snowmobile is preferred. Essentially, upon
    requesting Snowmobile, a truck is driven to your location, you plug your data
    center into the truck, and transfer the data. If you have multiple sites, choosing
    Snowmobile for data transfer is not an ideal option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS DataSync
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AWS DataSync is designed to move data from on-premises storage to AWS, or vice
    versa:'
  prefs: []
  type: TYPE_NORMAL
- en: It is an ideal product from AWS for data processing transfers, archival or cost-effective
    storage, disaster recovery, business continuity, and data migrations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a special data validation feature that verifies the original data with
    the data in AWS, as soon as the data arrives in AWS. In other words, it checks
    the integrity of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's understand this product in depth by considering an example of an on-premises
    data center that has SAN/NAS storage. When we run the AWS DataSync agent on a
    VMWare platform, this agent is capable of communicating with the NAS/SAN storage
    via an NFS/SMB protocol. Once it is on, it communicates with the AWS DataSync
    endpoint and, from there, it can connect with several different types of location,
    including various S3 storage classes or VPC-based resources, such as the **Elastic**
    **File** **System** (**EFS**) and FSx for Windows Server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows you to schedule data transfers during specific periods. By configuring
    the built-in bandwidth throttle, you can limit the amount of network bandwidth
    that DataSync uses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing stored data on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several services for processing the data stored in AWS. We will go
    through AWS Batch and AWS EMR (Elastic MapReduce) in this section. EMR is a product
    from AWS that primarily runs MapReduce jobs and Spark applications in a managed
    way. AWS Batch is used for long-running, compute-heavy workloads.
  prefs: []
  type: TYPE_NORMAL
- en: AWS EMR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'EMR is a managed implementation of Apache Hadoop provided as a service by AWS.
    It includes other components of the Hadoop ecosystem, such as Spark, HBase, Flink,
    Presto, Hive, Pig, and many more. We will not cover these in detail for the certification
    exam:'
  prefs: []
  type: TYPE_NORMAL
- en: EMR clusters can be launched from the AWS console or via the AWS CLI with a
    specific number of nodes. The cluster can be a long-term cluster or an ad hoc
    cluster. If you have a long-running traditional cluster, then you have to configure
    the machines and manage them yourself. If you have jobs to be executed faster,
    then you need to manually add a cluster. In the case of EMR, these admin overheads
    are gone. You can request any number of nodes to EMR and it manages and launches
    the nodes for you. If you have autoscaling enabled on the cluster, then with the
    increased requirement, EMR launches new nodes in the cluster and decommissions
    the nodes once the load is reduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMR uses EC2 instances in the background and runs in one availability zone in
    a VPC. This enables faster network speeds between the nodes. AWS Glue uses EMR
    clusters in the background, where users do not need to worry about the operational
    understanding of AWS EMR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a use case standpoint, EMR can be used for processing or transforming the
    data stored in S3 and outputs data to be stored in S3\. EMR uses nodes (EC2 instances)
    as the computing units for data processing. EMR nodes come in different variants,
    including Master node, Core node, and Task node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EMR master node acts as a Hadoop namenode and manages the cluster and its
    health. It is responsible for distributing the job workload among the other core
    nodes and task nodes. If you have SSH enabled, then you can connect to the master
    node instance and access the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EMR cluster can have one or more core nodes. If you relate to the Hadoop
    ecosystem, then core nodes are similar to Hadoop data nodes for HDFS and they
    are responsible for running tasks therein.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task nodes are optional and they don't have HDFS storage. They are responsible
    for running tasks. If a task node fails for some reason, then this does not impact
    HDFS storage, but a core node failure causes HDFS storage interruptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMR has its filesystem called EMRFS. It is backed by S3 and this feature makes
    it regionally resilient. If a core node fails, the data is still safe in S3\.
    HDFS is efficient in terms of I/O and faster than EMRFS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will learn about AWS Batch, which is a managed
    batch processing compute service that can be used for long-running services.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Batch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a managed batch processing product. If you are using AWS Batch, then
    jobs can be run without end user interaction or can be scheduled to run:'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine an event-driven application that launches a Lambda function to process
    the data stored in S3\. If the processing time goes beyond 15 minutes, then Lambda
    has the execution time limit. For such scenarios, AWS Batch is a better solution,
    where computation-heavy workloads can be scheduled or driven through API events.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Batch is a good fit for use cases where a longer processing time is required
    or more computation resources are needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Batch runs a job that can be a script or an executable. One job can depend
    on another job. A job needs a definition, such as who can run a job (with IAM
    permissions), where the job can be run (resources to be used), mount points, and
    other metadata.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs are submitted to queues where they wait for compute environment capacity.
    These queues are associated with one or more compute environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute environments do the actual work of executing the jobs. These can be
    ECS or EC2 instances, or any computing resources. You can define their sizes and
    capacities too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environments receive jobs from the queues based on their priority and execute
    the jobs. They can be managed or unmanaged compute environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Batch can store the metadata in DynamoDB for further use and can also store
    the output to the S3 bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you get a question in exams on an event-style workload that requires flexible
    compute, a higher disk space, no time limit (more than 15 minutes), or an effective
    resource limit, then choose AWS Batch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about different ways of processing data in AWS.
    We also learned the capabilities in terms of extending our data centers to AWS,
    migrating data to AWS, and the ingestion process. We also learned the various
    ways of using the data to process it and make it ready for analysis in our way.
    We understood the magic of having a data catalog that helps us to query our data
    via AWS Glue and Athena.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about various machine learning algorithms
    and their usages.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have a large number of IoT devices sending data to AWS to be consumed
    by a large number of mobile devices, which of the following should you choose?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. SQS standard queue
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. SQS FIFO queue
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Kinesis stream
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you have a requirement to decouple a high-volume application, which of the
    following should you choose?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. SQS standard queue
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. SQS FIFO queue
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Kinesis stream
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following do I need to change to improve the performance of a Kinesis
    stream?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. The read capacity unit of a stream
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. The write capacity unit of a stream
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Shards of a stream
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. The region of a stream
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following ensures data integrity?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. AWS Kinesis
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. AWS DataSync
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. AWS EMR
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Snowmobile
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which storage gateway mode can replace a tape drive with S3 storage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Volume Gateway Stored
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Volume Gateway Cached
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. File
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. VTL
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which storage gateway mode can be used to present storage over SMB to clients?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Volume Gateway Stored
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Volume Gateway Cached
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. File
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. VTL
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which storage gateway mode is ideal for data center extension to AWS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Volume Gateway Stored
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Volume Gateway Cached
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. File
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. VTL
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What storage product in AWS can be used for Windows environments for shared
    storage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. S3
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. FSx
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. EBS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. EFS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which node within an EMR cluster handles operations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Master node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Core node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Task node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Primary node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which nodes in an EMR cluster are good for spot instances?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Master node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Core node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Task node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Primary node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you have large quantities of streaming data and add it to Redshift, which
    services would you use (choose three)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Kinesis Data Streams
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Kinesis Data Firehose
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. S3
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. SQS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: E. Kinesis Analytics
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Kinesis Firehose supports data transformation using Lambda.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. True
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. False
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of the following are valid destinations for Kinesis Data Firehose (choose
    five)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. HTTP
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Splunk
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Redshift
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. S3
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: E. Elastic Search
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: F. EC2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: G. SQS
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. C
  prefs: []
  type: TYPE_NORMAL
- en: 2\. A
  prefs: []
  type: TYPE_NORMAL
- en: 3\. C
  prefs: []
  type: TYPE_NORMAL
- en: 4\. B
  prefs: []
  type: TYPE_NORMAL
- en: 5\. D
  prefs: []
  type: TYPE_NORMAL
- en: 6\. C
  prefs: []
  type: TYPE_NORMAL
- en: 7\. B
  prefs: []
  type: TYPE_NORMAL
- en: 8\. B
  prefs: []
  type: TYPE_NORMAL
- en: 9\. A
  prefs: []
  type: TYPE_NORMAL
- en: 10\. C
  prefs: []
  type: TYPE_NORMAL
- en: 11\. A, B, C
  prefs: []
  type: TYPE_NORMAL
- en: 12\. A
  prefs: []
  type: TYPE_NORMAL
- en: 13\. A, B, C, D, E
  prefs: []
  type: TYPE_NORMAL
