<html><head></head><body>
		<div id="_idContainer109">
			<h1 id="_idParaDest-130"><em class="italic"><a id="_idTextAnchor152"/>Chapter 10</em>: Scaling Up Your Machine Learning Workflow</h1>
			<p>In this chapter, you will learn about diverse techniques and patterns to scale your <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) workflow in different scalability dimensions. We will look at using a Databricks managed environment to scale your MLflow development capabilities, adding Apache Spark for cases where you have larger datasets. We will explore NVIDIA RAPIDS and <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) support, and the Ray distributed frameworks to accelerate your ML workloads. The format of this chapter is a small <strong class="bold">proof-of-concept</strong> with a defined canonical dataset to demonstrate a technique and toolchain.</p>
			<p>Specifically, we will look at the following sections in this chapter: </p>
			<ul>
				<li>Developing models with a Databricks Community Edition environment</li>
				<li>Integrating MLflow with Apache Spark </li>
				<li>Integrating MLflow with NVIDIA RAPIDS (GPU) </li>
				<li>Integrating MLflow with the Ray platform</li>
			</ul>
			<p>This chapter will require researching the appropriate setup for each framework introduced, based on the standard official documentation for each of the cases. </p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor153"/>Technical requirements</h1>
			<p>For this chapter, you will need the following prerequisites: </p>
			<ul>
				<li>The latest version of Docker installed on your machine. If you don't already have it installed, please follow the instructions at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
				<li>The latest version of Docker Compose installed—please follow the instructions at <a href="https://docs.docker.com/compose/install/">https://docs.docker.com/compose/install/</a>.</li>
				<li>Access to Git in the command line, and installed as described in <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>.</li>
				<li>Access to a Bash terminal (Linux or Windows). </li>
				<li>Access to a browser.</li>
				<li>Python 3.5+ installed.</li>
				<li>The latest version of your ML library installed locally as described in <a href="B16783_03_Final_SB_epub.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a>, <em class="italic">Your Data Science Workbench</em>.</li>
				<li>An <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) account configured to run the MLflow model.</li>
			</ul>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor154"/>Developing models with a Databricks Community Edition environment</h1>
			<p>In many<a id="_idIndexMarker344"/> scenarios<a id="_idIndexMarker345"/> of small teams and companies, starting up a centralized ML environment might be a costly, resource-intensive, upfront investment. A team being able to quickly scale and getting a team up to speed is critical to unlocking the value of ML in an organization. The use of managed services is very relevant in these cases to start prototyping systems and to begin to understand the viability of using ML at a lower cost.</p>
			<p>A very popular managed ML and data platform is the Databricks platform, developed by the same company that developed MLflow. We will use in this section the Databricks Community Edition version and license targeted for students and personal use. </p>
			<p>In order to explore the Databricks platform to develop and share models, you need to execute the following steps:</p>
			<ol>
				<li>Sign up to <a id="_idIndexMarker346"/>Databricks Community Edition at <a href="https://community.cloud.databricks.com/">https://community.cloud.databricks.com/</a> and create an account.</li>
				<li>Log in to your account with your just-created credentials.</li>
				<li>Upload training data into Databricks. You can start by uploading the training data available in the <strong class="source-inline">Chapter10/databricks_notebooks/training_data.csv</strong> folder. In the following screenshot, you can see represented the <strong class="bold">Data</strong> tab on the left, and <a id="_idIndexMarker347"/>you<a id="_idIndexMarker348"/> should see your file uploaded to the platform:<div id="_idContainer103" class="IMG---Figure"><img src="image/image0016.jpg" alt="Figure 10.1 – Uploading training data to Databricks"/></div><p class="figure-caption">Figure 10.1 – Uploading training data to Databricks</p></li>
				<li>Upload training data to Databricks. You can start by uploading the training data available in the <strong class="source-inline">Chapter10/databricks_notebooks/input_prediction.csv</strong>  folder.</li>
				<li>Create a cluster to use for your workloads. You are allowed to have clusters for your workloads with a limit of 15 <strong class="bold">gigabytes</strong> (<strong class="bold">GB</strong>) of <strong class="bold">random-access memory</strong> (<strong class="bold">RAM</strong>) and with usage for a defined period of time. <p>You can<a id="_idIndexMarker349"/> see <a id="_idIndexMarker350"/>an overview of the cluster-creation process in the following screenshot:</p><div id="_idContainer104" class="IMG---Figure"><img src="image/image0027.jpg" alt=""/></div><p class="figure-caption">Figure 10.2 – Creating a cluster in Databricks Community Edition</p></li>
				<li>Create a new notebook in your Databricks platform on your landing workspace page by clicking on the <strong class="bold">Create a Blank Notebook</strong> button at the top right of the page, as illustrated in the following screenshot:<div id="_idContainer105" class="IMG---Figure"><img src="image/Image_003.jpg" alt="Figure 10.3 – Creating a new notebook in Databricks Community Edition"/></div><p class="figure-caption">Figure 10.3 – Creating a new notebook in Databricks Community Edition</p></li>
				<li>We <a id="_idIndexMarker351"/>are <a id="_idIndexMarker352"/>now ready to start a notebook to execute a basic training job in this managed environment. You can start by clicking on <strong class="bold">Create Notebook</strong>, as illustrated in the following screenshot:<div id="_idContainer106" class="IMG---Figure"><img src="image/image0047.jpg" alt="Figure 10.4 – Creating your new notebook"/></div><p class="figure-caption">Figure 10.4 – Creating your new notebook</p></li>
				<li>Upload <a id="_idIndexMarker353"/>training <a id="_idIndexMarker354"/>data to Databricks. You can start by uploading the training data available in the <strong class="source-inline">Chapter10/databricks_notebooks/input_prediction.csv</strong>  folder.</li>
				<li>Import the needed libraries. We will adapt a <strong class="source-inline">LogicRegression</strong> model used to classify our running business case of the price of a <strong class="source-inline">btc-usd</strong> ticker, as follows:<p class="source-code">import pandas</p><p class="source-code">import numpy as np</p><p class="source-code">import mlflow</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.metrics import f1_score, confusion_matrix</p><p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li>To read the data, due to the usage of the Databricks filesystem in the platform, it is more convenient to read the data in Spark and convert thereafter the DataFrame into <strong class="source-inline">pandas</strong>. We also split the data into training and test sets, as usual. Here<a id="_idIndexMarker355"/> is <a id="_idIndexMarker356"/>the code you'll need for this:<p class="source-code">df = (spark.read.option("header","true").csv("/FileStore/tables/training_data.csv"))</p><p class="source-code">pandas_df = df.toPandas()</p><p class="source-code">X=pandas_df.iloc[:,:-1]</p><p class="source-code">Y=pandas_df.iloc[:,-1]</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=4284, stratify=Y)</p></li>
				<li>Our next step will be to quickly train our classifier, as follows:<p class="source-code">mlflow.sklearn.autolog()</p><p class="source-code">model = LogisticRegression()</p><p class="source-code">with mlflow.start_run(run_name='logistic_regression_model_baseline') as run:</p><p class="source-code">    model.fit(X_train, y_train)</p><p class="source-code">    preds = model.predict(X_test)</p></li>
				<li>In the top corner of the page, you can click on the <strong class="bold">Experiment</strong> button to view more details about your run, and you can click further to look at your model experiment, in the familiar interface of experiments, as illustrated in the following screenshot:<div id="_idContainer107" class="IMG---Figure"><img src="image/image0055.jpg" alt=" Figure 10.5 – Experiment button"/></div><p class="figure-caption"> Figure 10.5 – Experiment button</p></li>
				<li>One interesting feature that can scale and accelerate your ability to collaborate with <a id="_idIndexMarker357"/>others is<a id="_idIndexMarker358"/> the ability to publish model notebooks that are publicly accessible to everyone with whom you share a link, as illustrated in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/image0064.jpg" alt="Figure 10.6 – Publishing notebooks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Publishing notebooks</p>
			<p>You can also export your notebook as a <strong class="source-inline">dbc</strong> file so that you can quickly start it up in a Databricks environment, and you can also share it in a repository, as you can see in the chapter folder, under <strong class="source-inline">/databricks-notebooks/bitpred_poc.dbc</strong>.</p>
			<p>Having dealt with ways to scale your ability to run, develop, and distribute models using a Databricks<a id="_idIndexMarker359"/> environment, we <a id="_idIndexMarker360"/>will next look at integrating an Apache Spark flow into our inference workflows to handle scenarios where we have access to large datasets.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor155"/>Integrating MLflow with Apache Spark </h1>
			<p>Apache<a id="_idIndexMarker361"/> Spark is a very scalable and popular <a id="_idIndexMarker362"/>big data framework that allows data processing at a large scale. For more details and documentation, please<a id="_idIndexMarker363"/> go to <a href="https://spark.apache.org/">https://spark.apache.org/</a>. As a big data tool, it can be used to speed up parts of your ML inference, as it can be set at a training or an inference level.</p>
			<p>In this particular case, we will illustrate how to implement it to use the model developed in the previous section on the Databricks environment to scale the batch-inference job to larger amounts of data.</p>
			<p>In other to explore Spark integration with MLflow, we will execute the following steps:</p>
			<ol>
				<li value="1">Create a new notebook named <strong class="source-inline">inference_job_spark</strong> in Python, linking to a running cluster where the <strong class="source-inline">bitpred_poc.ipynb</strong> notebook was just created.</li>
				<li>Upload your data to <strong class="source-inline">dbfs</strong> on the File/Upload data link in the environment.  </li>
				<li>Execute the following script in a cell of the notebook, changing the <strong class="source-inline">logged_model</strong> and <strong class="source-inline">df</strong> filenames for the ones in your environment:<p class="source-code">import mlflow</p><p class="source-code">logged_model = 'runs:/6815b44128e14df2b356c9db23b7f936/model'</p><p class="source-code">df = spark.read.format("csv").load("dbfs:/FileStore/shared_uploads/ input.csv")</p><p class="source-code"># Load model as a Spark UDF.</p><p class="source-code">loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model)</p><p class="source-code"># Predict on a Spark DataFrame.</p><p class="source-code">df.withColumn('predictions', loaded_model()).collect()</p></li>
			</ol>
			<p>This illustrative excerpt running on Databricks or on your own Spark cluster can scale to large datasets, using <a id="_idIndexMarker364"/>the power of distributed <a id="_idIndexMarker365"/>computing in Spark.</p>
			<p>From scaling inference with Apache Spark, we will look now at using GPUs with the support of MLflow to scale hyperparameter optimization jobs.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor156"/>Integrating MLflow with NVIDIA RAPIDS (GPU)</h1>
			<p>Training<a id="_idIndexMarker366"/> and tuning ML models is a long and<a id="_idIndexMarker367"/> computationally expensive operation and is one of the operations that can benefit the most from parallel processing. We will explore in this section the integration of your MLflow training jobs, including hyperparameter optimization, with the NVIDIA RAPIDS framework. </p>
			<p>To integrate the NVIDIA RAPIDS library, follow the next steps:</p>
			<ol>
				<li value="1">Install RAPIDS in the most convenient way for your environment, outlined as follows:<p>a. <a href="https://rapids.ai/start.html">https://rapids.ai/start.html</a> contains detailed information on deployment options.</p><p>b. <a href="https://developer.nvidia.com/blog/run-rapids-on-google-colab/">https://developer.nvidia.com/blog/run-rapids-on-google-colab/</a> details how to run <a id="_idIndexMarker368"/>RAPIDS on <strong class="bold">Google Colaboratory</strong> (<strong class="bold">Google Colab</strong>).</p></li>
				<li>Install MLflow in your environment.</li>
				<li>Import <a id="_idIndexMarker369"/>the needed libraries, as<a id="_idIndexMarker370"/> follows:<p class="source-code">import argparse</p><p class="source-code">from functools import partial</p><p class="source-code">import mlflow</p><p class="source-code">import mlflow.sklearn</p><p class="source-code">from cuml.metrics.accuracy import accuracy_score</p><p class="source-code">from cuml.preprocessing.model_selection import train_test_split</p><p class="source-code">from cuml.ensemble import RandomForestClassifier</p><p class="source-code">from hyperopt import fmin, tpe, hp, Trials, STATUS_OK</p></li>
				<li>Implement the <strong class="source-inline">load_data</strong> function, which is a helper function for loading data to be<a id="_idIndexMarker371"/> used by <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>)/GPU models. The <strong class="source-inline">cudf</strong> DataFrame is a DataFrame library for loading, joining, aggregating, and filtering without knowing the details of <strong class="bold">Compute Unified Device Architecture</strong> (<strong class="bold">CUDA</strong>) programming. Here is the code <a id="_idIndexMarker372"/>you'll need:<p class="source-code">def load_data(fpath):</p><p class="source-code">    import cudf</p><p class="source-code">    df = cudf.read_parquet(fpath)</p><p class="source-code">    X = df.drop(["ArrDelayBinary"], axis=1)</p><p class="source-code">    y = df["ArrDelayBinary"].astype("int32")</p><p class="source-code">    return train_test_split(X, y, test_size=0.2)Start the ray server </p><p class="source-code">ray.init()</p><p class="source-code">client = serve.start()</p></li>
				<li>Define <a id="_idIndexMarker373"/>a<a id="_idIndexMarker374"/> training loop, as follows:<p class="source-code">def _train(params, fpath):</p><p class="source-code">    max_depth, max_features, n_estimators = params</p><p class="source-code">    max_depth, max_features, n_estimators = (int(max_</p><p class="source-code">depth), float(max_features), int(n_estimators))</p><p class="source-code">    X_train, X_test, y_train, y_test = load_data(fpath)</p><p class="source-code">    mod = RandomForestClassifier(</p><p class="source-code">        max_depth=max_depth, max_features=max_features, n_estimators=n_estimators</p><p class="source-code">    )</p><p class="source-code">    mod.fit(X_train, y_train)</p><p class="source-code">    preds = mod.predict(X_test)</p><p class="source-code">    acc = accuracy_score(y_test, preds)</p><p class="source-code">    mlparams = {</p><p class="source-code">        "max_depth": str(max_depth),</p><p class="source-code">        "max_features": str(max_features),</p><p class="source-code">        "n_estimators": str(n_estimators),</p><p class="source-code">    }</p><p class="source-code">    mlflow.log_params(mlparams)</p><p class="source-code">    mlflow.log_metric("accuracy", acc)</p><p class="source-code">    mlflow.sklearn.log_model(mod, "saved_models")</p><p class="source-code">    return {"loss": acc, "status": STATUS_OK}</p></li>
				<li>Call <a id="_idIndexMarker375"/>the<a id="_idIndexMarker376"/> inner training loop, like this:<p class="source-code">def train(params, fpath, hyperopt=False):</p><p class="source-code">    </p><p class="source-code">    with mlflow.start_run(nested=True):</p><p class="source-code">        return _train(params, fpath, hyperopt)</p></li>
				<li>Set up your main flow by reading an argument, if you are using the version deployed in Docker. The code to do this is illustrated in the following snippet:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    parser = argparse.ArgumentParser()</p><p class="source-code">    parser.add_argument("--algo", default="tpe", </p><p class="source-code">choices=["tpe"], type=str)</p><p class="source-code">    parser.add_argument("--conda-env", required=True, type=str)</p><p class="source-code">    parser.add_argument("--fpath", required=True, type=str)</p><p class="source-code">    args = parser.parse_args()</p></li>
				<li>Define your trials and parameters to optimize, as follows:<p class="source-code">    search_space = [</p><p class="source-code">        hp.uniform("max_depth", 5, 20),</p><p class="source-code">        hp.uniform("max_features", 0.1, 1.0),</p><p class="source-code">        hp.uniform("n_estimators", 150, 1000),</p><p class="source-code">    ]</p><p class="source-code">    trials = Trials()</p><p class="source-code">    algorithm = tpe.suggest if args.algo == "tpe" else None</p><p class="source-code">    fn = partial(train, fpath=args.fpath, hyperopt=True)</p><p class="source-code">    experid = 0</p></li>
				<li>Run<a id="_idIndexMarker377"/> your <a id="_idIndexMarker378"/>main loop, as follows:<p class="source-code">    artifact_path = "Airline-Demo"</p><p class="source-code">    artifact_uri = None</p><p class="source-code">    with mlflow.start_run(run_name="RAPIDS-Hyperopt"):</p><p class="source-code">        argmin = fmin(fn=fn, space=search_space, algo=algorithm, max_evals=2, trials=trials)</p><p class="source-code">        print("===========")</p><p class="source-code">        fn = partial(train, fpath=args.fpath, hyperopt=False)</p><p class="source-code">        final_model = fn(tuple(argmin.values()))</p><p class="source-code">        mlflow.sklearn.log_model(</p><p class="source-code">            final_model,</p><p class="source-code">            artifact_path=artifact_path,</p><p class="source-code">            registered_model_name="rapids_mlflow_cli",</p><p class="source-code">            conda_env="envs/conda.yaml",</p><p class="source-code">        )</p></li>
			</ol>
			<p>After having dealt with using a highly scalable compute environment to serve models on top of the Ray <a id="_idIndexMarker379"/>platform, we will now consider <a id="_idIndexMarker380"/>a different problem, where we will look at options to track multiple runs from a local machine in a centralized cloud location.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor157"/>Integrating MLflow with the Ray platform</h1>
			<p>The Ray<a id="_idIndexMarker381"/> framework (<a href="https://docs.ray.io/en/master/">https://docs.ray.io/en/master/</a>) is<a id="_idIndexMarker382"/> a distributed platform that allows you to<a id="_idIndexMarker383"/> quickly scale the deployment infrastructure.</p>
			<p>With Ray, you can add arbitrary logic when running an ML platform that needs to scale in the same way as model serving. It's basically a web framework.</p>
			<p>We preloaded the model and contents that will be used into the following folder of the repository: https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter10/mlflow-ray-serve-integration.</p>
			<p>In order to execute your model serving into Ray, execute the following steps:</p>
			<ol>
				<li value="1">Install the Ray package by running the following command:<p class="source-code">pip install -U ray</p></li>
				<li>Install MLflow in your environment.</li>
				<li>Import the needed libraries, as follows:<p class="source-code">import ray</p><p class="source-code">from ray import serve</p><p class="source-code">import mlflow.pyfunc</p></li>
				<li>Implement<a id="_idIndexMarker384"/> the model backend, which basically <a id="_idIndexMarker385"/>means wrapping up the model-serving function into your Ray serving environment. Here's the code you'll need:<p class="source-code">class MLflowBackend:</p><p class="source-code">    def __init__(self, model_uri):</p><p class="source-code">        self.model = mlflow.pyfunc.load_model(model_</p><p class="source-code">uri=model_uri)</p><p class="source-code">    async def __call__(self, request):</p><p class="source-code">        return self.model.predict(request.data)</p></li>
				<li>Start the Ray server, as follows: <p class="source-code">ray.init()</p><p class="source-code">client = serve.start()</p></li>
				<li>Load the model and create a backend, like this:<p class="source-code">model_uri = "./tmp/0/31fc9974587243d181fdbebfd4d2b6ad/artifacts/model"</p><p class="source-code">client.create_backend("mlflow_backend", MLflowBackend, model_uri)</p></li>
				<li>Test the serving platform by running the following command:<p class="source-code">ray start --head # Start local Ray cluster.</p><p class="source-code">serve start # Start Serve on the local Ray cluster.</p></li>
			</ol>
			<p>After having <a id="_idIndexMarker386"/>dealt with using a highly scalable <a id="_idIndexMarker387"/>compute environment to serve models on top of the Ray platform, we will look at the performance and monitorin<a id="_idTextAnchor158"/>g component in the following chapter.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor159"/>Summary</h1>
			<p>In this chapter, we focused on scaling your ability to run, develop, and distribute models using a Databricks environment. We also looked at integrating an Apache Spark flow into our batch-inference workflows to handle scenarios where we have access to large datasets. </p>
			<p>We concluded the chapter with two approaches to scale hyperparameter optimization and <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) serving with scalability, using the NVIDIA RAPIDS framework and the Ray distributed framework.</p>
			<p>In the next chapter and in further sections of the book, we will focus on the observability and performance monitoring of ML models.</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor160"/>Further reading</h1>
			<p>In order to further your knowledge, you can consult the documentation at the following links: </p>
			<ul>
				<li><a href="https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html">https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html</a></li>
				<li><a href="https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/">https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/</a></li>
				<li><a href="https://docs.databricks.com/applications/mlflow/index.html">https://docs.databricks.com/applications/mlflow/index.html</a></li>
			</ul>
		</div>
	</body></html>