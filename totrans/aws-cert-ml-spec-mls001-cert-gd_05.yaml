- en: '*Chapter 3*: Data Preparation and Transformation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have probably heard that data scientists spend most of their time working
    on data preparation-related activities. It is now time to explain why that happens
    and which types of activities we are talking about.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to deal with categorical and numerical features,
    as well as applying different techniques to transform your data, such as one-hot
    encoding, binary encoders, ordinal encoding, binning, and text transformations.
    You will also learn how to handle missing values and outliers in your data, which
    are two important tasks you can implement to build good machine learning models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Identifying types of features
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with categorical features
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with numerical features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data distributions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing values
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with outliers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with unbalanced datasets
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with text data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a lengthy chapter, so bear with us! Knowing about these topics in detail
    will definitely put you in a good position for the AWS Machine Learning Specialty
    exam.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Identifying types of features
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We *cannot* start modeling without knowing what a **feature** is and which
    type of information it might store. You have already read about different processes
    that deal with features. For example, you know that feature engineering is related
    to the task of building and preparing features to your models; you also know that
    feature selection is related to the task of choosing the best set of features
    to feed a particular algorithm. These two tasks have one behavior in common: they
    may vary according to the types of features they are processing.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to understand this behavior (feature type versus applicable
    transformations) because it will help you eliminate invalid answers during your
    exam (and, most importantly, you will become a better data scientist).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'When we refer to types of features, we are talking about the data type that
    a particular feature is supposed to store. The following diagram shows how we
    could potentially describe the different types of features of a model:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Feature types'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Feature types
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 1*](B16735_01_Final_VK_ePub.xhtml#_idTextAnchor014)*, Machine
    Learning Fundamentals*, you were introduced to the feature classification shown
    in the preceding diagram. Now, let''s look at some real examples in order to eliminate
    any remaining questions you may have:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Real examples of feature values'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_002.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Real examples of feature values
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Although looking at the values of the variable may help you find its type,
    you should never rely only on this aspect. The nature of the variable is also
    very important for making such decisions. For example, someone could encode the
    cloud provider variable (shown in the preceding table) as follows: 1 (AWS), 2
    (MS), 3 (Google). In that case, the variable is still a nominal feature, even
    if it is now represented by discrete numbers.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然查看变量的值可能有助于您找到其类型，但您绝不应该只依赖这个方面。变量的性质对于做出这样的决定也非常重要。例如，有人可以将云服务提供商变量（如前表所示）编码如下：1（AWS），2（MS），3（Google）。在这种情况下，变量仍然是一个名义特征，即使它现在由离散数字表示。
- en: If you are building a ML model and you don't tell your algorithm that this variable
    is not a discrete number, but instead a nominal variable, the algorithm will treat
    it as a number and the model won't be interpretable anymore.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在构建机器学习模型，并且没有告诉您的算法这个变量不是一个离散数字，而是一个名义变量，那么算法将把它当作一个数字处理，模型将不再可解释。
- en: Important note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before feeding any ML algorithm with data, make sure your feature types have
    been properly identified.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在将任何机器学习算法与数据结合之前，请确保您的特征类型已经被正确识别。
- en: 'In theory, if you are happy with your features and have properly classified
    each of them, you should be ready to go into the modeling phase of the CRISP-DM
    methodology, shouldn''t you? Well, maybe not. There are many reasons you may want
    to spend a little more time on data preparation, even after you have correctly
    classified your features:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，如果您对您的特征满意并且已经正确地对每个特征进行了分类，您应该准备好进入CRISP-DM方法的建模阶段，不是吗？好吧，也许不是。即使您已经正确地分类了特征，您可能还有很多原因想要在数据准备上花费更多的时间：
- en: Some algorithm implementations, such as `scikit-learn`, may not accept string
    values on your categorical features.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些算法实现，如`scikit-learn`，可能不接受您的分类特征上的字符串值。
- en: The data distribution of your variable may not be the most optimal distribution
    for your algorithm.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您变量的数据分布可能不是您算法的最优分布。
- en: Your ML algorithm may be impacted by the scale of your data.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的机器学习算法可能会受到您数据规模的影响。
- en: Some observations (rows) of your variable may be missing information and you
    will have to fix it. These are also known as missing values.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的变量的一些观测值（行）可能缺少信息，您将不得不修复它们。这些也被称为缺失值。
- en: You may find outlier values of your variable that can potentially add bias to
    your model.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能会发现变量的异常值，这些异常值可能会给您的模型带来潜在的偏差。
- en: Your variable may be storing different types of information and you may only
    be interested in a few of them (for example, a date variable can store the day
    of the week or the week of the month).
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的变量可能存储着不同类型的信息，而您可能只对其中的一些感兴趣（例如，日期变量可以存储星期几或月份的哪一周）。
- en: You might want to find a mathematical representation for a text variable.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可能希望为文本变量找到一个数学表示。
- en: And believe me, this list will never end.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并且相信我，这个列表永远不会结束。
- en: In the following sections, we will understand how to address all these points,
    starting with categorical features.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将了解如何解决所有这些问题，从分类特征开始。
- en: Dealing with categorical features
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理分类特征
- en: Data transformation methods for categorical features will vary according to
    the sub-type of your variable. In the upcoming sections, we will understand how
    to transform nominal and ordinal features.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类特征的数据转换方法将根据您变量的子类型而有所不同。在接下来的章节中，我们将了解如何转换名义和有序特征。
- en: Transforming nominal features
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换名义特征
- en: You may have to create numerical representations of your categorical features
    before applying ML algorithms to them. Some libraries may have embedded logic
    to handle that transformation for you, but most of them do not.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用机器学习算法之前，您可能需要创建您分类特征的数值表示。一些库可能已经内置了处理这种转换的逻辑，但大多数都没有。
- en: 'The first transformation we will cover is known as **label encoding**. A label
    encoder is suitable for categorical/nominal variables and it will just associate
    a number with each distinct label of your variable. The following table shows
    how a label encoder works:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的第一种转换方法被称为**标签编码**。标签编码器适用于分类/名义变量，它将为您的变量的每个不同的标签分配一个数字。以下表格显示了标签编码器是如何工作的：
- en: '![Figure 3.3 – Label encoder in action'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.3 – 标签编码器在作用中'
- en: '](img/B16735_03_003.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_003.jpg)'
- en: Figure 3.3 – Label encoder in action
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 标签编码器在作用中
- en: A label encoder will always ensure that a unique number is associated with each
    distinct label. In the preceding table, although "India" appears twice, the same
    number was assigned to it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码器将始终确保每个不同的标签都关联一个唯一的数字。在前面的表格中，尽管“印度”出现了两次，但它被分配了相同的数字。
- en: You now have a numerical representation of each country, but this does not mean
    you can use that numerical representation in your models! In this particular case,
    we are transforming a nominal feature, *which does not have an order*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在有了每个国家的数值表示，但这并不意味着您可以在模型中使用这种数值表示！在这个特定的情况下，我们正在转换一个没有顺序的命名特征。
- en: According to the preceding table, if we pass the encoded version of the *country*
    variable to a model, it will make assumptions such as "Brazil (3) is greater than
    Canada (2)", which does not make any sense.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的表格，如果我们将*国家*变量的编码版本传递给模型，它将做出诸如“巴西（3）大于加拿大（2）”这样的假设，这是没有意义的。
- en: 'A possible solution for that scenario is applying another type of transformation
    on top of "*country"*: **one-hot encoding**. This transformation will represent
    all the categories from the original feature as individual features (also known
    as **dummy variables**), which will store the "presence or absence" of each category.
    The following table is transforming the same information we looked at in the preceding
    table, but this time it''s applying one-hot encoding:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况的一个可能解决方案是在"*国家"*上应用另一种类型的转换：**独热编码**。这种转换将表示原始特征中的所有类别作为单独的特征（也称为**虚拟变量**），这将存储每个类别的“存在或不存在”。以下表格正在转换我们在前面表格中查看的相同信息，但这次它正在应用独热编码：
- en: '![Figure 3.4 – One-hot encoding in action'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4 – 独热编码的实际应用'
- en: '](img/B16735_03_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片/B16735_03_004.jpg]'
- en: Figure 3.4 – One-hot encoding in action
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 独热编码的实际应用
- en: We can now use the one-hot encoded version of the *country* variable as a feature
    of a ML model. However, your work as a skeptical data scientist is never done,
    and your critical thinking ability will be tested in the AWS Machine Learning
    Specialty exam.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用*国家*变量的独热编码版本作为机器学习模型的特征。然而，作为一名怀疑论的数据科学家，您的工作永远不会结束，您的批判性思维能力将在AWS机器学习专业考试中得到考验。
- en: 'Let''s suppose you have 150 distinct countries in your dataset. How many dummy
    variables would you come up with? 150, right? Here, we just found a potential
    issue: apart from adding complexity to your model (which is not a desired characteristic
    of any model at all), dummy variables also add **sparsity** to your data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的数据集中有150个不同的国家。您会想出多少个虚拟变量？150个，对吧？这里，我们刚刚发现一个潜在问题：除了增加模型的复杂性（这绝不是任何模型所期望的特性）之外，虚拟变量还会给您的数据增加**稀疏性**。
- en: A sparse dataset has a lot of variables filled with zeros. Often, it is hard
    to fit this type of data structure into memory (you can easily run out of memory)
    and it is very time-consuming for ML algorithms to process sparse structures.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数据集有很多变量填充了零。通常，很难将这种数据结构拟合到内存中（您很容易耗尽内存），并且对于机器学习算法来说，处理稀疏结构非常耗时。
- en: You can work around the sparsity problem by grouping your original data and
    reducing the number of categories, and you can even use custom libraries that
    compress your sparse data and make it easier for manipulation (such as `scipy.sparse.csr_matrix`,
    from Python).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将原始数据分组并减少类别数量来绕过稀疏性问题，甚至可以使用自定义库来压缩稀疏数据，使其更容易操作（例如，Python中的`scipy.sparse.csr_matrix`）。
- en: Therefore, during the exam, remember that one-hot encoding is definitely the
    right way to go when you need to transform categorical/nominal data to feed ML
    models; however, take the number of unique categories of your original feature
    into account and think about if it makes sense to create dummy variables for all
    of them (maybe it does not make sense, if you have a very large number of unique
    categories).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在考试期间，请记住，当您需要将分类/名义数据转换为供机器学习模型使用时，独热编码绝对是正确的做法；然而，考虑到您原始特征中唯一类别的数量，并思考是否为所有这些类别创建虚拟变量是有意义的（如果您有非常多的唯一类别，可能并不合理）。
- en: Applying binary encoding
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用二进制编码
- en: For those types of variables with a higher number of unique categories, a potential
    approach to creating a numerical representation for them is applying **binary
    encoding**. In this approach, the goal is transforming a categorical column into
    multiple binary columns, but minimizing the number of new columns.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有更多唯一类别的变量类型，创建它们的数值表示的一个潜在方法是应用**二进制编码**。在这种方法中，目标是把一个分类列转换成多个二进制列，但最小化新列的数量。
- en: 'This process consists of three basic steps:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程包括三个基本步骤：
- en: The categorical data is converted into numerical data after being passed through
    an ordinal encoder.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类数据在通过序数编码器后转换为数值数据。
- en: The resulting number is then converted into a binary value.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将得到的数字转换为二进制值。
- en: The binary value is split into different columns.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 二进制值被分割成不同的列。
- en: 'Let''s reuse our data from *Figure 3.3* to see how we could use binary encoding
    in this particular case:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重用我们的*图3.3*中的数据来查看我们如何在这个特定情况下使用二进制编码：
- en: '![Figure 3.5 – Binary encoding in action'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.5 – 二进制编码的实际应用'
- en: '](img/B16735_03_005.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_005.jpg)'
- en: Figure 3.5 – Binary encoding in action
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 二进制编码的实际应用
- en: As we can see, we now have three columns (Col1, Col2, and Col3) instead of four.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们现在有三个列（Col1、Col2和Col3），而不是四个。
- en: Transforming ordinal features
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换序数特征
- en: 'Ordinal features have a very specific characteristic: *they have an order*.
    Because they have this quality, it does *not* make sense to apply one-hot encoding
    to them; if you do so, you will lose the magnitude of order of your feature.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 序数特征有一个非常具体的特征：*它们有顺序*。因为它们具有这种特性，所以对它们应用独热编码是没有意义的；如果你这样做，你将失去特征的顺序大小。
- en: 'The most common transformation for this type of variable is known as **ordinal
    encoding**. An ordinal encoder will associate a number with each distinct label
    of your variable, just like a label encoder does, but this time, it will respect
    the order of each category. The following table shows how an ordinal encoder works:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型变量的最常见转换方法被称为**序数编码**。序数编码器将每个变量的不同标签与一个数字相关联，就像标签编码器一样，但这次，它将尊重每个类别的顺序。以下表格显示了序数编码器是如何工作的：
- en: '![Figure 3.6 – Ordinal encoding in action'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.6 – 序数编码的实际应用'
- en: '](img/B16735_03_006.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_006.jpg)'
- en: Figure 3.6 – Ordinal encoding in action
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 – 序数编码的实际应用
- en: We can now pass the encoded variable to ML models and they will be able to handle
    this variable properly, with no need to apply one-hot encoding transformations.
    This time, comparisons such as "Sr Data Analyst is greater than Jr. Data Analyst"
    make total sense.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将编码后的变量传递给机器学习模型，它们将能够正确地处理这个变量，无需应用独热编码转换。这次，比较如“高级数据分析师大于初级数据分析师”是完全有意义的。
- en: Avoiding confusion in our train and test datasets
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免在训练和测试数据集中产生混淆
- en: 'Do not forget the following statement: encoders are **fitted** on training
    data and **transformed** on test and production data. This is how your ML pipeline
    should work.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记以下声明：编码器是在训练数据上**拟合**的，在测试和生产数据上**转换**的。这就是你的机器学习管道应该如何工作。
- en: Let's suppose you have created a one-hot encoder that fits the data from *Figure
    3.3* and returns data according to *Figure 3.4*. In this example, we will assume
    this is our training data. Once you have completed your training process, you
    may want to apply the same one-hot encoding transformation to your testing data
    to check the model's results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经创建了一个适合*图3.3*数据的独热编码器，并返回根据*图3.4*的数据。在这个例子中，我们将假设这是我们的训练数据。一旦你完成了训练过程，你可能想要将相同的独热编码转换应用到你的测试数据上，以检查模型的结果。
- en: In the scenario that we just described (which is a very common situation in
    modeling pipelines), you *cannot* retrain your encoder on top of the testing data!
    You should just reuse the previous encoder object that you have created on top
    of the training data. Technically, we say that you shouldn't use the `fit` method
    again, and use the `transform` method instead.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚才描述的场景中（这在建模管道中是一个非常常见的情况），你**不能**在测试数据上重新训练你的编码器！你应该只是重用你在训练数据上创建的之前的编码器对象。技术上，我们说你不应该再次使用`fit`方法，而应该使用`transform`方法。
- en: 'You may already know the reasons why you should follow this rule, but let''s
    recap: the testing data was created to extract the performance metrics of your
    model, so you should not use it to extract any other knowledge. If you do so,
    your performance metrics will be biased by the testing data and you cannot infer
    that the same performance (shown in the test data) is likely to happen in production
    (when new data will come in).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经知道为什么你应该遵循这个规则的原因，但让我们回顾一下：测试数据是为了提取你模型的性能指标而创建的，所以你不应该用它来提取任何其他知识。如果你这样做，你的性能指标将会受到测试数据的影响，你无法推断出相同的性能（在测试数据中显示）在生产环境中（当新数据到来时）很可能发生。
- en: Alright, all good so far. However, what if our testing set has a new category
    that was not present in the train set? How are we supposed to transform this data?
    Let's walk through this particular scenario.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，到目前为止一切顺利。但是，如果我们测试集中有一个训练集中没有的新类别，我们该如何转换这些数据呢？让我们来分析这个特定场景。
- en: 'Going back to our one-hot encoding example we looked at in *Figures 3.3* (input
    data) and *Figure* *3.4* (output data), our encoder knows how to transform the
    following countries: Australia, Brazil, Canada, and India. If we had a different
    country in the testing set, the encoder would not know how to transform it, and
    that''s why we need to define how it will behave in scenarios where there are
    exceptions.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们之前在 *图 3.3*（输入数据）和 *图 3.4*（输出数据）中看到的 one-hot 编码示例，我们的编码器知道如何转换以下国家：澳大利亚、巴西、加拿大和印度。如果我们测试集中有其他国家，编码器将不知道如何转换它，这就是为什么我们需要定义它在有异常情况下的行为。
- en: 'Most of the ML libraries provide specific parameters for these situations.
    In our example, we could program the encoder to either raise an error or set all
    zeros on our dummy variables, as shown in the following table:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习库都为这些情况提供了特定的参数。在我们的例子中，我们可以编程编码器在虚拟变量上引发错误或设置所有为零，如下表所示：
- en: '![Figure 3.7 – Handling unknown values on one-hot encoding transformations'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7 – 在 one-hot 编码转换中处理未知值'
- en: '](img/B16735_03_007.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_007.jpg)'
- en: Figure 3.7 – Handling unknown values on one-hot encoding transformations
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 在 one-hot 编码转换中处理未知值
- en: As we can see, Portugal was not present in the training set (*Figure 3.3*),
    so during the transformation, we are keeping the same list of known countries
    and saying that Portugal *IS NOT* any of them (all zeros).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，葡萄牙没有出现在训练集中（*图 3.3*），因此在转换过程中，我们保持相同的已知国家列表，并说葡萄牙*不是*其中任何一个（所有为零）。
- en: As the very good, skeptical data scientist we know you are becoming, should
    you be concerned about the fact that you have a particular category that has not
    been used during training? Well, maybe. This type of analysis really depends on
    your problem domain.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们知道的非常优秀、持怀疑态度的数据科学家，你应该担心这样一个事实，即你有一个在训练过程中没有使用过的特定类别吗？好吧，也许吧。这种分析类型真的取决于你的问题领域。
- en: Handling unknown values is very common and something that you should expect
    to do in your ML pipeline. However, you should also ask yourself, due to the fact
    that you did not use that particular category during your training process, if
    your model can be extrapolated and generalized.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 处理未知值是非常常见的事情，也是你在机器学习流程中应该预期要做的事情。然而，你也应该问自己，由于你在训练过程中没有使用那个特定的类别，你的模型是否可以被外推和泛化。
- en: Remember, your testing data must follow the same data distribution as your training
    data, and you are very likely to find all (or at least most) of the categories
    (of a categorical feature) either in the training or test sets. Furthermore, if
    you are facing overfitting issues (doing well in the training, but poorly in the
    test set) and, at the same time, you realize that your categorical encoders are
    transforming a lot of unknown values in the test set, guess what? It's likely
    that your training and testing samples are not following the same distribution,
    invalidating your model entirely.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你的测试数据必须遵循与你的训练数据相同的数据分布，你很可能在训练或测试集中找到所有（或至少大多数）类别（分类特征的类别）。此外，如果你面临过拟合问题（在训练中表现良好，但在测试集中表现不佳），同时你意识到你的分类编码器在测试集中转换了大量的未知值，猜猜看？很可能你的训练和测试样本没有遵循相同的分布，这完全无效化了你的模型。
- en: As you can see, slowly, we are getting there. We are talking about bias and
    investigation strategies in fine-grained detail! Now, let's move on and look at
    performing transformations on numerical features. Yes, each type of data matters
    and drives your decisions!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们正逐渐接近目标。我们正在详细讨论偏差和调查策略！现在，让我们继续前进，看看对数值特征进行转换。是的，每种类型的数据都很重要，并驱动着你的决策！
- en: Dealing with numerical features
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数值特征
- en: In terms of numerical features (discrete and continuous), we can think of transformations
    that rely on the training data and others that rely purely on the observation
    being transformed.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在数值特征（离散和连续）方面，我们可以考虑依赖于训练数据的转换，以及其他仅依赖于被转换的观察的转换。
- en: Those that rely on the training data will use the train set to learn the necessary
    parameters during `fit`, and then use them to transform any test or new data.
    The logic is pretty much the same as what we just reviewed for categorical features;
    however, this time, the encoder will learn different parameters.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 那些依赖于训练数据的将使用训练集在`fit`期间学习必要的参数，然后使用它们来转换任何测试或新数据。逻辑基本上与我们刚才讨论的分类特征相同；然而，这次，编码器将学习不同的参数。
- en: On the other hand, those that rely purely on observations do not care about
    train or test sets. They will simply perform a mathematical computation on top
    of an individual value. For example, we could apply an exponential transformation
    to a particular variable by squaring its value. There is no dependency on learned
    parameters from anywhere – just get the value and square it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，那些纯粹依赖于观察的算法不关心训练集或测试集。它们将简单地在一个单独的值上执行数学计算。例如，我们可以通过平方其值来对一个特定变量应用指数转换。这里没有依赖任何地方学习到的参数——只需获取值并平方它即可。
- en: 'At this point, you might be thinking about dozens of available transformations
    for numerical features! Indeed, there are so many options that we can''t describe
    all of them here, and you are not supposed to know all of them for the AWS Machine
    Learning Specialty exam anyway. We will cover the most important ones (for the
    exam) here, but I don''t want to limit your modeling skills: take a moment to
    think about the unlimited options you have by creating custom transformations
    according to your use case.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经在想针对数值特征的成百上千种可用的转换了！确实，选项如此之多，我们无法在这里全部描述，而且对于AWS机器学习专业考试，你也不需要知道所有这些。在这里，我们将介绍最重要的（对于考试而言）一些，但我不想限制你的建模技能：花点时间想想，根据你的用例创建自定义转换，你有多少无限的选择。
- en: Data normalization
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标准化
- en: Applying data **normalization** means changing the scale of the data. For example,
    your feature may store employee salaries that range between 20,000 and 200,000
    dollars/year and you want to put this data in the range of 0 and 1, where 20,000
    (the minimum observed value) will be transformed into 0 and 200,000 (the maximum
    observed value) will be transformed into 1.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 应用数据**标准化**意味着改变数据的规模。例如，你的特征可能存储员工年薪，范围在每年20,000到200,000美元之间，而你希望将这些数据放入0到1的范围内，其中20,000（观察到的最小值）将被转换为0，200,000（观察到的最大值）将被转换为1。
- en: This type of technique is specifically important when you want to fit your training
    data on top of certain types of algorithms that are impacted by the scale/magnitude
    of the underlying data. For instance, we can think about those algorithms that
    use the dot product of the input variables (such as neural networks or linear
    regression) and those algorithms that rely on distance measures (such as **k-nearest
    neighbor** (**KNN**) or **k-means**).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在你想将训练数据适配到受底层数据规模/幅度影响的特定类型算法上时尤为重要。例如，我们可以考虑那些使用输入变量点积（如神经网络或线性回归）的算法，以及那些依赖于距离度量的算法（如**k-最近邻**（**KNN**）或**k-均值**）。
- en: On the other hand, applying data normalization will not result in performance
    improvements for rule-based algorithms, such as decision trees, since they will
    be able to check the predictive power of the features (either via **entropy**
    or **information gain** analysis), regardless of the scale of the data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，应用数据标准化不会提高基于规则的算法（如决策树）的性能，因为它们将能够检查特征的可预测性（无论是通过**熵**还是**信息增益**分析），而不管数据的规模如何。
- en: Important note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We will learn about these algorithms, along with the appropriate details, in
    the later chapters of this book. For instance, you can look at entropy and information
    gain as two types of metrics used by decision trees to check feature importance.
    Knowing the predictive power of each feature helps the algorithm define the optimal
    root, intermediaries, and leaf nodes of the tree.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a moment to understand why data normalization will help those types
    of algorithms. We already know that the goal of a clustering algorithm is to find
    groups or clusters in your data, and one of the most used clustering algorithms
    is known as k-means. We will use k-means to see this problem in action, since
    it is impacted by data scaling.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows how different scales of the variable could change
    the hyper plan''s projection:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Plotting data of different scales in a hyper plan'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_008.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – Plotting data of different scales in a hyper plan
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side of the preceding image, we can see a single data point
    plotted in a hyper plan that has three dimensions (x, y, and z). All three dimensions
    (also known as features) were normalized to the scale of 0 and 1\. On the right-hand
    side, we can see the same data point, but this time, the "x" dimension was *not*
    normalized. We can clearly see that the hyper plan has changed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: In a real scenario, we would have far more dimensions and data points. The difference
    in the scale of the data would change the centroids of each clusters and could
    potentially change the assigned clusters of some points. This same problem will
    happen on other algorithms that rely on distances calculation, such as KNN.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Other algorithms, such as neural networks and linear regression, will compute
    weighted sums using your input data. Usually, these types of algorithms will perform
    operations such as *W1*X1 + W2*X2 + Wi*Xi*, where *Xi* and *Wi* refer to a particular
    feature value and its weight, respectively. Again, we will cover details of neural
    networks and linear models later, but can you see the data scaling problem by
    just looking at the calculations that we just described? We can easily come up
    with very large values if X (feature) and W (weight) are large numbers. That will
    make the algorithm's optimizations much more complex.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: I hope you now have a very good understanding about the reasons you should apply
    data normalization (and when you should not). Data normalization is often implemented
    in ML libraries as **Min Max Scaler**. If you find this term in the exam, then
    remember it is the same as data normalization.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, data normalization does not necessarily need to transform your
    feature into a range between 0 and 1\. In reality, we can transform the feature
    into any range we want. The following is how a normalization is formally defined:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Normalization formula'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_009.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – Normalization formula
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Here, *X*min and *X*max are the lower and upper values of the range; *X* is
    the value of the feature. Apart from data normalization, there is another very
    important technique regarding numerical transformations that you *must* be aware
    of, not only for the exam, but also for your data science career. We'll look at
    this in the next section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X*min和*X*max是范围的上下限值；*X*是特征的值。除了数据归一化外，还有另一个非常重要的关于数值转换的技术，您必须了解，这不仅是为了考试，也是为了您的数据科学职业生涯。我们将在下一节中探讨这一点。
- en: Data standardization
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据标准化
- en: 'Data **standardization** is another scaling method that transforms the distribution
    of the data, so that the mean will become zero and the standard deviation will
    become one. The following image formally describes this scaling technique, where
    *X* represents the value to be transformed, *µ* refers to the mean of *X*, and
    *σ* is the standard deviation of *X*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据**标准化**是另一种缩放方法，它转换数据的分布，使得均值变为零，标准差变为一。以下图像正式描述了这种缩放技术，其中*X*代表要转换的值，*µ*指的是*X*的均值，*σ*是*X*的标准差：
- en: '![Figure 3.10 – Standardization formula'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.10 – 标准化公式'
- en: '](img/B16735_03_010.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_010.jpg)'
- en: Figure 3.10 – Standardization formula
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – 标准化公式
- en: Unlike normalization, data standardization will *not* result in a predefined
    range of values. Instead, it will transform your data into a standard **Gaussian
    distribution**, where your transformed values will represent the number of standard
    deviations of each value to the mean of the distribution.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与归一化不同，数据标准化不会产生预定义的值范围。相反，它将您的数据转换成标准的**高斯分布**，其中您的转换值将代表每个值与分布均值的标准差数。
- en: Important note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Gaussian distribution, also known as normal distribution, is one of the most
    used distribution on statistical models. This is a continuous distribution with
    two main controlled parameters: µ (mean) and σ (standard deviation). Normal distributions
    are symmetric around the mean. In other words, most of the values will be close
    to the mean of the distribution.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布，也称为正态分布，是统计模型中最常用的分布之一。这是一个具有两个主要控制参数的连续分布：µ（均值）和σ（标准差）。正态分布围绕均值对称。换句话说，大多数值将接近分布的均值。
- en: 'Data standardization if often referred to as the **zscore** and is widely used
    to identify outliers on your variable, which we will see later in this chapter.
    For the sake of demonstration, the following table simulates the data standardization
    of a small dataset. The input value is present in the "Age" column, while the
    scaled value is present in the "Zscore" column:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化通常被称为**z分数**，广泛用于识别变量的异常值，我们将在本章后面看到。为了演示，以下表格模拟了一个小数据集的数据标准化。输入值位于“年龄”列中，而缩放值位于“Z分数”列中：
- en: '![Figure 3.11 – Data standardization in action'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.11 – 数据标准化过程'
- en: '](img/B16735_03_011.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16735_03_011.jpg)'
- en: Figure 3.11 – Data standardization in action
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – 数据标准化过程
- en: Make sure you are confident when applying normalization and standardization
    by hand in the AWS Machine Learning Specialty exam. They might provide a list
    of values, as well as mean and standard deviation, and ask you the scaled value
    of each element of the list.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS机器学习专业考试中，确保你在手动应用归一化和标准化时充满信心。他们可能会提供一个值列表，以及均值和标准差，并询问列表中每个元素的缩放值。
- en: Applying binning and discretization
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用分箱和离散化
- en: '**Binning** is a technique where you can group a set of values into a bucket
    or bin; for example, grouping people between 0 and 14 years old into a bucket
    named "children," another group of people between 15 and 18 years old into a bucket
    named "teenager," and so on.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**分箱**是一种技术，可以将一组值分组到一个桶或箱中；例如，将0到14岁之间的人分组到名为“儿童”的桶中，将15到18岁之间的人分组到名为“青少年”的桶中，依此类推。'
- en: '**Discretization** is the process of transforming a continuous variable into
    discrete or nominal attributes. These continuous values can be discretized by
    multiple strategies, such as **equal-width** and **equal-frequency**.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散化**是将连续变量转换为离散或名义属性的过程。这些连续值可以通过多种策略进行离散化，例如**等宽**和**等频**。'
- en: An equal-width strategy will split your data across multiple bins of the same
    width. Equal-frequency will split your data across multiple bins with the same
    number of frequencies.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 等宽策略将数据分布在多个相同宽度的桶中。等频策略将数据分布在具有相同频率的多个桶中。
- en: 'Let''s look at an example. Suppose we have the following list containing 16
    numbers: 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 90\. As we
    can see, this list ranges between 10 and 90\. Assuming we want to create four
    bins using an equal-width strategy, we would come up with the following bins:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Bin >= 10 <= 30 > 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 30 <= 50 >
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 50 <= 70 >
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 71 <= 90 > 90
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, the width of each bin is the same (20 units), but the observations
    are not equally distributed. Now, let''s simulate an equal-frequency strategy:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Bin >= 10 <= 13 > 10, 11, 12, 13
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 13 <= 17 > 14, 15, 16, 17
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 17 <= 21 > 18, 19, 20, 21
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bin > 21 <= 90 > 22, 23, 24, 90
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, all the bins have the same frequency of observations, although
    they have been built with different bin widths to make that possible.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have computed your bins, you should be wondering what''s next, right?
    Here, you have some options:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: You can name your bins and use them as a nominal feature on your model! Of course,
    as a nominal variable, you should think about applying one-hot encoding before
    feeding a ML model with this data.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might want to order your bins and use them as an ordinal feature.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maybe you want to remove some noise from your feature by averaging the minimum
    and maximum values of each bin and using that value as your transformed feature.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take a look at the following table to understand these approaches using our
    equal-frequency example:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Different approaches to working with bins and discretization'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_012.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Different approaches to working with bins and discretization
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Again, playing with different binning strategies will give you different results
    and you should analyze/test the best approach for your dataset. There is no standard
    answer here – it is all about data exploration!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Applying other types of numerical transformations
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Normalization and standardization rely on your training data to fit their parameters:
    minimum and maximum values in the case of normalization, and mean and standard
    deviation in the case of standard scaling. This also means you must fit those
    parameters using *only* your training data and never the testing data.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are other types of numerical transformations that do not require
    parameters from training data to be applied. These types of transformations rely
    purely on mathematical computations. For example, one of these transformations
    is known as **logarithmic transformation**. This is a very common type of transformation
    in machine learning models and is especially beneficial for **skewed** features.
    In case you don''t know what a skewed distribution is, take a look at the following
    diagram:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Skewed distributions'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_013.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.13 – Skewed distributions
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: In the middle, we have a normal distribution (or Gaussian distribution). On
    the left- and right-hand sides, we have skewed distributions. In terms of skewed
    features, there will be some values far away from the mean in one single direction
    (either left or right). Such behavior will push both the median and mean values
    of this distribution in the same direction of the long tail we can see in the
    preceding diagram.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: One very clear example of data that used to be skewed is the annual salaries
    of a particular group of professionals in a given region, such as senior data
    scientists working in Florida, US. This type of variable usually has most of its
    values close to the others (because people used to earn an average salary) and
    just has a few very high values (because a small group of people makes much more
    money than others).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, you can now easily understand why the mean and median values will
    move to the tail direction, right? The big salaries will push them in that direction.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Alright, but why will a logarithmic transformation be beneficial for this type
    of feature? The answer to this question can be explained by the math behind it:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Logarithmic properties'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_014.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 – Logarithmic properties
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the log of a number is the inverse of the exponential function. Log
    transformation will then reduce the scale of your number according to a given
    base (such as base 2, base 10, or base e, in the case of a natural logarithm).
    Looking at our salary''s distribution from the previous example, we would bring
    all those numbers down so that the higher the number, the higher the reduction;
    however, we would do this in a log scale and not in a linear fashion. Such behavior
    will remove the outliers of this distribution (making it closer to a normal distribution),
    which is beneficial for many ML algorithms, such as linear regression. The following
    table shows you some of the differences when transforming a number in a linear
    scale versus a log scale:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Differences between linear transformation and log transformation'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_015.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.15 – Differences between linear transformation and log transformation
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: I hope you can see that the linear transformation kept the original magnitude
    of the data (we can still see outliers, but in another scale), while the log transformation
    removed those differences of magnitude and still kept the order of the values.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Would you be able to think about another type of mathematical transformation
    that follows the same behavior of *log* (making the distribution closer to Gaussian)?
    OK, I can give you another: square root. Take the square root of those numbers
    shown in the preceding table and see yourself!'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, pay attention to this: both log and square root belong to a set of transformations
    known as **power transformations**, and there is very popular method, which is
    likely to be mentioned on your AWS exam, that can perform a range of power transformations
    like those we have seen. This method was proposed by George Box and David Cox
    and its name is **Box-Cox**.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: During your exam, if you see questions around the Box-Cox transformation, remember
    that it is a method that can perform many power transformations (according to
    a lambda parameter), and its end goal is making the original distribution closer
    to a normal distribution (do not forget that).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Just to conclude our discussion regarding why mathematical transformations can
    really make a difference to ML models, I will give you an intuitive example about
    **exponential transformations**.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you have a set of data points, such as those on the left-hand side
    of *Figure 3.16*. Your goal is to draw a line that will perfectly split blue and
    red points. Just by looking at the original data (again, on the left-hand side),
    we know that our best guess for performing this linear task would be the one you
    can see in the same image. However, the science (not magic) happens on the right-hand
    side of the image! By squaring those numbers and plotting them in another hyper
    plan, we can perfectly separate each group of points:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Exponential transformation in action'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_016.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.16 – Exponential transformation in action
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: I know you might be thinking about the infinite ways you can deal with your
    data. Although this is true, you should always take the business scenario you
    are working on into account and plan your work accordingly. Remember that model
    improvements or exploration is always possible, but you have to define your goals
    (remember the CRISP-DM methodology) and move on.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: By the way, data transformation is important, but just one piece of your work
    as a data scientist. Your modeling journey still has to move to other important
    topics, such as missing values and outliers handling, which we will look at next.
    However, before that, you may have noticed that you were introduced to Gaussian
    distributions during this section, so let's take a moment to discuss them in a
    bit more detail.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data distributions
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the Gaussian distribution is probably the most common distribution
    for statistical and machine learning models, you should be aware that it is not
    the only one. There are other types of data distributions, such as the **Bernoulli**,
    **Binomial**, and **Poisson** distributions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bernoulli distribution is a very simple one, as there are only two types
    of possible events: success or failure. The success event has a probability "p"
    of happening, while the failure one has a probability of "1-p".'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples that follow a Bernoulli distribution are rolling a six-sided
    die or flipping a coin. In both cases, you must define the event of success and
    the event of failure. For example, suppose our events for success and failure
    in the die example are as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Success: Getting a number 6'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Failure: Getting any other number'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can then say that we have a p probability of success (1/6 = 0.16 = 16%) and
    a 1-p probability of failure (1 - 0.16 = 0.84 = 84%).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: The Binomial distribution generalizes the Bernoulli distribution. The Bernoulli
    distribution has only one repetition of an event, while the Binomial distribution
    allows the event to be repeated many times, and we must count the number of successes.
    Let's continue with our prior example; that is, counting the number of times we
    got a 6 out of our 10 dice rolls. Due to the nature of this example, Binomial
    distribution has two parameters, n and p, where n is the number of repetitions
    and p is the probability of success in every repetition.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a Poisson distribution allows us to find a number of events in a time
    period, given the number of times an event occurs in an interval. It has three
    parameters: lambda, e, and k, where lambda is the average number of events per
    interval, e is the Euler number, and k is the number of times an event occurs
    in an interval.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: With all those distributions, including the Gaussian one, it is possible to
    compute the expected mean value and variance based on their parameters. This information
    is usually used in hypothesis tests to check whether some sample data follows
    a given distribution, by comparing the mean and variance **of the sample** against
    the **expected** mean and variance of the distribution.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: I hope you are now more familiar with data distributions generally, not only
    Gaussian distributions. We will keep talking about data distributions throughout
    this book. For now, let's move on to missing values and outlier detection.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the name suggests, missing values refer to the absence of data. Such absences
    are usually represented by tokens, which may or may not be implemented in a standard
    way.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Although using tokens is standard, the way those tokens are displayed may vary
    across different platforms. For example, relational databases represent missing
    data with *NULL*, core Python code will use *None*, and some Python libraries
    will represent missing numbers as (**Not a Number** (**NaN**).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: For numerical fields, don't replace those standard missing tokens with *zeros*.
    By default, zero is not a missing value, but another number. I said "by default"
    because, in data science, we may face some data quality issues, which we will
    cover next.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: However, in real business scenarios, you may or may not find those standard
    tokens. For example, a software engineering team might have designed the system
    to automatically fill missing data with specific tokens, such as "unknown" for
    strings or "-1" for numbers. In that case, you would have to search by those two
    tokens to find missing data. People can set anything.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, the software engineering team was still kind enough
    to give us standard tokens. However, there are many cases where legacy systems
    do not add any data quality layer in front of the user, and you may find an address
    field filled with "I don't want to share" or a phone number field filled with
    "Don't call me". This is clearly missing data, but not as standard as the previous
    example.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many more nuances that you will learn regarding missing data, all
    of which we will cover in this section, but be advised: before you start making
    decisions about missing values, you should prepare a good data exploration and
    make sure you find those values. You can either compute data frequencies or use
    missing plots, but please do something. Never assume that your missing data is
    represented only by those handy standard tokens.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Why should we care about this type of data? Well, first, because most algorithms
    (apart from decision trees implemented on very specific ML libraries) will raise
    errors when they find a missing value. Second (and maybe most importantly), by
    grouping all the missing data in the same bucket, you are assuming that they are
    all the same, but in reality, you don't know that.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Such a decision will not only add bias to your model – it will reduce its interpretability,
    as you will be unable to explain the missing data. Once we know why we want to
    treat the missing values, we can take a look at our options.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Theoretically, we can classify missing values into two main groups: **MCAR**
    or **MNAR**. MCAR stands for **Missing Completely at Random** and states that
    there is no pattern associated with the missing data. On the other hand, MNAR
    stands for **Missing Not at Random** and means that the underlying process used
    to generate the data is strictly connected to the missing values.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Let me give you an example of MNAR missing values. Suppose you are collecting
    user feedback about a particular product in an online survey. Your process of
    asking questions is dynamic and depends on user answers. When a user specifies
    an age lower than 18 years old, you never ask his/her marital status. In this
    case, missing values of marital status are connected to the age of the user (MNAR).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the class of missing values that you are dealing with will help you
    understand if you have any control over the underlying process that generates
    the data. Sometimes, you can come back to the source process and, somehow, complete
    your missing data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Although, in real scenarios, we usually have to treat missing data via exclusion
    or imputation, never forget that you can always try to look at the source process
    and check if you can retrieve (or, at least, better understand) the missing data.
    You may face this option in the exam.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have an opportunity to recover your missing data from somewhere,
    then you should move on to other approaches, such as **listwise deletion** and
    **imputation**.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Listwise deletion refers to the process of discarding some data, which is the
    downside of this choice. This may happen at the row level or at the column level.
    For example, suppose you have a DataFrame containing four columns and one of them
    has 90% of its data missing. In such cases, what usually makes more sense is dropping
    the entire feature (column), since you don't have that information for the majority
    of your observations (rows).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: From a row perspective, you may have a DataFrame with a small number of observations
    (rows) containing missing data in one of its features (columns). In such scenarios,
    instead of removing the entire feature, what makes more sense is removing only
    those few observations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of using this method is the simplicity of dropping a row or a column.
    Again, the downside is losing information. If you don't want to lose information
    while handling your missing data, then you should go for an imputation strategy.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Imputation is also known as replacement, where you will replace missing values
    by substituting a value. The most common approach of imputation is replacing the
    missing value with the mean of the feature. Please take a note of this approach
    because it is likely to appear in your exam:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Replacing missing values with the mean or median'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_017.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 – Replacing missing values with the mean or median
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding table shows a very simple dataset with one single feature and
    five observations, where the third observation has a missing value. If we decide
    to replace that missing data with the mean value of the feature, we will come
    up with 49\. Sometimes, when we have outliers in the data, the median might be
    more appropriate (in this case, the median would be 35):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Replacing missing values with the mean or median of the group'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_018.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.18 – Replacing missing values with the mean or median of the group
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: If you want to go deeper, you could find the mean or median value according
    to a given group of features. For example, in the preceding table, we expanded
    our previous dataset by adding the Job status column. Now, we have clues that
    help us suspect that our initial approach of changing the missing value by using
    the overall median (35 years old) is likely to be wrong (since that person is
    retired).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: What you can do now is replace the missing value with the mean or median of
    the other observations that belong to the same job status. Using this new approach,
    we can change the missing information to 77.5\. Taking into account that the person
    is retired, 77.5 makes more sense than 35 years old.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: In the case of categorical variables, you can replace the missing data with
    the value that has the highest occurrence in your dataset. The same logic of grouping
    the dataset according to specific features is still applicable.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: You can also use more sophisticated methods of imputation, including constructing
    a ML model to predict the value of your missing data. The downside of these imputation
    approaches (either by averaging or predicting the value) is that you are making
    inferences on the data, which are not necessarily right and will add bias to the
    dataset.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: To sum this up, the trade-off while dealing with missing data is having a balance
    between losing data or adding bias to the dataset. Unfortunately, there is no
    scientific manual that you can follow, whatever your problem. To decide on what
    you are going to do, you must look to your success criteria, explore your data,
    run experiments, and then make your decisions.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: We will now move to another headache for many ML algorithms, also known as outliers.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with outliers
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are not on this studying journey just to pass the AWS Machine Learning Specialty
    exam, but also to become better data scientists. There are many different ways
    to look at the outlier problem purely from a mathematical perspective; however,
    the datasets we use are derived from the underlying business process, so we must
    include a business perspective during an outlier analysis.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'An **outlier** is an atypical data point in a set of data. For example, the
    following chart shows some data points that have been plotted in a two-dimension
    plan; that is, x and y. The red point is an outlier, since it is an atypical value
    on this series of data:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Identifying an outlier'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_019.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.19 – Identifying an outlier
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: We want to treat outlier values because some statistical methods are impacted
    by them. Still, in the preceding chart, we can see this behavior in action. On
    the left-hand side, we drew a line that best fits those data points, ignoring
    the red point. On the right-hand side, we also drew the best line to fit the data
    but included the red point.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: We can visually conclude that, by ignoring the outlier point, we will come up
    with a better solution on the plan of the left-hand side of the preceding chart
    since it was able to pass closer to most of the values. We can also prove this
    by computing an associated error for each line (which we will discuss later in
    this book).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth reminding you that you have also seen the outlier issue in action
    in another situation in this book: specifically, in *Figure 3.17*, when we had
    to deal with missing values. In that example, we used the median (instead of the
    mean) to work around the problem. Feel free to go back and read it again, but
    what should be very clear at this point is that median values are less impacted
    by outliers than average values.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: You now know what outliers are and why you should treat them. You should always
    consider your business perspective while dealing with outliers, but there are
    mathematical methods to find them. Now, let's look at these methods of outlier
    detection.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'You have already learned about the most common method: zscore. In *Figure 3.11*,
    we saw a table containing a set of ages. Refer to it again to refresh your memory.
    In the last column of that table, we are computing the zscore of each age, according
    to the equation shown in *Figure 3.10*.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no well-defined range for those zscore values; however, in a normal
    distribution *without* outliers, they will range between -3 and 3\. Remember:
    zscore will give you the number of standard deviations from the mean of the distribution.
    The following diagram shows some of the properties of a normal distribution:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Normal distribution properties. Image adapted from https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_020.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.20 – Normal distribution properties. Image adapted from https://pt.wikipedia.org/wiki/Ficheiro:The_Normal_Distribution.svg
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: According to the normal distribution properties, 95% of values will belong to
    the range of -2 and 2 standard deviations from the mean, while 99% of the values
    will belong to the range of -3 and 3\. Coming back to the outlier detection context,
    we can set thresholds on top of those zscore values to specify whether a data
    point is an outlier or not!
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no standard threshold that you can use to classify outliers. Ideally,
    you should look at your data and see what makes more sense for you… usually (this
    is not a rule), you will use some number between 2 and 3 standard deviations from
    the mean to set outliers, since more than 95% of your data will be out of that
    range. You may remember that there are outliers *below* and *above* the mean value
    of the distribution, as shown in the following table, where we have flagged outliers
    with an **absolute** zscore greater than 3 (the value column is hidden for the
    sake of this demonstration):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Flagging outliers according to the zscore value'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_021.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.21 – Flagging outliers according to the zscore value
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'We found two outliers in the preceding table: row number three and row number
    five. Another way to find outliers in the data is by applying the **box plot**
    logic. You will learn about box plots in more detail in the next chapter of this
    book. For now, let''s focus on using this method to find outliers.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: When we look at a numerical variable, it is possible to extract many descriptive
    statistics from it, not only the mean, median, minimum, and maximum values, as
    we have seen previously. Another property that's present in data distributions
    is known as **quantiles**.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantiles are cut-off points that are established at regular intervals from
    the cumulative distribution function of a random variable. Those regular intervals,
    also known as *q-quantiles*, will be nearly the same size and will receive special
    names in some situations; for example:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The 4-quantiles are called quartiles.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 10-quantiles are called deciles.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 100-quantiles are called percentiles.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the 20th percentile (of a 100-quantile regular interval) specifies
    that 20% of the data is below that point. In a box plot, we use regular intervals
    of 4-quantiles (also known as *quartiles*) to expose the distribution of the data
    (Q1 and Q3), as shown in the following diagram:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Box plot definition'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_022.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.22 – Box plot definition
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Q1 is also known as the lower quartile or 25th quartile, and this means that
    25% of the data is below that point in the distribution. Q3 is also known as the
    upper quartile or 75th quartile, and this means that 75% of the data is below
    that point in the distribution.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Computing the difference between Q1 and Q3 will give you the **interquartile
    range (IQR)** value, which you can then use to compute the limits of the box plot,
    shown by the "minimum" and "maximum" labels in the preceding diagram.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: After all, we can finally infer that anything below the "minimum" value or above
    the "maximum" value of the box plot will be flagged as an outlier.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'You have now learned about two different ways you can flag outliers on your
    data: zscore and box plot. You can decide whether you are going to remove these
    points from your dataset or create another variable to specify that they are,
    indeed, outliers (as we did in *Figure 3.21*).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Let's continue our journey of data preparation and look at other types of problems
    we will find in real life. Next, you will learn that several use cases have something
    known as **rare events**, which makes ML algorithms focus on the wrong side of
    the problem and propose bad solutions. Luckily, we will learn how to either tune
    them or prepare the data to make them smarter.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with unbalanced datasets
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, I hope you have realized why data preparation is probably the
    longest part of our work. We have learned about data transformation, missing data
    values, and outliers, but the list of problems goes on. Don't worry – bear with
    me and let's master this topic together!
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Another well-known problem with ML models, specifically with binary classification
    problems, is unbalanced classes. In a binary classification model, we say that
    a dataset is unbalanced when most of its observations belong to the same class
    (target variable).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: This is very common in fraud identification systems, for example, where most
    of the events belong to a regular operation, while a very small number of events
    belong to a fraudulent operation. In this case, we can also say that fraud is
    a rare event.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: There is no strong rule for defining whether a dataset is unbalanced or not,
    in the sense of it being necessary to worry about it. Most challenge problems
    will present more than 99% of the observations in the majority class.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem with unbalanced datasets is very simple: ML algorithms will try
    to find the best fit in the training data to maximize their accuracy. In a dataset
    where 99% of the cases belong to one single class, without any tuning, the algorithm
    is likely to prioritize the assertiveness of the majority class. In the worst-case
    scenario, it will classify all the observations as the majority class and ignore
    the minority one, which is usually our interest when modeling.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'To deal with unbalanced datasets, we have two major directions we can follow:
    tuning the algorithm to handle the issue or resampling the data to make it more
    balanced.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: By tuning the algorithm, you have to specify the weight of each class under
    classification. This class weight configuration belongs to the algorithm, not
    to the training data, so it is a hyperparameter setting. It is important to keep
    in mind that not all algorithms will have that type of configuration, and that
    not all ML frameworks will expose it, either. As a quick reference, we can mention
    the `DecisionTreeClassifier` class, from the scikit-learn ML library, as a good
    example that does implement the class weight hyperparameter.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Another way to work around unbalanced problems is changing the training dataset
    by applying **undersampling** or **oversampling**. If you decide to apply undersampling,
    all you have to do is remove some observations from the majority class until you
    get a more balanced dataset. Of course, the downside of this approach is that
    you may lose important information about the majority class that you are removing
    observations from.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: The most common approach for undersampling is known as random undersampling,
    which is a naïve resampling approach where we randomly remove some observations
    from the training set.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, you can decide to go for oversampling, where you will create
    new observations/samples of the minority class. The simplest approach is the naïve
    one, where you randomly select observations from the training set (with replacement)
    for duplication. The downside of this method is the potential issue of overfitting,
    since you will be duplicating/highlighting the observed pattern of the minority
    class.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: To either underfit or overfit of your model, you should always test the fitted
    model on your testing set.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'The testing set cannot be under/over sampled: only the training set should
    pass through these resampling techniques.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also oversample the training set by applying synthetic sampling techniques.
    Random oversample does not add any new information to the training set: it just
    duplicates the existing ones. By creating synthetic samples, you are deriving
    those new observations from the existing ones (instead of simply copying them).
    This is a type of data augmentation technique known as the **Synthetic Minority
    Oversampling Technique** (**SMOTE**).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Technically, what SMOTE does is plot a line in the feature space of the minority
    class and extract points that are close to that line.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'You may find questions in your exam where the term SMOTE has been used. If
    that happens, keep the context where this term is applied in mind: oversampling.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the next topic regarding data preparation, where we will
    learn how to prepare text data for machine learning models.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with text data
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already learned how to transform categorical features into numerical
    representations, either using label encoders, ordinal encoders, or one-hot encoding.
    However, what if we have fields containing long piece of text in our dataset?
    How are we supposed to provide a mathematical representation for them in order
    to properly feed ML algorithms? This is a common issue in **natural language processing**
    (**NLP**), a subfield of AI.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: NLP models aim to extract knowledge from texts; for example, translating text
    between languages, identifying entities in a corpus of text (also known as **Name
    Entity Recognition** (**NER**)), classifying sentiments from a user review, and
    many other applications.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B16735_02_Final_VK_ePub.xhtml#_idTextAnchor032)*, AWS Application
    Services for AI/ML*, you learned about some AWS application services that apply
    NLP to their solutions, such as Amazon Translate and Amazon Comprehend. During
    the exam, you might be asked to think about the fastest or easiest way (with the
    least development effort) to build certain types of NLP applications. The fastest
    or easiest way is usually to use those out of the box AWS services, since they
    offer pre-trained models for some use cases (especially machine translation, sentiment
    analysis, topic modeling, document classification, and entity recognition).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: In a few chapters' time, you will also learn about some built-in AWS algorithms
    for NLP applications, such as BlazingText, **Latent Dirichlet Allocation** (**LDA**),
    **Neural Topic Modeling**, and the Sequence-to-Sequence algorithm. Those algorithms
    also let you create the same NLP solutions that are created by those out of box
    services; however, you have to use them on SageMaker and write your own solution.
    In other words, they offer more flexibility, but demand more development effort.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Keep that in mind for your exam!
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Although AWS offers many out of the box services and built-in algorithms that
    allow us to create NLP applications, we will not look at those AWS product features
    now (as we did in [*Chapter 2*](B16735_02_Final_VK_ePub.xhtml#_idTextAnchor032)*,
    AWS Application Services for AI/ML*, and will do so again in [*Chapter 7*](B16735_07_Final_VK_ePub.xhtml#_idTextAnchor136)*,
    Applying Machine Learning Algorithms*). We will finish this chapter by looking
    at some data preparation techniques that are extremely important for preparing
    your data for NLP.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first one we will cover is known as **bag of words** (**BoW**). This is
    a very common and simple technique, applied to text data, that creates matrix
    representations to describe the number of words within the text. BoW consists
    of two main steps: creating a vocabulary and creating a representation of the
    presence of those known words from the vocabulary in the text. These steps can
    be seen in the following diagram:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.23 – BoW in action'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_023.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.23 – BoW in action
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: First things first, we usually can't use raw texts to prepare a bag of words
    representation. There is a data cleansing step where we will lowercase the text;
    split each work into tokens; remove punctuation, non-alphabetical, and stop words;
    and, whenever necessary, apply any other custom cleansing technique you may want.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Once you have cleansed your raw text, you can add each word to a global vocabulary.
    Technically, this is usually a dictionary of tuples, in the form of (word, number
    of occurrences); for example, {(apple, 10), (watermelon, 20)}. As I mentioned
    previously, this is a global dictionary, and you should consider all the texts
    you are analyzing.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Now, with the cleansed text and updated vocabulary, we can build our text representation
    in the form of a matrix, where each column represents one word from the global
    vocabulary and each row represents a text you have analyzed. The way you represent
    those texts on each row may vary according to different strategies, such as binary,
    frequency, and count. Let's dive into these strategies a little more.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we are processing a single text but trying the three
    different strategies for bag of words. That's why you can see three rows on that
    table, instead of just one (in a real scenario, you have to choose one of them
    for implementation).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: In the first row, we have used a binary strategy, which will assign 1 if the
    word exists in the global vocabulary and 0 if not. Because our vocabulary was
    built on a single text, all the words from that text belong to the vocabulary
    (the reason you can only see 1s in the binary strategy).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: In the second row, we have used a frequency strategy, which will check the number
    of occurrences of each word within the text and divide it by the total number
    of words within the text. For example, the word "this" appears just once (1) and
    there are seven other words in the text (7), so 1/7 is equal to 0.14.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the third row, we have used a count strategy, which is a simple
    count of occurrences of each word within the text.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: This note is really important – you are likely to find it in your exam. You
    may have noticed that our BoW matrix contains **unique words** in the *columns*
    and **each text** representation is in the *rows*. If you have 100 long texts
    with only 50 unique words across them, your BoW matrix will have 50 columns and
    100 rows. During your exam, you are likely to receive a list of texts and be asked
    to prepare the BoW matrix.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: There is one more extremely important concept you should know about BoW, which
    is the **n-gram** configuration. The term n-gram is used to describe the way you
    would like to look at your vocabulary, either via single words (uni-gram), groups
    of two words (bi-gram), groups of three words (tri-gram), or even groups of n
    words (n-gram). So far, we have seen BoW representations using a uni-gram approach,
    but more sophisticated representations of BoW may use bi-grams, tri-grams, or
    n-grams.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'The main logic itself does not change, but you need to know how to represent
    n-grams in BoW. Still using our example from the preceding diagram, a bi-gram
    approach would combine those words in the following way: [this movie, movie really,
    really good, good although, although old, old production]. Make sure you understand
    this before taking the exam.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The power and simplicity of BoW comes from the fact that you can easily come
    up with a training set to test your algorithms, or even create a baseline model.
    If you look at *Figure 3.23*, can you see that having more data and just adding
    a classification column to that table, such as good or bad review, would allow
    us to train a binary classification model to predict sentiments?
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Alright – you might have noticed that many of the awesome techniques that I
    have introduced for you come with some downsides. The problem with BoW is the
    challenge of maintaining its vocabulary. We can easily see that, in a huge corpus
    of texts, the vocabulary size tends to become bigger and bigger and the matrices'
    representations tend to be sparse (I know – the sparsity issue again).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: One possible way to solve the vocabulary size issue is by using word hashing
    (also known in ML as the **hashing trick**). Hash functions map data of arbitrary
    sizes to data of a fixed size. This means you can use the hash trick to represent
    each text with a fixed number of features (regardless of the vocabulary's size).
    Technically, this hashing space allows collisions (different texts represented
    by the same features), so this is something to take into account when you're implementing
    feature hashing.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another problem that comes with BoW, especially when we use the frequency strategy
    to build the feature space, is that more frequent words will strongly boost their
    scores due to the high number of occurrences within the document. It turns out
    that, often, those words with high occurrences are not the key words of the document,
    but just other words that *also* appear many times in several other documents.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '**Term Frequency – Inverse Term Frequency** (**TF-IDF**) helps penalize these
    types of words, by checking how frequent they are in other documents and using
    that information to rescale the frequency of the words within the document.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the process, TF-IDF tends to give more importance to words that
    are unique to the document (document-specific words). Let's look at a concrete
    example so that you can understand it in depth.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Consider that we have a text corpus containing 100 words and the word "Amazon"
    appears three times. The **Term Frequency** (**TF**) of this word would be 3/100,
    which is equal to 0.03\. Now, suppose we have other 1,000 documents and that the
    word "Amazon" appears in 50 of these. In this case, the **Inverse Term Frequency**
    (**IDF**) would be given by the log as 1,000/50, which is equal to 1.30\. The
    TF-IDF score of the word "Amazon", in that specific document under analysis, will
    be the product of TF * IDF, which is 0.03 * 1.30 (*0.039*).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose that instead of 50 documents, the word "Amazon" had also appeared
    on another 750 documents – in other words, much more frequently than in the prior
    scenario. In this case, we will not change the TF part of this equation – it is
    still 0.03\. However, the IDF piece will change a little since this time, it will
    be log 1,000/750, which is equal to *0.0036*. As you can see, this time, the word
    "Amazon" has much less importance than in the previous example.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike traditional approaches, such as BoW and TD-IDF, modern methods of text
    representation will take care of the context of the information, as well as the
    presence or the frequency of words. One very popular and powerful approach that
    follows this concept is known as **word embedding**. Word embeddings create a
    dense vector of a fixed length that can store information about the context and
    meaning of the document.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Each word is represented by a data point in a multidimensional hyper plan, which
    we call an embedding space. This embedding space will have "n" dimensions, where
    each of these dimensions refers to a particular position of this dense vector.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it may sound confusing, the concept it is actually pretty simple.
    Let''s suppose we have a list of four words, and we want to plot them in an embedding
    space of five dimensions. The words are king, queen, live, and castle. The following
    table shows how to do this:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – An embedding space representation'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_024.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.24 – An embedding space representation
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Forget the hypothetical numbers in the preceding table and focus on the data
    structure; you will see that each word is now represented by "n" dimensions in
    the embedding space. This process of transforming words into vectors can be performed
    by many different methods, but the most popular ones are **word2vec** and **GloVe**.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Once you have each word represented as a vector of a fixed length, you can apply
    many other techniques to do whatever you need. One very common task is plotting
    those "words" (actually, their dimensions) in a hyper plan and, visually, checking
    how close they are to each other!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Technically, we don't use this to plot them as-is, since human brains cannot
    interpret more than three dimensions. Then, we usually apply a dimensionality
    reduction technique (such as principal component analysis, which you will learn
    about later) to reduce the number of dimensions to two, and finally plot the words
    in a cartesian plan. That's why you might have seen pictures like the one at the
    bottom of the following diagram. Have you ever asked yourself how is it possible
    to plot words on a graph?
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Plotting words'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_025.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.25 – Plotting words
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: In case you are wondering how we came up with the dimensions shown in *Figure
    3.24*, let's dive into it. Again, there are different methods to do this, but
    let's look at one of the most popular, which uses a co-occurrence matrix with
    a fixed context window.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: First, we have to come up with some logic to represent each word, keeping in
    mind that we also have to take their context into consideration. To solve the
    context requirement, we will define a **fixed context window**, which is going
    to be responsible for specifying how many words will be grouped together for context
    learning. For instance, let's set this fixed context window to 2.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create a **co-occurrence matrix**, which will count the number
    of occurrences of each pair of words, according to the pre-defined context window.
    Let''s see this in action. Consider the following text: "I will pass this exam,
    you will see. I will pass it".'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'The context window of the first word "pass" would be the ones in *bold*: "*I
    will* pass *this exam*, you will see. I will pass it". Considering this logic,
    let''s see how many times each pair of words appears in the context window:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.26 – Co-occurrence matrix'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_026.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.26 – Co-occurrence matrix
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the pair of words "I will" appears three times when we use
    a context window of size 2:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '*I will* pass this exam, you will see. I will pass it.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I will pass this exam, you *will* see. *I* will pass it.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I will pass this exam, you will see. *I will* pass it.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at the preceding table, the same logic should be applied to all other
    pairs of words, replacing "…" with the associated number of occurrences. You now
    have a numerical representation for each word!
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: You should be aware that there many alternatives to co-occurrence matrices with
    a fixed context window, such as using TD-IDF vectorization or even simpler counters
    of words per document. The most important message here is that, somehow, you must
    come up with a numerical representation for each word.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is finally finding those dimensions shown in *Figure 3.24*. You
    can do this by creating a multilayer model, usually based on neural networks,
    where the hidden layer will represent your embedding space. The following diagram
    shows a simplified example where we could potentially compress our words shown
    in the preceding table into an embedding space of five dimensions:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27 – Building embedding spaces with neural networks'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_03_027.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.27 – Building embedding spaces with neural networks
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: We will talk about neural networks in more detail later in this book. For now,
    understanding where the embedding vector comes from is already an awesome achievement!
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: Another important thing you should keep in mind while modeling natural language
    problems is that you can reuse a pre-trained embedding space in your models. Some
    companies have created modern neural network architectures, trained on billions
    of documents, which has become the state of the art in this field. For your reference,
    take a look at **Bidirectional Encoder Representations from Transformers** (**BERT**),
    which was proposed by Google and has been widely used by the data science community
    and industry.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: We have now reached the end of this long – but very important – chapter about
    data preparation and transformation. Let's take this opportunity to do a quick
    recap of the awesome things we have learned.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, you were introduced to the different types of features that you might
    have to work with. Identifying the type of variable you'll be working with is
    very important for defining the types of transformations and techniques that can
    be applied to each case.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned how to deal with categorical features. We saw that, sometimes,
    categorical variables do have an order (such as the ordinal ones), while other
    times, they don't (such as the nominal ones). You learned that one-hot encoding
    (or dummy variables) is probably the most common type of transformation for nominal
    features; however, depending on the number of unique categories, after applying
    one-hot encoding, your data might suffer from sparsity issues. Regarding ordinal
    features, you shouldn't create dummy variables on top of them, since you would
    be losing the information of order that's been incorporated into the variable.
    In those cases, ordinal encoding is the most appropriate transformation.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: We continued our journey by looking at numerical features, where we learned
    how to deal with continuous and discrete data. We walked through the most important
    types of transformations, such as normalization, standardization, binning, and
    discretization. You saw that some types of transformation rely on the underlying
    data to find their parameters, so it is very important to avoid using the testing
    set to learn anything from the data (it must be used strictly for testing).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: You have also seen that we can even apply pure math to transform our data; for
    example, you learned that power transformations can be used to reduce the skewness
    of your feature and make it more normal.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Next, we looked at missing data and got a sense for how critical this task is.
    When you are modeling, you *can't* look at the missing values as a simple computational
    problem, where you just have to replace x with y. This is a much bigger problem
    where you need to start solving it by exploring your data, and then checking if
    your missing data was generated at random or not.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: When you are making the decision to remove or replace missing data, you must
    be aware that you are either losing information or adding bias to the data, respectively.
    Remember to review all the important notes we gave you, since they are likely
    to be, somehow, present in your exam.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Next, you learned about outlier detection. You looked at different ways to find
    outliers, such as the zscore and box plot approaches. Most importantly, you learned
    that you can either flag or smooth them.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: At the start, I told you that this chapter would be a long but worthwhile journey
    about data preparation, which is why I needed to give you a good sense of how
    to deal with rare events, since this is one the most challenging problems on ML.
    You learned that, sometimes, your data might be unbalanced, and you must either
    trick your algorithm (by changing the class weight) or resample your data (applying
    undersampling and oversampling).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned how to deal with text data for NLP. You should now be able
    to manually compute bag of words and TF-IDF matrices! We went even deeper and
    learned how word embedding works. During this subsection, we learned that we can
    either create our own embedding space (using many different methods) or reuse
    a pre-trained one, such as BERT.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: We are done! I am so glad you made it here and I am sure this chapter is crucial
    to your success in the exam. Finally, we have prepared some practice questions
    for you; I hope you enjoy them.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into data visualization techniques.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are working as a data scientist for a healthcare company and are creating
    a machine learning model to predict fraud, waste, and abuse across the company's
    claims. One of the features of this model is the number of times a particular
    drug has been prescribed, to the same patient of the claim, in a period of 2 years.
    Which type of feature is this?
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Discrete
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Continuous
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Nominal
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Ordinal
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a, The feature is counting the number of times that a particular drug has been
    prescribed. Individual and countable items are classified as discrete data.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You are building a ML model for an educational group that owns schools and
    universities across the globe. Your model aims to predict how likely a particular
    student is to leave his/her studies. Many factors may contribute to school dropout,
    but one of your features is the current academic stage of each student: preschool,
    elementary school, middle school, or high school. Which type of feature is this?'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Discrete
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Continuous
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Nominal
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Ordinal
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d, The feature has an implicit order and should be considered a categorical/ordinal
    variable.
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are building a machine learning model for a car insurance company. The company
    wants to create a binary classification model that aims to predict how likely
    their insured vehicles are to be stolen. You have considered many features to
    this model, including the type of vehicle (economy, compact, premium, luxury,
    van, sport, and convertible). How would you transform the type of vehicle in order
    to use it in your model?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Applying ordinal encoding
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Applying one-hot encoding
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) No transformation is needed
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Options A and B are valid transformations for this problem
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b, In this case, we have a categorical/nominal variable (there is no order among
    each category). Additionally, the number of unique categories looks pretty manageable;
    furthermore, one-hot encoding would fit very well for this type of data.
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working as a data scientist for a financial company. The company wants
    to create a model that aims to classify improper payments. You have decided to
    use "type of transaction" as one of your features (local, international, pre-approved,
    and so on). After applying one-hot encoding to this variable, you realize that
    your dataset has many more variables, and your model is taking a lot of time to
    train. How could you potentially solve this problem?
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) By applying ordinal encoding instead of one-hot encoding. In this case, we
    would be creating just one feature.
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) By applying label encoding.
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) By analyzing which types of transactions have the most impact on improper/proper
    payments. Only apply one-hot encoding to the reduced types of transactions.
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) By porting your model to another programming language that can handle sparse
    data in a better way.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c, Your transformation resulted in more features due to the excessive number
    of categories in the original variable. Although the one-hot encoding approach
    looks right, since the variable is a nominal feature, the number of levels (unique
    values) for that feature is probably too high.
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, you could do exploratory data analysis to understand the most
    important types of transactions for your problem. Once you know that information,
    you can then restrict the transformation to just those specific types (reducing
    the sparsity of your data). It is worth adding that you would be missing some
    information during this process because now, your dummy variables would only be
    focusing only on a subset of categories, but it is a valid approach.
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working as a data scientist for a marketing company. Your company is
    building a clustering model that will be used to segment customers. You decided
    to normalize the variable "annual income", which ranges from between 50,000 and
    300,000.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After applying normalization, what would be the normalized values of a group
    of customers that earn 50,000, 125,000 and 300,000?
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) 1, 2, and 3
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) 0, 0.5, and 1
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) 0, 0.25, and 1
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) 5, 12, and 30
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b, Applying the normalization formula and assuming the expected range would
    be 0 and 1, the correct answer is b.
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Consider a dataset that stores the salaries of employees in a particular column.
    The mean value of salaries on this column is $2,000, while the standard deviation
    is equal to $300\. What is the standard scaled value of someone that earns $3,000?
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) 3.33
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) 6.66
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) 10
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) 1
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a, Remember the standard scale formula: (X - µ) / σ, which is (3,000 – 2,000)
    / 300.'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which type of data transformations can we apply to convert a continuous variable
    into a binary variable?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Binning and one-hot encoding
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Standardization and binning
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Normalization and one-hot encoding
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Standardization and one-hot encoding
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a, In this case, we could discretize a continuous variable by applying binning
    and then getting the dummy variables.
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are a data scientist for a financial company and you have been assigned
    the task of creating a binary classification model to predict whether a customer
    will leave the company or not (also known as churn). During your exploratory work,
    you realize that there is a particular feature (credit utilization amount) with
    some missing values. This variable is expressed in real numbers; for example,
    $1,000\. What would be the fastest approach to dealing with those missing values,
    assuming you don't want to lose information?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Dropping any missing values.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Creating a classification model to predict the credit utilization amount
    and use it to predict the missing data.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Creating a regression model to predict the credit utilization amount and
    use it to predict the missing data.
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Replacing the missing data with the mean or median value of the variable.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d, In this case, almost all the options are valid approaches to deal with missing
    data for this problem, except option b because predicting the missing data would
    require you to create a regression model, not a classification model. Option a
    is a valid approach to deal with missing data, but not to this problem where we
    don't want to lose information. Option c is also a valid approach, but not the
    fastest one. Option d is the most appropriate answer for this question.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have to create a machine learning model for a particular client, but you
    realize that most of the features have more than 50% of data missing. What's our
    best option on this critical case?
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Dropping entire columns with more than 50% of missing data
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Removing all the rows that contains at least one missing information
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Check with the dataset owner if you can retrieve the missing data from somewhere
    else
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Replace the missing information by the mean or median of the feature
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c, This is a very critical case where most of your information is actually missing.
    You should work with the dataset owner to understand why that problem is occurring
    and check the process that generates this data. If you decide to drop missing
    values, you would be losing a lot of information. On the other hand, if you decide
    to replace missing values, you would be adding a lot of bias to your data.
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working as a senior data scientist from a human resource company. You
    are creating a particular machine learning model that uses an algorithm that does
    not perform well on skewed features, such as the one in the following image:![](img/B16735_03_028.jpg)
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.28 – Skewed feature
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which transformations could you apply to this feature to reduce its skewness
    (choose all the correct answers)?
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) Normalization
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Log transformation
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Exponential transformation
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Box-Cox transformation
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b, d, To reduce skewness, power transformations are the most appropriate. Particularly,
    you could apply the log transformation or Box-Cox transformation to make this
    distribution more similar to a Gaussian one.
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working on a fraud identification issue where most of your labeled data
    belongs to one single class (not fraud). Only 0.1% of the data refers to fraudulent
    cases. Which modeling techniques would you propose to use on this use case (choose
    all the correct answers)?
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Applying random oversampling to create copies of the fraudulent cases.
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Applying random undersampling to remove observations from the not fraudulent
    cases.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) We can't create a classification model on such an unbalanced dataset. The
    best thing to do would be to ask for more fraudulent cases from the dataset owner.
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Applying synthetic oversampling to create copies of the not fraudulent cases.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a, b, Unbalanced datasets are very common with ML, and there are many different
    ways to deal with this problem. Option c is definitely not the right one. Pay
    attention to option d; you should be able to apply synthetic oversampling to your
    data, but to create more observations of the minority class, not from the majority
    class. Options a and b are correct.
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You are preparing text data for machine learning. This time, you want to create
    a bi-gram BoW matrix on top of the following texts:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"I will master this certification exam"'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"I will pass this certification exam"'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How many rows and columns would you have on your BoW matrix representation?
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) 7 rows and 2 columns
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) 2 rows and 7 columns
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) 14 rows and 4 columns
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) 4 columns and 14 rows
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b, Let's compute the matrix in the following table together.
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As you can see, the trick is knowing which tokens are common to the two texts.
    We only have two texts; the number of rows will be also two – one for each of
    them:'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B16735_03_029.jpg)'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 3.29 – Resulting bag of words
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are running a survey to check how many years of experience a group of people
    has on a particular development tool. You came up with the following distribution:![](img/B16735_03_030.jpg)
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 3.30 – Skewed data distribution
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What can you say about the mode, mean, and median of this distribution (choose
    all the correct answers)?
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The mean is greater than the median
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The mean is smaller than the median
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The median is greater than the mode
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) The median is smaller than the mode
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a, c, This is a skewed distribution (to the right-hand side). This means your
    mean value will be pushed to the same side of the tail, followed by the median.
    As result, we will have mode < median < mean.
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working as a data scientist for a retail company. You are building a
    decision tree-based model for classification purposes. During your evaluation
    process, you realize that the model's accuracy is not acceptable. Which of the
    following tasks is *not* a valid approach for increasing the accuracy of your
    decision tree model?
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Tuning the hyperparameters of the model.
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Scaling numerical features.
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Trying different approaches for transforming categorical features, such as
    binary encoding, one-hot encoding, and ordinal encoding (whenever applicable).
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) If the dataset is not balanced, it is worth trying different types of resampling
    techniques (undersampling and oversampling).
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b, Scaling numerical features is an important task with machine learning models
    but is not always needed. Particularly on decision tree-based models, changing
    the scale of the data will not result in better model performance, since this
    type of model is not impacted by the scale of the data.
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are working on a data science project where you must create an NLP model.
    You have decided to test Word2vec and GloVe during your model development, as
    an attempt to improve model accuracy. Word2vec and GloVe are two types of what?
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Pre-trained word embeddings
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Pre-trained TF-IDF vectors
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) One-hot encoding techniques
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) None of above
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answer
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a, Some NPL architectures may include embedding layers. If you are facing this
    type of project, you can either create your own embedding space by training a
    specific model for that purpose (using your own corpus of data) or you can use
    any pre-trained word embedding model, such as the Word2vec model, which is pre-trained
    by Google.
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
