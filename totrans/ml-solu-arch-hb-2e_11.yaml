- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building ML Solutions with AWS AI Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to this point, we have mainly focused on the skills and technologies required
    to build and deploy ML models using open-source technologies and managed ML platforms.
    To solve business problems with ML, however, you don’t always have to build, train,
    and deploy your ML models from scratch. An alternative option is to use fully
    managed AI services. AI services are fully managed APIs or applications with pre-trained
    models that perform specific ML tasks, such as object detection or sentiment analysis.
    Some AI services also allow you to train custom models with your data for a defined
    ML task, such as document classification. AI services promise to enable organizations
    to build ML-enabled solutions without requiring strong ML competencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to switch gears and talk about several AWS AI
    services and where they can be used in business applications. Please note that
    the focus of this chapter will not be to deep dive into individual AI services,
    as that warrants dedicated books. Instead, we will focus on ML use cases that
    can be powered by AI services, and the architecture patterns that you can use
    to deploy these AI services. After reading this chapter, you should be able to
    identify some use cases where AI services can be a good fit and know where to
    find additional resources to get a deeper understanding of these services. Specifically,
    we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are AI services?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of AWS AI services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building intelligent solutions using AI services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing an MLOps architecture for AI services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on lab – running ML tasks with AI services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will continue to use our AWS environment for the hands-on portion of this
    book. The associated code samples can be found at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter11](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter11)
  prefs: []
  type: TYPE_NORMAL
- en: What are AI services?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI services are pre-built fully managed services that perform a particular set
    of ML tasks out of the box, such as facial analysis or text analysis. The primary
    target users for AI services are application developers who want to build AI applications
    without the need to build ML models from scratch. In contrast, the target audiences
    for ML platforms are data scientists and ML engineers, who need to go through
    the full ML lifecycle to build and deploy ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an organization, AI services mainly solve the following key challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lack of high-quality training data for ML model development**: To train high-quality
    models, you need a large amount of high-quality curated data. For many organizations,
    data poses many challenges in data sourcing, data engineering, and data labeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of data science skills for building and deploying custom ML models**:
    Data science and ML engineering skills are scarce in the market and expensive
    to acquire.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slow product time-to-market**: Building and deploying custom models and engineering
    infrastructure is time-consuming. This can be a hurdle for a quick time-to-market
    product delivery goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Undifferentiated ML capabilities**: Many ML problems can be solved using
    commodity ML capabilities that do not provide unique competitive advantages. Spending
    resources on building undifferentiated ML capabilities can be a waste of scarce
    resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System scalability challenge**: Managing scalable infrastructure to meet
    dynamic market demands and growth is an engineering challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While AI services can provide a cost-effective way of building ML-enabled products
    quickly, they do come with limitations. The main limitation is the lack of customization
    flexibility for specific functional and technical requirements. AI services usually
    focus on specific ML tasks with a predefined set of algorithms, so you usually
    don’t have the flexibility to alter the functionality of AI services. With AI
    services, you normally do not have access to the underlying models, thus limiting
    your ability to deploy the model elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: The number of offerings in AI services has grown extensively in recent years,
    and we expect this trend to continue at an accelerated pace. Let’s shift our focus
    and talk about several AWS AI services.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of AWS AI services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS provides AI services in multiple ML domains, such as text and vision, as
    well as AI services for industrial use cases such as manufacturing anomaly detection
    and predictive maintenance. In this section, we will cover a subset of AWS AI
    services. The objective of this section will not be to deep dive into individual
    services but rather to make you aware of the fundamental capabilities offered
    by these AI services. This will let you know where and how these services can
    be integrated into your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Comprehend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NLP has gained significant interest across different industries in solving
    a range of business problems, such as automatic document processing, text summarization,
    document understanding, and document management and retrieval. **Amazon Comprehend**
    is an AI service that can perform NLP analysis on unstructured text documents.
    At its core, Amazon Comprehend provides the following main capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entity recognition**: Entities are the who, what, where, and when of text
    analytics. Entities can be the most important parts of a sentence as they identify
    the key components in a text. Examples of entities are proper nouns such as a
    person, place, or product. Entities can be used to create document search indexes
    and identify key information or relationships across documents. Comprehend provides
    APIs (for example, `DetectEntities`) for detecting entities with its built-in
    entity recognition models. It can detect entities such as people, places, organizations,
    and dates from the input text. You can also use Comprehend to train a custom entity
    recognizer for your custom entities if the built-in models do not meet your requirements.
    To train a custom entity recognizer, you can use the `CreateEntityRecognizer`
    API with your training data in the following two formats:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Annotation**: You provide the locations of entities (beginning and end offsets
    of target characters) in documents, along with the entity type for each pair of
    offsets. This helps Comprehend train on both the entities and the context they
    are in.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Entity list**: You provide a list of entities and their entity types in plaintext
    and Comprehend will train to detect these specific entities. You can evaluate
    the custom model using the metrics emitted by a Comprehend custom model training
    job. Example evaluation metrics include precision, recall, and F1 scores.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Additional details on the evaluation metrics for Comprehend can be found at
    [https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html](https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html).
    Once the model has been trained, you have the option to deploy the model behind
    a private prediction endpoint to serve predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Comprehend can detect sentiment in text with its `DetectSentiment`
    API. Sentiment analysis is widely used in many business use cases, such as analyzing
    customers’ sentiments in customer support calls or understanding customers’ perceptions
    of products and services in reviews.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic modeling**: Topic modeling has a wide range of uses, including document
    understanding, document categorization and organization, information retrieval,
    and content recommendation. Comprehend can discover common topics among documents
    with its `StartTopicsDetectionJob` API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language detection**: Comprehend can detect the dominant language that’s
    used in the text with its `DetectDominantLanguage` API. This feature can help
    with use cases such as routing incoming customer support calls to the right channel
    based on the language or classifying documents by different languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Syntax analysis**: Comprehend can perform **part-of-speech** (**POS**) analysis
    of sentences using its `DetectSyntax` API. Example POSes include nouns, pronouns,
    verbs, adverbs, conjunctions, and adjectives in a sentence. POS analysis can help
    with use cases such as checking for the correctness of syntax and grammar in written
    text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event detection**: Comprehend can detect a predefined list of financial events
    such as IPO, stock split, and bankruptcy. It also detects augments associated
    with events such as a person or company filing for bankruptcy. This relationship
    helps build a knowledge graph to help us understand the who-did-what of the different
    events. You can find a full list of event and augment types at [https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html](https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text classification**: You can train a custom text classifier using your
    training data with Comprehend. Comprehend lets you train multi-class and multi-label
    classifiers through its `CreateDocumentClassifier` API. Multi-class assigns a
    single label to a text, whereas multi-label assigns multiple labels to a text.
    To evaluate the performance of the custom classifier, Comprehend provides a list
    of metrics that include accuracy, recall, and F1 score. You can find the full
    list of metrics at [https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html](https://docs.aws.amazon.com/comprehend/latest/dg/cer-doc-class.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personally identifiable information (PII) detection**: You can detect PII
    within English or Spanish text documents. The PII detection process offers the
    option to either locate or redact PII entities within the text. For locating PII
    entities, real-time analysis or asynchronous batch jobs can be employed. On the
    other hand, redacting PII entities specifically requires the use of an asynchronous
    batch job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key phrase detection**: The Key Phrases functionality in Amazon Comprehend
    uses ML models to analyze the input text and extract the most significant phrases
    or topics. These key phrases can provide a concise summary or highlight the main
    ideas and concepts present in the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flywheels are a feature of Comprehend that provides a managed workflow for continuously
    improving a custom natural language processing model. A flywheel stores all model
    data and versions in an AWS-managed data lake. As new labeled datasets become
    available, you create flywheel iterations to retrain and evaluate new model versions
    using the latest data. Based on performance metrics, the best new version can
    be promoted to become the active model serving inferences. This iterative process
    allows the model accuracy to improve over time as regular model retraining incorporates
    fresh data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comprehend APIs can be invoked using the `boto3` library and AWS **command-line
    interface** (**CLI**). You can find a full list of supported `boto3` methods for
    Comprehend at [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html).
    The following shows the Python syntax for invoking Comprehend’s entity detection
    functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Amazon Comprehend can be a good fit for building intelligent document processing
    solutions and other NLP products. It can also serve as a good baseline tool that
    can be compared with custom NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Textract
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many business processes, such as loan application processing, expense processing,
    and medical claim processing, require extracting text and numbers from images
    and documents. Currently, many organizations largely handle these processes manually
    and the processes can be highly time-consuming and slow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Textract** is an **optical character recognition** (**OCR**) AI service
    that’s primarily used for extracting printed text, handwritten text, and numbers
    from images and PDF documents. Textract is normally used as a processing step
    for downstream tasks such as document analysis and data entries. The core Textract
    functionalities are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OCR**: OCR is a computer vision task that detects and extracts text data
    from PDF documents and images. The OCR component in Textract extracts raw text
    from the input documents and provides additional structural information about
    the documents. For example, the Textract output contains hierarchical structural
    relationships for the different objects in a document such as pages, paragraphs,
    sentences, and words. Textract also captures the positional information of the
    different objects in the input document. The hierarchical structural information
    and object positional data are useful when you’re extracting specific information
    from different locations in the documents. The OCR APIs are `DetectDocumentText`
    for detecting text synchronously and `StartDocumentTextDetection` for detecting
    text asynchronously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table extraction**: Many documents contain tabular data structures and need
    to be processed as a table. For example, you might have an insurance claim document
    that contains a list of claim items and their details in different columns, and
    you may want to enter these claim items into a system. The table extraction component
    in Textract can extract tables and cells in the tables from a document. To use
    the table extraction feature, you can use the `AnalyzeDocument` API for synchronous
    operations and `StartDocumentAnalysis` for asynchronous operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Form extraction**: Documents such as paystubs and loan application forms
    contain many name-value pairs whose relationships need to be preserved when they’re
    processed automatically. The form extraction component in Textract can detect
    these name-value pairs and their relationships for downstream processing, such
    as entering the names in those documents into a system. The form extraction component
    shares the same `AnalyzeDocument` and `StartDocumentAnalysis` APIs as the table
    extraction component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Signature in document analysis**: Textract can detect signature locations
    in documents, returning bounding boxes specifying signature positions and confidence
    scores. Signature detection can run independently or alongside other features
    like forms, tables, and custom queries. When combined with forms or tables, Textract
    relates detected signatures to corresponding cells or key-value pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Queries in document analysis:** Textract allows adding custom queries to
    extract specific information from documents. A query like “What is the applicant’s
    address?” will return just that data point from the analyzed document in a separate
    response structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Textract also has features for analyzing a specific kind of documents such as
    invoices and identity documents. For example, Textract can extract relevant data
    such as vendor and receiver contact information from an invoice or receipt without
    the need for any templates or configuration. Similarly, it can extract relevant
    information from passports, driver’s licenses, and other identity documentation
    issued by the US Government.
  prefs: []
  type: TYPE_NORMAL
- en: The Textract APIs are supported in the `boto3` library. The following code sample
    shows how to detect text using the `boto3` library. The full list of Textract
    APIs for `boto3` can be found at [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Textract also integrates with the **Amazon Augmented AI** (**A2I**) service
    to enable human-in-the-loop workflow integration for reviewing low-confidence
    prediction results from Textract. You can find more information about the A2I
    service at [https://aws.amazon.com/augmented-ai](https://aws.amazon.com/augmented-ai).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Rekognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon Rekognition** is a video and image analysis AI service. It supports
    a range of use cases, such as metadata extraction from images and videos, content
    moderation, and security and surveillance. The core capabilities of Rekognition
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Label or object detection**: Label detection can be applied to use cases
    such as media metadata extraction for search and discovery, item identification
    and counting for insurance claim processing, and brand and logo detection. Rekognition
    can detect different objects, scenes, and activities in images and videos, and
    assign labels to them such as `soccer`, `outdoor`, and `playing soccer`. For the
    common objects that are detected, it also provides bounding boxes for the objects
    to indicate their specific positions in the image or videos. To use Rekognition
    for label detection, you can call the `DetectLabels` API. If Rekognition cannot
    detect specific objects in your images, you can also train a custom label detector
    with your training data using the `CreateProject` API. Once the model has been
    trained, you have the option to deploy a private prediction endpoint using the
    `StartProjectVersion` API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial analysis and recognition**: Facial analysis and recognition are useful
    for use cases such as video surveillance and security, automatic people labeling
    in images and video for content search, and understanding demographics. Rekognition
    can identify and analyze faces in images and videos. For example, you can perform
    analysis on faces to detect gender, age, and sentiment. You can also build an
    index of faces and assign names to them. Rekognition can map a detected face to
    a face in the index if a match is found. The main APIs for facial analysis and
    recognition are `DetectFaces`, `SearchFaces`, `IndexFaces`, and `CompareFaces`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content moderation**: Rekognition has APIs (`StartContentModeration`) for
    detecting images and videos with explicit content and scenes, such as violence.
    Organizations can use this feature to filter out inappropriate and offensive content
    before making the content available to consumers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Short text detection**: Rekognition can detect short text in images and provide
    bounding boxes around the detected text using its `DetectText` and `StartTextDetection`
    APIs. This feature can be used to detect street names, the names of stores, and
    license plate numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personal protection equipment (PPE) detection**: Rekognition provides a built-in
    feature for detecting PPE in images and videos using the `DetectProtectiveEquipment`
    API. This feature can be used for automated PPE compliance monitoring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Celebrity identification**: Rekognition also maintains a celebrity database
    that can be used for identifying known celebrities in images and videos. It has
    a list of APIs for this feature, including `RecognizeCelebrities` and `GetCelebrityInfo`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Rekognition also has native integration with Amazon Kinesis Video, a video stream
    service from AWS. You can build solutions to detect faces in real-time video streams.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Transcribe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon Transcribe** (**Transcribe**) is a speech-to-text AI service. It can
    be used to transcribe video and audio files and streams to text for a range of
    use cases, such as media content and meeting subtitling, call analytics, and converting
    medical conversations into electronic health records.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Transcribe supports both real-time transcription and batch transcription
    and has the following key capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Media transcription**: Transcribe has pre-trained models for converting media
    files or streams into text in different languages, such as English, Chinese, and
    Spanish. It also adds punctuation and capitalization to make the transcribed text
    more readable. To kick off transcription, you can use the `StartTranscriptionJob`
    and `StartMedicalTranscriptionJob` APIs for batch transcription, the `StartStreamingTranscription`
    API for streaming transcription, and the `StartMedicalStreamTranscription` API
    for streaming medical input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom models**: You can provide your training data to train custom language
    models to increase the accuracy of the transcription for industry-specific terms
    or acronyms. The API for creating custom models is `CreateLanguageModel`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Call analytics**: Transcribe provides built-in analytics capabilities for
    calls. The transcripts for calls are displayed in a turn-by-turn format. Some
    examples of supported analytics are sentiment analysis, call categorization, issue
    detection (the reason behind the call), and call characteristics (talk time, non-talk
    time, loudness, interruption). The API for starting a call analytics job is `StartCallAnalyticsJob`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redaction**: Transcribe can automatically mask or remove sensitive **personally
    identifiable information** (**PII**) data from transcripts to preserve privacy.
    When transcribing with redaction, Transcribe replaces PII information with **[PII]**
    in the transcript. To enable redaction, you can configure the `ContentRedaction`
    parameter in the batch transcription jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subtitle**: Transcribe can generate out-of-the-box subtitle files in WebVTT
    and SubRip format to use as video subtitles. To enable subtitle file generation,
    you can configure the `Subtitles` parameter for the transcription job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detecting toxic speech**: Transcribe Toxicity Detection leverages both audio
    and text-based cues to identify and classify voice-based toxic content across
    seven categories including sexual harassment, hate speech, threat, abuse, profanity,
    insult, and graphic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redacting transcripts**: Redaction helps securely remove sensitive data like
    names, addresses, and account details from speech-to-text outputs before further
    processing or analytics. This preserves privacy while still allowing leveraging
    transcripts for other downstream applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partitioning speakers**: In your transcription output, Amazon Transcribe
    allows you to distinguish between various speakers. The system can identify up
    to 10 distinct speakers and assigns a unique label to the text associated with
    each speaker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-channel transcription**: When dealing with audio containing two channels,
    you can employ channel identification to transcribe the speech from each channel
    independently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also a medical-related transcription service called Amazon Transcribe
    Medical, a HIPAA-eligible **automatic speech recognition** (**ASR**) service that
    is designed specifically for transcribing medical speech. The service can automatically
    identify and transcribe medical terminology, anatomical references, medications,
    procedures, and other clinically relevant information with high accuracy. Transcribe
    Medical also supports multiple input sources, including audio files, streaming
    data, and real-time transcription for live sessions, making it a versatile solution
    for healthcare providers, medical researchers, and life sciences organizations
    to efficiently convert medical speech into text for documentation, analysis, and
    downstream applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transcribe has a set of APIs for these different operations. The following
    code sample shows how to use the `boto3` library to kick off a transcription job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can find the full list of `boto3` APIs for Transcribe at [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/transcribe.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/transcribe.html).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Personalize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Personalized recommendations can help you optimize user engagement and revenues
    for many businesses such as e-commerce, financial product recommendation, and
    media content delivery. **Amazon Personalize** allows you to build personalized
    recommendation models using your data. You can use Personalize as the recommendation
    engine to power product and content recommendations based on individual tastes
    and behaviors. At a high level, the Personalize service provides the following
    three core functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User personalization**: Predicts the items a user will interact with or explore'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Similar items**: Computes similar items based on the co-occurrence of items
    and item metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized re-ranking**: Re-ranks the input list of items for a given user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon Personalize does not provide pre-trained models for recommendations.
    Instead, you need to train custom models using your data with the built-in algorithms
    provided by Personalize. To train a personalized model, you need to provide three
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Item dataset**: The item dataset contains the attributes of the items you
    want to recommend. This dataset helps Personalize learn about the contextual information
    about the items for better recommendations. This dataset is optional.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User dataset**: The user dataset contains attributes of the users. This allows
    Personalize to have a better representation of each user to provide highly personalized
    recommendations. This dataset is also optional.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User-item interaction dataset**: This is a required dataset, and it provides
    the historical interaction between users and items, such as viewing a movie or
    purchasing a product. Personalize uses this data to learn the behaviors of individual
    users toward different items to generate highly personalized recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To help understand how Personalize works, let’s review some of the main Personalize
    concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset group**: A dataset group contains related datasets (item, user, and
    interaction dataset) for model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recipe**: A recipe is the ML algorithm that’s used for model training. Personalize
    provides multiple recipes for the three main functionalities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solution**: A solution represents a trained Personalize model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Campaign**: A Personalize campaign is a hosted endpoint for a trained Personalize
    model to handle recommendation and ranking requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To train and deploy a custom model using Personalize, you must follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prepare and ingest the dataset**: In this step, you prepare the dataset in
    the required format, store it in S3, and then load the dataset into Personalize.
    There are three main API actions involved in this step – `CreateDatasetGroup`,
    `CreateDataset`, and `CreateDatasetImportJob`. `CreateDatasetGroup` creates an
    empty dataset group. `CreateDataset` adds datasets (for example, item dataset,
    user dataset, and interaction dataset) to a dataset group, and `CreateDatasetImportJob`
    kicks off a data ingestion job to load data from S3 to the Personalize data repository
    for subsequent model training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pick a recipe for model training**: In this step, you choose a recipe (ML
    algorithm) to use for the different model training processes. There are multiple
    recipe options available for user personalization, related items, and personalized
    ranking. You can use the `ListRecipes` API to get the full list of recipes. The
    recipes are designed for specific use cases such as next best action, trending
    and popular items, or similar items. Pick the appropriate recipes based on the
    use cases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a solution**: In this step, you configure a solution with the dataset
    group and recipe for the model training job using the `CreateSolution` API. Then,
    you use the `CreateSolutionVersion` API to kick off the training job.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate the model**: In this step, you evaluate the model metrics and determine
    if they meet the performance target. If they do not, then consider retraining
    the model using higher-quality and/or more data. Personalize outputs several evaluation
    metrics for the trained models, such as coverage, mean reciprocal rank, precision,
    and normalized discounted accumulative gain. You can find more details about these
    metrics at [https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html](https://docs.aws.amazon.com/personalize/latest/dg/working-with-training-metrics.html).
    The performance metrics are available in the Personalize management console. You
    can also get the metrics programmatically using the `GetSolutionMetrics` API.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a campaign**: In this final step, you deploy a solution (trained model)
    into the prediction endpoint so that you can use it in your applications. To do
    this, you can use the `CreateCampaign` API. You can provide additional configurations
    such as the **minimum provisioned transaction per second** (**minProvisionedTPS**)
    throughput, as well as item exploration configuration. Item exploration configuration
    allows Personalize to show a percentage of random items to users that are not
    based on user personalization. The idea is to let users explore items that they
    have not interacted with before to gauge interest. The item exploration configuration
    is only applicable for user personalization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can use the Personalize management console to build Personalize solutions
    and campaigns. Alternatively, you can use `boto3` to access the `personalize`
    API. The following code sample shows the Python syntax for creating a campaign.
    You can find the full list of `boto3` APIs for Personalize at [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/personalize.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Personalize also provides several advanced functionalities, such as filters,
    which allow you to remove items from your list of items based on rules. You can
    also optimize the model training using a business objective such as customer loyalty.
    This feature allows you to give recommendations that optimize a certain business
    outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Lex V2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Conversational agents have been broadly adopted across many different industries
    to improve the user engagement experience, such as self-service customer support
    and automating IT functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Lex V2 facilitates the creation of conversational interfaces using voice
    and text for applications. It offers functionalities like **natural language understanding**
    (**NLU**) and **automatic speech recognition** (**ASR**), allowing developers
    to build user-friendly interactions. Amazon Lex V2 has the following key concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bot**: An automated tool designed for tasks like placing orders, hotel bookings,
    or flower orders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language**: Amazon Lex V2 bots can handle multiple languages independently.
    Configurable to engage users with native expressions, the platform supports a
    variety of languages and locales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intent**: Representing user actions, intents are created to support related
    functionalities. For instance, an intent for ordering pizzas may include details
    like the intent name (e.g., `PlaceOrder`), sample utterances, and fulfillment
    instructions, typically executed through a Lambda function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slot**: Intents may require parameters known as slots, such as destination
    or date, with slot types defining the expected values. It prompts users for these
    values and ensures all required information is provided before fulfilling the
    intent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slot Type**: Each slot is associated with a slot type, which can be custom
    or built-in. For instance, sizes may have an enumeration of `Small`, `Medium`,
    and `Large`, while built-in types like `AMAZON.Number` handle numeric inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version**: A version in Amazon Lex V2 represents a snapshot of the bot’s
    configuration. It allows for different versions to be used in various workflow
    stages, like development, beta deployment, or production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alias**: An alias serves as a pointer to a specific version of a bot, enabling
    seamless updates for client applications. By changing the alias to point to a
    new version, all clients receive the updated functionality without requiring individual
    updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To build a bot, you outline the conversation flow in the Amazon Lex V2 console,
    which dynamically manages the dialog and responses. The console supports the building,
    testing, and publishing of text or voice chatbots for integration into platforms
    like mobile devices and web applications. It also integrates with AWS Lambda and
    other AWS services, enhancing connectivity to data in various applications. In
    addition to the console, you can also use bot templates and automated bot designers
    to create bots.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Lex V2 also utilizes the generative AI features of Amazon Bedrock to
    facilitate the development of bots. With Amazon Bedrock, you can create new bots,
    incorporating relevant intents and slot types through natural language descriptions.
    The tool automates the generation of sample utterances tailored to your bot’s
    intents. Additionally, Amazon Bedrock facilitates the creation of specialized
    intents designed to address customer inquiries.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Kendra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon Kendra** is a fully managed intelligent search service. It uses ML
    to understand your natural language requests and perform NLU on the target data
    sources to return the relevant information. Instead of searching for answers using
    keywords such as `IT desk location` and getting a list of documents containing
    these keywords, you can ask natural language questions such as *Where is the IT
    desk?* and get the location of the IT desk, such as *3rd floor, room 301*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use Amazon Kendra to solve several use cases. For example, you can
    use it as part of a contact center workflow where customer agents can quickly
    find the most relevant information for customer requests. You can also use it
    within an enterprise for information discovery across different data sources to
    improve productivity. At a high level, Kendra has the following key functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document reading understanding**: Kendra performs reading comprehension on
    the source document and returns the specific information requested by the user
    in their questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequently asked question (FAQ) matching**: If you provide a list of FAQs,
    Kendra can automatically match the questions to the answers in the list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document ranking**: Kendra can return a list of documents that contain the
    relevant information for the questions asked. To return the list in the order
    of semantic relevancies, Kendra uses ML to understand the semantic meaning of
    the documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand how Kendra works, let’s review some of the key technical Amazon
    Kendra concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Index**: An index provides search results for the documents and FAQ lists
    that it has indexed. Kendra generates indexes for documents and FAQ lists that
    allow them to be searched.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documents**: Documents can be structured (FAQs) and unstructured (HTML, PDFs)
    and can be indexed by the Kendra index engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data sources**: Data sources are locations where the documents are located.
    These can be S3 locations, Amazon RDS databases, and Google Workspace drives,
    among others. Kendra has a list of built-in connectors for connecting to different
    data sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Queries**: Queries are used for getting results from indexes. Queries can
    be natural language containing criteria and filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tags**: Tags are metadata that can be assigned to indexes, data sources,
    and FAQs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two main steps in setting up Kendra to perform an intelligent search
    against your documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate index**: The first step is to set up an index for your documents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Add documents to index**: Once the index has been created, you can add document
    sources to the index to be indexed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the index has been created, you use the Kendra `query()` API to get responses
    for your index with queries. The following code snippet shows the Python syntax
    for querying an index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Kendra has built-in connectors for a range of data sources, so you don’t have
    to build custom code to extract data from those sources. It also has native application
    integration with Amazon Lex, which allows Lex to send user queries directly to
    a Kendra index for fulfillment.
  prefs: []
  type: TYPE_NORMAL
- en: Kendra is being increasingly used along with large language models to provide
    a better user experience and accuracy for intelligent enterprise search solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Q is a generative AI-powered service designed to be an assistant tailored
    for various business needs and developer tasks. There are multiple sub-Q assistants
    for the different domains and services including Q for business, Q for builder,
    Q for QuickSight (an AWS business intelligence tool), and Q for Connect (a contact
    center solution). In this section, we will briefly cover Q for business as it
    is designed to help businesses connect to their own data. Business users, such
    as marketers, project and program managers, and sales representatives, can engage
    in customized conversations, address issues, create content, and execute various
    actions through Amazon Q for business. This platform is cognizant of the specific
    systems these users can access, enabling them to pose intricate and detailed queries.
    The responses they receive are tailored, ensuring that the results incorporate
    only information for which they have authorized access. To learn how Amazon Q
    for business works, check out the documentation at [https://docs.aws.amazon.com/amazonq/latest/business-use-dg/getting-started.html](https://docs.aws.amazon.com/amazonq/latest/business-use-dg/getting-started.html).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating AWS AI services for ML use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To determine if an AI service is a good fit for your use cases, you need to
    evaluate it across multiple dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Functional requirements**: Identify the functional requirements for your
    ML use cases and test whether the target AI services provide the features you
    are looking for. For example, Rekognition is a computer vision service, but it
    does not support all computer vision tasks. If you have an instance segmentation
    computer vision use case, you will have to build a model using an algorithm that
    supports it, such as Mask-RCNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model performance against your data**: AWS AI services are trained with data
    sources to solve common use cases. To ensure the models perform well against your
    data, use your test dataset to evaluate the model metrics for your specific needs.
    If the pre-built models do not meet your performance target, then try the custom
    model building options if the services support it. If neither option works, then
    consider building custom models with your data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API latency and throughput requirements**: Determine your latency and throughput
    requirements for your application and test the target AI service’s API against
    your requirements. In general, AWS AI services are designed for low latency and
    high throughput. However, you might have use cases that require extremely low
    latency, such as computer vision tasks at the edge. If the AI services cannot
    meet your requirements, then consider building models and hosting them in dedicated
    hosting infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and integration requirements**: Determine your security and integration
    requirements and validate whether the AI services meet your requirements. For
    example, you might have custom requirements around authentication and might need
    to develop a custom integration architecture to enable support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model reproducibility requirements**: Since AI services manage the pre-trained
    models and ML algorithms for custom models, those models and algorithms can change
    over time. If you have strict reproducibility requirements, such as training a
    custom model using an old version of an algorithm for compliance reasons, then
    verify if the AI service provides such support before using it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: Understand your usage pattern requirements and evaluate the cost
    of using the AI services. If the cost of developing and hosting a custom model
    is more cost-effective, and the operational overhead does not outweigh the cost
    benefits of a custom model, then consider the build-your-own option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other considerations when it comes to adopting AI services, such as
    monitoring metrics, versioning the control of APIs for audit requirements, and
    data types and volume requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Building intelligent solutions with AI services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI services can be used for building different intelligent solutions. To determine
    if you can use an AI service for your use case, you must identify the business
    and ML requirements and then evaluate if an AI service offers the functional and
    non-functional capabilities you are looking for. In this section, we will present
    several business use cases and architecture patterns that incorporate AI services.
  prefs: []
  type: TYPE_NORMAL
- en: Automating loan document verification and data extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we apply for a loan from a bank, we need to provide the bank with physical
    copies of documentation such as tax returns, pay stubs, bank statements, and photo
    ID. Upon receiving those documents, the bank needs to verify them and enter the
    information from the documents into loan application systems for further processing.
    At the time of writing, many banks still perform this verification and data extraction
    process manually, which is time-consuming and error-prone.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine if you can use any AI services to solve your problem, you need
    to identify the ML problems to be solved. In this particular business workflow,
    we can identify the following ML problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document classification**: Documentation classification is an ML task where
    the documents are classified into different types, such as driver’s license, pay
    stubs, and bank statements. This process identifies the document types and ensures
    the required documents are received and can be further processed based on their
    types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data extraction**: Data extraction is the task of identifying the relevant
    information from the documents and extracting it. Examples of such information
    include customer names and addresses, income information, data of birth details,
    and bank balances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we have learned, these two tasks can be performed by the Comprehend and
    Textract AI services. The following diagram shows the architecture flow that incorporates
    these two services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B20836_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: The loan document verification and data extraction process'
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture, we use a combination of Textract, Comprehend, and Amazon
    Augmented AI services to support loan document classification and the loan data
    processing flow.
  prefs: []
  type: TYPE_NORMAL
- en: Loan document classification workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we need to train a custom text classification model for classifying the
    text that appears in each type of document. Here, we will train a custom classification
    model using Comprehend. The training data for Comprehend’s custom classifier consists
    of the necessary input text and labels. Note that Comprehend has limits on the
    input text size and the maximum number of classes, and this limit can change.
    Check out the official documentation for the latest limitation details. Once the
    model has been trained, you get a private API endpoint for the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the custom model has been trained and deployed, the main flow of the architecture
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data extraction**: Once the documents have been received and digitized as
    images or PDFs, Textract can be used to extract text, tabular data, and form data
    from the documents. The output will be in JSON format and stored as files in S3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Human review**: To ensure the high accuracy of the extracted data by Textract,
    a human-in-the-loop process can be implemented to verify low-confidence predictions
    and manually correct them. This human-in-the-loop workflow can be implemented
    using the Amazon Augmented AI service.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document classification**: The JSON outputs are processed to generate classification
    prediction using the custom Comprehend model that has been trained.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Update downstream systems**: The prediction outputs are passed to downstream
    systems for further processing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are alternative architecture options available. For example, you can also
    treat documents as images and perform image classification using the Rekognition
    service. Another option is to train a custom model using your algorithms, such
    as LayoutLM, and prepare a training dataset with the output of Textract. It is
    prudent to validate multiple options to achieve the optimal price/performance
    trade-off when deciding on the right technology.
  prefs: []
  type: TYPE_NORMAL
- en: Loan data processing flow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The loan data processing flow is concerned with processing the JSON outputs
    from the data extraction process. The JSON document contains raw text and structure
    details for the entire document, and only a subset of text is needed for downstream
    processing and storage. The processing scripts can parse the documents using the
    structures in the JSON file to identify and extract the specific data points required.
    Then, it can input those data points into the downstream databases or systems.
  prefs: []
  type: TYPE_NORMAL
- en: Media processing and analysis workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The media and entertainment industry has accumulated a huge number of digital
    media assets over the years, and the growth of these new digital assets is accelerating.
    One key capability in digital asset management is search and discovery. This capability
    not only impacts the user experience but also the effective monetization of media
    content. To quickly surface the most relevant content, media companies need to
    enrich the content with metadata for indexing and searching.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this particular business challenge, we can identify the following ML problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speech-to-text transcription**: The audio portion of videos and audio files
    need to be transcribed into text transcripts. The transcripts can then be further
    analyzed for additional information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text NLP analysis**: NLP analysis such as entity extraction, sentiment analysis,
    and topic modeling can be performed on the transcripts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object/people/scene/activity detection**: Compute vision tasks can be performed
    on video frames and images to extract objects, people, scenes, and activities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows an architecture that uses Transcribe, Comprehend,
    and Rekognition to perform the identified ML tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, diagram  Description automatically generated](img/B20836_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: Media tagging and analysis architecture'
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture, we build a pipeline for subtitle and text analysis of
    video content, video tagging and analysis, and image tagging and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: For live video sources such as broadcasting, the AWS Elemental services can
    take live broadcasting streams, process them, and store them in S3\. You can find
    more details about the Elemental services at [https://aws.amazon.com/elemental-live/](https://aws.amazon.com/elemental-live/).
    Images and video file data sources can be ingested into S3 using a variety of
    different capabilities, including S3 APIs or higher-level services such as AWS
    Transfer for **Secure File Transfer Protocol** (**SFTP**).
  prefs: []
  type: TYPE_NORMAL
- en: 'As there are multiple parallel processing streams in the pipeline, we can use
    AWS Step Functions to orchestrate the parallel execution of different streams.
    These can generate the following output streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Subtitle and text analysis stream**: This stream primarily uses the Amazon
    Transcribe and Amazon Comprehend AI services. Transcribe transcribes the audio
    portion of the videos and generates both subtitle files and regular transcripts.
    The regular transcripts are then used by Comprehend to run text analysis. Some
    example metadata that’s extracted from this stream can include the entities of
    people and places, the language used, and sentiment for different sections of
    the transcripts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video tagging and analysis stream**: This stream identifies objects, scenes,
    activities, people, celebrities, and text with timestamps in the different video
    frames.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image tagging and analysis stream**: This stream identifies objects, scenes,
    activities, celebrities, and text in different images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs from the media processing streams can be further processed and organized
    as useful metadata for the different media assets. Once this has been done, they
    are stored in a media metadata repository to support content search and discovery.
  prefs: []
  type: TYPE_NORMAL
- en: E-commerce product recommendation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Product recommendation is an important capability in e-commerce. It is a key
    enabler for increasing sales, improving engagement experience, and retaining customer
    loyalty.
  prefs: []
  type: TYPE_NORMAL
- en: 'In e-commerce product recommendation, multiple functional requirements can
    be framed as ML problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recommendations based on customer behaviors and profiles**: ML algorithms
    can learn the intrinsic characteristics and purchasing patterns of customers from
    their past e-commerce interactions to predict the products they will like.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ability to address recommendations of cold items (items without history)**:
    ML algorithms can explore customers’ reactions toward cold items and adjust their
    recommendations to balance explore (recommending new items) and exploit (recommending
    known items).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ability to recommend similar items**: ML algorithms can learn the intrinsic
    characteristics of products based on product attributes and collective interaction
    patterns from a group of customers to determine product similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these functional requirements in mind, the following architecture diagram
    illustrates an e-commerce architecture that uses Amazon Personalize as the recommendation
    engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – e-commerce site and recommendation architecture ](img/B20836_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: E-commerce site and recommendation architecture'
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture, we use Personalize as the recommendation engine to power
    both the online user experience as well as the target user marketing experience.
  prefs: []
  type: TYPE_NORMAL
- en: The RDS database, DynamoDB, and Elasticsearch are the main data sources for
    item, user, and interaction data. Glue ETL jobs are used to transform the source
    data into the datasets required for Personalize solution building.
  prefs: []
  type: TYPE_NORMAL
- en: Once a Personalize solution has been evaluated to meet the desired criteria,
    it is deployed as a Personalize campaign to serve recommendation requests from
    customers visiting the e-commerce website.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Pinpoint is a managed target marketing service. You can use Pinpoint
    to manage user segmentation and send email and SMS marketing campaigns. In this
    architecture, the Pinpoint service gets a list of recommended products for a group
    of target customers and sends out email or SMS campaigns to those users with personalized
    recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Customer self-service automation with intelligent search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Good customer service boosts customer satisfaction and builds long-term customer
    loyalty. However, customer support is very labor-intensive and can result in poor
    customer satisfaction due to long waiting times and unknowledgeable support agents.
    The customer self-service capability has been widely adopted by organizations
    in different industries to deflect customer support call volumes and improve customer
    satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a customer self-service scenario, we can identify the following ML problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automatic speech recognition** (**ASR**): This ML task recognizes human speech
    and converts it into text, and then uses NLU to understand the meaning of the
    text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural language understanding** (**NLU**): NLU is a subfield of NLP, and
    it deals with intent understanding and reading comprehension. NLU focuses on the
    meaning and intent of the text. For example, if the text is *Can I get the cash
    balance in my savings account?*, then the intent here is *get account balance*.
    Another example of NLU is understanding the text and extracting specific information
    from it based on the semantic meaning of the question and the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text to speech**: This ML task converts text into natural human voices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows a sample architecture for implementing a self-service
    chat functionality for customers to look up customer-related details, as well
    as general information and FAQs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Self-service chat portal with an intelligent virtual assistant
    ](img/B20836_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Self-service chat portal with an intelligent virtual assistant'
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture, an Amazon Lex bot is used to provide the text-based conversational
    interface for customer engagement. The customer uses the self-service chat portal
    to initiate the conversation and the chat portal integrates with the Lex bots
    via the Lex API.
  prefs: []
  type: TYPE_NORMAL
- en: Lex bots support several different intents, such as *look up account info*,
    *update customer profile*, and *How do I return a purchase?*.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the intent, the Lex bot will route the fulfillment requests to
    a different backend. For customer account-related inquiries, it will use a Lambda
    function for fulfillment. For information search-related questions, the Lex bot
    will send the query to a Kendra index for fulfillment.
  prefs: []
  type: TYPE_NORMAL
- en: Having explored the various AI services and their practical business applications,
    the subsequent sections will focus on operational considerations related to the
    adoption of these services. This includes delving into MLOps, code promotion,
    and monitoring processes to enhance the operational efficiency of AI implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Designing an MLOps architecture for AI services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing custom AI service models requires a data engineering, model training,
    and model deployment pipeline. This process is similar to the process of building,
    training, and deploying models using an ML platform. As such, we can also adopt
    MLOps practice for AI services when running them at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, MLOps for AI services intends to deliver similar benefits as
    MLOps for the ML platform, including process consistency, tooling reusability,
    reproducibility, delivery scalability, and auditability. Architecturally, we can
    implement a similar MLOps pattern for AI services.
  prefs: []
  type: TYPE_NORMAL
- en: AWS account setup strategy for AI services and MLOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To isolate the different environments, we can adopt a multi-account strategy
    for configuring the MLOps environment for AI services. The following diagram illustrates
    a design pattern for a multi-account AWS environment. Depending on your organizational
    requirements for separation of duties and control, you may also consider consolidating
    these into fewer environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – MLOps architecture for AI services on AWS  ](img/B20836_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: MLOps architecture for AI services on AWS'
  prefs: []
  type: TYPE_NORMAL
- en: In this multi-account AWS environment, developers use the custom model development
    environment to build and test the pipelines for data engineering, model training,
    and model deployment. When ready, the pipelines are promoted for formal model
    building and testing using production training data in the model development environment.
    Since trained AI services models cannot normally be exported, we will need to
    replicate the model training workflow in the production environment for model
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The shared services environment hosts CI/CD tools such as AWS CodePipeline
    and AWS CodeBuild. You use the CI/CD tools to build different pipelines for data
    engineering, model building, and model deployment running in different environments.
    For example, a pipeline for the UAT environment could have the following components
    and steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CodePipeline definition**: This definition would have a CodeBuild step, a
    CloudFormation execution step, and a Step Functions workflow execution step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CodeBuild step**: The CodeBuild step enriches the CloudFormation template
    with additional inputs needed to create a Step Functions workflow that orchestrates
    data engineering, dataset creation, data ingestion, model training, and model
    deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CloudFormation execution step**: This step executes the CloudFormation template
    to create the Step Functions workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step Functions workflow execution step**: This step kicks off the Step Functions
    workflow to run the various steps, such as data engineering and model training,
    in the workflow. For example, if we build a Step Functions workflow for Personalize
    model training and deployment, the workflow will consist of six steps: create
    dataset group, create dataset, import dataset, create solution, create solution
    version, and create campaign.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a multi-account environment, there could also be other purpose-built accounts
    for data management, monitoring, and security.
  prefs: []
  type: TYPE_NORMAL
- en: Code promotion across environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the pattern we use for the ML platform, we can use a code repository
    as the mechanism to promote code to different environments. For example, during
    code development, a developer creates code artifacts such as data engineering
    scripts for Glue ETL jobs and CloudFormation template skeletons and builds specification
    files for CodeBuild to run different commands. Once the code is deemed ready to
    promote them for formal model building and testing, the developer checks the code
    into a release branch in the code repository. The code check-in event can trigger
    a CodePipeline job to run the CodeBuild step in the shared services and then run
    a Step Functions workflow step in the model development environment. When it is
    ready for production release, a deployment CodePipeline job can be triggered in
    the shared services environment to execute a CloudFormation template to deploy
    the model in the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring operational metrics for AI services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AI services emit operational statuses to CloudWatch. For example, Amazon Personalize
    sends metrics such as the number of successful recommendation calls or training
    job errors. Rekognition sends metrics such as successful request counts and response
    time. Alarms can be configured to send alerts when specified metrics meet a defined
    threshold. The following diagram shows a sample monitoring architecture for Amazon
    Personalize:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Monitoring architecture for Amazon Personalize ](img/B20836_11_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: Monitoring architecture for Amazon Personalize'
  prefs: []
  type: TYPE_NORMAL
- en: With this monitoring architecture, CloudWatch collects metrics from the Personalize
    service. A scheduled CloudWatch event triggers a Lambda function, which pulls
    a set of CloudWatch metrics and sends events to the EventBridge service. EventBridge
    rules can be configured to trigger Lambda functions to update Personalize configuration,
    such as updating `minProvisionedTPS` configuration for Personalize when throttling
    is detected or sending an email notification when certain errors occur.
  prefs: []
  type: TYPE_NORMAL
- en: You can also adopt similar monitoring architecture patterns to other AI services,
    such as Comprehend and Rekognition.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on lab – running ML tasks using AI services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this hands-on lab, you will perform a list of ML tasks using Rekognition,
    Comprehend, Textract, Personalize and Transcribe. After the lab, you will have
    developed hands-on experience with the core features of several AI services and
    how they can be used for various ML tasks. Follow these steps to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch the SageMaker Studio profile you created in *Chapter 8*, *Building a
    Data Science Environment Using AWS ML Services*. You will create and run new notebooks
    in this profile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We need to provide the new notebooks with permission to access AI services.
    To do this, find the Studio execution role for the Studio environment and attach
    the `AdministratorAccess` IAM policy to it. We will use this policy for simplicity
    here. In a controlled environment, you would need to design a policy to provide
    the specific permissions needed to access different services.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clone [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/)
    into your Studio environment using the `git clone https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/`
    command if you have not already done so.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run NLP tasks using Comprehend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `comprehend.ipynb` notebook in the `Chapter11` directory. This notebook
    performs a list of ML tasks using Comprehend, including language detection, entity
    detection, sentiment detection, PII detection, key phrase detection, and syntax
    analysis.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create some sample text you would like to run NLP analysis on and save it as
    `comprehend_sample.txt` in the data directory.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following code in the notebook to import the library and set up the
    `boto3` client for Comprehend:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to detect the dominant language in the
    text:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to detect entities:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to detect sentiment:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to detect PII entities:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to detect key phrases:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to detect syntax:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run an audio transcription job using Transcribe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `transcribe.ipynb` notebook in the `Chapter11` directory. This notebook
    runs a transcription job using a sample audio file in the data directory.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Find a sample MP3 audio file that you would like to run transcription on and
    save it as `transcribe_sample.mp3` in the data directory.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following code in the notebook to set up a `boto3` client for Transcribe:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to create an S3 bucket for storing the
    audio file:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to kick off the transcription job:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Navigate to the **Transcribe** console. Under the **Transcription Jobs** section,
    you will see the newly created transcription job.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait until the status changes to **Complete** and click on the job link; you
    will see the transcripts under the **Text** tab in the **transcription preview**
    section.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run computer vision with Rekognition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `rekognition.ipynb` notebook in the `Chapter11` directory. This notebook
    runs a list of text extraction tasks, including text extraction, table extraction,
    and form extraction.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Save a sample image for analysis as `textract_sample.jpeg` in the `data` directory.
    Try to use a sample image with text, tables, and forms in it.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following code in the notebook to set up a `boto3` client for Textract:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to load the image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to detect tables and forms:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code in the notebook to detect text:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Train a recommendation model using Personalize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `personalize.ipynb` notebook in the `Chapter11` directory. This notebook
    trains a Personalize model for movie review recommendations using the movie lens
    dataset. It goes through the process of creating a dataset group/dataset, importing
    the data, building the solution, and creating a Personalize campaign.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Follow the instructions in the notebook and run all the cells in sequence to
    complete all the steps.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! You have successfully used several AWS AI services and their
    APIs. As you can see, it is quite straightforward to use AI services with pre-trained
    models to perform different ML tasks. Training a custom model using AI services
    involves some additional steps, but the underlying infrastructure and data science
    details are abstracted away to make it easy for non-data-scientists to use these
    services as well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we covered topics surrounding AI services. We went over a list
    of AWS AI services and where they can be used to build ML solutions. We also talked
    about adopting MLOps for AI services deployment. Now, you should have a good understanding
    of what AI services are and know that you don’t need to always build custom models
    to solve ML problems. AI services provide you with a quick way to build AI-enabled
    applications when they are a good fit.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deep into AI risk management, an important
    area for ML practitioners to become familiar with as it is critical to understand
    the key risks and mitigation approaches throughout the entire ML lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code70205728346636561.png)'
  prefs: []
  type: TYPE_IMG
