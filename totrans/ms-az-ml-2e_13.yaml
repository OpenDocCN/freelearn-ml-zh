- en: '*Chapter 10*: Training Deep Neural Networks on Azure'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to train and score classical ML models
    using non-parametric tree-based ensemble methods. While these methods work well
    on many small- and medium-sized datasets that contain categorical variables, they
    don't generalize well on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will train complex parametric models using **deep learning**
    (**DL**) for even better generalization with very large datasets. This will help
    you understand **deep neural networks** (**DNNs**), how to train and use them,
    and when they perform better than traditional models.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will provide a short and practical overview of why and when DL works
    well and focus on understanding the general principles and rationale rather than
    the theoretical approach. This will help you to assess which use cases and datasets
    need DL and how it works in general.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will look at one of the popular application domains for DL – computer
    vision. We will train a simple **convolutional neural network** (**CNN**) model
    for image classification using the Azure Machine Learning service and additional
    Azure infrastructure. We will compare the performance to a model that has been
    fine-tuned on a pre-trained **residual neural network** (**ResNet**) model. This
    will set you up to train your models from scratch, fine-tune existing models for
    your application domain, and overcome situations where not enough training data
    is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Deep Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a CNN for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the following Python libraries and versions to
    create decision tree-based ensemble classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`azureml-core 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`azureml-sdk 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy 1.19.5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas 1.3.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn 0.24.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to the previous chapters, you can execute this code using either a local
    Python interpreter or a notebook environment hosted in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the code examples in this chapter can be found in this book''s GitHub repository:
    [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter10](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter10).'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has revolutionized the ML domain recently and is constantly outperforming
    classical statistical approaches, and even humans, in various tasks such as image
    classification, object detection, segmentation, speech transcription, text translation,
    text understanding, sales forecasting, and much more. In contrast to classical
    models, DL models use many millions of parameters, parameter sharing, optimization
    techniques, and implicit feature extraction to outperform all previously hand-crafted
    feature detectors and ML models when trained with enough data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will help you understand the basics of neural networks and
    the path to training deeper models with more parameters, better generalization,
    and hence better performance. This will help you understand how DL-based approaches
    work, as well as why and when they make sense for certain domains and datasets.
    If you are already an expert in DL, feel free to skip this section and go directly
    to the practical examples in the *Training a CNN for image classification* section.
  prefs: []
  type: TYPE_NORMAL
- en: Why Deep Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many traditional optimization, classification, and forecasting processes have
    worked well over the past decades using classical ML approaches, such as k-nearest
    neighbor, linear and logistic regression, naïve Bayes, **support vector machines**
    (**SVMs**), tree-based ensemble models, and others. They worked well on various
    types of data (transactional, time series, operational, and so on) and data types
    (binary, numerical, and categorical) for small- to mid-sized datasets.
  prefs: []
  type: TYPE_NORMAL
- en: However, in some domains, data generation has exploded, and classical ML models
    couldn't achieve better performance even with an increasing amount of training
    data. This especially affected the domains of computer vision and NLP around late
    2010\. That's when researchers had a breakthrough with neural networks – also
    called **multilayer perceptrons** (**MLPs**) – a technique that was used in the
    late 80s to capture the vast number of features in a large image dataset by using
    multiple nested layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following chart captures this idea very well. While traditional ML approaches
    work very well on small- and medium-sized datasets, their performance usually
    does not improve with more training data. However, DL models are massive parametric
    models that can capture a vast number of details from training data. Hence, we
    can see that their prediction performance increases as the amount of data increases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – The effectiveness of DL versus traditional ML ](img/B17928_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – The effectiveness of DL versus traditional ML
  prefs: []
  type: TYPE_NORMAL
- en: Traditional models often use pre-engineered features and are optimized for datasets
    of various data types and ranges. In the previous chapter, we saw that gradient-boosted
    trees perform extremely well on categorical data. However, in domains that contain
    highly structured data or data of variable lengths, many traditional models reach
    their limits. This is especially true for pixel information in two- and three-dimensional
    images and videos, as well as waveforms in audio data and characters and character
    sequences in free-text data. ML models used to process such data using complex
    manually tuned feature extractors, such as **histogram of oriented gradients**
    (**HoG**) filters, **scale-invariant feature transform** (**SIFT**) features,
    or **local binary patterns** (**LBPs**) – just to name a few filters in the computer
    vision domain.
  prefs: []
  type: TYPE_NORMAL
- en: What makes this data so complicated is that no obvious linear relationship between
    the input data (for example, a single pixel) and the output exists – in most cases,
    seeing a single pixel of an image won't help determine the brand of a car in that
    image. Therefore, there was an increasing need to train larger and more capable
    parametric models that used raw, unprocessed data as input to capture these relationships
    from the input pixel to make a final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to understand that the need for deeper models with many more
    parameters comes from the vastly increasing amount of highly structured training
    data in specific domains, such as vision, audio, and language. These new models
    often have millions of parameters to capture the massive amounts of raw and augmented
    training data, as well as developing an internal generalized conceptual representation
    of the training data. Keep this in mind when choosing an ML approach for your
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: A quick look at your training data often helps to determine whether a DL-based
    model is suitable for the task – given that DL models have millions of parameters
    to train. If your data is stored in a SQL database or CSV or Excel files, then
    you should probably look into classical ML approaches, such as parametric statistical
    (linear regression, SVM, and so on) or non-parametric (decision tree-based ensembles)
    approaches. If your data is so big that it doesn't fit into memory or is stored
    in a **Hadoop Distributed File System** (**HDFS**), blob storage, or a file storage
    server, then you could use a DL-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: From neural networks to deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The foundation of neural networks and hence today's DL-based approaches – the
    **perceptron** – is a concept that is over half a century old and was developed
    in the 1950s. In this section, we will take a look at the basics, and work our
    way back to **MLPs** – also called **artificial neural networks** (**ANNs**) –
    and **CNNs** in the 1980s, and then to **DNNs** and DL in the last decade. This
    will help you understand the foundational concepts of neural networks and hence
    DL, as well as how model architectures and training techniques have evolved over
    the last century into the state-of-the-art techniques we are using today.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron – a classifier from the 50s
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perceptrons are the foundational building blocks of today''s neural networks
    and are modeled on cells in the human brain (so-called **neurons**). They are
    simple non-linear functions consisting of two components: a weighted sum of all
    the inputs and an activation function that fires if the output is larger than
    the specified threshold. While this analogy of a neuron is a great way to model
    how a brain works, it is a poor model to understand how the input signal is transformed
    into its output.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than neurons in the brain, we prefer a much simpler, non-biological approach
    to explain the perceptron, MLPs, and CNNs – namely, a simple geometric approach.
    When simplified, this method requires you to only understand the two-dimensional
    line equation. Once you understand the basics in two dimensions, the concept can
    be extended to multiple dimensions, where the line becomes a plane or hyperplane
    in a higher-dimensional feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at a single perceptron, it describes a **weighted sum** of its inputs
    plus constant bias with an **activation function**. Let''s break down the two
    components of the perceptron. Do you know what is also described as a weighted
    sum of its inputs plus bias? Right, the line equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10.1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, *x* is the input, *k* is the weight, and *b* is
    the bias term. You have probably seen this equation at some point in your math
    curriculum. A property of this equation is that when you''re inserting a point''s
    *x* and *y* coordinates into the line equation, it yields 0 = 0 for all the points
    that lie on the line. We can use this information to derive the vector form of
    the line equation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10.2.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, ![](img/Formula_10.3.png) is *0* when the point lies on the line. What
    happens if we insert the coordinates of a point that does not lie on the line?
    A good guess is that the result will be either positive or negative but certainly
    not 0\. A property of the vector line equation is that the sign of this result
    describes which side of the line the point lies on. Hence, the point lies either
    on the left or the right-hand side of the line when ![](img/Formula_10.4.png)
    is positive or negative but not null.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine the side of the line, we can apply the sign function to ![](img/Formula_10.5.png).
    The sign function is often also referred to as the step function, as its output
    is either *1* or *-1*, hence positive or negative. The sign or step function here
    is our activation function and hence the second component of the perceptron. The
    output of the perceptron, ![](img/Formula_10.6.png), can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10.7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following chart, we can see two points, a line, and their shortest distance
    to the line. Both points are not lying on the line, so the line separates both
    points from each other. If we insert both points'' coordinates into the vector
    line equation, then one point would result in a positive value ![](img/Formula_10.8.png),
    whereas the other point would result in a negative value ![](img/Formula_10.9.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – A simple binary classifier ](img/B17928_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – A simple binary classifier
  prefs: []
  type: TYPE_NORMAL
- en: The result would tell us which side of the line the point lies on. This line
    is a geometric description of the perceptron, which is a very simple classifier.
    The trained perceptron is defined through the line equation (or a hyperplane in
    multiple dimensions), which separates a space into left and right. This line is
    the decision boundary for a classification, and a point is an observation. By
    inserting a point into the line equation and applying the step function, we return
    the resulting class of the observation, which is left or right, -1 or +1, or class
    A or B. This describes a binary classifier.
  prefs: []
  type: TYPE_NORMAL
- en: And how do we find the decision boundary? To find the optimal decision boundary,
    we can follow an iterative training process while using labeled training samples.
    First, we must initialize a random decision boundary, then compute the distance
    from each sample to the decision boundary and move the decision boundary into
    the direction that minimizes the total sum of distances. The optimal vector to
    move the decision boundary is if we move it along the negative gradient, such
    that the distance between the point and the line reaches a minimum. By using a
    learning rate factor, we iterate this process a few times and end up with a perfectly
    aligned decision boundary, if the training samples are linearly separable. This
    process is called **gradient descent**, where we iteratively modify the classifier
    weights (decision boundaries, in this example) to find the optimal boundary with
    minimal error.
  prefs: []
  type: TYPE_NORMAL
- en: The multilayer perceptron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A perceptron describes a simple classifier whose decision boundary is a line
    (or hyperplane) that''s been defined through the weighted inputs. However, instead
    of using a single classifier, we can simply increase the number of neurons, which
    will result in multiple decision boundaries, as shown in the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Combining multiple perceptrons ](img/B17928_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Combining multiple perceptrons
  prefs: []
  type: TYPE_NORMAL
- en: Each neuron describes a decision boundary and hence will have separate weights
    and a separate output – left or right of the decision boundary. By stacking multiple
    neurons in layers, we can create classifiers whose inputs are the output of the
    previous ones. This allows us to combine the results from multiple decision boundaries
    into a single output – for example, finding all the samples that are enclosed
    by the decision boundaries of three neurons, as shown in the preceding chart.
  prefs: []
  type: TYPE_NORMAL
- en: While a single layer of perceptrons describes a linear combination of inputs
    and outputs, researchers began to stack these perceptrons into multiple sequential
    layers, where each layer was followed by an activation function. This is called
    MLP, or an ANN. Using the geometric model as an analogy, you could simply stack
    multiple decision boundaries on complex geometric objects to create more complex
    decision boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Another analogy is that the classifier's decision boundary is always a straight
    hyperplane, but the input samples are transformed to be linearly separated through
    the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: The same geometric analogy helps us understand the layers in DL models. While
    the first layers of a network describe very low-level geometric features, such
    as straight edges and lines, the higher levels describe complicated nested combinations
    of these low-level features; for example, four lines build a square, five squares
    build a more complex shape, and a combination of those shapes looks like a human
    face. We just built a face detector using a three-layer neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Google DeepDream experiment is a fantastic example of this analogy. In
    the following figure, we can visualize how three layers of different depths in
    a pre-trained DNN represent features in an image of a cloudy sky. The layers are
    extracted from the beginning, middle, and end of a DNN and transform the input
    image to minimize the loss of each layer. Here, we can see how the earlier layer
    focuses mostly on lines and edges (left), whereas the middle layer sees abstract
    shapes (middle), and the last layer activates on very specific high-level features
    in the image (right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – DeepDream – minimizing loss for the layers of a DNN ](img/B17928_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – DeepDream – minimizing loss for the layers of a DNN
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using multiple high-dimensional hyperplane equations, where each output feeds
    into each input of the following layer, requires a very large number of parameters.
    While a high number of parameters is required to model a massive amount of complex
    training data, a so-called fully connected neural network is not the best way
    to describe these connections. So, what's the problem?
  prefs: []
  type: TYPE_NORMAL
- en: In a fully connected network, each output is fed to each neuron of the consecutive
    layer as input. In each neuron, we require a weight for each input, so we need
    as many weights as there are input dimensions. This number quickly explodes when
    we start stacking multiple layers of perceptrons. Another problem is that the
    network cannot generalize because it learns all the individual weights separately
    for each dimension.
  prefs: []
  type: TYPE_NORMAL
- en: In the 1980s, CNNs were invented to solve these problems. Their purpose was
    to reduce the number of connections and parameters on a single layer to a fixed
    set of parameters, independent of the number of input dimensions. The parameters
    of a layer are now shared within all the inputs. The idea of this approach comes
    from signal processing, where filters are applied to a signal through a convolution
    operation. Convolution means applying a single set of weights, such as a window
    function, to multiple regions of the input and later summing up all the signal
    responses of the filter for each location.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was the same idea for the convolution layers of CNNs. By using a fixed-sized
    filter that is convolved with the input, we can greatly reduce the number of parameters
    for each layer and add more nested layers to the network. By using a so-called
    pooling layer, we can also reduce the image size and apply filters to a downscaled
    version of the input. Let''s take a look at the popular layers that are used for
    building CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully connected (FC)**: The FC layer is a layer of fully connected neurons,
    as described in the previous section about perceptrons – it connects every output
    from the previous layer with a neuron. In DNN, FC layers are often used at the
    end of the network to combine all the spatially distributed activations of the
    previous convolution layers. The FC layers also have the largest number of parameters
    in a model (usually around 90%).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolution**: A convolution layer consists of spatial (often two-dimensional)
    filters that are convolved along the spatial dimensions and summed up along the
    depth dimension of the input. Due to weight sharing, they are much more efficient
    than fully connected layers and have a lot fewer parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pooling**: Convolution layers are often followed by a pooling layer to reduce
    the spatial dimension of the volume for the next filter – this is the equivalent
    of a subsampling operation. The pooling operation itself has no learnable parameters.
    Most of the time, **max pooling** layers are used in DL models due to their simple
    gradient computation. Another popular choice is **avg pooling**, which is mostly
    used as a classifier at the end of a network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalization**: In modern DNNs, normalization layers are often used to stabilize
    gradients throughout the network. Due to the unbounded behavior of some activation
    functions, filter responses have to be normalized. A commonly used normalization
    technique is **batch normalization**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the main components of CNNs, we can look into how these
    models were stacked even deeper to improve generalization and hence improve the
    prediction's performance.
  prefs: []
  type: TYPE_NORMAL
- en: From CNNs to DL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The perceptron from the 50s, as well as ANNs and CNNs from the 80s, build the
    foundation for all the DL models that are used today. By stabilizing the gradients
    during the training process, researchers could overcome the exploding and vanishing
    gradients problem and build deeper models. This was achieved by using additional
    normalization layers, rectified linear activation, auxiliary losses, and residual
    connections.
  prefs: []
  type: TYPE_NORMAL
- en: Deeper models have more learnable parameters – often well over 100 million parameters
    – so they can find higher-level patterns and learn more complex transformations.
    However, to train deeper models, you must also use more training data. Therefore,
    companies and researchers built massive labeled datasets (such as ImageNet) to
    feed these models with training data.
  prefs: []
  type: TYPE_NORMAL
- en: This development process was facilitated by the availability of cheap parallelizable
    compute in the form of GPUs and cloud computing. Training these deep models quickly
    went from months to days to hours within a couple of years. Today, we can train
    a typical DNN in under an hour with a highly parallelized compute infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of research also went into new techniques for stacking layers, from very
    deep networks with skip connections, as in ResNet152, to networks with parallel
    layer groups, as in GoogLeNet. A combination of both layer types led to extremely
    efficient network architectures such as SqueezeNet and Inception. New layer types
    such as LSTM, GRU, and attention enabled significantly better prediction performance,
    while the GAN and transform models created entirely new ways to train and optimize
    models.
  prefs: []
  type: TYPE_NORMAL
- en: All these advances helped make DL what it has become today – a ubiquitous ML
    technique that, given enough training data, can outperform traditional ML models
    and often even humans in most prediction tasks. Today, DL is applied to almost
    any domain where there is sufficient data at hand.
  prefs: []
  type: TYPE_NORMAL
- en: DL versus traditional ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at the main differences between classical ML- and DL-based approaches
    and find out what DL models can do with so many more parameters and how they benefit
    from them.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the image or audio processing domain before 2012, we will see
    that ML models were not usually trained on the raw data itself. Instead, the raw
    data went through a manually crafted feature extractor and converted into a lower-dimensional
    feature space. When dealing with images of 256 x 256 x 3 dimensions (RGB) – which
    corresponds to a 196,608-dimensional feature space – and converting these into,
    say, a 2,048-dimensional feature embedding as input for the ML models, we greatly
    reduce the computational requirements for these models. The feature extractors
    for image and audio features often use a convolution operator and a specific filter
    (such as an edge detector, blob detector, spike/dip detector, and so on). However,
    the filter is usually constructed manually.
  prefs: []
  type: TYPE_NORMAL
- en: The classical ML models that have been developed in the past 50+ years are still
    the ones we are successfully using today. Among those are tree-based ensemble
    techniques, linear and logistic regression, SVMs, and MLPs. The MLP model is also
    known as a fully connected neural network with hidden layers and still serves
    as a classification or regression head in some of the early DL architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the typical pipeline of a classical ML approach
    in the computer vision domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Traditional ML classifier ](img/B17928_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Traditional ML classifier
  prefs: []
  type: TYPE_NORMAL
- en: First, the raw data is converted into a lower-dimensional feature embedding
    using hand-crafted image filters (SIFT, SURF, HoG, LBPs, Haar filters, and so
    on). Then, feature embedding is used to train an ML model; for example, a multi-layer,
    fully connected neural network or decision-tree classifier, as shown in the preceding
    diagram.
  prefs: []
  type: TYPE_NORMAL
- en: When it is difficult for a human being to express a relationship between an
    input image and an output label in simple rules, then it is most likely also difficult
    for a classical computer vision and ML approach to find such rules. DL-based approaches
    perform a lot better in these cases. The reason for this is that DL models are
    trained on raw input data instead of manually extracted features. Since convolution
    layers are the same as randomized and trained image filters, these filters for
    feature extraction are implicitly learned by the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a DL approach to image classification, which is
    similar to the previous diagram for the classical ML approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – DL-based classifier ](img/B17928_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – DL-based classifier
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the raw input data of the image is fed directly to the network,
    which outputs the final image label. This is why we often refer to a DL model
    as an end-to-end model – because it creates an end-to-end transformation between
    the input data (literally, the raw pixel values) and the model's output.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, the DL-based model is an end-to-end model
    that learns both the feature extractor and the classifier in a single model. However,
    we often refer to the last fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Look at the type of data before choosing your ML model. If you are dealing with
    images, video, audio, time series, language, or text, you may wish to use a DL
    model or feature extractor for embedding, clustering, classification, or regression.
    If you are working with operational or business data, then a classic ML approach
    would be a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: Using traditional ML with DL-based feature extractors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many cases, especially when you have small datasets, not enough compute resources,
    or knowledge to train end-to-end DL models, you can also reuse a pre-trained DL
    model as a feature extractor. This can be done by loading a pre-trained model
    and performing a forward pass until the classification/regression head. It returns
    a multi-dimensional embedding (a so-called latent space representation) that you
    can directly plug into a classical ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of such a hybrid approach. We are using the `IncpetionV3`
    model as a feature extractor, pre-trained on the `imagenet` data. The DL model
    is only used to transform the raw input image data into a lower-dimensional feature
    representation. Then, an SVM model is trained on top of the image features. Let''s
    look at the source code for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we used TensorFlow to load the `InceptionV3` model with
    the ImageNet-based weights but without any classification or regression head.
    This is achieved by setting the `include_top` property to `False`. Then, we squeezed
    the output of the prediction – the image's latent representation – into a single
    vector. Finally, we trained an SVM on the image features using scikit-learn and
    a default train/test split.
  prefs: []
  type: TYPE_NORMAL
- en: We started with the classical approach, where feature extraction and ML were
    separated into two steps. However, the filters in the classical approach were
    hand-crafted and applied directly to the raw input data. In a DL approach, we
    implicitly learn the feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN for image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have a good understanding of why and when to use DL models, we
    can start to implement one and run it using Azure Machine Learning. We will start
    with a task that DL performed very well with over the past years – computer vision,
    or more precisely, image classification. If you feel that this is too easy for
    you, you can replace the actual training script with any other computer vision
    technique and follow along with the steps in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will power up an Azure Machine Learning compute instance, which will
    serve as our Jupyter Notebook authoring environment. First, we will write a training
    script and execute it in the authoring environment to verify that it works properly,
    checkpoints the model, and logs the training and validation metrics. We will train
    the model for a few epochs to validate the setup, the code, and the resulting
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will try to improve the algorithm by adding data augmentation to the
    training script. While this seems like an easy task, I want to reiterate that
    this is necessary and strongly recommended for any DL-based ML approach. Image
    data can easily be augmented to improve generalization and therefore model scoring
    performance. However, through this technique, training the model will take even
    longer than before because more training data is being used for each epoch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we must move the training script from the authoring environment to a GPU
    cluster – a remote compute environment. We will do all this – upload the data,
    generate the training scripts, create the cluster, execute the training script
    on the cluster, and retrieve the trained model – from within the authoring environment
    in the Azure Machine Learning service. If you are already training ML models yourself
    on your server, then this section will show you how to move your training scripts
    to a remote execution environment and how to benefit from dynamically scalable
    compute (both vertically and horizontally, hence larger and more machines), auto-scaling,
    cheap data storage, and much more.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have successfully trained a CNN from scratch, you will want to move
    on to the next level in terms of model performance and complexity. A good and
    recommended approach is to fine-tune pre-trained DL models rather than train them
    from scratch. Using this approach, we can often also use a pre-trained model from
    a specific task, drop the classification head (usually the last one or two layers)
    from the model, and reuse the feature extractor for another task by training our
    classification head on top of it. This is called transfer learning and is widely
    used for training state-of-the-art models for various domains.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's open a Jupyter notebook and start training a CNN image classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN from scratch in your notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s train a CNN on Jupyter on the Azure Machine Learning service. First,
    we want to simply train a model in the current authoring environment, which means
    we must use the compute (CPU and memory) from the compute instance. This is a
    standard Python/Jupyter environment, so it is no different from training an ML
    model on your local machine. So, let''s go ahead and create a new compute instance
    in our Azure Machine Learning service workspace, and then open the Jupyter environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin creating our CNN model, we need some training data. As we train
    the ML model on the authoring computer, the data needs to be on the same machine.
    For this example, we will use the MNIST image dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we loaded the training and testing data and put it in
    the `data` directory of the current environment where the code executes. In the
    next section, we will learn how to make the data available on any compute instance
    in the ML workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must load the data, parse it, and store it in multi-dimensional NumPy
    arrays. We will use a helper function, `load`, which is defined in the accompanying
    source code for this chapter. After that, we must preprocess the training data
    by normalizing the pixel values to a range between `0` and `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using the `reshape` method, we checked that the training and testing labels
    are one-dimensional vectors with a single label per training and testing sample.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the training data, it is time to decide which Python framework
    to use to train the neural network models. While you are not limited to any specific
    framework in Azure Machine Learning, it is recommended you use either TensorFlow
    (with Keras) or PyTorch to train neural networks and DL models. TensorFlow and
    Keras are great choices when you're training and deploying standard production
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is a great choice for tinkering with exotic models and custom layers
    and debugging customized models. In my opinion, PyTorch is a bit easier to get
    started with, whereas TensorFlow is more complex and mature and has a bigger ecosystem.
    In this chapter, we will use TensorFlow due to its large ecosystem, Keras integration,
    great documentation, and good support in the Azure Machine Learning service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having chosen an ML framework, we can start to construct a simple CNN. Let''s
    use `keras` to construct a sequential model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we took advantage of the `keras.Sequential` model API
    to construct a simple CNN model. We went with the default initialization of the
    weights and solely specified the model structure here. You can also see the typical
    combination of a feature extractor until the `Flatten` layer, and the MLP classification
    head outputting 10 probabilities using the `softmax` activation function at the
    end.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at the model, which has, in total, `409034` parameters,
    as shown in the following diagram. Please note that we specifically constructed
    a simple CNN from a tiny image size of `28x28` grayscale images. The following
    diagram shows the compact structure of the model defined. Here, we can observe
    that the largest number of parameters is the fully connected layer after the feature
    extractor, which contains 98% of the parameters of the total model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – DL model architecture ](img/B17928_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – DL model architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'After defining the model structure, we need to define the `loss` metric that
    we are trying to optimize and specify an optimizer. The optimizer is responsible
    for computing the changes for all the weights per training iteration, given the
    total and backpropagated loss. With Keras and TensorFlow, we can easily choose
    a state-of-the-art optimizer and use a default metric for classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we defined a `categorical_crossentropy` loss and the
    `adam` optimizer to train the CNN. We also tracked another metric besides the
    loss – `accuracy`. This makes it easier to estimate and measure the performance
    of the CNN during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start training, we must define a model checkpoint. This is important
    as it allows us to pause and resume training at any given time after an epoch.
    Using Keras, it is quite simple to implement this, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can start the training locally by invoking the `fit` method on
    the Keras model. We must supply the training data as well as the batch size and
    the number of epochs (iterations) for training. We must also pass the previously
    created `callback` model checkpoint so that we can save the model after each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can use the trained model of the last epoch to compute the final
    score on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we can see that training a CNN on a compute instance
    in Azure Machine Learning is straightforward and similar to training a model on
    the local machine. The only difference is that we have to be sure that all the
    required libraries (and required versions) have been installed and that the data
    is available.
  prefs: []
  type: TYPE_NORMAL
- en: Generating more input data using augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DL models usually have many millions of parameters to represent the model with
    the training set distribution. Hence, when dealing with DL, be it in custom vision
    using Cognitive Services, Azure Machine Learning Studio, or custom models in ML
    service workspaces, you should always implement data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation is a way of creating more training data by slightly modifying
    the available data and providing the modified data to the ML algorithm. Depending
    on the use case, this could include mirroring, translating, scaling, or skewing
    images, as well as changing the brightness, luminosity, or color information of
    images. These modifications strongly improve the generalization of the model,
    such as enabling better scale, translation, rotation, and transformation invariance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefit of using TensorFlow and Keras is that data augmentation is a built-in
    capability. First, we can create an `ImageDataGenerator` object, which stores
    all our modifications and can generate iterators through the augmented dataset.
    The data augmentation techniques for this generator can be configured when the
    generator is being initialized. However, we want to use the generator to simply
    iterate through the training images without augmentation and add augmentation
    once we have connected all the pieces. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement an image data generator in Keras using the `ImageDataGenerator`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can return a data iterator from the image data generator by passing
    the original training image data and labels to the generator. Before we sample
    images from the generator, we need to compute the training set statistics that
    will be required for further augmentations. Similar to the scikit-learn `BaseTransformer`
    interface, we need to call the `fit` method on the generator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must create an iterator by using the `flow` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If instead of loading the images into NumPy arrays beforehand, we wanted to
    read individual images from a folder, we could use a different generator function
    to do so, as shown in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: However, in our example, the training images have been combined into a single
    file, so we don't need to load the image data ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'The iterator can now be used to loop through the data generator and yield new
    training samples with each iteration. To do so, we need to replace the `fit` function
    with the `fit_generator` function, which expects an iterator instead of a training
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, we can pass the same arguments for `epoch` and `callback` to
    the `fit_generator` function as we did to the `fit` function. The only difference
    is that now, we need to fix several steps per epoch so that the iterator yields
    new images. Once we have added augmentation methods to the generator, we could
    theoretically generate unlimited modifications of each training image per epoch.
    Hence, with this argument, we can define how many batches of data we wish to train
    each epoch with, which should roughly correspond to the number of training samples
    divided by the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can configure the data augmentation techniques. The default image
    data generator supports a variety of augmentations through different arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: Translation or shifts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal or vertical flips
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brightness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoom
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s go back to the image data generator and activate data augmentation techniques.
    Here is an example generator that is often used for data augmentation in image
    processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: By using this data generator, we can train the model with augmented image data
    and further improve the performance of the CNN. As we saw previously, this is
    a crucial and strongly recommended step in any DL training pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move all the code that we have developed so far into a file called `scripts/train.py`.
    We will use this file in the next section to schedule and run it on a GPU cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Training on a GPU cluster using Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a training script ready, verified that the script works, and
    added data augmentation, we can move this training script to a more performant
    execution environment. In DL, many operations, such as convolutions, pooling,
    and general tensor operators, can benefit from parallel execution. Therefore,
    we will execute the training script on a GPU cluster and track its status in the
    authoring environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'One benefit of using Azure Machine Learning is that we can set up and run everything
    in Python from the authoring environment – that is, the Jupyter notebook running
    on the Azure Machine Learning compute instance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must configure our Azure Machine Learning workspace, which is a single
    statement without arguments on the compute instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must load or create a GPU cluster with autoscaling for the training
    process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As shown in the preceding code snippet, creating a GPU cluster with autoscaling
    only requires a couple of lines of code within Jupyter with Azure Machine Learning.
    But how did we choose the VM size and the number of nodes for the GPU cluster?
  prefs: []
  type: TYPE_NORMAL
- en: In general, you can decide between the NC, ND, and NV types from the N-series
    VMs in Azure. A later version number (for example, v2 or v3) usually means updated
    hardware, hence a newer CPU and GPU, and better memory. You can think of the different
    N-series versions in terms of applications (*NC*, where *C* means compute; *ND*,
    where *D* means deep learning; and *NV*, where *V* means video). The following
    table will help you compare the different N-series VM types and their GPU configurations.
    Most machines can be scaled up to four GPUs per VM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows an Azure VM N-series comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Azure VM N-series costs ](img/B17928_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Azure VM N-series costs
  prefs: []
  type: TYPE_NORMAL
- en: The prices in the preceding table represent pay-as-you-go prices for Linux VMs
    in the West US 2 region for December 2021\. Please note that these prices may
    have changed by the time you are reading this, but it should give you an indication
    of the different options and configurations to choose from.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better understanding of the costs and performance, we can look at
    a typical workload for training a ResNet50 model on the ImageNet dataset. The
    following table, provided by Nvidia, shows that it makes sense to choose the latest
    GPU models as their performance increase is much better and the costs are more
    efficient than the older GPU models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – GPU costs ](img/B17928_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – GPU costs
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding table, the performance increase that's visible in
    the lower training duration for the same task pays off and results in a much lower
    cost for the overall task.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the `STANDARD_NC6` model is a great starting point, from a pricing perspective,
    for experimenting with GPUs, CNNs, and DNNs in Azure. The only thing that we have
    to make sure of is that our model can fit into the available GPU memory of the
    VM. A common way to calculate this is to compute the number of parameters for
    the model, times 2 for storing gradients (times 1 when performing only inferencing),
    times the batch size, and times 4 for the single-precision size in bytes (or times
    2 for half-precision).
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the CNN architecture requires 1.6 MB to store the trainable
    parameters (weights and biases). To also store backpropagated losses for a batch
    size of 16, we would require around 51.2 MB (1.6 MB x 16 x 2) of GPU memory to
    perform the whole end-to-end training on a single GPU. This also fits easily in
    our 12 GB of GPU memory in the smallest NC instance.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: While these numbers seem small for our test case, you will often deal with larger
    models (with up to 100 million parameters) and larger image sizes. To put that
    into perspective, ResNet152, when trained on image dimensions of 224 x 224 x 3,
    has approximately 60 million parameters and a size of 240 MB. On the `STANDARD_NC6`
    instance, we could train, at most, at a batch size of 24, according to our equation.
  prefs: []
  type: TYPE_NORMAL
- en: By adding more GPUs or nodes to the cluster, we must introduce a different framework
    to take advantage of the distributed setup. We will discuss this in more detail
    in [*Chapter 12*](B17928_12_ePub.xhtml#_idTextAnchor189), *Distributed Machine
    Learning on Azure*. However, we can add more nodes with autoscaling to the cluster
    so that multiple people can submit multiple jobs simultaneously. The number of
    maximum nodes can be computed as *simultaneous models/node * number of peak models
    to be trained simultaneously*. In our test scenario, we will go with a cluster
    size of `3` so that we can schedule a few models at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have decided on a VM size and GPU configuration, we can continue
    with the training process. Next, we need to make sure that the cluster can access
    the training data. To do so, we will use the default datastore on the Azure Machine
    Learning workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we copied the training data from the local machine to
    the default datastore – the blob storage account. As we discussed in [*Chapter
    4*](B17928_04_ePub.xhtml#_idTextAnchor071), *Ingesting Data and Managing Datasets*,
    there are also other ways to upload your data to blob storage or another storage
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mounting blob storage to a machine, or even a cluster, is usually not a straightforward
    task. Yes, you could have a NAS and mount it as a network drive on every node
    in the cluster, but this is tedious to set up and scale. Using the Azure Machine
    Learning datastore API, we can simply request a reference to the datastore, which
    can be used to mount the correct folder on every machine that needs to access
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command returns a `Datastore Mount` object, which doesn't look
    particularly powerful. However, if we pass this reference as a parameter to the
    training script, it can automatically mount the datastore and read the content
    from the datastore on each training compute in Azure Machine Learning. If you
    have ever played with mount points or `fstab`, you will understand that this one-liner
    can speed up your daily workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create an Azure Machine Learning configuration. Let''s create `ScriptRunConfiguration`
    so that we can schedule the training script on the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To read the data from the specified default datastore, we need to parse the
    argument in the `train.py` script. Let''s go back to the script and replace the
    file-loading code with the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This leaves us with scheduling and running the script on the GPU cluster. However,
    before doing so, we want to make sure that all the runs are tracked in the Azure
    Machine Learning service. Therefore, we must also add `Run` to the `train.py`
    file and reuse the Keras callback for Azure Machine Learning from [*Chapter 3*](B17928_03_ePub.xhtml#_idTextAnchor054),
    *Preparing the Azure Machine Learning Workspace*. Here is what the training script
    will look like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, we added the `Run` configuration and the Keras callback to track
    all the metrics during the epochs. We also collected the final test set metric
    and reported it to the Azure Machine Learning service. You can find the complete
    runnable example in the code provided with this book.
  prefs: []
  type: TYPE_NORMAL
- en: Improving your performance through transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many cases, you won't have a dataset containing hundreds of millions of labeled
    training samples, and that's completely understandable. So, how can you still
    benefit from all the previous work and benchmarks? Shouldn't a feature extractor
    trained on recognizing animals also perform well on recognizing faces? The classifier
    would certainly be different, but the visual features that are extracted from
    the images should be similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the idea behind `faces` dataset, the `CoCo` dataset, and so on) and
    attach a custom classifier to the end of the model. Transfer learning means that
    we can transfer the features from a model from one task to another task: for example,
    from classification to object detection. It may be a bit confusing at first regarding
    whether we would want to reuse features for a different task. However, if a model
    has been taught to identify patterns of geographical shapes in images, this same
    feature extractor could certainly be reused for any image-related task in the
    same domain.'
  prefs: []
  type: TYPE_NORMAL
- en: One useful property of transfer learning is that the initial learning task doesn't
    necessarily need to be a supervised ML task, so it is not necessary to have annotated
    training data to train the feature extractor. A popular unsupervised ML technique
    is called auto-encoders, where an ML model tries to generate a similar-looking
    output, given input, using a feature extractor and an upsampling network. By minimizing
    the error between the generated output and the input, the feature extractor learns
    to efficiently represent the input data in latent space. Auto-encoders are popular
    for pre-training network architectures before the pre-trained weights for the
    actual ML task are used.
  prefs: []
  type: TYPE_NORMAL
- en: We need to make sure that the pre-trained model was trained on a dataset in
    the same domain. Images of biological cells look very different from faces, and
    clouds look very different from buildings. In general, the ImageNet dataset covers
    a broad spectrum of photograph-style images for many standard visual features,
    such as buildings, cars, animals, and more. Therefore, it is a good choice to
    use a pre-trained model for many computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is not only tied to image data and modeling data for computer
    vision. Transfer learning has proven valuable in any domain where datasets are
    sufficiently similar, such as for human voices or written text. Hence, whenever
    you are implementing a DL model, do your research on what datasets could be used
    for transfer learning and to ultimately improve the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s bring the theory into practice and dive into some examples. We saw a
    similar example earlier in this chapter, where we piped the output of the feature
    extractor to an SVM. In this section, we want to achieve something similar, but
    the result will be a single end-to-end model. Therefore, in this example, we will
    build a network architecture for the new model consisting of a pre-trained feature
    extractor and a newly initialized classification head:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must define the number of output classes and the input shape and
    load the base model from Keras:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, most of the magic for pre-training happens thanks to
    Keras. First, we specified the image dataset that will be used to train this model
    using the `weights` argument, which will automatically initialize the model weights
    with the pre-trained `imagenet` weights. With the third argument, `include_top=False`,
    we told Keras to only load the feature extractor part of the model. Using the
    `pooling` argument, we also specified how the last pooling operation should be
    performed. In this case, we chose average pooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must freeze the layers of the model by setting their `trainable` property
    to `False`. To do so, we can simply loop over all the layers in the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can attach any network architecture to the model that we want.
    In this case, we will attach the same classifier head that we used in the CNN
    network from the previous section. Finally, we must construct the final model
    class by using the new architecture and output as the classifier output layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it! You have successfully built a new end-to-end model that combines
    a pre-trained ResNet50 feature extractor on ImageNet with your custom classifier.
    You can now use this Keras model and plug it into your preferred optimizer and
    send it off to the GPU cluster. The output of the training process will be a single
    model that can be managed and deployed as any other custom model.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: You are not limited to always freezing all the layers of the original network.
    A common approach is to also unfreeze later layers in the network, decrease the
    learning rate by at least a factor of 10, and continue training. By repeating
    this procedure, we could even retrain (or fine-tune) all the layers of the network
    in a step-by-step approach with a decreasing learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Independently of your choice and use case, you should add transfer learning
    to your standard repertoire for training DL models. Treat it like other popular
    preprocessing and training techniques, such as data augmentation, which should
    always be used when you're training DL models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned when and how to use DL to train an ML model on Azure.
    We used both a compute instance and a GPU cluster from within the Azure Machine
    Learning service to train a model using Keras and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: First, we found out that DL works very well on highly structured data with non-obvious
    relationships from the raw input data to the resulting prediction. Good examples
    include image classification, speech-to-text, and translation. We also saw that
    DL models are parametric models with a large number of parameters, so we often
    need a large amount of labeled or augmented input data. In contrast to traditional
    ML approaches, the extra parameters are used to train a fully end-to-end model,
    also including feature extraction from the raw input data.
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN using the Azure Machine Learning service is not difficult. We
    saw many approaches, from prototyping in Jupyter to augmenting the training data,
    to running the training on a GPU cluster with autoscaling. The difficult part
    in DL is preparing and providing enough high-quality training data, finding a
    descriptive error metric, and optimizing between costs and performance. We provided
    an overview of how to decide on the best VM and GPU size and configuration for
    your job, something that I recommend you do before starting your first GPU cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go one step further and look into hyperparameter
    tuning and automated ML, a feature in the Azure Machine Learning service that
    lets you train and optimize stacked models automatically.
  prefs: []
  type: TYPE_NORMAL
