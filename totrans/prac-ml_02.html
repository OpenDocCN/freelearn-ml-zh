<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Machine learning and Large-scale datasets"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Machine learning and Large-scale datasets</h1></div></div></div><p>We have seen a dramatic change in the way data has been handled in the recent past with the advent of big data. The field of Machine learning has seen the need to include scaling up strategies to handle the new age data requirements. This actually means that some of the traditional Machine learning implementations will not all be relevant in the context of big data now. Infrastructure and tuning requirements are now the challenges with the need to store and process large scale data complimented by the data format complexities.</p><p>With the evolution of hardware architectures, accessibility of cheaper hardware with distributed architectures and new programming paradigms for simplified parallel processing options, which can now be applied to many learning algorithms, we see a rising interest in scaling up the Machine learning systems.</p><p>The topics listed next are covered in-depth in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An introduction to big data and typical challenges of large-scale Machine learning</li><li class="listitem" style="list-style-type: disc">The motivation behind scaling up and scaling out Machine learning, and an overview of parallel and distributed processing for huge datasets</li><li class="listitem" style="list-style-type: disc">An overview of Concurrent Algorithm design, Big O notations, and task decomposition techniques for achieving parallelism</li><li class="listitem" style="list-style-type: disc">The advent of cloud frameworks to provide cloud clustering, distributed data storage, fault tolerance, and high availability coupled with effective utilization of computational resources</li><li class="listitem" style="list-style-type: disc">Frameworks and platform options for implementing large-scale Machine learning (Parallel Processing Frameworks such as MapReduce in <a id="id214" class="indexterm"/><span class="strong"><strong>Massive Parallel Processing</strong></span> (<span class="strong"><strong>MPP</strong></span>), MRI, platforms as GPU, FPGA, and Multicore)</li></ul></div><div class="section" title="Big data and the context of large-scale Machine learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Big data and the context of large-scale Machine learning</h1></div></div></div><p>I have covered some of the<a id="id215" class="indexterm"/> core aspects of big data in my previous Packt <a id="id216" class="indexterm"/>book titled <span class="emphasis"><em>Getting Started with Greenplum for Big Data Analytics</em></span>. In this section, we will quickly recap some of the core aspects of big data and its impact in the field of Machine learning:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The definition of large-scale is a scale of terabytes, petabytes, exabytes, or higher. This is typically the volume that cannot be handled by traditional database engines. The following chart lists the orders of magnitude that represents data volumes:<div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th colspan="3" style="text-align: center" valign="bottom">
<p>Multiples of bytes</p>
</th></tr></thead><tbody><tr><td colspan="2" style="text-align: center" valign="top">
<p>
<span class="strong"><strong>SI decimal prefixes</strong></span>
</p>
</td><td rowspan="2" style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Binary Usage</strong></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Name(Symbol)</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Value</strong></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Kilobyte (KB)</p>
</td><td style="text-align: left" valign="top">
<p>103</p>
</td><td style="text-align: left" valign="top">
<p>210</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Megabyte (MB)</p>
</td><td style="text-align: left" valign="top">
<p>106</p>
</td><td style="text-align: left" valign="top">
<p>220</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Gigabyte (GB)</p>
</td><td style="text-align: left" valign="top">
<p>109</p>
</td><td style="text-align: left" valign="top">
<p>230</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Terabyte (TB)</p>
</td><td style="text-align: left" valign="top">
<p>1012</p>
</td><td style="text-align: left" valign="top">
<p>240</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Petabyte (PB)</p>
</td><td style="text-align: left" valign="top">
<p>1015</p>
</td><td style="text-align: left" valign="top">
<p>250</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Exabyte (EB)</p>
</td><td style="text-align: left" valign="top">
<p>1018</p>
</td><td style="text-align: left" valign="top">
<p>260</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Zettabyte (ZB)</p>
</td><td style="text-align: left" valign="top">
<p>1021</p>
</td><td style="text-align: left" valign="top">
<p>270</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Yottabyte (YB)</p>
</td><td style="text-align: left" valign="top">
<p>1024</p>
</td><td style="text-align: left" valign="top">
<p>280</p>
</td></tr></tbody></table></div></li><li class="listitem" style="list-style-type: disc">Data formats that are referred to in this context are distinct; they are generated and consumed, and need not be structured (for example, DBMS and relational data stores). Now, there are new sources of data; this data can be generated by social networking sites, equipment, and more. This can be streaming data that is heterogeneous in nature (for example, videos, emails, tweets, and so on). Again, none of the traditional data marts / data stores and data mining applications support these formats today.</li><li class="listitem" style="list-style-type: disc">Additionally, all the large-scale processing always happened in batches, but we are now seeing the need to support <span class="emphasis"><em>real-time</em></span> processing capabilities. The new <span class="strong"><strong>Lambda Architectures</strong></span> (<span class="strong"><strong>LA</strong></span>)<a id="id217" class="indexterm"/> address the need to support both batch and real-time data ingestion and processing.</li><li class="listitem" style="list-style-type: disc">Overall, the response time windows are shrinking and this adds to the challenge.</li></ul></div><p>Let's recap the four key characteristics<a id="id218" class="indexterm"/> of big data. All of these need special tools, frameworks, infrastructure, and capabilities:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Higher volumes (to the degree of petabytes )</li><li class="listitem" style="list-style-type: disc">The need for availability/accessibility of data (more real-time)</li><li class="listitem" style="list-style-type: disc">Diversified data formats</li><li class="listitem" style="list-style-type: disc">The increase in unlabeled data, and thus the <span class="strong"><strong>Noise</strong></span><div class="mediaobject"><img src="graphics/B03980_02_01.jpg" alt="Big data and the context of large-scale Machine learning"/></div></li></ul></div><div class="section" title="Functional versus Structural – A methodological mismatch"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec29"/>Functional versus Structural – A methodological mismatch</h2></div></div></div><p>We could never have <a id="id219" class="indexterm"/>imagined even five years ago that Relational Databases or non-relational databases like object databases will become only a single kind of database technology, and not the database technology in itself. Internet-scale data processing has changed the way we process data.</p><p>The new generation architectures, such as Facebook, Wikipedia, Salesforce, and more, are founded on principles and paradigms, which are radically different from the well-established theoretical foundations on which the current data management technologies are developed.</p><div class="section" title="Commoditizing information"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec24"/>Commoditizing information</h3></div></div></div><p>The <a id="id220" class="indexterm"/>Apple App Store, SaaS, Ubiquitous Computing, Mobility, Cloud-Based Multi-Tenant architectures have unleashed, in business terms, an ability to commoditize information delivery. This model changes almost all the architecture decision making—as we now need to think in terms of what is the "units of information" that can be offered and billed as services, instead of thinking in terms of the <span class="strong"><strong>Total Cost of Ownership</strong></span> (<span class="strong"><strong>TCO</strong></span>)<a id="id221" class="indexterm"/> of the solution.</p></div><div class="section" title="Theoretical limitations of RDBMS"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec25"/>Theoretical limitations of RDBMS</h3></div></div></div><p>As Michael Stonebreaker, the <a id="id222" class="indexterm"/>influential database theorist, has been writing in recent times, at the heart of the Internet-Scale Architectures is a new theoretical model of<a id="id223" class="indexterm"/> data processing and management. The theories of database management are now more than three decades old, and they were designed for mainframe-type computing environments and unreliable electronic components. Nature and the capabilities of systems and applications have since evolved significantly. With reliability becoming a quality attribute of the underlying environment, systems are composed of parallel processing cores, and the nature of data creation and usage has undergone tremendous change. In order to conceptualize solutions for these new environments, we need to approach the designing of solution architectures from a computing perspective and not only from an engineering perspective.</p><p>Six major forces that are driving the data revolution today are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Massive Parallel Processing</li><li class="listitem" style="list-style-type: disc">Commoditized Information Delivery</li><li class="listitem" style="list-style-type: disc">Ubiquitous Computing and Mobile Devices</li><li class="listitem" style="list-style-type: disc">Non-RDBMS and Semantic Databases</li><li class="listitem" style="list-style-type: disc">Community Computing</li><li class="listitem" style="list-style-type: disc">Cloud Computing</li></ul></div><p>
<span class="strong"><strong>Hadoop</strong></span>
<a id="id224" class="indexterm"/> and <span class="strong"><strong>MapReduce</strong></span>
<a id="id225" class="indexterm"/> have unleashed massive parallel processing of data on a colossal scale, and have made the complex computing algorithms in a programmatic platform. This has changed analytics and Business Intelligence forever. Similarly, the web services and API-driven architectures have made information delivery commoditized on an enormous scale.</p><p>Today, it is possible to build very large systems in such a way that each subsystem or component is a complete platform in itself, hosted and managed by a different entity altogether.</p><p>Dijkstra once made an insightful remark that:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Computer Science is no more about computers than astronomy is about telescopes"</em></span></p></blockquote></div><p>He would <a id="id226" class="indexterm"/>perhaps be a happy man today, as computing has liberated<a id="id227" class="indexterm"/> itself from the clutches of a personal computer, also known as workstations and servers. Most of our information consumption today is from the devices that we hardly call computers. Mobile devices, wearable devices, and information everywhere are changing the way data is created, assembled, consumed, and analyzed.</p><p>As the limitations of the traditional databases have been exposed, in recent years, many special purpose databases have emerged—in-memory, columnar, graph-DB, and semantic stores are all now commercially available.</p><p>The previously mentioned innovations have changed the traditional data architecture completely. Especially, the semantic computing, ontology-driven modelling of information has turned data design over its head. Philosophically, data architecture is going through an factual underpinning. In the traditional data models, we first design the "data model"—a fixed, design time understanding of the world and its future. A data model fixes the meaning of data forever into a fixed structure. A table is nothing but a category, a set of something. As a result, data has to mean if we understand the set/category to which it belongs. For example, if we design an automobile processing system into some categories, such as four-wheelers, two-wheelers, commercial vehicles, and so on, then this division itself has a relevant meaning embedded into it. The data that is stored in each of these categories does not reveal the purpose of the design that is embedded in the way the categories are designed. For example, another system might view the world of automobiles regarding of its drivetrain—electric, petroleum powered, nuclear powered, and so on.</p><p>This categorization itself reveals the purpose of the system in some manner, which is impossible to obtain the attributes of any single record. Semantic and Metadata-Driven architectures can turn such a data model over its head. In a metadata model, it is the object that exists first.</p><p>Some of the core characteristics of how data is stored and managed in an RDBMS-based storage system are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data is stored in a table that is typically characterized by rows and columns</li><li class="listitem" style="list-style-type: disc">Tables are linked using relationships between data attributes</li><li class="listitem" style="list-style-type: disc">It is known for efficiency and flexibility</li><li class="listitem" style="list-style-type: disc">This supports normalization techniques that reduce data duplication</li></ul></div><p>On the other hand:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <a id="id228" class="indexterm"/>metadata driven / NoSQL / Semantic<a id="id229" class="indexterm"/> data architectures are free from relationships that tie down the purpose of the usage of data</li><li class="listitem" style="list-style-type: disc">The focus is more on accommodating constant changes in business requirements that results in least changes in the software system being built</li><li class="listitem" style="list-style-type: disc">Support for large datasets with distributed storage techniques, with lowered storage costs is of great importance in the metadata driven / NoSQL /semantic data architecture</li></ul></div></div><div class="section" title="Scaling-up versus Scaling-out storage"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec26"/>Scaling-up versus Scaling-out storage</h3></div></div></div><p>With the advent of big<a id="id230" class="indexterm"/> data, there is now a need to scale data storage equipment to be able to store the petabyte-scale data. There are two ways of scaling storage equipment:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Scaling-up (vertical scalability)</li><li class="listitem" style="list-style-type: disc">Scaling-out (horizontal scalability)</li></ul></div><p>
<span class="strong"><strong>Scaling up</strong></span> or vertical scalability is about adding more resources to the existing system that in turn increases the ability to hold more data. Here, resources can mean RAM, computation power, hard drive, and more.</p><p>
<span class="strong"><strong>Scaling out</strong></span> or horizontal scalability is about adding new components to the system. This requires the data to be stored and distributed, and there are tasks that can be parallelized. This usually adds complexity to the system, and most of the time requires a redesign of the system.</p><p>All the big data technologies work on and support the scaling out of the infrastructure.</p><div class="mediaobject"><img src="graphics/B03980_02_02.jpg" alt="Scaling-up versus Scaling-out storage"/></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Scaling up (Vertical Scalability)</p>
</th><th style="text-align: left" valign="bottom">
<p>Scaling out (Horizontal Scalability)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Lesser and high capacity<a id="id231" class="indexterm"/> server</p>
</td><td style="text-align: left" valign="top">
<p>More and moderate, or low capacity server</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>There could be a threshold beyond which an infrastructure can cease to scale vertically</p>
</td><td style="text-align: left" valign="top">
<p>There is no limit, the infrastructure can be scaled on a need basis without any impact on the design</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Can accommodate larger VMs</p>
</td><td style="text-align: left" valign="top">
<p>Runs with lower VMs and can be affected by failure in the host</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Shared everything data architecture</p>
</td><td style="text-align: left" valign="top">
<p>Shared nothing data architecture</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Higher TCO</p>
</td><td style="text-align: left" valign="top">
<p>Relatively lower and variable costs</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Lower network equipment</p>
</td><td style="text-align: left" valign="top">
<p>Needs relatively larger number of equipments (routers, switches, and more…)</p>
</td></tr></tbody></table></div></div><div class="section" title="Distributed and parallel computing strategies"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec27"/>Distributed and parallel computing strategies</h3></div></div></div><p>Though distributed<a id="id232" class="indexterm"/> and parallel processing<a id="id233" class="indexterm"/> have been around for several years now, but with the advent of usability priorities needed for cost-effective solutions, these strategies have become critical for the Machine learning tasks.</p><p>The following diagram depicts Flynn's taxonomy for computing. The categorization is done based on the number of data streams versus the number of instruction streams.</p><div class="mediaobject"><img src="graphics/B03980_02_03.jpg" alt="Distributed and parallel computing strategies"/></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Single Instruction Single Data</strong></span> (<span class="strong"><strong>SISD</strong></span>): This is a<a id="id234" class="indexterm"/> case of a single processor with no parallelism in data or instruction. A single instruction is executed on a single data in a sequential manner, for example, a uniprocessor.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Multiple Instruction Single Data</strong></span> (<span class="strong"><strong>MISD</strong></span>): Here, multiple<a id="id235" class="indexterm"/> instructions operate on a single data stream; a typical example can be fault tolerance.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Single Instruction Multiple Data</strong></span> (<span class="strong"><strong>SIMD</strong></span>): This is a<a id="id236" class="indexterm"/> case of natural parallelism; a single instruction triggers operation on multiple data streams.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Multiple Instructions Multiple Data</strong></span> (<span class="strong"><strong>MIMD</strong></span>): This <a id="id237" class="indexterm"/>is a case where multiple independent instructions operate on multiple and independent data streams. Since the data streams are multiple, the memory can either be shared or distributed. Distributed processing can be categorized here. The previous figure depicts MIMD and a variation in a "distributed" context.</li></ul></div><p>The following diagram explains parallel processor architectures and categorization:</p><div class="mediaobject"><img src="graphics/B03980_02_04.jpg" alt="Distributed and parallel computing strategies"/></div><p>One of the critical <a id="id238" class="indexterm"/>requirements of parallel/distributed processing systems is High Availability and fault tolerance. There are several programming paradigms to implement parallelism. The following list details the important ones:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The Master/Workers Model</strong></span>: Master model <a id="id239" class="indexterm"/>is the driver where the work is held and then disseminated to the workers. Pivotal Greenplum Database and HD (Pivotal's Hadoop's distribution) modules implement this pattern.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The Producer/Consumer Model</strong></span>: Here, there <a id="id240" class="indexterm"/>is no owner who triggers the work. Producer generates work items and consumer subscribes and executes asynchronously. The <span class="strong"><strong>Enterprise Service Bus</strong></span> (<span class="strong"><strong>ESB</strong></span>)<a id="id241" class="indexterm"/> based data integration <a id="id242" class="indexterm"/>systems implement this pattern.</li></ul></div><div class="mediaobject"><img src="graphics/B03980_02_05.jpg" alt="Distributed and parallel computing strategies"/></div><p>In theory, there are two types of parallelization; one is data parallelization, the other one is execution or task parallelization:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data parallelization</strong></span>: It <a id="id243" class="indexterm"/>deals with running the same computations with multiple inputs in parallel. In the Machine learning world, this is a case where we consider running the same algorithm across different data samples without really worrying about how the data samples are distributed.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Execution or Task parallelization</strong></span>: Unlike <a id="id244" class="indexterm"/>data parallelization, this is about breaking the functionality into multiple pieces and running them in a parallel manner. These pieces of work may work on the same dataset, but this is possible only for the tasks that can be parallelized and have no dependencies between the sub tasks.</li></ul></div><p>Task parallelization can be fine grained or coarse grained.</p><p>There are many distributed platform options to bring efficiency and scale to Machine learning algorithms and can process large datasets. Some of the options include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Field-Programmable Gate Arrays</strong></span> (<span class="strong"><strong>FPGAs</strong></span>)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Graphics Processing Units</strong></span> (<span class="strong"><strong>GPUs</strong></span>)</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>High-Performance Computing</strong></span> (<span class="strong"><strong>HPC</strong></span>)</li><li class="listitem" style="list-style-type: disc">Multicore and multi-processor parallel systems</li><li class="listitem" style="list-style-type: disc">Cloud Infrastructures for virtual-large scale clusters</li></ul></div><p>Besides the multiple platform options available, there are other highly adopted frameworks available that have out-of-box APIs for building Machine learning algorithms. The choice of this framework depends on the choice of hardware in particular.</p><p>It is important that we take an option that can take maximum advantage of the existing architecture, and suits the choice of learning algorithm and the data structure.</p></div></div><div class="section" title="Machine learning: Scalability and Performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec30"/>Machine learning: Scalability and Performance</h2></div></div></div><p>There are two <a id="id245" class="indexterm"/>important ways in which Machine learning algorithms <a id="id246" class="indexterm"/>can be scaled:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sampling</li><li class="listitem" style="list-style-type: disc">Distributed systems with parallel processing</li></ul></div><p>It is possible to concurrently execute a given learning algorithm as separate chunks of work and consolidate the results. This sounds like a fairly simple way of parallelizing and being able to scale and perform well on a bigger dataset. This comes with an assumption that the datasets are discrete and there isn't any dependency between these distributed sets of data.</p><p>By the virtue of the proliferation of data sources, we now have access to large sets that are already distributed, and this brings in a need for the ability to have the learning algorithms running in a distributed mode.</p><p>There are now a variety of options for distributed and parallel framework for Machine learning. Let's look at some key differentiating factors between these platforms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The degree of granularity in parallelization is a critical aspect. Support for fine-grained versus coarse-grained parallelization is what it refers to. A lower degree of granularity defines a fine-grained task parallelization, while a higher level of granularity defines coarse-grained task parallelization.</li><li class="listitem" style="list-style-type: disc">The degree to which algorithm customization is supported.</li><li class="listitem" style="list-style-type: disc">Support for mixing a variety of programming paradigms.</li><li class="listitem" style="list-style-type: disc">The ease with which datasets can be scaled-out.</li><li class="listitem" style="list-style-type: disc">The degree to which batch and real-time processing is supported.</li></ul></div><p>Given a problem context, the choice of the platform and programming framework should be guided by the previous criteria.</p><p>Following are <a id="id247" class="indexterm"/>some key metrics to measure the computational<a id="id248" class="indexterm"/> performance of parallel algorithms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Performance</strong></span> is the ratio of solution time for the sequential algorithms versus parallel process</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Efficiency</strong></span> or <span class="strong"><strong>Throughput</strong></span> measures the ratio of performance across multiple processors</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Scalability</strong></span> is the percentage improvement in efficiency with the growing number of processors</li></ul></div><p>The next section covers some key characteristics of the Machine learning problem that motivate scaling-up the Machine learning algorithms.</p><div class="section" title="Too many data points or instances"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec28"/>Too many data points or instances</h3></div></div></div><p>We now see that in most <a id="id249" class="indexterm"/>of the Machine learning problems, there<a id="id250" class="indexterm"/> is an abundance of datasets and in many cases, all these data points are relevant in model building and refining. These data points can potentially run into terabyte scale with all their relevance.</p><p>This brings in a need to support distributed storage and a bandwidth to process these data points in the cluster. High-capacity storage systems with the ability to run parallel programming language paradigms like MapReduce and LINQ are used here.</p></div><div class="section" title="Too many attributes or features"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec29"/>Too many attributes or features</h3></div></div></div><p>The datasets that form an<a id="id251" class="indexterm"/> input to a building model can come <a id="id252" class="indexterm"/>with too many features, attributes, or dimensions. In this case, the Machine learning algorithms group the dependent or more relevant attributes and run the algorithms in iteration. These kind of datasets can be seen in case of Text mining and <a id="id253" class="indexterm"/>
<span class="strong"><strong>Natural language processing</strong></span> (<span class="strong"><strong>NLP</strong></span>), where the number of features can run into multiples of millions. In this case, parallelizing the computation across features can get us to solve the problem effectively by the way of eliminating irrelevant features. Random forest and Decision trees are some of the examples. Also, some specific feature selection techniques such as the regularization methods will be covered in the chapters to come.</p></div><div class="section" title="Shrinking response time windows – need for real-time responses"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec30"/>Shrinking response time windows – need for real-time responses</h3></div></div></div><p>There are certain <a id="id254" class="indexterm"/>Machine learning requirements such as speech recognition that will demand a real-time response from the systems. In these applications, response time from a Machine learning implementation is critical, and the response itself will become irrelevant otherwise. Parallelization can bring in this efficiency.</p><p>Latency and performance of the model are more important a problem to deal with than the throughput. There are many use cases where this latency in inference can invalidate the model itself, as the response becomes obsolete.</p><p>For these kinds of problems, highly parallelized hardware architectures such as GPUs or FPGAs will be very effective.</p></div><div class="section" title="Highly complex algorithm"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec31"/>Highly complex algorithm</h3></div></div></div><p>This is a case where the <a id="id255" class="indexterm"/>algorithm of choice itself is complex, for example, a computational intensive function or any non-linear models. Let's take an example of a text or image content; it is inherently non-linear in nature. This complexity can easily be addressed using distributed computing.</p><p>There are many ways we can solve these problems and one way is to prioritize features and still target for higher accuracies. However this will remove the automation part in the learning. There always needs to be a step that engineers the features before running the algorithm.</p><p>The cases where there is more data complexity, there is a computational complexity. Unless the platform is scaled, there is no way to get the learning process run faster.</p><p>Multicore and GPU systems are apt for this kind of requirement. They bring in both; storage scale and computational efficiency.</p></div><div class="section" title="Feed forward, iterative prediction cycles"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec32"/>Feed forward, iterative prediction cycles</h3></div></div></div><p>There are some unique use <a id="id256" class="indexterm"/>cases in the Machine learning space that do not stop at one level of execution of the algorithm. The algorithm runs iteratively and sequentially where the output from an iteration feeds into another iteration. This is critical for the outcome of the model. There can also be a need to consolidate the inferences across all the iterations that are run sequentially. This can make the model execution process quite complex. We can deal with inference process as a one-shot process, which will bring up the computational costs, or there can be stages of parallelization of individual tasks.</p><p>Some real-world examples are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Speech recognition</li><li class="listitem" style="list-style-type: disc">Machine translation</li></ul></div></div></div><div class="section" title="Model selection process"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec31"/>Model selection process</h2></div></div></div><p>In some cases, we will need to run multiple<a id="id257" class="indexterm"/> models in parameters on the same training and test sets with the different priority of features, and compare the accuracy to choose an appropriate model for the given problem domain. These trials can run in parallel as there will not be any dependencies between these models. The complexity increases when we will have to tune the parameters of learning algorithms and evaluate across multiple executions to infer from the learning.</p><p>The very fact that there is no dependency between the executions makes it highly parallelizable and requires no intercommunication. One of the examples of this use case is statistical significance testing. The usefulness of the parallel platforms is obvious for these tasks, as they can be easily performed concurrently without the need to parallelize actual learning and inference algorithms.</p></div><div class="section" title="Potential issues in large-scale Machine learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec32"/>Potential issues in large-scale Machine learning</h2></div></div></div><p>Let's now look at some potential <a id="id258" class="indexterm"/>issues encountered in the large-scale Machine learning implementations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Parallel execution</strong></span>: Managing the<a id="id259" class="indexterm"/> accuracy of the parallel execution requires special care and a different design paradigm.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Load balancing</strong></span> and <span class="strong"><strong>managing skews</strong></span>: With data and execution now distributed and running<a id="id260" class="indexterm"/> parallel, it is <a id="id261" class="indexterm"/>very imperative to manage the data and compute skews. No single node will need to take relatively more data storage or computations.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Monitoring</strong></span>: With a variety<a id="id262" class="indexterm"/> of hardware, effective monitoring and automatic recovery systems need to be placed.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Fault tolerance</strong></span>: A foolproof <a id="id263" class="indexterm"/>failover and recovery system is a mandate.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Auto scaling</strong></span>: The <a id="id264" class="indexterm"/>scaling out and scaling up process is automatic.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Job scheduling</strong></span>: <span class="emphasis"><em>Batch</em></span> jobs will <a id="id265" class="indexterm"/>need to be scheduled.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Workflow Management</strong></span>: Choreography <a id="id266" class="indexterm"/>and Orchestration process to coordinate and <a id="id267" class="indexterm"/>monitor work execution among the nodes of the cluster.</li></ul></div></div></div></div>
<div class="section" title="Algorithms and Concurrency"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Algorithms and Concurrency</h1></div></div></div><p>Let's now look at some basics of algorithms<a id="id268" class="indexterm"/> in general, the time complexity; and the order of magnitude <a id="id269" class="indexterm"/>measurements, before we start talking about building concurrency in executing algorithms, then explore the approaches to parallelizing algorithms.</p><p>An algorithm can be defined as a sequence of steps that takes an input to produce the desired output. They are agnostic technology representations; let's look at a sorting algorithm example:</p><div class="informalexample"><pre class="programlisting">Input: A sequence of <span class="emphasis"><em>n</em></span> number—<span class="emphasis"><em>a1, a2, …,an</em></span>
Output: A permutation (reordering)—<span class="emphasis"><em>a1', a2', …,an'</em></span> such that <span class="emphasis"><em>a1'&lt;=a2'&lt;=… &lt;=an'</em></span>
</pre></div><p>The following algorithm is an insertion-sort algorithm:</p><div class="informalexample"><pre class="programlisting">INSERTION-SORT(A)
1. for j = 2 to length[A]
2. dokey&lt;-A[j]
3. //insert A[j] to sorted sequence A[1..j-1]
4. i&lt;-j-1
5. while i&gt;0 and A[i]&gt;key
6. do A[i+1] &lt;- A[i] //move A[i] one position right
7. i&lt;-i-1
8. A[i+1]&lt;-key</pre></div><p>For measuring the time and space complexity of algorithms, one of the elements is the input size. The time complexity is a measure of how "fast enough" the algorithm is for the defined needs; more importantly, how the algorithm would react when the volume of the data is increased.</p><p>Frequency count is one of the <a id="id270" class="indexterm"/>key measures for an algorithm. It is a prediction of <a id="id271" class="indexterm"/>how many times each instruction of the algorithm will run for an execution. For example:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Instruction</p>
</th><th style="text-align: left" valign="bottom">
<p>Code</p>
</th><th style="text-align: left" valign="bottom">
<p> Frequency count (FC)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">for (int i=0; i&lt; n ; i++)</pre></div><p>
</p>
</td><td style="text-align: left" valign="top">
<p> n+1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">count &lt;&lt; i</pre></div><p>
</p>
</td><td style="text-align: left" valign="top">
<p> N</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>
</p><div class="informalexample"><pre class="programlisting">p = p + 1</pre></div><p>
</p>
</td><td style="text-align: left" valign="top">
<p> N</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p> 3n +1</p>
</td></tr></tbody></table></div><p>The FC measure is relatively meaningless unless it considers the relative performance to volume. There is another measure called "order of magnitude" that is an estimate of performance versus data volume. The <span class="emphasis"><em>Big-O</em></span> is a measure of the rate at which the algorithm performance degrades as the function of the amount of data that it requires to process.</p><p>For example, <span class="emphasis"><em>O(n)</em></span> represents linear performance degradation and <span class="emphasis"><em>O(n2)</em></span> represents quadratic performance degradation.</p><div class="section" title="Developing concurrent algorithms"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec33"/>Developing concurrent algorithms</h2></div></div></div><p>The first step in developing a <a id="id272" class="indexterm"/>parallel algorithm is to decompose the problem into tasks that can be executed concurrently. A given problem may be decomposed into tasks in many different ways. Tasks may be of same or different sizes:</p><p>Task dependency graph is a<a id="id273" class="indexterm"/> directed graph with nodes corresponding to tasks and edges indicating that the result of one task is required for processing the next task.</p><p>Example: This is the database query processing.</p><p>Consider the following execution of the query:</p><div class="informalexample"><pre class="programlisting">MODEL = ``CIVIC'' AND YEAR = 2001 AND (COLOR = ``GREEN'' OR COLOR = ``WHITE)</pre></div><p>on the following <a id="id274" class="indexterm"/>database:</p><div class="mediaobject"><img src="graphics/B03980_02_06.jpg" alt="Developing concurrent algorithms"/></div><p>There can be <a id="id275" class="indexterm"/>fine-grained and coarse-grained task decomposition. The degree of concurrency increases as the decomposition becomes finer.</p><p>There are many decomposition techniques and there is no single best way of doing it. Following are some techniques:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Recursive decomposition</li><li class="listitem" style="list-style-type: disc">Data decomposition</li><li class="listitem" style="list-style-type: disc">Exploratory decomposition</li><li class="listitem" style="list-style-type: disc">Speculative decomposition</li></ul></div><p>Decomposition results in several tasks and some characteristics of these tasks critically affect the performance of the parallel algorithms. Some of these features are task interactions (inter-task communication), the size of data that each task handles, and the task size. Some important aspects that need to be kept in mind while designing parallel execution algorithms include decoupling tasks in such a way that there is minimal interaction and handling granularity trade-offs.</p></div></div>
<div class="section" title="Technology and implementation options for scaling-up Machine learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Technology and implementation options for scaling-up Machine learning</h1></div></div></div><p>In this section, we<a id="id276" class="indexterm"/> will explore some parallel programming techniques and distributed platform options that Machine learning implementations can adopt. The Hadoop platform will be introduced in the next chapter, and we will look into some practical examples starting from <a class="link" href="ch03.html" title="Chapter 3. An Introduction to Hadoop's Architecture and Ecosystem">Chapter 3</a>, <span class="emphasis"><em>An Introduction to Hadoop's Architecture and Ecosystem</em></span> with some real-world examples.</p><div class="section" title="MapReduce programming paradigm"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec34"/>MapReduce programming paradigm</h2></div></div></div><p>MapReduce<a id="id277" class="indexterm"/> is a<a id="id278" class="indexterm"/> parallel programming paradigm that abstracts the parallelizing computing and data complexities in a distributed computing environment. It works on the concept of taking the compute function to the data rather than taking the data to the compute function.</p><p>MapReduce is more of a programming framework that comes with many built-in functions that the developer need not worry about building, and can alleviate many implementation complexities like data partitioning, scheduling, managing exceptions, and intersystem communications.</p><p>The following figure depicts a typical composition of the MapReduce function:</p><div class="mediaobject"><img src="graphics/B03980_02_07.jpg" alt="MapReduce programming paradigm"/></div><p>MapReduce was<a id="id279" class="indexterm"/> originally designed and adopted by Google as a programming model for processing large data sets on a cluster with parallel processing over distributed storage.</p><p>The MapReduce paradigm now has become an industry standard and many platforms are internally built on this paradigm and support MapReduce implementation. For example, Hadoop is an open source implementation that can be run either in-house or on cloud computing services such as, <span class="strong"><strong>Amazon EC2</strong></span>
<a id="id280" class="indexterm"/> with elastic MapReduce.</p><p>This has, at the core, the <code class="literal">Map()</code> and <code class="literal">Reduce()</code> functions that are capable of running in parallel across the nodes in the cluster. The <code class="literal">Map()</code> function works on the distributed data and runs the required functionality in parallel, and the <code class="literal">Reduce()</code> function runs a summary operation of the data.</p></div><div class="section" title="High Performance Computing (HPC) with Message Passing Interface (MPI)"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec35"/>High Performance Computing (HPC) with Message Passing Interface (MPI)</h2></div></div></div><p>MPI <a id="id281" class="indexterm"/>is designed to provide access to advanced parallel hardware, and is meant to work with <a id="id282" class="indexterm"/>heterogeneous networks<a id="id283" class="indexterm"/> and clusters. It is an impressive specification and provides a portable way to implement the parallel programs.</p><p>Message passing is a process of data transfer and synchronization between the sender and the receiver. The following figure demonstrates the message passing between sender and receiver:</p><div class="mediaobject"><img src="graphics/B03980_02_09.jpg" alt="High Performance Computing (HPC) with Message Passing Interface (MPI)"/></div><p>The processes can be grouped; the message sharing between the sender and the receiver needs to happen in the same context. Communicator thus is a combination of a group and the context. The data in a message is sent or received as triples.</p><p>MPI can be used to achieve portability and can improve performance through parallel processing. It can support unique data structures, and libraries can be built for reuse. MPI does not support liberal fault tolerance.</p></div><div class="section" title="Language Integrated Queries (LINQ) framework"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec36"/>Language Integrated Queries (LINQ) framework</h2></div></div></div><p>The LINQ framework<a id="id284" class="indexterm"/> is a<a id="id285" class="indexterm"/> general-purpose system for large-scale data and parallel computing. Similar to the MapReduce paradigm, it comes with a high level of abstraction that comes with base implementations, and helps developers reduce the development complexities of the parallel and distributed execution.</p><p>With the <a id="id286" class="indexterm"/>Machine learning functions moving out of general data handling and operating on diverse data types including documents, images, and graphs, the need for generic implementation paradigms is increasing. This framework pertains to the .NET languages only.</p></div><div class="section" title="Manipulating datasets with LINQ"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec37"/>Manipulating datasets with LINQ</h2></div></div></div><p>LINQ is shipped <a id="id287" class="indexterm"/>with a set of functions that operate on collections<a id="id288" class="indexterm"/> of .NET objects. These collections are <a id="id289" class="indexterm"/>modified by the LINQ functions that contain the .NET datatypes.</p><div class="mediaobject"><img src="graphics/B03980_02_10.jpg" alt="Manipulating datasets with LINQ"/></div></div><div class="section" title="Graphics Processing Unit (GPU)"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec38"/>Graphics Processing Unit (GPU)</h2></div></div></div><p>GPUs are<a id="id290" class="indexterm"/> electronic circuits designed to handle the memory requirements and rapidly create images in the frame buffers for visual display.</p><p>GPUs have been consistently supporting growing computational capabilities. They were initially meant to handle image processing and rendering, but the advanced GPUs are now positioned as self-contained, general purpose computing platforms.</p><p>While CPUs are designed to perform well on heterogeneous workloads, GPUs are built for tasks that are meant to ensure the availability of massive datasets and run in a parallel manner.</p><div class="mediaobject"><img src="graphics/B03980_02_11.jpg" alt="Graphics Processing Unit (GPU)"/></div><p>GPUs are mainly used in deep learning and training neural networks that can potentially need larger training datasets, lesser computational power, and storage space optimization. They are being employed in solving both classification and prediction problems in the cloud. Most of the social media companies have been in the list of early adopters for GPUs.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>With GPUs, pre-recorded speech or multimedia content can be transcribed much more quickly. Compared to a CPU implementation we are able to perform recognition up to 33x faster.</p></div></div></div><div class="section" title="Field Programmable Gate Array (FPGA)"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec39"/>Field Programmable Gate Array (FPGA)</h2></div></div></div><p>FPGAs are emerging<a id="id291" class="indexterm"/> in many areas of HPC. FPGAs can be used in the context of massive parallel processing. In this <a id="id292" class="indexterm"/>section, we will look at understanding some of the architecture and implementation aspects of FPGA.</p><p>FPGAs are known to provide high performance. They support different parallel computation applications. They have an on-chip memory to facilitate easy memory access to the processor. Above all, the memory is coupled to the algorithm logic and this means that we will not need any additional high-speed memory.</p><div class="mediaobject"><img src="graphics/B03980_02_12.jpg" alt="Field Programmable Gate Array (FPGA)"/></div><p>FPGA contains an enormous number of <a id="id293" class="indexterm"/>
<span class="strong"><strong>Configurable Logical Blocks</strong></span> (<span class="strong"><strong>CLB</strong></span>); each of these CLBs are connected using programmable<a id="id294" class="indexterm"/> interfaces that pass signals among them. The I/O blocks are the connections points for CLBs to the outside world.</p><p>FPGAs offer a variety of paradigms that help speed up computations in a hardware and software design. FPGAs are cost effective and the hardware resources are used in an optimal way. IBM Netezza leverages FPGA architecture.</p></div><div class="section" title="Multicore or multiprocessor systems"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec40"/>Multicore or multiprocessor systems</h2></div></div></div><p>Multiprocessor systems<a id="id295" class="indexterm"/> usually have multiple CPUs that need not necessarily <a id="id296" class="indexterm"/>be on the same chip. The new age multiprocessors are on the same physical board, and the communication happens via high-speed connection interfaces.</p><div class="mediaobject"><img src="graphics/B03980_02_13.jpg" alt="Multicore or multiprocessor systems"/></div><p>Multicore processors<a id="id297" class="indexterm"/> represent a family of processors that may contain many<a id="id298" class="indexterm"/> CPUs on one chip (such as two, four, and eight. In case of multicore systems, the efficiency of the multi-threading implementation is determined by how well-parallel the code is written).</p><p>Further to all the hardware and infrastructure advancements, we have just seen that the cloud frameworks for Machine learning are picking up considerable traction based on their ability to scale Machine learning processes at an optimal cost.</p><p>With the emergence of cloud computing, infrastructure service providers, such as Amazon Web Services, offer access to virtually unlimited computing power on demand that can be paid for, based on the usage.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Summary</h1></div></div></div><p>In this chapter we have explored the qualifiers of large datasets, their common characteristics, the problems of repetition, and the reasons for the hyper-growth in volumes; in fact, the big data context.</p><p>The need for applying conventional Machine learning algorithms to large datasets has given rise to new challenges for Machine learning practitioners. Traditional Machine learning libraries do not quite support, processing huge datasets. Parallelization using modern parallel computing frameworks, such as MapReduce, have gained popularity and adoption; this has resulted in the birth of new libraries that are built over these frameworks.</p><p>The concentration was on methods that are suitable for massive data, and have potential for the parallel implementation. The landscape of Machine learning applications has changed dramatically in the last decade. Throwing more machines doesn't always prove to be a solution. There is a need to revisit traditional algorithms and models in the way they are being executed as now an another dimension in the study of Machine learning techniques is the scalability, parallel execution, load balancing, fault tolerance, and dynamic scheduling.</p><p>We have also taken a look at the emerging parallelization and distribution architectures and frameworks in the context of large datasets, and understood the need for scaling up and scaling out Machine learning. Furthermore, we have recapped the internals of some parallel and distributed platform techniques for Machine learning such as MapReduce, GPUs, FGPA, and more.</p><p>In the next chapter, we will look at how Hadoop is the best platform for large-scale Machine learning.</p></div></body></html>