<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Resampling Methods</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft3">In this chapter, we will be introduced to the fundamental concept of sampling. We'll also learn about resampling and why it's important.</p>
<p class="CDPAlignLeft3">Sampling is the process of selecting a subset of observations from the population with the purpose of estimating some parameters about the whole population. Resampling methods, on the other hand, are used to improve the estimates of the population parameters.</p>
<p class="calibre2"><span class="calibre5">In this chapter, we will cover the following recipes:</span></p>
<ul class="calibre10">
<li class="calibre11">Introduction to sampling</li>
<li class="calibre11">k-fold and leave-one-out cross-validation</li>
<li class="calibre11">Bootstrap sampling</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to sampling</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft3"><span class="calibre5">Sampling techniques can be broadly classified into non-probability sampling techniques and probability sampling techniques. Non-probability sampling techniques are based on the judgement of the user, whereas in probability sampling, the observations are selected by chance. </span></p>
<p class="CDPAlignLeft3">Probability sampling most often includes <strong class="calibre4">simple random sampling (SRS)</strong>, stratified sampling, and systematic sampling:</p>
<ul class="calibre10">
<li class="CDPAlignLeft4"><strong class="calibre1">SRS</strong>: In SRS, each observation in the population has an equal probability of being chosen for the sample.</li>
<li class="CDPAlignLeft4"><strong class="calibre1">Stratified sampling</strong>: <span><span>In stratified sampling, the population data is divided into separate groups, called <strong class="calibre1">strata</strong>. A probability sample is then drawn from each group.</span></span></li>
<li class="CDPAlignLeft4"><strong class="calibre1">Systematic sampling</strong>: In this method, a sample is drawn from the population by choosing observations at regular intervals.</li>
</ul>
<div class="packtinfobox"><span>If the sample is too small or too large, it may lead to incorrect findings. For this reason, it's important that we've got the right sample size. A well-designed sample can help identify the biasing factors that can skew the accuracy and reliability of the expected outcome.</span></div>
<p class="calibre2">Errors might be introduced to our samples for a variety of reasons. An error might occur due to random sampling, for example, which is known as a <strong class="calibre4">sampling error</strong>, or because the method of drawing observations causes the samples to be skewed, which is known as <strong class="calibre4">sample bias</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft3">In <a href="2dbc8735-7eef-42ba-8b19-5644632c3197.xhtml" class="calibre9">Chapter 1</a>, <em class="calibre13">Get Closer to your Data</em><em class="calibre13">,</em> we manipulated and prepared the data from the <kbd class="calibre12">HousePrices.csv</kbd> file and dealt with the missing values. In this example, we're going to use the final dataset to demonstrate these sampling and resampling techniques.</p>
<p class="CDPAlignLeft3">You can get the prepared dataset from the GitHub.</p>
<p class="calibre2">We'll import the required libraries. We'll read the data and take a look at the dimensions of our dataset:</p>
<pre class="calibre18"># import os for operating system dependent functionalities<br class="title-page-name"/>import os<br class="title-page-name"/><br class="title-page-name"/># import other required libraries<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/><br class="title-page-name"/># Set your working directory according to your requirement<br class="title-page-name"/>os.chdir(".../Chapter 3/Resampling Methods")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2">Let's read our data. We'll prefix the DataFrame name with <kbd class="calibre12">df_</kbd> to make it easier to understand:</p>
<pre class="calibre18">df_housingdata = pd.read_csv("Final_HousePrices.csv")</pre>
<p class="calibre2">In the next section, we'll look at how to use <kbd class="calibre12">train_test_split()</kbd> from <kbd class="calibre12">sklean.model_selection</kbd> to split our data into random training and testing subsets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we have read our dataset, let's look at how to do the sampling:</p>
<ol class="calibre14">
<li class="calibre11">We check the dimensions of our <span>DataFrame,</span> as follows:</li>
</ol>
<pre class="calibre18">df_housingdata.shape</pre>
<p class="calibre20"><span class="calibre5"><span class="calibre5">We can see the dimension of our DataFrame:</span></span></p>
<p class="CDPAlignCenter"><img class="aligncenter30" src="assets/2a52b784-69a5-4a03-a9cf-8b2c4bd9f643.png"/></p>
<ol start="2" class="calibre14">
<li class="calibre11">We then look to see if our <span>DataFrame</span> has any missing values:</li>
</ol>
<pre class="calibre18">df_housingdata.isnull().sum()</pre>
<p class="calibre20">We notice that there are no missing values in <kbd class="calibre12">df_housingdata</kbd></p>
<ol start="3" class="calibre14">
<li class="calibre11">We separate the predictor and response variable into two different <span>DataFrames,</span> as follows:</li>
</ol>
<pre class="calibre18"># create feature &amp; response variables<br class="title-page-name"/>X = df_housingdata.iloc[:,0:79]<br class="title-page-name"/>Y = df_housingdata['SalePrice']</pre>
<ol start="4" class="calibre14">
<li class="calibre11">We split both our predictor and our response datasets into training and testing subsets using <kbd class="calibre12">train_test_split()</kbd>:</li>
</ol>
<pre class="calibre18"># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3)</pre>
<ol start="5" class="calibre14">
<li class="calibre11">We can find the number of observations and columns in each subset as follows:</li>
</ol>
<pre class="calibre18">print(X_train.shape)<br class="title-page-name"/>print(Y_train.shape)<br class="title-page-name"/>print(X_test.shape)<br class="title-page-name"/>print(Y_test.shape)</pre>
<p class="calibre2">We can see that 70% of the data has been allocated to the training dataset and 30% has been allocated to the testing dataset:</p>
<p class="CDPAlignCenter"><img src="assets/794a1517-b761-4d47-9a3c-0eee716977f7.png" class="calibre29"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft3">In <em class="calibre13">Step 1</em> and <em class="calibre13">Step 2</em>, we looked at the dimensions of our DataFrame and found that our dataset had no missing values. In <em class="calibre13">Step 3</em>, we separated out the features and the response variable. In <em class="calibre13">Step 4</em>, we used the <kbd class="calibre12">train_test_split()</kbd> function from <kbd class="calibre12">sklearn.model_selection</kbd> <span class="calibre5">to split our data and create the training and testing subsets. Notice that we passed two parameters, <kbd class="calibre12">train_size</kbd> and <kbd class="calibre12">test_size</kbd></span>, <span class="calibre5">and set the values to <kbd class="calibre12">0.7</kbd> and <kbd class="calibre12">0.3</kbd>, respectively. <kbd class="calibre12">train_size</kbd> and <kbd class="calibre12">test_size</kbd> can take values between 0.0 and 1.0, which represent the proportion of the dataset allocated to each. If an integer value is provided, the number represents the absolute number of observations.</span></p>
<div class="packtinfobox">We can choose not to provide either of the two parameters, that is <kbd class="calibre19">train_size</kbd> or <kbd class="calibre19">test_size</kbd>. If we set the value of the <span><kbd class="calibre19">train_size</kbd> </span>to <kbd class="calibre19">None</kbd> or if we do not provide it at all, <span>the value is automatically set to complement the test size. Similarly, if <kbd class="calibre19">test_size</kbd> is unspecified or we set its value to <kbd class="calibre19">None</kbd>, the value is automatically set to complement the train size. </span></div>
<p class="calibre2">In <em class="calibre13">Step 5</em>, we looked at the shape of the subsets that were created by the <kbd class="calibre12">train_test_split()</kbd> function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this example, we're going to use a dataset in which we measure a dichotomous categorical target variable. It's important to understand that the distribution of both classes of our target variable is similar in both the training and testing subsets:</p>
<ol class="calibre14">
<li class="calibre11">We start by reading our dataset and looking at its dimensions:</li>
</ol>
<pre class="calibre18">df_creditcarddata = pd.read_csv("creditcarddefault.csv")<br class="title-page-name"/>df_creditcarddata.shape</pre>
<p class="calibre20">We have 30,000 observations with 25 variables. The last variable, the default payment next month, is our target variable, which has values that are either <em class="calibre13">0</em> or <em class="calibre13">1</em>.</p>
<ol start="2" class="calibre14">
<li class="calibre11">We separate our data into a feature set and the response variable and split it into training and testing subsets using the following code:</li>
</ol>
<pre class="calibre18"># create feature &amp; response set<br class="title-page-name"/>X = df_creditcarddata.iloc[:,0:24]<br class="title-page-name"/>Y = df_creditcarddata['default payment next month']<br class="title-page-name"/><br class="title-page-name"/># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3, stratify=Y)</pre>
<p class="calibre2">Note that, this time, we've used a parameter, <kbd class="calibre12">stratify</kbd>, in our <kbd class="calibre12">train_test_split()</kbd> function. <span class="calibre5">The <kbd class="calibre12">stratify</kbd> parameter makes a split so that the proportion of values in the sample that's produced is equal to the proportion of values in the variable that's provided to it</span><span class="calibre5">. Also n</span>ote that we've assigned the response variable, <kbd class="calibre12">Y</kbd>, to the <kbd class="calibre12">stratify</kbd> parameter.</p>
<p class="calibre2">We can now see the distribution of our dichotomous class in our target variable for both the training and testing subsets:</p>
<pre class="calibre15">print(pd.value_counts(Y_train.values)*100/Y_train.shape)<br class="title-page-name"/>print(pd.value_counts(Y_test.values)*100/Y_test.shape)</pre>
<p class="calibre2">In the following output, we can see that the distributions of both the classes are the same in both subsets:</p>
<p class="CDPAlignCenter"><img class="aligncenter31" src="assets/949d0716-b0f3-4781-bacb-b0188c5d69c6.png"/></p>
<div class="packttip">We can also pass another parameter, <kbd class="calibre19">shuffle</kbd>, to <kbd class="calibre19">train_test_split()</kbd>. This takes a Boolean value, <kbd class="calibre19">True</kbd> or <kbd class="calibre19">False</kbd>, to indicate <span>whether or not to shuffle the data before splitting it. If <kbd class="calibre19">shuffle=False</kbd></span>, <span>then <kbd class="calibre19">stratify</kbd> must be <kbd class="calibre19">None</kbd>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">The scikit-learn guide to <kbd class="calibre12">sklearn.model_selection</kbd>: <a href="https://bit.ly/2px08Ii" class="calibre9">https://bit.ly/2px08Ii</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">k-fold and leave-one-out cross-validation</h1>
                </header>
            
            <article>
                
<div class="title-page-name">
<p class="calibre2">Machine learning models often face the problem of generalization when they're applied to unseen data to make predictions. To avoid this problem, the model isn't trained using the complete dataset. Instead, the dataset is split into training and testing subsets. The model is trained on the training data and evaluated on the testing set, which it doesn't see during the training process. This is the fundamental idea behind cross-validation.</p>
<p class="calibre2">The simplest kind of cross-validation is the holdout method, which we saw in the previous recipe, <em class="calibre13">Introduction to sampling</em>. In the holdout method, when we split our data into training and testing subsets, there's a possibility that the testing set isn't that similar to the training set because of the high dimensionality of the data. This can lead to instability in the outcome. For this reason, it's very important that we sample our data efficiently. We can solve this problem using other cross-validation methods such as <strong class="calibre4">leave-one-out cross-validation</strong> (<strong class="calibre4">LOOCV</strong>) or <strong class="calibre4">k-fold cross-validation</strong> (<strong class="calibre4">k-fold CV</strong>).</p>
<p class="calibre2">k-fold CV is a widely used approach that's used for estimating test errors. <span class="calibre5">The original dataset with <em class="calibre13">N</em> observations is divided into </span><em class="calibre13">K</em><span class="calibre5"> subsets and the holdout method is repeated </span><em class="calibre13">K</em><span class="calibre5"> times. In each iteration, <em class="calibre13">K-1</em> subsets are used as the training set and the rest are used as the testing set. The error is calculated as follows:</span></p>
<p class="CDPAlignCenter"><img class="aligncenter32" src="assets/35929789-baf8-4ee5-9649-8d5a4251c834.png"/></p>
</div>
<div class="title-page-name">
<p class="calibre2"><span class="calibre5">In LOOCV, the number of subsets <em class="calibre13">K</em> is equal to the number of observations in the dataset, <em class="calibre13">N</em>. LOOCV uses one observation from the original dataset as the validation set and the remaining <em class="calibre13">N-1</em> observations as the training set. This is iterated <em class="calibre13">N</em> times, so that each observation in the sample is used as the validation data in each iteration. This is the same as k-fold CV, in which <em class="calibre13">K</em> equals <em class="calibre13">N</em>, the number of data points in the set. LOOCV usually takes a lot of computational power because of the large number of iterations required.</span></p>
</div>
<div class="packtinfobox"><span>In LOOCV, the estimates from each fold are highly correlated and their average can have a high level of variance. </span></div>
<div class="title-page-name">
<p class="calibre2">Estimating the test error is based on a single observation and is represented as <em class="calibre13">MSE =</em> <img class="fm-editor-equation6" src="assets/c3d1a06d-4629-40a7-8e98-9201740fd95e.png"/>. We can compute the average of the MSEs for all the folds as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation7" src="assets/247c3c78-5e6a-4fe0-a069-bb90cd6c73c7.png"/></p>
<p class="calibre2">This calculation is no different from the calculation involved in k-fold CV. We'll use scikit-learn libraries to see how techniques such as k-fold CV and LOOCV can be implemented.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<div class="title-page-name">
<p class="calibre2">In the following code block, we can see how we can import the required libraries:</p>
<pre class="calibre18">import pandas as pd<br class="title-page-name"/><br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/>from sklearn.linear_model import LinearRegression<br class="title-page-name"/>from sklearn.metrics import mean_squared_error, r2_score<br class="title-page-name"/>from sklearn.model_selection import KFold <br class="title-page-name"/>import matplotlib.pyplot as plt</pre>
<p class="calibre2">We read our data and split the features and the response variable:</p>
</div>
<pre class="calibre18"># Let's read our data. <br class="title-page-name"/>df_autodata = pd.read_csv("autompg.csv")<br class="title-page-name"/><br class="title-page-name"/># Fill NAs with the median value<br class="title-page-name"/>df_autodata['horsepower'].fillna(df_autodata['horsepower'].median(), inplace=True)<br class="title-page-name"/><br class="title-page-name"/># Drop carname variable<br class="title-page-name"/>df_autodata.drop(['carname'], axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/>X = df_autodata.iloc[:,1:8]<br class="title-page-name"/>Y = df_autodata.iloc[:,0]<br class="title-page-name"/>X=np.array(X)<br class="title-page-name"/>Y=np.array(Y)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2">The k-folds cross-validator <span class="calibre5">provides us with the train and test indices to split the data into training and testing subsets:</span></p>
<ol class="calibre14">
<li class="calibre11"><span>We'll split the dataset into <em class="calibre23">K</em> consecutive folds (without shuffling by default) with <em class="calibre23">K=10</em>:</span></li>
</ol>
<pre class="calibre18">kfoldcv = KFold(n_splits=10)<br class="title-page-name"/>kf_ytests = []<br class="title-page-name"/>kf_predictedvalues = []<br class="title-page-name"/>mean_mse = 0.0<br class="title-page-name"/><br class="title-page-name"/>for train_index, test_index in kfoldcv.split(X):<br class="title-page-name"/>    X_train, X_test = X[train_index], X[test_index] <br class="title-page-name"/>    Y_train, Y_test = Y[train_index], Y[test_index]<br class="title-page-name"/>    <br class="title-page-name"/>    model = LinearRegression()<br class="title-page-name"/>    model.fit(X_train, Y_train) <br class="title-page-name"/>    Y_pred = model.predict(X_test)<br class="title-page-name"/>        <br class="title-page-name"/>    # there is only one y-test and y-pred per iteration over the kfoldcv.split, <br class="title-page-name"/>    # so we append them to the respective lists.<br class="title-page-name"/>        <br class="title-page-name"/>    kf_ytests += list(Y_test)<br class="title-page-name"/>    kf_predictedvalues += list(Y_pred)<br class="title-page-name"/>    mse = mean_squared_error(kf_ytests, kf_predictedvalues)<br class="title-page-name"/>    r2score = r2_score(kf_ytests, kf_predictedvalues)<br class="title-page-name"/>    print("R^2: {:.2f}, MSE: {:.2f}".format(r2score, mse))<br class="title-page-name"/>    mean_mse += mse   </pre>
<ol start="2" class="calibre14">
<li class="calibre11">We can look at our coefficient of determination using <kbd class="calibre12">r2_score()</kbd> and the mean squared error using <kbd class="calibre12">mse()</kbd>:</li>
</ol>
<pre class="calibre18">print("Average CV Score :" ,mean_mse/10) </pre>
<p class="calibre2"/>
<p class="calibre20">The results of the preceding code are as follows:</p>
<p class="CDPAlignCenter"><img class="aligncenter33" src="assets/635b6459-6f3f-4046-8923-bf567ae9a1d9.png"/></p>
<ol start="3" class="calibre14">
<li class="calibre11">We plot the predicted values against the actual values of the response variable:</li>
</ol>
<pre class="calibre18">## Let us plot the model<br class="title-page-name"/>plt.scatter(kf_ytests, kf_predictedvalues)<br class="title-page-name"/>plt.xlabel('Reported mpg')<br class="title-page-name"/>plt.ylabel('Predicted mpg')</pre>
<p class="calibre20">The plot generated by the preceding code is as follows:</p>
<p class="CDPAlignCenter"><img class="aligncenter34" src="assets/1d3bbb0c-ad84-4502-857a-ec195573f3db.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, the k-fold cross validator splits the dataset into <em class="calibre13">K</em> consecutive folds with <em class="calibre13">K</em>=10. The k-fold cross-validator provides us with the train and test indices and then splits the data into training and testing subsets. In <em class="calibre13">Step</em> 2, we looked at the coefficient of determination using <kbd class="calibre12">r2_score()</kbd> and the mean squared error using <kbd class="calibre12">mse()</kbd>. The coefficient of determination and the mean squared error are 79% and 12.85, respectively. In <em class="calibre13">Step 3</em>, we plotted the predicted values against the actual values of the response variable, <kbd class="calibre12">mpg</kbd>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2">We'll now do the same exercise with LOOCV by using <kbd class="calibre12">LeaveOneOut</kbd> from <kbd class="calibre12">sklearn.model_selection</kbd>:</p>
<ol start="1" class="calibre14">
<li class="calibre11">We'll read our data once again and split it into the features and response sets:</li>
</ol>
<pre class="calibre18"># Let's read our data. <br class="title-page-name"/>df_autodata = pd.read_csv("autompg.csv")<br class="title-page-name"/><br class="title-page-name"/># Fill NAs with the median value<br class="title-page-name"/>df_autodata['horsepower'].fillna(df_autodata['horsepower'].median(), inplace=True)<br class="title-page-name"/><br class="title-page-name"/># Drop carname variable<br class="title-page-name"/>df_autodata.drop(['carname'], axis=1, inplace=True)<br class="title-page-name"/><br class="title-page-name"/>X = df_autodata.iloc[:,1:8]<br class="title-page-name"/>Y = df_autodata.iloc[:,0]<br class="title-page-name"/>X=np.array(X)<br class="title-page-name"/>Y=np.array(Y)</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We use <span>LOOCV</span> to build our models:</li>
</ol>
<pre class="calibre18">from sklearn.model_selection import LeaveOneOut <br class="title-page-name"/>loocv = LeaveOneOut()<br class="title-page-name"/><br class="title-page-name"/>loo_ytests = []<br class="title-page-name"/>loo_predictedvalues = []<br class="title-page-name"/>mean_mse = 0.0<br class="title-page-name"/><br class="title-page-name"/>for train_index, test_index in loocv.split(X):<br class="title-page-name"/>    # the below requires arrays. So we converted the dataframes to arrays<br class="title-page-name"/>    X_train, X_test = X[train_index], X[test_index] <br class="title-page-name"/>    Y_train, Y_test = Y[train_index], Y[test_index]<br class="title-page-name"/>    <br class="title-page-name"/>    model = LinearRegression()<br class="title-page-name"/>    model.fit(X_train, Y_train) <br class="title-page-name"/>    Y_pred = model.predict(X_test)<br class="title-page-name"/>        <br class="title-page-name"/>    # there is only one y-test and y-pred per iteration over the loo.split, <br class="title-page-name"/>    # so we append them to the respective lists.<br class="title-page-name"/>        <br class="title-page-name"/>    loo_ytests += list(Y_test)<br class="title-page-name"/>    loo_predictedvalues += list(Y_pred)<br class="title-page-name"/>    <br class="title-page-name"/>    mse = mean_squared_error(loo_ytests, loo_predictedvalues)<br class="title-page-name"/>    r2score = r2_score(loo_ytests, loo_predictedvalues)<br class="title-page-name"/>    print("R^2: {:.2f}, MSE: {:.2f}".format(r2score, mse))<br class="title-page-name"/>    mean_mse += mse </pre>
<ol start="3" class="calibre14">
<li class="calibre11">We can look at our coefficient of determination using<span> </span><kbd class="calibre12">r2_score()</kbd><span> </span>and the mean squared error using<span> </span><kbd class="calibre12">mse()</kbd>:</li>
</ol>
<pre class="calibre18">print("Average CV Score :" ,mean_mse/X.shape[0]) </pre>
<p class="calibre20">We can take a look at the coefficient of determination, and the mean squared error for the LOOCV results:</p>
<p class="CDPAlignCenter"><img class="aligncenter35" src="assets/0bc1d360-d30b-4a3b-b22b-16fc5935b2ce.png"/></p>
<ol start="4" class="calibre14">
<li class="calibre11">We can plot the predicted values against the actual values of the response variable:</li>
</ol>
<pre class="calibre18">## Let us plot the model<br class="title-page-name"/>plt.scatter(kf_ytests, kf_predictedvalues)<br class="title-page-name"/>plt.xlabel('Reported mpg')<br class="title-page-name"/>plt.ylabel('Predicted mpg')</pre>
<p class="calibre20">The plot that is generated by the preceding code gives us the following output:</p>
<p class="CDPAlignCenter"><img class="aligncenter36" src="assets/969f0588-1f82-4dde-8732-f3a96c8f59a4.png"/></p>
<p class="calibre2"><span class="calibre5">In LOOCV, there is no randomness in the splitting method, so it'll always provide you with the same result.</span></p>
<p class="calibre2">The stratified k-fold CV method is often used in classification problems. This is a variation of the k-fold CV method that returns stratified folds. Each set contains a similar percentage of samples of each target class as the original dataset. <kbd class="calibre12">startifiedShuffleSplit</kbd> is a variation of shuffle splits, which creates splits by maintaining the same percentage for every target class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">The scikit-learn guide to other methods of cross-validation: <a href="https://bit.ly/2px08Ii" class="calibre9">https://bit.ly/2px08Ii</a></li>
</ul>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bootstrapping</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Bootstrapping is based on the jackknife method, which was proposed by </span>Quenouille in 1949, and then refined by Tukey in 1958. The jackknife method is used for testing hypotheses and estimating confidence intervals. It's obtained by calculating the estimate after leaving out each observation and then computing the average of these calculations. With a sample of size <em class="calibre13">N</em>, the jackknife estimate can be found by aggregating the estimates of every <em class="calibre13">N-1</em> sized sub-sample. It's similar to bootstrap samples, but while the bootstrap method is sampling with replacement, the jackknife method samples the data without replacement.</p>
<p class="calibre2"><span class="calibre5">Bootstrapping is a powerful, non-parametric resampling technique that's used to assess the uncertainty in the estimator. In bootstrapping, a large number of samples with the same size are drawn repeatedly from an original sample. This allows a given observation to be included in more than one sample, which is known as <strong class="calibre4">sampling with replacement</strong>. </span><span class="calibre5">In the bootstrap method, <em class="calibre13">n</em> samples are created from the original data by sampling with replacement. Each sample is of identical size. </span><span class="calibre5">The larger <em class="calibre13">n</em>, the closer the set of samples will be to the ideal bootstrap sample.</span></p>
<div class="packtquote1">"The essence of bootstrapping is the idea that in the absence of any other knowledge about a population, the distribution of values found in a random sample of size n from the population is the best guide to the distribution in the population. Therefore to approximate what would happen if the population was resampled, it's sensible to resample the sample. In other words, the infinite population that consists of the n observed sample values, each with probability 1/n, is used to model the unknown real population."</div>
<div class="packtquote2">–Bryan F. J. Manly</div>
<p class="calibre2">A diagrammatic representation of a bootstrap sample would look as follows:</p>
<p class="CDPAlignCenter"><img class="aligncenter37" src="assets/46b71607-5c54-458b-a8f9-15f877441fdd.png"/></p>
<p class="calibre2"><span class="calibre5">As we can see in the preceding diagram, some of the data points in the <strong class="calibre4">S1</strong> subset also appear in <strong class="calibre4">S2</strong> and <strong class="calibre4">S4</strong>. </span></p>
<p class="calibre2">Let's say that we have <em class="calibre13">n</em> bootstrap samples from our original sample. <img class="fm-editor-equation8" src="assets/f0c73234-b84a-449c-a947-9dd4a49c0086.png"/> denotes the estimates of the <em class="calibre13">n</em> bootstrap samples where <em class="calibre13">i=1,2,3...,n</em>. If <img class="fm-editor-equation2" src="assets/0a211672-b39b-48fd-9297-9052c060f949.png"/> denotes the estimate of the parameter for the original sample, the standard error for <img class="fm-editor-equation2" src="assets/f1a80abe-c907-4179-871f-e64341fd5190.png"/><span class="calibre5"> is given as follows:</span></p>
<p class="CDPAlignCenter"><img class="fm-editor-equation9" src="assets/51c33b62-2975-455f-b6a0-220137c1667c.png"/></p>
<p class="calibre2"><img class="fm-editor-equation10" src="assets/565f5de0-0f0c-45fa-828d-f9f32d366603.png"/><span class="calibre5"> is given as follows:</span></p>
<p class="CDPAlignCenter"><img class="fm-editor-equation11" src="assets/8efea2c2-bddf-4d60-ace9-2ef3f4ac5708.png"/></p>
<p class="CDPAlignLeft3"> <span class="calibre5"><img class="fm-editor-equation10" src="assets/565f5de0-0f0c-45fa-828d-f9f32d366603.png"/> is the mean of the estimates across the <em class="calibre13">n</em> bootstrap samples.</span></p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">We need to import the required libraries as usual. This time, we will use the <kbd class="calibre12">resample</kbd> class from <kbd class="calibre12">sklean.utils</kbd>, which we've not used previously:</p>
<pre class="calibre18">import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/>from sklearn.linear_model import SGDRegressor<br class="title-page-name"/>from sklearn.metrics import mean_squared_error, r2_score<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/><br class="title-page-name"/>from sklearn.utils import resample</pre>
<p class="calibre2">We load our data and fill in the missing values with the median for the <kbd class="calibre12">horsepower</kbd> variable. We also drop the <kbd class="calibre12">carname</kbd> variable:</p>
<pre class="calibre18"># Let's read our data. We prefix the data frame name with "df_" for easier understanding.<br class="title-page-name"/>df_autodata = pd.read_csv("autompg.csv")<br class="title-page-name"/>df_autodata['horsepower'].fillna(df_autodata['horsepower'].median(), inplace=True)<br class="title-page-name"/>df_autodata.drop(['carname'], axis=1, inplace=True)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we have read our data, let's see how we can perform bootstrap sampling:</p>
<ol start="1" class="calibre14">
<li class="calibre11">We write a custom function, <kbd class="calibre12">create_bootstrap_oob()</kbd>, which takes a <span>DataFrame</span> as a parameter and uses the <kbd class="calibre12">resample()</kbd> function from <kbd class="calibre12">sklearn.utils</kbd> to create a bootstrap sample with 100 observations:</li>
</ol>
<pre class="calibre18"># This custom function takes a dataframe as an argument<br class="title-page-name"/>def create_bootstrap_oob(df):<br class="title-page-name"/>    global df_OOB<br class="title-page-name"/>    global df_bootstrap_sample <br class="title-page-name"/>    <br class="title-page-name"/>    # creating the bootstrap sample<br class="title-page-name"/>    df_bootstrap_sample = resample(df, replace=True, n_samples=100)<br class="title-page-name"/>    <br class="title-page-name"/>    # creating the OOB sample <br class="title-page-name"/>    bootstrap_sample_index = tuple(df_bootstrap_sample.index)<br class="title-page-name"/>    bootstrap_df = df.index.isin(bootstrap_sample_index)<br class="title-page-name"/>    df_OOB = df[~bootstrap_df]</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We loop through 50 iterations and call the custom function by passing the <kbd class="calibre12">df_autodata</kbd> DataFrame. We capture the mean of the <kbd class="calibre12">mpg</kbd> variable for each bootstrap sample, which we'll measure against the mean of the <kbd class="calibre12">mpg</kbd> variable in our original <span>DataFrame</span>, which is <kbd class="calibre12"><span>df_autodata</span></kbd>:</li>
</ol>
<pre class="calibre18">iteration=50<br class="title-page-name"/>bootstap_statistics=list()<br class="title-page-name"/>originalsample_statistics=list()<br class="title-page-name"/><br class="title-page-name"/>for i in range(iteration):<br class="title-page-name"/>    # Call custom function create_bootstrap_oob(). Pass df_autodata<br class="title-page-name"/>    create_bootstrap_oob(df_autodata)<br class="title-page-name"/>    <br class="title-page-name"/>    # Capture mean value of mpg variable for all bootstrap samples<br class="title-page-name"/>    bootstap_statistics.append(df_bootstrap_sample.iloc[:,0].mean())<br class="title-page-name"/>    <br class="title-page-name"/>    originalsample_statistics.append(df_autodata['mpg'].mean())</pre>
<ol start="3" class="calibre14">
<li class="calibre11">We plot the mean of the <kbd class="calibre12">mpg</kbd> variable for each iteration, for which a separate bootstrap sample has been considered. We capture the mean of the <kbd class="calibre12">mpg</kbd> variable for <span>each bootstrap sample in each iteration:</span></li>
</ol>
<pre class="calibre18">import matplotlib.pyplot as plt<br class="title-page-name"/>f, ax= plt.subplots(figsize=(6,6))<br class="title-page-name"/><br class="title-page-name"/>plt.plot(bootstap_statistics, 'c--', label='Bootstrap Sample Statistic')<br class="title-page-name"/>plt.plot(originalsample_statistics, 'grey', label='Original Sample Statistic')<br class="title-page-name"/>plt.xlabel('Iterations')<br class="title-page-name"/>plt.ylabel('Statistic (Mean of mpg)')<br class="title-page-name"/>plt.legend(loc=4)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We finally plot the mean of the <kbd class="calibre12">mpg</kbd> variable against each iteration, which can be seen in the following image:</p>
<p class="CDPAlignCenter"><img class="aligncenter38" src="assets/179ce760-d743-439f-8db3-2cb4d4d1bc97.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, we created a custom function, <kbd class="calibre12">create_bootstrap_oob( )</kbd>, and used the <kbd class="calibre12">resample()</kbd> function from <kbd class="calibre12">sklearn.utils</kbd> to create a bootstrap sample with 100 observations. The <kbd class="calibre12"><span>create_bootstrap_oob( )</span></kbd> <span class="calibre5">custom function </span><span class="calibre5">took a DataFrame as an input parameter and created both bootstrap and <strong class="calibre4">Out-Of-the-Bag</strong> (<strong class="calibre4">OOB</strong>) samples.</span></p>
<div class="packttip"><span>We mentioned that bootstrap sampling is sampling with replacement. This means that any given observation can appear more than once in a single sample.</span></div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">In <em class="calibre13">Step 2</em>, we looped through the 50 iterations and called the <kbd class="calibre12"><span>create_bootstrap_oob( )</span></kbd> <span class="calibre5">custom function </span><span class="calibre5">by passing</span> <kbd class="calibre12">df_autoframe</kbd><span class="calibre5">. We captured the mean of the</span> <kbd class="calibre12">mpg</kbd> <span class="calibre5">variable for each bootstrap sample. </span>In <em class="calibre13">Step 3</em>, we considered a separate bootstrap sample for each iteration. We captured the mean of the <kbd class="calibre12">mpg</kbd> variable against each iteration and then <span class="calibre5">plotted the mean of the </span><kbd class="calibre12">mpg</kbd><span class="calibre5"> variable for each iteration. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">The scikit-learn guide to <kbd class="calibre12">sklearn.cross_validation.Bootstrap</kbd>: <a href="https://bit.ly/2RC5MYv" class="calibre9">https://bit.ly/2RC5MYv</a></li>
</ul>


            </article>

            
        </section>
    </body></html>