<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Forecasting with Regression Algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we're going to take a brief look at forecasting using regression algorithms. We'll additionally discuss time-series analysis and how we can use techniques from digital-signal processing to aid in our analysis. By the end of the chapter, you will have seen a number of patterns commonly found in time-series and continuous-valued data and will have an understanding of which types of regressions fit on which types of data. Additionally, you will have learned a few digital signal processing techniques, such as filtering, seasonality analysis, and Fourier transformations.</p>
<p>Forecasting is a very broad concept that covers many types of tasks. This chapter will provide you with an initial toolbox of concepts and algorithms that apply broadly to time-series data. We will focus on the fundamentals, and discuss the following topics:</p>
<ul>
<li>Regression versus classification</li>
<li>Regression basics</li>
<li>Linear, exponential, and polynomial regression</li>
<li>Time-series analysis basics</li>
<li>Low-pass and high-pass filtering</li>
<li>Seasonality and subtractive analysis</li>
<li>Fourier analysis</li>
</ul>
<p>These concepts build an essential toolbox that you can use when working with real-world forecasting and analysis problems. There are many other tools that apply to specific situations, but I consider these topics to be the absolute essentials.</p>
<p>Let's begin by looking at the similarities—and differences—between regression and classification in <strong>machine learning</strong> (<strong>ML</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression versus classification</h1>
                </header>
            
            <article>
                
<p>Much of this book has been involved with classification tasks, where the objective of the analysis is to fit a data point to one of a number of predefined classes or labels. When classifying data, you are able to judge your algorithm's accuracy by comparing predictions to true values; a guessed label is either correct or incorrect. In classification tasks, you can often determine the likelihood or probability that a guessed label fits the data, and you typically choose the label with the maximum likelihood.</p>
<p>Let's compare and contrast classification tasks to regression tasks. Both are similar in that the ultimate goal is to make a prediction, informed by prior knowledge or data. Both are similar in that we want to create some kind of function or logic that maps input values to output values, and make that mapping function both as accurate and as generalized as possible. However, the major difference between regression and classification is that in regression, your goal is to determine the <em>quantity</em> of a value rather than its label.</p>
<p>Imagine you have historical data about the processing load over time on a server that you manage. This data is <em>time-series</em>, because the data evolves over time. The data is also <em>continuous</em> (as opposed to <em>discrete</em>), because the output values can be any real number: 1, or 2.3, or 2.34353, and so on. The goal in time-series analysis or regression analysis is not to label the data, but rather to predict what your server load will be next Thursday evening at 20:15 p.m., for instance. To accomplish this goal, you must analyze the time-series data and attempt to extract patterns from it, and then use those patterns to make a future prediction. Your prediction will also be a real and continuous number, such as <em>I predict the server load next Thursday night will be 2.75</em>.</p>
<p>In classification tasks, you can judge the accuracy of your algorithm by comparing predictions to true values and counting how many predictions were correct or incorrect. Because regression tasks involve themselves with continuous values, one cannot simply determine whether a prediction was correct or incorrect. If you predict that server load will be 2.75 and it ends up truly being 2.65, can you say that the prediction was correct? Or incorrect? What about if it ends up being 2.74? When classifying <em>spam</em> or <em>not spam</em>, you either get the prediction right or you get it wrong. When you compare continuous values, however, you can only determine how close you got the prediction and therefore must use some other metric to define the accuracy of your algorithm.</p>
<p>In general, you will use a different set of algorithms to analyze continuous or time-series data than you would use for classification tasks. However, there are some ML algorithms that can handle both regression and classification tasks with minor modifications. Most notably, decision trees, random forests, and neural networks can all be used both for classification and regression tasks.</p>
<p>In this chapter, we will look at the following concepts:</p>
<ul>
<li>Least-squares regressions techniques, such as linear regression, polynomial regression, power law regression, and others</li>
<li>Trend analysis or smoothing</li>
<li>Seasonality analysis or pattern subtraction</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression basics</h1>
                </header>
            
            <article>
                
<p>When performing regression analysis, there are two primary and overarching goals. First, we want to determine and identify any underlying, systemic patterns in the data. If we can identify the systemic patterns, we may be able to identify the phenomena underlying the patterns and develop a deeper understanding of the system as a whole. If, through your analysis, you find that there is a pattern that repeats itself every 16 hours, you will be in a much better position to figure out what phenomenon is causing the pattern and take action. As with all ML tasks, that 16-hour pattern may be buried deep within the data and may not be identifiable at a glance.</p>
<p>The second major goal is to use the knowledge of the underlying patterns to make future predictions. The predictions that you make will only be as good as the analysis that powers the predictions. If there are four different systemic patterns in your data, and you've only identified and modeled three of them, your predictions may be inaccurate since you haven't fully modeled the real-world phenomena involved.</p>
<p>Achieving both of these goals relies on your ability to identify and formally (that is, mathematically) describe the patterns and phenomena. In some cases, you may not be able to fully identify the root cause of a pattern; even then, if the pattern is reliable and your analysis is good, you will still be able to predict the future behavior of the system even if you don't fully understand the cause. This is the case with all ML problems; ML ultimately analyzes behaviors and results—the things we can measure—but having a deep understanding of the causes can only help.</p>
<p>In all ML problems, we must also contend with noise. In classification problems, noise can take many forms, such as missing or incorrect values, or undefinable human behavior. Noise can take many forms in regression problems as well: sensors may be susceptible to environmental noise, there can be random fluctuations in the underlying processes, or noise can be caused by many small, hard-to-predict, systemic factors.</p>
<p>Noise always makes patterns more difficult to identify, whether you're performing regression or classification analysis. In regression analysis, your goal is to be able to separate the systemic behaviors (the actual patterns) from the random sources of noise in the data. In some cases, it's important to also model the noise as a behavior, because the noise itself can have a significant effect on your predictions; in other cases, the noise can be ignored.</p>
<p>To illustrate the difference between systemic patterns and noise, consider the following dataset. There are no units on the graph, as this is just a conceptual example of some dependent parameter, <em>Y</em>, that varies with some independent parameter, <em>X</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-112 image-border" src="assets/342e53f9-ac57-4b05-9286-d8b22306ed87.png" style="width:46.42em;height:28.50em;"/></div>
<p>In this example, we can clearly see the difference between the systemic pattern and the noise. The systemic pattern is the steady, linear growth—the Y-values generally increase as the X-values increase, despite some fluctuations from point to point due to noise. By modeling the systemic pattern in this data, we would be able to make a reasonable prediction for what the Y-value will be when the X-value is 75, or 100, or -20. Whether or not the noise is significant will depend on the specific application; you can either ignore the noise, or you can model it and include it in your analysis.</p>
<p>In <a href="370a39e1-e618-475d-9722-9b4315a9a6ee.xhtml" target="_blank">Chapter 1</a>, <em>Exploring the Potential of JavaScript</em>, we learned about one technique for dealing with noise: smoothing with a moving average. Instead of graphing individual points, we can take groups of three points together and plot their averages. If the noise is truly random and distributed evenly (that is, the average of all the effects of noise comes out close to zero), a moving average will tend to cancel out some of the noise. If you are averaging three points, and the effect due to noise on each of those points adds +1, -2, and +1.2 to each point, respectively, the moving average will reduce the total effect of the noise to +0.2. When we graph the moving average, we will typically find a smoother pattern:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-113 image-border" src="assets/c0ee9697-e607-458f-9c5c-fa5dc5bd941d.png" style="width:47.75em;height:29.33em;"/></div>
<p>The moving average has reduced the effect of the noise, and helps us focus on the systemic pattern a bit more—but we are not much closer to being able to predict future values, such as when <em>X</em> is 75. The moving average only helps us reduce the effect of noise on the data points within the dataset. When you look at the Y-value when <em>X = 4</em>, for instance, the measured value is around 21 while the smoothed value is 28. In this case, the smoothed value of 28 better represents the <em>systemic pattern</em> at <em>X = 4</em>, even though the actual measured value at this point was 21. Most likely, the big difference between the measurement and the systemic pattern was caused by a significant source of random noise when this measurement was taken.</p>
<p>Be cautious when dealing with noise. It's important to recognize that, in the preceding example, the actual measured Y-value was indeed 21 at <em>X = 4</em>. The smoothed, moving average version is an idealization. It is our attempt to cut through the noise in order to see the signal, but we cannot forget that the actual measurement was significantly affected by noise. Whether this fact is important to your analysis depends significantly on the problem you are trying to solve.</p>
<p>How, then, do we approach the problem of predicting a future value of this data? The moving average may help us when <em>interpolating</em> data, but not when <em>extrapolating</em> to a future X-value. You can, of course, make a guess as to what the value will be when <em>X = 75</em>, since this example is simple and easy to visualize. However, since this is a book about ML, we can assume that real-world problems will not be so easy to analyze by eye, and we will need to introduce new tools.</p>
<p>The solution to this problem is the <em>regression.</em> As with all predictive ML problems, we want to create some kind of abstract function that can map input values to output values, and use that function to make predictions. In classification tasks, that mapping function may be a Bayesian predictor or a heuristic based on random forests. In regression tasks, the mapping function will often be a mathematical function that describes a line, or a polynomial, or some other kind of shape that fits the data well.</p>
<p>If you've ever graphed data in Excel or Google Sheets, there's a good chance you have already used linear regressions. The <em>trendline</em> feature of these programs performs a linear regression in order to determine a mapping function that best fits the data. <span>The following graph is a trendline determined by a </span><em>linear regression</em><span>, which is a type of algorithm that is used to find the mathematical line that best fits the data:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-114 image-border" src="assets/a85c0ab2-389f-42fe-b705-a3715dc8ecf3.png" style="width:48.42em;height:29.75em;"/></div>
<p>Additionally, Excel gives us another piece of information, called the <strong>R<sup>2</sup> value</strong>, which is a representation of how well the trendline fits the data. An R<sup>2</sup> value closer to 1.0 indicates that the trendline explains much of the variance between points; a low R<sup>2</sup> value indicates that the model does not explain the variance.</p>
<p>The major difference between the trendline and the moving average we saw earlier is that the trendline is an actual mathematical model. When you find a trendline with a linear regression, you will have a mathematical formula that describes the entire line. The moving average only exists where the data points exist; we can only have a moving average between X = 0 and X = 50. The trendline, on the other hand, is described by the mathematical formula for a straight line, and it extends out to infinity both to the left and to the right. If you know the formula for the trendline, you can plug in <em>any</em> value of <em>X</em> to that formula and get a prediction for the value of <em>Y</em>. If, for example, you find that the formula for a line is <em>Y = 2.5 x X + 22</em>, you can plug in <em>X = 75</em> and you will get a prediction of <em>Y = 2.5 x 75 + 22</em>, or <em>Y = 209.5</em>. There is no way to get such a prediction from a moving average.</p>
<p>Linear regression is just one type of regression algorithm, specifically used to find a straight line that fits the data. In this chapter, we will explore several other types of regression algorithms, each with a different shape. In all cases, you can use a metric that describes how well the regression fits the data. Typically, this metric will be <strong>root mean squared error</strong> (<strong>RMSE</strong>), which is the square root of the average of the squared error for each point compared to the trendline. Most regression algorithms are <em>least-squares</em> regressions, which aim to find the trendline that minimizes the RMSE.</p>
<p>Let's take a look at several examples of regression shapes and how to fit them to data in JavaScript.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 1 – linear regression</h1>
                </header>
            
            <article>
                
<p>Before we dive into the first example, let's take a minute to set up our project folder and dependencies. Create a new folder called <kbd>Ch7-Regression</kbd>, and inside that folder add the following <kbd>package.json</kbd> file:</p>
<pre>{<br/>  <span>"name"</span>: <span>"Ch7-Regression"</span>,<br/>  <span>"version"</span>: <span>"1.0.0"</span>,<br/>  <span>"description"</span>: <span>"ML in JS Example for Chapter 7 - Regression"</span>,<br/>  <span>"main"</span>: <span>"src/index.js"</span>,<br/>  <span>"author"</span>: <span>"Burak Kanber"</span>,<br/>  <span>"license"</span>: <span>"MIT"</span>,<br/>  <span>"scripts"</span>: {<br/>    <span>"build-web"</span>: <span>"browserify src/index.js -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/>    <span>"build-cli"</span>: <span>"browserify src/index.js --node -o dist/index.js -t [ babelify --presets [ env ] ]"</span>,<br/>    <span>"start"</span>: <span>"yarn build-cli &amp;&amp; node dist/index.js"<br/></span><span>  </span>},<br/>  <span>"dependencies"</span>: {<br/>    <span>"babel-core"</span>: <span>"^6.26.0"</span>,<br/>    <span>"babel-plugin-transform-object-rest-spread"</span>: <span>"^6.26.0"</span>,<br/>    <span>"babel-preset-env"</span>: <span>"^1.6.1"</span>,<br/>    <span>"babelify"</span>: <span>"^8.0.0"</span>,<br/>    <span>"browserify"</span>: <span>"^15.1.0"</span>,<br/>    <span>"dspjs"</span>: <span>"^1.0.0"</span>,<br/>    <span>"regression"</span>: <span>"^2.0.1"<br/></span><span>  </span>}<br/>}</pre>
<p>Then run the <kbd>yarn install</kbd> <span>command </span><span>from the command line to install all dependencies. Next, create a folder called</span> <kbd>src</kbd><span>, and add an empty file called</span> <kbd>index.js</kbd><span>. Finally, download the</span> <kbd>data.js</kbd> <span>file from the book's GitHub repository into the</span> <kbd>src</kbd> <span>folder.</span></p>
<p>In this example, we're going to work on the noisy linear data from the previous section. As a reminder, the data itself looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6c6d5b17-4315-459f-8800-ff764c663ad3.png" style="width:45.08em;height:27.67em;"/></div>
<p>Our goal is to find the formula for a line that fits the data and make a prediction for a future value when <em>X = 75</em>. We'll use Tom Alexander's <kbd>regression</kbd> library, which can perform a number of types of regressions and also provides the capability to make predictions based on the resultant regression.</p>
<p>In the <kbd>index.js</kbd> file, add the following import statements to the top of the file:</p>
<pre><span>import </span>* <span>as </span>data <span>from </span><span>'./data'</span>;<br/><span>import </span>regression <span>from </span><span>'regression'</span>;</pre>
<p>As with all ML problems, you should start by visualizing your data and trying to understand the overall shape of the data before you choose the algorithm. In this case, we can see that the data follows a linear trend so we will choose the linear regression algorithm.</p>
<p>In linear regression, the goal is to determine the parameters for the formula for a line that best fits the data. The formula for a straight line has the following form: <em>y = mx + b</em>, sometimes written as <em>y = ax + b</em>, where <em>x</em> is the input variable or independent variable, <em>y</em> is the target or dependent variable, <em>m</em> (or <em>a</em>) is the <em>slope</em> or <em>gradient</em> of the line, and <em>b</em> is the <em>y-intercept</em> of the line (the Y-value of the line when <em>X = 0</em>). Therefore, the minimum requirements for the output of a linear regression are the values for <em>a</em> and <em>b</em>, the only two parameters that determine the shape of the line.</p>
<p>Add the following import lines to <kbd>index.js</kbd>:</p>
<pre><span>console</span>.<span>log</span>(<span>"Performing linear regression:"</span>);<br/><span>console</span>.<span>log</span>(<span>"============================="</span>);<br/><span>const </span><span>linearModel </span>= regression.linear(data.<span>linear</span>);<br/><span>console</span>.<span>log</span>(<span>"Slope and intercept:"</span>);<br/><span>console</span>.<span>log</span>(<span>linearModel</span>.<span>equation</span>);<br/><span>console</span>.<span>log</span>(<span>"Line formula:"</span>);<br/><span>console</span>.<span>log</span>(<span>linearModel</span>.<span>string</span>);<br/><span>console</span>.<span>log</span>(<span>"R^2 fitness: " </span>+ <span>linearModel</span>.<span>r2</span>);<br/><span>console</span>.<span>log</span>(<span>"Predict X = 75: " </span>+ <span>linearModel</span>.<span>predict</span>(<span>75</span>)[<span>1</span>]);</pre>
<p>Performing a linear regression on the data will return a model; the model essentially encapsulates the values of <em>a</em> and <em>b</em>, or the slope and the intercept of the line. This particular library not only returns the line's parameters in the <kbd>linearModel.equation</kbd> property, but also gives us a string representation of the line formula, calculates the R<sup>2</sup> fit of the regression, and gives us a method, called <kbd>predict</kbd>, which we can use to plug a new X-value into the model.</p>
<p>Run the code by issuing the <kbd>yarn start</kbd> command from the command line. You should see the following output:</p>
<pre><strong> Performing linear regression:</strong><br/><strong> =============================</strong><br/><strong> Slope and intercept:</strong><br/><strong> [ 2.47, 22.6 ]</strong><br/><strong> Line formula:</strong><br/><strong> y = 2.47x + 22.6</strong><br/><strong> R^2 fitness: 0.96</strong><br/><strong> Predict X = 75: 207.85</strong></pre>
<p>The regression has determined that the formula for the line that best fits our data is <em>y = 2.47x + 22.6</em>. The original formula I used to create this test data was <em>y = 2.5x + 22</em>. The slight difference between the determined equation and the actual equation is due to the effect of the random noise I added to the dataset. As you can see, the linear regression has done a good job of looking past the noise and discovering the underlying pattern. If we chart these results, we will see the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-117 image-border" src="assets/ec75338a-7bc4-4a74-9a00-aebf94f1a9c3.png" style="width:45.92em;height:28.33em;"/></div>
<p>The results of the regression, as seen in the preceding graph, are exactly the same as the results given by the trendline feature of Excel or Google Sheets, with the difference being that we generated the trendline in JavaScript.</p>
<p>When asked to predict a future value where <em>X = 75</em>, the regression returns <em>Y = 207.85</em>. Using my original formula, the true value would have been 209.5. The amount of noise I added to the data amounts to a random and uniform noise level of +/- 12.5 for any given point, so the predicted value turns out to be very close to the actual value when you account for the uncertainty due to noise.</p>
<p>It should be noted, however, that errors in regression compound as you make predictions further away from the domain of the original data. When predicting for X = 75, the error between the prediction and the actual value is only 1.65. If we were to predict for X = 1000, on the other hand, the true formula would return 2,522, but the regression would predict 2,492.6. The error between the actual value and the prediction at X = 1000 is now 29.4, nearly 30, far beyond the uncertainty due to noise. Regressions are very useful predictors, but you must always keep in mind that these errors can compound and therefore the predictions will become less accurate as you get further away from the domain of the dataset.</p>
<p>The reason for this type of prediction error lies in the regression for the slope of the equation. The slope of the line in the original equation is 2.5. This means that for every unit change in the <em>X</em> value, we should expect a change of 2.5 units in the <em>Y</em> value. The regression, on the other hand, determines a slope of 2.47. Therefore, for every unit change in the <em>X</em> value, the regression is inheriting a slight error of -0.03. Predicted values will be slightly lower than actual values by that amount, multiplied by the <em>X</em> distance for your prediction. For every 10 units of <em>X</em>, the regression inherits a total error of -0.3. For every 100 units of <em>X</em>, the regression inherits an error of -3.0, and so on. When we extrapolate out to X=1000, we've inherited an error of -30, because that slight per-unit error of -0.03 gets multiplied by the distance we travel along the <em>x</em> axis.</p>
<p>When we look at values within our data domain—values between X = 0 and X = 50—we only get very small prediction errors due to this slight difference in slope. Within our data domain, the regression has corrected for the error in slope by slightly increasing the y-intercept value (the original is +22, the regression returned +22.6). Due to the noise in our data, the regression formula of <em>y = 2.47x + 22.6</em> is a better fit than the actual formula of <em>y = 2.5x + 22</em>. The regression finds a slightly less-steep slope and makes up for it by raising the entire line by 0.6 units (the difference in y-intercepts), because this fits both the data and the noise better. This model fits the data very well between X = 0 and X = 50, but when we try to predict the value when X = 1000, the slight +0.6 modification in the y-intercept is no longer enough to make up for the decreased slope over such a vast distance.</p>
<p>Linear trends like the one found in this example are very common. There are many types of data that exhibit linear relationships and can be modeled simply yet accurately as long as you don't attempt to over-extrapolate the data. In the next example, we'll take a look at exponential regressions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 2 – exponential regression</h1>
                </header>
            
            <article>
                
<p>Another common trend in continuous data patterns is <em>exponential growth</em><em>,</em> which is also commonly seen as <em>exponential decay. </em>In exponential growth, a future value is proportionally related to the current value. The general formula for this type of growth can be written as: </p>
<div class="CDPAlignCenter CDPAlign"><em>y = y<sub>0</sub> (1 + r) x</em></div>
<p>Where <em>y<sub>0</sub></em> is the quantity's initial value (when <em>x</em> = 0), and <em>r</em> is the growth rate of the quantity.</p>
<p>For instance, if you are investing money in the stock market and expect a 5% rate of return per year (<em>r = 0.05</em>) with an initial investment of $10,000, after five years you can expect $12,763. The exponential growth formula applies here because the amount of money you have next year is proportionally related to the amount of money you have this year, and the amount of money you have two years from now is related to the money you have next year, and so on. This only applies if you reinvest your returns, causing the amount of money you're actively investing to increase with each passing year.</p>
<p>Another form for the exponential growth equation is given as:</p>
<p style="padding-left: 270px"> <em>y = ae<sup>bx</sup></em></p>
<p>Where <em>b = ln(1 + r)</em>, <em>a</em> is the initial value <em>y<sub>0</sub></em>, and <em>e</em> is Euler's constant of approximately 2.718. This slight transformation in form is easier to manipulate mathematically and is typically the preferred form used by mathematicians for analysis. In our stock market investment example, we can rewrite the formula for five year growth as <em>y = 10000*e<sup>ln(1.05)*5</sup></em>, and we will get the same result of $12,763.</p>
<p>Exponential growth is sometimes called <strong>hockey-stick growth</strong> due to the shape of the curve resembling the outline of a hockey stick:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-116 image-border" src="assets/4dd1b88a-4d6a-472b-aaeb-21d67069bb13.png" style="width:44.25em;height:27.25em;"/></div>
<p>Some examples of exponential growth include:</p>
<ul>
<li>Population growth; that is, world population or bacterial culture growth</li>
<li>Viral growth, such as analysis of disease infections or the viral spread of YouTube videos</li>
<li>Positive feedback loops in mechanics or signal processing</li>
<li>Economic growth, including compound interest</li>
<li>Processing power of computers under Moore's law</li>
</ul>
<p>It is important to note that, in almost all circumstances, exponential growth is unsustainable. For example, if you are predicting the growth of a bacterial colony in a Petri dish, you may observe exponential growth for a short time, however once the Petri dish runs out of food and space, other factors will take over and the growth will no longer be exponential. Similarly, if your website incentivizes new users to invite their friends, you may see exponential growth in your membership for a while, but eventually you will saturate the market and growth will slow. Therefore, you must be cautious in analyzing exponential growth models and understand that the conditions that fuel exponential growth may ultimately change. Similar to linear regression, an exponential regression will only apply to modest extrapolations of your data. Your website's membership may grow exponentially for a year, but not for ten years; you cannot have 20 billion members if there are only seven billion people on Earth.</p>
<p>If the growth rate, <em>r</em>, or the parameter <em>k</em> (called the <strong>growth constant</strong>), is negative, you will have exponential decay rather than exponential growth. Exponential decay is still exponential growth in the sense that future values are proportional to the current value, however in exponential decay, future values are proportionately <em>smaller</em> than the current value.</p>
<p>One real-world use of exponential decay is in carbon-dating analysis. Because the radioactive carbon-14 isotope decays into non-radioactive carbon-12 with a half-life of 5,730 years—meaning that, in aggregate, half the carbon-14 decays into carbon-12 every 5,730 years—scientists can use the exponential decay formula to figure out how old an object must be in order to have the appropriate ratio of carbon-14 to carbon-12.</p>
<p>Exponential decay is also seen in physics and mechanics, particularly in the spring-mass-damper problem. It can also be used by coroners and medical examiners to determine the time of death of a subject, based on the fact that a warm body will cool down and approach the ambient temperature of the room in an exponentially decaying fashion.</p>
<p>In exponential regression, our goal is to determine the values of the parameters <em>a</em> and <em>b</em><span>—</span>the initial value and the growth constant. Let's try this in JavaScript. The data we wish to analyze is exponentially decaying, with random sensor noise added:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-119 image-border" src="assets/928b67bb-e521-4c2a-af8a-64d5cad67500.png" style="width:49.58em;height:30.42em;"/></div>
<p>The preceding chart exhibits some quantity that starts near 100 and decays down to approximately 0. This could represent, for instance, the number of visitors over time to a post that was shared on Facebook.</p>
<p>Attempting to fit a trendline with Excel or Google Sheets does not help us in this case. The linear trendline does not fit the exponential curve, and the inappropriateness of the fit is indicated by the poor R<sup>2</sup> value:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-120 image-border" src="assets/e6682428-e2cf-4d50-8bae-007dd6ce7e70.png" style="width:47.17em;height:29.00em;"/></div>
<p>Let's now use JavaScript to find the regression for this data and also make a prediction of the value for one second before the dataset started. Add the following code to <kbd>index.js</kbd>; it is the linear regression code:</p>
<pre><span>console</span>.<span>log</span>(<span>"Performing exponential regression:"</span>);<br/><span>console</span>.<span>log</span>(<span>"============================="</span>);<br/><span>const </span><span>expModel </span>= regression.exponential(data.<span>exponential</span>);<br/><span>console</span>.<span>log</span>(<span>"Initial value and rate:"</span>);<br/><span>console</span>.<span>log</span>(<span>expModel</span>.<span>equation</span>);<br/><span>console</span>.<span>log</span>(<span>"Exponential formula:"</span>);<br/><span>console</span>.<span>log</span>(<span>expModel</span>.<span>string</span>);<br/><span>console</span>.<span>log</span>(<span>"R^2 fitness: " </span>+ <span>expModel</span>.<span>r2</span>);<br/><span>console</span>.<span>log</span>(<span>"Predict X = -1: " </span>+ <span>expModel</span>.<span>predict</span>(-<span>1</span>)[<span>1</span>]);</pre>
<p>Run the program with <kbd>yarn start</kbd> and you should see the following output, following the output for the linear regression example:</p>
<pre><strong> Performing exponential regression:</strong><br/><strong> =============================</strong><br/><strong> Initial value and rate:</strong><br/><strong> [ 94.45, -0.09 ]</strong><br/><strong> Exponential formula:</strong><br/><strong> y = 94.45e^(-0.09x)</strong><br/><strong> R^2 fitness: 0.99</strong><br/><strong> Predict X = -1: 103.34</strong></pre>
<p>We can immediately see the high R<sup>2</sup> value of 0.99, indicating that the regression has found a good fit to the data. If we chart this regression along with the original data, we see the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-121 image-border" src="assets/7bfcc118-4465-45e4-9359-e1ac6b4c739e.png" style="width:54.25em;height:33.42em;"/></div>
<p>Additionally, we get a prediction for X = -1 of 103, which fits our data well. The original parameters for the equation I used to generate the test data were <em>a = 100</em> and <em>b = -0.1</em>, while the predicted parameters were <em>a = 94.5</em> and <em>b = -0.09</em>. The presence of noise has made a significant impact on the starting value, which would have been 100 if there were no noise in the system, but was actually measured at 96. When comparing the regressed value for <em>a</em> to the actual value of <em>a</em>, you must also consider the fact that the regressed value of <em>a</em> was close to the measured, noisy value, even though it is pretty far from the systemic value.</p>
<p>In the next section, we will take a look at the polynomial regression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example 3 – polynomial regression</h1>
                </header>
            
            <article>
                
<p>The polynomial regression can be considered a more generalized form of the linear regression. A polynomial relationship has the form:</p>
<p style="padding-left: 150px"><em>y = a<sub>0</sub> + a<sub>1</sub>x<sup>1</sup> + a<sub>2</sub>x<sup>2</sup> + a<sub>3</sub>x<sup>3</sup> + ... + a<sub>n</sub>x<sup>n</sup></em></p>
<p>A polynomial can have any number of terms, which is called the <strong>degree</strong> of the polynomial. For each degree of the polynomial, the independent variable, <em>x</em>, is multiplied by some parameter, <em>a<sub>n</sub></em>,, and the X-value is raised to the power <em>n</em>. A straight line is considered a polynomial of degree <em>1</em>; if you update the preceding polynomial formula to remove all degrees above one, you are left with:</p>
<p style="padding-left: 210px"><em> y = a<sub>0</sub> + a<sub>1</sub>x</em></p>
<p>Where <em>a<sub>0</sub></em> is the y-intercept and <em>a<sub>1</sub></em> is the slope of the line. Despite the slight difference in notation, this is equivalent to <em>y = mx + b</em>.</p>
<p>Quadratic equations, which you may recall from high school math, are simply polynomials of degree <em>2</em>, or <em>y = a<sub>0</sub> + a<sub>1</sub>x + a<sub>2</sub>x<sup>2</sup></em>. Cubic equations are polynomials of degree 3, quadratic equations are polynomials of degree 4, and so on.</p>
<p>The property of polynomials and polynomial regressions that makes them so powerful is the fact that nearly any shape can be described by a polynomial of sufficient degree, within a limited range of values. Polynomial regressions can even fit sinusoidal shapes, as long as you don't try to extrapolate too far. Polynomial regressions exhibit properties similar to other machine learning algorithms in the sense that they can overfit and become very inaccurate for new data points if you attempt to extrapolate too far.</p>
<p>Because polynomials can be of any degree, you must also configure the regression with an additional parameter; this parameter can be guessed or you can search for the degree that maximizes the R<sup>2</sup> fit. This approach is similar to the approach we used for k-means when you don't know the number of clusters in advance.</p>
<p>The data we wish to fit, when graphed, looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-124 image-border" src="assets/8bcfab46-a8df-43ab-b29e-87973f606582.png" style="width:47.92em;height:29.25em;"/></div>
<p>This small window of data looks sinusoidal but is in fact polynomial; remember that polynomial equations can reproduce many types of shapes.</p>
<p>Add the following code to the bottom of <kbd>index.js</kbd>:</p>
<pre><span>console</span>.<span>log</span>(<span>"Performing polynomial regression:"</span>);<br/><span>console</span>.<span>log</span>(<span>"============================="</span>);<br/><span>const </span><span>polyModel </span>= regression.polynomial(data.<span>polynomial</span>, {<span>order</span>: <span>2</span>});<br/><span>console</span>.<span>log</span>(<span>"Polynomial parameters"</span>);<br/><span>console</span>.<span>log</span>(<span>polyModel</span>.<span>equation</span>);<br/><span>console</span>.<span>log</span>(<span>"Polynomial formula:"</span>);<br/><span>console</span>.<span>log</span>(<span>polyModel</span>.<span>string</span>);<br/><span>console</span>.<span>log</span>(<span>"R^2 fitness: " </span>+ <span>polyModel</span>.<span>r2</span>);<br/><span>console</span>.<span>log</span>(<span>"Predict X = 6: " </span>+ <span>polyModel</span>.<span>predict</span>(<span>6</span>)[<span>1</span>]);</pre>
<p>Note that we have configured the regression with <kbd>{order: 2}</kbd>, that is, we are attempting to fit the data with a quadratic formula. Run the program with <kbd>yarn start</kbd> to see the following output:</p>
<pre><strong> Performing polynomial regression:</strong><br/><strong> =============================</strong><br/><strong> Polynomial parameters</strong><br/><strong> [ 0.28, -17.83, -6.6 ]</strong><br/><strong> Polynomial formula:</strong><br/><strong> y = 0.28x^2 + -17.83x + -6.6</strong><br/><strong> R^2 fitness: 0.75</strong><br/><strong> Predict X = 6: -103.5</strong></pre>
<p>The R<sup>2</sup> fit for this data is quite low, at <kbd>0.75</kbd>, indicating that we have probably used an incorrect value for the <kbd>order</kbd> parameter. Try increasing the order to <kbd>{order: 4}</kbd> and re-run the program to get the following:</p>
<pre><strong> Performing polynomial regression:</strong><br/><strong> =============================</strong><br/><strong> Polynomial parameters</strong><br/><strong> [ 0.13, 1.45, -2.59, -40.45, 0.86 ]</strong><br/><strong> Polynomial formula:</strong><br/><strong> y = 0.13x^4 + 1.45x^3 + -2.59x^2 + -40.45x + 0.86</strong><br/><strong> R^2 fitness: 0.99</strong><br/><strong> Predict X = 6: 146.6</strong></pre>
<p>The regression is a much better fit now, at the expense of having added extra polynomial terms to the equation. If we graph this regression against the original data, we will see the following output, which indeed fits the data very well:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-125 image-border" src="assets/135d82e1-3f14-42d7-a7cd-29ae566248cb.png" style="width:46.75em;height:28.75em;"/></div>
<p>In the next section, we will explore some other types of analyses that can be performed on time-series data, including low-pass and high-pass filters, and seasonality analysis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other time-series analysis techniques</h1>
                </header>
            
            <article>
                
<p>Regressions are a great starting point for analyzing continuous data, however, there are many other techniques one can employ when analyzing time-series data specifically. While regressions can be used for any continuous data mapping, time-series analysis is specifically geared toward continuous data that evolves over time.</p>
<p>There are many examples of time-series data, for instance:</p>
<ul>
<li>Server load over time</li>
<li>Stock prices over time</li>
<li>User activity over time</li>
<li>Weather patterns over time</li>
</ul>
<p>The objective when analyzing time-series data is similar to the objective in analyzing continuous data with regressions. We wish to identify and describe the various factors that influence the changing value over time. This section will describe a number of techniques above and beyond regressions that you can use to analyze time-series data.</p>
<p>In this section, we will look at techniques that come from the field of digital signal processing, which has applications in electronics, sensor analysis, and audio signals. While your specific time-series problem may not be related to any of these fields, the tools used in digital signal processing applications can be applied to any problem domain that deals with digital signals. Among the most significant tools and techniques are filtering, seasonality detection, and frequency analysis. We'll discuss these techniques, but I will leave it up to you to implement your own examples and experiments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Filtering</h1>
                </header>
            
            <article>
                
<p>Filtering, in a digital signal processing context, is a technique used to filter out either high-frequency or low-frequency components of a signal. These are called <strong>low-pass filters</strong> and <strong>high-pass filters</strong>, respectively; a low-pass filter allows low-frequency signals to <em>pass</em> while removing high-frequency components from the signal. There are also <em>band-pass</em> and <em>notch</em> filters, which either allow a range of frequencies to pass or cut a range of frequencies from the signal.</p>
<p>In electronics, filters are designed by using capacitors, resistors, and other simple electronic components in order to allow only frequencies above or below a <em>cut-off</em> <em>frequency</em> to pass through the circuit. In digital signal processing, the same effect can be achieved with an <em>infinite impulse response</em> filter, which is an algorithm that can reproduce the effects of an electronic circuit on time-series data.</p>
<p>To illustrate this, consider the following data:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-127 image-border" src="assets/0f3482fb-e2bf-4a0b-ae03-693641f263d6.png" style="width:53.00em;height:32.50em;"/></div>
<p>This data was generated by combining two sinusoidal signals, one low-frequency signal and one high-frequency signal. If we chart the two signals individually, we can see how they combine to create the overall signal:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-128 image-border" src="assets/4c71d815-d1a7-4dd8-8731-5ecdc504f319.png" style="width:52.67em;height:32.33em;"/></div>
<p>When filtering the overall signal, the goal is to extract either the low-frequency or the high-frequency component of the signal, while filtering the other out. This is called <strong>subtractive processing</strong>, since we are removing (filtering) a component from the signal.</p>
<p>In general, you should use low-pass filtering to isolate large, general, periodic trends in time-series data while ignoring faster periodic trends. High-pass filtering, on the other hand, should be used when you wish to explore the short-term periodic trends while ignoring the long-term trends. One example of this approach is when analyzing visitor traffic; you can use high-pass and low-pass filtering to selectively ignore monthly trends versus daily trends.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Seasonality analysis</h1>
                </header>
            
            <article>
                
<p>Building on the previous section, we can also use digital signal processing to analyze seasonal trends. Seasonal trends are long-term periodic (that is, low-frequency) trends that you wish to subtract from your overall data in order to analyze other, potentially non-periodic trends in data. Consider the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-130 image-border" src="assets/e6c51674-759f-47b5-b62d-43960d3bdacd.png" style="width:51.42em;height:31.75em;"/></div>
<p>This data exhibits a combination of linear growth on top of periodic fluctuations in activity. Specifically, there are two periodic components (one low-frequency and one high-frequency) and one linear component to this data trend.</p>
<p>In order to analyze this data, the approach would be to first identify the linear trend, either through a large moving-average window or through a linear regression. Once the linear trend has been identified, you can subtract it from the data to isolate the periodic portions only. This approach is illustrated as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-131 image-border" src="assets/8ac0f8fa-ab8c-42b4-96dc-220c50a0db30.png" style="width:49.17em;height:30.08em;"/></div>
<p>Because signals are additive, you are able to subtract the linear trend from the original data in order to isolate the non-linear components of the signal. If you've identified multiple trends, either through regressions or other means, you can continue to subtract the trends that you've identified from the original signal and you'll be left with only the unidentified signal components. Once you've identified and subtracted all of the systemic patterns, you'll have only the sensor noise remaining.</p>
<p>In this case, once you've identified and subtracted the linear trend from the data, you can either perform filtering on the resultant signal in order to isolate the low- and high-frequency components, or you can perform a <em>Fourier analysis</em> on the leftover signal to identify the specific frequencies and amplitudes of the remaining components.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fourier analysis</h1>
                </header>
            
            <article>
                
<p>Fourier analysis is a mathematical technique used to decompose a time-series signal into its individual frequency components. Recall that polynomial regressions of arbitrary degree can reproduce nearly any signal shape. In a similar manner, the sum of a number of sinusoidal oscillators can reproduce nearly any periodic signal. If you've ever seen an <em>oscilloscope</em> or <em>spectrum analyzer</em> in action, you've seen the real-time results of a Fourier transform being applied to a signal. In short, a Fourier transformation turns a periodic signal, such as the ones we saw in the last section, into a formula similar to:</p>
<p style="padding-left: 90px"><em>a<sub>1</sub>sin(f<sub>1</sub>+φ<sub>1</sub>) + a<sub>2</sub>sin(f<sub>2</sub>+φ<sub>2</sub>) + a<sub>3</sub>sin(f<sub>3</sub>+φ<sub>3</sub>) + ... + a<sub>n</sub>sin(f<sub>n</sub>+φ<sub>n</sub>)</em></p>
<p>Where <em>f<sub>n</sub></em> represents a frequency, <em>a<sub>n</sub></em> represents its amplitude, and <em>φ<sub>n</sub></em> represents a phase offset. By combining an arbitrary number of these sinusoidal signals together, one can replicate nearly any periodic signal.</p>
<p>There are many reasons to perform a Fourier analysis. The most intuitive examples relate to audio and sound processing. If you take a one-second-long audio sample of the note A4 being played on a piano and perform a Fourier transform on it, you would see that the frequency of 440 Hz has the largest amplitude. You would also see that the harmonics of 440 Hz, like 880 Hz and 1,320 Hz, also have some energy. You can use this data to aid in audio fingerprinting, auto-tuning, visualizing, and many other applications. The Fourier transformation is a sampling algorithm, and so it is susceptible to aliasing and other sampling errors. A Fourier transformation can be used to partially recreate an original signal, but much detail would be lost in translation. This process would be similar to down-sampling an image and then trying to up-sample it again.</p>
<p>There are many other applications for Fourier transforms in nearly every domain. The Fourier transform's popularity is due to the fact that, mathematically, many types of operations are easier to perform in the frequency domain than the time domain. There are many types of problems in mathematics, physics, and engineering that are very difficult to solve in the time domain but easy to solve in the frequency domain.</p>
<p>A Fourier transformation is a mathematical process that specific algorithms perform. The most popular Fourier transform algorithm is called the <strong>Fast Fourier Transform</strong> (<strong>FFT</strong>), named s0 because it was much faster than its predecessor, the <em>Discrete Fourier Transform</em>. The FFT has one significant limitation in that the number of samples to be analyzed must be a power of 2, that is, it must be 128, 256, 512, 1,024, 2,048, and so on, samples long. If you have 1,400 samples to analyze, you must either truncate it down to 1,024 samples or pad it up to 2,048 samples. Most often, you will be <em>windowing</em> a larger sample; in the example of the piano note recording, we have windowed one second of samples from a live or recorded signal. If the audio sample rate is 44,100 Hz, then we would have 44,100 samples (one second's worth) to give to the Fourier transformation.</p>
<p>When padding, truncating, or windowing samples from a larger signal, you should use a <em>window function</em>, which is a function that tapers the signal at both ends so that it is not sharply cut off by your window. There are many types of window functions, each with their own mathematical properties and unique effects on your signal processing. Some popular window functions include the rectangular and triangular windows, as well as the Gaussian, Lanczos, Hann, Hamming, and Blackman windows, which each have desirable properties in different types of analyses.</p>
<p>The output of a Fourier transform algorithm, like the FFT algorithm, is a <em>frequency domain spectrum</em>. More concretely, the output of the FFT algorithm will be an array or a hash table where the keys are frequency buckets (such as 0-10 Hz, 10-20 Hz, and so on), and the values are amplitude and phase. These may be represented as complex numbers, multidimensional arrays, or some other structure specific to the algorithm implementation.</p>
<p>Some limitations apply to all sampling algorithms; these are limitations of signal processing itself. For instance, aliasing can occur if your signal contains components at frequencies above the <em>Nyquist frequency</em>, or half the sampling rate. In audio, where a sampling rate of 44,100 Hz is common, any frequencies above 22,050 Hz will be aliased, or misrepresented as low-frequency signals. Preprocessing signals with a low-pass filter is therefore a common technique. Similarly, the FFT algorithm can only resolve frequencies up to the Nyquist frequency. The FFT algorithm will return only as many frequency buckets as the sample buffer size, so if you give it 1,024 samples you will only get 1,024 frequency buckets. In audio, this means that each frequency bucket will have a bandwidth of 44,100 Hz / 1,024 = 43 Hz. This means that you would not be able to tell the difference between 50 Hz and 55 Hz, but you would easily be able to tell the difference between 50 Hz and 500 Hz. In order to get a higher resolution, you would need to provide more samples, however, this, in turn, will reduce the time resolution of your windows.</p>
<p>You can use the FFT to analyze the periodic portion of the time-series data we saw in the last section. It would be best to perform the FFT after you have subtracted the linear trend from the signal. However, if you have a high enough frequency resolution, the linear trend may only be interpreted as a low-frequency component of the Fourier transformation, so whether or not you need to subtract the linear trend will depend on your specific application.</p>
<p>By adding the FFT to the other tools you have learned about in this chapter, you are prepared to tackle most real-world regression or time-series analysis tasks. Each problem will be unique, and you will have to carefully consider which specific tools you will need for your task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned a number of techniques used in forecasting, signal processing, regression, and time-series data analysis. Because forecasting and time-series analysis is a broad category, there is no single algorithm you can use that covers every case. Instead, this chapter has given you an initial toolbox of important concepts and algorithms that you can start applying to your forecasting and regression tasks.</p>
<p>Specifically, you learned about the difference between regression and classification. While classification assigns labels to data points, regression attempts to predict the numerical value of a data point. Not all regression is necessarily forecasting, but regression is the single most significant technique used in forecasting.</p>
<p>After learning the basics of regression, we explored a few specific types of regression. Namely, we discussed linear, polynomial, and exponential regression. We saw how regression deals with noise and how we can use it to predict future values.</p>
<p>We then turned to the broader concept of time-series analysis, and discussed core concepts, such as extracting trends from signals. We discussed tools used in digital signal processing that are applicable to time-series analysis, such as low-pass and high-pass filters, seasonality analysis, and Fourier transformations.</p>
<p>In the next chapter, we're going to look at more advanced machine learning models. Specifically, we're going to learn about the neural network—which, by the way, can also perform regressions.</p>


            </article>

            
        </section>
    </body></html>