<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Instance and Kernel Methods Based Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Instance and Kernel Methods Based Learning</h1></div></div></div><p>We have covered Decision tree models for solving classification and regression problems in the previous chapter. In this chapter, we will cover two important models of supervised and unsupervised learning techniques which are the Nearest Neighbors method, which uses the instance-based learning model, and the <a id="id751" class="indexterm"/>
<span class="strong"><strong>Support Vector Machines</strong></span> (<span class="strong"><strong>SVM</strong></span>) model, which uses kernel methods based learning model. For both methods, we will learn the basics of the technique and see how it can be implemented in Apache Mahout, R, Julia, Apache Spark, and Python. The following figure depicts different learning models covered in this book and the techniques highlighted will be dealt in covered in this chapter.</p><div class="mediaobject"><img src="graphics/B03980_06_01.jpg" alt="Instance and Kernel Methods Based Learning"/></div><p>The following topics are covered in-depth in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Instance-based learning models</li><li class="listitem" style="list-style-type: disc">Introduction to instance-based learning</li><li class="listitem" style="list-style-type: disc">Lazy and eager learning</li><li class="listitem" style="list-style-type: disc">A brief look at different algorithms/approaches of instance-based learning techniques Nearest Neighbor method, Case-based reasoning, Locally weighed regression, and Radial basis functions</li><li class="listitem" style="list-style-type: disc">A deep dive into KNN (k-Nearest Neighbor) algorithm with a real-world use case example; mechanisms to speed up KNN</li><li class="listitem" style="list-style-type: disc">Sample implementation of Apache Mahout, R, Apache Spark, Julia and Python (scikit-learn) libraries and modules</li><li class="listitem" style="list-style-type: disc">Kernel-based learning models<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introduction to kernel-based learning</li><li class="listitem" style="list-style-type: disc">A brief look at different algorithms/approaches of Kernel-based learning techniques, Support Vector Machines (SVM), Linear Discriminate Analysis (LDA), and more</li><li class="listitem" style="list-style-type: disc">A deep dive into SVM algorithm with a real-world use case example</li></ul></div></li></ul></div><div class="section" title="Instance-based learning (IBL)"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec34"/>Instance-based learning (IBL)</h1></div></div></div><p>The IBL technique approaches learning by<a id="id752" class="indexterm"/> simply storing the provided training data and using it as a reference for predicting/determining the behavior of a new query. As learned in <a class="link" href="ch01.html" title="Chapter 1. Introduction to Machine learning">Chapter 1</a>, <span class="emphasis"><em>Introduction to Machine learning</em></span>, instances are nothing but subsets of datasets. The instance-based learning model works on an identified instance or groups of instances that are critical to the problem. The results across instances are compared and can include an instance of new data as well. This comparison uses a particular similarity measure to find the best match and predict. Since it uses historical data stored in memory, this learning technique is also called memory-based or <a id="id753" class="indexterm"/>case-based learning. Here, the focus is on the representation of the instances and similarity measures for comparison between them.</p><div class="mediaobject"><img src="graphics/B03980_06_02.jpg" alt="Instance-based learning (IBL)"/></div><p>Every time a new query<a id="id754" class="indexterm"/> instance is received for processing, a set of similar, related instances are retrieved from memory, and then this data is used to classify the new query instance.</p><p>Instance-based learners are also called <a id="id755" class="indexterm"/>lazy learners. Overall, the entire database is used to predict behavior. A set of data points referred to as <span class="strong"><strong>neighbors</strong></span> <a id="id756" class="indexterm"/>are identified, having a history of agreeing with the target attribute. Once a neighborhood of data points is formed, the preferences of neighbors are combined to produce a prediction or <span class="strong"><strong>top-K recommendation</strong></span> <a id="id757" class="indexterm"/>for the active target attribute.</p><p>These methods are applicable for complex target functions that can be expressed using less complex local approximations. Unfortunately, with these methods, the cost of classifying a new instance is always high and in cases where there is a curse of dimensionality, these methods might end up with a bigger footprint as all the attributes of all the instances are considered. Classifiers and regressions are what we will cover in this and the next chapters that are to come. With classifiers, we try to predict a category and, with regression, we predict a real number. We will first look at the Nearest Neighbor algorithm that can be used both for classification and regression problems.</p><p>Rote Learner <a id="id758" class="indexterm"/>is one of the instance-based classifiers and focuses on memorizing the entire training data. Classification is primarily done only if the target attribute value exactly matches with the attribute value in the training example. The other classifier is the Nearest Neighbor, which classifies based on the closest neighbor(s). In the next section, let's dive deeply into the Nearest Neighbor algorithm.</p><div class="section" title="Nearest Neighbors"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec79"/>Nearest Neighbors</h2></div></div></div><p>Before we <a id="id759" class="indexterm"/>start <a id="id760" class="indexterm"/>understanding what the Nearest Neighbor algorithm is all about, let's start with an example; the following graph shows the plotting of data points <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> that have two classes: stars and triangles. Let's not really worry about what is the exact data representation or the<a id="id761" class="indexterm"/> data points. If we had to solve<a id="id762" class="indexterm"/> intuitively the problem of finding what that particular red square box data point is, then the answer would obviously be a green triangle. This is an intuition and, without actually understanding or analyzing the data points, we can arrive at this conclusion. But what actually happened here is that we have seen the traits of the neighbors of the data point in context and have predicted the class to which the new data point could possibly belong to. Overall, the basis for the learning algorithm is actually the behavior of the nearby or neighboring points.</p><div class="mediaobject"><img src="graphics/B03980_06_03.jpg" alt="Nearest Neighbors"/></div><p>Nearest Neighbor is an algorithm that uses this basic technique of intuition. This algorithm finds the Nearest Neighbor using some distance measurement techniques that will be discussed in the following sections. Let's now extend this to another example data set; and again, the new data point with a question mark (?) will be needed for classification.</p><div class="mediaobject"><img src="graphics/B03980_06_04.jpg" alt="Nearest Neighbors"/></div><p>Let's now <a id="id763" class="indexterm"/>assume that the class the new data point <a id="id764" class="indexterm"/>belongs to is the yellow star. An important aspect of the distance measure is that the Nearest Neighbor is never just a single point but is usually a region. The following figure shows a region and all the data points that fall in this region belong to the class yellow star. This region is called the <a id="id765" class="indexterm"/>
<span class="strong"><strong>Voronoi cell</strong></span>. This region is usually a polygon with straight lines in case the distance measure used is the <a id="id766" class="indexterm"/>
<span class="strong"><strong>Euclidean</strong></span> distance measure.</p><div class="mediaobject"><img src="graphics/B03980_06_05.jpg" alt="Nearest Neighbors"/></div><p>For each<a id="id767" class="indexterm"/> training example if the Voronoi cells are<a id="id768" class="indexterm"/> computed, we can see the Voronoi tessellation as shown next. This tessellation represents the partition of space into the non-overlapping region and typically each region has one example.</p><div class="mediaobject"><img src="graphics/B03980_06_06.jpg" alt="Nearest Neighbors"/></div><p>The size of the cell is determined by the number of examples available. The more the examples, the less the size of the regions. Another interesting aspect of the Voronoi tessellation is that there can be boundaries carved that form a separation for the classes, as shown in the following figure. The right side of the bold line belongs to the triangle class, and the left side belongs to the star class.</p><div class="mediaobject"><img src="graphics/B03980_06_07.jpg" alt="Nearest Neighbors"/></div><p>One of the<a id="id769" class="indexterm"/> major complications with the <a id="id770" class="indexterm"/>Nearest Neighbor approach is its insensitivity to the outliers, thus really messing up the boundaries, and one way of solving this problem is to consider more than one neighbor, which this would make the model more stable and smooth. Hence, a consideration of k-Neighbors signifies the k-Nearest Neighbor algorithm.</p><div class="mediaobject"><img src="graphics/B03980_06_08.jpg" alt="Nearest Neighbors"/></div><p>Let's now <a id="id771" class="indexterm"/>look at how the KNN classification<a id="id772" class="indexterm"/> algorithm works:</p><p>Given the <span class="emphasis"><em>{x<sub>i</sub>, y<sub>i</sub>}</em></span> training examples, where <span class="emphasis"><em>x</em></span><sub>i</sub> represents attribute values, <span class="emphasis"><em>y</em></span><sub>i</sub> represents class labels, and there is a new test point <span class="emphasis"><em>X</em></span> that needs to be classified, the following steps are performed in a KNN classification algorithm:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Distance is computed between <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>x</em></span><sub>i</sub> for every given value of <span class="emphasis"><em>x</em></span><sub>i</sub>.</li><li class="listitem">Choose k nearest neighbors <span class="emphasis"><em>xi1, … xik</em></span> and the respective class labels <span class="emphasis"><em>yi1, … yik</em></span>.</li><li class="listitem">Return a <span class="emphasis"><em>y</em></span> that is the most frequent in the list of labels <span class="emphasis"><em>yi1, … yik</em></span>.</li></ol></div><p>Let's now see how different the KNN regression algorithm is among the important differences. Instead of outputting a class, we will output real numbers like ratings or age, and so on. The algorithm is identical, but the only variation is in the return value, Step 3, and instead of a most frequent value, we take the mean of <span class="emphasis"><em>y's</em></span>.</p><div class="section" title="Value of k in KNN"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec77"/>Value of k in KNN</h3></div></div></div><p>The value of <span class="emphasis"><em>k</em></span> has a tremendous effect on the KNN performance. If the <a id="id773" class="indexterm"/>value of <span class="emphasis"><em>k</em></span> is too large, the KNN algorithm uses the previous value and thus might result in inaccuracies. And in the case where the <span class="emphasis"><em>k</em></span> value is too small, the model would become too sensitive to outliers as we saw in the previous section. Hence, an accurate <span class="emphasis"><em>k</em></span> value usually lies midway through the smallest and largest value. The approach is to choose a value in this range and measure the error on the training data and pick a <span class="emphasis"><em>k</em></span> that gives the best generalization performance.</p><p>The following figure depicts 1, 2, and 3 Nearest Neighbors for the point <span class="emphasis"><em>x</em></span>:</p><div class="mediaobject"><img src="graphics/B03980_06_09.jpg" alt="Value of k in KNN"/></div><p>k-Nearest <a id="id774" class="indexterm"/>Neighbors for the point <span class="emphasis"><em>x</em></span> are all the data points that have the k smallest distance from <span class="emphasis"><em>x</em></span>.</p></div><div class="section" title="Distance measures in KNN"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec78"/>Distance measures in KNN</h3></div></div></div><p>This is <a id="id775" class="indexterm"/>one of the<a id="id776" class="indexterm"/> attributes of the Nearest Neighbor algorithm and possibly the only area that one can experiment or try alternatives in. There are many distance measurement options and in this section, we will discuss some of the commonly used measures. The primary purpose of distance measure is to identify the examples that are similar or dissimilar. Similar to the k value, distance measure determines the performance of KNN.</p><div class="section" title="Euclidean distance"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec22"/>Euclidean distance</h4></div></div></div><p>The Euclidean <a id="id777" class="indexterm"/>distance is the default option for <a id="id778" class="indexterm"/>numeric attributes. The distance measure formula is given here:</p><div class="mediaobject"><img src="graphics/B03980_06_24.jpg" alt="Euclidean distance"/></div><p>The Euclidean distance measure is symmetrical and spherical and treats all the dimensions equally. One of the drawbacks of this measure is its sensitivity to the extreme values within a single attribute. This is similar to the mean squared error.</p></div><div class="section" title="Hamming distance"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec23"/>Hamming distance</h4></div></div></div><p>The <a id="id779" class="indexterm"/>Hamming distance measure is a default option if<a id="id780" class="indexterm"/> we need to deal with categorical attributes. The primary function of a Hamming distance measure is to check whether the two attributes are equal or not. When they are equal, the distance is 0, otherwise it is 1; in effect, we check the number of attributes between two instances. The formula for the Hamming distance measure is as given here:</p><div class="mediaobject"><img src="graphics/B03980_06_25.jpg" alt="Hamming distance"/></div><p>Different attributes are measured on different scales, and there is a need to normalize the attributes.</p></div><div class="section" title="Minkowski distance"><div class="titlepage"><div><div><h4 class="title"><a id="ch06lvl4sec24"/>Minkowski distance</h4></div></div></div><p>We will now<a id="id781" class="indexterm"/> look at the <span class="emphasis"><em>p</em></span>-norm distance measures<a id="id782" class="indexterm"/> family that is a generalization of the Euclidean distance measures. These measures are relatively quite flexible.</p><p>The Minkowski distance formula looks similar to Euclidean and is as follows:</p><div class="mediaobject"><img src="graphics/B03980_06_26.jpg" alt="Minkowski distance"/></div><p>If <span class="emphasis"><em>p=0</em></span>, the distance <a id="id783" class="indexterm"/>measure is the Hamming<a id="id784" class="indexterm"/> measure.</p><p>If <span class="emphasis"><em>p=1</em></span>, the distance measure is the Manhattan measure.</p><p>If <span class="emphasis"><em>p=2</em></span>, the distance measure is the Euclidean measure.</p><div class="mediaobject"><img src="graphics/B03980_06_10.jpg" alt="Minkowski distance"/></div></div></div><div class="section" title="Case-based reasoning (CBR)"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec79"/>Case-based reasoning (CBR)</h3></div></div></div><p>CBR is an advanced<a id="id785" class="indexterm"/> instance-based learning method used with more complex instance objects. In addition to having a fixed database of past cases, CBR accumulates and stores the new data that is classified. Like all other instance-based learning methods, CBR matches new cases to find similar past cases. Semantic nets-based distance measures for matching the data is applied in this case. This is a diagrammatic matching method unlike other methods such as the Euclidean distance measure.</p><p>Similar to the other instance-based learning methods, CBR is a lazy learner, and the power comes from the organization and content of the cases.</p><p>Reusing past cases is one of the key factors in the way human problem solving and reasoning works. Since CBR is modeled on human problem solving, it is more understandable to humans. This means the way CBR works can be altered by experts or with the consultation of experts.</p><p>By the virtue of its ability to handle very complex instances, CBR is often used in medical diagnosis for detecting heart diseases, hearing defects, and other relatively complex conditions. The following figure depicts a typical CBR learning flow and is famously called the R4 Model.</p><p>Lazy learning in Machine learning is all about delaying the process of generalization beyond the training data until the time of the query. The advantage is that we can now perform parallel processing while the downside is higher memory requirements. The following diagram presents a process flow for a CBR function:</p><div class="mediaobject"><img src="graphics/B03980_06_11.jpg" alt="Case-based reasoning (CBR)"/></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, a New<a id="id786" class="indexterm"/> Case is received.</li><li class="listitem">Then, a matching process is triggered where the received case is matched to the Case Base that has existing cases and already classified cases. This is the retrieval process.</li><li class="listitem">Check if the Matched Cases perfectly fits the new case.</li><li class="listitem">If yes, Reuse it, otherwise Revise it.</li><li class="listitem">Output the final recommended solution.</li><li class="listitem">At a later<a id="id787" class="indexterm"/> point in time, based on the facts, if the recommendation is in agreement, retain the learning and add to the case base. The learning phase may also add rules to the knowledge base that the eventual facts suggest.</li></ol></div></div><div class="section" title="Locally weighed regression (LWR)"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec80"/>Locally weighed regression (LWR)</h3></div></div></div><p>LWR is a particular<a id="id788" class="indexterm"/> case of linear regression where, due to noise, the dataset is no more linear, and linear regression underfits the training data. The problem of non-linearity is solved by assigning weights to the Nearest Neighbors. The assigned weights are usually bigger for data points that are closer to the data that needs a prediction.</p></div></div><div class="section" title="Implementing KNN"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec80"/>Implementing KNN</h2></div></div></div><p>Refer to the source <a id="id789" class="indexterm"/>code provided for this chapter for implementing the k-Nearest Neighbor algorithm (source code path<code class="literal"> .../chapter6/...</code> under each of the folders for the technology).</p><div class="section" title="Using Mahout"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec81"/>Using Mahout</h3></div></div></div><p>Refer to<a id="id790" class="indexterm"/> the <a id="id791" class="indexterm"/>folder <code class="literal">.../mahout/chapter6/knnexample/</code>.</p></div><div class="section" title="Using R"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec82"/>Using R</h3></div></div></div><p>Refer to <a id="id792" class="indexterm"/>the <a id="id793" class="indexterm"/>folder <code class="literal">.../r/chapter6/knnexample/</code>.</p></div><div class="section" title="Using Spark"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec83"/>Using Spark</h3></div></div></div><p>Refer <a id="id794" class="indexterm"/>to the <a id="id795" class="indexterm"/>folder <code class="literal">.../spark/chapter6/knnexample/.</code>
</p></div><div class="section" title="Using Python (scikit-learn)"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec84"/>Using Python (scikit-learn)</h3></div></div></div><p>Refer to <a id="id796" class="indexterm"/>the <a id="id797" class="indexterm"/>folder <code class="literal">.../python scikit learn/ chapter6/knnexample/</code>.</p></div><div class="section" title="Using Julia"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec85"/>Using Julia</h3></div></div></div><p>Refer to <a id="id798" class="indexterm"/>the<a id="id799" class="indexterm"/> folder <code class="literal">.../julia/chapter6/knnexample/</code>.</p></div></div></div></div>
<div class="section" title="Kernel methods-based learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec35"/>Kernel methods-based learning</h1></div></div></div><p>We have just seen what <a id="id800" class="indexterm"/>instance-based learning methods are, and we have taken a deep dive into the Nearest Neighbor algorithm and covered specific implementation aspects. In this section, we will look into kernels and the kernel-based Machine learning algorithms.</p><p>A kernel, in simple terms, is a similarity function that is fed into a Machine learning algorithm. It takes two inputs and suggests how similar they are. For example, if we are dawned with a task of classifying images, the input data is a key-value pair (image, label). So, in terms of the flow, the image data is taken, features are computed, and a vector of features are fed into the Machine learning algorithm. But, in the case of similarity functions, we can define a kernel function that internally computes the similarity between images, and feed this into the learning algorithm along with the images and label data. The outcome of this is a classifier.</p><p>The standard regression or SVM or Perceptron frameworks work with kernels and only use vectors. To address this requirement, we will have the Machine learning algorithms expressed as dot products so that kernel functions can be used.</p><p>Kernels are preferable to feature vectors. There are many advantages; one of the key reasons being the ease of computing. Also, feature vectors need more storage space in comparison to dot products. It is possible to write Machine learning algorithms to use dot products and later map them to use kernels. This way, the usage of feature vectors can be completely avoided. This will support us in working with highly complex, efficient-to-compute, and yet high<a id="id801" class="indexterm"/> performing kernels effortlessly, without really developing multi-dimensional vectors.</p><div class="section" title="Kernel functions"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec81"/>Kernel functions</h2></div></div></div><p>Let's understand what exactly<a id="id802" class="indexterm"/> kernel functions are; the following figure represents a 1D function using a simple 1-Dimensional example. Assume that given points are as follows:</p><div class="mediaobject"><img src="graphics/B03980_06_12.jpg" alt="Kernel functions"/></div><p>A general 1-Dimensional hyperplane, as depicted previously, will be a vertical line and no other vertical lines will separate the dataset. If we look at the 2-Dimensional representation, as shown next, there is a hyperplane (an arbitrary line in 2-Dimensions) that separates red and blue points, thus eligible for a separation using SVMs.</p><div class="mediaobject"><img src="graphics/B03980_06_13.jpg" alt="Kernel functions"/></div><p>With the growing dimensional space, the need to be able to separate data increases. This mapping, <span class="emphasis"><em>x</em></span> -&gt; (<span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>x2</em></span>), is called the kernel function.</p><p>In case of growing dimensional space, the computations become more complex and <span class="strong"><strong>kernel trick</strong></span> needs to be applied to address these computations cheaply.</p></div><div class="section" title="Support Vector Machines (SVM)"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec82"/>Support Vector Machines (SVM)</h2></div></div></div><p>SVMs are used <a id="id803" class="indexterm"/>in solving classification problems. Overall, as an approach, the goal is to find that hyperplane effectively divides the class representation of data. Hyperplane can be defined as a generalization of a line in 2-Dimensions and a plane in 3-Dimensions. Let's now take an example to understand how SVM works for linearly separable binary datasets. We will use the same example as we have in the Nearest Neighbor algorithms. The following diagram represents data with two features <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> and available classes being triangles and stars.</p><div class="mediaobject"><img src="graphics/B03980_06_14.jpg" alt="Support Vector Machines (SVM)"/></div><p>The goal of <a id="id804" class="indexterm"/>SVM is to find the hyperplane that separates these two classes. The following diagram depicts some of the possible hyperplanes that can divide the datasets. The choice of the best hyperplane is defined by the extent to which a maximum margin is left for both classes. The margin is the distance between the hyperplane and the closest point in the classification.</p><div class="mediaobject"><img src="graphics/B03980_06_15.jpg" alt="Support Vector Machines (SVM)"/></div><p>Let's take two hyperplanes among others and check the margins represented by <span class="strong"><strong>M1</strong></span> and <span class="strong"><strong>M2</strong></span>. It is very clear that margin <span class="strong"><strong>M1</strong></span> &gt; <span class="strong"><strong>M2</strong></span>, so the choice of the hyperplane that separates best is the new plane between the green and blue planes.</p><div class="mediaobject"><img src="graphics/B03980_06_16.jpg" alt="Support Vector Machines (SVM)"/></div><p>The new <a id="id805" class="indexterm"/>plane can be represented by a linear equation as:</p><p>f(x) = ax + b</p><p>Let's assume that this equation delivers all values ≥ 1 from the triangle class and ≤ -1 for the star class. The distance of this plane from the closest points in both the classes is at least one; the modulus is one.</p><p>
<span class="emphasis"><em>f(x) ≥ 1</em></span> for triangles and <span class="emphasis"><em>f(x) ≤ 1</em></span> or <span class="emphasis"><em>|f(x)| = 1</em></span> for star</p><p>The distance between the hyperplane and the point can be computed using the following equation.</p><p>M1 = |f(x)| / ||a|| = 1 / ||a||</p><p>The total margin is <span class="emphasis"><em>1 / ||a|| + 1 / ||a|| = 2 / ||a|</em></span>.</p><p>To maximize the<a id="id806" class="indexterm"/> separability, which is the goal of SVM, we will need to maximize the <span class="emphasis"><em>||a||</em></span> value. This value is referred to as a weight vector. This process of minimizing the <span class="emphasis"><em>a</em></span> weight value is a non-linear optimization task. One method is to use the <span class="strong"><strong>Karush-Kuhn-Tucker</strong></span> (<span class="strong"><strong>KKT</strong></span>)<a id="id807" class="indexterm"/> condition, using the Lagrange multiplier <span class="emphasis"><em>λ</em></span><sub>i</sub>.</p><div class="mediaobject"><img src="graphics/B03980_06_17.jpg" alt="Support Vector Machines (SVM)"/></div><div class="mediaobject"><img src="graphics/B03980_06_18.jpg" alt="Support Vector Machines (SVM)"/></div><p>Let's take an <a id="id808" class="indexterm"/>example of two points between the two attributes <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span>. We need to find a point between these two points that has a maximum distance between these points. This requirement is represented in the graph depicted next. The optimal point is depicted using the red circle.</p><div class="mediaobject"><img src="graphics/B03980_06_19.jpg" alt="Support Vector Machines (SVM)"/></div><p>The maximum margin weight vector is parallel to the line from <span class="emphasis"><em>(1, 1)</em></span> to <span class="emphasis"><em>(2, 3)</em></span>. The weight vector is at <span class="emphasis"><em>(1,2)</em></span>, and this becomes a decision boundary that is halfway between and in perpendicular, that passes through <span class="emphasis"><em>(1.5, 2)</em></span>.</p><p>So, <span class="emphasis"><em>y = x1 +2x2 − 5.5</em></span> and the geometric margin is computed as <span class="emphasis"><em>√5</em></span>.</p><p>Following are the steps to compute SVMs:</p><p>With <span class="emphasis"><em>w = (a, 2a)</em></span> for <span class="emphasis"><em>a</em></span> the functions of the points (1,1) and (2,3) can be represented as shown here:</p><p>a + 2a + ω<sub>0</sub> = -1 for the point (1,1)</p><p>2a + 6a + ω<sub>0</sub> = 1 for the point (2,3)</p><p>The weights can be computed as follows:</p><div class="mediaobject"><img src="graphics/B03980_06_27.jpg" alt="Support Vector Machines (SVM)"/></div><div class="mediaobject"><img src="graphics/B03980_06_28.jpg" alt="Support Vector Machines (SVM)"/></div><p>These are the <a id="id809" class="indexterm"/>support vectors:</p><div class="mediaobject"><img src="graphics/B03980_06_29.jpg" alt="Support Vector Machines (SVM)"/></div><p>Lastly, the final equation is as follows:</p><div class="mediaobject"><img src="graphics/B03980_06_30.jpg" alt="Support Vector Machines (SVM)"/></div><div class="section" title="Inseparable Data"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec86"/>Inseparable Data</h3></div></div></div><p>SVMs can <a id="id810" class="indexterm"/>probably help you to find out a separating hyperplane if it exists. There might be cases where there is no possibility to define a hyperplane, which can happen due to noise in the data. In fact, another reason can be a non-linear boundary as well. The following first graph depicts noise and the second one shows a non-linear boundary.</p><div class="mediaobject"><img src="graphics/B03980_06_20.jpg" alt="Inseparable Data"/></div><div class="mediaobject"><img src="graphics/B03980_06_21.jpg" alt="Inseparable Data"/></div><p>In the case of<a id="id811" class="indexterm"/> problems that arise due to noise in the data, the best way to look at it is to reduce the margin itself and introduce slack.</p><div class="mediaobject"><img src="graphics/B03980_06_22.jpg" alt="Inseparable Data"/></div><p>The non-linear boundary problem can be solved by introducing a kernel. Some of the kernel functions that can be introduced are depicted in the following diagram:</p><div class="mediaobject"><img src="graphics/B03980_06_23.jpg" alt="Inseparable Data"/></div></div></div><div class="section" title="Implementing SVM"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec83"/>Implementing SVM</h2></div></div></div><p>Refer to the source<a id="id812" class="indexterm"/> code provided for this chapter to implement the SVM algorithm (source code path <code class="literal">.../chapter6/...</code> under each of the folders for the technology).</p><div class="section" title="Using Mahout"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec87"/>Using Mahout</h3></div></div></div><p>Refer<a id="id813" class="indexterm"/> to the<a id="id814" class="indexterm"/> folder <code class="literal">.../mahout/chapter6/svmexample/</code>.</p></div><div class="section" title="Using R"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec88"/>Using R</h3></div></div></div><p>Refer to <a id="id815" class="indexterm"/>the <a id="id816" class="indexterm"/>folder <code class="literal">.../r/chapter6/svmexample/</code>.</p></div><div class="section" title="Using Spark"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec89"/>Using Spark</h3></div></div></div><p>Refer to <a id="id817" class="indexterm"/>the<a id="id818" class="indexterm"/> folder <code class="literal">.../spark/chapter6/svmexample/</code>.</p></div><div class="section" title="Using Python (Scikit-learn)"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec90"/>Using Python (Scikit-learn)</h3></div></div></div><p>Refer<a id="id819" class="indexterm"/> to the<a id="id820" class="indexterm"/> folder <code class="literal">.../python-scikit-learn/chapter6/svmexample/</code>.</p></div><div class="section" title="Using Julia"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec91"/>Using Julia</h3></div></div></div><p>Refer to <a id="id821" class="indexterm"/>the<a id="id822" class="indexterm"/> folder <code class="literal">.../julia/chapter6/svmexample/</code>.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec36"/>Summary</h1></div></div></div><p>In this chapter, we have explored two learning algorithms, instance-based and kernel methods, and we have seen how they address the classification and prediction requirements. In the instance-based learning methods, we explored the Nearest Neighbor algorithm in detail and have seen how to implement this using our technology stack, Mahout, Spark, R, Julia, and Python. Similarly, in the kernel-based methods, we have explored SVM. In the next chapter, we will cover the Association Rule-based learning methods with a focus on Apriori and FP-growth algorithms.</p></div></body></html>