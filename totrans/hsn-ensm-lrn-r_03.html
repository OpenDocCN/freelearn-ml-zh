<html><head></head><body>
<div class="book" title="Chapter&#xA0;3.&#xA0;Bagging" id="PNV61-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03" class="calibre1"/>Chapter 3. Bagging</h1></div></div></div><p class="calibre7">Decision trees were introduced in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, and then applied to five different classification problems. Here, they can be seen to work better for some databases more than others. We had almost only used the <code class="literal">default</code> settings for the <code class="literal">rpart</code> function when constructing decision trees. This chapter begins with the exploration of some options that are likely to improve the performance of the decision tree. The previous chapter introduced the <code class="literal">bootstrap</code> method, used mainly for statistical methods and models. In this chapter, we will use it for trees. The method is generally accepted as a machine learning technique. Bootstrapping decision trees is widely known as <span class="strong"><em class="calibre9">bagging</em></span>. A similar kind of classification method is k-nearest neighborhood classification, abbreviated as <span class="strong"><em class="calibre9">k</em></span>-NN. We will introduce this method in the third section and apply the bagging technique for this method in the concluding section of the chapter.</p><p class="calibre7">In this chapter, we will cover the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Classification trees and related pruning/improvement methods</li><li class="listitem">Bagging the classification tree</li><li class="listitem">Introduction and application of the <span class="strong"><em class="calibre9">k</em></span>-NN classifier</li><li class="listitem"><span class="strong"><em class="calibre9">k</em></span>-NN bagging extension</li></ul></div></div>

<div class="book" title="Chapter&#xA0;3.&#xA0;Bagging" id="PNV61-2006c10fab20488594398dc4871637ee">
<div class="book" title="Technical requirements"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch03lvl1sec25" class="calibre1"/>Technical requirements</h1></div></div></div><p class="calibre7">We will be using the following libraries in the chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="literal">class</code></li><li class="listitem"><code class="literal">FNN</code></li><li class="listitem"><code class="literal">ipred</code></li><li class="listitem"><code class="literal">mlbench</code></li><li class="listitem"><code class="literal">rpar</code></li></ul></div></div></div>
<div class="book" title="Classification trees and pruning"><div class="book" id="QMFO2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec26" class="calibre1"/>Classification trees and pruning</h1></div></div></div><p class="calibre7">A classification tree is a particular type of decision tree, and its focus is mainly on classification problems. Breiman, et al. (1984) invented the decision tree and Quinlan (1984) independently introduced<a id="id139" class="calibre1"/> the C4.5 algorithm. Both of these had a lot in common, but we will focus on the Breiman school of decision trees. Hastie, et al. (2009) gives a comprehensive treatment of decision trees, and Zhang and Singer (2010) offer a treatise on<a id="id140" class="calibre1"/> the recursive partitioning methods. An intuitive and systematic R programmatic development of the trees can be found in <a class="calibre1" title="Chapter 9. Ensembling Regression Models" href="part0062_split_000.html#1R42S1-2006c10fab20488594398dc4871637ee">Chapter 9</a>, <span class="strong"><em class="calibre9">Ensembling Regression Models</em></span>, of Tattar (2017).</p><p class="calibre7">A classification tree has many arguments that can be fine-tuned for improving performance. However, we will first simply construct the classification tree with default settings and visualize the tree. The <code class="literal">rpart</code> function from the <code class="literal">rpart</code> package can create classification, regression, as well as survival trees. The function first inspects whether the regress and is a categorical, numerical, or survival object and accordingly sets up the respective classification, regression, or survival trees as well as using the relevant split function.</p><p class="calibre7">The <span class="strong"><strong class="calibre8">German credit</strong></span> dataset is loaded <a id="id141" class="calibre1"/>and the exercise of splitting the data into train and test parts is carried out as in earlier settings:</p><div class="informalexample"><pre class="programlisting">&gt; load("../Data/GC2.RData")
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(GC2),replace = TRUE,
+    prob = c(0.7,0.3))
&gt; GC2_Train &lt;- GC2[Train_Test=="Train",]
&gt; GC2_TestX &lt;- within(GC2[Train_Test=="Test",],rm(good_bad))
&gt; GC2_TestY &lt;- GC2[Train_Test=="Test","good_bad"]
&gt; nte &lt;- nrow(GC2_TestX)
&gt; GC2_Formula &lt;- as.formula("good_bad~.")</pre></div><p class="calibre7">You can refer to <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span> to understand how the R code is performed. Now, using the training dataset and the chosen formula, we will create the first classification tree. The <code class="literal">rpart</code> function applied on the formula and dataset will create a classification tree. The tree is visualized using the <code class="literal">plot</code> function, and the <code class="literal">uniform=TRUE</code> option ensures that the display aligns the splits at their correct hierarchical levels. Furthermore, the text function will display the variable names at the split points and <code class="literal">use.n=TRUE</code> will give the distribution of the Ys at the node. Finally, the fitted classification tree is then used to <code class="literal">predict</code> the good/bad loans for the test dataset, comparisons are made for the test sample, and we find the accuracy of the tree is 70.61%, which is the same as in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>:</p><div class="informalexample"><pre class="programlisting">&gt; DT_01 &lt;- rpart(GC2_Formula,GC2_Train)
&gt; windows(height=200,width=200)
&gt; par(mfrow=c(1,2))
&gt; plot(DT_01,uniform=TRUE,main="DT - 01"); text(DT_01,use.n=TRUE)
&gt; DT_01_predict &lt;- predict(DT_01,newdata = GC2_TestX,type="class")
&gt; DT_01_Accuracy &lt;- sum(DT_01_predict==GC2_TestY)/nte
&gt; DT_01_Accuracy
[1] 0.7060703
&gt; DT_01$variable.importance
checking duration  savings  purpose employed  history   amount    coapp 
 38.5358  19.6081  15.6824  12.8583  12.5501   9.2985   8.9475   8.1326 
     age  existcr property      job resident telephon  housing  depends 
  7.3921   6.0250   5.5503   5.2012   2.6356   1.6327   1.3594   0.6871 
 marital installp  foreign 
  0.6871   0.4836   0.2045 </pre></div><p class="calibre7">The plot of the preceding<a id="id142" class="calibre1"/> code is the left tree display, <code class="literal">DT-01</code>, of the next diagram. It can be<a id="id143" class="calibre1"/> seen from the display that there are way too many terminal nodes and there also appears to be many splits, meaning that there is a chance that we are overfitting the data. Some of the terminal nodes have as few as seven observations while many terminal nodes have less than 20 observations. Consequently, there is room for improvement.</p><p class="calibre7">In the next iteration of the decision tree, we ask the tree algorithm not to split further if we have less than 30 observations in the node <code class="literal">(minsplit=30)</code>, and that the minimum bucket size <code class="literal">(minbucket=15)</code> must be at least 15. This change should give us an improvisation over the tree, <code class="literal">DT_01</code>. For the new tree, we will again check the change in accuracy:</p><div class="informalexample"><pre class="programlisting">&gt; DT_02 &lt;- rpart(GC2_Formula,GC2_Train,minsplit=30,minbucket=15)
&gt; plot(DT_02,uniform=TRUE,main="DT - 02"); text(DT_02,use.n=TRUE)
&gt; DT_02_predict &lt;- predict(DT_02,newdata = GC2_TestX,type="class")
&gt; DT_02_Accuracy &lt;- sum(DT_02_predict==GC2_TestY)/nte
&gt; DT_02_Accuracy
[1] 0.7252396
&gt; DT_02$variable.importance
checking duration  savings  purpose  history   amount    coapp employed 
 35.2436  15.5220  15.3025  11.6655   7.8141   7.5564   7.1990   5.6960 
property  existcr      age resident  foreign  depends  marital      job 
  3.7257   1.7646   1.3781   1.1833   0.7883   0.6871   0.6871   0.5353 
 housing installp 
  0.5072   0.4581 </pre></div><div class="mediaobject"><img src="../images/00169.jpeg" alt="Classification trees and pruning" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: Classification trees for German data</p></div></div><p class="calibre11"> </p><p class="calibre7">The <code class="literal">DT-02</code> tree appears cleaner<a id="id144" class="calibre1"/> than <code class="literal">DT-01</code>, and each terminal node has a considerably good number of observations. Importantly, the<a id="id145" class="calibre1"/> accuracy is improved to <code class="literal">0.7252 - 0.7061 = 0.0191</code>, or about 2%, which is an improvement.</p><p class="calibre7">The <span class="strong"><strong class="calibre8">complexity parameter</strong></span>, <span class="strong"><strong class="calibre8">Cp</strong></span>, is an<a id="id146" class="calibre1"/> important aspect of the trees and we will now use it in improving the classification tree. Using the argument <code class="literal">cp=0.005</code> along with the <code class="literal">minsplit</code> and <code class="literal">minbucket</code>, we will try to improve the performance of the tree:</p><div class="informalexample"><pre class="programlisting">&gt; DT_03 &lt;- rpart(GC2_Formula,GC2_Train,minsplit=30,minbucket=15,
+                cp=0.005)
&gt; plot(DT_03,uniform=TRUE,main="DT - 03"); text(DT_03,use.n=TRUE)
&gt; DT_03_predict &lt;- predict(DT_03,newdata = GC2_TestX,type="class")
&gt; DT_03_Accuracy &lt;- sum(DT_03_predict==GC2_TestY)/nte
&gt; DT_03_Accuracy
[1] 0.7316294
&gt; DT_03$variable.importance
checking duration  savings  purpose  history employed   amount    coapp 
 35.7201  15.5220  15.3025  11.6655   7.8141   7.7610   7.5564   7.1990 
property      age  existcr resident  marital  foreign installp  depends 
  3.7257   1.8547   1.7646   1.5010   1.0048   0.7883   0.7758   0.6871 
     job  housing 
  0.5353   0.5072</pre></div><p class="calibre7">The performance has now changed from <code class="literal">0.7252</code> to <code class="literal">0.7316</code>, and this is an improvement again. The tree complexity structure <a id="id147" class="calibre1"/>does not seem to have changed much in <code class="literal">DT-03</code>, the left-side<a id="id148" class="calibre1"/> tree of the following diagram. We now carry out two changes simultaneously. First, we change the split criteria from Gini to information, and then we add a loss matrix for misclassification.</p><p class="calibre7">What is a loss matrix for misclassification? If a good loan is identified or predicted by the model as a good loan in this way, we have no misclassification. Furthermore, if a bad loan is classified as a bad loan, it is also the correct decision identified by the algorithm. The consequence of misclassifying a good loan as bad is not the same as classifying the bad as good. For instance, if a bad customer is granted a loan, the loss would be a four-six digit revenue loss, while a good customer who is denied a loan might apply again in 3 months' time. If you ran <code class="literal">matrix( c(0,200,500,0),byrow = TRUE,nrow=2)</code>, the output would be the following:</p><div class="informalexample"><pre class="programlisting">&gt; matrix(c(0,200,500,0),byrow = TRUE,nrow=2)
     [,1] [,2]
[1,]    0  200
[2,]  500    0</pre></div><p class="calibre7">This means that the penalty for misclassifying a good loan as bad is 200 while misclassifying a bad loan as good is 500. Penalties help a lot and gives weight to the classification problem. With this option and the split criteria, we set up the next classification tree:</p><div class="informalexample"><pre class="programlisting">&gt; DT_04 &lt;- rpart(GC2_Formula,GC2_Train,minsplit=30,minbucket=15,
+                parms = list(split="information",
+                             loss=matrix(c(0,200,500,0),byrow = TRUE,nrow=2)))
&gt; plot(DT_04,uniform=TRUE,main="DT - 04"); text(DT_04,use.n=TRUE)
&gt; DT_04_predict &lt;- predict(DT_04,newdata = GC2_TestX,type="class")
&gt; DT_04_Accuracy &lt;- sum(DT_04_predict==GC2_TestY)/nte
&gt; DT_04_Accuracy
[1] 0.7380192
&gt; DT_04$variable.importance
checking  savings duration  purpose employed  history   amount  existcr 
 26.0182  10.4096  10.2363   5.0949   5.0434   2.1544   1.5439   0.9943 
resident      age  depends  marital property 
  0.9648   0.7457   0.6432   0.6432   0.5360 
</pre></div><div class="mediaobject"><img src="../images/00170.jpeg" alt="Classification trees and pruning" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: Classification trees with further options</p></div></div><p class="calibre11"> </p><p class="calibre7">Note that the decision tree <code class="literal">DT-04</code> appears<a id="id149" class="calibre1"/> to have far fewer splits than <code class="literal">DT-01-03</code>, and does not appear to over-train the data.</p><p class="calibre7">Here, we can see<a id="id150" class="calibre1"/> many options that can be used to tweak the decision trees, and some of these are appropriate for the classification tree only. However, with some of the parameters tweaking might need expert knowledge, although it is nice to be aware of the options. Note how the order of the variable importance changes from one decision tree to another. Given the data and the tree structure, how reliably can we determine the variable importance of a given variable? This question will be covered in the next section. A general way of improving the performance of the decision tree will also be explored next.</p></div>
<div class="book" title="Bagging"><div class="book" id="RL0A2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec27" class="calibre1"/>Bagging</h1></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre8">Bagging</strong></span> stands for <span class="strong"><strong class="calibre8">B</strong></span>oostap <span class="strong"><strong class="calibre8">AGG</strong></span>regat<span class="strong"><strong class="calibre8">ING</strong></span>. This was invented by Breiman (1994). Bagging is an example of an <span class="strong"><em class="calibre9">homogeneous ensemble</em></span> and this is because the base learning algorithm remains as the <a id="id151" class="calibre1"/>classification tree. Here, each bootstrap tree will be a base learner. This also means that when we bootstrapped the linear regression model in <a class="calibre1" title="Chapter 2. Bootstrapping" href="part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee">Chapter 2</a>, <span class="strong"><em class="calibre9">Bootstrapping</em></span>, we actually performed an ensemble there. A few remarks with regards to combining the results of multiple trees is in order here.</p><p class="calibre7">Ensemble methods combine the outputs<a id="id152" class="calibre1"/> from multiple models, also known as base learners, and produce a single result. A benefit of this approach is that if each of these base learners possesses a <span class="strong"><em class="calibre9">desired property</em></span>, then the combined result will have increased stability. If a certain base learner is over-trained in a specific region of the covariate space, the other base learner will nullify such an undesired prediction. It is the increased stability that is expected from the ensemble, and bagging many times helps to improve the performance of fitted values from a given set of models. Berk (2016), Seni and Elder (2010), and Hastie, et al. (2009) may be referred to for more details.</p><p class="calibre7">
<span class="strong"><em class="calibre9">A basic result!</em></span> If <span class="strong"><em class="calibre9">N</em></span> observations are drawn with replacements from <span class="strong"><em class="calibre9">N</em></span> units, then 37% of the observations are left out on average.</p><p class="calibre7">This result is an important one. Since we will be carrying out a bootstrap method, it means that on average for each tree, we will have a holdout sample of 37%. A brief simulation program will compute the probability for us. For an <span class="strong"><em class="calibre9">N</em></span> value ranging from 11 to 100, we will draw a sample with a replacement, find how many indexes are left out, and divide the number by <span class="strong"><em class="calibre9">N</em></span> to obtain the empirical probability of the number of units left out of this simulation. The empirical probability is obtained via <span class="strong"><em class="calibre9">B = 100,000</em></span> a number of times, and that average is reported as the probability of any individual not being selected in a draw of <span class="strong"><em class="calibre9">N</em></span> items drawn from <span class="strong"><em class="calibre9">N</em></span> items with replacements:</p><div class="informalexample"><pre class="programlisting">&gt; N &lt;- 11:100
&gt; B &lt;- 1e5
&gt; Prob_Avg &lt;- NULL
&gt; for(i in N){
+   set &lt;- 1:i
+   leftout &lt;- 0
+   for(j in 1:B){
+     s1 &lt;- sample(set,i,replace=TRUE)
+     leftout &lt;- leftout+(i-length(unique(s1)))/i
+   }
+   Prob_Avg[i-10] &lt;- leftout/B
+ }
&gt; Prob_Avg
 [1] 0.3504 0.3517 0.3534 0.3549 0.3552 0.3563 0.3571 0.3574 0.3579 0.3585
[11] 0.3585 0.3594 0.3594 0.3601 0.3604 0.3606 0.3610 0.3612 0.3613 0.3614
[21] 0.3620 0.3622 0.3625 0.3622 0.3626 0.3627 0.3627 0.3626 0.3631 0.3634
[31] 0.3635 0.3637 0.3636 0.3638 0.3639 0.3638 0.3640 0.3641 0.3641 0.3641
[41] 0.3644 0.3642 0.3645 0.3643 0.3645 0.3647 0.3645 0.3646 0.3649 0.3649
[51] 0.3648 0.3650 0.3648 0.3650 0.3651 0.3653 0.3649 0.3649 0.3653 0.3653
[61] 0.3654 0.3654 0.3654 0.3654 0.3653 0.3655 0.3654 0.3655 0.3655 0.3657
[71] 0.3657 0.3657 0.3655 0.3658 0.3658 0.3660 0.3656 0.3658 0.3658 0.3658
[81] 0.3658 0.3658 0.3660 0.3658 0.3659 0.3659 0.3662 0.3660 0.3661 0.3661</pre></div><p class="calibre7">Consequently, we can see that in a sample of <span class="strong"><em class="calibre9">N </em></span>drawn from <span class="strong"><em class="calibre9">N </em></span>items with replacements, approximately 0.37 or 37% observations are not selected.</p><p class="calibre7">The bagging algorithm is<a id="id153" class="calibre1"/> given as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Draw a random sample of size <span class="strong"><em class="calibre9">N</em></span> with replacements from the data consisting of <span class="strong"><em class="calibre9">N </em></span>observations. The selected random sample<span class="strong"><em class="calibre9"> </em></span>is called a <span class="strong"><strong class="calibre8">bootstrap sample</strong></span><span class="strong"><em class="calibre9">.</em></span></li><li class="listitem">Construct a classification tree from the bootstrap sample.</li><li class="listitem">Assign a class to each terminal node and store the predicted class of each observation.</li><li class="listitem">Repeat steps 1-3 a large number of times, for example, <span class="strong"><em class="calibre9">B</em></span>.</li><li class="listitem">Assign each observation to a final class by a majority vote over the set of trees.</li></ul></div><p class="calibre7">The main purpose of the bagging procedure is to reduce instability, and this is mainly achieved through the <code class="literal">bootstrap</code> method. As noted earlier, and proven by a simulation program, when we draw <span class="strong"><em class="calibre9">N</em></span> observations with replacements from <span class="strong"><em class="calibre9">N</em></span> observations, on average, 37% observations would be excluded from the sample. The <code class="literal">bagging</code> method takes advantage of this sampling with a replacement technique and we call the unselected observations <span class="strong"><strong class="calibre8">out-of-bag</strong></span> (<span class="strong"><strong class="calibre8">OOB</strong></span>) observations. As with the <code class="literal">bootstrap</code> method, the resampling technique gives us multiple estimates of the different parameters and, using such a sampling distribution, it is then possible to carry out the appropriate statistical inference. For a more rigorous justification of this technique, the original Breiman (1996) paper is a good read.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note02" class="calibre1"/>Note</h3><p class="calibre7">
<span class="strong"><em class="calibre9">A word of caution</em></span>: Suppose an observation has been an out-of-bag observation for, say, 100 times out of 271 trees. In the 100th instance when the observation is marked for testing purposes, the observation might be classified as <code class="literal">TRUE</code> in <span class="strong"><em class="calibre9">70</em></span> of them. It is then tempting to conclude for the observation that <span class="strong"><em class="calibre9">P(TRUE) = 70/100 = 0.7</em></span>. Such an interpretation can be misleading since the samples in the bag are not independent.</p></div><p class="calibre7">Before we proceed to the software implementation, two important remarks from Hastie, et al. (2009) are in order. First, the bagging technique helps in reducing the variance of the estimated prediction function and it also works for high-variance, low-bias models, such as trees. Second, the core aim of bagging in aggregation is to average many noisy models that are unbiased, hence the subsequent reduction in variance. As with the bootstrap method, bagging reduces bias as it is a smoothing method. We will now illustrate the bagging method using the German credit data.</p><p class="calibre7">The <code class="literal">bagging </code>function from the <code class="literal">ipred </code>package will help in setting up the procedure:</p><div class="informalexample"><pre class="programlisting">&gt; B &lt;- 500
&gt; GC2_Bagging &lt;- bagging(GC2_Formula,data=GC2_Train,coob=FALSE,
+                        nbagg=B,keepX=TRUE)
&gt; GC2_Margin &lt;- predict(GC2_Bagging,newdata = GC2_TestX,
+                       aggregation="weighted",type="class")
&gt; sum(GC2_Margin==GC2_TestY)/nte
[1] 0.7795527</pre></div><p class="calibre7">Aggregation has helped here<a id="id154" class="calibre1"/> by significantly increasing the accuracy. Note that each run of the bagging method will result in a different answer. This is mainly because each tree is set up with different samples and the seeds generating the random samples are allowed to change dynamically. Until now, made attempts to fix the seeds at a certain number to reproduce the results. Going forward, the seeds will seldom be fixed. As an exercise to test your knowledge, work out what the <code class="literal">keepx</code> and <code class="literal">coob options</code> specified in the bagging function are. Do this by using <code class="literal">?bagging</code>.</p><p class="calibre7">Back to the German credit problem! We have created <span class="strong"><em class="calibre9">B = 500</em></span> trees and for some crazy reason we want to view all of them. Of course, Packt (the publisher of this book) would probably be a bit annoyed if the author insisted on printing all 500 trees, and besides, the trees would look ugly as they are not pruned. The program must be completed with respect to the book size constraints. With that in mind, let's kick off with the following code:</p><div class="informalexample"><pre class="programlisting">&gt; pdf("../Output/GC2_Bagging_Trees.pdf")
&gt; for(i in 1:B){
+   tt &lt;- GC2_Bagging$mtrees[[i]]
+   plot(tt$btree)
+   text(tt$btree,use.n=TRUE)
+ }
&gt; dev.off()
pdf 
  2 </pre></div><p class="calibre7">The following steps were performed in the preceding code:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">We will first invoke the PDF device.</li><li class="listitem" value="2">A new file is then created in the <code class="literal">Output</code> folder.</li><li class="listitem" value="3">Next, we begin a loop from <code class="literal">1</code> to <code class="literal">B</code>. The <code class="literal">bagging</code> object constitutes of <code class="literal">B = 500</code> trees and in a temporary object, <code class="literal">tt</code>, we store the details of the ith tree.</li><li class="listitem" value="4">We then plot that tree using the <code class="literal">plot</code> function by extracting the tree details out of <code class="literal">tt</code>, and adding the relevant text associated with the nodes and splits of that tree.</li><li class="listitem" value="5">After the loop is completed, the <code class="literal">dev.off</code> line is run, which will then save the <code class="literal">GC2_Bagging_Trees.pdf</code> file. This portable document file will consist of 500 trees.</li></ol><div class="calibre13"/></div><p class="calibre7">A lot has been said about the benefits of bootstrapping and a lot has also been illustrated in the previous chapter. However, putting aside the usual advantages of bagging, which are shown in many blogs and references, we will show here how to also get reliable inference of the variable importance. It can be easily seen that the variable importance varies a lot across the trees. This is not a problem, however. When we are asked about the overall reliability of the <code class="literal">variable importance</code> in decision trees for each variable, we can now look at their values across the trees and carry out the inference:</p><div class="informalexample"><pre class="programlisting">&gt; VI &lt;- data.frame(matrix(0,nrow=B,ncol=ncol(GC2)-1))
&gt; vnames &lt;- names(GC2)[-20]
&gt; names(VI) &lt;- vnames
&gt; for(i in 1:B){
+   VI[i,] &lt;- GC2_Bagging$mtrees[[i]]$btree$variable.importance[vnames]
+ }
&gt; colMeans(VI)
checking duration  history  purpose   amount  savings employed installp 
  50.282   58.920   33.540   48.301   74.721   30.838   32.865   18.722 
 marital    coapp resident property      age  housing  existcr      job 
  17.424    8.795   18.171   20.591   51.611    9.756   11.433   14.015 
 depends telephon  foreign 
      NA       NA       NA </pre></div><p class="calibre7">The preceding program<a id="id155" class="calibre1"/> needs an explanation. Recollect that <code class="literal">variable.importance</code> is displayed in descending order. If you have gone through the <code class="literal">GC2_Bagging_Trees.pdf</code> file (even a cursory look will do), it can be seen that different trees have different variables to the primary split, and consequently the order of importance for variables would be different. Thus, we first save the order of variable that we need in <code class="literal">vnames</code> object, and then order for each tree <code class="literal">variable.importance[vnames]</code> by the same order as in <code class="literal">vnames</code>. Each tree in the loop is extracted with <code class="literal">$mtrees</code> and <code class="literal">$btree$variable.importance</code> to do what is required. Thus, the <code class="literal">VI data.frame</code> object now consists of the variable importance of all 500 trees set up by the bagging procedure. The <code class="literal">colMeans</code> gives the desired aggregate of the importance across the 500 trees, and the desired statistical inference can be carried out by looking across the detailed information in the <code class="literal">VI</code> frame. Note that the last three variables have <code class="literal">NA</code> in the aggregated mean. The reason for the <code class="literal">NA</code> result is that in some classification trees, these variables provide no gains whatsoever and are not even among any surrogate split. We can quickly discover how many trees contain no information on the importance of these three variables, and then repeat the calculation of the <code class="literal">colMeans</code> using the <code class="literal">na.rm=TRUE</code> option:</p><div class="informalexample"><pre class="programlisting">&gt; sapply(VI,function(x) sum(is.na(x)))
checking duration  history  purpose   amount  savings employed installp 
       0        0        0        0        0        0        0        0 
 marital    coapp resident property      age  housing  existcr      job 
       0        0        0        0        0        0        0        0 
 depends telephon  foreign 
       9       35       20 
&gt; colMeans(VI,na.rm=TRUE)
checking duration  history  purpose   amount  savings employed installp 
  50.282   58.920   33.540   48.301   74.721   30.838   32.865   18.722 
 marital    coapp resident property      age  housing  existcr      job 
  17.424    8.795   18.171   20.591   51.611    9.756   11.433   14.015 
 depends telephon  foreign 
   6.345    5.167    3.200 </pre></div><p class="calibre7">In the previous section, we explored a host of options in <code class="literal">minsplit</code>, <code class="literal">minbucket</code>, <code class="literal">split</code>, and <code class="literal">loss</code> arguments. Can bagging incorporate these metrics? Using the <code class="literal">control</code> argument for the <code class="literal">bagging</code> function, we will now improvise on the earlier results. The choice of the additional parameters<a id="id156" class="calibre1"/> is kept the same as earlier. After fitting the bagging object, we inspect the accuracy, and then write the classification trees to the <code class="literal">GC2_Bagging_Trees_02.pdf</code> file. Clearly, the trees in this file are much more readable than the counterparts in the <code class="literal">GC2_Bagging_Trees.pdf</code> file, and expectedly so. The variable information table is also obtained for the <code class="literal">B = 500</code> trees with the following code:</p><div class="informalexample"><pre class="programlisting">&gt; GC2_Bagging_02 &lt;- bagging(GC2_Formula,data=GC2_Train,coob=FALSE,
+                        nbagg=B,keepX=TRUE,
+                        control=rpart.control(minsplit=30,minbucket=15,
+split="information",loss=matrix(c(0,200,500,0), byrow = TRUE, nrow=2)))
&gt; GC2_Margin_02 &lt;- predict(GC2_Bagging_02,newdata = GC2_TestX,
+                       aggregation="weighted",type="class")
&gt; sum(GC2_Margin_02==GC2_TestY)/nte
[1] 0.7604
&gt; pdf("../Output/GC2_Bagging_Trees_02.pdf")
&gt; for(i in 1:B){
+   tt &lt;- GC2_Bagging_02$mtrees[[i]]
+   plot(tt$btree)
+   text(tt$btree,use.n=TRUE)
+ }
&gt; dev.off()
null device 
          1 
&gt; VI_02 &lt;- data.frame(matrix(0,nrow=B,ncol=ncol(GC2)-1))
&gt; names(VI_02) &lt;- vnames
&gt; for(i in 1:B){
+   VI_02[i,] &lt;- GC2_Bagging_02$mtrees[[i]]$btree$variable.importance[vnames]
+ }
&gt; colMeans(VI_02,na.rm=TRUE)
checking duration  history  purpose   amount  savings employed installp 
 38.3075  18.9377  11.6756  19.1818  18.4385  16.1309   9.6110   3.6417 
 marital    coapp resident property      age  housing  existcr      job 
  4.3520   4.4913   3.4810   6.5278  10.0255   3.3401   3.1011   4.5115 
 depends telephon  foreign 
  1.6432   2.5535   0.9193 </pre></div><p class="calibre7">The number of trees<a id="id157" class="calibre1"/> for bagging has been chosen arbitrarily at 500. There is no particular reason for this. We will now see how the accuracy over test data changes over the number of trees chosen. The inspection will happen over the choice of the number of trees varying from 1 to 25 at an increment of 1, and then by incrementing by 25 to 50, 75, …, 475, 500. The reader is left to make sense out of this plot. Meanwhile, the following program is straightforward and does not require further explanation:</p><div class="informalexample"><pre class="programlisting">&gt; Bags &lt;- c(1:24,seq(25,B,25))
&gt; Bag_Acc &lt;- NULL
&gt; for(i in 1:length(Bags)){
+   TBAG &lt;- bagging(GC2_Formula,data=GC2_Train,coob=FALSE,
+                       nbagg=i,keepX=TRUE,
+                       control=rpart.control(minsplit=30,minbucket=15,
+                                             split="information",
+                                             loss=matrix(c(0,200,500,0),
+                                                         byrow = TRUE,
+                                                         nrow=2)))
+   GC2_Margin_TBAG &lt;- predict(TBAG,newdata = GC2_TestX,
+                            aggregation="weighted",type="class")
+   Bag_Acc[i] &lt;- sum(GC2_Margin_TBAG==GC2_TestY)/nte
+   print(Bags[i])
+ }
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5

[1] 23
[1] 24
[1] 25
[1] 50
[1] 75

[1] 475
[1] 500
&gt; plot(Bags,Bag_Acc,"l",ylab="Accuracy")</pre></div><p class="calibre7">The following is the <a id="id158" class="calibre1"/>output that is generated:</p><div class="mediaobject"><img src="../images/00171.jpeg" alt="Bagging" class="calibre10"/><div class="caption"><p class="calibre14">Figure 3: Bagging accuracy over the number of trees</p></div></div><p class="calibre11"> </p><p class="calibre7">Analytical techniques<a id="id159" class="calibre1"/> don't belong to the family of alchemies. If such a procedure is invented, we won't have to worry about modeling. The next example shows that bagging can also go wrong.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Bagging is not a guaranteed recipe!</strong></span>
</p><p class="calibre7">In the first edition of his book, Berk (2016) cautions the reader not to fall prey to the proclaimed superiority of the newly invented methods. Recollect the Pima Indians Diabetes problem introduced in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>. The accuracy table in there shows that the decision tree gives an accuracy of <code class="literal">0.7588</code>. We now apply the bagging method for the same partition and compute the accuracy as follows:</p><div class="informalexample"><pre class="programlisting">&gt; data("PimaIndiansDiabetes")
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(PimaIndiansDiabetes),replace = TRUE,prob = c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; PimaIndiansDiabetes_Train &lt;- PimaIndiansDiabetes[Train_Test=="Train",]
&gt; PimaIndiansDiabetes_TestX &lt;- within(PimaIndiansDiabetes[Train_Test=="Test",],
+                                     rm(diabetes))
&gt; PimaIndiansDiabetes_TestY &lt;- PimaIndiansDiabetes[Train_Test=="Test","diabetes"]
&gt; PID_Formula &lt;- as.formula("diabetes~.")
&gt; PID_Bagging &lt;- bagging(PID_Formula,data=PimaIndiansDiabetes_Train,coob=FALSE,
+                        nbagg=1000,keepX=TRUE)
&gt; PID_Margin &lt;- predict(PID_Bagging,newdata = PimaIndiansDiabetes_TestX,
+                       aggregation="weighted",type="class")
&gt; sum(PID_Margin==PimaIndiansDiabetes_TestY)/257
[1] 0.7548638</pre></div><p class="calibre7">The overall accuracy here is <code class="literal">0.7548</code> while that of the single tree classification model was <code class="literal">0.7588</code>, which means that bagging has decreased the accuracy. This is not a worry if one understands the purpose of bagging. The purpose was always to increase the stability of the predictions, and, as such, we would rather accept that bagging would have decreased the variance term.</p><p class="calibre7">Bagging is a very<a id="id160" class="calibre1"/> important technique, and you need not restrict it to classification trees alone, but should rather go ahead and explore with other regression methods too, such as neural networks and support vector machines. In the next section, we will introduce the different approach of k-nearest neighbors (<span class="strong"><em class="calibre9">k</em></span>-NN).</p></div>

<div class="book" title="k-NN classifier" id="SJGS1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec28" class="calibre1"/>
<span class="strong"><em class="calibre9">k</em></span>-NN classifier
</h1></div></div></div><p class="calibre7">In <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, we became familiar with a variety of classification models. Some<a id="id161" class="calibre1"/> readers might already be familiar with the <span class="strong"><em class="calibre9">k</em></span>-NN model. The <span class="strong"><em class="calibre9">k</em></span>-NN classifier is one of the most simple, intuitive, and non-assumptive models. The name of the model itself suggests how it might be working - nearest neighborhoods! And that's preceded by <span class="strong"><em class="calibre9">k</em></span>! Thus, if we have <span class="strong"><em class="calibre9">N</em></span> points in a study, we find the <span class="strong"><em class="calibre9">k</em></span>-nearest points in neighborhood, and then make a note of the class of the k-neighbors. The majority class of the <span class="strong"><em class="calibre9">k-</em></span>neighbors is then assigned to the unit. In case of regression, the average of the neighbors is assigned to the unit. The following is a visual depiction of <span class="strong"><em class="calibre9">k</em></span>-NN:</p><div class="mediaobject"><img src="../images/00172.jpeg" alt="k-NN classifier" class="calibre10"/><div class="caption"><p class="calibre14">Figure 4: Visual depiction of <span class="strong"><em class="calibre9">k</em></span>-NN</p></div></div><p class="calibre11"> </p><p class="calibre7">The top left part of the <a id="id162" class="calibre1"/>visual depiction of <span class="strong"><em class="calibre9">k</em></span>-NN shows the scatterplot of 27 observations, 16 of which are circles and the remaining 11 are squares. The circles are marked in orange <span class="strong"><img src="../images/00173.jpeg" alt="k-NN classifier" class="calibre15"/></span> while the squares are marked in blue <span class="strong"><img src="../images/00174.jpeg" alt="k-NN classifier" class="calibre15"/></span>. Suppose we choose to set up a classifier based on k = 3 neighbors. For every point, we then find its three neighbors and assign the majority color to it. Thus, if a circle remains orange, it has been correctly identified, and, similarly, if a square is correctly identified, its color will remain blue. However, if a circle point has two or more of its three nearest neighbors as squares, its color will change to blue and this will be denoted by <span class="strong"><img src="../images/00175.jpeg" alt="k-NN classifier" class="calibre15"/></span>. Similarly, the color of an incorrectly classified square will change to orange <span class="strong"><img src="../images/00176.jpeg" alt="k-NN classifier" class="calibre15"/></span>. In the top-right block of the preceding diagram, we have the 3-NN predictions, while the 7-NN predictions can be found in the lower-left panel of the diagram. Note that with 3-NN, we have five misclassifications while with 7-NN, we have seven. Thus, increasing the number of the nearest neighbors does not mean that there will be an increase in the accuracy.</p><p class="calibre7">The bottom right panel is a 1-NN, which will always give a perfect classification. However, it is akin to a broken clock that shows the time perfectly only twice a day.</p></div>

<div class="book" title="k-NN classifier" id="SJGS1-2006c10fab20488594398dc4871637ee">
<div class="book" title="Analyzing waveform data"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec28" class="calibre1"/>Analyzing waveform data</h2></div></div></div><p class="calibre7">Next, we will perform the analyses for the waveform data. Repeat the code from <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, to obtain the waveform data and then partition it for the train and test parts:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(123)
&gt; Waveform &lt;- mlbench.waveform(5000)
&gt; Waveform$classes &lt;- ifelse(Waveform$classes!=3,1,2)
&gt; Waveform_DF &lt;- data.frame(cbind(Waveform$x,Waveform$classes)) # Data Frame
&gt; names(Waveform_DF) &lt;- c(paste0("X",".",1:21),"Classes")
&gt; Waveform_DF$Classes &lt;- as.factor(Waveform_DF$Classes)
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(Waveform_DF),replace = TRUE,
+    prob = c(0.7,0.3))
&gt; Waveform_DF_Train &lt;- Waveform_DF[Train_Test=="Train",]
&gt; Waveform_DF_TestX &lt;- within(Waveform_DF[Train_Test=="Test",],rm(Classes))
&gt; Waveform_DF_TestY &lt;- Waveform_DF[Train_Test=="Test","Classes"]
&gt; Waveform_DF_Formula &lt;- as.formula("Classes~.")
&gt; plot(Waveform_DF_Train$X.1,Waveform_DF_Train$X.8,col=Waveform_DF_Train$Classes)</pre></div><p class="calibre7">A simple scatterplot<a id="id163" class="calibre1"/> display of the two variables <code class="literal">X.1</code> and <code class="literal">X.8</code> by the classes is given in the following diagram. A lot of interplay of colors can be seen in the display and it does not appear that any form of logistic regression model or decision tree will help the case here. It might be that <span class="strong"><em class="calibre9">k</em></span>-NN will be useful in this case:</p><div class="mediaobject"><img src="../images/00177.jpeg" alt="Analyzing waveform data" class="calibre10"/><div class="caption"><p class="calibre14">Figure 5: Scatterplot for waveform data</p></div></div><p class="calibre11"> </p><p class="calibre7">For <code class="literal">k = 10</code>, we first set up a <span class="strong"><em class="calibre9">k</em></span>-NN model for the waveform data. The <code class="literal">knn</code> function from the <code class="literal">class</code> package is used here:</p><div class="informalexample"><pre class="programlisting">&gt; WF_knn &lt;- knn(train=Waveform_DF_Train[,-22],test=Waveform_DF_TestX,
+               cl=Waveform_DF_Train$Classes,k=10)
&gt; sum(Waveform_DF_TestY==WF_knn)/nrow(Waveform_DF_TestX)
[1] 0.903183</pre></div><p class="calibre7">The accuracy of 90.32% looks<a id="id164" class="calibre1"/> promising, and it is better than all the models considered in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, except for the support vector machine. We will expand the search grid for k over 2 to 50:</p><div class="informalexample"><pre class="programlisting">&gt; k &lt;- c(2:15,seq(20,50,5))
&gt; knn_accuracy &lt;- NULL
&gt; for(i in 1:length(k)){
+   WF_temp_knn &lt;- knn(train=Waveform_DF_Train[,-22],test=Waveform_DF_TestX,
+                 cl=Waveform_DF_Train$Classes,k=k[i])
+   knn_accuracy &lt;- c(knn_accuracy,sum(Waveform_DF_TestY==WF_temp_knn)/
+                       nrow(Waveform_DF_TestX))
+ }
&gt; knn_accuracy
 [1] 0.8561 0.8919 0.8893 0.8886 0.8932 0.8985 0.8972 0.8992 0.9012 0.9025
[11] 0.9032 0.9058 0.9105 0.9019 0.8999 0.9072 0.9065 0.9098 0.9118 0.9085
[21] 0.9072</pre></div><p class="calibre7">Note that for k = 40, we get the maximum accuracy. Why? We will put the <span class="strong"><em class="calibre9">k</em></span>-NN in a bag in the next section.</p></div></div>
<div class="book" title="k-NN bagging" id="TI1E1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec29" class="calibre1"/>
<span class="strong"><em class="calibre9">k</em></span>-NN bagging
</h1></div></div></div><p class="calibre7">The <span class="strong"><em class="calibre9">k-NN</em></span> classifier introduced a classification model in the previous section. We can make this robust using the bootstrap method. The broader algorithm remains the same. As with the typical bootstrap<a id="id165" class="calibre1"/> method, we can always write a program consisting of the loop and depending on the number of required bootstrap samples, or bags, the control can be specified easily. However, here we will use a function from the <code class="literal">FNN</code> R package. The <code class="literal">ownn</code> function is useful for carrying out the bagging method on the <span class="strong"><em class="calibre9">k</em></span>-NN classifier.</p><p class="calibre7">The <code class="literal">ownn</code> function requires all variables in the dataset to be numeric. However, we do have many variables that are factor variables. Consequently, we need to tweak the data so that we can use the <code class="literal">ownn</code> function. The covariate data from the training and test dataset are first put together using the <code class="literal">rbind</code> function. Using the <code class="literal">model.matrix</code> function with the formula <code class="literal">~.-1</code>, we convert all factor variables into numeric variables. The important question here is how does the <code class="literal">model.matrix</code> function work? To keep the explanation simple, if a factor variable has m levels, it will create m – 1 new binary variables, which will span the <span class="strong"><em class="calibre9">m</em></span> dimensions. The reason why we combine the training and test covariates is that if any factor has less levels in any of the partition, the number of variables will be unequal and we would not be able to adopt the model built on the training dataset on the test dataset. After obtaining the covariate<a id="id166" class="calibre1"/> matrix with all numeric variables, we will split the covariates into the train and test regions again, specify the <span class="strong"><em class="calibre9">k</em></span>-NN set up with the <code class="literal">ownn</code> function, and predict the accuracy as a consequence of using the bagging method. The program is as follows:</p><div class="informalexample"><pre class="programlisting">&gt; All_Cov &lt;- rbind(GC2_Train[,-20],GC2_TestX)
&gt; All_CovX &lt;- model.matrix(~.-1,All_Cov)
&gt; GC2_Train_Cov &lt;- All_CovX[1:nrow(GC2_Train),]
&gt; GC2_Test_Cov &lt;- All_CovX[(nrow(GC2_Train)+1):nrow(All_CovX),]
&gt; k &lt;- seq(5,50,1)
&gt; knn_accuracy &lt;- NULL
&gt; for(i in 1:length(k)){
+   GC2_knn_Bagging &lt;- ownn(train=GC2_Train_Cov, test=GC2_Test_Cov,
+                           cl=GC2_Train$good_bad,testcl=GC2_TestY,k=k[i])
+  knn_accuracy[i] &lt;- GC2_knn_Bagging$accuracy[3]
+ }
&gt; knn_accuracy
 [1] 0.6198083 0.6293930 0.6357827 0.6549521 0.6549521 0.6645367 0.6869010
 [8] 0.6932907 0.7028754 0.7092652 0.7092652 0.7188498 0.7284345 0.7316294
[15] 0.7348243 0.7348243 0.7412141 0.7412141 0.7444089 0.7476038 0.7476038
[22] 0.7507987 0.7476038 0.7476038 0.7476038 0.7476038 0.7444089 0.7444089
[29] 0.7444089 0.7444089 0.7444089 0.7444089 0.7412141 0.7444089 0.7444089
[36] 0.7444089 0.7412141 0.7412141 0.7412141 0.7412141 0.7444089 0.7444089
[43] 0.7444089 0.7444089 0.7444089 0.7444089
&gt; windows(height=100,width=100)
&gt; plot.ts(knn_accuracy,main="k-NN Accuracy")</pre></div><p class="calibre7">The <code class="literal">train</code>, <code class="literal">test</code>, <code class="literal">cl</code>, and <code class="literal">testcl</code> arguments are straightforward to follow; see <code class="literal">ownn</code>, and we will vary the number of neighbors on a grid of 5-50. Did we specify the number of bags or the bootstrap samples? Now, the bagging is carried out and the bagged predictions are given. It seems that there might be an approximation of the estimate as the package and the function clearly says that<a id="id167" class="calibre1"/> the predictions are based on bagging:</p><div class="informalexample"><pre class="programlisting">&gt; GC2_knn_Bagging$accuracy
      knn      ownn       bnn 
0.7444089 0.7444089 0.7444089 
&gt; GC2_knn_Bagging$bnnpred
  [1] good good good good good good good good good good good good good
 [14] good good good good good good good good good bad  good good good
 [27] good good good good good good good good good good bad  good good

[274] good good good good good good good good good good good good good
[287] good good good good good good good good good bad  bad  good good
[300] good good good good good good good good good good good good good
[313] good
Levels: bad good</pre></div><p class="calibre7">The following accuracy plot shows that the model accuracy is stabilizing after about 20 neighborhoods. Thus, we have carried out the <span class="strong"><em class="calibre9">k</em></span>-NN bagging technique:</p><div class="mediaobject"><img src="../images/00178.jpeg" alt="k-NN bagging" class="calibre10"/><div class="caption"><p class="calibre14">Figure 6: Accuracy of the <span class="strong"><em class="calibre9">k</em></span>-NN bagging</p></div></div><p class="calibre11"> </p></div>
<div class="book" title="Summary" id="UGI01-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec30" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">Bagging is essentially an ensembling method that consists of homogeneous base learners. Bagging was introduced as a bootstrap aggregation method, and we saw some of the advantages of the bootstrap method in <a class="calibre1" title="Chapter 2. Bootstrapping" href="part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee">Chapter 2</a>, <span class="strong"><em class="calibre9">Bootstrapping</em></span>. The advantage of the bagging method is the stabilization of the predictions. This chapter began with modifications for the classification tree, and we saw different methods of improvising the performance of a decision tree so that the tree does not overfit the data. The bagging of the decision tress and the related tricks followed in the next section. We then introduced <span class="strong"><em class="calibre9">k</em></span>-NN as an important classifier and illustrated it with a simple example. The chapter concluded with the bagging extension of the <span class="strong"><em class="calibre9">k</em></span>-NN classifier.</p><p class="calibre7">Bagging helps in reducing the variance of the decision trees. However, the trees of the two bootstrap samples are correlated since a lot of common observations generate them. In the next chapter, we will look at innovative resampling, which will uncorrelate two decision trees.</p></div></body></html>