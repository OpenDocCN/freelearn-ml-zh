- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a Custom ML Model with XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, all of the supervised learning models we have explored have utilized
    the **Amazon Redshift Auto ML** feature, which uses **Amazon SageMaker Autopilot**
    behind the scenes. In this chapter, we will explore how to create custom **machine
    learning** (**ML**) models. Training a custom model gives you the flexibility
    to choose the model type and the hyperparameters to use. This chapter will provide
    examples of this modeling technique. By the end of this chapter, you will know
    how to create a custom XGBoost model and how to prepare the data to train your
    model using Redshift SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing an XGBoost use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost model with Auto off feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires a web browser and access to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Amazon Redshift Serverless endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Redshift Query Editor v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the code used in this chapter here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter10/chapter10.sql](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter10/chapter10.sql)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**XGBoost** gets its name because it is built on the **Gradient Boosting**
    framework. Using a tree-boosting technique provides a fast method for solving
    ML problems. As you have seen in previous chapters, you can specify the model
    type, which can help speed up model training since **SageMaker Autopilot** does
    not have to determine which model type to use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about XGBoost here: [https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html).'
  prefs: []
  type: TYPE_NORMAL
- en: When you create a model with Redshift ML and specify XGBoost as the model type,
    and optionally specify AUTO OFF, this turns off SageMaker Autopilot and you have
    more control of model tuning. For example, you can specify the hyperparameters
    you wish to use. You will see an example of this in the *Creating a binary classification
    model using* *XGBoost* section.
  prefs: []
  type: TYPE_NORMAL
- en: You will have to perform preprocessing when you set **AUTO** to **OFF**. Carrying
    out the preprocessing ensures we will get the best possible model and is also
    necessary since all inputs must be numeric when you set **AUTO** to **OFF**, for
    example, by making sure data is cleansed, categorical variables are encoded, and
    numeric variables are standardized. You will also need to identify the type of
    problem that you have and select an appropriate model to train. You will be able
    to create train and test datasets and evaluate models yourself. You also have
    the ability to tune the hyperparameters. In summary, you get total control of
    the end-to-end ML model training and building.
  prefs: []
  type: TYPE_NORMAL
- en: By using XGBoost with Amazon Redshift ML, you can solve both regression and
    classification problems. You also can specify the learning objective of your model.
    For example, if you are solving a binary classification problem, you would choose
    `binary:logistic` as your objective or use `multi:softmax` for multi-class classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book, the supported learning objectives are `reg:squarederror`,
    `reg:squaredlogerror`, `reg:logistic`, `reg:pseudohubererror`, `reg:tweedie`,
    `binary:logistic`, `binary:hinge`, and `multi:softmax`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about these objectives, see the *Learning Task Parameters*
    section of the XGBoost documentation here: [https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned what XGBoost is, we will take a look at a use case
    where we can apply XGBoost and solve a common business problem using binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing an XGBoost use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be discussing a use case where we want to predict
    whether credit card transactions are fraudulent. We will be going through the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the business problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading, analyzing, and preparing data for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting data into training and testing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing the input variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will use a credit card payment transaction dataset to build
    a binary classification model using XGBoost in Redshift ML. This dataset contains
    customer and terminal information along with the date and amount related to the
    transaction. This dataset also has some derived fields based on **recency**, **frequency**,
    and **monetary** numeric features, along with a few categorical variables, such
    as whether a transaction occurred during the weekend or at night. Our goal is
    to identify whether a transaction is fraudulent or non-fraudulent. This use case
    is taken from [https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook](https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook).
    Please refer to the GitHub repository to learn more about this data generation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: '*Reproducible Machine Learning for Credit Card Fraud Detection - Practical
    Handbook*, Le Borgne, Yann-Aël and Siblini, Wissam and Lebichot, Bertrand and
    Bontempi, Gianluca, [https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook](https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook),
    2022, Université Libre de Bruxelles'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will load our dataset into Amazon Redshift ML and prepare it for model
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading, analyzing, and preparing data for training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we begin, let’s first connect to Redshift as an admin or database developer
    and then load data into Amazon Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: In the following steps, you will create a schema for all of the tables and objects
    needed for this exercise, which involves creating all the needed tables, loading
    data, and creating the views used for data transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to Query Editor v2, connect to the serverless endpoint, and then connect
    to the **dev** database, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Connect to Query Editor v2](img/B19071_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Connect to Query Editor v2
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following step to create the schema. This schema will be used for
    all objects and models created in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, copy the following SQL statement into Query Editor v2 to create the table
    for hosting the customer payment transaction history, which we will load in the
    subsequent step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that you have created the table, you can execute the following command
    in Query Editor v2 to load the table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that you have loaded the data, it’s a good practice to sample some data
    to make sure our data is loaded properly. Run the following query to sample 10
    records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following screenshot, we can see that we have loaded the data correctly
    with a sampling of different transaction IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Data sample](img/B19071_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Data sample
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in earlier chapters, the target variable is the value that we are
    trying to predict in our model. In our use case, we are trying to predict whether
    a transaction is fraudulent. In our dataset, this is the `tx_fraud` attribute,
    which is our target. Let us check our table to see how many transactions were
    flagged as fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command in Query Editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We identify fraudulent transactions in our dataset as those with a `tx_fraud`
    value of `1`. We have identified 14,681 transactions as fraudulent in our dataset.
    Conversely, a `tx_fraud` value of `0` indicates that a transaction is not fraudulent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Fraudulent transactions](img/B19071_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Fraudulent transactions
  prefs: []
  type: TYPE_NORMAL
- en: Let us look at the trend of fraudulent and non-fraudulent transactions over
    the months. We want to analyze whether there are any unusual spikes in fraudulent
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following SQL command in Query Editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that fraudulent transactions increased by nearly 8 percent in 202207
    over 202206:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Fraudulent transaction trends](img/B19071_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Fraudulent transaction trends
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have loaded our data, let’s get our data prepared for model training
    by splitting the data into train and test datasets. The training data is used
    to train the model and the testing data is used to run our prediction queries.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data into train and test datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train the model, we will have transactions that are older than `2022-10-01`,
    which is ~ 80 percent of the transactions.
  prefs: []
  type: TYPE_NORMAL
- en: To test the model, we will use transactions from after `2022-09-30`, which is
    20 percent of the transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the input variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a combination of numeric and categorical variables in our input fields.
    We need to preprocess the categorical variables into one-hot-encoded values and
    standardize the numeric variables. Since we will be using **AUTO OFF**, SageMaker
    does not automatically preprocess the data. Hence, it is important to transform
    various numeric, datetime, and categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical features** (also referred to as nominal) have distinct categories
    or levels. These can be categories without an order to them, such as country or
    gender. Or they can have an order such as level of education (also referred to
    as ordinal).'
  prefs: []
  type: TYPE_NORMAL
- en: Since ML models need to operate on **numeric variables**, we need to apply ordinal
    encoding or one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make things easier, we have created the following view to take care of the
    transformation logic. This view is somewhat lengthy, but actually, what the view
    is doing is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the transaction time in seconds and days
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying one-hot encoding by assigning `0` or `1` to classify transactions as
    weekday, weekend, daytime, or nighttime (such as `TX_DURING_WEEKEND` or `TX_DURING_NIGHT`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying window functions to transactions so that we make it easy to visualize
    the data in 1-day, 7-day, and 30-day intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Execute the following SQL command in Query Editor v2 to create the view by
    applying the transformation logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now that the view is created, let’s sample 10 records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following command in Query Editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see some of our transformed values, such as `tx_time_seconds` and `txn_time_days`,
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Transformed data](img/B19071_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Transformed data
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s quickly review why we needed to create this view:'
  prefs: []
  type: TYPE_NORMAL
- en: Since we are using XGBoost with Auto OFF, we must do our own data preprocessing
    and feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We applied one-hot encoding to our categorical variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We scaled our numeric variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a summary of the view logic:'
  prefs: []
  type: TYPE_NORMAL
- en: The target variable we used is `TX_FRAUD`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The categorical variables we used are `TX_DURING_WEEKEND_IND`, `TX_DURING_WEEKDAY_IND`,
    `TX_DURING_NIGHT_IND`, and `TX_DURING_DAY_IND`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scaled numeric variables are `s_customer_id_nb_tx_1day_window`, `s_customer_id_avg_amount_1day_window`,
    `s_customer_id_nb_tx_7day_window`, `s_customer_id_avg_amount_7day_window,s_customer_id_nb_tx_30day_window`,
    `s_customer_id_avg_amount_30day_window`, `s_terminal_id_nb_tx_1day_window`, `s_terminal_id_risk_1day_window`,
    `s_terminal_id_nb_tx_7day_window`, `s_terminal_id_risk_7day_window`, `s_terminal_id_nb_tx_30day_window`,
    and `s_terminal_id_risk_30day_window`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have now completed data preparation and are ready to create your model!
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model using XGBoost with Auto Off
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this exercise, we are going to create a custom binary classification model
    using the XGBoost algorithm. You can achieve this by setting **AUTO off**. Here
    are the parameters that are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AUTO OFF**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MODEL_TYPE**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OBJECTIVE**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HYPERPARAMETERS**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the complete list of hyperparameter values that are available and their
    defaults, please read the documentation found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_auto_off_create_model](https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html#r_auto_off_create_model)'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a basic understanding of the parameters available with XGBoost,
    you can create the model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a binary classification model using XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s create a model to predict whether a transaction is fraudulent or non-fraudulent.
    As you learned in the previous chapters, creating models with Amazon Redshift
    ML is simply done by running a SQL command that creates a function. As inputs
    (or features), you will be using the attributes from the view that you created
    in the previous section. You will specify `tx_fraud` as the target and give the
    function name, which you will use later in your prediction queries. Additionally,
    you will specify hyperparameters to do your own model tuning. Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following commands in Query Editor v2\. The following is a code
    snippet; you may retrieve the full code from the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/chapter10.sql](https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/chapter10.sql)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The `CREATE MODEL` function is going to invoke the XGBoost algorithm and train
    a binary classification model. We have set `num_round` hyperparameter value to
    `100`, which is the number of rounds to run the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s run `SHOW MODEL` to see whether model training is completed. Run
    the following command in Query Editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Note **Model State** in the following screenshot, which shows your model is
    still training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Show model output](img/B19071_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Show model output
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding screenshot, we notice that the value of `CREATE MODEL` statement
    – **Model Type** is set to **xgboost**. **objective** is set to **binary:logistic**
    and the **num_round** parameter is set to **100**.
  prefs: []
  type: TYPE_NORMAL
- en: When you have a custom model with **AUTO OFF** and specify the hyperparameters,
    the model can be trained much faster. This model will usually finish in under
    10 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the `SHOW MODEL` command again after 10 minutes to check whether model
    training is complete or not. As you can see from the following screenshot, model
    training has completed and the **train:error** field reports the error rate. Most
    datasets have a threshold of *.5*, so our value of **0.051870** is very good,
    as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – SHOW MODEL output](img/B19071_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – SHOW MODEL output
  prefs: []
  type: TYPE_NORMAL
- en: Now, your model is complete and has a good score based on `score – train_error`,
    which is `0.051870`. You are now ready to use it for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Generating predictions and evaluating model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the following query in Query Editor v2, which will compare the actual `tx_fraud`
    value with the `predicted_tx_fraud` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the sample output. In this screenshot, our predicted
    values are the same as the actual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Inference query output](img/B19071_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Inference query output
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we did not get the F1 value for our model from Redshift ML, let’s calculate
    it. We will create a view that contains the logic to accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following SQL command in Query Editor v2 to check the F1 score that
    we calculated in the view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see our accuracy is 90 percent and our F1 score is 87 percent, which
    are both very good. Additionally, our confusion matrix values tell us how many
    times we correctly predicted `True` and correctly predicted `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – F1 score](img/B19071_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – F1 score
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s check actual versus prediction counts. Run the following query in
    Query Editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output in the following screenshot shows, for a given value, what our prediction
    was compared to the actual value and the count of those records. Our model incorrectly
    predicted a fraudulent transaction 178 times and incorrectly predicted a non-fraudulent
    transaction 1,081 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Confusion matrix](img/B19071_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: This demonstrates how Redshift ML can help you confidently predict whether a
    transaction is fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned what XGBoost is and how to apply it to a business
    problem. You learned how to specify your own hyperparameters when using the **Auto
    Off** option and how to specify the objective for a binary classification problem.
    Additionally, you learned how to do your own data preprocessing and calculate
    the F1 score to validate the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to bring your own models from Amazon
    SageMaker for in-database or remote inference.
  prefs: []
  type: TYPE_NORMAL
