<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer031">
<h1 id="_idParaDest-35"><em class="italic"><a id="_idTextAnchor034"/>Chapter 3</em>: Identifying and Fixing Missing Values</h1>
<p>I think I speak for many data scientists when I say that rarely is there something so seemingly small and trivial that is as of much consequence as the missing value. We spend a good deal of our time worrying about missing values because they can have a dramatic, and surprising, effect on our analysis. This is most likely to happen when missing values are not random – that is, when they are correlated with a feature or target. For example, let's say we are doing a longitudinal study of earnings, but individuals with lower education are more likely to skip the earnings question each year. There is a decent chance that this will bias our parameter estimate for education.</p>
<p>Of course, identifying missing values is not even half of the battle. We then need to decide how to handle them. Do we remove any observation with a missing value for one or more features? Do we impute a value based on a sample-wide statistic such as the mean? Or do we assign a value based on a more targeted statistic, such as the mean for those in a certain class? Do we think of this differently for time series or longitudinal data where the nearest temporal value may make the most sense? Or should we use a more complex multivariate technique for imputing values, perhaps based on linear regression or <strong class="bold">k-nearest neighbors</strong> (<strong class="bold">KNN</strong>)?</p>
<p>The answer to all of these questions is <em class="italic">yes</em>. At some point, we will want to use each of these techniques. We will want to be able to answer why or why not to all of these possibilities when making a final choice about missing value imputation. Each will make sense, depending on the situation.</p>
<p>In this chapter, we'll look at techniques for identifying missing values for each feature or target, and for observations where values for a large number of the features are absent. Then, we will explore strategies for imputing values, such as setting values to the overall mean, to the mean for a given category, or forward filling. We will also examine multivariate techniques for imputing values for missing values and discuss when they are appropriate.</p>
<p>Specifically, in this chapter, we will cover the following topics:</p>
<ul>
<li>Identifying missing values</li>
<li>Cleaning missing values</li>
<li>Imputing values with regression</li>
<li>Using KNN imputation</li>
<li>Using random forest for imputation</li>
</ul>
<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Technical requirements</h1>
<p>This chapter will rely heavily on the pandas and NumPy libraries, but you don't require any prior knowledge of these. If you have installed Python from a scientific distribution, such as Anaconda or WinPython, these libraries are probably already installed. We will also be using the <strong class="source-inline">statsmodels</strong> library for linear regression, and machine learning algorithms from <strong class="source-inline">sklearn</strong> and <strong class="source-inline">missingpy</strong>. If you need to install any of these packages, you can do so by running <strong class="source-inline">pip install [package name]</strong> from a terminal window or Windows PowerShell.</p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Identifying missing values</h1>
<p>Since identifying missing values is such an important part of an analyst's workflow, any tool we use needs to <a id="_idIndexMarker260"/>make it easy to regularly check for such values. Fortunately, pandas makes it quite simple to identify missing values.</p>
<p>We will be working with the <strong class="bold">National Longitudinal Survey</strong> (<strong class="bold">NLS</strong>) in this chapter. The NLS has one <a id="_idIndexMarker261"/>observation per survey respondent. Data for employment, earnings, and college enrollment for each year are stored in columns with suffixes representing the year, such as <strong class="source-inline">weeksworked16</strong> and <strong class="source-inline">weeksworked17</strong> for weeks worked in 2016 and 2017, respectively.</p>
<p class="callout-heading">Note</p>
<p class="callout">We will also work with the COVID-19 data again. This dataset has one observation for each country that specifies the total COVID-19 cases and deaths, as well as some demographic data for each country.</p>
<p>Follow these steps to identify our missing values:</p>
<ol>
<li>Let's start by loading the NLS and COVID-19 data:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">nls97 = pd.read_csv("data/nls97b.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">covidtotals.set_index("iso_code", inplace=True)</p></li>
<li>Next, we <a id="_idIndexMarker262"/>count the number of missing values for columns that we may use as features. We can use the <strong class="source-inline">isnull</strong> method to test whether each feature value is missing. It will return <strong class="source-inline">True</strong> if the value is missing and <strong class="source-inline">False</strong> if not. Then, we can use <strong class="source-inline">sum</strong> to count the number of <strong class="source-inline">True</strong> values since <strong class="source-inline">sum</strong> will treat each <strong class="source-inline">True</strong> value as 1 and each <strong class="source-inline">False</strong> value as 0. We use <strong class="source-inline">axis=0</strong> to sum over the rows for each column:<p class="source-code">covidtotals.shape</p><p class="source-code"><strong class="bold">(221, 16)</strong></p><p class="source-code">demovars = ['population_density','aged_65_older',</p><p class="source-code">  'gdp_per_capita', 'life_expectancy', </p><p class="source-code">  'diabetes_prevalence']</p><p class="source-code">covidtotals[demovars].isnull().sum(axis=0)</p><p class="source-code"><strong class="bold">population_density        15</strong></p><p class="source-code"><strong class="bold">aged_65_older             33</strong></p><p class="source-code"><strong class="bold">gdp_per_capita            28</strong></p><p class="source-code"><strong class="bold">life_expectancy            4</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence       21</strong></p></li>
</ol>
<p>As we can see, 33 of the 221 countries have null values for <strong class="source-inline">aged_65_older</strong>. We have <strong class="source-inline">life_expectancy</strong> for almost all countries.</p>
<ol>
<li value="3">If we want the number of missing values for each row, we can specify <strong class="source-inline">axis=1</strong> when summing. The following code creates a Series, <strong class="source-inline">demovarsmisscnt</strong>, with the number of missing values for the demographic features for each country. 181 countries have values for all of the features, 11 are missing values for four of the five features, and three are missing values for all of the features:<p class="source-code">demovarsmisscnt = covidtotals[demovars].isnull().sum(axis=1)</p><p class="source-code">demovarsmisscnt.value_counts().sort_index()</p><p class="source-code"><strong class="bold">0        181</strong></p><p class="source-code"><strong class="bold">1        15</strong></p><p class="source-code"><strong class="bold">2         6</strong></p><p class="source-code"><strong class="bold">3         5</strong></p><p class="source-code"><strong class="bold">4        11</strong></p><p class="source-code"><strong class="bold">5         3</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
<li>Let's take <a id="_idIndexMarker263"/>a look at a few of the countries with four or more missing values. There is very little demographic data available for these countries:<p class="source-code">covidtotals.loc[demovarsmisscnt &gt; = 4, ['location'] +</p><p class="source-code">  demovars].sample(6, random_state=1).T</p><p class="source-code"><strong class="bold">iso_code                         FLK   NIU        MSR\</strong></p><p class="source-code"><strong class="bold">location            Falkland Islands  Niue  Montserrat</strong></p><p class="source-code"><strong class="bold">population_density               NaN   NaN         NaN</strong></p><p class="source-code"><strong class="bold">aged_65_older                    NaN   NaN         NaN</strong></p><p class="source-code"><strong class="bold">gdp_per_capita                   NaN   NaN         NaN</strong></p><p class="source-code"><strong class="bold">life_expectancy                   81    74          74</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence              NaN   NaN         NaN</strong></p><p class="source-code"><strong class="bold">iso_code                         COK    SYR        GGY</strong></p><p class="source-code"><strong class="bold">location                Cook Islands  Syria   Guernsey</strong></p><p class="source-code"><strong class="bold">population_density               NaN    NaN        NaN</strong></p><p class="source-code"><strong class="bold">aged_65_older                    NaN    NaN        NaN</strong></p><p class="source-code"><strong class="bold">gdp_per_capita                   NaN    NaN        NaN</strong></p><p class="source-code"><strong class="bold">life_expectancy                   76      7        NaN</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence              NaN    NaN        NaN</strong></p></li>
<li>Let's also check missing values for total cases and deaths. 29 countries have missing values for cases per million in population, and 36 have missing deaths per million:<p class="source-code">totvars = </p><p class="source-code">  ['location','total_cases_mill','total_deaths_mill']</p><p class="source-code">covidtotals[totvars].isnull().sum(axis=0)</p><p class="source-code"><strong class="bold">location                0</strong></p><p class="source-code"><strong class="bold">total_cases_mill       29</strong></p><p class="source-code"><strong class="bold">total_deaths_mill      36</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
<li>We should <a id="_idIndexMarker264"/>also get a sense of which countries are missing both. 29 countries are missing both cases and deaths, and we only have both for 185 countries:<p class="source-code">totvarsmisscnt = </p><p class="source-code">  covidtotals[totvars].isnull().sum(axis=1)</p><p class="source-code">totvarsmisscnt.value_counts().sort_index()</p><p class="source-code"><strong class="bold">0        185</strong></p><p class="source-code"><strong class="bold">1        7</strong></p><p class="source-code"><strong class="bold">2        29</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
</ol>
<p>Sometimes, we have logical missing values that we need to transform into actual missing values. This happens when the dataset designers use valid values as codes for missing values. These are often values such as 9, 99, or 999, based on the allowable number of digits for the variable. Or it might be a more complicated coding scheme where there are codes for different reasons for there being missings. For example, in the NLS dataset, the codes reveal why the respondent did not provide an answer for a question: <strong class="source-inline">-3</strong> is an invalid skip, <strong class="source-inline">-4</strong> is a valid skip, and <strong class="source-inline">-5</strong> is a non-interview.</p>
<ol>
<li value="7">The last four columns in the NLS DataFrame have data at the highest grade completed <a id="_idIndexMarker265"/>for the respondent's mother and father, parental income, and mother's age when the respondent was born. Let's examine logical missings for those columns, starting with the highest grade that was completed for the respondent's mother:<p class="source-code">nlsparents = nls97.iloc[:,-4:]</p><p class="source-code">nlsparents.shape</p><p class="source-code"><strong class="bold">(8984, 4)</strong></p><p class="source-code">nlsparents.loc[nlsparents.motherhighgrade.between(-5, </p><p class="source-code">  -1), 'motherhighgrade'].value_counts()</p><p class="source-code"><strong class="bold">-3        523</strong></p><p class="source-code"><strong class="bold">-4        165</strong></p><p class="source-code"><strong class="bold">Name: motherhighgrade, dtype: int64</strong></p></li>
<li>There are 523 invalid skips and 165 valid skips. Let's look at a few individuals that have at least one of these non-response values for these four features:<p class="source-code">nlsparents.loc[nlsparents.apply(lambda x: x.between(</p><p class="source-code">  -5,-1)).any(axis=1)]</p><p class="source-code"><strong class="bold">        motherage  parentincome  fatherhighgrade  motherhighgrade</strong></p><p class="source-code"><strong class="bold">personid  </strong></p><p class="source-code"><strong class="bold">100284    22            50000            12         -3</strong></p><p class="source-code"><strong class="bold">100931    23            60200            -3         13</strong></p><p class="source-code"><strong class="bold">101122    25               -4            -3         -3</strong></p><p class="source-code"><strong class="bold">101414    27            24656            10         -3</strong></p><p class="source-code"><strong class="bold">101526    -3            79500            -4         -4</strong></p><p class="source-code"><strong class="bold">         ...              ...            ...       ...</strong></p><p class="source-code"><strong class="bold">999087    -3           121000            -4         16</strong></p><p class="source-code"><strong class="bold">999103    -3            73180            12         -4</strong></p><p class="source-code"><strong class="bold">999406    19               -4            17         15</strong></p><p class="source-code"><strong class="bold">999698    -3            13000            -4         -4</strong></p><p class="source-code"><strong class="bold">999963    29               -4            12         13</strong></p><p class="source-code"><strong class="bold">[3831 rows x 4 columns]</strong></p></li>
<li>For our <a id="_idIndexMarker266"/>analysis, the reason why there is a non-response is not important. Let's just count the number of non-responses for each of the features, regardless of the reason for the non-response:<p class="source-code">nlsparents.apply(lambda x: x.between(-5,-1).sum())</p><p class="source-code"><strong class="bold">motherage              608</strong></p><p class="source-code"><strong class="bold">parentincome          2396</strong></p><p class="source-code"><strong class="bold">fatherhighgrade       1856</strong></p><p class="source-code"><strong class="bold">motherhighgrade        688</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
<li>We should set these values to <strong class="source-inline">missing</strong> before using these features in our analysis. We can use <strong class="source-inline">replace</strong> to set all the values between -5 and -1 to <strong class="source-inline">missing</strong>. When we check for actual missings, we get the expected counts:<p class="source-code">nlsparents.replace(list(range(-5,0)), </p><p class="source-code">  np.nan, inplace=True)</p><p class="source-code">nlsparents.isnull().sum()</p><p class="source-code"><strong class="bold">motherage            608</strong></p><p class="source-code"><strong class="bold">parentincome        2396</strong></p><p class="source-code"><strong class="bold">fatherhighgrade     1856</strong></p><p class="source-code"><strong class="bold">motherhighgrade      688</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
</ol>
<p>This section <a id="_idIndexMarker267"/>demonstrated some very handy pandas techniques for identifying the number of missing values for each feature, as well as observations with a large number of missing values. We also learned how to find logical missing values and convert them into actual missings. Next, we'll take our first look at cleaning missing values.</p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Cleaning missing values</h1>
<p>In this section, we'll go over some of the most straightforward approaches for handling missing values. This includes <a id="_idIndexMarker268"/>dropping observations where there are missing values; assigning a sample-wide summary statistic, such as the mean, to the missing values; and assigning values based on the mean value for an appropriate subset of the data:</p>
<ol>
<li value="1">Let's load the NLS data and select some of the educational data:<p class="source-code">import pandas as pd</p><p class="source-code">nls97 = pd.read_csv("data/nls97b.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p><p class="source-code">schoolrecordlist = </p><p class="source-code">  ['satverbal','satmath','gpaoverall','gpaenglish',</p><p class="source-code">  'gpamath','gpascience','highestdegree',</p><p class="source-code">  'highestgradecompleted']</p><p class="source-code">schoolrecord = nls97[schoolrecordlist]</p><p class="source-code">schoolrecord.shape</p><p class="source-code"><strong class="bold">(8984, 8)</strong></p></li>
<li>We can use the techniques we explored in the previous section to identify missing values. <strong class="source-inline">schoolrecord.isnull().sum(axis=0)</strong> gives us the number <a id="_idIndexMarker269"/>of missing values for each feature. The overwhelming majority of observations have missing values for <strong class="source-inline">satverbal</strong>, with 7,578 out of 8,984. Only 31 observations have missing values for <strong class="source-inline">highestdegree</strong>:<p class="source-code">schoolrecord.isnull().sum(axis=0)</p><p class="source-code"><strong class="bold">satverbal                        7578</strong></p><p class="source-code"><strong class="bold">satmath                          7577</strong></p><p class="source-code"><strong class="bold">gpaoverall                       2980</strong></p><p class="source-code"><strong class="bold">gpaenglish                       3186</strong></p><p class="source-code"><strong class="bold">gpamath                          3218</strong></p><p class="source-code"><strong class="bold">gpascience                       3300</strong></p><p class="source-code"><strong class="bold">highestdegree                      31</strong></p><p class="source-code"><strong class="bold">highestgradecompleted            2321</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
<li>We can create a Series, <strong class="source-inline">misscnt</strong>, that specifies the number of missing features for each observation with <strong class="source-inline">misscnt = schoolrecord.isnull().sum(axis=1)</strong>. 946 observations have seven missing values for the educational data, while 11 are missing values for all eight features:<p class="source-code">misscnt = schoolrecord.isnull().sum(axis=1)</p><p class="source-code">misscnt.value_counts().sort_index()</p><p class="source-code"><strong class="bold">0         1087</strong></p><p class="source-code"><strong class="bold">1          312</strong></p><p class="source-code"><strong class="bold">2         3210</strong></p><p class="source-code"><strong class="bold">3         1102</strong></p><p class="source-code"><strong class="bold">4          176</strong></p><p class="source-code"><strong class="bold">5          101</strong></p><p class="source-code"><strong class="bold">6         2039</strong></p><p class="source-code"><strong class="bold">7          946</strong></p><p class="source-code"><strong class="bold">8           11</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
<li>Let's also take <a id="_idIndexMarker270"/>a look at a few observations with seven or more missing values. It looks like <strong class="source-inline">highestdegree</strong> is often the one feature that is present, which is not surprising, given that we have already discovered that <strong class="source-inline">highestdegree</strong> is rarely missing:<p class="source-code">schoolrecord.loc[misscnt&gt;=7].head(4).T</p><p class="source-code"><strong class="bold">personid              101705  102061  102648  104627</strong></p><p class="source-code"><strong class="bold">satverbal                NaN     NaN     NaN     NaN</strong></p><p class="source-code"><strong class="bold">satmath                  NaN     NaN     NaN     NaN</strong></p><p class="source-code"><strong class="bold">gpaoverall               NaN     NaN     NaN     NaN</strong></p><p class="source-code"><strong class="bold">gpaenglish               NaN     </strong><strong class="bold">NaN     NaN     NaN</strong></p><p class="source-code"><strong class="bold">gpamath                  NaN     NaN     NaN     NaN</strong></p><p class="source-code"><strong class="bold">gpascience               NaN     NaN     NaN     NaN</strong></p><p class="source-code"><strong class="bold">highestdegree          1.GED  0.None   </strong><strong class="bold">1.GED  0.None</strong></p><p class="source-code"><strong class="bold">highestgradecompleted    NaN     NaN     NaN     NaN</strong></p></li>
<li>Let's drop observations that have missing values for seven or more features out of eight. We can accomplish this by setting the <strong class="source-inline">thresh</strong> parameter of <strong class="source-inline">dropna</strong> to <strong class="source-inline">2</strong>. This <a id="_idIndexMarker271"/>will drop observations that have fewer than two non-missing values; that is, 0 or 1 non-missing values. We get the expected number of observations after using <strong class="source-inline">dropna</strong>; that is, 8,984 - 946 - 11 = 8,027:<p class="source-code">schoolrecord = schoolrecord.dropna(thresh=2)</p><p class="source-code">schoolrecord.shape</p><p class="source-code"><strong class="bold">(8027, 8)</strong></p><p class="source-code">schoolrecord.isnull().sum(axis=1).value_counts().sort_index()</p><p class="source-code"><strong class="bold">0      1087</strong></p><p class="source-code"><strong class="bold">1       312</strong></p><p class="source-code"><strong class="bold">2      3210</strong></p><p class="source-code"><strong class="bold">3      1102</strong></p><p class="source-code"><strong class="bold">4       176</strong></p><p class="source-code"><strong class="bold">5       101</strong></p><p class="source-code"><strong class="bold">6      2039</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
</ol>
<p>There are a fair number of missing values for <strong class="source-inline">gpaoverall</strong> – that is, 2,980 – though we have valid values for two-thirds of observations ((8,984 – 2,980)/8,984). We might be able to salvage this as a feature if we do a good job of imputing the missing values. This is likely more desirable than just removing these observations. We do not want to lose that data if we can avoid it, particularly if individuals with missing <strong class="source-inline">gpaoverall</strong> are different from others in ways that will matter for our predictions.</p>
<ol>
<li value="6">The most straightforward approach is to assign the overall mean for <strong class="source-inline">gpaoverall</strong> to the missing values. The following code uses the pandas Series <strong class="source-inline">fillna</strong> method to assign all missing values of <strong class="source-inline">gpaoverall</strong> to the mean value of the <a id="_idIndexMarker272"/>Series. The first argument to <strong class="source-inline">fillna</strong> is the value you want for all missing values – in this case, <strong class="source-inline">schoolrecord.gpaoverall.mean()</strong>. Note that we need to remember to set the <strong class="source-inline">inplace</strong> parameter to <strong class="source-inline">True</strong> to overwrite the existing values:<p class="source-code">schoolrecord.gpaoverall.agg(['mean','std','count'])</p><p class="source-code"><strong class="bold">mean         281.84</strong></p><p class="source-code"><strong class="bold">std           61.64</strong></p><p class="source-code"><strong class="bold">count</strong><strong class="bold">      6,004.00</strong></p><p class="source-code"><strong class="bold">Name: gpaoverall, dtype: float64</strong></p><p class="source-code">schoolrecord.gpaoverall.fillna(</p><p class="source-code">  schoolrecord.gpaoverall.mean(), inplace=True)</p><p class="source-code">schoolrecord.gpaoverall.isnull().sum()</p><p class="source-code"><strong class="bold">0</strong></p><p class="source-code">schoolrecord.gpaoverall.agg(['mean','std','count'])</p><p class="source-code"><strong class="bold">mean      281.84</strong></p><p class="source-code"><strong class="bold">std        53.30</strong></p><p class="source-code"><strong class="bold">count   8,027.00</strong></p><p class="source-code"><strong class="bold">Name: gpaoverall, dtype: float64</strong></p></li>
</ol>
<p>The mean has not changed. However, there is a substantial reduction in the standard deviation, from 61.6 to 53.3. This is one of the disadvantages of using the dataset's mean for all missing values.</p>
<ol>
<li value="7">The NLS <a id="_idIndexMarker273"/>data also has a fair number of missing values for <strong class="source-inline">wageincome</strong>. The following code shows that 3,893 observations have missing values:<p class="source-code">wageincome = nls97.wageincome.copy(deep=True)</p><p class="source-code">wageincome.isnull().sum()</p><p class="source-code"><strong class="bold">3893</strong></p><p class="source-code">wageincome.agg(['mean','std','count'])</p><p class="source-code"><strong class="bold">mean      49,477.02</strong></p><p class="source-code"><strong class="bold">std       40,677.70</strong></p><p class="source-code"><strong class="bold">count      5,091.00</strong></p><p class="source-code"><strong class="bold">Name: wageincome, dtype: float64</strong></p><p class="source-code">wageincome.head().T</p><p class="source-code"><strong class="bold">personid</strong></p><p class="source-code"><strong class="bold">100061      12,500</strong></p><p class="source-code"><strong class="bold">100139     120,000</strong></p><p class="source-code"><strong class="bold">100284      58,000</strong></p><p class="source-code"><strong class="bold">100292</strong><strong class="bold">         NaN</strong></p><p class="source-code"><strong class="bold">100583      30,000</strong></p><p class="source-code"><strong class="bold">Name: wageincome, dtype: float64</strong></p><p class="callout-heading">Note</p><p class="callout">Here, we made a deep copy with the <strong class="source-inline">copy</strong> method, setting <strong class="source-inline">deep</strong> to <strong class="source-inline">True</strong>. We wouldn't normally do this but, in this case, we don't want to change the values of <strong class="source-inline">wageincome</strong> in the underlying DataFrame. We have avoided this here because we will demonstrate a different method of imputing values in the next couple of code blocks.</p></li>
<li>Rather than assigning the mean value of <strong class="source-inline">wageincome</strong> to the missings, we could <a id="_idIndexMarker274"/>use another common technique for imputing values: we could assign the nearest non-missing value from a preceding observation. The <strong class="source-inline">ffill</strong> option of <strong class="source-inline">fillna</strong> will do this for us:<p class="source-code">wageincome.fillna(method='ffill', inplace=True)</p><p class="source-code">wageincome.head().T</p><p class="source-code">personid</p><p class="source-code"><strong class="bold">100061       12,500</strong></p><p class="source-code"><strong class="bold">100139      120,000</strong></p><p class="source-code"><strong class="bold">100284       58,000</strong></p><p class="source-code"><strong class="bold">100292       58,000</strong></p><p class="source-code"><strong class="bold">100583       30,000</strong></p><p class="source-code"><strong class="bold">Name: wageincome, dtype: float64</strong></p><p class="source-code">wageincome.isnull().sum()</p><p class="source-code"><strong class="bold">0</strong></p><p class="source-code">wageincome.agg(['mean','std','count'])</p><p class="source-code"><strong class="bold">mean      49,549.33</strong></p><p class="source-code"><strong class="bold">std       40,014.34</strong></p><p class="source-code"><strong class="bold">count      8,984.00</strong></p><p class="source-code"><strong class="bold">Name: wageincome, dtype: float64</strong></p></li>
<li>We could have done a backward fill instead by setting the <strong class="source-inline">method</strong> parameter of <strong class="source-inline">fillna</strong> to <strong class="source-inline">bfill</strong>. This sets missing values to the nearest following value. This produces the following output:<p class="source-code">wageincome = nls97.wageincome.copy(deep=True)</p><p class="source-code">wageincome.std()</p><p class="source-code"><strong class="bold">40677.69679818673</strong></p><p class="source-code">wageincome.fillna(method='bfill', inplace=True)</p><p class="source-code">wageincome.head().T</p><p class="source-code"><strong class="bold">personid</strong></p><p class="source-code"><strong class="bold">100061       12,500</strong></p><p class="source-code"><strong class="bold">100139      120,000</strong></p><p class="source-code"><strong class="bold">100284       58,000</strong></p><p class="source-code"><strong class="bold">100292       30,000</strong></p><p class="source-code"><strong class="bold">100583       30,000</strong></p><p class="source-code"><strong class="bold">Name: wageincome, dtype: float64</strong></p><p class="source-code">wageincome.agg(['mean','std','count'])</p><p class="source-code"><strong class="bold">mean    49,419.05</strong></p><p class="source-code"><strong class="bold">std     41,111.54</strong></p><p class="source-code"><strong class="bold">count    8,984.00</strong></p><p class="source-code"><strong class="bold">Name: wageincome, dtype: float64</strong></p></li>
</ol>
<p>If missing values are randomly distributed, then forward or backward filling has one advantage over using the mean: it is more likely to approximate the distribution <a id="_idIndexMarker275"/>of the non-missing values for the feature. Notice that the standard deviation did not drop substantially.</p>
<p>There are times when it makes sense to base our imputation of values on the mean or median value for similar observations; say, those that have the same value for a related feature. If we are imputing values for feature X1, and X1 is correlated with X2, we can use the relationship between X1 and X2 to impute a value for X1 that may make more sense than the dataset's mean. This is pretty straightforward when X2 is categorical. In this case, we can impute the mean value of X1 for the associated value of X2. </p>
<ol>
<li value="10">In the NLS DataFrame, weeks worked in 2017 correlates with the highest degree earned. The following code shows how the mean value of weeks worked changes with degree attainment. The mean for weeks worked is 39, but it is much lower for those without a degree (28.72) and much higher for those with a professional <a id="_idIndexMarker276"/>degree (47.20). In this case, it may be a better choice to assign 28.72 to the missing values for weeks worked for individuals who have not attained a degree, rather than 39:<p class="source-code">nls97.weeksworked17.mean()</p><p class="source-code"><strong class="bold">39.01664167916042</strong></p><p class="source-code">nls97.groupby(['highestdegree'])['weeksworked17'</p><p class="source-code">  ].mean()</p><p class="source-code"><strong class="bold">highestdegree</strong></p><p class="source-code"><strong class="bold">0. None                  28.72</strong></p><p class="source-code"><strong class="bold">1. GED                   34.59</strong></p><p class="source-code"><strong class="bold">2. High School</strong><strong class="bold">           38.15</strong></p><p class="source-code"><strong class="bold">3. Associates            40.44</strong></p><p class="source-code"><strong class="bold">4. Bachelors             43.57</strong></p><p class="source-code"><strong class="bold">5. Masters               45.14</strong></p><p class="source-code"><strong class="bold">6. PhD                   44.31</strong></p><p class="source-code"><strong class="bold">7. Professional          47.20</strong></p><p class="source-code"><strong class="bold">Name: weeksworked17, dtype: float64</strong></p></li>
<li>The following code assigns the mean value of weeks worked across observations with the same degree attainment level, for those observations missing weeks worked. We do this by using <strong class="source-inline">groupby</strong> to create a groupby DataFrame, <strong class="source-inline">groupby(['highestdegree'])['weeksworked17']</strong>. Then, we use <strong class="source-inline">fillna</strong> within <strong class="source-inline">apply</strong> to fill those missing values with the mean for the highest <a id="_idIndexMarker277"/>degree group. Notice that we make sure to only do this imputation for observations where the highest degree is not missing, <strong class="source-inline">~nls97.highestdegree.isnull()</strong>. We will still have missing values for observations that are missing both the highest degree and weeks worked:<p class="source-code">nls97.loc[~nls97.highestdegree.isnull(),</p><p class="source-code">  'weeksworked17imp'] = </p><p class="source-code">  nls97.loc[ ~nls97.highestdegree.isnull() ].</p><p class="source-code">  groupby(['highestdegree'])['weeksworked17'].</p><p class="source-code">  apply(lambda group: group.fillna(np.mean(group)))</p><p class="source-code">nls97[['weeksworked17imp','weeksworked17',</p><p class="source-code">  'highestdegree']].head(10)</p><p class="source-code"><strong class="bold">       weeksworked17imp  weeksworked17   highestdegree</strong></p><p class="source-code"><strong class="bold">personid                                              </strong></p><p class="source-code"><strong class="bold">100061            48.00         48.00   2. High School</strong></p><p class="source-code"><strong class="bold">100139            52.00         52.00   2. High School</strong></p><p class="source-code"><strong class="bold">100284       </strong><strong class="bold">      0.00          0.00          0. None</strong></p><p class="source-code"><strong class="bold">100292            43.57           NaN     4. Bachelors</strong></p><p class="source-code"><strong class="bold">100583            52.00         52.00   2. High School</strong></p><p class="source-code"><strong class="bold">100833            47.00</strong><strong class="bold">         47.00   2. High School</strong></p><p class="source-code"><strong class="bold">100931            52.00         52.00    3. Associates</strong></p><p class="source-code"><strong class="bold">101089            52.00         52.00   2. High School</strong></p><p class="source-code"><strong class="bold">101122            38.15           NaN</strong><strong class="bold">   2. High School</strong></p><p class="source-code"><strong class="bold">101132            44.00         44.00          0. None</strong></p><p class="source-code">nls97[['weeksworked17imp','weeksworked17']].\</p><p class="source-code">  agg(['mean','count'])</p><p class="source-code"><strong class="bold">       weeksworked17imp  weeksworked17</strong></p><p class="source-code"><strong class="bold">mean          38.52         39.02</strong></p><p class="source-code"><strong class="bold">count</strong><strong class="bold">      8,953.00      6,670.00</strong></p></li>
</ol>
<p>These imputation strategies – removing observations with missing values, assigning a dataset's mean or median, using forward or backward filling, or using a group mean for a correlated feature – are fine for many predictive analytics projects. They work best when the missing values are not correlated with the target. When that is true, imputing values allows <a id="_idIndexMarker278"/>us to retain the other information from those observations without biasing our estimates.</p>
<p>Sometimes, however, that is not the case and more complicated imputation strategies are required. The next few sections will explore multivariate techniques for cleaning missing data.</p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Imputing values with regression</h1>
<p>We ended the previous section by assigning a group mean to the missing values rather than the overall <a id="_idIndexMarker279"/>sample mean. As we discussed, this is useful when <a id="_idIndexMarker280"/>the feature that determines the groups is correlated with the feature that has the missing values. Using regression to impute values is conceptually similar to this, but we typically use it when the imputation will be based on two or more features.</p>
<p>Regression imputation replaces a feature's missing values with values predicted by a regression model of correlated features. This particular kind of imputation is known as deterministic regression imputation since the imputed values all lie on the regression line, and no error or randomness is introduced.  </p>
<p>One potential drawback of this approach is that it can substantially reduce the variance of the feature with missing values. We can use stochastic regression imputation to address this drawback. We will explore both approaches in this section.</p>
<p>The <strong class="source-inline">wageincome</strong> feature <a id="_idIndexMarker281"/>in the NLS dataset has several <a id="_idIndexMarker282"/>missing values. We can use linear regression to impute values. The wage income value is the reported earnings for 2016:</p>
<ol>
<li value="1">Let's start by loading the NLS data again and checking for missing values for <strong class="source-inline">wageincome</strong> and features that might be correlated with <strong class="source-inline">wageincome</strong>. We also load the <strong class="source-inline">statsmodels</strong> library.</li>
</ol>
<p>The <strong class="source-inline">info</strong> method tells us that we are missing values for <strong class="source-inline">wageincome</strong> for nearly 3,000 observations. There are fewer missing values for the other features:</p>
<p class="source-code">import pandas as pd</p>
<p class="source-code">import numpy as np</p>
<p class="source-code">import statsmodels.api as sm</p>
<p class="source-code">nls97 = pd.read_csv("data/nls97b.csv")</p>
<p class="source-code">nls97.set_index("personid", inplace=True)</p>
<p class="source-code">nls97[['wageincome','highestdegree','weeksworked16',</p>
<p class="source-code">  'parentincome']].info()</p>
<p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p>
<p class="source-code"><strong class="bold">Int64Index: 8984 entries, 100061 to 999963</strong></p>
<p class="source-code"><strong class="bold">Data columns (total 4 columns):</strong></p>
<p class="source-code"><strong class="bold"> #  Column               Non-Null Count       Dtype</strong></p>
<p class="source-code"><strong class="bold">--  -------               --------------      -----  </strong></p>
<p class="source-code"><strong class="bold"> 0  </strong><strong class="bold">wageincome            5091 non-null       float64</strong></p>
<p class="source-code"><strong class="bold"> 1  highestdegree         8953 non-null       object </strong></p>
<p class="source-code"><strong class="bold"> 2  weeksworked16         7068 non-null       float64</strong></p>
<p class="source-code"><strong class="bold"> 3  parentincome          </strong><strong class="bold">8984 non-null       int64</strong></p>
<p class="source-code"><strong class="bold">dtypes: float64(2), int64(1), object(1)</strong></p>
<p class="source-code"><strong class="bold">memory usage: 350.9+ KB</strong></p>
<ol>
<li value="2">Let's convert the <strong class="source-inline">highestdegree</strong> feature into a numeric value. This will make the analysis we'll be doing in the rest of this section easier:<p class="source-code">nls97['hdegnum'] =</p><p class="source-code">  nls97.highestdegree.str[0:1].astype('float')</p><p class="source-code">nls97.groupby(['highestdegree','hdegnum']).size() </p><p class="source-code"><strong class="bold">highestdegree    hdegnum</strong></p><p class="source-code"><strong class="bold">0. None                0            953</strong></p><p class="source-code"><strong class="bold">1. GED                 1           1146</strong></p><p class="source-code"><strong class="bold">2. High School         2           3667</strong></p><p class="source-code"><strong class="bold">3. Associates</strong><strong class="bold">          3            737</strong></p><p class="source-code"><strong class="bold">4. Bachelors           4           1673</strong></p><p class="source-code"><strong class="bold">5. Masters             5            603</strong></p><p class="source-code"><strong class="bold">6. PhD                 6             54</strong></p><p class="source-code"><strong class="bold">7. Professional        7            120</strong></p></li>
<li>As we've already <a id="_idIndexMarker283"/>discovered, we need to replace logical <a id="_idIndexMarker284"/>missing values for <strong class="source-inline">parentincome</strong> with actual missings. After that, we can run some correlations. Each of the features has some correlation with <strong class="source-inline">wageincome</strong>, particularly <strong class="source-inline">hdegnum</strong>:<p class="source-code">nls97.parentincome.replace(list(range(-5,0)), np.nan,</p><p class="source-code">  inplace=True)</p><p class="source-code">nls97[['wageincome','hdegnum','weeksworked16', </p><p class="source-code">  'parentincome']].corr()</p><p class="source-code"><strong class="bold">            wageincome  hdegnum  weeksworked16  parentincome</strong></p><p class="source-code"><strong class="bold">wageincome     1.00      0.40         0.18        0.27</strong></p><p class="source-code"><strong class="bold">hdegnum        0.40</strong><strong class="bold">      1.00         0.24        0.33</strong></p><p class="source-code"><strong class="bold">weeksworked16  0.18      0.24         1.00        0.10</strong></p><p class="source-code"><strong class="bold">parentincome   0.27      0.33         0.10        1.00</strong></p></li>
<li>We should check whether observations with missing values for wage income are different <a id="_idIndexMarker285"/>in some important way from those with <a id="_idIndexMarker286"/>non-missing values. The following code shows that these observations have significantly lower degree attainment levels, parental income, and weeks worked. This is a clear case where assigning the overall mean would not be the best choice:<p class="source-code">nls97['missingwageincome'] =</p><p class="source-code">  np.where(nls97.wageincome.isnull(),1,0)</p><p class="source-code">nls97.groupby(['missingwageincome'])[['hdegnum', </p><p class="source-code">  'parentincome', 'weeksworked16']].agg(['mean', </p><p class="source-code">  'count'])</p><p class="source-code"><strong class="bold">                 hdegnum    parentincome weeksworked16</strong></p><p class="source-code"><strong class="bold">                 mean count mean   count  mean   count</strong></p><p class="source-code"><strong class="bold">missingwageincome                                    </strong></p><p class="source-code"><strong class="bold">0</strong><strong class="bold">                2.76 5072  48,409.13 3803 48.21  5052</strong></p><p class="source-code"><strong class="bold">1                1.95 3881  43,565.87 2785</strong><strong class="bold"> 16.36  2016</strong></p></li>
<li>Let's try regression imputation instead. Let's start by cleaning up the data a little bit more. We can replace the missing <strong class="source-inline">weeksworked16</strong> and <strong class="source-inline">parentincome</strong> values with their means. We should also collapse <strong class="source-inline">hdegnum</strong> into those attaining less than a college degree, those with a college degree, and those with a post-graduate degree. We can set those up as dummy variables, with 0 or 1 values when they're <strong class="source-inline">False</strong> or <strong class="source-inline">True</strong>, respectively. This is a tried and true method for treating categorical data in regression analysis as it allows us to estimate different <em class="italic">y</em> intercepts based on group membership:<p class="source-code">nls97.weeksworked16.fillna(nls97.weeksworked16.mean(),</p><p class="source-code">  inplace=True)</p><p class="source-code">nls97.parentincome.fillna(nls97.parentincome.mean(),</p><p class="source-code">  inplace=True)</p><p class="source-code">nls97['degltcol'] = np.where(nls97.hdegnum&lt;=2,1,0)</p><p class="source-code">nls97['degcol'] = np.where(nls97.hdegnum.between(3,4),</p><p class="source-code">  1,0)</p><p class="source-code">nls97['degadv'] = np.where(nls97.hdegnum&gt;4,1,0)</p><p class="callout-heading">Note</p><p class="callout">scikit-learn has preprocessing features that can help us with tasks like these. We will cover some of them in the next chapter.</p></li>
<li>Next, we define a function, <strong class="source-inline">getlm</strong>, to run a linear model using the <strong class="source-inline">statsmodels</strong> module. This function has parameters for the name of the target or <a id="_idIndexMarker287"/>dependent variable, <strong class="source-inline">ycolname</strong>, and <a id="_idIndexMarker288"/>the names of the features or independent variables, <strong class="source-inline">xcolnames</strong>. Much of the work is done by the <strong class="source-inline">fit</strong> method of <strong class="source-inline">statsmodels</strong>; that is, <strong class="source-inline">OLS(y, X).fit()</strong>:<p class="source-code">def getlm(df, ycolname, xcolnames):</p><p class="source-code">  df = df[[ycolname] + xcolnames].dropna()</p><p class="source-code">  y = df[ycolname]</p><p class="source-code">  X = df[xcolnames]</p><p class="source-code">  X = sm.add_constant(X)</p><p class="source-code">  lm = sm.OLS(y, X).fit()</p><p class="source-code">  coefficients = pd.DataFrame(zip(['constant'] +</p><p class="source-code">    xcolnames,lm.params, lm.pvalues), columns = [</p><p class="source-code">    'features' , 'params','pvalues'])</p><p class="source-code">  return coefficients, lm</p></li>
<li>Now, we can use the <strong class="source-inline">getlm</strong> function to get the parameter estimates and the model summary. All of the coefficients are positive and significant at the 95% level since <a id="_idIndexMarker289"/>they have <strong class="source-inline">pvalues</strong> less than 0.05. As <a id="_idIndexMarker290"/>expected, wage income increases with the number of weeks worked and with parental income. Having a college degree gives a nearly $16K boost to earnings, compared with not having a college degree. A post-graduate degree bumps up the earnings prediction even more – almost $37K more than for those with less than a college degree:<p class="callout-heading">Note</p><p class="callout">The coefficients of <strong class="source-inline">degcol</strong> and <strong class="source-inline">degadv</strong> are interpreted as relative to those without a college degree since that is the omitted dummy variable.</p><p class="source-code">xvars = ['weeksworked16', 'parentincome', 'degcol', </p><p class="source-code">  'degadv']</p><p class="source-code">coefficients, lm = getlm(nls97, 'wageincome', xvars)</p><p class="source-code">coefficients</p><p class="source-code"><strong class="bold">     features           params           pvalues</strong></p><p class="source-code"><strong class="bold">0    constant           7,389.37         0.00</strong></p><p class="source-code"><strong class="bold">1    weeksworked16      494.07           0.00</strong></p><p class="source-code"><strong class="bold">2    parentincome       0.18             0.00</strong></p><p class="source-code"><strong class="bold">3    degcol             15,770.07        0.00</strong></p><p class="source-code"><strong class="bold">4    degadv             36,737.84        0.00</strong></p></li>
<li>We can use this model to impute values for wage income where they are missing. We need to add a constant for the predictions since our model included a constant. We can convert the predictions into a DataFrame and then join it with the <a id="_idIndexMarker291"/>rest of the NLS data. Then, we can create <a id="_idIndexMarker292"/>a new wage income feature, <strong class="source-inline">wageincomeimp</strong>, that gets the predicted value when wage income is missing, and the original wage income value otherwise. Let's also take a look at some of the predictions to see whether they make sense:<p class="source-code">pred = lm.predict(sm.add_constant(nls97[xvars])).</p><p class="source-code">  to_frame().rename(columns= {0: 'pred'})</p><p class="source-code">nls97 = nls97.join(pred)</p><p class="source-code">nls97['wageincomeimp'] = </p><p class="source-code">  np.where(nls97.wageincome.isnull(),</p><p class="source-code">  nls97.pred, nls97.wageincome)</p><p class="source-code">pd.options.display.float_format = '{:,.0f}'.format</p><p class="source-code">nls97[['wageincomeimp','wageincome'] + xvars].head(10)</p><p class="source-code"><strong class="bold">wageincomeimp  wageincome  weeksworked16  parentincome  degcol  degadv</strong></p><p class="source-code"><strong class="bold">personid                                          </strong></p><p class="source-code"><strong class="bold">100061     12,500     12,500    48</strong><strong class="bold">     7,400    0    0</strong></p><p class="source-code"><strong class="bold">100139    120,000    120,000    53    57,000    0    0</strong></p><p class="source-code"><strong class="bold">100284     58,000</strong><strong class="bold">     58,000    47    50,000    0    0</strong></p><p class="source-code"><strong class="bold">100292     36,547        NaN     4    62,760    1</strong><strong class="bold">    0</strong></p><p class="source-code"><strong class="bold">100583     30,000     30,000    53    18,500    0    0</strong></p><p class="source-code"><strong class="bold">100833     39,000     39,000    45</strong><strong class="bold">    37,000    0    0</strong></p><p class="source-code"><strong class="bold">100931     56,000     56,000    53    60,200    1    0</strong></p><p class="source-code"><strong class="bold">101089     36,000</strong><strong class="bold">     36,000    53    32,307    0    0</strong></p><p class="source-code"><strong class="bold">101122     35,151        NaN    39    46,362    0</strong><strong class="bold">    0</strong></p><p class="source-code"><strong class="bold">101132          0          0    22     2,470    0    0</strong></p></li>
<li>We should look at some summary statistics for our prediction and compare those with the actual wage income values. The mean for the imputed wage income <a id="_idIndexMarker293"/>feature is lower than the original <a id="_idIndexMarker294"/>wage income mean. This is not surprising since, as we have seen, individuals with missing wage income have lower values for positively correlated features. What is surprising is the sharp reduction in the standard deviation. This is one of the drawbacks of deterministic regression imputation:<p class="source-code">nls97[['wageincomeimp','wageincome']].</p><p class="source-code">  agg(['count','mean','std'])</p><p class="source-code"><strong class="bold">       wageincomeimp  wageincome</strong></p><p class="source-code"><strong class="bold">count        8,984        5,091</strong></p><p class="source-code"><strong class="bold">mean        42,559       49,477</strong></p><p class="source-code"><strong class="bold">std         33,406       40,678</strong></p></li>
<li>Stochastic regression imputation adds a normally distributed error to the predictions based on the residuals from our model. We want this error to have a mean of 0 with the same standard deviation as our residuals. We can use NumPy's normal function for that with <strong class="source-inline">np.random.normal(0, lm.resid.std(), nls97.shape[0])</strong>. The <strong class="source-inline">lm.resid.std()</strong> parameter gets us the standard deviation of the residuals from our model. The final parameter value, <strong class="source-inline">nls97.shape[0]</strong>, indicates how many values to create; in this case, we want a value for every row in our data.</li>
</ol>
<p>We can join those values with our data and then add the error, <strong class="source-inline">randomadd</strong>, to our prediction:</p>
<p class="source-code">randomadd = np.random.normal(0, lm.resid.std(),</p>
<p class="source-code">  nls97.shape[0])</p>
<p class="source-code">randomadddf = pd.DataFrame(randomadd, </p>
<p class="source-code">  columns=['randomadd'], index=nls97.index)</p>
<p class="source-code">nls97 = nls97.join(randomadddf)</p>
<p class="source-code">nls97['stochasticpred'] = nls97.pred + nls97.randomadd</p>
<p class="source-code">nls97['wageincomeimpstoc'] =</p>
<p class="source-code">  np.where(nls97.wageincome.isnull(),</p>
<p class="source-code">  nls97.stochasticpred, nls97.wageincome)</p>
<ol>
<li value="11">This should <a id="_idIndexMarker295"/>increase the variance but not have <a id="_idIndexMarker296"/>much of an effect on the mean. Let's confirm this:<p class="source-code">nls97[['wageincomeimpstoc','wageincome']].agg([</p><p class="source-code">  'count','mean','std'])</p><p class="source-code"> </p><p class="source-code"><strong class="bold">       wageincomeimpstoc  wageincome</strong></p><p class="source-code"><strong class="bold">count        8,984           5,091</strong></p><p class="source-code"><strong class="bold">mean        42,517          49,477</strong></p><p class="source-code"><strong class="bold">std         41,381</strong><strong class="bold">          40,678</strong></p></li>
</ol>
<p>That seemed to have worked. Our stochastic prediction has pretty much the same standard deviation as the original wage income feature.</p>
<p>Regression imputation is a good way to take advantage of all the data we have to impute values for a feature. It is often superior to the imputation methods we examined in the previous section, particularly when missing values are not random. If we use stochastic regression imputation, we will not artificially reduce our variance.</p>
<p>Before we started using machine learning for this work, this was our go-to multivariate approach for <a id="_idIndexMarker297"/>imputation. We now have the option of using algorithms <a id="_idIndexMarker298"/>such as KNN for this task, which has advantages over regression imputation in some cases. KNN imputation, unlike regression imputation, does not assume a linear relationship between features, or that those features are normally distributed. We will explore KNN imputation in the next section. </p>
<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Using KNN imputation</h1>
<p>KNN is a popular machine learning technique because it is intuitive, easy to run, and yields good results when there are not a large number of features and observations. For the same reasons, it is often used to impute missing values. As its name suggests, KNN identifies the k observations whose features are most similar to each observation. When it's used to impute missing values, KNN uses the nearest neighbors to determine what fill values to use.</p>
<p>We can use KNN imputation to do <a id="_idIndexMarker299"/>the same imputation we did in the previous section on regression imputation:</p>
<ol>
<li value="1">Let's start by importing <strong class="source-inline">KNNImputer</strong> from scikit-learn and loading the NLS data again:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.impute import KNNImputer</p><p class="source-code">nls97 = pd.read_csv("data/nls97b.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p></li>
<li>Next, we must prepare the features. We collapse degree attainment into three categories – less than college, college, and post-college degree – with each category represented by a different dummy variable. We must also convert the logical missing values for parent income into actual missings:<p class="source-code">nls97['hdegnum'] =</p><p class="source-code">  nls97.highestdegree.str[0:1].astype('float')</p><p class="source-code">nls97['degltcol'] = np.where(nls97.hdegnum&lt;=2,1,0)</p><p class="source-code">nls97['degcol'] = </p><p class="source-code">  np.where(nls97.hdegnum.between(3,4),1,0)</p><p class="source-code">nls97['degadv'] = np.where(nls97.hdegnum&gt;4,1,0)</p><p class="source-code">nls97.parentincome.replace(list(range(-5,0)), np.nan, </p><p class="source-code">  inplace=True)</p></li>
<li>Let's create a DataFrame that contains just the wage income and a few correlated features:<p class="source-code">wagedatalist = ['wageincome','weeksworked16', </p><p class="source-code">  'parentincome','degltcol','degcol','degadv']</p><p class="source-code">wagedata = nls97[wagedatalist]</p></li>
<li>We <a id="_idIndexMarker300"/>are now ready to use the <strong class="source-inline">fit_transform</strong> method of the KNN imputer to get values for all the missing values in the passed DataFrame, <strong class="source-inline">wagedata</strong>. <strong class="source-inline">fit_transform</strong> returns a NumPy array that contains all the non-missing values from <strong class="source-inline">wagedata</strong>, plus the imputed ones. We can convert this array into a DataFrame using the same index as <strong class="source-inline">wagedata</strong>. This will make it easy to join the data in the next step.<p class="callout-heading">Note</p><p class="callout">We will use this technique throughout this book when we're working with the NumPy arrays that are returned when we use scikit-learn's <strong class="source-inline">transform</strong> and <strong class="source-inline">fit_transform</strong> methods.</p></li>
</ol>
<p>We need to specify the value to use for the number of nearest neighbors, for k. We use a general rule of thumb for determining k – the square root of the number of observations divided by 2 (<em class="italic">sqrt(N)/2</em>). That gives us 47 for k in this case:</p>
<p class="source-code">impKNN = KNNImputer(n_neighbors=47)</p>
<p class="source-code">newvalues = impKNN.fit_transform(wagedata)</p>
<p class="source-code">wagedatalistimp = ['wageincomeimp','weeksworked16imp',</p>
<p class="source-code">  'parentincomeimp','degltcol','degcol','degadv']</p>
<p class="source-code">wagedataimp = pd.DataFrame(newvalues,</p>
<p class="source-code">  columns=wagedatalistimp, index=wagedata.index)</p>
<ol>
<li value="5">Now, we must join the imputed wage income and weeks worked columns with the original NLS wage data and make a few observations. Notice that, with KNN imputation, we did not need to do any pre-imputation for missing values of correlated <a id="_idIndexMarker301"/>features (with regression imputation, we set weeks worked and parent income to their dataset means). That does mean, however, that KNN imputation will return an imputation, even when there is not a lot of information, such as with <strong class="source-inline">101122</strong> for <strong class="source-inline">personid</strong> in the following code block:<p class="source-code">wagedata = wagedata.join(wagedataimp[['wageincomeimp', </p><p class="source-code">  'weeksworked16imp']])</p><p class="source-code">wagedata[['wageincome','weeksworked16','parentincome',</p><p class="source-code">  'degcol','degadv','wageincomeimp']].head(10)</p><p class="source-code"><strong class="bold">wageincome  weeksworked16  parentincome degcol  degadv wageincomeimp</strong></p><p class="source-code"><strong class="bold">personid         </strong></p><p class="source-code"><strong class="bold">100061     12,500    48     7,400    0    </strong><strong class="bold">0     12,500</strong></p><p class="source-code"><strong class="bold">100139    120,000    53    57,000    0    0    120,000</strong></p><p class="source-code"><strong class="bold">100284     58,000    47    50,000    0    0     58,000</strong></p><p class="source-code"><strong class="bold">100292        </strong><strong class="bold">NaN     4    62,760    1    0     28,029</strong></p><p class="source-code"><strong class="bold">100583     30,000    53    18,500    0    0     30,000</strong></p><p class="source-code"><strong class="bold">100833     39,000    45    37,000    0    </strong><strong class="bold">0     39,000</strong></p><p class="source-code"><strong class="bold">100931     56,000    53    60,200    1    0     56,000</strong></p><p class="source-code"><strong class="bold">101089     36,000    53    32,307    0    0     36,000</strong></p><p class="source-code"><strong class="bold">101122        </strong><strong class="bold">NaN   NaN       NaN    0    0     33,977</strong></p><p class="source-code"><strong class="bold">101132          0     22    2,470    0    0          0</strong></p></li>
<li>Let's take a look at the summary statistics for the original and imputed features. Not surprisingly, the imputed wage income's mean is lower than the original mean. As <a id="_idIndexMarker302"/>we discovered in the previous section, observations with missing wage incomes have lower degree attainment, weeks worked, and parental income. We also lose some of the variance in wage income:<p class="source-code">wagedata[['wageincome','wageincomeimp']].agg(['count',</p><p class="source-code">  'mean','std'])</p><p class="source-code"><strong class="bold">           </strong><strong class="bold">wageincome  wageincomeimp </strong></p><p class="source-code"><strong class="bold">count          5,091        8,984</strong></p><p class="source-code"><strong class="bold">mean          49,477        44,781</strong></p><p class="source-code"><strong class="bold">std           40,678        32,034</strong></p></li>
</ol>
<p>KNN does imputations without making any assumptions about the distribution of the underlying data. With regression imputation, the standard assumptions for linear regression apply – that is, that there is a linear relationship between features and that they are distributed normally. If this is not the case, KNN is likely a better approach for imputation.</p>
<p>Despite these advantages, KNN imputation does have limitations. First, we must tune the model with an initial assumption about a good value for k, sometimes informed by little more than our knowledge of the size of the dataset. KNN is also computationally expensive and may be impractical for very large datasets. Finally, KNN imputation may not perform well when the correlation is weak between the feature to be imputed and the predictor features. An alternative to KNN for imputation, random forest imputation, can help us <a id="_idIndexMarker303"/>avoid the disadvantages of both KNN and regression imputation. We will explore random forest imputation in the next section.</p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Using random forest for imputation</h1>
<p>Random forest is an ensemble learning <a id="_idIndexMarker304"/>method. It uses bootstrap aggregating, also <a id="_idIndexMarker305"/>known as bagging, to improve model accuracy. It makes predictions by repeatedly taking the mean of multiple trees, yielding progressively better estimates. We will use the <strong class="source-inline">MissForest</strong> algorithm in this section, which is an application of the random forest algorithm to find <a id="_idIndexMarker306"/>missing value imputation.</p>
<p><strong class="source-inline">MissForest</strong> starts by filling in the median or mode (for continuous or categorical features, respectively) for missing values, then uses random forest to predict values. Using this transformed dataset, with missing values replaced with initial predictions, <strong class="source-inline">MissForest</strong> generates new predictions, perhaps replacing the initial prediction with a better one. <strong class="source-inline">MissForest</strong> will typically go through at least four iterations of this process.</p>
<p>Running <strong class="source-inline">MissForest</strong> is even easier than using the KNN imputer, which we used in the previous section. We will impute values for the same wage income data that we worked with previously:</p>
<ol>
<li value="1">Let's start by importing the <strong class="source-inline">MissForest</strong> module and loading the NLS data:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import sys</p><p class="source-code">import sklearn.neighbors._base</p><p class="source-code">sys.modules['sklearn.neighbors.base'] =</p><p class="source-code">  sklearn.neighbors._base</p><p class="source-code">from missingpy import MissForest</p><p class="source-code">nls97 = pd.read_csv("data/nls97b.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p><p class="callout-heading">Note</p><p class="callout">We need to address a conflict in the name of <strong class="source-inline">sklearn.neighbors._base</strong>, which can be either <strong class="source-inline">sklearn.neighbors._base</strong> or <strong class="source-inline">sklearn.neighbors.base</strong>, depending on your version of scikit-learn. At the time of writing, <strong class="source-inline">MissForest</strong> uses the older name.</p></li>
<li>Let's do the same data cleaning that we did in the previous section:<p class="source-code">nls97['hdegnum'] = </p><p class="source-code">  nls97.highestdegree.str[0:1].astype('float')</p><p class="source-code">nls97.parentincome.replace(list(range(-5,0)), np.nan,</p><p class="source-code">  inplace=True)</p><p class="source-code">nls97['degltcol'] = np.where(nls97.hdegnum&lt;=2,1,0)</p><p class="source-code">nls97['degcol'] = np.where(nls97.hdegnum.between(3,4), </p><p class="source-code">  1,0)</p><p class="source-code">nls97['degadv'] = np.where(nls97.hdegnum&gt;4,1,0)</p><p class="source-code">wagedatalist = ['wageincome','weeksworked16',</p><p class="source-code">  'parentincome','degltcol','degcol','degadv']</p><p class="source-code">wagedata = nls97[wagedatalist]</p></li>
<li>Now, we <a id="_idIndexMarker307"/>are ready to run <strong class="source-inline">MissForest</strong>. Notice that this process is quite similar to our process of using the KNN imputer:<p class="source-code">imputer = MissForest()</p><p class="source-code">newvalues = imputer.fit_transform(wagedata)</p><p class="source-code">wagedatalistimp = ['wageincomeimp','weeksworked16imp', </p><p class="source-code">  'parentincomeimp','degltcol','degcol','degadv']</p><p class="source-code">wagedataimp = pd.DataFrame(newvalues, </p><p class="source-code">  columns=wagedatalistimp , index=wagedata.index)</p><p class="source-code">Iteration: 0</p><p class="source-code">Iteration: 1</p><p class="source-code">Iteration: 2</p><p class="source-code">Iteration: 3</p></li>
<li>Let's take a look at a few of our imputed values and some summary statistics. The imputed values have a lower mean. This is not surprising, given that we have <a id="_idIndexMarker308"/>already learned that the missing values are not distributed randomly, that individuals with lower degree attainment and weeks worked are more likely to have missing values for wage income:<p class="source-code">wagedata = wagedata.join(wagedataimp[['wageincomeimp', </p><p class="source-code">  'weeksworked16imp']])</p><p class="source-code">wagedata[['wageincome','weeksworked16','parentincome',</p><p class="source-code">  'degcol','degadv','wageincomeimp']].head(10)</p><p class="source-code"><strong class="bold">     wageincome  weeksworked16  parentincome  degcol  degadv  wageincomeimp</strong></p><p class="source-code"><strong class="bold">personid                                         </strong></p><p class="source-code"><strong class="bold">100061     12,500    48     7,400    0</strong><strong class="bold">    0     12,500</strong></p><p class="source-code"><strong class="bold">100139    120,000    53    57,000    0    0    120,000</strong></p><p class="source-code"><strong class="bold">100284     58,000    47    50,000    0</strong><strong class="bold">    0     58,000</strong></p><p class="source-code"><strong class="bold">100292        NaN     4    62,760    1    0     42,065</strong></p><p class="source-code"><strong class="bold">100583     30,000    53    18,500    0    0     30,000</strong></p><p class="source-code"><strong class="bold">100833</strong><strong class="bold">     39,000    45    37,000    0    0     39,000</strong></p><p class="source-code"><strong class="bold">100931     56,000    53    60,200    1    0     56,000</strong></p><p class="source-code"><strong class="bold">101089     36,000     5    32,307    0</strong><strong class="bold">    0     36,000</strong></p><p class="source-code"><strong class="bold">101122        NaN   NaN       NaN    0    0     32,384</strong></p><p class="source-code"><strong class="bold">101132          0    22     2,470    0    0</strong><strong class="bold">          0</strong></p><p class="source-code">wagedata[['wageincome','wageincomeimp', </p><p class="source-code">  'weeksworked16','weeksworked16imp']].agg(['count', </p><p class="source-code">  'mean','std'])</p><p class="source-code"><strong class="bold">    wageincome  wageincomeimp  weeksworked16  weeksworked16imp</strong></p><p class="source-code"><strong class="bold">count    5,091        8,984        7,068         8,984</strong></p><p class="source-code"><strong class="bold">mean    49,477       43,140           39            37</strong></p><p class="source-code"><strong class="bold">std     40,678       34,725           21            21</strong></p></li>
</ol>
<p><strong class="source-inline">MissForest</strong> uses the random forest algorithm to generate highly accurate predictions. Unlike KNN, it doesn't need to be tuned with an initial value for k. It also is computationally <a id="_idIndexMarker309"/>less expensive than KNN. Perhaps most importantly, random forest imputation is less sensitive to low or very high correlation among features, though that was not an issue in this example.</p>
<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Summary</h1>
<p>In this chapter, we explored the most popular approaches for missing value imputation and discussed the advantages and disadvantages of each approach. Assigning an overall sample mean is not usually a good approach, particularly when observations with missing values are different from other observations in important ways. We can also substantially reduce our variance. Forward or backward filling allows us to maintain the variance in our data, but it works best when the proximity of observations is meaningful, such as with time series or longitudinal data. In most non-trivial cases, we will want to use a multivariate technique, such as regression, KNN, or random forest imputation.</p>
<p>So far, we haven't touched on the important issue of data leakage and how to create separate training and testing datasets. To avoid data leakage, we need to work with training data independently of the testing data as soon as we begin our feature engineering. We will look at feature engineering in more detail in the next chapter. There, we will encode, transform, and scale features, while also being careful to separate the training and testing data.</p>
</div>
</div>
</body></html>