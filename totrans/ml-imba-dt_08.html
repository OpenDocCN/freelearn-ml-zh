<html><head></head><body>
		<div id="_idContainer156">
			<h1 id="_idParaDest-160" class="chapter-number"><a id="_idTextAnchor235"/>8</h1>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor236"/>Algorithm-Level Deep Learning Techniques</h1>
			<p>The data-level deep learning techniques have problems very similar to classical ML techniques. Since deep learning algorithms are quite different from classical ML techniques, we’ll explore some algorithm-level techniques for addressing data imbalance in this chapter. These algorithm-level techniques won’t change the data but accommodate the model instead. This exploration might uncover new insights or methods to better handle <span class="No-Break">imbalanced data.</span></p>
			<p>This chapter will be on the same lines as <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Cost-Sensitive Learning</em>, extending the ideas to deep learning models. We will look at algorithm-level deep learning techniques to handle the imbalance in data. Generally, these techniques do not modify the training data and often require no pre-processing steps, offering the benefit of no increased training times or additional runtime <span class="No-Break">hardware costs.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Motivation for <span class="No-Break">algorithm-level techniques</span></li>
				<li><span class="No-Break">Weighting techniques</span></li>
				<li>Explicit loss <span class="No-Break">function modification</span></li>
				<li>Discussing other <span class="No-Break">algorithm-based techniques</span></li>
			</ul>
			<p>By the end of this chapter, you’ll understand how to manage imbalanced data through model weight adjustments and loss function modifications using PyTorch APIs. We’ll also explore other algorithmic strategies, equipping you to make informed decisions in <span class="No-Break">real-world applications.</span></p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor237"/>Technical requirements</h1>
			<p>We will mostly be using standard functions from PyTorch and <strong class="source-inline">torchvision</strong> throughout this chapter. We will also use the Hugging Face Datasets library for dealing with <span class="No-Break">text data.</span></p>
			<p>The code and notebooks for this chapter are available on GitHub at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08">https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter08</a>. As usual, you can open the GitHub notebook using Google Colab by clicking on the <strong class="bold">Open in Colab</strong> icon at the top of this chapter’s notebook or by launching it from <a href="https://colab.research.google.com">https://colab.research.google.com</a> using the GitHub URL of <span class="No-Break">the notebook.</span></p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor238"/>Motivation for algorithm-level techniques</h1>
			<p>In this chapter, we will concentrate on deep learning techniques that have gained popularity in both the vision and text domains. We will mostly use a long-tailed imbalanced version of the MNIST dataset, similar to what we used in <a href="B17259_07.xhtml#_idTextAnchor205"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Data-Level Deep Learning Methods</em>. We will also consider CIFAR10-LT, the long-tailed version of CIFAR10, which is quite popular among researchers working with <span class="No-Break">long-tailed datasets.</span></p>
			<p>In this chapter, the ideas will be very similar to what we learned in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Cost-Sensitive Learning</em>, where the high-level idea was to increase the weight of the positive (minority) class and decrease the weight of the negative (majority) class in the cost function of the model. To facilitate this adjustment to the loss function, frameworks such as <strong class="source-inline">scikit-learn</strong> and XGBoost offer specific parameters. <strong class="source-inline">scikit-learn</strong> provides options such as <strong class="source-inline">class_weight</strong> and <strong class="source-inline">sample_weight</strong>, while XGBoost offers <strong class="source-inline">scale_pos_weight</strong> as <span class="No-Break">a parameter.</span></p>
			<p>In deep learning, the idea remains the same, and PyTorch provides a <strong class="source-inline">weight</strong> parameter in the <strong class="source-inline">torch.nn.CrossEntropyLoss</strong> class to implement this <span class="No-Break">weighting idea.</span></p>
			<p>However, we will see some advanced techniques that are more relevant and might give better results for the deep <span class="No-Break">learning models.</span></p>
			<p>With imbalanced datasets, the majority class examples contribute much more to the overall loss than the minority class examples. This happens because the majority class examples heavily outnumber the minority class examples. This means that the loss function being used is naturally biased toward the majority classes, and it fails to capture the error from minority classes. Keeping this in mind, can we change the loss function to account for this discrepancy for imbalanced datasets? Let’s try to figure <span class="No-Break">this out.</span></p>
			<p>The cross-entropy loss for binary classification is defined <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">CrossEntropyLoss</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space">               </span><span class="_-----MathTools-_Math_Variable">if</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">minority</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">class</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">term</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">    </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space">     </span><span class="_-----MathTools-_Math_Variable">otherwise</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">majority</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">class</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">term</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>Let’s say <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> represents the minority class and it’s the class we are trying to predict. So, we can try to increase the minority class term by multiplying it with a higher value of weight to increase its attribution to the <span class="No-Break">overall loss.</span></p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor239"/>Weighting techniques</h1>
			<p>Let’s continue to use the imbalanced MNIST dataset from the previous chapter, which has long-tailed data distribution, as shown in the following bar chart (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B17259_08_01.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Figure">Figure 8.1 – Imbalanced MNIST dataset</p>
			<p>Here, the <em class="italic">x</em> axis is the class label, and the <em class="italic">y</em> axis is the count of samples of various classes. In the next section, we will see how to use the weight parameter <span class="No-Break">in PyTorch.</span></p>
			<p>We will use the following model code for all the vision-related tasks in this chapter. We have defined a PyTorch neural network class called <strong class="source-inline">Net</strong> with two convolutional layers, a dropout layer, and two fully connected layers. The <strong class="source-inline">forward</strong> method applies these layers sequentially along with ReLU activations and max-pooling to process the input, <strong class="source-inline">x</strong>. Finally, it returns the <strong class="source-inline">log_softmax</strong> activation of <span class="No-Break">the output:</span></p>
			<pre class="source-code">
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = torch.nn.Dropout2d()
        self.fc1 = torch.nn.Linear(320, 50)
        self.fc2 = torch.nn.Linear(50, 10)
    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)</pre>			<p>Since our final layer of the model uses <strong class="source-inline">log_softmax</strong>, we will be using negative log-likelihood loss (<strong class="source-inline">torch.nn.functional.nll_loss</strong> or <strong class="source-inline">torch.nn.NLLLoss</strong>) <span class="No-Break">from PyTorch.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor240"/>Using PyTorch’s weight parameter</h2>
			<p>In the <strong class="source-inline">torch.nn.CrossEntropyLoss</strong> API, we have a <span class="No-Break"><strong class="source-inline">weight</strong></span><span class="No-Break"> parameter:</span></p>
			<pre class="source-code">
torch.nn.CrossEntropyLoss(weight=None, …)</pre>			<p>Here, <strong class="source-inline">weight</strong> is a one-dimensional tensor that assigns weight to <span class="No-Break">each class.</span></p>
			<p>We can use the <strong class="source-inline">compute_class_weight</strong> function from <strong class="source-inline">scikit-learn</strong> to get the weights of <span class="No-Break">various classes:</span></p>
			<pre class="source-code">
from sklearn.utils import class_weight
y = imbalanced_train_loader.dataset.targets
class_weights=class_weight.compute_class_weight( \
    'balanced', np.unique(y), y.numpy())
print(class_weights)</pre>			<p>This outputs <span class="No-Break">the following:</span></p>
			<pre class="source-code">
array([0.25002533, 0.41181869, 0.68687384, 1.14620743, 1.91330749, 3.19159483, 5.32697842, 8.92108434, 14.809 , 24.68166667])</pre>			<p>The <strong class="source-inline">compute_class_weight</strong> function computes the weights according to the following formula for each class, as we saw in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Cost-Sensitive Learning</em></span><span class="No-Break">:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">weight</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">class</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number">  </span><span class="_-----MathTools-_Math_Base">_______________________</span><span class="_-----MathTools-_Math_Base">   </span><span class="_-----MathTools-_Math_Variable">total</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">num</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">samples</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">for</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">class</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">total</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">number</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">of</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">samples</span><span class="_-----MathTools-_Math_Variable">  </span><span class="_-----MathTools-_Math_Base">___________________</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Variable">number</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">of</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">classes</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.2</em>, these weights have been plotted using a bar chart to help us see how they relate to the class frequency (<em class="italic">y</em> axis) for each class (<span class="No-Break"><em class="italic">x</em></span><span class="No-Break"> axis):</span></p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B17259_08_02.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Bar chart showing weights corresponding to each class</p>
			<p>As this figure shows, the fewer the number of samples a class has, the higher <span class="No-Break">its weight.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The key takeaway here is that the weight of a class is inversely proportional to the number of samples of that class, also called inverse class <span class="No-Break">frequency weighting.</span></p>
			<p class="callout">Another point to remember is that the class weights should always be computed from the training data. Using validation data or test data to compute the class weights might lead to the infamous data leakage or label leakage problem in ML. Formally, data leakage can happen when some information from outside of the training data is fed to the model. In this case, if we use test data to compute the class weights, then our evaluation of the model’s performance is going to be biased <span class="No-Break">and invalid.</span></p>
			<p>The comic in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em> shows a juggler managing weights of different sizes, each labeled with a distinct class label, symbolizing the varying weights assigned to different classes to tackle class imbalance during <span class="No-Break">model training:</span></p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B17259_08_03.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Comic illustrating the core idea behind class weighting</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Another way to compute weights is to empirically tune <span class="No-Break">the weights.</span></p>
			<p>Let’s write the <span class="No-Break">training loop:</span></p>
			<pre class="source-code">
def train(train_loader):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = torch.nn.functional.nll_loss(output, target weight)
        loss.backward()
        optimizer.step()</pre>			<p>A lot of other loss functions in PyTorch, including <strong class="source-inline">NLLLoss</strong>, <strong class="source-inline">MultiLabelSoftMarginLoss</strong>, <strong class="source-inline">MultiMarginLoss</strong>, and <strong class="source-inline">BCELoss</strong>, accept <strong class="source-inline">weight</strong> as a parameter <span class="No-Break">as well.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.4</em> compares the accuracy of various classes when using class weights versus when not using <span class="No-Break">class weights:</span></p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B17259_08_04.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Performance comparison of a model trained using cross-entropy loss with no class weights and with class weights</p>
			<p>As we can see, although the accuracy dropped for classes 0-4, it improved dramatically for the most imbalanced classes of 5-9. The overall accuracy of the model went up <span class="No-Break">as well.</span></p>
			<p class="callout-heading">Warning</p>
			<p class="callout">Please note that some loss functions, such as <strong class="source-inline">BCEWithLogitsLoss</strong>, provide two weighting parameters (<strong class="source-inline">BCEWithLogitsLoss</strong> can be used for binary classification or <span class="No-Break">multi-label classification):</span></p>
			<p class="callout">  • The <strong class="source-inline">weight</strong> parameter is the manual rescaling weight parameter for each example of the batch. This is more like the <strong class="source-inline">sample_weight</strong> parameter of the <span class="No-Break"><strong class="source-inline">sklearn</strong></span><span class="No-Break"> library.</span></p>
			<p class="callout">  • The <strong class="source-inline">pos_weight</strong> parameter is used to specify a weight for the positive class. It is similar to the <strong class="source-inline">class_weight</strong> parameter in the <span class="No-Break"><strong class="source-inline">sklearn</strong></span><span class="No-Break"> library.</span></p>
			<p class="callout-heading">🚀 Class reweighting in production at OpenAI</p>
			<p class="callout">OpenAI was trying to solve the problem of bias in training data using the image generation model DALL-E 2 [1]. DALL-E 2 is trained on a massive dataset of images from the internet, which can contain biases. For example, the dataset may contain more images of men than women or more images of people from certain racial or ethnic groups <span class="No-Break">than others.</span></p>
			<p class="callout">To limit undesirable model capabilities (such as generating violent images), they first filtered out such images from the training dataset. However, filtering training data can amplify biases. Why? In their blog [1], they explain using an example that when generating images for the prompt “a CEO,” their filtered model showed a stronger male bias than the unfiltered one. They suspected this amplification arose from two sources: dataset bias toward sexualizing women and potential classifier bias, despite their efforts to mitigate them. This may have resulted in the filter removing more images of women, skewing the <span class="No-Break">training data.</span></p>
			<p class="callout">To fix this bias, OpenAI applied reweighting to the DALL-E 2 training data by training a classifier to predict whether an image was from the unfiltered dataset. The weights for each image were then computed based on the classifier’s prediction. This scheme was shown to reduce the frequency change induced by filtering, which means that it was effective at counteracting the biases in the <span class="No-Break">training data.</span></p>
			<p>Next, to show its extensive applicability, we will apply the class weighting technique to <span class="No-Break">textual data.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor241"/>Handling textual data</h2>
			<p>Let’s work with some text data. We will use the <strong class="source-inline">datasets</strong> and <strong class="source-inline">transformers</strong> libraries from Hugging Face. Let’s import the <strong class="source-inline">trec</strong> dataset (the <strong class="bold">Text Retrieval Conference</strong> (<strong class="bold">TREC</strong>), a question classification dataset containing 5,500 labeled questions in the training set and 500 in the <span class="No-Break">test set):</span></p>
			<pre class="source-code">
from datasets import load_dataset
dataset = load_dataset("trec")</pre>			<p>This dataset is balanced, so we randomly remove examples from classes ABBR and DESC, making those classes the most imbalanced. Here is how the distribution of various classes looks like in this dataset, confirming the imbalance <span class="No-Break">in data:</span></p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B17259_08_05.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Frequency of various classes in the trec dataset from the Hugging Face Datasets library</p>
			<p>Let’s create a tokenizer (that splits text into words or sub-words) for the pre-trained DistilBERT language model vocabulary with a maximum input token length <span class="No-Break">of 512:</span></p>
			<pre class="source-code">
from transformers import AutoTokenizer
model_name = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.model_max_length = 512</pre>			<p>Next, we will create tokenized train and test sets from the dataset we just imported by invoking <span class="No-Break">the tokenizer:</span></p>
			<pre class="source-code">
def tokenize_function(examples):
    return tokenizer(examples["text"], padding=False, truncation=True)
tokenized_train_dataset = dataset["train"].shuffle(seed=42).\
    map(tokenize_function, batched=True)
tokenized_test_dataset = dataset["test"].shuffle(seed=42).\
    map(tokenize_function, batched=True)</pre>			<p>Next, let’s instantiate <span class="No-Break">the model:</span></p>
			<pre class="source-code">
from transformers import \
    AutoModelForSequenceClassification, TrainingArguments
model = AutoModelForSequenceClassification.from_pretrained(\
    model_name, num_labels=6)</pre>			<p>Now, let’s define and invoke a function to get <span class="No-Break">training arguments:</span></p>
			<pre class="source-code">
def get_training_args(runname):
    training_args = TrainingArguments(
        run_name=runname, output_dir="./results", \
        num_train_epochs=5, evaluation_strategy="epoch",\
        save_strategy="epoch", warmup_ratio=0.1, \
        lr_scheduler_type='cosine', \
        auto_find_batch_size=True, \
        gradient_accumulation_steps=4, fp16=True, \
        log_level="error"
    )
    return training_args
training_args = get_training_args(model_name)</pre>			<p>The following <strong class="source-inline">custom_compute_metrics()</strong> function returns a dictionary containing the precision, recall, and <span class="No-Break">F1 score:</span></p>
			<pre class="source-code">
from transformers import EvalPrediction
from typing import Dict
from sklearn.metrics import precision_score, recall_score, f1_score
def custom_compute_metrics(res: EvalPrediction) -&gt; Dict:
    pred = res.predictions.argmax(axis=1)
    target = res.label_ids
    precision = precision_score(target, pred, average='macro')
    recall = recall_score(target, pred, average='macro')
    f1 = f1_score(target, pred, average='macro')
    return {'precision': precision, 'recall': recall, 'f1': f1}</pre>			<p>Now, let’s implement the class containing the loss function that uses <span class="No-Break">class weights:</span></p>
			<pre class="source-code">
from transformers import Trainer
class CustomTrainerWeighted(Trainer):
    def compute_loss(self, model, inputs, return_outputs):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get('logits')
        loss_fct = nn.CrossEntropyLoss( \
            weight=torch.from_numpy(class_weights).cuda(0).float()
        )
        loss = loss_fct(logits.view(-1,self.model.config.num_labels),\
            labels.view(-1))
        return (loss, outputs) if return_outputs else loss</pre>			<p>We can initialize the weights similar to how we did previously using the <strong class="source-inline">compute_class_weight</strong> function in <strong class="source-inline">sklearn</strong>, and then feed it to the <strong class="source-inline">CrossEntropyLoss</strong> function in our <span class="No-Break"><strong class="source-inline">CustomTrainerWeighted</strong></span><span class="No-Break"> class:</span></p>
			<pre class="source-code">
modelWeighted = AutoModelForSequenceClassification \
    .from_pretrained(model_name, num_labels=4)
trainerWeighted = CustomTrainerWeighted(\
    model=modelWeighted, \
    args=training_args, \
    train_dataset=tokenized_train_dataset, \
    eval_dataset=tokenized_test_dataset, \
    tokenizer=tokenizer, \
    data_collator=data_collator, \
    compute_metrics=custom_compute_metrics)
trainerWeighted.train()</pre>			<p>As shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6</em>, we can see improvements in performance for the most imbalanced classes. However, a slight reduction was observed for the majority <span class="No-Break">class (trade-off!):</span></p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B17259_08_06.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Confusion matrix using no class weighting (left) and with class weights (right)</p>
			<p>As we can see, the minority classes, <strong class="bold">ABBR</strong> and <strong class="bold">DESC</strong>, have improved performance after class weighting at the cost of reduced performance for the <strong class="bold">ENTY</strong> class. Also, looking at some of the off-diagonal entries, we can see that the confusion between the <strong class="bold">ABBR</strong> and <strong class="bold">DESC</strong> classes (0.33 in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6</em> (left)) and between the <strong class="bold">DESC</strong> and <strong class="bold">ENTY</strong> classes (0.08 in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6</em> (left)) significantly dropped when using class weights (0.22 and <span class="No-Break">0.04, respectively).</span></p>
			<p>Some variants that deal with NLP tasks in particular suggest weighting the samples as the inverse of the square root of class frequency for their corresponding class instead of using the previously used inverse class frequency <span class="No-Break">weighting technique.</span></p>
			<p>In essence, class weighting can usually help with any kind of deep learning model, including textual data, when working with imbalanced data. Since data augmentation techniques are not as straightforward for text as they are for images, class weighting can be a useful technique for <span class="No-Break">NLP problems.</span></p>
			<p class="callout-heading">🚀 Class reweighting in production at Wayfair</p>
			<p class="callout">Wayfair used the BERT language model to improve the accuracy of its product search and recommendation system [2]. This was a challenging problem because the number of products that Wayfair sells is very large, and the number of products that a customer is likely to be interested in is <span class="No-Break">much smaller.</span></p>
			<p class="callout">There was an imbalance in data because the number of products that a customer had interacted with (for example, viewed, added to cart, or purchased) was much smaller than the number of products that the customer hadn’t interacted with. This made it difficult for BERT to learn to accurately predict which products a customer was likely to be <span class="No-Break">interested in.</span></p>
			<p class="callout">Wayfair used class weighting to address the data imbalance problem. They assigned a higher weight to positive examples (that is, products that a customer had interacted with) than to negative examples (that is, products that a customer had not interacted with). This helped ensure that BERT learned to accurately classify both positive and negative examples, even when the data <span class="No-Break">was imbalanced.</span></p>
			<p class="callout">The model was deployed to production. Wayfair is using the model to improve the accuracy of its product search and recommendation system and to provide a better experience <span class="No-Break">for customers.</span></p>
			<p>In the next section, we will discuss a minor variant of class weighting that can sometimes be more helpful than just the <span class="No-Break">weighting technique.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor242"/>Deferred re-weighting – a minor variant of the class weighting technique</h2>
			<p>There is a deferred re-weighting technique (mentioned by Cao et al. [3]) similar to the two-phase sampling approach we discussed in <a href="B17259_07.xhtml#_idTextAnchor205"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Data-Level Deep Learning Methods</em>. Here, we defer the re-weighting to later, wherein in the first phase of training, we train the model on the full imbalanced dataset without any weighting or sampling. In the second phase, we re-train the same model from the first phase with class weights (that are inversely proportional to the class frequencies) that have been applied to the loss function and, optionally, use a smaller learning rate. The first phase of training serves as a good form of initialization for the model for the second phase of training with reweighted losses. Since we use a smaller learning rate in the second phase of training, the weights of the model do not move very far from what they were in the first phase <span class="No-Break">of training:</span></p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B17259_08_07.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – The deferred re-weighting technique</p>
			<p>The comic in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.8</em> shows a magician who pulls out a large rabbit from a hat, followed by a smaller one, illustrating the two-phase process of initially training on the imbalanced dataset and subsequently applying re-weighting for more balanced training in the <span class="No-Break">second phase:</span></p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B17259_08_08.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – A comic illustrating the core idea of deferred re-weighting</p>
			<p>Please refer to the notebook titled <strong class="source-inline">Deferred_reweighting_DRW.ipynb</strong> in this book’s GitHub repository for more details. After applying the two-phase training part of the deferred re-weighting technique, we can see that the accuracy of our most imbalanced classes improves compared to training with <span class="No-Break">cross-entropy loss:</span></p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B17259_08_09.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Performance comparison of deferred re-weighting with cross-entropy loss</p>
			<p>Next, we will look at defining custom loss functions when the PyTorch standard loss functions don’t do everything that we want them <span class="No-Break">to do.</span></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor243"/>Explicit loss function modification</h1>
			<p>In PyTorch, we can formulate custom loss functions by deriving a subclass from the <strong class="source-inline">nn.Module</strong> class and overriding the <strong class="source-inline">forward()</strong> method. The <strong class="source-inline">forward()</strong> method for a loss function accepts the predicted and actual outputs as inputs, subsequently returning the computed <span class="No-Break">loss value.</span></p>
			<p>Even though class weighting does assign different weights to balance the majority and minority class examples, this alone is often insufficient, especially in cases of extreme class imbalance. What we would like is to reduce the loss from easily classified examples as well. The reason is that such easily classified examples usually belong to the majority class, and since they are higher in number, they dominate our training loss. This is the main idea of focal loss and allows for a more nuanced handling of examples, irrespective of the class they belong to. We’ll look at this in <span class="No-Break">this section.</span></p>
			<p class="callout-heading">Understanding the forward() method in PyTorch</p>
			<p class="callout">In PyTorch, you’ll encounter the <strong class="source-inline">forward()</strong> method in both neural network layers and loss functions. That’s because both a neural network layer and a loss function are derived from <strong class="source-inline">nn.Module</strong>. While it might seem confusing at first, understanding the context can help clarify <span class="No-Break">its role:</span></p>
			<p class="callout"><strong class="bold">🟠</strong><strong class="bold"> In neural </strong><span class="No-Break"><strong class="bold">network layers</strong></span><span class="No-Break">:</span></p>
			<p class="callout">The <strong class="source-inline">forward()</strong> method defines the transformation that input data undergoes as it passes through the layer. This could involve operations such as linear transformations, activation functions, <span class="No-Break">and more.</span></p>
			<p class="callout"><strong class="bold">🟢</strong><strong class="bold"> In </strong><span class="No-Break"><strong class="bold">loss functions</strong></span><span class="No-Break">:</span></p>
			<p class="callout">The <strong class="source-inline">forward()</strong> method computes the loss between the predicted output and the actual target values. This loss serves as a measure of how well the model <span class="No-Break">is performing.</span></p>
			<p class="callout"><strong class="bold">🔑</strong><strong class="bold"> </strong><span class="No-Break"><strong class="bold">Key takeaway</strong></span><span class="No-Break">:</span></p>
			<p class="callout">In PyTorch, both neural network layers and loss functions inherit from <strong class="source-inline">nn.Module</strong>, providing a unified interface. The <strong class="source-inline">forward()</strong> method is central to both, serving as the computational engine for data transformation in layers and loss computation in loss functions. Think of <strong class="source-inline">forward()</strong> as the “engine” for <span class="No-Break">either process.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor244"/>Focal loss</h2>
			<p>The techniques we’ve studied so far presume that minority classes need higher weights due to weak representation. However, some minority classes may be adequately represented, and over-weighting their samples could degrade the overall model performance. Hence, Tsung-Yi et al. [4] from Facebook (now Meta) introduced <strong class="bold">focal loss</strong>, a sample-based weighting technique where each example’s weight is determined by its difficulty and measured by the loss the model incurs <span class="No-Break">on it.</span></p>
			<p>The focal loss technique has roots in dense object detection tasks, where there are significantly more observations in one class than <span class="No-Break">the other:</span></p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B17259_08_10.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Class imbalance in object detection – majority as background, few as foreground</p>
			<p>Focal loss downweighs easy-to-classify examples and focuses on hard-to-classify examples. What this means is that it would reduce the model’s overconfidence; this overconfidence usually prevents the model from <span class="No-Break">generalizing well.</span></p>
			<p>Focal loss is an extension of cross-entropy loss. It is especially good for multi-class classification, where<a id="_idIndexMarker594"/> some classes are easy and others are difficult <span class="No-Break">to classify.</span></p>
			<p>Let’s start with our well-known cross-entropy loss for binary classification. If we let <span class="_-----MathTools-_Math_Variable">p</span> be the predicted probability that <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>, then the cross-entropy loss can be defined <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">CrossEntropyLoss</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space">               </span><span class="_-----MathTools-_Math_Variable">if</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space">     </span><span class="_-----MathTools-_Math_Variable">otherwise</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>This can be rewritten as <span class="_-----MathTools-_Math_Variable">CrossEntropyLoss</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base">)</span>, where <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span>, the probability of the true class, can be defined <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Space">               </span><span class="_-----MathTools-_Math_Variable">if</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Space">     </span><span class="_-----MathTools-_Math_Variable">otherwise</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<p>Here, <span class="_-----MathTools-_Math_Variable">p</span> is the predicted probability that <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">1</span> from <span class="No-Break">the model.</span></p>
			<p>The problem with this loss function is that in the case of imbalanced datasets, this loss function is dominated by the loss contribution from majority classes, and the loss contribution from the minority class is very small. This can be fixed via <span class="No-Break">focal loss.</span></p>
			<p>So, what is <span class="No-Break">focal loss?</span></p>
			<p><span class="_-----MathTools-_Math_Variable">FocalLoss</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">α</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
			<p>This formula looks slightly different from cross-entropy loss. There are two extra terms – <span class="_-----MathTools-_Math_Variable">α</span> and <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span>. Let’s try to understand the significance of each of <span class="No-Break">these terms:</span></p>
			<ul>
				<li><span class="_-----MathTools-_Math_Variable">α</span>: This value can be set to be inversely proportional to the number of examples of positive (minority) classes and is used to weigh the minority class examples more than the majority class. It can also be treated as a hyperparameter that can <span class="No-Break">be tuned.</span></li>
				<li><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span>: This term is called the <strong class="bold">modulating factor</strong>. If an example is too easy for the model to classify, that would<a id="_idIndexMarker595"/> mean that <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Space"> </span>is very high and the whole modulating factor value will be close to zero (assuming <span class="_-----MathTools-_Math_Variable">γ</span> &gt; 1), and the model won’t focus on this example much. On the other hand, if an example is hard – that is, <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Space"> </span>is low – then the modulating factor value will be high, and the model will focus on this<a id="_idIndexMarker596"/> <span class="No-Break">example more.</span></li>
			</ul>
			<h3>Implementation</h3>
			<p>Here’s the implementation <a id="_idIndexMarker597"/>of focal loss <span class="No-Break">from scratch:</span></p>
			<pre class="source-code">
class FocalLoss(torch.nn.Module):
    def __init__(self, gamma: float = 2, alpha =.98) -&gt; None:
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
    def forward(self, pred: torch.Tensor, target: torch.Tensor):
        # pred is tensor with log probabilities
        nll_loss = torch.nn.NLLLoss(pred, target)
        p_t = torch.exp(-nll_loss)
        loss = (1 – p_t)**self.gamma * self.alpha * nll_loss
        return loss.mean()</pre>			<p>Though the focal loss technique has roots in computer vision and object detection, we can potentially reap its benefits while working with tabular data and text data too. Some recent research has ported focal loss to classical ML frameworks such as XGBoost [5] and LightGBM [6], as well as to text data that uses <span class="No-Break">transformer-based models.</span></p>
			<p>The comic in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.11</em> shows an archer aiming at a small distant target, overlooking a large nearby target, symbolizing the focal loss’s emphasis on challenging minority <span class="No-Break">class examples:</span></p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B17259_08_11.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Illustration of focal loss</p>
			<p>PyTorch’s <strong class="source-inline">torchvision</strong> library <a id="_idIndexMarker598"/>already has this loss implemented for us <span class="No-Break">to use:</span></p>
			<pre class="source-code">
torchvision.ops.sigmoid_focal_loss(\
    inputs: Tensor, targets: Tensor, alpha: float = 0.25,\
    gamma: float = 2, reduction: str = 'none')</pre>			<p>The <strong class="source-inline">alpha</strong> and <strong class="source-inline">gamma</strong> values can be challenging to tune for the model and dataset being used. Using an <strong class="source-inline">alpha</strong> value of <strong class="source-inline">0.25</strong> and a <strong class="source-inline">gamma</strong> value of <strong class="source-inline">2</strong> with <strong class="source-inline">reduction= 'mean'</strong> on CIFAR10-LT (the long-tailed version of the CIFAR10 dataset) seems to do better than the regular cross-entropy loss, as shown in the following graph. For more details, please check the <strong class="source-inline">CIFAR10_LT_Focal_Loss.ipynb</strong> notebook in this book’s <span class="No-Break">GitHub repository:</span></p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B17259_08_12.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Model accuracy using cross-entropy loss versus focal loss (alpha=0.25, gamma=2) on the CIFAR10-LT dataset as training progresses</p>
			<p>In the Pascal VOC<a id="_idIndexMarker599"/> dataset for object detection [7], the focal loss helps detect a motorbike in the image, while the cross-entropy loss wasn’t able to detect it (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B17259_08_13.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Motorbike not detected by cross-entropy loss (left), while focal loss does detect it (right) on the Pascal VOC dataset. Source: fastai library GitHub repo [8]</p>
			<p>Though focal loss was initially designed for dense object detection, it has gained traction in class-imbalanced tasks due to its ability to assign higher weights to challenging examples that are commonly found in minority classes. While the proportion of such samples is higher in <a id="_idIndexMarker600"/>minority classes, the absolute count is higher in the majority class due to its larger size. Consequently, assigning high weights to challenging samples across all classes could still cause bias in the neural network’s performance. This motivates us to explore other loss functions that can reduce <span class="No-Break">this bias.</span></p>
			<p class="callout-heading">🚀 Focal loss in production at Meta</p>
			<p class="callout">There was a need to detect harmful content, such as hate speech and violence, at Meta (previously Facebook) [9]. ML models were trained on a massive dataset of text and images that included both <a id="_idIndexMarker601"/>harmful and non-harmful content. However, the system was struggling to learn from the harmful content examples because they were much fewer in number than the non-harmful examples. This was causing the system to overfit the non-harmful examples, and it was not performing well in terms of detecting harmful content in the <span class="No-Break">real world.</span></p>
			<p class="callout">To solve the problem, Meta used focal loss. Focal loss, as we’ve seen, is a technique that down-weighs the easy-to-classify examples so that the system focuses on learning from the hard-to-classify examples. Meta implemented focal loss in their training pipeline and was able to improve the performance of their AI system when it came to detecting harmful content by up to 10%. This is a significant improvement, and it shows that focal loss is a promising technique for training AI systems to detect rare or difficult-to-classify events. The new system has been deployed into production at Meta, and it has helped to substantially improve the safety of <span class="No-Break">the platform.</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor245"/>Class-balanced loss</h2>
			<p>The paper by Cui et al. [10] made a<a id="_idIndexMarker602"/> minor change to the <a id="_idIndexMarker603"/>equation for cross-entropy loss by adding a multiplicative coefficient of <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">)</span> to the loss function – that is, we use a value of <span class="_-----MathTools-_Math_Variable">α</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">)</span>, where <span class="_-----MathTools-_Math_Variable">β</span> is a hyperparameter between 0 and 1, and n is the number of samples of <span class="No-Break">a class:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">ClassBalancedCrossEntropyLoss</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable">β</span> = 0 means no weighting at all, while <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">→</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> means re-weighting by inverse class frequency. So, we can consider this method to be a way for the class weight of a particular class to be adjustable between 0 and (1/frequency of a class), depending on the value of the hyperparameter, <span class="_-----MathTools-_Math_Variable">β</span>, which is a <span class="No-Break">tunable parameter.</span></p>
			<p>This same term can be <a id="_idIndexMarker604"/>used in place of the alpha value. It can be used in conjunction with focal <span class="No-Break">loss too:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">ClassBalancedFocalLoss</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">γ</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
			<p>According to Cui et al., the recommended <a id="_idIndexMarker605"/>setting for the beta value is (N-1)/N, where N is the total number of <span class="No-Break">training examples.</span></p>
			<p>The comic in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.14</em> illustrates the core idea of this loss. It shows a tightrope walker who maintains balance using a pole with weights labeled “beta” on both ends, representing the adjustment of class weights to address <span class="No-Break">class imbalance:</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B17259_08_14.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Illustration of class-balanced loss</p>
			<h3>Implementation</h3>
			<p>Let’s take a look at the <a id="_idIndexMarker606"/>code for implementing <span class="No-Break">class-balanced loss:</span></p>
			<pre class="source-code">
class ClassBalancedCrossEntropyLoss(torch.nn.Module):
    def __init__(self, samples_per_cls, no_of_classes, beta=0.9999):
        super().__init__()
        self.beta = beta
        self.samples_per_cls = samples_per_cls
        self.no_of_classes = no_of_classes
    def forward(self, model_log_probs: torch.Tensor, \
                labels: torch.Tensor):
        effective_num = 1.0-np.power(self.beta,self.samples_per_cls)
        weights = (1.0 - beta)/np.array(effective_num)
        weights = weights/np.sum(weights) * self.no_of_classes
        weights = torch.tensor(weights).float()
        loss = torch.nn.NLLLoss(weights)
        cb_loss = loss(model_log_probs, labels)
        return cb_loss</pre>			<p>In the <strong class="source-inline">forward()</strong> function, <strong class="source-inline">effective_num</strong> effectively computes (1-<span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">)</span> as a vector, where <strong class="source-inline">n</strong> is a vector containing the number of samples per class. So, the <strong class="source-inline">weights</strong> vector is <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">)</span>. Using these weights, we compute the loss by using <strong class="source-inline">NLLLoss</strong> between the output of the model and the corresponding labels. <em class="italic">Table 8.1</em> shows the class-wise accuracy when the model is trained using class-balanced cross-entropy loss for 20 epochs. Here, we <a id="_idIndexMarker607"/>can see an accuracy improvement for the most imbalanced classes of 9, 8, 7, 6, <span class="No-Break">and 5:</span></p>
			<table id="table001-7" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Class</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">CrossEntropyLoss</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">ClassBalancedLoss</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">99.9</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">99.0</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">99.6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">99.0</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">98.1</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.3</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">96.8</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">94.7</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.5</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>5</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">94.2</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">97.4</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>6</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">92.8</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">98.3</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>7</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">81.2</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">94.3</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>8</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">63.6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">93.8</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>9</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">49.1</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">91.4</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – Class-wise accuracy using cross-entropy loss (left) and class-balanced cross-entropy loss (right)</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.15</em> compares the performance of class-balanced loss and <span class="No-Break">cross-entropy loss:</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B17259_08_15.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – Overall accuracy versus class-wise accuracy using class-balanced loss compared to the baseline model</p>
			<p class="callout-heading">🚀 Class-balanced loss in production at Apple</p>
			<p class="callout">The accessibility team at Apple aimed to ensure usability for all by addressing the lack of proper accessibility<a id="_idIndexMarker608"/> information in many apps. They made these apps usable for individuals with disabilities through features such as screen recognition. The researchers aimed to automatically generate accessibility metadata [11] for mobile apps based on their visual interfaces, a problem that had significant class imbalance due to the diverse range of UI elements. UI elements such as text, icons, and sliders were identified from app screenshots. The text elements were highly represented with 741,285 annotations, while sliders were least represented with <span class="No-Break">1,808 annotations.</span></p>
			<p class="callout">The dataset consisted of 77,637 screens from 4,068 iPhone apps, with a wide variety of UI elements, leading to a highly imbalanced dataset, especially considering the hierarchical nature of the <span class="No-Break">UI elements.</span></p>
			<p class="callout">A class-balanced loss function and data augmentation were employed to handle the class imbalance effectively. This allowed the model to focus more on underrepresented UI classes, thereby improving the overall performance. The model was designed to be robust and fast, enabling on-device deployment. This ensured that the accessibility features could be generated in real time, enhancing the user experience for screen <span class="No-Break">reader users.</span></p>
			<p>Modern ConvNet classifiers tend to overfit the minority classes in imbalanced datasets. What if we could prevent that from happening? The <strong class="bold">Class-Dependent Temperature</strong> (<strong class="bold">CDT</strong>) loss function aims to <span class="No-Break">do that.</span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor246"/>Class-dependent temperature Loss</h2>
			<p>In addressing imbalanced <a id="_idIndexMarker609"/>datasets, traditional explanations suggest that a model’s inferior performance on minority classes, compared to majority classes, stems from its inclination to minimize<a id="_idIndexMarker610"/> average per-instance loss. This biases the model toward predicting majority classes. To counteract this, re-sampling and re-weighting strategies have <span class="No-Break">been proposed.</span></p>
			<p>However, Ye et al. [12] introduced the <strong class="bold">Class-Dependent Temperature</strong> (<strong class="bold">CDT</strong>) Loss, presenting a novel perspective. Their research indicates that ConvNets tend to overfit minority class examples, as evident from a larger feature deviation between training and test sets for minority classes compared to majority ones. Feature deviation occurs when a model learns the training data distribution of feature values excessively well, subsequently failing to generalize to new data. With CDT loss, the model’s decision values for training examples are divided by a “temperature” factor, dependent on each class’s frequency. This division makes the training more attuned to feature deviation and aids in effective learning across both prevalent and <span class="No-Break">scarce categories.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.16</em> portrays how CDT loss modifies class weights according to class frequencies, using the visual analogy of a juggler on a unicycle handling items marked with different <span class="No-Break">class names:</span></p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B17259_08_16.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – A unicyclist juggling items, adjusting class weights based on frequencies</p>
			<p>The following class<a id="_idIndexMarker611"/> implements<a id="_idIndexMarker612"/> this <span class="No-Break">loss function:</span></p>
			<pre class="source-code">
class CDT(torch.nn.Module):
    def __init__(self, num_class_list, gamma=0.36):
        super(CDT, self).__init__()
        self.gamma = gamma
        self.num_class_list = num_class_list
        self.cdt_weight = torch.FloatTensor(
            [
            (max(self.num_class_list)/i) ** self.gamma\
            for i in self.num_class_list
            ]
        ).to(device)
    def forward(self, inputs, targets):
        inputs = inputs/self.cdt_weight
        loss=torch.nn.functional.nll_loss(inputs,targets)
        return loss</pre>			<p>Here is an explanation of the <span class="No-Break"><strong class="source-inline">CDT</strong></span><span class="No-Break"> class:</span></p>
			<ul>
				<li><strong class="source-inline">self.num_class_list</strong> stores the number of examples in <span class="No-Break">each class.</span></li>
				<li><strong class="source-inline">self.cdt_weight = torch.FloatTensor([...]).to(device)</strong> computes the class-dependent temperature weights for each class. For each class, the weight is <a id="_idIndexMarker613"/>computed as <strong class="source-inline">(max(num_class_list) / num_class_list[i]) ** </strong><span class="No-Break"><strong class="source-inline">gamma</strong></span><span class="No-Break">.</span><p class="list-inset">The larger the number of examples in a class, the smaller its value in the <strong class="source-inline">self.cdt_weight</strong> list. Majority class examples have lower values, while minority class examples have <span class="No-Break">higher values.</span></p></li>
				<li><strong class="source-inline">inputs = inputs /self.cdt_weight</strong> scales the log probabilities (as inputs) from the model by the class-dependent temperature weights. This increases the<a id="_idIndexMarker614"/> absolute values of the negative log probabilities for minority class examples, making them more significant in the loss calculation than those for the majority class. This intends to make the model focus more on the minority <span class="No-Break">class examples.</span><p class="list-inset">In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.17</em>, we’re plotting the overall accuracy of CDT loss and cross-entropy loss (left) and the accuracies of various <span class="No-Break">classes (right):</span></p></li>
			</ul>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B17259_08_17.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Performance comparison between cross-entropy loss and CDT loss</p>
			<p>As we can see, there is an accuracy improvement for some classes, such as 9, 7, 6, 5, and 3, but a decrease in performance for some of the other classes. It seems to give a lukewarm performance on the imbalanced MNIST dataset that we used, but it can potentially be helpful <a id="_idIndexMarker615"/>for <span class="No-Break">other datasets.</span></p>
			<p>What if we could <a id="_idIndexMarker616"/>dynamically adjust the weights of the classes according to their difficulty for the model during training? We could measure the class difficulty by the accuracy of its predictions for the examples’ class and then use this difficulty to compute the weight for <span class="No-Break">that class.</span></p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor247"/>Class-wise difficulty-balanced loss</h2>
			<p>The paper from Sinha et al. [13] proposed that the weight for a class, <span class="_-----MathTools-_Math_Variable">c</span>, after training time, <span class="_-----MathTools-_Math_Variable">t</span>, should be<a id="_idIndexMarker617"/> directly proportional to the difficulty of the class. The lower the accuracy of the class, the higher <span class="No-Break">its</span><span class="No-Break"><a id="_idIndexMarker618"/></span><span class="No-Break"> difficulty.</span></p>
			<p>Mathematically, this can be represented <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">τ</span></span></p>
			<p>Here, <span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Space"> </span>is the weight of class <span class="_-----MathTools-_Math_Variable">c</span> after training time <span class="_-----MathTools-_Math_Variable">t</span>, and <span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Space"> </span>is the class difficulty, which is defined by the <span class="No-Break">following equation:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Variable">Accuracy</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">c</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">t</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
			<p>Here, <span class="_-----MathTools-_Math_Variable">Accuracy</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Space"> </span>is the accuracy of class <span class="_-----MathTools-_Math_Variable">c</span> on the validation dataset after time <span class="_-----MathTools-_Math_Variable">t</span>, and <span class="_-----MathTools-_Math_Variable">τ</span> is <span class="No-Break">a hyperparameter.</span></p>
			<p>The point here is that we would want to dynamically increase the weight of the class for which the model’s accuracy is lower as training progresses. We could do this every epoch or every few epochs of training and feed the updated weights to the cross-entropy loss. Please look at the corresponding notebook titled <strong class="source-inline">Class_wise_difficulty_balanced_loss.ipynb</strong> in this book’s GitHub repository for the full <span class="No-Break">training loop:</span></p>
			<pre class="source-code">
class ClassDifficultyBalancedLoss(torch.nn.Module):
    def __init__(self, class_difficulty, tau=1.5):
        super().__init__()
        self.class_difficulty = class_difficulty
        self.weights = self.class_difficulty ** float(tau)
        self.weights = self.weights / (
            self.weights.sum() * len(self.weights))
        self.loss = torch.nn.NLLLoss(
            weight= torch.FloatTensor(self.weights))
    def forward(self, input: torch.Tensor, target: torch.Tensor):
        return self.loss(input, target)</pre>			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.18</em> illustrates <a id="_idIndexMarker619"/>the concept of difficulty-balanced loss using a comic with an acrobat on a trampoline. Each bounce is<a id="_idIndexMarker620"/> labeled with an accuracy score, highlighting how classes with lower accuracy receive increasing weight as the acrobat bounces higher <span class="No-Break">each time:</span></p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B17259_08_18.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – Illustration of difficulty-balanced loss – the acrobat’s bounces show increasing weight for lower-accuracy classes</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.19</em> shows <a id="_idIndexMarker621"/>the performance<a id="_idIndexMarker622"/> of class-wise difficulty-balanced loss compared to cross-entropy loss as <span class="No-Break">the baseline:</span></p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B17259_08_19.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 – Performance comparison of models trained using class-wise difficulty-balanced loss and cross-entropy loss</p>
			<p>Here, we can see that the performance of several classes improves, including the biggest jump of 40% to 63.5% for the most imbalanced <span class="No-Break">class (9).</span></p>
			<p>Next, we will look <a id="_idIndexMarker623"/>at some of the other miscellaneous algorithm-based techniques that can still help us deal with <span class="No-Break">imbalanced </span><span class="No-Break"><a id="_idIndexMarker624"/></span><span class="No-Break">datasets.</span></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor248"/>Discussing other algorithm-based techniques</h1>
			<p>In this section, we’ll explore a diverse set of algorithm-level techniques that we haven’t covered so far. Intriguingly, these methods – from regularization techniques that mitigate overfitting to Siamese networks skilled in one-shot and few-shot learning, to deeper neural architectures and threshold adjustments – also have a beneficial side effect: they can occasionally mitigate the impact of <span class="No-Break">class imbalance.</span></p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor249"/>Regularization techniques</h2>
			<p>The paper from S. Alshammari et al. [14] found that well-known regularization techniques such as L2-regularization<a id="_idIndexMarker625"/> and the MaxNorm constraint are quite helpful in long-tailed recognition. The paper proposes to do these only at the last layer of classification (sigmoid or softmax, for example). Here are <span class="No-Break">their findings:</span></p>
			<ul>
				<li><strong class="bold">L2-regularization</strong> (also called weight decay) generally<a id="_idIndexMarker626"/> keeps the weights in check and helps the model <a id="_idIndexMarker627"/>generalize better by preventing the model <span class="No-Break">from overfitting.</span></li>
				<li>The <strong class="bold">MaxNorm</strong> constraint is a<a id="_idIndexMarker628"/> form of regularization where we clip or limit the weight to not go beyond a maximum limit. Keras has a built-in API called <strong class="source-inline">tf.keras.constraints.MaxNorm</strong>, while PyTorch has <strong class="source-inline">torch.clamp</strong> to help <span class="No-Break">with this.</span></li>
			</ul>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor250"/>Siamese networks</h2>
			<p>On a similar note, previous <a id="_idIndexMarker629"/>research has found <strong class="bold">Siamese networks</strong> to be very robust to the adverse effects of <a id="_idIndexMarker630"/>class imbalance. Siamese networks have been quite useful in the areas of one-shot learning (classifying new data when we have only one example of each class in the training data) and few-shot learning (classifying new data when we have only a few examples of each class in the training data). Siamese networks use a contrastive loss function that takes in pairs of input images and then computes a similarity metric (Euclidean distance, Manhattan distance, or cosine distance) to figure out how similar or dissimilar they are. This can be used to compute the embeddings of each unique class of images in the training data. At inference time or test time, the distance of the <a id="_idIndexMarker631"/>new input image from each unique class can be computed to find the appropriate class of the image. The best part of this technique is that it provides a way to learn the feature representation of each class. Siamese<a id="_idIndexMarker632"/> networks have found a wide variety of practical applications in the industry regarding vision problems (for example, whether two images are of the same person or not) as well as NLP problems (for example, finding out whether two questions/queries are similar or not on, say, platforms such as Stack Overflow, Quora, Google, and <span class="No-Break">so on).</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.20</em> shows a Siamese network where two inputs are fed into the model to get their embeddings, which are then compared for similarity using a <span class="No-Break">distance metric:</span></p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B17259_08_20.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.20 – High-level working of the Siamese network model</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor251"/>Deeper neural networks</h2>
			<p>A study by Ding et al. 2017 [15] discovered that deeper neural networks (more than 10 layers) are more helpful in <a id="_idIndexMarker633"/>general with imbalanced <a id="_idIndexMarker634"/>datasets for <span class="No-Break">two reasons:</span></p>
			<ul>
				<li>A faster rate <span class="No-Break">of convergence</span></li>
				<li>Better <span class="No-Break">overall performance</span></li>
			</ul>
			<p>This is attributed to the fact that deep networks are exponentially more efficient at capturing the complexity of data. Though their experiment was for facial action recognition tasks, this may be useful for trying out deeper networks on other kinds of data <span class="No-Break">and domains.</span></p>
			<p>However, the cons of longer training times, increased hardware cost, and increased complexity may not always be worth the hassle in <span class="No-Break">industry settings.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor252"/>Threshold adjustment</h2>
			<p>As we discussed in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Cost-Sensitive Learning</em>, threshold adjustment is a cost-sensitive meta-learning<a id="_idIndexMarker635"/> technique. Threshold adjustment applies equally well to deep learning models, and it can be critical to make sure that<a id="_idIndexMarker636"/> the thresholds for classification are properly tuned and adjusted, especially when the training data distribution has been changed (for example, oversampled or undersampled) or even when class weights or new loss functions <span class="No-Break">are used.</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor253"/>Summary</h1>
			<p>In this chapter, we explored various loss functions as remedies to class imbalance. We started with the class-weighting technique and deferred re-weighting, both designed to penalize errors on minority class samples. As we progressed, we encountered focal loss, where we shifted from class-centric to sample-centric weighting, focusing on the difficulty of samples. Despite its merits, we learned that focal loss may still be biased toward the majority class when assigning weights to challenging samples across all classes. Subsequent discussions on class-balanced loss, CDT loss, and class-wise difficulty-balanced loss were provided, each introducing unique strategies to dynamically adjust weights or modulate the model’s focus between easy and challenging samples, aiming to enhance performance on <span class="No-Break">imbalanced datasets.</span></p>
			<p>To summarize, algorithm-level techniques usually modify the loss functions used by the model in some way to accommodate for imbalances in the dataset. They typically do not increase the training time and cost, unlike data-level techniques. They are well suited for problems or domains with large amounts of data or where gathering more data is hard <span class="No-Break">or expensive.</span></p>
			<p>Even though these techniques improve the performance of minority classes, the majority classes may sometimes suffer as a result. In the next chapter, we will look at some of the hybrid techniques that can combine the data-level and algorithm-level techniques so that we can get the best of <span class="No-Break">both worlds.</span></p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor254"/>Questions</h1>
			<ol>
				<li>Mean false error and mean squared <span class="No-Break">false error:</span><p class="list-inset">Wang et al. [16]proposed that regular loss functions poorly capture the errors from minority classes in the case of high data imbalance due to lots of negative samples that dominate the loss function. Hence, they proposed a new loss function where the main idea was to split the training error into four different kinds <span class="No-Break">of errors:</span></p><ul><li>False Positive Error (FPE) = (1/number_of_negative_samples) * (error from <span class="No-Break">negative samples)</span></li><li>False Negative Error (FNE) = (1/number_of_positive_samples) * (error from <span class="No-Break">positive samples)</span></li><li>Mean False Error (MFE) = <span class="No-Break">FPE+ FNE</span></li><li>Mean Squared False Error (MSFE) = FPE<span class="superscript">2</span> + <span class="No-Break">FNE</span><span class="No-Break"><span class="superscript">2</span></span></li></ul><p class="list-inset">The error here could be computed using the usual cross-entropy loss or any other loss used for classification. Implement the MFE and MSFE loss functions for both the imbalanced MNIST and CIFAR10-LT datasets, and see whether the model performance improves over the baseline of <span class="No-Break">cross-entropy loss.</span></p></li>
				<li>In this chapter, while implementing the CDT loss function, replace the imbalanced MNIST dataset with CIFAR10-LT (the long-tailed version of CIFAR-10). Check whether you still achieve improved performance over the baseline. You may have to play with the gamma value or perform any of the other tricks mentioned in the original paper [12] to get an improvement over <span class="No-Break">the baseline.</span></li>
				<li>Tversky Loss was introduced in the paper by Salehi et al. [17]. Please read this paper to understand the Tversky loss function and its implementation details. Finally, implement the Tversky loss on an imbalanced MNIST dataset and compare its performance with a <span class="No-Break">baseline model.</span></li>
				<li>We used the class-weighting technique and cross-entropy loss with the <strong class="source-inline">trec</strong> dataset in this chapter. Replace cross-entropy loss with focal loss, and see whether model <span class="No-Break">performance improves.</span></li>
			</ol>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor255"/>References</h1>
			<ol>
				<li value="1"><em class="italic">DALL·E 2 pre-training mitigations</em>, <span class="No-Break">2022, </span><a href="https://openai.com/research/dall-e-2-pre-training-mitigations"><span class="No-Break">https://openai.com/research/dall-e-2-pre-training-mitigations</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">BERT Does Business: Implementing the BERT Model for Natural Language Processing at Wayfair</em>, <span class="No-Break">2019, </span><a href="https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair"><span class="No-Break">https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair</span></a><span class="No-Break">.</span></li>
				<li>K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, <em class="italic">Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss</em>, [Online]. Available <span class="No-Break">at </span><a href="https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf"><span class="No-Break">https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf</span></a><span class="No-Break">.</span></li>
				<li>T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, <em class="italic">Focal Loss for Dense Object Detection</em>. arXiv, Feb. 07, <span class="No-Break">2018, </span><a href="http://arxiv.org/abs/1708.02002"><span class="No-Break">http://arxiv.org/abs/1708.02002</span></a><span class="No-Break">.</span></li>
				<li>Wang et al., <em class="italic">Imbalance-XGBoost: leveraging weighted and focal losses for binary label-imbalanced classification with </em><span class="No-Break"><em class="italic">XGBoost</em></span><span class="No-Break">, </span><a href="https://arxiv.org/pdf/1908.01672.pdf"><span class="No-Break">https://arxiv.org/pdf/1908.01672.pdf</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">Focal loss implementation for </em><span class="No-Break"><em class="italic">LightGBM</em></span><span class="No-Break">, </span><a href="https://maxhalford.github.io/blog/lightgbm-focal-loss"><span class="No-Break">https://maxhalford.github.io/blog/lightgbm-focal-loss</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">The PASCAL VOC </em><span class="No-Break"><em class="italic">project</em></span><span class="No-Break">, </span><a href="http://host.robots.ox.ac.uk/pascal/VOC/"><span class="No-Break">http://host.robots.ox.ac.uk/pascal/VOC/</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">fastai library</em>, <span class="No-Break">2018, </span><a href="https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb"><span class="No-Break">https://github.com/fastai/fastai1/blob/master/courses/dl2/pascal-multi.ipynb</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">Community Standards report</em>, <span class="No-Break">2019, </span><a href="https://ai.meta.com/blog/community-standards-report/"><span class="No-Break">https://ai.meta.com/blog/community-standards-report/</span></a><span class="No-Break">.</span></li>
				<li>Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, <em class="italic">Class-Balanced Loss Based on Effective Number of Samples</em>, <span class="No-Break">p. 10.</span></li>
				<li>X. Zhang et al., <em class="italic">Screen Recognition: Creating Accessibility Metadata for Mobile Applications from Pixels</em>, in Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, Yokohama Japan: ACM, May 2021, pp. 1–15. doi: 10.1145/3411764.3445186. <span class="No-Break">Blog: </span><a href="https://machinelearning.apple.com/research/mobile-applications-accessible"><span class="No-Break">https://machinelearning.apple.com/research/mobile-applications-accessible</span></a><span class="No-Break">.</span></li>
				<li>H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao, <em class="italic">Identifying and Compensating for Feature Deviation in Imbalanced Deep Learning</em>. arXiv, Jul. 10, 2022. Accessed: Dec. 14, 2022. [Online]. <span class="No-Break">Available: </span><a href="http://arxiv.org/abs/2001.01385"><span class="No-Break">http://arxiv.org/abs/2001.01385</span></a><span class="No-Break">.</span></li>
				<li>S. Sinha, H. Ohashi, and K. Nakamura, <em class="italic">Class-Wise Difficulty-Balanced Loss for Solving Class-Imbalance</em>. arXiv, Oct. 05, 2020. Accessed: Dec. 17, 2022. [Online]. Available <span class="No-Break">at </span><a href="http://arxiv.org/abs/2010.01824"><span class="No-Break">http://arxiv.org/abs/2010.01824</span></a><span class="No-Break">.</span></li>
				<li>S. Alshammari, Y.-X. Wang, D. Ramanan, and S. Kong, <em class="italic">Long-Tailed Recognition via Weight Balancing</em>, in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, USA, Jun. 2022, pp. 6887–6897. <span class="No-Break">Doi: 10.1109/CVPR52688.2022.00677</span><span class="No-Break">.</span></li>
				<li>W. Ding, D.-Y. Huang, Z. Chen, X. Yu, and W. Lin, <em class="italic">Facial action recognition using very deep networks for highly imbalanced class distribution</em>, in 2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), Kuala Lumpur, Dec. 2017, pp. 1368–1372. <span class="No-Break">doi: 10.1109/APSIPA.2017.8282246</span><span class="No-Break">.</span></li>
				<li>S. Wang, W. Liu, J. Wu, L. Cao, Q. Meng, and P. J. Kennedy, <em class="italic">Training deep neural networks on imbalanced datasets</em>, in 2016 International Joint Conference on Neural Networks (IJCNN), Vancouver, BC, Canada, Jul. 2016, pp. 4368–4374. <span class="No-Break">doi: 10.1109/IJCNN.2016.7727770</span><span class="No-Break">.</span></li>
				<li>S. S. M. Salehi, D. Erdogmus, and A. Gholipour, <em class="italic">Tversky loss function for image segmentation using 3D fully convolutional deep networks</em>. arXiv, Jun. 18, 2017. Accessed: Dec. 23, 2022. [Online]. Available <span class="No-Break">at </span><a href="http://arxiv.org/abs/1706.05721"><span class="No-Break">http://arxiv.org/abs/1706.05721</span></a><span class="No-Break">.</span></li>
			</ol>
		</div>
	</body></html>