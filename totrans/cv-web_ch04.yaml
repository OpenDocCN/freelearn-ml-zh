- en: Chapter 4. Smile and Wave, Your Face Has Been Tracked!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most commonly seen object in our lives is a human face. We interact with
    people everywhere even when we do not meet them in person; we write a lot of messages
    via social networks, such as Twitter and Facebook, or e-mails and text messages
    using our phones. Face detection and tracking has many applications. In some cases,
    you might want to create a human computer interface, which will take the head
    position as an input or, more likely, you might want to help your users with tagging
    their friends. Actually, there are a lot of face detection libraries, which are
    written on JavaScript; these outnumber the libraries that focus on image processing
    itself. This is a good opportunity to choose the library that you really need.
    In addition to face detection, many libraries support face particle recognition
    and recognition of other objects.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the JSFeat ([http://inspirit.github.io/jsfeat/](http://inspirit.github.io/jsfeat/)),
    tracking.js ([http://inspirit.github.io/jsfeat/](http://inspirit.github.io/jsfeat/)),
    and headtrackr ([https://github.com/auduno/headtrackr](https://github.com/auduno/headtrackr))
    libraries. The last library supports head tracking instead of just recognition.
    Most of the libraries focus on Haar-like features detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the help of several examples, we will cover the following topics in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Face detection with JSFeat
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tagging people with tracking.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Head tracking with Camshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face detection with JSFeat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw detection of various objects in the previous chapter. The human face
    is much more complicated than just a regular color object, for example. More complex
    detectors share in common such things like the usage of brightness information
    and the patterns that this information forms. First of all, we need to see how
    face recognition is done. Without that, the tracking process will be quite difficult
    to understand. Actually, in most cases, the face recognition part is just the
    first step of face tracking algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We start with the JSFeat project. This awesome library provides a functionality
    to detect a face in two ways. Both have many applications in the real world. We
    will see how both of them work from the inside and discuss the API provided by
    JSFeat.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection using Haar-like features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is probably the most popular face detector nowadays. Most of the libraries
    use exactly this algorithm as a common face detector. It is easy to implement
    and use. In addition to this, it can be used in any application as it gives good
    precision in face detection. The method itself forms a Viola-Jones object detection
    framework, which was proposed by Paul Viola and Michael Jones in 2001.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the convolution kernels from [Chapter 2](cv-web_ch02.html#aid-I3QM1
    "Chapter 2. Turn Your Browser into Photoshop"), *Turn Your Browser into Photoshop*?
    Take a look at this picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Face detection using Haar-like features](img/image00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The rectangles are called Haar-like features or Haar features. They are just
    convolution kernels, where we subtract pixels under the white rectangle and add
    pixels where under the black part. To compute them fast, we use integral images.
    If you do not remember the concept, then you had better refresh your memory by
    referring to the section on integral image under the section *What is filtering
    and how to use it* in [Chapter 2](cv-web_ch02.html#aid-I3QM1 "Chapter 2. Turn
    Your Browser into Photoshop"), *Turn Your Browser into Photoshop*. Briefly, the
    integral image provides substantial support in fast calculation of the sum of
    pixels in a rectangular area of an image.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the features shown in the right-hand side of the picture too. They
    are rotated by 45 degrees. For that case, we use the tilted integral. They can
    capture more object details. The functionality for tilted features is available
    in most of the libraries. But there is a problem which prevents its usage in real-world
    applications—the Haar features are usually applied on low resolution parts of
    an image, for example, 20 x 20 or 24 x 24; when we rotate a feature (or integral
    image), we may face rounding errors. Because of this, those features are rarely
    used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can these features help us? Using them, we can describe an object by selecting
    the unique features of it. For example, you see an ordinary female face under
    low resolution in the following images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Face detection using Haar-like features](img/image00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Usually, the part with the eyes is darker than the lower part. Furthermore,
    the nose is brighter than the eyes and the brows. We already found two unique
    face features!
  prefs: []
  type: TYPE_NORMAL
- en: For the input image, we use a sliding window to apply those kernels and check
    whether an object in the window is a face or not. We need to do this for all possible
    sizes and locations of kernels, which is practically impossible. Even for a 24
    x 24 window, we need to check more than 160,000 features. Of course, there is
    a solution for this. We need to train a classifier and save only those features
    that are relevant to the detected object, in our case, it is a face. Unfortunately,
    JavaScript libraries do not provide such a functionality. Actually, they do not
    need to do so, since the libraries we use already contain most of necessary classifiers
    for face detection. Besides, the training time can take from several hours to
    months. However, if you need to detect something else or improve the detection
    accuracy, then you will probably want to see other libraries, for example, OpenCV
    ([http://opencv.org](http://opencv.org)). They provide the functionality to train
    your own classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In short, during the training process, the algorithm checks all possible sizes
    and positions for features and selects the best of them that describe the object.
    After the first step we get several thousands of features. Still, this is too
    much. The next step provides a solution for this problem. We group these features
    into different stages of the classifier. When the algorithm checks the slide window,
    the algorithm evaluates it on each group one-by-one. If the first group fails
    the checking, then we discard that window and move on to another. This whole process
    structure is called a **Cascade of classifiers**. Eventually, the training process
    significantly reduces the number of features that need checking.
  prefs: []
  type: TYPE_NORMAL
- en: To make the algorithm scale invariant, it is applied using various window sizes.
    Unfortunately, the algorithm is not rotation invariant. You can try to apply this
    by rotating the source image but in that case, you may face incorrect results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you have an idea of how the whole algorithm works. Let''s see how we can
    apply it in the JSFeat library by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to define an object which we want to detect. In our case it
    is a face. To set an object, we need to add a JavaScript file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, set the classifier in the code. It contains cascades, the original window
    size, and the tilted integral flag, if it is required:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we get the image data from the context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define an image and convert it to grayscale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sometimes, it is a good choice to increase the image contrast and remove some
    noise, which can be done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then predefine arrays for integrals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take a close look at what we do here. We compute the tilted integral only if
    it is set to `true` in the classifier. There is a part that is not required, but
    in some cases, it helps to speed up the computation and remove noisy elements.
    We will check the edges'' density using the Canny edge detector and its integral:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the number of edges in a window is less than the edges'' density. Then the
    program will skip that window without checking the Haar features. You can set
    the density threshold in JSFeat as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we set the other parameters and call the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you look into the frontalface.js file, you will see that the original window
    size is 20 x 20 pixels, but we set the `minScale` variable in the preceding code
    block assuming that there will be no face that is smaller than 40x40 pixels. The
    `scaleFactor` variable is the factor for the scale. The process stops when the
    window increases to the image size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm returns multiple rectangles for each face. Why? Because when
    the algorithm moves the window, the movement can be too small to make a big difference
    to the image. The JSFeat library provides a method to group those rectangles,
    where the last parameter indicates how many neighbors the result should have in
    order to be grouped with another one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, the algorithm returns confidence for each detection, and if we want
    to print only the best detections, then we can sort the result array and print
    only the most confident ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying this to an image, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Face detection using Haar-like features](img/image00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: On the first image, we painted all rectangles without grouping; see how many
    detections we got for the faces? The different sizes represent different window
    scales. On the second image, we painted the faces after grouping. Already good
    enough, isn't it? And for the last one, we chose the four most confident detections.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the algorithm has many interesting parts and it really helps
    to detect faces on photos. There are various implementations of that algorithm.
    In addition, this method has many extensions. We will discuss one of them in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Brightness binary features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the section name, you may conclude that this method works with a change
    in image brightness, probably with its pixels, and that it compares those intensity
    values to receive some sort of a binary check. You are totally right! In some
    ways, it is like getting FAST corners, but the whole idea is a bit more complex.
    Let's discuss it.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between brightness binary features and Haar features is
    that it uses distinct pixels instead of convolutions. Moreover, it uses different
    image pyramids not different sliding window sizes to compute the required features.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get an idea from the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Brightness binary features](img/image00121.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We kept the resolution of all three images the same for a better view. But you
    need to keep in mind that the images are 24x24, 12x12, and 6x6 pixels. Besides,
    white and black points represent pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the idea is very similar to what you saw while learning the Haar features.
    For example, eyes are much darker than other face particles and because of that,
    we indicate them as dark or black points. The correct instance of an object must
    follow the rules: all white points i and black points j in a window should satisfy
    the expression `I(i) > I(j)`, where `I(position)` is a pixel value of a window
    at this position.'
  prefs: []
  type: TYPE_NORMAL
- en: The number of points may vary, it is chosen during the classifier training process.
    The format for the classifier is different from the format of the Haar features.
    The training process is much more complex, since it needs to get various point
    combinations. In case you want to train your own classifier, you may want to follow
    the CCV library ([http://libccv.org/doc/doc-bbf/](http://libccv.org/doc/doc-bbf/)).
    This is a C library that provides implementations of various Computer Vision algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is harder to find a BBF algorithm implementation, since it is more complicated
    and the training process is much more difficult. Also, the JSFeat library provides
    the algorithm for the same. First, you need to include the classifier file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you need to preallocate some data before the computation starts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, we work with grayscale images; we get one using the standard JSFeat
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the important steps is generating an image pyramid. The input parameters
    are: input image, minimum dimensions of the image in the pyramid, and an interval.
    It sets the number of original scale levels in the pyramid; the larger this number,
    the more pyramid levels you get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then call the function which takes the image pyramid and the cascade as input
    parameters. After all this, we group the resulting rectangles together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result we get with our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Brightness binary features](img/image00122.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For the first part, we took the result without rectangle grouping and for the
    second, with it. As for the Haar features, you may select only the most confident
    results.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding image, we see that the result performed poorly compared to
    the Haar features. It is hard to say why it gives such result. In many cases,
    it highly depends on the implementation or classifier training or maybe just on
    the input image.
  prefs: []
  type: TYPE_NORMAL
- en: We saw two different algorithms, you can select one by your choice. It is probably
    better to stay with the Haar features, since you will find a lot of realizations
    of that algorithm. In contrast, if you want to extend the Computer Vision practical
    boundaries, you may want to tune the BBF implementation or just write your own.
    It is all in your hands!
  prefs: []
  type: TYPE_NORMAL
- en: Tagging people with tracking.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To see more about Haar-like features and its implementation, we will discuss
    tracking.js library. It provides nearly the same functionality as the JSFeat library.
    What is interesting is that it supplies classifiers for other different objects,
    for example, face particles. Eventually, we will see how to make it possible to
    tag friends.
  prefs: []
  type: TYPE_NORMAL
- en: Haar features with tracking.js
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tracking.js provides the functionality to detect not only a face, but various
    face particles too. It is very easy to do that. You need to perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to add object files for what you want to detect:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, initialize the `ObjectTracker` function. We did not discuss this in the
    previous chapter, since it is mostly focused on face detection, not just a regular
    object. Anyway, we initialize it with the names of the objects we want to track:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are also custom functions that you can call. One of them is the `setStepSize`
    function, which sets the step size for a sliding window or how it is called in
    the tracking.js library block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define the postprocessing function. What we need is to plot our result
    on a canvas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need the `plot` function itself:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we mentioned, there is no functionality to plot different objects with distinct
    colors. For now, you can operate with different objects by creating several different
    trackers at once. Eventually, the last thing you need to do is to call the `track`
    function on a canvas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'There are various functions that you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setEdgesDensity`: This is the same as in the JSFeat library, you just set
    a threshold for a sliding window edge''s density. It may significantly improve
    the result; the higher the value, the more edges a window needs to contain to
    be a candidate for an object we want to find.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setInitialScale`: This is the initial scale for a sliding window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setScaleFactor`: This is the scale factor for the sliding window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using these functions, you can tune the algorithm a bit to get a better result.
  prefs: []
  type: TYPE_NORMAL
- en: 'We tested the algorithm by applying three detectors one-by-one. For the face,
    eye, and mouth we used red, blue, and green colors, respectively. Here is the
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Haar features with tracking.js](img/image00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the result for faces is much better than those for face particles.
    This can be due to the bad lighting conditions or poorly trained classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Tagging people in photos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tagging people in photos is a common procedure that you use a lot in social
    networks. If you want to create similar functionality on your website, the JavaScript
    world can offer something for you. Actually, you can do that with any library
    which provides face detection methods, you just need to write some additional
    methods. To simplify the code, we will follow an example from the tracking.js
    library. It is easy to understand and implement:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to place our image to the HTML code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is an array that holds all the names that need to be tagged:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we start from initializing our `ObjectTracker` function with a `face`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The whole magic goes on in a post processing function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's review it a bit. First, we sort all rectangles by x coordinates, it will
    be much easier to plot the result when we know the order of detections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we filter our object array and skip all detections in which width is
    less then must be "50" pixels. That will help us to omit background or noisy detections.
    Moreover, we present a new `tag` function, which will tag all detections on a
    photo. See the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The function creates an `<input>` tag for a name, then takes the first element
    of an array and appends the input element to the `<div>` rectangle.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The last thing we need to do is to call our tracker on an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tagging people in photos](img/image00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have successfully removed the background detection and tagged
    all four people in the correct order.
  prefs: []
  type: TYPE_NORMAL
- en: Head tracking with Camshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Head tracking is another huge topic in the field of Computer Vision. It is very
    useful when you want to create a human computer interface. For example, it is
    usually used in web browser games to move objects or control a 3D interface. There
    are differences between object detection and tracking. First of all, tracking
    works only on videos, since you track an object (not reestimate) a new instance
    in each frame. Consequently, we need to assume that the object we track is the
    same as it was on the previous frame.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking can be done for multiple objects but here we will focus on a single
    object, in our case, it is a head or more precisely—face. There is a wonderful
    library that can help us to track it. It is called headtrackr ([https://github.com/auduno/headtrackr](https://github.com/auduno/headtrackr)).
    In addition to face tracking, it provides a functionality to create an interface
    that helps to control your browser applications using head motion. We will not
    focus on the motion estimation part here, since the chapter is focused on face
    detection and tracking. But do not worry, we will get to that in the next chapter.
    First, we will see how the tracking algorithm works and then we will focus on
    its practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind head tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many object tracking algorithms but most of them are not suitable
    for JavaScript and web browsers due to computational complexity. For Haar features,
    it is very difficult to apply them in a rotation-invariant manner because when
    you do that for several skewed images, the algorithm becomes non-real time. The
    headtrackr library tends to solve that problem. It introduces a framework that
    can help you to track a face. Its main focus is creating a human interface, but
    it provides enough flexibility to use it for other tasks as well.
  prefs: []
  type: TYPE_NORMAL
- en: How does the tracking work? Suppose you have found an object on an initial frame,
    for example, using Haar features or another method. We can work with a video file
    or just a webcam. In that case, the difference between neighboring frames will
    not be that huge. These are our core assumptions, let's move on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will talk here about objects, not just a face. Let''s assume that our object
    is a group of points and we want to find that group on the next frame. Look at
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The idea behind head tracking](img/image00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The circle (window) **C1** is the location of an object on a previous frame.
    The circle **C2** binds the group of points that we want to find. f we get a sum
    of them in the **C2** circle by adding the *x* and *y* coordinates and dividing
    their sums by the number of points in that circle, we will get point **c_n**,
    which is called **centroid**. After you find the centroid, we move the start circle
    center **c_s** to the new center **c_n**. The algorithm continues the iterating
    process by finding a new centroid until it converges in the end center **c_e**.
    You have found the position of our object on a new frame! This algorithm of finding
    centers of point densities is called **Meanshift**.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we get the density for the Meanshift algorithm when we use a face?
    The common approach is to generate a skin map, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The idea behind head tracking](img/image00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the right-hand side image, each pixel represents the probability of this
    pixel being a skin point. Let's call it a density picture. We get a centroid location
    using these intensity points in a window.
  prefs: []
  type: TYPE_NORMAL
- en: Can you see a problem with the Meanshift approach? We are not changing the size
    of a window. What if an object gets closer or further from the camera? We need
    to adapt the size of an object somehow. This issue was solved by the **CAMshift**
    (**Continuously Adaptive Meanshift**) algorithm. The first stage of the algorithm
    is the Meanshift approach. When we find the window with the highest density, the
    Camshift algorithm updates the window size based on the sum of the intensity values
    in that window. The higher the intensity and the more nonzero points in a window,
    the larger the output size will be. After all, the window converges to the required
    object. Moreover, the algorithm provides computation of a possible head rotation
    using the density picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The idea behind head tracking](img/image00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The first one shows the original rectangle (the smaller one) and the final detection
    by Camshift. The right-hand side image shows the result after the rectangle angle
    calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The headtrackr library can initialize the Meanshift algorithm in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: The user manually selects an object on a video and the tracking is done using
    the user input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm can use Haar features to detect the face to be tracked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will see an example with the second approach, when a face is detected automatically
    for the first frame using Haar features, and for the other frames, the library
    uses the Camshift approach.
  prefs: []
  type: TYPE_NORMAL
- en: The head tracking application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is relatively easy to use the headtrackr library. It provides a flexible
    way to create a head tracking application. We will discuss the APIs and opportunities
    it provides:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is to add a headtrackr script. The Haar detector
    is already included there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to define HTML inputs so that we can easily display the content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first one will hold the data required for the library. The video file will
    hold the video. The other two tags are optional, the third one provides a canvas
    to draw the tracking result rectangle on it. The last is used to display the density
    image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you want, you can add a tag for the headtrackr output text, which will display
    various messages during the working process, so you can understand the stage at
    which the tracker is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, we need to get all the necessary data on the JavaScript side:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The canvas should be above the video, so we set its style to be so.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you need to initialize tracker parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are a lot of parameters, we will focus on those which are useful in this
    example. By default, the headtrackr library works with a web camera. If you do
    not have one or your browser does not support it, you can provide a video file
    using the `altVideo` parameter. To calculate the head angle, we use the `calcAngles`
    variable, which is `false` by default. The `ui` parameter sets debugging messages
    for a tag with the `headtrackerMessage` id. For a density image, we need to set
    the `debug` parameter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we init the tracker with a video and canvas inputs. Then, we start the
    tracker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To stop the tracking process, you can use the `stop` function. In that case,
    the library will reinitiate the whole process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To display the result using overlay, we need to add a listener to the `facetrackingEvent`.
    Besides, you can see how we get the rotated version of a rectangle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result includes a video with the overlay over it and the debug information
    on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The head tracking application](img/image00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there is nothing difficult in applying the head tracking with
    that library. To use the library in a proper manner, you just need to know some
    parts of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A face is a really complex object. To detect and track it, you need to use a
    new level of algorithms. Fortunately, the JavaScript libraries provide such an
    opportunity through Haar, Brightness Binary features, Meanshift, and Camshift
    algorithms. All of them have their own area of usage. You can apply these wonderful
    methods in different programs, for example, people tagging. We discussed them
    and provided examples which you can start using right away. In addition to face
    detection, there is a potential to detect other objects such as face particles.
    Of course, the detection quality may vary significantly and you should be careful
    when you use other classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we already touched on the tracking applications a bit and discussed
    how the tracking can help to create a human interface. In the next chapter, we
    will learn how to control your browser with motion and how the object tracking
    can be used in those applications.
  prefs: []
  type: TYPE_NORMAL
