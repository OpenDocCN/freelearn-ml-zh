<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer254">
<h1 class="chapter-number" id="_idParaDest-141"><a id="_idTextAnchor150"/><a id="_idTextAnchor151"/>7</h1>
<h1 id="_idParaDest-142"><a id="_idTextAnchor152"/>SageMaker Deployment Solutions</h1>
<p>After training our <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) model, we can proceed with deploying it to a web API. This API can then be invoked by other applications (for example, a mobile application) to perform a “prediction” or inference. For example, the ML model we trained in <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>, can be deployed to a web API and then be used to predict the likelihood of customers canceling their reservations or not, given a set of inputs. Deploying the ML model to a web API allows the ML model to be accessible to different applications and systems.</p>
<p>A few years ago, ML practitioners had to spend time building a custom backend API to host and deploy a model from scratch. If you were given this requirement, you might have used a Python framework such as <strong class="bold">Flask</strong>, <strong class="bold">Pyramid</strong>, or <strong class="bold">Django</strong> to deploy the ML model. Building a custom API to serve as an inference endpoint can take about a week or so since most of the application logic needs to be coded from scratch. If we were to set up <strong class="bold">A/B testing</strong>, <strong class="bold">auto-scaling</strong>, or <strong class="bold">model monitoring</strong> for the API, then we may have to spend a few additional weeks on top of the initial time spent to set up the base API. ML engineers and software developers generally underestimate the amount of work required to build and maintain ML inference endpoints. Requirements evolve over time and the custom application code becomes harder to manage as the requirements and solutions pile up. At this point, you might ask, “Is there a better and faster way to do this?”. The good news is that we could do all of it in “less than a day” if we were to use <strong class="bold">SageMaker</strong> to deploy our model! Instead of building everything from scratch, SageMaker has already automated most of the work and all we need to do is specify the right configuration parameters. If needed, SageMaker allows us to customize certain components and we can easily replace some of the default automated solutions with our own custom implementations.</p>
<p>One of the misconceptions when using SageMaker is that ML models need to be trained in SageMaker first before they can be deployed in the <strong class="bold">SageMaker hosting services</strong>. It is important to note that “this is not true” since the service was designed and built to support different scenarios, which include deploying a pre-trained model straight away. This means that if we have a pre-trained model trained outside of SageMaker, then we <em class="italic">can</em> proceed with deploying it without having to go through the training steps again. In this chapter, you’ll discover how easy it is to use the <strong class="bold">SageMaker Python SDK</strong> when performing model deployments. In just a few lines of code, we will show you how to deploy our pre-trained model into a variety of inference endpoint types – <strong class="bold">real-time</strong>, <strong class="bold">serverless</strong>, and <strong class="bold">asynchronous inference endpoints</strong>. We will also discuss when it’s best to use each of these inference endpoint types later in this chapter. At the same time, we will discuss the different strategies and best practices when performing model deployments in SageMaker.</p>
<p>That said, we will cover the following topics:</p>
<ul>
<li>Getting started with model deployments in SageMaker </li>
<li>Preparing the pre-trained model artifacts</li>
<li>Preparing the SageMaker script mode prerequisites</li>
<li>Deploying a pre-trained model to a real-time inference endpoint</li>
<li>Deploying a pre-trained model to a serverless inference endpoint</li>
<li>Deploying a pre-trained model to an asynchronous inference endpoint</li>
<li>Cleaning up</li>
<li>Deployment strategies and best practices</li>
</ul>
<p>We will wrap up with a quick discussion of the other alternatives and options when deploying ML models. After you have completed the hands-on solutions in this chapter, you will be more confident in deploying different types of ML models in SageMaker. Once you reach a certain level of familiarity and mastery using the SageMaker Python SDK, you should be able to set up an ML inference endpoint in just a few hours, or maybe even in just a few minutes!</p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor153"/>Technical requirements</h1>
<p>Before we start, it is important to have the following ready:</p>
<ul>
<li>A web browser (preferably Chrome or Firefox)</li>
<li>Access to the AWS account and <strong class="bold">SageMaker Studio</strong> domain used in the first chapter of the book</li>
</ul>
<p>The Jupyter notebooks, source code, and other files used for each chapter are available in this repository: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS">https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS</a>.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">It is recommended to use an IAM user with limited permissions instead of the root account when running the examples in this book. We will discuss this along with other security best practices in detail in <a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em>. If you are just starting with using AWS, you may proceed with using the root account in the meantime.</p>
<h1 id="_idParaDest-144"><a id="_idTextAnchor154"/>Getting started with model deployments in SageMaker</h1>
<p>In <a href="B18638_06.xhtml#_idTextAnchor132"><em class="italic">Chapter 6</em></a>, <em class="italic">SageMaker Training and Debugging Solutions</em>, we trained and deployed<a id="_idIndexMarker733"/> an image classification model using the <strong class="bold">SageMaker Python SDK</strong>. We made use of a built-in algorithm while working<a id="_idIndexMarker734"/> on the hands-on solutions in that chapter. When using<a id="_idIndexMarker735"/> a built-in algorithm, we just need to prepare the training dataset along with specifying a few configuration parameters and we are good to go! Note that if we want to train a custom model using our favorite ML framework (such as TensorFlow and PyTorch), then we can prepare our custom<a id="_idIndexMarker736"/> scripts and make them work in SageMaker using <strong class="bold">script mode</strong>. This gives us a bit more flexibility since we can tweak how SageMaker interfaces with our model through a custom script that allows us to use different libraries and frameworks when training our model. If we want the highest level of flexibility for the environment where the training scripts will run, then we can opt to use our own custom container image instead. SageMaker has its own set of pre-built container images used when training models. However, we may decide to build and use our own if needed. </p>
<div>
<div class="IMG---Figure" id="_idContainer232">
<img alt="Figure 7.1 – The different options when training and deployment models " height="647" src="image/B18638_07_001.jpg" width="1148"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – The different options when training and deployment models</p>
<p>As we can see in <em class="italic">Figure 7.1</em>, the options available when training ML<a id="_idIndexMarker737"/> models in SageMaker are also available when deploying models<a id="_idIndexMarker738"/> using the SageMaker hosting services. Here, we tag each approach or option with an arbitrary label (for example, <strong class="bold">T1</strong> or <strong class="bold">T2</strong>) to help us discuss these options in more detail. When performing model deployments in SageMaker, we can choose to deploy a model using the container of a built-in algorithm (<strong class="bold">D1</strong>). We also have the option to deploy our deep<a id="_idIndexMarker739"/> learning models using <strong class="bold">script mode</strong> (<strong class="bold">D2</strong>). With this option, we need to prepare custom scripts that will run inside the pre-built <strong class="bold">Deep Learning Containers</strong>. We also have the option to provide and use our own custom container images for the environment where our ML model will be deployed (<strong class="bold">D3</strong>). </p>
<p class="callout-heading">Important Note</p>
<p class="callout">Choosing which combination of options to use generally depends on the level of customization required (in the form of custom scripts and container images) when performing ML experiments and deployments. When getting started with SageMaker, it is recommended to use the SageMaker built-in algorithms to have a better feel for how things work when training a model (<strong class="bold">T1</strong>) and when deploying a model (<strong class="bold">D1</strong>). If we need to use frameworks such as TensorFlow, PyTorch, or MXNet on top of the managed infrastructure of AWS with SageMaker, we will need to prepare a set of custom scripts to be used during training (<strong class="bold">T2</strong>) and deployment (<strong class="bold">D2</strong>). Finally, when we need a much greater level of flexibility, we can prepare custom container images and use these when training a model (<strong class="bold">T3</strong>) and deploying a model (<strong class="bold">D3</strong>). </p>
<p class="callout">It is important to note that we can combine and use different options when training and deploying models. For example, we can train an ML model using script mode (<strong class="bold">T2</strong>) and use a custom container image during model deployment (<strong class="bold">D3</strong>). Another example involves training a model outside of SageMaker (<strong class="bold">T4</strong>) and using the pre-built inference container image for a built-in algorithm during model deployment (<strong class="bold">D1</strong>).</p>
<p>Now, let’s talk about how<a id="_idIndexMarker740"/> model deployment works using<a id="_idIndexMarker741"/> the SageMaker hosting services:</p>
<div>
<div class="IMG---Figure" id="_idContainer233">
<img alt="Figure 7.2 – Deploying a model using the SageMaker hosting services " height="559" src="image/B18638_07_002.jpg" width="1359"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Deploying a model using the SageMaker hosting services</p>
<p>In <em class="italic">Figure 7.2</em>, we have a high-level diagram of how model deployment works using the SageMaker hosting services. Assuming that a <strong class="source-inline">model.tar.gz</strong> file (containing the ML model artifacts and output files) has been uploaded to an S3 bucket after the training step, the <strong class="source-inline">model.tar.gz</strong> file is downloaded from the S3 bucket into an ML compute instance that serves as the dedicated server for the ML inference endpoint. Inside this ML compute instance, the model artifacts stored inside the <strong class="source-inline">model.tar.gz</strong> file are loaded inside a running container<a id="_idIndexMarker742"/> containing the inference code, which can load<a id="_idIndexMarker743"/> the model and use it for inference for incoming requests. As mentioned earlier, the inference code and the container image used for inference can either be provided by AWS (built-in or pre-built) or provided by ML engineers using SageMaker (custom). </p>
<p>Let’s show a few sample blocks of code to help us explain these concepts. Our first example involves training<a id="_idIndexMarker744"/> and deploying a model using the built-in <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) algorithm – an algorithm that can be used in use cases such as dimensionality reduction and data compression:</p>
<pre class="source-code">from sagemaker import PCA
<strong class="bold"># [1] TRAINING</strong>
estimator = PCA(
    role=role,
    instance_count=1,
    instance_type='ml.c4.xlarge',
    num_components=2,
    sagemaker_session=session
)
estimator.fit(...)
<strong class="bold"># [2] DEPLOYMENT</strong>
predictor = estimator.<strong class="bold">deploy</strong>(
    initial_instance_count=1,
    instance_type='ml.t2.medium'
)</pre>
<p>Here, SageMaker makes use of a pre-built container image when training and deploying the PCA model. This container image has been prepared by the AWS team so that we won’t have to worry about implementing this ourselves when using the built-in algorithms. Note that we can also skip<a id="_idIndexMarker745"/> the training step and proceed<a id="_idIndexMarker746"/> with the deployment step in SageMaker as long as we have a pre-trained model available that is compatible with the pre-built container available for the built-in algorithm.</p>
<p>Now, let’s quickly take a look at an example of how to use custom scripts when deploying models in SageMaker:</p>
<pre class="source-code">from sagemaker.pytorch.model import PyTorchModel
<strong class="bold"># [1] HERE, WE DON'T SHOW THE TRAINING STEP</strong>
model_data = estimator.model_data
<strong class="bold"># [2] DEPLOYMENT</strong>
model = PyTorchModel(
    model_data=model_data, 
    role=role, 
    source_dir="scripts",
    entry_point='<strong class="bold">inference.py</strong>', 
    framework_version='1.6.0',
    py_version="py3"
)
predictor = model.<strong class="bold">deploy</strong>(
    instance_type='ml.m5.xlarge', 
    initial_instance_count=1
)</pre>
<p>In this example, SageMaker makes use of a pre-built Deep Learning Container image to deploy PyTorch models. As discussed in <a href="B18638_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Deep Learning Containers</em>, the relevant packages and dependencies are already installed inside these container images. During the deployment step, the container runs the custom code specified in a custom <strong class="source-inline">inference.py</strong> script provided during the initialization of the <strong class="source-inline">PyTorchModel</strong> object. The custom<a id="_idIndexMarker747"/> code would then load the model<a id="_idIndexMarker748"/> and use it when processing the requests sent to the SageMaker inference endpoint.</p>
<p class="callout-heading">Note</p>
<p class="callout">In the example provided, we initialized a <strong class="source-inline">PyTorchModel</strong> object and used the <strong class="source-inline">deploy()</strong> method to deploy the model to a real-time inference endpoint. Inside the inference endpoint, a container using the PyTorch inference container image will run the inference code that loads the model and uses it for inference. Note that we also have the corresponding <strong class="source-inline">Model</strong> classes for the other libraries and frameworks such as <strong class="source-inline">TensorFlowModel</strong>, <strong class="source-inline">SKLearnModel</strong>, and <strong class="source-inline">MXNetModel</strong>. Once the <strong class="source-inline">deploy()</strong> method is called, the appropriate inference container (with the relevant installed packages and dependencies) would be used inside the inference endpoint.</p>
<p>If we want to specify and use our own custom container image, we can use the following block of code:</p>
<pre class="source-code">from sagemaker.model import Model
<strong class="bold"># [1] HERE, WE DON'T SHOW THE TRAINING STEP</strong>
model_data = estimator.model_data
<strong class="bold"># [2] DEPLOYMENT</strong>
image_uri = "<strong class="bold">&lt;INSERT ECR URI OF CUSTOM CONTAINER IMAGE&gt;</strong>"
model = Model(
    image_uri=image_uri, 
    model_data=model_data,
    role=role,
    sagemaker_session=session
)
predictor = model.<strong class="bold">deploy</strong>(
    initial_instance_count=1, 
    instance_type='ml.m5.xlarge'
)</pre>
<p>In this example, SageMaker makes use of the custom<a id="_idIndexMarker749"/> container image stored<a id="_idIndexMarker750"/> in the location specified in the <strong class="source-inline">image_uri</strong> variable. Here, the assumption is that we have already prepared and tested the custom container image, and we have pushed this container image to an <strong class="bold">Amazon Elastic Container Registry</strong> repository before we have performed<a id="_idIndexMarker751"/> the model deployment step.</p>
<p class="callout-heading">Note</p>
<p class="callout">It takes a bit of trial and error when preparing custom scripts and custom container images (similar to how we prepared and tested our custom container image in <a href="B18638_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Deep Learning Containers</em>). If you are using a Notebook instance, you can use the SageMaker <strong class="bold">local mode</strong>, which gives us a way to test the custom<a id="_idIndexMarker752"/> scripts and custom container images in the local environment before running these in the managed ML instances.</p>
<p>The code samples shown in this section assumed that we would be deploying our ML model in a real-time inference endpoint. However, there are different options<a id="_idIndexMarker753"/> to choose from when deploying ML models in SageMaker:</p>
<ul>
<li>The first option<a id="_idIndexMarker754"/> involves deploying and hosting our model in a <strong class="bold">real-time inference endpoint</strong>. </li>
<li>The second option<a id="_idIndexMarker755"/> involves tweaking the configuration a bit when using the SageMaker Python SDK to deploy our model in a <strong class="bold">serverless inference endpoint</strong>. </li>
<li>The third option<a id="_idIndexMarker756"/> is to host our model in an <strong class="bold">asynchronous inference endpoint</strong>. </li>
</ul>
<p>We will cover these options in the hands-on portion of this chapter and we will discuss the relevant use cases and scenarios for each of these as well. </p>
<p class="callout-heading">Note</p>
<p class="callout">It is important to note that it is also possible to perform inference with a model without having<a id="_idIndexMarker757"/> to set up an inference endpoint. This involves using <strong class="bold">Batch Transform</strong> where a model is loaded and used to process multiple input payload values and perform predictions all in one go. To see a working example<a id="_idIndexMarker758"/> of Batch Transform, feel free to check out the following link: <a href="https://bit.ly/3A9wrVy">https://bit.ly/3A9wrVy</a>.</p>
<p>Now that we have a better idea<a id="_idIndexMarker759"/> of how SageMaker model deployment<a id="_idIndexMarker760"/> works, let’s proceed with the hands-on portion of this chapter. In the next section, we will prepare the <strong class="source-inline">model.tar.gz</strong> file containing the ML model artifacts that we will use for the model deployment solutions in this chapter.</p>
<h1 id="_idParaDest-145"><a id="_idTextAnchor155"/>Preparing the pre-trained model artifacts </h1>
<p>In <a href="B18638_06.xhtml#_idTextAnchor132"><em class="italic">Chapter 6</em></a>, <em class="italic">SageMaker Training and Debugging Solutions</em>, we created<a id="_idIndexMarker761"/> a new folder named <strong class="source-inline">CH06</strong>, along with a new Notebook using the <strong class="source-inline">Data Science</strong> image inside the created folder. In this section, we will create a new folder (named <strong class="source-inline">CH07</strong>), along with a new Notebook inside the created folder. Instead of the <strong class="source-inline">Data Science</strong> image, we will use the <strong class="source-inline">PyTorch 1.10 Python 3.8 CPU Optimized</strong> image as the image used in the Notebook since we will download the model<a id="_idIndexMarker762"/> artifacts of a pre-trained <strong class="bold">PyTorch</strong> model using the <strong class="bold">Hugging Face</strong> <strong class="source-inline">transformers</strong> library. Once the Notebook<a id="_idIndexMarker763"/> is ready, we will use the Hugging Face <strong class="source-inline">transformers</strong> library to download a pre-trained model that can be used for sentiment analysis. Finally, we will zip the model artifacts into a <strong class="source-inline">model.tar.gz</strong> file and upload it to an S3 bucket.</p>
<p class="callout-heading">Note</p>
<p class="callout">Make sure that you have completed the hands-on solutions in the <em class="italic">Getting started with SageMaker and SageMaker Studio</em> section of <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>, before proceeding. It is important to note that the hands-on section in this chapter is not a continuation of what we completed in <a href="B18638_06.xhtml#_idTextAnchor132"><em class="italic">Chapter 6</em></a>, <em class="italic">SageMaker Training and Debugging Solutions</em>. As long as we have SageMaker Studio set up, we should be good to go.</p>
<p>In the next set <a id="_idIndexMarker764"/>of steps, we will prepare the <strong class="source-inline">model.tar.gz</strong> file containing the model artifacts and then upload it to an S3 bucket:</p>
<ol>
<li>Navigate to <strong class="bold">SageMaker Studio</strong> by typing <strong class="source-inline">sagemaker studio</strong> into the search bar<a id="_idIndexMarker765"/> of the AWS Management Console and then selecting <strong class="bold">SageMaker Studio</strong> from the list of results under <strong class="bold">Features</strong>. We click <strong class="bold">Studio</strong> under <strong class="bold">SageMaker Domain</strong> in the sidebar and then we select <strong class="bold">Studio</strong> from the list of drop-down options under the <strong class="bold">Launch app</strong> drop-down menu (on the <strong class="bold">Users</strong> pane). Wait for a minute or two for the SageMaker Studio interface to load.</li>
</ol>
<p class="callout-heading">Important Note</p>
<p class="callout">This chapter assumes that we are using the <strong class="bold">Oregon</strong> (<strong class="source-inline">us-west-2</strong>) region when using services to manage and create different types of resources. You may use a different region but make sure to perform any adjustments needed in case certain resources need to be transferred to the region of choice.</p>
<ol>
<li value="2">Right-click on the empty space in the <strong class="bold">File Browser</strong> sidebar pane to open the context menu containing the <strong class="bold">New Folder</strong> option (along with the other options available). Select <strong class="bold">New Folder</strong> to create a new folder inside the current directory. Name the folder <strong class="source-inline">CH07</strong>. Finally, navigate to the <strong class="source-inline">CH07</strong> directory by double-clicking the folder name in the sidebar.</li>
<li>Create a new Notebook by clicking the <strong class="bold">File</strong> menu and choosing <strong class="bold">Notebook</strong> from the list of options under the <strong class="bold">New</strong> submenu. In the <strong class="bold">Set up notebook environment</strong> window, specify the following configuration values:<ul><li><strong class="bold">Image</strong>: <strong class="source-inline">PyTorch 1.10 Python 3.8 CPU Optimized</strong></li>
<li><strong class="bold">Kernel</strong>: <strong class="source-inline">Python 3</strong></li>
<li><strong class="bold">Start-up script</strong>: <strong class="source-inline">No script</strong> </li>
</ul></li>
<li>Click the <strong class="bold">Select</strong> button afterward. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Wait for the kernel to start. This step may take around 3 to 5 minutes while an ML instance is being provisioned to run the Jupyter notebook cells.</p>
<ol>
<li value="5">Rename the notebook from <strong class="source-inline">Untitled.ipynb</strong> to <strong class="source-inline">01 - Prepare model.tar.gz file.ipynb</strong>. </li>
<li>Now that our notebook<a id="_idIndexMarker766"/> is ready, we can proceed with generating the pre-trained model artifacts and storing these inside a <strong class="source-inline">model.tar.gz</strong> file. In the first cell of the Jupyter Notebook, let’s run the following, which will install the Hugging Face <strong class="source-inline">transformers</strong> library:<pre class="source-code">!pip3 install <strong class="bold">transformers</strong>==4.4.2</pre></li>
<li>Install <strong class="source-inline">ipywidgets</strong> using <strong class="source-inline">pip</strong> as well:<pre class="source-code">!pip3 install ipywidgets --quiet</pre></li>
<li>Next, let’s run the following block of code to restart the kernel:<pre class="source-code">import IPython</pre><pre class="source-code">kernel = IPython.Application.instance().kernel</pre><pre class="source-code">kernel.<strong class="bold">do_shutdown</strong>(True)</pre></li>
</ol>
<p class="list-inset">This should yield an output value similar to <strong class="source-inline">{'status': 'ok', 'restart': True}</strong> and restart the kernel accordingly to ensure that we will not encounter issues using the packages we just installed.</p>
<ol>
<li value="9">Let’s download<a id="_idIndexMarker767"/> a pre-trained model using the <strong class="bold">Hugging Face</strong> <strong class="source-inline">transformers</strong> library. We’ll download a model that can be used for sentiment analysis and to classify whether a statement is <em class="italic">POSITIVE</em> or <em class="italic">NEGATIVE</em>. Run the following block of code to download the artifacts of the pre-trained <strong class="source-inline">distilbert</strong> model into the current directory:<pre class="source-code">from transformers import AutoModelForSequenceClassification as AMSC</pre><pre class="source-code">pretrained = "<strong class="bold">distilbert-base-uncased-finetuned-sst-2-english</strong>"</pre><pre class="source-code">model = AMSC.from_pretrained(pretrained)</pre><pre class="source-code">model.<strong class="bold">save_pretrained</strong>(save_directory=".")</pre></li>
</ol>
<p class="list-inset">This should generate<a id="_idIndexMarker768"/> two files in the same directory as the <strong class="source-inline">.ipynb</strong> notebook file: </p>
<ul>
<li><strong class="source-inline">config.json</strong></li>
<li> <strong class="source-inline">pytorch_model.bin</strong></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout"><em class="italic">How should this work?</em> If we have an “<strong class="source-inline">I love reading the book MLE on AWS!</strong>” statement, for example, the trained model should classify it as a <em class="italic">POSITIVE</em> statement. If we have a “<strong class="source-inline">This is the worst spaghetti I've had</strong>” statement, the trained model should then classify it as a <em class="italic">NEGATIVE</em> statement.</p>
<ol>
<li value="10">Prepare the <strong class="source-inline">model.tar.gz</strong> (compressed archive) file containing the model artifact files generated in the previous step using the following block of code:<pre class="source-code">import tarfile</pre><pre class="source-code">tar = tarfile.open("<strong class="bold">model.tar.gz</strong>", "w:gz")</pre><pre class="source-code">tar.add("<strong class="bold">pytorch_model.bin</strong>")</pre><pre class="source-code">tar.add("<strong class="bold">config.json</strong>")</pre><pre class="source-code">tar.close()</pre></li>
<li>Use the <strong class="source-inline">rm</strong> command to clean up the model files by deleting the model artifacts generated in the previous steps:<pre class="source-code">%%bash</pre><pre class="source-code">rm <strong class="bold">pytorch_model.bin</strong></pre><pre class="source-code">rm <strong class="bold">config.json</strong></pre></li>
<li>Specify the S3 bucket <a id="_idIndexMarker769"/>name and prefix. Make sure to replace the value of <strong class="source-inline">&lt;INSERT S3 BUCKET NAME HERE&gt;</strong> with a unique S3 bucket name before running the following block of code:<pre class="source-code">s3_bucket = "<strong class="bold">&lt;INSERT S3 BUCKET NAME HERE&gt;</strong>"</pre><pre class="source-code">prefix = "chapter07"</pre></li>
</ol>
<p class="list-inset">Make sure to specify a bucket name for an S3 bucket that does not exist yet. In the case that you want to reuse one of the buckets created in the previous chapters, you may do so, but make sure to use an S3 bucket in the same region where SageMaker Studio is set up and configured.</p>
<ol>
<li value="13">Create a new S3 bucket using the <strong class="source-inline">aws s3 mb</strong> command:<pre class="source-code">!aws s3 mb s3://{s3_bucket}</pre></li>
</ol>
<p class="list-inset">You can skip this step if you are planning to reuse one of the existing S3 buckets created in the previous chapters.</p>
<ol>
<li value="14">Prepare the S3 path where we will upload the model files:<pre class="source-code"><strong class="bold">model_data</strong> = "s3://{}/{}/model/<strong class="bold">model.tar.gz</strong>".format(</pre><pre class="source-code">    s3_bucket, prefix</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Note that at this point, a <strong class="source-inline">model.tar.gz</strong> file does not exist in the specified S3 path yet. Here, we are simply preparing the S3 location (string) where the <strong class="source-inline">model.tar.gz</strong> file will be uploaded.</p>
<ol>
<li value="15">Now, let’s use the <strong class="source-inline">aws s3 cp</strong> command to copy and upload the <strong class="source-inline">model.tar.gz</strong> file to the S3 bucket:<pre class="source-code">!<strong class="bold">aws s3 cp</strong> model.tar.gz {<strong class="bold">model_data</strong>}</pre></li>
<li>Use the <strong class="source-inline">%store</strong> magic to store the variable values for <strong class="source-inline">model_data</strong>, <strong class="source-inline">s3_bucket</strong>, and <strong class="source-inline">prefix</strong>:<pre class="source-code">%store <strong class="bold">model_data</strong></pre><pre class="source-code">%store <strong class="bold">s3_bucket</strong></pre><pre class="source-code">%store <strong class="bold">prefix</strong></pre></li>
</ol>
<p class="list-inset">This should<a id="_idIndexMarker770"/> allow us to use these variable values for one or more of the succeeding sections in this chapter, similar to what we have here in <em class="italic">Figure 7.3</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer234">
<img alt="Figure 7.3 – The %store magic " height="631" src="image/B18638_07_003.jpg" width="1192"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – The %store magic</p>
<p>Make sure not to restart the kernel<a id="_idIndexMarker771"/> or else we will lose the variable values saved using the <strong class="source-inline">%store</strong> magic.</p>
<h1 id="_idParaDest-146"><a id="_idTextAnchor156"/>Preparing the SageMaker script mode prerequisites</h1>
<p>In this chapter, we will be preparing<a id="_idIndexMarker772"/> a custom script to use a pre-trained model for predictions. Before<a id="_idIndexMarker773"/> we can proceed with using the <strong class="bold">SageMaker Python SDK</strong> to deploy our pre-trained model to an inference endpoint, we’ll need to ensure that all the script mode prerequisites are ready.</p>
<div>
<div class="IMG---Figure" id="_idContainer235">
<img alt="Figure 7.4 – The desired file and folder structure " height="764" src="image/B18638_07_004.jpg" width="1284"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – The desired file and folder structure</p>
<p>In <em class="italic">Figure 7.4</em>, we can see that there are three prerequisites we’ll need to prepare:</p>
<ul>
<li><strong class="source-inline">inference.py</strong></li>
<li><strong class="source-inline">requirements.txt</strong></li>
<li><strong class="source-inline">setup.py</strong></li>
</ul>
<p>We will store these prerequisites inside the <strong class="source-inline">scripts</strong> directory. We’ll discuss these prerequisites in detail<a id="_idIndexMarker774"/> in the succeeding pages of this chapter. Without further ado, let’s start by preparing the <strong class="source-inline">inference.py</strong> script file!</p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor157"/>Preparing the inference.py file</h2>
<p>In this section, we will prepare<a id="_idIndexMarker775"/> a custom Python script that will be used by SageMaker when processing inference requests. Here, we can influence how the input request is deserialized, how a custom model is loaded, how the prediction step is performed, and how the output prediction is serialized and returned as a response. To do all of this, we will need to override the following inference handler functions inside our script file: <strong class="source-inline">model_fn()</strong>, <strong class="source-inline">input_fn()</strong>, <strong class="source-inline">predict_fn()</strong>, and <strong class="source-inline">output_fn()</strong>. We will discuss how these functions work shortly.</p>
<p>In the next set of steps, we will prepare our custom Python script and override the default implementations of the inference handler functions:</p>
<ol>
<li value="1">Right-click on the empty space in the <strong class="bold">File Browser</strong> sidebar pane to open a context menu similar to that shown in <em class="italic">Figure 7.5</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer236">
<img alt="Figure 7.5 – Creating a new folder inside the CH07 directory " height="290" src="image/B18638_07_005.jpg" width="722"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Creating a new folder inside the CH07 directory</p>
<p class="list-inset">Select <strong class="bold">New Folder</strong> from the list of options available in the context menu, as highlighted in <em class="italic">Figure 7.5</em>. Note that we can also press the envelope button (with a plus) just beside the <strong class="bold">+</strong> button to create a new folder as well.</p>
<ol>
<li value="2">Name the new folder <strong class="source-inline">scripts</strong>. </li>
<li>Next, double-click the <strong class="source-inline">scripts</strong> folder to navigate to the directory.</li>
<li>Create a new text file by clicking the <strong class="bold">File</strong> menu and choosing <strong class="bold">Text File</strong> from the list of options under the <strong class="bold">New</strong> submenu.</li>
<li>Right-click<a id="_idIndexMarker776"/> on the <strong class="bold">untitled.txt</strong> file and select <strong class="bold">Rename</strong> from the list of options in the context menu. Rename the file <strong class="source-inline">inference.py</strong>.</li>
<li>Click on the <strong class="bold">Editor</strong> pane, as highlighted in <em class="italic">Figure 7.6</em>, to edit the content of our <strong class="source-inline">inference.py</strong> script:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer237">
<img alt="Figure 7.6 – Getting ready to add code to the inference.py file in the Editor pane " height="393" src="image/B18638_07_006.jpg" width="975"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Getting ready to add code to the inference.py file in the Editor pane</p>
<p class="list-inset">We will add the succeeding blocks of code into the <strong class="source-inline">inference.py</strong> file. Make sure that there’s an extra blank line after each block of code.</p>
<ol>
<li value="7">In the <strong class="bold">Editor</strong> pane, import the prerequisites by adding the following block of code to the <strong class="source-inline">inference.py</strong> file:<pre class="source-code">import json</pre><pre class="source-code">from transformers import AutoModelForSequenceClassification as AMSC</pre><pre class="source-code">from transformers import Trainer</pre><pre class="source-code">from transformers import TrainingArguments</pre><pre class="source-code">from torch.nn import functional as F</pre><pre class="source-code">from transformers import AutoTokenizer</pre><pre class="source-code">from time import sleep</pre></li>
<li>Specify the tokenizer:<pre class="source-code">TOKENIZER = "<strong class="bold">distilbert-base-uncased-finetuned-sst-2-english</strong>"</pre></li>
</ol>
<p class="list-inset">Here, we specify<a id="_idIndexMarker777"/> the appropriate tokenizer for the model we will use to perform the prediction in a later step.</p>
<p class="callout-heading">Note</p>
<p class="callout">What’s a tokenizer? A <strong class="bold">tokenizer</strong> processes, splits, and converts<a id="_idIndexMarker778"/> an input payload into the corresponding set of token strings. For example, if our original string is <strong class="source-inline">"I am hungry"</strong>, then a tokenizer will split it into the tokens <strong class="source-inline">"I"</strong>, <strong class="source-inline">"am"</strong>, and <strong class="source-inline">"</strong><strong class="source-inline">hungry"</strong>. Note that this is a simplified example and there’s more to tokenization than what can be explained in a few sentences. For more<a id="_idIndexMarker779"/> details, feel free to check out the following link: <a href="https://huggingface.co/docs/transformers/main_classes/tokenizer">https://huggingface.co/docs/transformers/main_classes/tokenizer</a>.</p>
<ol>
<li value="9">Define the <strong class="source-inline">model_fn()</strong> function:<pre class="source-code">def <strong class="bold">model_fn</strong>(model_dir):</pre><pre class="source-code">    model = AMSC.<strong class="bold">from_pretrained</strong>(model_dir)</pre><pre class="source-code">    </pre><pre class="source-code">    return model</pre></li>
</ol>
<p class="list-inset">Here, we defined a model function that returns a model object used to perform predictions and process inference requests. Since we are planning to load and use a pre-trained model, we used the <strong class="source-inline">from_pretrained()</strong> method of <strong class="source-inline">AutoModelForSequenceClassification</strong> from the <strong class="source-inline">transformers</strong> library to load the model artifacts in the specified model directory. The <strong class="source-inline">from_pretrained()</strong> method then returns a model object that can be used during the prediction step.</p>
<ol>
<li value="10">Now, let’s define the <strong class="source-inline">humanize_prediction()</strong> function:<pre class="source-code">def <strong class="bold">humanize_prediction</strong>(output):</pre><pre class="source-code">    class_a, class_b = F.softmax(</pre><pre class="source-code">        output[0][0], </pre><pre class="source-code">        dim = 0</pre><pre class="source-code">    ).tolist()</pre><pre class="source-code">    </pre><pre class="source-code">    prediction = "-"</pre><pre class="source-code">    </pre><pre class="source-code">    if class_a &gt; class_b:</pre><pre class="source-code">        prediction = "<strong class="bold">NEGATIVE</strong>"</pre><pre class="source-code">    else:</pre><pre class="source-code">        prediction = "<strong class="bold">POSITIVE</strong>"</pre><pre class="source-code">        </pre><pre class="source-code">    return prediction</pre></li>
</ol>
<p class="list-inset">The <strong class="source-inline">humanize_prediction()</strong> function simply<a id="_idIndexMarker780"/> accepts the raw output produced by the model after processing the input payload during the prediction step. It returns either a <strong class="source-inline">"POSITIVE"</strong> or a <strong class="source-inline">"NEGATIVE"</strong> prediction to the calling function. We will define this <em class="italic">calling function</em> in the next step.</p>
<ol>
<li value="11">Next, let’s define <strong class="source-inline">predict_fn()</strong> using the following block of code:<pre class="source-code">def <strong class="bold">predict_fn</strong>(input_data, model):</pre><pre class="source-code">    # sleep(30)</pre><pre class="source-code">    </pre><pre class="source-code">    sentence = input_data['text']</pre><pre class="source-code">    </pre><pre class="source-code">    tokenizer = AutoTokenizer.from_pretrained(</pre><pre class="source-code">        TOKENIZER</pre><pre class="source-code">    )</pre><pre class="source-code">    </pre><pre class="source-code">    batch = tokenizer(</pre><pre class="source-code">        [sentence],</pre><pre class="source-code">        padding=True,</pre><pre class="source-code">        truncation=True,</pre><pre class="source-code">        max_length=512,</pre><pre class="source-code">        return_tensors="pt"</pre><pre class="source-code">    )</pre><pre class="source-code">    output = model(**batch)</pre><pre class="source-code">    prediction = <strong class="bold">humanize_prediction</strong>(output)</pre><pre class="source-code">    </pre><pre class="source-code">    return prediction</pre></li>
</ol>
<p class="list-inset">The <strong class="source-inline">predict_fn()</strong> function accepts<a id="_idIndexMarker781"/> the deserialized input request data and the loaded model as input. It then uses these two parameter values to produce a prediction. How? Since the loaded model is available as the second parameter, we simply use it to perform predictions. The input payload to this prediction step would be the deserialized request data, which is available as the first parameter to the <strong class="source-inline">predict_fn()</strong> function. Before the output is returned, we make use of the <strong class="source-inline">humanize_prediction()</strong> function to convert the raw output to either <strong class="source-inline">"POSITIVE"</strong> or <strong class="source-inline">"NEGATIVE"</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout">Why do we have a commented line containing <strong class="source-inline">sleep(30)</strong>? Later in the <em class="italic">Deploying a pre-trained model to an asynchronous inference endpoint section</em>, we will emulate an inference endpoint with a relatively long processing time using an artificial 30-second delay. For now, we’ll keep this line commented out and we will undo this later in that section.</p>
<ol>
<li value="12">Let’s also define the <strong class="source-inline">input_fn()</strong> function that is used to convert the serialized input request<a id="_idIndexMarker782"/> data into its deserialized form. This deserialized form will be used for prediction at a later stage:<pre class="source-code">def <strong class="bold">input_fn</strong>(serialized_input_data, </pre><pre class="source-code">             content_type='application/json'):</pre><pre class="source-code">    if content_type == 'application/json':</pre><pre class="source-code">        input_data = json.loads(serialized_input_data)</pre><pre class="source-code">        </pre><pre class="source-code">        return input_data</pre><pre class="source-code">    else:</pre><pre class="source-code">        raise <strong class="bold">Exception</strong>('Unsupported Content Type')</pre></li>
</ol>
<p class="list-inset">In the <strong class="source-inline">input_fn()</strong> function, we also ensure that the specified content type is within the list of support content types we define by raising <strong class="source-inline">Exception</strong> for unsupported content types.</p>
<ol>
<li value="13">Finally, let’s define <strong class="source-inline">output_fn()</strong>:<pre class="source-code">def <strong class="bold">output_fn</strong>(prediction_output, </pre><pre class="source-code">              accept='application/json'):</pre><pre class="source-code">    if accept == 'application/json':</pre><pre class="source-code">        return json.dumps(prediction_output), accept</pre><pre class="source-code">    </pre><pre class="source-code">    raise Exception('Unsupported Content Type')</pre></li>
</ol>
<p class="list-inset">The purpose of <strong class="source-inline">output_fn()</strong> is to serialize the prediction result into the specified content type. Here, we also ensure that the specified content type is within the list of support content types we define by raising <strong class="source-inline">Exception</strong> for unsupported content types.</p>
<p class="callout-heading">Note</p>
<p class="callout">We can think of <em class="italic">serialization</em> and <em class="italic">deserialization</em> as data transformation steps<a id="_idIndexMarker783"/> that convert data<a id="_idIndexMarker784"/> from one form to another. For example, the input request data may be passed as a valid JSON <em class="italic">string</em> to the inference endpoint. This input request data passes through the <strong class="source-inline">input_fn()</strong> function, which converts it into a <em class="italic">JSON</em> or a <em class="italic">dictionary</em>. This deserialized value is then passed as a payload to the <strong class="source-inline">predict_fn()</strong> function. After this, the <strong class="source-inline">predict_fn()</strong> function returns a prediction as the result. This result is then converted to the specified content type using the <strong class="source-inline">output_fn()</strong> function. </p>
<ol>
<li value="14">Save the changes<a id="_idIndexMarker785"/> by pressing <em class="italic">CTRL</em> + <em class="italic">S</em>. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you are using a Mac, use <em class="italic">CMD</em> + <em class="italic">S</em> instead. Alternatively, you can just click <strong class="bold">Save Python File</strong> under the list of options under the <strong class="bold">File</strong> menu.</p>
<p>At this point, you might be wondering how all of these fit together. To help us understand how the inference handler functions interact with the data and with each other, let’s quickly check out the diagram shown in <em class="italic">Figure 7.7</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer238">
<img alt="Figure 7.7 – The inference handler functions " height="665" src="image/B18638_07_007.jpg" width="1084"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – The inference handler functions</p>
<p>In <em class="italic">Figure 7.7</em>, we can see that the <strong class="source-inline">model_fn()</strong> function is used to load the ML model object. This model object<a id="_idIndexMarker786"/> will be used by the <strong class="source-inline">predict_fn()</strong> function to perform predictions once a request comes in. When a request comes in, the <strong class="source-inline">input_fn()</strong> function processes the serialized request data and converts it into its deserialized form. This deserialized request data is then passed to the <strong class="source-inline">predict_fn()</strong> function, which then uses the loaded ML model to perform a prediction using the request data as a payload. The <strong class="source-inline">predict_fn()</strong> function then returns the output prediction, which is serialized by the <strong class="source-inline">output_fn()</strong> function. </p>
<p class="callout-heading">Note</p>
<p class="callout">For more information<a id="_idIndexMarker787"/> about this topic, feel free to check out the following link: <a href="https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml#serve-a-pytorch-model">https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.xhtml</a>.</p>
<p>Now that we have our inference script ready, let’s proceed with preparing the <strong class="source-inline">requirements.txt</strong> file in the next section!</p>
<h2 id="_idParaDest-148"><a id="_idTextAnchor158"/>Preparing the requirements.txt file</h2>
<p>Since the <strong class="source-inline">transformers</strong> package<a id="_idIndexMarker788"/> is not included in the SageMaker PyTorch Docker containers, we will need to include it through a <strong class="source-inline">requirements.txt</strong> file, which is used by SageMaker to install additional packages at runtime. If this is your first time dealing with a <strong class="source-inline">requirements.txt</strong> file, it is simply a text file that contains a list of packages to be installed using the <strong class="source-inline">pip install</strong> command. If your <strong class="source-inline">requirements.txt</strong> file contains a single line (for example, <strong class="source-inline">transformers==4.4.2</strong>), then this will map to a <strong class="source-inline">pip install transformers==4.4.2</strong> during the installation step. If the <strong class="source-inline">requirements.txt</strong> file contains multiple lines, then each of the packages listed will be installed using the <strong class="source-inline">pip install</strong> command.</p>
<p class="callout-heading">Note</p>
<p class="callout">We can optionally <em class="italic">pin</em> the listed packages and dependencies to a specific version using <strong class="source-inline">==</strong> (equal). Alternatively, we can also use <strong class="source-inline">&lt;</strong> (less than), <strong class="source-inline">&gt;</strong> (greater than), and other variations to manage the upper-bound and lower-bound values of the version numbers of the packages to be installed.</p>
<p>In the next set of steps, we will create and prepare the <strong class="source-inline">requirements.txt</strong> file inside the <strong class="source-inline">scripts</strong> directory:</p>
<ol>
<li value="1">Create a new text file by clicking the <strong class="bold">File</strong> menu and choosing <strong class="bold">Text File</strong> from the list of options under the <strong class="bold">New</strong> submenu:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer239">
<img alt="Figure 7.8 – Creating a new text file inside the scripts directory " height="227" src="image/B18638_07_008.jpg" width="807"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Creating a new text file inside the scripts directory</p>
<p class="list-inset">In <em class="italic">Figure 7.8</em>, we can see that we’re in the <strong class="source-inline">scripts</strong> directory (<strong class="bold">File Browser</strong> sidebar pane) while we’re creating<a id="_idIndexMarker789"/> the new text file. This will create the new file inside the <strong class="source-inline">scripts</strong> directory.</p>
<ol>
<li value="2">Rename the file <strong class="source-inline">requirements.txt</strong></li>
<li>In the <strong class="bold">Editor</strong> pane, update the contents of the <strong class="source-inline">requirements.txt</strong> file to the following:<pre class="source-code"><strong class="bold">transformers==4.4.2</strong></pre></li>
<li>Make sure to save the changes by pressing <em class="italic">CTRL</em> + <em class="italic">S</em>. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you are using a Mac, use <em class="italic">CMD</em> + <em class="italic">S</em> instead. Alternatively, you can just click <strong class="bold">Save Python File</strong> under the list of options under the <strong class="bold">File</strong> menu.</p>
<p>Wasn’t that easy? Let’s now proceed with the last prerequisite – the <strong class="source-inline">setup.py</strong> file.</p>
<h2 id="_idParaDest-149"><a id="_idTextAnchor159"/>Preparing the setup.py file</h2>
<p>In addition to the <strong class="source-inline">requirements.txt</strong> file, we will prepare<a id="_idIndexMarker790"/> a <strong class="source-inline">setup.py</strong> file that will contain some additional information and metadata. </p>
<p class="callout-heading">Note</p>
<p class="callout">We won’t dive deep into the differences between the usage of <strong class="source-inline">requirements.txt</strong> and <strong class="source-inline">setup.py</strong> files. Feel free<a id="_idIndexMarker791"/> to check out the following link for more information: <a href="https://docs.python.org/3/distutils/setupscript.xhtml">https://docs.python.org/3/distutils/setupscript.xhtml</a>.</p>
<p>In the next set of steps, we will create and prepare the <strong class="source-inline">setup.py</strong> file inside the <strong class="source-inline">scripts</strong> directory:</p>
<ol>
<li value="1">Using the same set of steps as in the previous section, create a new text file and rename it <strong class="source-inline">setup.py</strong>. Make sure that this file is in the same directory (<strong class="source-inline">scripts</strong>) as the <strong class="source-inline">inference.py</strong> and <strong class="source-inline">requirements.txt</strong> files.</li>
<li>Update the contents of the <strong class="source-inline">setup.py</strong> file to contain the following block of code:<pre class="source-code">from setuptools import setup, find_packages</pre><pre class="source-code"><strong class="bold">setup</strong>(<strong class="bold">name</strong>='distillbert',</pre><pre class="source-code">      <strong class="bold">version</strong>='1.0',</pre><pre class="source-code">      <strong class="bold">description</strong>='distillbert',</pre><pre class="source-code">      packages=find_packages(</pre><pre class="source-code">          exclude=('tests', 'docs')</pre><pre class="source-code">     ))</pre></li>
</ol>
<p class="list-inset">The setup script simply makes use of the <strong class="source-inline">setup()</strong> function to describe the module distribution. Here, we specify metadata such as the <strong class="source-inline">name</strong>, <strong class="source-inline">version</strong>, and <strong class="source-inline">description</strong> when the <strong class="source-inline">setup()</strong> function is called.</p>
<ol>
<li value="3">Finally, make sure to save the changes by pressing <em class="italic">CTRL</em> + <em class="italic">S</em>. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you are using a Mac, use <em class="italic">CMD</em> + <em class="italic">S</em> instead. Alternatively, you can just click <strong class="bold">Save Python File</strong> under the list of options under the <strong class="bold">File</strong> menu.</p>
<p>At this point, we have prepared all the prerequisites<a id="_idIndexMarker792"/> needed to run all the succeeding sections of this chapter. With this, let’s proceed with deploying our pre-trained model to a real-time inference endpoint in the next section!</p>
<h1 id="_idParaDest-150"><a id="_idTextAnchor160"/>Deploying a pre-trained model to a real-time inference endpoint</h1>
<p>In this section, we will use the SageMaker Python SDK<a id="_idIndexMarker793"/> to deploy a pre-trained model<a id="_idIndexMarker794"/> to a real-time inference endpoint. From the name itself, we can tell that a real-time inference endpoint can process input payloads and perform predictions in real time. If you have built an API endpoint before (which can process GET and POST requests, for example), then we can think of an inference endpoint as an API endpoint that accepts an input request and returns a prediction as part of a response. How are predictions made? The inference endpoint simply loads the model into memory and uses it to process the input payload. This will yield an output that is returned as a response. For example, if we have a pre-trained sentiment analysis ML model deployed in a real-time inference endpoint, then it would return a response of either <strong class="source-inline">"POSITIVE"</strong> or <strong class="source-inline">"</strong><strong class="source-inline">NEGATIVE"</strong> depending on the input string payload provided in the request.</p>
<p class="callout-heading">Note</p>
<p class="callout">Let’s say that our inference endpoint receives the statement <strong class="source-inline">"I love reading the book MLE on AWS!"</strong>  via a POST request. The inference endpoint would then process the request input data and use the ML model for inference. The result of the ML model inference step (for example, number values that represent a <strong class="source-inline">"POSITIVE"</strong> result) would then be returned as part of the response.</p>
<div>
<div class="IMG---Figure" id="_idContainer240">
<img alt="Figure 7.9 – The desired file and folder structure   " height="703" src="image/B18638_07_009.jpg" width="1424"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – The desired file and folder structure</p>
<p>To get this to work, we just need to make sure<a id="_idIndexMarker795"/> that the prerequisite files, including<a id="_idIndexMarker796"/> the inference script file (for example, <strong class="source-inline">inference.py</strong>) and the <strong class="source-inline">requirements.txt</strong> file, are ready before using the SageMaker Python SDK to prepare the real-time inference endpoint. Make sure to check and review the folder structure in <em class="italic">Figure 7.9</em> before proceeding with the hands-on solutions in this section.</p>
<p>In the next set of steps, we will use the SageMaker Python SDK to deploy our pre-trained model to a real-time inference endpoint:</p>
<ol>
<li value="1">In <strong class="bold">File Browser</strong>, navigate back to the <strong class="source-inline">CH07</strong> directory and create a new Notebook using the <strong class="source-inline">Data Science</strong> image. Rename the notebook <strong class="source-inline">02 - Deploying a real-time inference endpoint.ipynb</strong>.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">The new notebook should be next to <strong class="source-inline">01 - Prepare model.tar.gz file.ipynb</strong>, similar to what is shown in <em class="italic">Figure 7.9</em>.</p>
<ol>
<li value="2">Let’s run the following code block in the first cell of the new notebook:<pre class="source-code">%store -r <strong class="bold">model_data</strong></pre><pre class="source-code">%store -r <strong class="bold">s3_bucket</strong></pre><pre class="source-code">%store -r <strong class="bold">prefix</strong></pre></li>
</ol>
<p class="list-inset">Here, we use the <strong class="source-inline">%store</strong> magic to load the variable values for <strong class="source-inline">model_data</strong>, <strong class="source-inline">s3_bucket</strong>, and <strong class="source-inline">prefix</strong>.</p>
<ol>
<li value="3">Next, let’s prepare<a id="_idIndexMarker797"/> the IAM execution role<a id="_idIndexMarker798"/> for use by SageMaker:<pre class="source-code">from sagemaker import get_execution_role </pre><pre class="source-code">role = get_execution_role()</pre></li>
<li>Initialize the <strong class="source-inline">PyTorchModel</strong> object:<pre class="source-code">from sagemaker.pytorch.model import PyTorchModel</pre><pre class="source-code">model = <strong class="bold">PyTorchModel</strong>(</pre><pre class="source-code">    model_data=model_data, </pre><pre class="source-code">    role=role, </pre><pre class="source-code">    source_dir="<strong class="bold">scripts</strong>",</pre><pre class="source-code">    entry_point='<strong class="bold">inference.py</strong>', </pre><pre class="source-code">    framework_version='1.6.0',</pre><pre class="source-code">    py_version="py3"</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Let’s check out <em class="italic">Figure 7.10</em> to help us visualize what has happened in the previous block of code:</p>
<div>
<div class="IMG---Figure" id="_idContainer241">
<img alt="Figure 7.10 – Deploying a real-time inference endpoint " height="372" src="image/B18638_07_010.jpg" width="1212"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – Deploying a real-time inference endpoint</p>
<p class="list-inset">In <em class="italic">Figure 7.10</em>, we can see<a id="_idIndexMarker799"/> that we initialized a <strong class="source-inline">Model</strong> object<a id="_idIndexMarker800"/> by passing several configuration parameters during the initialization step: (1) the model data, (2) the framework version, and (3) the path to the <strong class="source-inline">inference.py</strong> script file. There are other arguments we can set but we will simplify things a bit and focus on these three. In order for SageMaker to know how to use the pre-trained model for inference, the <strong class="source-inline">inference.py</strong> script file should contain the custom logic, which loads the ML model and uses it to perform predictions.</p>
<p class="callout-heading">Note</p>
<p class="callout">It is important to note that we are not limited to naming the inference script file <strong class="source-inline">inference.py</strong>. We can use a different naming convention as long as we specify the correct <strong class="source-inline">entry_point</strong> value.</p>
<p class="list-inset">This is the case if we are using SageMaker’s script mode when deploying ML models. Note that there are other options available, such as using a custom container image where instead of passing a script, we’ll be passing a container image that we’ve prepared ahead of time. When deploying ML models trained using the <strong class="bold">built-in algorithms</strong> of SageMaker, we can proceed with deploying these models right away without any custom scripts or container images, since SageMaker already has provided all the prerequisites needed for deployment.</p>
<ol>
<li value="5">Use the <strong class="source-inline">deploy()</strong> method<a id="_idIndexMarker801"/> to deploy the model to a real-time inference<a id="_idIndexMarker802"/> endpoint:<pre class="source-code">%%time</pre><pre class="source-code">from sagemaker.serializers import JSONSerializer</pre><pre class="source-code">from sagemaker.deserializers import JSONDeserializer</pre><pre class="source-code">predictor = model.<strong class="bold">deploy</strong>(</pre><pre class="source-code">    instance_type='ml.m5.xlarge', </pre><pre class="source-code">    initial_instance_count=1,</pre><pre class="source-code">    serializer=<strong class="bold">JSONSerializer()</strong>,</pre><pre class="source-code">    deserializer=<strong class="bold">JSONDeserializer()</strong></pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">This step should take around 3 to 8 minutes to complete.</p>
<p class="callout-heading">Note</p>
<p class="callout">When using the <strong class="source-inline">deploy()</strong> method to deploy an ML model using the SageMaker Python SDK, we are given the ability to specify the instance type. Choosing the right instance type for the model is important and finding the optimal balance between cost and performance is not a straightforward process. There are many instance types and sizes to choose from and ML engineers may end up having a suboptimal setup when deploying models in the SageMaker hosting <a id="_idIndexMarker803"/>services. The good news is that SageMaker has a capability called <strong class="bold">SageMaker Inference Recommender</strong>, which can help you decide which<a id="_idIndexMarker804"/> instance type to use. For more information, you can check out the following link: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml</a>.</p>
<ol>
<li value="6">Now that our real-time inference endpoint is running, let’s perform a sample prediction using the <strong class="source-inline">predict()</strong> method:<pre class="source-code">payload = {</pre><pre class="source-code">    "text": "I love reading the book MLE on AWS!"</pre><pre class="source-code">}</pre><pre class="source-code">predictor.<strong class="bold">predict</strong>(payload)</pre></li>
</ol>
<p class="list-inset">This should yield an output value of <strong class="source-inline">'POSITIVE'</strong>.</p>
<ol>
<li value="7">Let’s also test a negative scenario:<pre class="source-code">payload = {</pre><pre class="source-code">    "text": "This is the worst spaghetti I've had"</pre><pre class="source-code">}</pre><pre class="source-code">predictor.<strong class="bold">predict</strong>(payload)</pre></li>
</ol>
<p class="list-inset">This should yield an output value of <strong class="source-inline">'NEGATIVE'</strong>. Feel free to test different values before deleting the endpoint in the next step.</p>
<ol>
<li value="8">Finally, let’s delete the inference endpoint using the <strong class="source-inline">delete_endpoint()</strong> method:<pre class="source-code">predictor.<strong class="bold">delete_endpoint()</strong></pre></li>
</ol>
<p class="list-inset">This will help us avoid any unexpected charges for unused inference endpoints.</p>
<p>Wasn’t that easy? Deploying<a id="_idIndexMarker805"/> a pre-trained model<a id="_idIndexMarker806"/> to a real-time inference endpoint (inside an ML instance with the specified instance type) using the SageMaker Python SDK is so straightforward! A lot of the engineering work has been automated for us and all we need to do is call the <strong class="source-inline">Model</strong> object’s <strong class="source-inline">deploy()</strong> method. </p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor161"/>Deploying a pre-trained model to a serverless inference endpoint</h1>
<p>In the initial chapters<a id="_idIndexMarker807"/> of this book, we’ve worked with several serverless<a id="_idIndexMarker808"/> services that allow us to manage and reduce costs. If you are wondering whether there’s a serverless option when deploying ML models in SageMaker, then the answer to that would be a sweet yes. When you are dealing with intermittent and unpredictable traffic, using serverless inference endpoints to host your ML model can be a more cost-effective option. Let’s say that we can tolerate <strong class="bold">cold starts</strong> (where a request takes longer to process after periods of inactivity) and we only expect a few requests per day – then, we can make use of a serverless inference endpoint instead of the real-time option. Real-time inference endpoints are best used when we can maximize the inference endpoint. If you’re expecting your endpoint to be utilized most of the time, then the real-time option may do the trick. </p>
<div>
<div class="IMG---Figure" id="_idContainer242">
<img alt="Figure 7.11 – The desired file and folder structure " height="591" src="image/B18638_07_011.jpg" width="972"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – The desired file and folder structure</p>
<p>Deploying a pre-trained ML model to a serverless inference endpoint using the SageMaker Python SDK is similar to how it is done for real-time inference endpoints. The only major differences would be the following:</p>
<ul>
<li>The initialization of the <strong class="source-inline">ServerlessInferenceConfig</strong> object</li>
<li>Passing this object as an argument when calling the <strong class="source-inline">Model</strong> object’s <strong class="source-inline">deploy()</strong> method</li>
</ul>
<p>In the next set of steps, we will use the SageMaker Python SDK<a id="_idIndexMarker809"/> to deploy our pre-trained model<a id="_idIndexMarker810"/> to a serverless inference endpoint:</p>
<ol>
<li value="1">In <strong class="bold">File Browser</strong>, navigate back to the <strong class="source-inline">CH07</strong> directory and create a new Notebook using the <strong class="source-inline">Data Science</strong> image. Rename the notebook <strong class="source-inline">03 - Deploying a serverless inference endpoint.ipynb</strong>.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">The new notebook should be next to <strong class="source-inline">01 - Prepare model.tar.gz file.ipynb</strong>, similar to what is shown in <em class="italic">Figure 7.11</em>.</p>
<ol>
<li value="2">In the first cell of the new notebook, let’s run the following block of code to load the variable values for <strong class="source-inline">model_data</strong>, <strong class="source-inline">s3_bucket</strong>, and <strong class="source-inline">prefix</strong>:<pre class="source-code">%store -r <strong class="bold">model_data</strong></pre><pre class="source-code">%store -r <strong class="bold">s3_bucket</strong></pre><pre class="source-code">%store -r <strong class="bold">prefix</strong></pre></li>
</ol>
<p class="list-inset">If you get an error when running this block of code, make sure that you have completed the steps specified in the <em class="italic">Preparing the pre-trained model artifacts</em> section of this chapter.</p>
<ol>
<li value="3">Prepare the IAM execution role to be used by SageMaker:<pre class="source-code">from sagemaker import get_execution_role </pre><pre class="source-code">role = get_execution_role()</pre></li>
<li>Initialize and configure the <strong class="source-inline">ServerlessInferenceConfig</strong> object:<pre class="source-code">from sagemaker.serverless import ServerlessInferenceConfig</pre><pre class="source-code">serverless_config = <strong class="bold">ServerlessInferenceConfig</strong>(</pre><pre class="source-code">  memory_size_in_mb=4096,</pre><pre class="source-code">  max_concurrency=5,</pre><pre class="source-code">)</pre></li>
<li>Initialize the <strong class="source-inline">PyTorchModel</strong> object<a id="_idIndexMarker811"/> and use the <strong class="source-inline">deploy()</strong> method<a id="_idIndexMarker812"/> to deploy the model to a serverless inference endpoint:<pre class="source-code">from sagemaker.pytorch.model import PyTorchModel</pre><pre class="source-code">from sagemaker.serializers import JSONSerializer</pre><pre class="source-code">from sagemaker.deserializers import JSONDeserializer</pre><pre class="source-code">model = <strong class="bold">PyTorchModel</strong>(</pre><pre class="source-code">    model_data=model_data, </pre><pre class="source-code">    role=role, </pre><pre class="source-code">    source_dir="<strong class="bold">scripts</strong>",</pre><pre class="source-code">    entry_point='<strong class="bold">inference.py</strong>', </pre><pre class="source-code">    framework_version='1.6.0',</pre><pre class="source-code">    py_version="py3"</pre><pre class="source-code">)</pre><pre class="source-code">predictor = model.<strong class="bold">deploy</strong>(</pre><pre class="source-code">    instance_type='ml.m5.xlarge', </pre><pre class="source-code">    initial_instance_count=1,</pre><pre class="source-code">    serializer=JSONSerializer(),</pre><pre class="source-code">    deserializer=JSONDeserializer(),</pre><pre class="source-code">    <strong class="bold">serverless_inference_config=serverless_config</strong></pre><pre class="source-code">)</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">The model deployment should take around 3 to 8 minutes to complete.</p>
<ol>
<li value="6">Now that our real-time inference endpoint<a id="_idIndexMarker813"/> is running, let’s perform<a id="_idIndexMarker814"/> a sample prediction using the <strong class="source-inline">predict()</strong> method:<pre class="source-code">payload = {</pre><pre class="source-code">    "text": "I love reading the book MLE on AWS!"</pre><pre class="source-code">}</pre><pre class="source-code">predictor.<strong class="bold">predict</strong>(payload)</pre></li>
</ol>
<p class="list-inset">This should yield an output value of <strong class="source-inline">'POSITIVE'</strong>.</p>
<ol>
<li value="7">Let’s also test a negative scenario:<pre class="source-code">payload = {</pre><pre class="source-code">    "text": "This is the worst spaghetti I've had"</pre><pre class="source-code">}</pre><pre class="source-code">predictor.predict(payload)</pre></li>
</ol>
<p class="list-inset">This should yield an output value of <strong class="source-inline">'NEGATIVE'</strong>. Feel free to test different values before deleting the endpoint in the next step.</p>
<ol>
<li value="8">Finally, let’s delete the inference endpoint using the <strong class="source-inline">delete_endpoint()</strong> method:<pre class="source-code">predictor.<strong class="bold">delete_endpoint()</strong></pre></li>
</ol>
<p class="list-inset">This will help us avoid any unexpected charges for unused inference endpoints.</p>
<p>As you can see, everything is almost the same, except for the initialization and usage of the <strong class="source-inline">ServerlessInferenceConfig</strong> object. When using a serverless endpoint, SageMaker manages the compute resources for us and performs the following automatically:</p>
<ul>
<li>Auto-assigns compute resources proportional to the <strong class="source-inline">memory_size_in_mb</strong> parameter value we specified when initializing <strong class="source-inline">ServerlessInferenceConfig</strong></li>
<li>Uses the configured maximum concurrency value to manage how many concurrent invocations can happen at the same time</li>
<li>Scales down the resources automatically to zero if there are no requests</li>
</ul>
<p>Once you see more examples<a id="_idIndexMarker815"/> of how to use the SageMaker Python SDK, you’ll start<a id="_idIndexMarker816"/> to realize how well this SDK has been designed and implemented. </p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor162"/>Deploying a pre-trained model to an asynchronous inference endpoint</h1>
<p>In addition to real-time<a id="_idIndexMarker817"/> and serverless inference endpoints, SageMaker also offers<a id="_idIndexMarker818"/> a third option when deploying models – <strong class="bold">asynchronous inference endpoints</strong>. Why is it called asynchronous? For one thing, instead of expecting the results to be available immediately, requests are queued, and results are made available <em class="italic">asynchronously</em>. This works for ML requirements that involve one or more of the following:</p>
<ul>
<li>Large input payloads (up to 1 GB) </li>
<li>A long prediction processing duration (up to 15 minutes)</li>
</ul>
<p>A good use case for asynchronous inference endpoints would be for ML models that are used to detect objects in large video files (which may take more than 60 seconds to complete). In this case, an inference may take a few minutes instead of a few seconds.</p>
<p><em class="italic">How do we use asynchronous inference endpoints?</em> To invoke<a id="_idIndexMarker819"/> an asynchronous inference endpoint, we do<a id="_idIndexMarker820"/> the following:</p>
<ol>
<li value="1">The request payload is uploaded to an Amazon S3 bucket.</li>
<li>The S3 path or location (where the request payload is stored) is used as the parameter value when calling the <strong class="source-inline">predict_async()</strong> method of the <strong class="source-inline">AsyncPredictor</strong> object (which maps or represents the ML inference endpoint).</li>
<li>Upon invocation of the endpoint, the asynchronous inference endpoint queues the request for processing (once the endpoint can).</li>
<li>After processing the request, the output inference results are stored and uploaded to the output S3 location.</li>
<li>An SNS notification (for example, a success or error notification) is sent (if set up).</li>
</ol>
<p>In this section, we will deploy our NLP model to an asynchronous inference endpoint. To emulate a delay, we’ll call the <strong class="source-inline">sleep()</strong> function in our inference script so that the prediction step takes longer than usual. Once we can get this relatively simple setup to work, working on more complex requirements such as object detection for video files will definitely be easier.</p>
<div>
<div class="IMG---Figure" id="_idContainer243">
<img alt="Figure 7.12 – The file and folder structure " height="689" src="image/B18638_07_012.jpg" width="1330"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – The file and folder structure</p>
<p>To get this setup to work, we will need<a id="_idIndexMarker821"/> to prepare a file that contains<a id="_idIndexMarker822"/> an input payload similar to that shown in <em class="italic">Figure 7.12</em> (for example, <em class="italic">data</em> or <strong class="source-inline">input.json</strong>). Once the input file has been prepared, we will upload it to an Amazon S3 bucket and then proceed with deploying our pre-trained ML model to an asynchronous inference endpoint.</p>
<p>With this in mind, let’s proceed with creating the input JSON file!</p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor163"/>Creating the input JSON file</h2>
<p>In the next set of steps, we will<a id="_idIndexMarker823"/> create a sample file containing the input JSON<a id="_idIndexMarker824"/> value that will be used when invoking the asynchronous inference endpoint in the next section:</p>
<ol>
<li value="1">Right-click on the empty space in the <strong class="bold">File Browser</strong> sidebar pane to open a context menu similar to that shown in <em class="italic">Figure 7.13</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer244">
<img alt="Figure 7.13 – Creating a new folder " height="351" src="image/B18638_07_013.jpg" width="706"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – Creating a new folder</p>
<p class="list-inset">Make sure<a id="_idIndexMarker825"/> that you are in the <strong class="source-inline">CH07</strong> directory<a id="_idIndexMarker826"/> in <strong class="bold">File Browser</strong> before performing this step.</p>
<ol>
<li value="2">Rename the folder <strong class="source-inline">data</strong>.</li>
<li>Double-click the <strong class="source-inline">data</strong> folder in the <strong class="bold">File Browser</strong> sidebar pane to navigate to the directory.</li>
<li>Create a new text file by clicking the <strong class="bold">File</strong> menu and choosing <strong class="bold">Text File</strong> from the list of options under the <strong class="bold">New</strong> submenu:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer245">
<img alt="Figure 7.14 – Creating a new text file " height="226" src="image/B18638_07_014.jpg" width="822"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – Creating a new text file</p>
<p class="list-inset">Make sure that you are in the <strong class="source-inline">data</strong> directory when creating a new text file, similar to that in <em class="italic">Figure 7.14</em>.</p>
<ol>
<li value="5">Rename the file <strong class="source-inline">input.json</strong>, as in <em class="italic">Figure 7.15</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer246">
<img alt="Figure 7.15 – Renaming the text file " height="281" src="image/B18638_07_015.jpg" width="609"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Renaming the text file</p>
<p class="list-inset">To rename the <strong class="source-inline">untitled.txt</strong> file, right-click<a id="_idIndexMarker827"/> on the file in the <strong class="bold">File Browser</strong> sidebar pane<a id="_idIndexMarker828"/> and then select <strong class="bold">Rename</strong> from the list of options in the drop-down menu. Specify the desired filename (<strong class="source-inline">input.json</strong>) to replace the default name value.</p>
<ol>
<li value="6">In the <strong class="bold">Editor</strong> pane, update the content of the <strong class="source-inline">input.json</strong> file with the following JSON value:<pre class="source-code">{"text": "I love reading the book MLE on AWS!"}</pre></li>
<li>Make sure to save your changes by pressing <em class="italic">CTRL</em> + <em class="italic">S</em>. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you are using a Mac, use <em class="italic">CMD</em> + <em class="italic">S</em> instead. Alternatively, you can just click <strong class="bold">Save Python File</strong> under the list of options under the <strong class="bold">File</strong> menu.</p>
<p>Again, the input file<a id="_idIndexMarker829"/> is only needed when we’re planning<a id="_idIndexMarker830"/> to deploy our ML model to an asynchronous inference endpoint. With this prepared, we can now proceed to the next set of steps.</p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor164"/>Adding an artificial delay to the inference script</h2>
<p>Before proceeding<a id="_idIndexMarker831"/> with using the SageMaker Python SDK to deploy our pre-trained model to an asynchronous inference endpoint, we will add an artificial delay to the prediction step. This will help us emulate inference or prediction requests that take a bit of time to complete.</p>
<p class="callout-heading">Note</p>
<p class="callout">When troubleshooting<a id="_idIndexMarker832"/> an asynchronous inference endpoint, you may opt to test an ML model that performs predictions within just a few seconds first. This will help you know right away if there’s something wrong since the output is expected to be uploaded to the S3 output path within a few seconds (instead of a few minutes). That said, you may want to remove the artificial delay temporarily if you’re having issues getting the setup to work.</p>
<p>In the next set of steps, we will update the <strong class="source-inline">inference.py</strong> script to add a 30-second delay when performing a prediction:</p>
<ol>
<li value="1">Continuing where we left off in the previous section, let’s navigate to the <strong class="source-inline">CH07</strong> directory in <strong class="bold">File Browser</strong>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer247">
<img alt="Figure 7.16 – Navigating to the CH07 directory " height="181" src="image/B18638_07_016.jpg" width="546"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Navigating to the CH07 directory</p>
<p class="list-inset">Here, we click the <strong class="source-inline">CH07</strong> link, as highlighted in <em class="italic">Figure 7.16</em>.</p>
<ol>
<li value="2">Double-click<a id="_idIndexMarker833"/> the <strong class="source-inline">scripts</strong> folder, as shown in <em class="italic">Figure 7.17</em>, to navigate to the directory:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer248">
<img alt="Figure 7.17 – Navigating to the scripts directory " height="256" src="image/B18638_07_017.jpg" width="583"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – Navigating to the scripts directory</p>
<p class="list-inset">Make sure that you have completed the hands-on steps in the <em class="italic">Preparing the SageMaker script mode prerequisites</em> section before proceeding with the next step. The <strong class="source-inline">scripts</strong> directory should contain three files: </p>
<ul>
<li><strong class="source-inline">inference.py</strong></li>
<li><strong class="source-inline">requirements.txt</strong></li>
<li><strong class="source-inline">setup.py</strong></li>
</ul>
<ol>
<li value="3">Double-click and open the <strong class="source-inline">inference.py</strong> file, as highlighted in <em class="italic">Figure 7.18</em>. Locate the <strong class="source-inline">predict_fn()</strong> function and uncomment the line of code containing <strong class="source-inline">sleep(30)</strong>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer249">
<img alt="" height="354" src="image/B18638_07_018.jpg" width="964"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – Updating the inference.py file</p>
<p class="list-inset">To uncomment the line of code, simply remove the hash and the empty space (<strong class="source-inline"># </strong>) before <strong class="source-inline">sleep(30)</strong>, similar to what we can see in <em class="italic">Figure 7.18</em>. </p>
<ol>
<li value="4">Make sure to save the changes by pressing <em class="italic">CTRL</em> + <em class="italic">S</em>. </li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you are using a Mac, use <em class="italic">CMD</em> + <em class="italic">S</em> instead. Alternatively, you can just click <strong class="bold">Save Python File</strong> under the list of options under the <strong class="bold">File</strong> menu.</p>
<p>Now that we have finished adding<a id="_idIndexMarker834"/> an artificial 30-second delay, let’s proceed with using the SageMaker Python SDK to deploy our asynchronous inference endpoint.</p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor165"/>Deploying and testing an asynchronous inference endpoint</h2>
<p>Deploying a pre-trained ML model<a id="_idIndexMarker835"/> to an asynchronous inference endpoint<a id="_idIndexMarker836"/> using the SageMaker Python SDK is similar to how it is done for real-time and serverless inference endpoints. The only major differences would be (1) the initialization of the <strong class="source-inline">AsyncInferenceConfig</strong> object, and (2) passing this object as an argument when calling the <strong class="source-inline">Model</strong> object’s <strong class="source-inline">deploy()</strong> method.</p>
<p>In the next set of steps, we will use the SageMaker Python SDK to deploy our pre-trained model to an asynchronous inference endpoint:</p>
<ol>
<li value="1">Continuing<a id="_idIndexMarker837"/> where we left off in the <em class="italic">Adding an artificial delay to the inference script section</em>, let’s navigate to the <strong class="source-inline">CH07</strong> directory in <strong class="bold">File Browser</strong> and create<a id="_idIndexMarker838"/> a new Notebook using the <strong class="source-inline">Data Science</strong> image. Rename the notebook <strong class="source-inline">04 - Deploying an asynchronous inference endpoint.ipynb</strong>.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">The new notebook should be next to <strong class="source-inline">01 - Prepare model.tar.gz file.ipynb</strong>.</p>
<ol>
<li value="2">In the first cell of the new notebook, let’s run the following block of code to load the variable values for <strong class="source-inline">model_data</strong>, <strong class="source-inline">s3_bucket</strong>, and <strong class="source-inline">prefix</strong>:<pre class="source-code">%store -r <strong class="bold">model_data</strong></pre><pre class="source-code">%store -r <strong class="bold">s3_bucket</strong></pre><pre class="source-code">%store -r <strong class="bold">prefix</strong></pre></li>
</ol>
<p class="list-inset">If you get an error when running this block of code, make sure that you have completed the steps specified in the <em class="italic">Preparing the pre-trained model artifacts</em> section of this chapter.</p>
<ol>
<li value="3">Prepare the path where we will upload the inference input file:<pre class="source-code">input_data = "s3://{}/{}/data/input.json".format(</pre><pre class="source-code">    s3_bucket,</pre><pre class="source-code">    prefix</pre><pre class="source-code">)</pre></li>
<li>Upload the <strong class="source-inline">input.json</strong> file to the S3 bucket using the <strong class="source-inline">aws s3 cp</strong> command:<pre class="source-code">!aws s3 cp data/input.json {input_data}</pre></li>
<li>Prepare the IAM<a id="_idIndexMarker839"/> execution role for use<a id="_idIndexMarker840"/> by SageMaker:<pre class="source-code">from sagemaker import get_execution_role </pre><pre class="source-code">role = get_execution_role()</pre></li>
<li>Initialize the <strong class="source-inline">AsyncInferenceConfig</strong> object:<pre class="source-code">from sagemaker.async_inference import AsyncInferenceConfig</pre><pre class="source-code">output_path = f"s3://{s3_bucket}/{prefix}/output"</pre><pre class="source-code">async_config = <strong class="bold">AsyncInferenceConfig</strong>(</pre><pre class="source-code">    output_path=output_path</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">While initializing the <strong class="source-inline">AsyncInferenceConfig</strong> object, we specify the value for the <strong class="source-inline">output_path</strong> parameter where the results will be saved.</p>
<ol>
<li value="7">Next, let’s initialize the <strong class="source-inline">PyTorchModel</strong> object:<pre class="source-code">from sagemaker.pytorch.model import PyTorchModel</pre><pre class="source-code">model = <strong class="bold">PyTorchModel</strong>(</pre><pre class="source-code">    model_data=model_data, </pre><pre class="source-code">    role=role, </pre><pre class="source-code">    source_dir="<strong class="bold">scripts</strong>",</pre><pre class="source-code">    entry_point='<strong class="bold">inference.py</strong>', </pre><pre class="source-code">    framework_version='1.6.0',</pre><pre class="source-code">    py_version="py3"</pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Here, we specify the configuration values for the parameters, such as <strong class="source-inline">model_data</strong>, <strong class="source-inline">role</strong>, <strong class="source-inline">source_dir</strong>, <strong class="source-inline">entry_point</strong>, <strong class="source-inline">framework_version</strong>, and <strong class="source-inline">py_version</strong>.</p>
<ol>
<li value="8">Use the <strong class="source-inline">deploy()</strong> method<a id="_idIndexMarker841"/> to deploy the model<a id="_idIndexMarker842"/> to an asynchronous inference endpoint:<pre class="source-code">%%time</pre><pre class="source-code">from sagemaker.serializers import JSONSerializer</pre><pre class="source-code">from sagemaker.deserializers import JSONDeserializer</pre><pre class="source-code">predictor = model.<strong class="bold">deploy</strong>(</pre><pre class="source-code">    instance_type='ml.m5.xlarge', </pre><pre class="source-code">    initial_instance_count=1,</pre><pre class="source-code">    serializer=JSONSerializer(),</pre><pre class="source-code">    deserializer=JSONDeserializer(),</pre><pre class="source-code">    <strong class="bold">async_inference_config=async_config</strong></pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">Here, we specify the <strong class="source-inline">AsyncInferenceConfig</strong> object we initiated in a previous step as the parameter value to <strong class="source-inline">async_inference_config</strong>.</p>
<div>
<div class="IMG---Figure" id="_idContainer250">
<img alt="Figure 7.19 – Deploying an asynchronous inference endpoint " height="523" src="image/B18638_07_019.jpg" width="1129"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – Deploying an asynchronous inference endpoint</p>
<p class="list-inset">In <em class="italic">Figure 7.19</em>, we can see that the <strong class="source-inline">deploy()</strong> method accepts<a id="_idIndexMarker843"/> the parameter value<a id="_idIndexMarker844"/> for SageMaker to configure an asynchronous inference endpoint instead of a real-time inference endpoint. </p>
<p class="callout-heading">Note</p>
<p class="callout">The model deployment should take around 3 to 8 minutes to complete.</p>
<ol>
<li value="9">Once the inference endpoint is ready, let’s use the <strong class="source-inline">predict_async()</strong> method to perform the prediction:<pre class="source-code">response = predictor.<strong class="bold">predict_async</strong>(</pre><pre class="source-code">    input_path=<strong class="bold">input_data</strong></pre><pre class="source-code">)</pre></li>
</ol>
<p class="list-inset">This should invoke the asynchronous inference endpoint using the data stored in the <strong class="source-inline">input.json</strong> file stored in S3.</p>
<div>
<div class="IMG---Figure" id="_idContainer251">
<img alt="Figure 7.20 – How the predict_async() method works " height="347" src="image/B18638_07_020.jpg" width="930"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 – How the predict_async() method works</p>
<p class="list-inset">In <em class="italic">Figure 7.20</em>, we can see<a id="_idIndexMarker845"/> that the input payload for an asynchronous inference endpoint<a id="_idIndexMarker846"/> comes from an S3 bucket. Then, after the endpoint processes the request, the output is saved to S3. This would probably not make any sense if your input payload were small (for example, less than <em class="italic">1 MB</em>). However, if the input payload involves larger files such as video files, then uploading this into S3 and utilizing an asynchronous inference endpoint for predictions would make a lot more sense.</p>
<ol>
<li value="10">Use the <strong class="source-inline">sleep()</strong> function to wait for 40 seconds before calling the <strong class="source-inline">get_result()</strong> function of the <strong class="source-inline">response</strong> object:<pre class="source-code">from time import sleep</pre><pre class="source-code">sleep(40)</pre><pre class="source-code">response.<strong class="bold">get_result()</strong></pre></li>
</ol>
<p class="list-inset">This should yield an output value of <strong class="source-inline">'POSITIVE'</strong>.</p>
<p class="callout-heading">Note</p>
<p class="callout">Why wait for 40 seconds? Since we added an artificial 30-second delay in the prediction step, we would have to wait for at least 30 seconds before the output file is available in the specified S3 location. </p>
<ol>
<li value="11">Store the S3 path string value in the <strong class="source-inline">output_path</strong> variable:<pre class="source-code">output_path = response.output_path</pre></li>
<li>Use the <strong class="source-inline">aws s3 cp</strong> command<a id="_idIndexMarker847"/> to download a copy of the output file<a id="_idIndexMarker848"/> to the Studio notebook instance: <pre class="source-code">!<strong class="bold">aws s3 cp</strong> {output_path} sample.out</pre></li>
<li>Now that we have downloaded the output file, let’s use the <strong class="source-inline">cat</strong> command to check its contents:<pre class="source-code">!cat sample.out</pre></li>
</ol>
<p class="list-inset">This should give us an output value of <strong class="source-inline">'POSITIVE'</strong>, similar to what we obtained after using the <strong class="source-inline">get_result()</strong> method in an earlier step.</p>
<ol>
<li value="14">Let’s do a quick cleanup by deleting the copy of the output file using the <strong class="source-inline">rm</strong> command:<pre class="source-code">!rm sample.out</pre></li>
<li>Finally, let’s delete the inference endpoint using the <strong class="source-inline">delete_endpoint()</strong> method:<pre class="source-code">predictor.<strong class="bold">delete_endpoint()</strong></pre></li>
</ol>
<p class="list-inset">This will help us avoid any unexpected charges for unused inference endpoints.</p>
<p>It is important to note that in a production setup, it is preferable to update the architecture<a id="_idIndexMarker849"/> to be more event-driven and utilize the <strong class="bold">Amazon Simple Notification Service</strong> (<strong class="bold">SNS</strong>) when handling success and error notifications. We can use SNS to send our team an email every time there is a failure or have it trigger a Lambda function, which can be used to automatically perform a defined set of tasks. To configure the asynchronous inference endpoint to push notification events to SNS, the <strong class="source-inline">notification_config</strong> parameter value must be updated with the appropriate dictionary of values when initializing the <strong class="source-inline">AsyncInferenceConfig</strong> object. For more<a id="_idIndexMarker850"/> information, feel free to check out the following link: <a href="https://sagemaker.readthedocs.io/en/stable/overview.xhtml#sagemaker-asynchronous-inference">https://sagemaker.readthedocs.io/en/stable/overview.xhtml#sagemaker-asynchronous-inference</a>.</p>
<p class="callout-heading">Note</p>
<p class="callout">What’s SNS? SNS is a fully managed messaging service<a id="_idIndexMarker851"/> that allows architectures to be event-driven. Messages from a source (<em class="italic">publisher</em>) can fan out and be sent across a variety of receivers (<em class="italic">subscribers</em>). If we were to configure the SageMaker asynchronous inference endpoint to push notification messages to SNS, then it is best if we also register and set up a subscriber that waits for a success (or error) notification message once the prediction step is completed. This subscriber then proceeds with performing a pre-defined operation once the results are available.</p>
<h1 id="_idParaDest-156"><a id="_idTextAnchor166"/>Cleaning up</h1>
<p>Now that we have completed working<a id="_idIndexMarker852"/> on the hands-on solutions of this chapter, it is time for us to clean up and turn off any resources we will no longer use. In the next set of steps, we will locate and turn off any remaining running instances in SageMaker Studio:</p>
<ol>
<li value="1">Click the <strong class="bold">Running Instances and Kernels</strong> icon in the sidebar, as highlighted in <em class="italic">Figure 7.21</em>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer252">
<img alt="Figure 7.21 – Turning off the running instance " height="234" src="image/B18638_07_021.jpg" width="500"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.21 – Turning off the running instance</p>
<p class="list-inset">Clicking the <strong class="bold">Running Instances and Kernels</strong> icon should open and show the running instances, apps, and terminals in SageMaker Studio.</p>
<ol>
<li value="2">Turn off all running instances under <strong class="bold">RUNNING INSTANCES</strong> by clicking the <strong class="bold">Shut down</strong> button for each of the instances, as highlighted in <em class="italic">Figure 7.21</em>. Clicking the <strong class="bold">Shut down</strong> button will open a pop-up window verifying the instance shutdown operation. Click the <strong class="bold">Shut down all</strong> button to proceed.</li>
<li>Make sure to check for and delete all the running inference endpoints under <strong class="bold">SageMaker resources</strong> as well (if there are any):</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer253">
<img alt="Figure 7.22 – Checking the list of running inference endpoints " height="579" src="image/B18638_07_022.jpg" width="960"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.22 – Checking the list of running inference endpoints</p>
<p class="list-inset">To check whether there<a id="_idIndexMarker853"/> are running inference endpoints, click the <strong class="bold">SageMaker resources</strong> icon as highlighted in <em class="italic">Figure 7.22</em> and then select <strong class="bold">Endpoints</strong> from the list of options in the drop-down menu.</p>
<p>It is important to note that this cleanup operation needs to be performed after using SageMaker Studio. These resources are not turned off automatically by SageMaker even during periods of inactivity.</p>
<p class="callout-heading">Note</p>
<p class="callout">If you are looking into other ways to reduce costs when running ML workloads in SageMaker, you can check<a id="_idIndexMarker854"/> how you can utilize other features and capabilities such as <strong class="bold">SageMaker Savings Plans</strong> (which helps reduce costs in exchange for a consistent usage commitment<a id="_idIndexMarker855"/> for a 1-year or 3-year term), <strong class="bold">SageMaker Neo</strong> (which helps optimize ML models for deployment, speeding up inference and reducing costs), and <strong class="bold">SageMaker Inference Recommender</strong> (which helps you select the best instance<a id="_idIndexMarker856"/> type for the inference endpoint through automated load testing). We won’t discuss these in further detail in this book, so feel free<a id="_idIndexMarker857"/> to check out the following link for more information on these topics: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-cost-optimization.xhtml</a>.</p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor167"/>Deployment strategies and best practices </h1>
<p>In this section, we will discuss<a id="_idIndexMarker858"/> the relevant deployment strategies<a id="_idIndexMarker859"/> and best practices when using the SageMaker hosting services. Let’s start by talking about the different ways we can invoke an existing SageMaker inference endpoint. The solution we’ve been using so far involves the usage of the SageMaker Python SDK to invoke an existing endpoint:</p>
<pre class="source-code">from sagemaker.predictor import <strong class="bold">Predictor</strong>
from sagemaker.serializers import <strong class="bold">JSONSerializer</strong>
from sagemaker.deserializers import <strong class="bold">JSONDeserializer</strong>
endpoint_name = "<strong class="bold">&lt;INSERT NAME OF EXISTING ENDPOINT&gt;</strong>"
predictor = <strong class="bold">Predictor</strong>(endpoint_name=endpoint_name)
predictor.serializer = <strong class="bold">JSONSerializer</strong>() 
predictor.deserializer = <strong class="bold">JSONDeserializer</strong>()
payload = {
^  "text": "I love reading the book MLE on AWS!"
}
predictor.<strong class="bold">predict</strong>(payload)</pre>
<p>Here, we initialize a <strong class="source-inline">Predictor</strong> object<a id="_idIndexMarker860"/> and point it to an existing inference endpoint<a id="_idIndexMarker861"/> during the initialization step. We then use the <strong class="source-inline">predict()</strong> method of this <strong class="source-inline">Predictor</strong> object to invoke the inference endpoint.</p>
<p>Note that we can also invoke<a id="_idIndexMarker862"/> the same endpoint using the <strong class="bold">boto3</strong> library, similar to what is shown in the following block of code:</p>
<pre class="source-code">import boto3 
import json
endpoint_name = "<strong class="bold">&lt;INSERT NAME OF EXISTING ENDPOINT&gt;</strong>"
runtime = boto3.Session().client('<strong class="bold">sagemaker-runtime</strong>')
payload = {
    "text": "I love reading the book MLE on AWS!"
}
response = sagemaker_client.<strong class="bold">invoke_endpoint</strong>(
    EndpointName=endpoint_name, 
    ContentType='application/json', 
    Body=json.dumps(payload)
)
json.loads(<strong class="bold">response['Body']</strong>.read().decode('utf-8'))</pre>
<p>Here, we use the <strong class="source-inline">invoke_endpoint()</strong> method when performing predictions and inference using the existing ML inference endpoint. As you can see, even without the SageMaker Python SDK installed, we should be able to invoke an existing ML inference endpoint from an <strong class="bold">AWS Lambda</strong> function or even a Python web framework<a id="_idIndexMarker863"/> such as Django, Pyramid, or Flask using the <strong class="bold">boto3</strong> library. Note that we can even<a id="_idIndexMarker864"/> invoke the SageMaker inference endpoint<a id="_idIndexMarker865"/> from the terminal using the <strong class="bold">AWS CLI</strong> or through an HTTP <strong class="source-inline">POST</strong> request using the <strong class="source-inline">InvokeEndpoint</strong> API.</p>
<p class="callout-heading">Note</p>
<p class="callout">If your backend application code makes use of a language other than Python (for example, Ruby, Java, or JavaScript), then all you need to do is look for an existing SDK for that language along with the corresponding function or method to call. For more information, you can check out the following link containing<a id="_idIndexMarker866"/> the different tools, along with the SDKs available for each language: <a href="https://aws.amazon.com/tools/">https://aws.amazon.com/tools/</a>.</p>
<p>There are several solutions<a id="_idIndexMarker867"/> possible if you want to prepare an HTTP API<a id="_idIndexMarker868"/> that invokes and interfaces with an existing SageMaker inference endpoint. Here’s a quick list of possible solutions:</p>
<ul>
<li><em class="italic">Option 1</em>: <em class="italic">Amazon API Gateway HTTP API + AWS Lambda function + boto3 + SageMaker ML inference endpoint</em> – The <strong class="bold">Amazon API Gateway</strong> <strong class="bold">HTTP API</strong> receives the HTTP request and invokes the <strong class="bold">AWS Lambda</strong> function. The AWS Lambda function<a id="_idIndexMarker869"/> then uses the <strong class="source-inline">boto3</strong> library to invoke<a id="_idIndexMarker870"/> the SageMaker ML inference endpoint. </li>
<li><em class="italic">Option 2</em>: <em class="italic">AWS Lambda function + boto3 + SageMaker ML inference endpoint (Lambda function URLs)</em> – The AWS Lambda function is invoked directly from a Lambda function URL (which is a dedicated endpoint for triggering a Lambda function). The AWS Lambda function then uses the <strong class="source-inline">boto3</strong> library to invoke the SageMaker ML inference endpoint.</li>
<li><em class="italic">Option 3</em>: <em class="italic">Amazon API Gateway HTTP API + SageMaker ML inference endpoint (API Gateway mapping templates)</em> – The Amazon API Gateway HTTP API receives<a id="_idIndexMarker871"/> the HTTP request and invokes the SageMaker ML inference endpoint directly using the <strong class="bold">API Gateway mapping templates</strong> (without the usage of Lambda functions).</li>
<li><em class="italic">Option 4</em>: <em class="italic">Custom container-based web application using a web framework (for example, Flask or Django) inside an EC2 instance + boto3 + SageMaker ML inference endpoint</em> – The web application (running inside a container in an <strong class="bold">EC2</strong> instance) receives the HTTP request<a id="_idIndexMarker872"/> and uses the <strong class="source-inline">boto3</strong> library to invoke the SageMaker ML Inference endpoint.</li>
<li><em class="italic">Option 5</em>: <em class="italic">Custom container-based web application using a web framework (for example, Flask or Django) inside an Elastic Container Service (ECS) + boto3 + SageMaker ML inference endpoint</em> – The web application (running inside a container using the <strong class="bold">Amazon Elastic Container Service</strong>) receives the HTTP request and uses<a id="_idIndexMarker873"/> the <strong class="source-inline">boto3</strong> library to invoke the SageMaker ML inference endpoint.</li>
<li><em class="italic">Option 6</em>: <em class="italic">Custom container-based web application using a web framework (for example, Flask or Django) with Elastic Kubernetes Service (EKS) + boto3 + SageMaker ML inference endpoint</em> – The web application (running inside an <strong class="bold">Amazon Elastic Kubernetes Service</strong> cluster) receives the HTTP request<a id="_idIndexMarker874"/> and uses the <strong class="source-inline">boto3</strong> library to invoke the SageMaker ML inference endpoint.</li>
<li><em class="italic">Option 7</em>: <em class="italic">AWS AppSync (GraphQL API) + AWS Lambda function + boto3 + SageMaker ML inference endpoint</em> – The <strong class="bold">AWS AppSync</strong> API receives the HTTP request and invokes<a id="_idIndexMarker875"/> the Lambda function, which uses the <strong class="source-inline">boto3</strong> library to invoke the SageMaker ML inference endpoint.</li>
</ul>
<p>Note that this is not an exhaustive<a id="_idIndexMarker876"/> list and there are definitely other ways to set up an HTTP API<a id="_idIndexMarker877"/> invoking an existing SageMaker inference endpoint. Of course, there are scenarios as well where we would want to invoke an existing inference endpoint directly from another AWS service resource. This would mean that we no longer need to prepare a separate HTTP API that serves as a middleman between the two services.</p>
<p>It is important<a id="_idIndexMarker878"/> to note <a id="_idIndexMarker879"/>that we can also invoke a SageMaker inference<a id="_idIndexMarker880"/> endpoint directly from <strong class="bold">Amazon Aurora</strong>, <strong class="bold">Amazon Athena</strong>, <strong class="bold">Amazon Quicksight</strong>, or <strong class="bold">Amazon Redshift</strong>. In <a href="B18638_04.xhtml#_idTextAnchor079"><em class="italic">Chapter 4</em></a>, <em class="italic">Serverless Data Management on AWS</em>, we used Redshift<a id="_idIndexMarker881"/> and Athena to query our data. In addition to the database queries already available using these services, we can perform ML inference directly using a syntax similar to that in the following block of code (a sample query for Athena):</p>
<pre class="source-code">USING EXTERNAL FUNCTION <strong class="bold">function_name</strong>(value INT)
RETURNS DOUBLE
SAGEMAKER '<strong class="bold">&lt;INSERT EXISTING ENDPOINT NAME&gt;</strong>'
SELECT label, value, <strong class="bold">function_name</strong>(value) AS alias
FROM athena_db.athena_table</pre>
<p>Here, we define and use a custom function that invokes an existing SageMaker inference endpoint<a id="_idIndexMarker882"/> for prediction when using<a id="_idIndexMarker883"/> Amazon Athena. For more<a id="_idIndexMarker884"/> information, feel free to check out the following resources and links: </p>
<ul>
<li><strong class="bold">Amazon Athena</strong> + <strong class="bold">Amazon SageMaker</strong>: <a href="https://docs.aws.amazon.com/athena/latest/ug/querying-mlmodel.xhtml">https://docs.aws.amazon.com/athena/latest/ug/querying-mlmodel.xhtml</a>.</li>
<li><strong class="bold">Amazon Redshift</strong> + <strong class="bold">Amazon SageMaker</strong>: <a href="https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.xhtml">https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.xhtml</a>.</li>
<li><strong class="bold">Amazon Aurora</strong> + <strong class="bold">Amazon SageMaker</strong>: <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.xhtml">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-ml.xhtml</a>.</li>
<li><strong class="bold">Amazon QuickSight</strong> + <strong class="bold">Amazon SageMaker</strong>: <a href="https://docs.aws.amazon.com/quicksight/latest/user/sagemaker-integration.xhtml">https://docs.aws.amazon.com/quicksight/latest/user/sagemaker-integration.xhtml</a>.</li>
</ul>
<p>If we want to deploy<a id="_idIndexMarker885"/> a model outside of the SageMaker hosting services, we<a id="_idIndexMarker886"/> can do that as well. For example, we can train our model using SageMaker and then download the <strong class="source-inline">model.tar.gz</strong> file from the S3 bucket containing the model artifact files generated during the training process. The model artifact files generated can be deployed outside of SageMaker, similar to how we deployed and invoked the model in <a href="B18638_02.xhtml#_idTextAnchor041"><em class="italic">Chapter 2</em></a>, <em class="italic">Deep Learning AMIs</em>, and <a href="B18638_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Deep Learning Containers</em>. At this point, you might ask yourself: why deploy ML models using the SageMaker hosting services? Here’s a quick list of things you can easily perform and set up if you were to deploy ML models in the SageMaker hosting services:</p>
<ul>
<li>Setting up automatic scaling (<strong class="bold">autoscaling</strong>) of the infrastructure resources (ML instances) used to host<a id="_idIndexMarker887"/> the ML model. Autoscaling automatically adds new ML instances when the traffic or workload increases and reduces the number of provisioned ML instances once the traffic or workload decreases.</li>
<li>Deploying multiple<a id="_idIndexMarker888"/> ML models in a single inference endpoint using the <strong class="bold">multi-model endpoint</strong> (<strong class="bold">MME</strong>) and <strong class="bold">multi-container endpoint</strong> (<strong class="bold">MCE</strong>) support of SageMaker. It is<a id="_idIndexMarker889"/> also possible to set up a <strong class="bold">serial inference pipeline</strong> behind a single endpoint<a id="_idIndexMarker890"/> that involves a sequence of containers (for example, pre-processing, prediction and post-processing) used to process ML inference requests. </li>
<li>Setting up <strong class="bold">A/B testing</strong> of ML models by distributing traffic<a id="_idIndexMarker891"/> to multiple variants under a single inference endpoint.</li>
<li>Setting up automated model monitoring and monitor (1) data quality, (2) model quality, (3) bias drift, and (4) feature attribution drift with just a few lines of code using the SageMaker Python SDK. We will dive deeper into model monitoring in <a href="B18638_08.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Monitoring and Management Solutions</em>.</li>
<li>Using <strong class="bold">Elastic Inference</strong> when deploying models to add<a id="_idIndexMarker892"/> inference acceleration to the SageMaker inference endpoint to improve throughput and decrease latency.</li>
<li>Using a variety of traffic shifting modes when performing blue/green deployments when updating the deployed model. We can use the <strong class="bold">All-at-once</strong> traffic shifting mode<a id="_idIndexMarker893"/> if we want to shift all the traffic from the old setup to the new setup all in one go. We can use the <strong class="bold">Canary</strong> traffic shifting mode if we want to shift<a id="_idIndexMarker894"/> the traffic from the old setup to the new setup in two steps. This involves only shifting a portion of the traffic in the first shift and shifting the remainder of the traffic in the second shift. Finally, we can use the <strong class="bold">Linear</strong> traffic shifting mode to iteratively shift<a id="_idIndexMarker895"/> the traffic from the old setup to the new setup in a predetermined number of steps. </li>
<li>Setting up <strong class="bold">CloudWatch</strong> alarms along with the<a id="_idIndexMarker896"/> SageMaker auto-rollback configuration to automate the deployment rollback process. </li>
</ul>
<p>All of these are relatively<a id="_idIndexMarker897"/> easy to set up if we are to use SageMaker for model<a id="_idIndexMarker898"/> deployment. When using these features and capabilities, all we would need to worry about would be the configuration step, as a big portion of the work has already been automated by SageMaker.</p>
<p>So far, we’ve been talking about the different options and solutions when deploying ML models in the cloud. Before ending this section, let’s quickly<a id="_idIndexMarker899"/> discuss ML model deployments on <strong class="bold">edge devices</strong> such as mobile devices and smart cameras. There are several advantages to this approach, including real-time prediction latency, privacy preservation, and cost reduction associated with network connectivity. Of course, there are challenges when running and managing ML models on edge devices due to the resource limitations<a id="_idIndexMarker900"/> involved such as compute and memory. These challenges can be solved with <strong class="bold">SageMaker Edge Manager</strong>, which is a capability that makes<a id="_idIndexMarker901"/> use of several other<a id="_idIndexMarker902"/> services, capabilities, and features (such as <strong class="bold">SageMaker Neo</strong>, <strong class="bold">IoT Greengrass</strong>, and <strong class="bold">SageMaker Model Monitor</strong>) when optimizing, running, monitoring, and updating ML<a id="_idIndexMarker903"/> models on edge devices. We won’t dive any deeper<a id="_idIndexMarker904"/> into the details so feel free to check out https://docs.aws.amazon.com/sagemaker/latest/dg/edge.xhtml for more information about this topic.</p>
<h1 id="_idParaDest-158"><a id="_idTextAnchor168"/>Summary</h1>
<p>In this chapter, we discussed and focused on several deployment options and solutions using SageMaker. We deployed a pre-trained model into three different types of inference endpoints – (1) a real-time inference endpoint, (2) a serverless inference endpoint, and (3) an asynchronous inference endpoint. We also discussed the differences of each approach, along with when each option is best used when deploying ML models. Toward the end of this chapter, we talked about some of the deployment strategies, along with the best practices when using SageMaker for model deployments.</p>
<p>In the next chapter, we will dive deeper into <strong class="bold">SageMaker Model Registry</strong> and <strong class="bold">SageMaker Model Monitor</strong>, which are capabilities of SageMaker that can help us manage and monitor our models in production.</p>
<h1 id="_idParaDest-159"><a id="_idTextAnchor169"/>Further reading</h1>
<p>For more information on the topics covered in this chapter, feel free to check out the following resources:</p>
<ul>
<li><em class="italic">The Hugging Face DistilBERT model</em> (<a href="https://huggingface.co/docs/transformers/model_doc/distilbert">https://huggingface.co/docs/transformers/model_doc/distilbert</a>)</li>
<li><em class="italic">SageMaker – Deploying Models for Inference</em> (https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.xhtml)</li>
<li><em class="italic">SageMaker – Inference Recommender</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.xhtml</a>)</li>
<li><em class="italic">SageMaker – Deployment guardrails</em> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/deployment-guardrails.xhtml</a>)</li>
</ul>
</div>
<div>
<div id="_idContainer255">
</div>
</div>
</div>


<div id="sbo-rt-content"><div>
<div class="Basic-Graphics-Frame" id="_idContainer256">
</div>
</div>
<div class="Content" id="_idContainer257">
<h1 id="_idParaDest-160"><a id="_idTextAnchor170"/>Part 4:<a id="_idTextAnchor171"/>Securing, Monitoring, and Managing Machine Learning Systems and Environments</h1>
<p>In this section, readers will learn how to properly secure, monitor, and manage production ML systems and deployed models.</p>
<p>This section comprises the following chapters:</p>
<ul>
<li><a href="B18638_08.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Monitoring and Management Solutions</em></li>
<li><a href="B18638_09.xhtml#_idTextAnchor187"><em class="italic">Chapter 9</em></a>, <em class="italic">Security, Governance, and Compliance Strategies</em></li>
</ul>
</div>
<div>
<div id="_idContainer258">
</div>
</div>
</div>
</body></html>