- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Navigating the ML Lifecycle with ML Solutions Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of **artificial intelligence** (**AI**) and **machine learning** (**ML**)
    has had a long history. Over the last 70+ years, ML has evolved from checker game-playing
    computer programs in the 1950s to advanced AI capable of beating the human world
    champion in the game of *Go*. More recently, **Generative AI** (**GenAI**) technology
    such as ChatGPT has been taking the industry by storm, generating huge interest
    among company executives and consumers alike, promising new ways to transform
    businesses such as drug discovery, new media content, financial report analysis,
    and consumer product design. Along the way, the technology infrastructure for
    ML has also evolved from a single machine/server for small experiments and models
    to highly complex end-to-end ML platforms capable of training, managing, and deploying
    tens of thousands of ML models. The hyper-growth in the AI/ML field has resulted
    in the creation of many new professional roles, such as **MLOps** **engineering**,
    **AI/ML product management**, **ML software engineering**, **AI risk manager**,
    and **AI strategist** across a range of industries.
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine learning solutions architecture** (**ML solutions architecture**)
    is another relatively new discipline that is playing an increasingly critical
    role in the full end-to-end ML lifecycle as ML projects become increasingly complex
    in terms of *business impact*, *science sophistication*, and the *technology landscape*.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will help you understand where ML solutions architecture fits in
    the full data science lifecycle. We will discuss the different steps it will take
    to get an ML project from the ideation stage to production and the challenges
    faced by organizations, such as use case identification, data quality issues,
    and shortage of ML talent when implementing an ML initiative. Finally, we will
    finish the chapter by briefly discussing the core focus areas of ML solutions
    architecture, including system architecture, workflow automation, and security
    and compliance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: ML versus traditional software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ML lifecycle and its key challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is ML solutions architecture, and where does it fit in the overall lifecycle?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upon completing this chapter, you will understand the role of an ML solutions
    architect and what business and technology areas you need to focus on to support
    end-to-end ML initiatives. The intent of this chapter is to offer a fundamental
    introduction to the ML lifecycle for those in the early stages of their exploration
    in the field. Experienced ML practitioners may wish to skip this foundational
    overview and proceed directly to more advanced content.
  prefs: []
  type: TYPE_NORMAL
- en: The more advanced section commences in *Chapter 4*; however, many technical
    practitioners may find *Chapter 2* helpful, as numerous technical practitioners
    often need more business understanding of where ML can be applied in different
    businesses and workflows. Additionally, *Chapter 3*, could prove beneficial for
    certain practitioners, as it provides an introduction to ML algorithms for those
    new to this topic and can also serve as a refresher for those practicing these
    concepts regularly.
  prefs: []
  type: TYPE_NORMAL
- en: ML versus traditional software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before I started working in the field of AI/ML, I spent many years building
    computer software platforms for large financial services institutions. Some of
    the business problems I worked on had complex rules, such as identifying companies
    for comparable analysis for investment banking deals or creating a master database
    for all the different companies’ identifiers from the different data providers.
    We had to implement hardcoded rules in database-stored procedures and application
    server backends to solve these problems. We often debated if certain rules made
    sense or not for the business problems we tried to solve.
  prefs: []
  type: TYPE_NORMAL
- en: As rules changed, we had to reimplement the rules and make sure the changes
    did not break anything. To test for new releases or changes, we often replied
    to human experts to exhaustively test and validate all the business logic implemented
    before the production release. It was a very time-consuming and error-prone process
    and required a significant amount of engineering, testing against the documented
    specification, and rigorous change management for deployment every time new rules
    were introduced, or existing rules needed to be changed. We often replied to users
    to report business logic issues in production, and when an issue was reported
    in production, we sometimes had to open up the source code to troubleshoot or
    explain the logic of how it worked. I remember I often asked myself if there were
    better ways to do this.
  prefs: []
  type: TYPE_NORMAL
- en: After I started working in the field of AI/ML, I started to solve many similar
    challenges using ML techniques. With ML, I did not need to come up with complex
    rules that often require deep data and domain expertise to create or maintain
    the complex rules for decision making. Instead, I focused on collecting high-quality
    data and used ML algorithms to learn the rules and patterns from the data directly.
    This new approach eliminated many of the challenging aspects of creating new rules
    (for example, a deep domain expertise requirement, or avoiding human bias) and
    maintaining existing rules. To validate the model before the production release,
    we could examine model performance metrics such as **accuracy**. While it still
    required data science expertise to interpret the model metrics against the nature
    of the business problems and dataset, it did not require exhaustive manual testing
    of all the different scenarios. When a model was deployed into production, we
    would monitor if the model performed as expected by monitoring any significant
    changes in production data versus the data we have collected for model training.
    We would collect new unseen data and labels for production data and test the model
    performance periodically to ensure that its predictive accuracy remains robust
    when faced with new, previously unseen production data. To explain why a model
    made a decision the way it did, we did not need to open up the source code to
    re-examine the hardcoded logic. Instead, we would rely on ML techniques to help
    explain the relative importance of different input features to understand what
    factors were most influential in the decision-making by the ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows a graphical view of the process differences between
    developing a piece of software and training an ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: ML and computer software'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the difference between ML and traditional software, it is
    time to dive deep into understanding the different stages in an ML lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: ML lifecycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the early ML projects that I worked on was a fascinating yet daunting
    sports predictive analytics problem for a major league brand. I was given a list
    of predictive analytics outcomes to think about to see if there were ML solutions
    for the problems. I was a casual viewer of the sport; I didn’t know anything about
    the analytics to be generated, nor the rules of the games in the detail that was
    needed. I was provided with some sample data but had no idea what to do with it.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing I started to work on was an immersion in the sport itself. I
    delved into the intricacies of the game, studying the different player positions
    and events that make up each game and play. Only after being armed with the newfound
    domain knowledge did the data start to make sense. Together with the stakeholder,
    we evaluated the impact of the different analytics outcomes and assessed the modeling
    feasibility based on the data we had. With a clear understanding of the data,
    we came up with a couple of top ML analytics with the most business impact to
    focus on. We also decided how they would be integrated into the existing business
    workflow, and how they would be measured on their impacts.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, I delved deeper into the data to ascertain what information was
    available and what was lacking. The raw dataset had a lot of irrelevant data points
    that needed to be removed while the relevant data points needed to be transformed
    to provide the strongest signals for model training. I processed and prepared
    the dataset based on a few of the ML algorithms I had considered and conducted
    experiments to determine the best approach. I lacked a tool to track the different
    experiment results, so I had to document what I had done manually. After some
    initial rounds of experimentation, it became evident that the existing data was
    not sufficient to train a high-performance model. Hence, I decided to build a
    custom deep learning model to incorporate data of different modalities as the
    data points had temporal dependencies and required additional spatial information
    for the modeling. The data owner was able to provide the additional datasets I
    required, and after more experiments with custom algorithms and significant data
    preparations and feature engineering, I eventually trained a model that met the
    business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: After completing the model, another hard challenge began – deploying and operationalizing
    the model in production and integrating it into the existing business workflow
    and system architecture. We engaged in many architecture and engineering discussions
    and eventually built out a deployment architecture for the model.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from my personal experience, the journey from business idea to
    ML production deployment involved many steps. A typical lifecycle of an ML project
    follows a formal structure, which includes several essential stages like business
    understanding, data acquisition and understanding, data preparation, model building,
    model evaluation, and model deployment. Since a big component of the lifecycle
    is experimentation with different datasets, features, and algorithms, the whole
    process is highly iterative. Furthermore, it is essential to note that there is
    no guarantee of a successful outcome. Factors such as the availability and quality
    of data, feature engineering techniques (the process of using domain knowledge
    to extract useful features from raw data), and the capability of the learning
    algorithms, among others, can all affect the final results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: ML lifecycle'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure illustrates the key steps in ML projects, and in the subsequent
    sections, we will delve into each of these steps in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Business problem understanding and ML problem framing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first stage in the lifecycle is **business understanding**. This stage
    involves the understanding of the business goals and defining business metrics
    that can measure the project’s success. For example, the following are some examples
    of business goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Cost reduction for operational processes, such as document processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigation of business or operational risks, such as fraud and compliance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product or service revenue improvements, such as better target marketing, new
    insight generation for better decision making, and increased customer satisfaction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To measure the success, you may use specific business metrics such as the number
    of hours reduced in a business process, an increased number of true positive frauds
    detected, a conversion rate improvement from target marketing, or the number of
    churn rate reductions. This is an essential step to get right to ensure there
    is sufficient justification for an ML project and that the outcome of the project
    can be successfully measured.
  prefs: []
  type: TYPE_NORMAL
- en: After you have defined the business goals and business metrics, you need to
    evaluate if there is an ML solution for the business problem. While ML has a wide
    scope of applications, it is not always an optimal solution for every business
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The saying that “data is the new oil” holds particularly true for ML. Without
    the required data, you cannot move forward with an ML project. That’s why the
    next step in the ML lifecycle is **data acquisition**, **understanding**, and
    **preparation**.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the business problems and ML approach, you will need to gather and
    comprehend the available data to determine if you have the right data and data
    volume to solve the ML problem. For example, suppose the business problem to address
    is credit card fraud detection. In that case, you will need datasets such as historical
    credit card transaction data, customer demographics, account data, device usage
    data, and networking access data. Detailed data analysis is then necessary to
    determine if the dataset features and quality are sufficient for the modeling
    tasks. You also need to decide if the data needs labeling, such as `fraud` or
    `not-fraud`. During this step, depending on the data quality, a significant amount
    of data wrangling might be performed to prepare and clean the data and to generate
    the dataset for model training and model evaluation, depending on the data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the training and validation datasets established, a data scientist must
    run a number of experiments using different ML algorithms and dataset features
    for feature selection and model development. This is a highly iterative process
    and could require numerous runs of data processing and model development to find
    the right algorithm and dataset combination for optimal model performance. In
    addition to model performance, factors such as data bias and model explainability
    may need to be considered to comply with internal or regulatory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to deployment into production, the model quality must be validated using
    the relevant technical metrics, such as the **accuracy score**. This is usually
    accomplished using a **holdout dataset**, also known as a **test dataset**, to
    gauge how the model performs on unseen data. It is crucial to understand which
    metrics are appropriate for model validation, as they vary depending on the ML
    problems and the dataset used. For example, model accuracy would be a suitable
    validation metric for a document classification use case if the number of document
    types is relatively balanced. However, model accuracy would not be a good metric
    to evaluate the model performance for a fraud detection use case – this is because
    the number of frauds is small and even if the model predicts `not-fraud` all the
    time, the model accuracy could still be very high.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the model is fully trained and validated to meet the expected performance
    metric, it can be deployed into production and the business workflow. There are
    two main deployment concepts here. The first involves the deployment of the model
    itself to be used by a client application to generate predictions. The second
    concept is to integrate this prediction workflow into a business workflow application.
    For example, deploying the credit fraud model would either host the model behind
    an API for real-time prediction or as a package that can be loaded dynamically
    to support batch predictions. Moreover, this prediction workflow also needs to
    be integrated into business workflow applications for fraud detection, which might
    include the fraud detection of real-time transactions, decision automation based
    on prediction output, and fraud detection analytics for detailed fraud analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Model monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ML lifecycle does not end with model deployment. Unlike software, whose
    behavior is highly deterministic since developers explicitly code its logic, an
    ML model could behave differently in production from its behavior in model training
    and validation. This could be caused by changes in the production data characteristics,
    data distribution, or the potential manipulation of request data. Therefore, model
    monitoring is an important post-deployment step for detecting model performance
    degradation (a.k.a model drift) or dataset distribution change in the production
    environment (a.k.a data drift).
  prefs: []
  type: TYPE_NORMAL
- en: Business metric tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The actual business impact should be tracked and measured as an ongoing process
    to ensure the model delivers the expected business benefits. This may involve
    comparing the business metrics before and after the model deployment, or A/B testing
    where a business metric is compared between workflows with or without the ML model.
    If the model does not deliver the expected benefits, it should be re-evaluated
    for improvement opportunities. This could also mean framing the business problem
    as a different ML problem. For example, if churn prediction does not help improve
    customer satisfaction, then consider a personalized product/service offering to
    solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: ML challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the years, I have worked on many real-world problems using ML solutions
    and encountered different challenges faced by different industries during ML adoptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'I often get the same question when working on ML projects: *We have a lot of
    data – can you help us figure out what insights we can generate using ML?* I refer
    to companies with this question as having a *business use case challenge*. Not
    being able to identify business use cases for ML is a very big hurdle for many
    companies. Without a properly identified business problem and its value proposition
    and benefit, it becomes difficult to initiate an ML project.'
  prefs: []
  type: TYPE_NORMAL
- en: In my conversations with different companies across their industries, data-related
    challenges emerge as a frequent issue. This includes data quality, data inventory,
    data accessibility, data governance, and data availability. This problem affects
    both data-poor and data-rich companies and is often exacerbated by data silos,
    data security, and industry regulations.
  prefs: []
  type: TYPE_NORMAL
- en: The shortage of data science and ML talent is another major challenge I have
    heard from many companies. Companies, in general, are having a tough time attracting
    and retaining top ML talents, which is a common problem across all industries.
    As ML platforms become more complex and the scope of ML projects increases, the
    need for other ML-related functions starts to surface. Nowadays, in addition to
    just data scientists, an organization would also need functional roles for ML
    product management, ML infrastructure engineering, and ML operations management.
  prefs: []
  type: TYPE_NORMAL
- en: Based on my experiences, I have observed that cultural acceptance of ML-based
    solutions is another significant challenge for broad adoption. There are individuals
    who perceive ML as a threat to their job functions, and their lack of knowledge
    in ML makes them hesitant to adopt these new methods in their business workflows.
  prefs: []
  type: TYPE_NORMAL
- en: The practice of ML solutions architecture aims to help solve some of the challenges
    in ML. In the next section, we will explore ML solutions architecture and its
    role in the ML lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: ML solutions architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When I initially worked with companies as an ML solutions architect, the landscape
    was quite different from what it is now. The focus was mainly on data science
    and modeling, and the problems at hand were small in scope. Back then, most of
    the problems could be solved using simple ML techniques. The datasets were small,
    and the infrastructure required was not too demanding. The scope of the ML initiative
    at these companies was limited to a few data scientists or teams. As an ML architect
    at that time, I primarily needed to have solid data science skills and general
    cloud architecture knowledge to get the job done.
  prefs: []
  type: TYPE_NORMAL
- en: In more recent years, the landscape of ML initiatives has become more intricate
    and multifaceted, necessitating involvement from a broader range of functions
    and personas at companies. My engagement has expanded to include discussions with
    business executives about ML strategies and organizational design to facilitate
    the broad adoption of AI/ML throughout their enterprises. I have been tasked with
    designing more complex ML platforms, utilizing a diverse range of technologies
    for large enterprises to meet stringent security and compliance requirements.
    ML workflow orchestration and operations have become increasingly crucial topics
    of discussion, and more and more companies are looking to train large ML models
    with enormous amounts of training data. The number of ML models trained and deployed
    by some companies has skyrocketed to tens of thousands from a few dozen models
    in just a few years. Furthermore, sophisticated and security-sensitive customers
    have sought guidance on topics such as ML privacy, model explainability, and data
    and model bias. As an ML solutions architect, I’ve noticed that the skills and
    knowledge required to be successful in this role have evolved significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Trying to navigate the complexities of a business, data, science, and technology
    landscape can be a daunting task. As an ML solutions architect, I have seen firsthand
    the challenges that companies face in bringing all these pieces together. In my
    view, ML solutions architecture is an essential discipline that serves as a bridge
    connecting the different components of an ML initiative. Drawing on my years of
    experience working with companies of all sizes and across diverse industries,
    I believe that an ML solutions architect plays a pivotal role in identifying business
    needs, developing ML solutions to address these needs, and designing the technology
    platforms necessary to run these solutions. By collaborating with various business
    and technology partners, an ML solutions architect can help companies unlock the
    full potential of their data and realize tangible benefits from their ML initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates the core functional areas covered by the ML
    solutions architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: ML solutions architecture coverage'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, we will explore each of these areas in greater detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business understanding**: Business problem understanding and transformation
    using AI and ML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identification and verification of ML techniques**: Identification and verification
    of ML techniques for solving specific ML problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System architecture of the ML technology platform**: System architecture
    design and implementation of the ML technology platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLOps**: ML platform automation technical design.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and compliance**: Security, compliance, and audit considerations
    for the ML platform and ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding and ML transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of the business workflow analysis is to identify inefficiencies in
    the workflows and determine if ML can be applied to help eliminate pain points,
    improve efficiency, or even create new revenue opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Picture this: you are tasked with improving a call center’s operations. You
    know there are inefficiencies that need to be addressed, but you’re not sure where
    to start. That’s where business workflow analysis comes in. By analyzing the call
    center’s workflows, you can identify pain points such as long customer wait times,
    knowledge gaps among agents, and the inability to extract customer insights from
    call recordings. Once you have identified these issues, you can determine what
    data is available and which business metrics need to be improved. This is where
    ML comes in. You can use ML to create virtual assistants for common customer inquiries,
    transcribe audio recordings to allow for text analysis, and detect customer intent
    for product cross-sell and up-sell. But sometimes, you need to modify the business
    process to incorporate ML solutions. For example, if you want to use call recording
    analytics to generate insights for cross-selling or up-selling products, but there’s
    no established process to act on those insights, you may need to introduce an
    automated target marketing process or a proactive outreach process by the sales
    team.'
  prefs: []
  type: TYPE_NORMAL
- en: Identification and verification of ML techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have come up with a list of ML options, the next step is to determine
    if the assumption behind the ML approach is valid. This could involve conducting
    a simple **proof of concept** (**POC**) modeling to validate the available dataset
    and modeling approach, or technology POC using pre-built AI services, or testing
    of ML frameworks. For example, you might want to test the feasibility of text
    transcription from audio files using an existing text transcription service or
    build a customer propensity model for a new product conversion from a marketing
    campaign.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that ML solutions architecture does not focus on developing
    new machine algorithms, a job best suited for applied data scientists or research
    data scientists. Instead, ML solutions architecture focuses on identifying and
    applying ML algorithms to address a range of ML problems such as predictive analytics,
    computer vision, or natural language processing. Also, the goal of any modeling
    task here is not to build production-quality models but rather to validate the
    approach for further experimentations by full-time applied data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: System architecture design and implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important aspect of the ML solutions architect’s role is the technical
    architecture design of the ML platform. The platform will need to provide the
    technical capability to support the different phases of the ML cycle and personas,
    such as data scientists and operations engineers. Specifically, an ML platform
    needs to have the following core functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data explorations and experimentation**: Data scientists use ML platforms
    for data exploration, experimentation, model building, and model evaluation. ML
    platforms need to provide capabilities such as data science development tools
    for model authoring and experimentation, data wrangling tools for data exploration
    and wrangling, source code control for code management, and a package repository
    for library package management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data management and large-scale data processing**: Data scientists or data
    engineers will need the technical capability to ingest, store, access, and process
    large amounts of data for cleansing, transformation, and feature engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training infrastructure management**: ML platforms will need to provide
    model training infrastructure for different modeling training using different
    types of computing resources, storage, and networking configurations. It also
    needs to support different types of ML libraries or frameworks, such as **scikit-learn**,
    **TensorFlow**, and **PyTorch**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model hosting/serving**: ML platforms will need to provide the technical
    capability to host and serve the model for prediction generations, for real-time,
    batch, or both.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model management**: Trained ML models will need to be managed and tracked
    for easy access and lookup, with relevant metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature management**: Common and reusable features will need to be managed
    and served for model training and model serving purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML platform workflow automation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A key aspect of ML platform design is **workflow automation** and **continuous
    integration**/**continuous deployment** (**CI**/**CD**), also known as MLOps.
    ML is a multi-step workflow – it needs to be automated, which includes data processing,
    model training, model validation, and model hosting. Infrastructure provisioning
    automation and self-service is another aspect of automation design. Key components
    of workflow automation include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline design and management**: The ability to create different automation
    pipelines for various tasks, such as model training and model hosting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline execution and monitoring**: The ability to run different pipelines
    and monitor the pipeline execution status for the entire pipeline and each of
    the steps in the ML cycle such as data processing and model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model monitoring configuration**: The ability to monitor the model in production
    for various metrics, such as data drift (where the distribution of data used in
    production deviates from the distribution of data used for model training), model
    drift (where the performance of the model degrades in the production compared
    with training results), and bias detection (the ML model replicating or amplifying
    bias towards certain individuals).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security and compliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another important aspect of ML solutions architecture is the security and compliance
    consideration in a sensitive or enterprise setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Authentication and authorization**: The ML platform needs to provide authentication
    and authorization mechanisms to manage access to the platform and different resources
    and services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network security**: The ML platform needs to be configured for different
    network security controls such as a firewall and an IP address access allowlist
    to prevent unauthorized access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data encryption**: For security-sensitive organizations, data encryption
    is another important aspect of the design consideration for the ML platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audit and compliance**: Audit and compliance staff need the information to
    help them understand how decisions are made by the predictive models if required,
    the lineage of a model from data to model artifacts, and any bias exhibited in
    the data and model. The ML platform will need to provide model explainability,
    bias detection, and model traceability across the various datastore and service
    components, among other capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various industry technology providers have established best practices to guide
    the design and implementation of ML infrastructure, which is part of the ML solutions
    architect’s practices. Amazon Web Services, for example, created *Machine Learning
    Lens* to provide architectural best practices across crucial domains like operational
    excellence, security, reliability, performance, cost optimization, and sustainability.
    Following these published guidelines can help practitioners implement robust and
    effective ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, I have shared some of my personal experience as an ML solutions
    architect and provided an overview of core concepts and components involved in
    the ML lifecycle. We discussed the key responsibilities of the ML solutions architect
    role throughout the lifecycle. This chapter aimed to give you an understanding
    of the technical and business domains required to work effectively as an ML solutions
    architect. With this foundational knowledge, you should now have an appreciation
    for the breadth of this role and its integral part in delivering successful ML
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we will dive into various ML use cases across different
    industries, such as financial services and media and entertainment, to gain further
    insights into the practical applications of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code70205728346636561.png)'
  prefs: []
  type: TYPE_IMG
