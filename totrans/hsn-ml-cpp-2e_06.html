<html><head></head><body>
		<div id="_idContainer445">
			<h1 class="chapter-number" id="_idParaDest-129"><a id="_idTextAnchor301"/>6</h1>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor302"/>Dimensionality Reduction</h1>
			<p>In this chapter, we’ll go through a number of dimension-reduction tasks. We’ll look at the conditions in which dimension-reduction is required and learn how to use dimension-reduction algorithms efficiently in C++ with various libraries. Dimensionality reduction involves transforming high-dimensional data into a new representation with fewer dimensions while preserving the most crucial information from the original data. Such a transformation can help us visualize multidimensional space, which can be useful in the data exploration stage or when identifying the most relevant features in dataset samples. Some <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) techniques can perform better or faster if our data has a smaller number of features since it can consume fewer computational resources. The main purpose of this kind of transformation is to save the essential features—those features that hold the most critical information present in the <span class="No-Break">original data.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>An overview of <span class="No-Break">dimension-reduction methods</span></li>
				<li>Exploring linear methods <span class="No-Break">for dimension-reduction</span></li>
				<li>Exploring non-linear methods <span class="No-Break">for dimension-reduction</span></li>
				<li>Understanding dimension-reduction algorithms with various <span class="No-Break">С++ libraries</span><a id="_idTextAnchor303"/></li>
			</ul>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor304"/>Technical requirements</h1>
			<p>The technologies you’ll need for this chapter are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <span class="No-Break">Tapkee library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library</span></li>
				<li>The <span class="No-Break"><strong class="source-inline">plotcpp</strong></span><span class="No-Break"> library</span></li>
				<li>A modern C++ compiler with <span class="No-Break">C++20 support</span></li>
				<li>The CMake build system, version &gt;= <span class="No-Break">3.24</span></li>
			</ul>
			<p>The code files for this chapter can be found at the following GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter06"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter06</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor305"/><a id="_idTextAnchor306"/>An overview of dimension-reduction methods</h1>
			<p>The main goal of <a id="_idIndexMarker670"/>dimension-reduction methods is to make the dimension of the transformed representation correspond with the internal dimension of the data. In other words, it should be similar to the minimum number of variables necessary to express all the possible properties of the data. Reducing the dimension helps mitigate the impact of the curse of dimensionality and other undesirable properties that occur in high-dimensional spaces. As a result, reducing dimensionality can effectively solve problems regarding classification, visualization, and compressing high-dimensional data. It makes sense to apply dimensionality reduction only when particular data is redundant; otherwise, we can lose important information. In other words, if we are able to solve the problem using data of smaller dimensions with the same level of efficiency and accuracy, then some of our data is redundant. Dimensionality reduction allows us to reduce the time and computational costs of solving a problem. It also makes data and the results of data analysis easier <span class="No-Break">to interpret.</span></p>
			<p>It makes sense to reduce the number of features when the information that can be used to solve the problem at hand qualitatively is contained in a specific subset of features. Non-informative features are a source of additional noise and affect the accuracy of the model parameter’s estimation. In addition, datasets with a large number of features can contain groups of correlated variables. The presence of such feature groups leads to the duplication of information, which may distort the model’s results and affect how well it estimates the values of <span class="No-Break">its parameters.</span></p>
			<p>The methods surrounding dimensionality reduction are mainly unsupervised because we don’t know which features or variables can be excluded from the original dataset without losing the most <span class="No-Break">crucial information.</span></p>
			<p>Some real-life examples of dimensionality reduction include <span class="No-Break">the following:</span></p>
			<ul>
				<li>In recommender systems, dimensionality reduction can be used to represent users and items as vectors in a lower-dimensional space, making it easier to find similar users <span class="No-Break">or items</span></li>
				<li>In image recognition, dimensionality reduction techniques, such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>), can be applied to reduce the size of images while preserving their <span class="No-Break">important features</span></li>
				<li>In text analysis, dimensionality reduction can be employed to transform large collections of documents into lower-dimensional representations that capture the main topics<a id="_idIndexMarker671"/> discussed in <span class="No-Break">the documents</span></li>
			</ul>
			<p>Dimensionality reduction methods can be classified into two groups: feature selection and the creation of new low-dimensional features. These methods can then be subdivided into <strong class="bold">linear</strong> and <strong class="bold">non-linear</strong> approaches, depending on the nature of the data and the mathematical apparatus <span class="No-Break">being use<a id="_idTextAnchor307"/>d<a id="_idTextAnchor308"/>.</span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor309"/>Feature selection methods</h2>
			<p>Feature selection methods don’t change the initial values of the variables or features; instead, they remove the irrelevant features<a id="_idIndexMarker672"/> from the source dataset. Some of the feature selection methods we can use are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Missing value ratio</strong>: This method is based on the idea that a feature that misses many values should be eliminated from a dataset because it doesn’t contain valuable information and can distort the model’s performance results. So, if we have some criteria for identifying missing values, we can calculate their ratio to typical values and set a threshold that we can use to eliminate features with a high missing <span class="No-Break">value ratio.</span></li>
				<li><strong class="bold">Low variance filter</strong>: This method is used to remove features with low variance because such features don’t contain enough information to improve model performance. To apply this method, we need to calculate the variance for each feature, sort them in ascending order by this value, and leave only those with the highest <span class="No-Break">variance values.</span></li>
				<li><strong class="bold">High correlation filter</strong>: This method is based on the idea that if two features have a high correlation, then they carry similar information. Also, highly correlated features can significantly reduce the performance of some ML models, such as linear and logistic regression. Therefore, the primary goal of this method is to leave only the features that have a high correlation with target values and don’t have much correlation with <span class="No-Break">each other.</span></li>
				<li><strong class="bold">Random forest</strong>: This method can be used for feature selection effectively (although it wasn’t initially designed for this kind of task). After we’ve built the forest, we can estimate <a id="_idIndexMarker673"/>what features are most important by estimating the impurity factor in the tree’s nodes. This factor shows the measure of split distinctness in the tree’s nodes, and it demonstrates how well the current feature (a random tree only uses one feature in a node to split input data) splits data into two distinct buckets. Then, this estimation can be averaged across all the trees in the forest. Features that split data better than others can be selected as the most <span class="No-Break">important ones.</span></li>
				<li><strong class="bold">Backward feature elimination and forward feature selection</strong>: These are iterative methods that are used for feature selection. In backward feature elimination, after we’ve trained the model with a full feature set and estimated its performance, we remove its features one by one and train the model with a reduced feature set. Then, we compare the model’s performances and decide how much performance is improved by removing feature changes—in other words, we’re deciding how important each feature is. In forward feature selection, the training process goes in the opposite direction. We start with one feature and then add more of them. These methods are very computationally expensive and can only be used on <span class="No-Break">small datas<a id="_idTextAnchor310"/>e<a id="_idTextAnchor311"/>ts.</span></li>
			</ul>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor312"/>Dimensionality reduction methods</h2>
			<p>Dimensionality reduction<a id="_idIndexMarker674"/> methods transform an original feature set into a new feature set that usually contains new features that weren’t present in the initial dataset. These methods can also be divided into two subclasses—linear and non-linear. The non-linear methods are usually more computationally expensive, so if we have a prior assumption about our feature’s data linearity, we can choose the more suitable class of methods at the <span class="No-Break">initial stage.</span></p>
			<p>The following sections will <a id="_idIndexMarker675"/>describe the various linear and non-linear methods we can use <span class="No-Break">for dimension-redu<a id="_idTextAnchor313"/>c<a id="_idTextAnchor314"/>tion.</span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor315"/>Exploring linear methods for dimension-reduction</h1>
			<p>In this section, we will <a id="_idIndexMarker676"/>describe the most popular linear methods that are used for dimension-reduction, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><span class="No-Break">PCA</span></li>
				<li><strong class="bold">Singular value </strong><span class="No-Break"><strong class="bold">decomposition</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SVD</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Independent component </strong><span class="No-Break"><strong class="bold">analysis</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ICA</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Linear discriminant </strong><span class="No-Break"><strong class="bold">analysis</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LDA</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">Factor analysis</span></li>
				<li><strong class="bold">Multidimensional <a id="_idTextAnchor316"/></strong><span class="No-Break"><strong class="bold">scaling<a id="_idTextAnchor317"/></strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MDS</strong></span><span class="No-Break">)</span></li>
			</ul>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor318"/>Principal component analysis</h2>
			<p><strong class="bold">PCA</strong> is one of the<a id="_idIndexMarker677"/> most intuitively simple and frequently used methods for applying dimension-reduction to data and projecting it onto an orthogonal subspace of features. In a very general form, it can be represented as the assumption that all our observations look like some ellipsoid in the subspace of our original space. Our new basis in this space coincides with the axes of this ellipsoid. This assumption allows us to get rid of strongly correlated features simultaneously since the basis vectors of the space we project them onto <span class="No-Break">are orthogonal.</span></p>
			<p>The dimension of this ellipsoid is equal to the dimension of the original space, but our assumption that the data lies in a subspace of a smaller dimension allows us to discard the other subspaces in the new projection; namely, the subspace with the least extension of the ellipsoid. We can do this greedily, choosing a new element one by one on the basis of our new subspace, and then taking the axis of the ellipsoid with maximum dispersion successively from the <span class="No-Break">remaining dimensions.</span></p>
			<p>To reduce the dimension of our data from <img alt="" role="presentation" src="image/B19849_Formula_001.png"/> to <img alt="" role="presentation" src="image/B19849_Formula_002.png"/>, we need to choose the top <img alt="" role="presentation" src="image/B19849_Formula_003.png"/> axes of such an ellipsoid, sorted in descending order by dispersion along the axes. To begin with, we calculate the variances and<a id="_idIndexMarker678"/> covariances of the original features. This is done by using a <strong class="bold">covariance matrix</strong>. By the definition of covariance, for two signs, <img alt="" role="presentation" src="image/B19849_Formula_004.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_005.png"/>, their covariance<a id="_idIndexMarker679"/> should be <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer237">
					<img alt="" role="presentation" src="image/B19849_Formula_006.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_007.png"/> is the mean of the <img alt="" role="presentation" src="image/B19849_Formula_008.png"/> <span class="No-Break">feature.</span></p>
			<p>In this case, we note that the covariance is symmetric and that the covariance of the vector itself is equal to its dispersion. Thus, the covariance matrix is a symmetric matrix where the dispersions of the corresponding features lie on the diagonal and the covariances of the corresponding pairs of features lie outside the diagonal. In the matrix view, where <img alt="" role="presentation" src="image/B19849_Formula_009.png"/> is the observation matrix, our covariance matrix looks <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer241">
					<img alt="" role="presentation" src="image/B19849_Formula_010.jpg"/>
				</div>
			</div>
			<p>The covariance matrix is a generalization of variance in the case of multidimensional random variables—it also describes the shape (spread) of a random variable, as does the variance. Matrices, such as linear operators, have eigenvalues and eigenvectors. They are interesting because when we act on the corresponding linear space or transform it with our matrix, the eigenvectors remain in place, and they are only multiplied by the corresponding eigenvalues. This means they define a subspace that remains in place or <em class="italic">goes into itself</em> when we apply a linear operator matrix to it. Formally, an eigenvector, <img alt="" role="presentation" src="image/B19849_Formula_011.png"/><span class="subscript">,</span> with an eigenvalue for a matrix is defined simply <span class="No-Break">as <img alt="" role="presentation" src="image/B19849_Formula_012.png"/>.</span></p>
			<p>The covariance matrix for our sample, <img alt="" role="presentation" src="image/B19849_Formula_013.png"/><span class="subscript">,</span> can be represented as a product, <img alt="" role="presentation" src="image/B19849_Formula_014.png"/>. From the Rayleigh relation, it follows that the maximum variation of our dataset can be achieved along the eigenvector of this matrix, which corresponds to the maximum eigenvalue. This is also true for projections on a higher number of dimensions—the variance (covariance matrix) of the projection onto the <em class="italic">m</em>-dimensional space is maximum in the direction of <img alt="" role="presentation" src="image/B19849_Formula_015.png"/> eigenvectors with maximum eigenvalues. Thus, the principal components that we would like to project our data for are simply the eigenvectors of the corresponding top <em class="italic">k</em> pieces of the eigenvalues of <span class="No-Break">this matrix.</span></p>
			<p>The largest vector has a direction similar to the regression line, and by projecting our sample onto it, we lose information, similar to the sum of the residual members of the regression. It is necessary to make the operation, <img alt="" role="presentation" src="image/B19849_Formula_016.png"/> (the vector length (magnitude) should be equal to one), perform the projection. If we don’t have a single vector and have a hyperplane instead, then<a id="_idIndexMarker680"/> instead of the vector, <img alt="" role="presentation" src="image/B19849_Formula_017.png"/>, we take the matrix of basis vectors, <img alt="" role="presentation" src="image/B19849_Formula_018.png"/>. The resulting vector (or matrix) is an array of projections of our observations; that is, we need to multiply our data matrix on the basis vectors matrix, and we get the projection of our data orthogonally. Now, if we multiply the transpose of our data matrix and the matrix of the principal component vectors, we restore the original sample in the space where <a id="_idIndexMarker681"/>we projected it onto the basis of the principal components. If the number of components is less than the dimension of the original space, we lose <span class="No-Break">some information.</span></p>
			<h2 id="_idParaDest-137">Singul<a id="_idTextAnchor319"/><a id="_idTextAnchor320"/>ar value decomposition</h2>
			<p>SVD is an important method that’s used to analyze data. The resulting matrix decomposition has a meaningful interpretation from an ML point of view. It can also be used to calculate PCA. SVD<a id="_idIndexMarker682"/> is rather slow. Therefore, when the matrices are too large, randomized algorithms are used. However, the<a id="_idIndexMarker683"/> SVD calculation is computationally more efficient than the calculation for the covariance matrix and its eigenvalues in the original PCA approach. Therefore, PCA is often implemented in terms of SVD. Let’s take <span class="No-Break">a look.</span></p>
			<p>The essence of SVD is straightforward—any matrix (real or complex) is represented as a product of <span class="No-Break">three matrices:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer250">
					<img alt="" role="presentation" src="image/B19849_Formula_019.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_020.png"/> is a unitary matrix of order <img alt="" role="presentation" src="image/B19849_Formula_0211.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_0221.png"/> is a matrix of size <img alt="" role="presentation" src="image/B19849_Formula_023.png"/> on the main diagonal, which is where there<a id="_idIndexMarker684"/> are non-negative numbers called singular values (elements outside the main diagonal are zero—such matrices are sometimes called rectangular diagonal <a id="_idIndexMarker685"/>matrices). <img alt="" role="presentation" src="image/B19849_Formula_024.png"/> is a Hermitian-conjugate <img alt="" role="presentation" src="image/B19849_Formula_025.png"/> matrix of order <img alt="" role="presentation" src="image/B19849_Formula_026.png"/>. The <img alt="" role="presentation" src="image/B19849_Formula_027.png"/> columns of the matrices <img alt="" role="presentation" src="image/B19849_Formula_028.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_029.png"/> columns of the matrix <img alt="" role="presentation" src="image/B19849_Formula_030.png"/> are called the left and right singular vectors of matrix <img alt="" role="presentation" src="image/B19849_Formula_0311.png"/>, respectively. To reduce the number of dimensions, matrix <img alt="" role="presentation" src="image/B19849_Formula_0221.png"/> is important, the elements of which, when raised to the second power, can be interpreted as a variance that each component puts into a joint distribution, and they are in descending order: <img alt="" role="presentation" src="image/B19849_Formula_033.png"/>. Therefore, when we choose the number of components in SVD (as in PCA), we should take the sum of their variances <span class="No-Break">into account.</span></p>
			<p>The relation between <a id="_idIndexMarker686"/>SVD and PCA can be described in the following way: <img alt="" role="presentation" src="image/B19849_Formula_034.png"/> is the covariance matrix given by <img alt="" role="presentation" src="image/B19849_Formula_035.png"/>. It is a symmetric matrix, so it can be diagonalized as <img alt="" role="presentation" src="image/B19849_Formula_036.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_025.png"/> is a matrix of eigenvectors (each column is an eigenvector) and <img alt="" role="presentation" src="image/B19849_Formula_038.png"/> is a diagonal matrix of <a id="_idIndexMarker687"/>eigenvalues, <img alt="" role="presentation" src="image/B19849_Formula_039.png"/>, in decreasing order on the diagonal. The eigenvectors are called principal axes or principal directions of <a id="_idIndexMarker688"/>the data. Projections of the data on the principal axes are called <strong class="bold">principal components</strong>, also known as <strong class="bold">principal component scores</strong>. They are newly transformed variables. The <img alt="" role="presentation" src="image/B19849_Formula_040.png"/> principal component is given<a id="_idIndexMarker689"/> by the <img alt="" role="presentation" src="image/B19849_Formula_0411.png"/> column of <img alt="" role="presentation" src="image/B19849_Formula_042.png"/>. The coordinates of the <img alt="" role="presentation" src="image/B19849_Formula_043.png"/> data point in the new principal component’s space are given by the <img alt="" role="presentation" src="image/B19849_Formula_044.png"/> row <span class="No-Break">of <img alt="" role="presentation" src="image/B19849_Formula_045.png"/>.</span></p>
			<p>By performing SVD on <img alt="" role="presentation" src="image/B19849_Formula_046.png"/><span class="subscript">,</span> we get <img alt="" role="presentation" src="image/B19849_Formula_047.png"/><span class="subscript">,</span> where <img alt="" role="presentation" src="image/B19849_Formula_048.png"/> is a unitary matrix and <img alt="" role="presentation" src="image/B19849_Formula_049.png"/> is the diagonal matrix of singular values, <img alt="" role="presentation" src="image/B19849_Formula_050.png"/>. We can observe that <img alt="" role="presentation" src="image/B19849_Formula_0511.png"/>, which means that the right singular vectors, <img alt="" role="presentation" src="image/B19849_Formula_052.png"/>, are principal directions and that singular values are related to the eigenvalues of the covariance matrix via <img alt="" role="presentation" src="image/B19849_Formula_053.png"/>. Principal components are given <span class="No-Break">by <img alt="" role="presentation" src="image/B19849_Formula_054.png"/>.</span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor321"/>Independent component analysis</h2>
			<p>The<a id="_idTextAnchor322"/><a id="_idTextAnchor323"/> <strong class="bold">ICA</strong> method was proposed as a <a id="_idIndexMarker690"/>way to solve the problem of <strong class="bold">blind signal separation</strong> (<strong class="bold">BSS</strong>); that is, selecting<a id="_idIndexMarker691"/> independent signals from mixed data. Let’s look at an example of the task of BSS. Suppose we have two <a id="_idIndexMarker692"/>people in the same room who are talking and generating acoustic waves. We have two microphones in different parts of the room, recording sound. The<a id="_idIndexMarker693"/> analysis system receives two signals from the two microphones, each of which is a digitized mixture of two acoustic waves—one from people speaking and one from some other noise (for example, playing music). Our goal is to select our initial<a id="_idIndexMarker694"/> signals from the incoming mixtures. Mathematically, the problem can be described as follows. We represent the incoming mixture in the<a id="_idIndexMarker695"/> form of a linear combination, where <img alt="" role="presentation" src="image/B19849_Formula_055.png"/> represents the displacement coefficients and <img alt="" role="presentation" src="image/B19849_Formula_056.png"/> represents the values of the vector of <span class="No-Break">independent components:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer288">
					<img alt="" role="presentation" src="image/B19849_Formula_057.jpg"/>
				</div>
			</div>
			<p>In matrix form, this can be expressed <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer289">
					<img alt="" role="presentation" src="image/B19849_Formula_058.jpg"/>
				</div>
			</div>
			<p>Here, we have to find <span class="No-Break">the following:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer290">
					<img alt="" role="presentation" src="image/B19849_Formula_059.jpg"/>
				</div>
			</div>
			<p>In this equation, <img alt="" role="presentation" src="image/B19849_Formula_060.png"/> is a matrix of input signal values, <img alt="" role="presentation" src="image/B19849_Formula_0611.png"/> is a matrix of displacement coefficients or mixing matrix, and <img alt="" role="presentation" src="image/B19849_Formula_062.png"/> is a matrix of independent components. Thus, the problem is divided into two. The first part is to get an estimate, <img alt="" role="presentation" src="image/B19849_Formula_063.png"/>, of the variables,<img alt="" role="presentation" src="image/B19849_Formula_064.png"/>, of the original independent<a id="_idIndexMarker696"/> components. The second part is to find the matrix, <img alt="" role="presentation" src="image/B19849_Formula_065.png"/>. How this method works is based on <span class="No-Break">two principles:</span></p>
			<ul>
				<li>Independent components <a id="_idIndexMarker697"/>must be statistically independent (<img alt="" role="presentation" src="image/B19849_Formula_066.png"/> matrix values). Roughly speaking, the values of one vector of an independent component do not affect the values of <span class="No-Break">another component.</span></li>
				<li>Independent components must have a <span class="No-Break">non-Gaussian distribution.</span></li>
			</ul>
			<p>The theoretical basis of ICA is the central limit theorem, which states that the distribution of the sum (average or linear combination) of <img alt="" role="presentation" src="image/B19849_Formula_067.png"/> independent random variables approaches Gaussian for <img alt="" role="presentation" src="image/B19849_Formula_068.png"/>. In particular, if <img alt="" role="presentation" src="image/B19849_Formula_069.png"/> are random variables independent of each other, taken from an arbitrary distribution with an average, <img alt="" role="presentation" src="image/B19849_Formula_070.png"/>, and a variance of <img alt="" role="presentation" src="image/B19849_Formula_0711.png"/>, then if we denote the mean of these variables as <img alt="" role="presentation" src="image/B19849_Formula_072.png"/>, we can say that <img alt="" role="presentation" src="image/B19849_Formula_073.png"/> approaches the Gaussian with a mean of <strong class="source-inline">0</strong> and a variance of <strong class="source-inline">1</strong>. To solve the BSS problem, we need to find the matrix, <img alt="" role="presentation" src="image/B19849_Formula_074.png"/>, so that <img alt="" role="presentation" src="image/B19849_Formula_075.png"/>. Here, the <img alt="" role="presentation" src="image/B19849_Formula_076.png"/> should be as close<a id="_idIndexMarker698"/> as possible to the original independent sources. We can consider this approach as the inverse process of the central limit theorem. All ICA methods are based on the same fundamental approach —finding a matrix, <em class="italic">W</em>, that maximizes <a id="_idIndexMarker699"/>non-Gaussianity, thereby minimizing the independence <span class="No-Break">of <img alt="" role="presentation" src="image/B19849_Formula_076.png"/>.</span></p>
			<p>The Fast ICA algorithm aims to maximize the function, <img alt="" role="presentation" src="image/B19849_Formula_078.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_079.png"/> are components of <img alt="" role="presentation" src="image/B19849_Formula_080.png"/>. Therefore, we can rewrite the function’s equation in the <span class="No-Break">following form:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer312">
					<img alt="" role="presentation" src="image/B19849_Formula_0811.jpg"/>
				</div>
			</div>
			<p>Here, the <img alt="" role="presentation" src="image/B19849_Formula_0821.png"/> vector is the <em class="italic">i</em><span class="superscript">th</span> row of the <span class="No-Break">matrix, </span><span class="No-Break"><em class="italic">W</em></span><span class="No-Break">.</span></p>
			<p>The ICA algorithm performs<a id="_idIndexMarker700"/> the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>It chooses the initial value <span class="No-Break">of </span><span class="No-Break"><em class="italic">w</em></span><span class="No-Break">.</span></li>
				<li>It calculates <img alt="" role="presentation" src="image/B19849_Formula_083.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_084.png"/> is the derivative of the <span class="No-Break">function, </span><span class="No-Break"><em class="italic">G(z)</em></span><span class="No-Break">.</span></li>
				<li>It <span class="No-Break">normalizes <img alt="" role="presentation" src="image/B19849_Formula_085.png"/></span><span class="No-Break">.</span></li>
				<li>It repeats the previous two steps until <em class="italic">w</em> <span class="No-Break">stops changing.</span></li>
			</ol>
			<p>To measure non-Gaussianity, Fast ICA relies on a nonquadratic non-linear function, <em class="italic">G (z)</em>, that can take the <span class="No-Break">following forms:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer317">
					<img alt="" role="presentation" src="image/B19849_Formula_086.jpg"/>
				</div>
			</div>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor324"/>Linear discriminant analysis</h2>
			<p>LDA is a type of multivari<a id="_idTextAnchor325"/><a id="_idTextAnchor326"/>ate analysis that <a id="_idIndexMarker701"/>allows us to estimate differences between two or more groups of <a id="_idIndexMarker702"/>objects at the same time. The basis of discriminant analysis is the assumption that the descriptions of the objects of each <em class="italic">k</em><span class="superscript">th</span> class are instances of a multidimensional random variable that’s distributed according to the normal (Gaussian) law, <img alt="" role="presentation" src="image/B19849_Formula_087.png"/>, with an average, <img alt="" role="presentation" src="image/B19849_Formula_088.png"/><span class="subscript">,</span> and the following <span class="No-Break">covariance matrix:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer320">
					<img alt="" role="presentation" src="image/B19849_Formula_089.jpg"/>
				</div>
			</div>
			<p>The index, <img alt="" role="presentation" src="image/B19849_Formula_015.png"/>, indicates the dimension of the feature space. Consider a simplified geometric interpretation of the LDA algorithm for the case of two classes. Let the discriminant variables, <img alt="" role="presentation" src="image/B19849_Formula_0911.png"/>, be the axes of the <img alt="" role="presentation" src="image/B19849_Formula_092.png"/>-dimensional Euclidean space. Each object (sample) is a point of this space with coordinates representing the fixed values of each variable. If both classes differ from each other in observable variables (features), they can be represented as clusters <a id="_idIndexMarker703"/>of points in different regions of the considered space that may partially overlap. To determine the position of each class, we can <a id="_idIndexMarker704"/>calculate its <strong class="bold">centroid</strong>, which is an imaginary point whose coordinates are the average values of the variables (features) in the class. The task of discriminant analysis is to create an additional <img alt="" role="presentation" src="image/B19849_Formula_093.png"/> axis that passes through a cloud of points in such a way that the projections on it provide the best <a id="_idIndexMarker705"/>separability into two classes (in other words, it maximizes the distance<a id="_idIndexMarker706"/> between classes). Its position is given by a <strong class="bold">linear discriminant</strong> (<strong class="bold">LD</strong>) function with weights, <img alt="" role="presentation" src="image/B19849_Formula_094.png"/>, that determine the contribution of each initial <span class="No-Break">variable, <img alt="" role="presentation" src="image/B19849_Formula_095.png"/>:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer327">
					<img alt="" role="presentation" src="image/B19849_Formula_096.jpg"/>
				</div>
			</div>
			<p>If we assume that the covariance matrices of the objects of classes 1 and 2 are equal, that <span class="No-Break">is, <img alt="" role="presentation" src="image/B19849_Formula_097.png"/></span><span class="No-Break">,</span>
 then the vector of coefficients, <img alt="" role="presentation" src="image/B19849_Formula_098.png"/>, of the LD, <img alt="" role="presentation" src="image/B19849_Formula_099.png"/>, can be calculated using the formula <img alt="" role="presentation" src="image/B19849_Formula_100.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_1011.png"/> is the inverse of the covariance matrix and <img alt="" role="presentation" src="image/B19849_Formula_088.png"/> is the mean of the <img alt="" role="presentation" src="image/B19849_Formula_103.png"/> class. The resulting axis coincides with the equation of a line passing through the centroids of two groups of class objects. The generalized Mahalanobis distance, which is equal to the distance between them in the multidimensional feature space, is estimated <span class="No-Break">as <img alt="" role="presentation" src="image/B19849_Formula_104.png"/>.</span></p>
			<p>Thus, in addition to the assumption regarding the normal (Gaussian) distribution of class data, which in practice occurs quite rarely, the LDA has a stronger assumption about the statistical equality of intragroup dispersions and correlation matrices. If there are no significant differences between them, they are combined into a calculated covariance matrix, <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer336">
					<img alt="" role="presentation" src="image/B19849_Formula_105.jpg"/>
				</div>
			</div>
			<p>This principle can be generalized to a larger number of classes. The final algorithm may look <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer337">
					<img alt="" role="presentation" src="image/B19849_Formula_106.jpg"/>
				</div>
			</div>
			<p>The interclass scattering <a id="_idIndexMarker707"/>matrix is calculated <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer338">
					<img alt="" role="presentation" src="image/B19849_Formula_107.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_108.png"/> is the mean of all objects (samples), <img alt="" role="presentation" src="image/B19849_Formula_029.png"/> is the number of classes, <img alt="" role="presentation" src="image/B19849_Formula_110.png"/> is the number of objects in the <em class="italic">i</em><span class="superscript">th</span> class, <img alt="" role="presentation" src="image/B19849_Formula_1111.png"/> is the intraclass’ mean, <img alt="" role="presentation" src="image/B19849_Formula_1121.png"/> is the scattering matrix for the <em class="italic">i</em><span class="superscript">th</span> class, and <img alt="" role="presentation" src="image/B19849_Formula_1131.png"/> is a centering matrix, where <img alt="" role="presentation" src="image/B19849_Formula_114.png"/> is the <em class="italic">n</em> x <em class="italic">n</em> matrix of <span class="No-Break">all 1s.</span></p>
			<p>Based on these matrices, the <img alt="" role="presentation" src="image/B19849_Formula_115.png"/> matrix is calculated, for which the eigenvalues and the corresponding eigenvectors are determined. In the diagonal elements of the matrix, we must select the <em class="italic">s</em> of the largest eigenvalues and transform the matrix, leaving only the <a id="_idIndexMarker708"/>corresponding <em class="italic">s</em> rows in it. The resulting matrix can be used to convert all objects into <span class="No-Break">lower-dimensional space.</span></p>
			<p>This method requires labeled data, meaning it is a <span class="No-Break">supervised method.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor327"/>Factor analysis</h2>
			<p><strong class="bold">Factor analysis</strong> is used to reduce the number of variables t<a id="_idTextAnchor328"/>h<a id="_idTextAnchor329"/>at are used to describe data and determine the relationships<a id="_idIndexMarker709"/> between them. During the analysis, variables that correlate with each other are combined into one factor. As a result, the <a id="_idIndexMarker710"/>dispersion between components is redistributed, and the structure of factors becomes more understandable. After combining the variables, the correlation of components within each factor becomes higher than their correlation with components from other factors. It is assumed that known variables depend on a smaller number of unknown variables and that we have a random error that can be expressed <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer347">
					<img alt="" role="presentation" src="image/B19849_Formula_116.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_117.png"/> is the load and <img alt="" role="presentation" src="image/B19849_Formula_118.png"/> is <span class="No-Break">the factor.</span></p>
			<p>The concept of <strong class="bold">factor load</strong> is essential. It is <a id="_idIndexMarker711"/>used to describe the role of the factor (variable) when we wish to form a specific vector from a new basis. The essence of factor analysis is the procedure of rotating factors, that is, redistributing the dispersion according to a specific method. The purpose of rotations is to define a simple structure of factor loadings. Rotation can be orthogonal and oblique. In the first form of rotation, each successive factor is determined to maximize the variability that remains from the previous factors. Therefore, the<a id="_idIndexMarker712"/> factors are independent and uncorrelated with each other. The second type is a transformation in which factors correlate with each other. There are about 13 methods of rotation that are used in both forms. The factors that have a similar effect on the elements of the new basis are combined into one group. Then, from each group, it is recommended to leave one representative. Some algorithms, instead of choosing a representative, calculate a new factor with some heuristics that become central to <a id="_idIndexMarker713"/><span class="No-Break">the group.</span></p>
			<p>Dimensionality reduction occurs while transitioning to a system of factors that are representatives of groups, and the other factors are discarded. There are several commonly used criteria for determining the number of factors. Some of these criteria can be used together to complement each other. An example of a criterion that’s used to determine the number of factors is the Kaiser criterion or the eigenvalue criterion: only factors with eigenvalues equal to or greater than <em class="italic">one</em> are selected. This means that if a factor does not select a variance equivalent to at least one variance of one variable, then it is omitted. The general factor analysis algorithm follows <span class="No-Break">these steps:</span></p>
			<ol>
				<li>It calculates the <span class="No-Break">correlation matrix.</span></li>
				<li>It selects the number of factors for inclusion, for example, with the <span class="No-Break">Kaiser criterion.</span></li>
				<li>It extracts the initial set of factors. There are several different extraction methods, including maximum likelihood, PCA, and principal <span class="No-Break">axis extraction.</span></li>
				<li>It rotates the factors to a<a id="_idIndexMarker714"/> final solution that is equal to the one that was obtained in the initial extraction but that has the most <span class="No-Break">straightforward interpretation.</span></li>
			</ol>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor330"/>Multidimensional scaling</h2>
			<p>MDS can be considered as an alternative to facto<a id="_idTextAnchor331"/><a id="_idTextAnchor332"/>r analysis when, in addition to the correlation matrices, an arbitrary type of <a id="_idIndexMarker715"/>object similarity matrix can be used as input data. MDS is not so much a formal mathematical procedure but rather a method of efficiently placing objects, thus keeping an appropriate distance between them in a new feature space. The dimension of the new space in MDS is always substantially less than the original space. The data<a id="_idIndexMarker716"/> that’s used for analysis by MDS is often obtained from the matrix of pairwise comparisons <span class="No-Break">of objects.</span></p>
			<p>The main MDS algorithm’s goal is to restore the unknown dimension, <img alt="" role="presentation" src="image/B19849_Formula_119.png"/>, of the analyzed feature space and assign coordinates to each object in such a way that the calculated pairwise Euclidean distances between the objects coincide as much as possible with the specified pairwise comparison matrix. We are talking about restoring the coordinates of the new reduced feature space with the accuracy of orthogonal transformation, ensuring the pairwise distances between the objects do <span class="No-Break">not change.</span></p>
			<p>Thus, the aim of MDS methods can also be formulated in order to display the configuration information of the original multidimensional data that’s given by the pairwise comparison matrix. This is provided as a configuration of points in the corresponding space of <span class="No-Break">lower dimension.</span></p>
			<p>Classical MDS assumes that the unknown coordinate matrix, <img alt="" role="presentation" src="image/B19849_Formula_009.png"/>, can be expressed by eigenvalue decomposition, <img alt="" role="presentation" src="image/B19849_Formula_1211.png"/>. <img alt="" role="presentation" src="image/B19849_Formula_1221.png"/> can be computed from the proximity matrix <img alt="" role="presentation" src="image/B19849_Formula_1231.png"/> (a matrix with distances between samples) by using double centering. The general MDS algorithm follows <span class="No-Break">these steps:</span></p>
			<ol>
				<li>It computes the squared proximity <span class="No-Break">matrix, <img alt="" role="presentation" src="image/B19849_Formula_1241.png"/></span><span class="No-Break"><span class="subscript">.</span></span></li>
				<li>It applies double centering, <img alt="" role="presentation" src="image/B19849_Formula_125.png"/><span class="subscript">,</span> using the centering matrix, <img alt="" role="presentation" src="image/B19849_Formula_126.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_026.png"/> is the number <span class="No-Break">of objects.</span></li>
				<li>It determines the <img alt="" role="presentation" src="image/B19849_Formula_128.png"/> largest eigenvalues, <img alt="" role="presentation" src="image/B19849_Formula_129.png"/>, and the corresponding eigenvectors, <img alt="" role="presentation" src="image/B19849_Formula_130.png"/>, of <img alt="" role="presentation" src="image/B19849_Formula_1311.png"/> (where <img alt="" role="presentation" src="image/B19849_Formula_1321.png"/> is the number of dimensions desired for <span class="No-Break">the output).</span></li>
				<li>It computes <img alt="" role="presentation" src="image/B19849_Formula_133.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_134.png"/> is the matrix of <img alt="" role="presentation" src="image/B19849_Formula_135.png"/> eigenvectors and <img alt="" role="presentation" src="image/B19849_Formula_136.png"/> is the diagonal matrix of <img alt="" role="presentation" src="image/B19849_Formula_137.png"/> eigenvalues <span class="No-Break">of <img alt="" role="presentation" src="image/B19849_Formula_138.png"/>.</span></li>
			</ol>
			<p>The disadvantage of the MDS <a id="_idIndexMarker717"/>method is that it does not take into account the distribution of nearby points since it uses Euclidean distances in calculations. If you ever find multidimensional data lying <a id="_idIndexMarker718"/>on a curved manifold, the distance between data points can be much more <span class="No-Break">than Euclidean.</span></p>
			<p>Now that we’ve discussed the linear methods we can use for dimension-reduction, let’s look at what non-linear <span class="No-Break">methods exist.</span></p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor333"/>Exploring non-linear methods for dimension-reduction</h1>
			<p>In this section, we’ll discuss<a id="_idIndexMarker719"/> the<a id="_idTextAnchor334"/> <a id="_idTextAnchor335"/>widespread non-linear methods and algorithms that are used for dimension-reduction, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><span class="No-Break">Kernel PCA</span></li>
				<li><span class="No-Break">Isomap</span></li>
				<li><span class="No-Break">Sammon</span><span class="No-Break"><a id="_idIndexMarker720"/></span><span class="No-Break"> mapping</span></li>
				<li>Distributed <strong class="bold">stochastic neighbor </strong><span class="No-Break"><strong class="bold">embedding</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SNE</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">Autoe<a id="_idTextAnchor336"/>ncoders</span></li>
			</ul>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor337"/>Kernel PCA</h2>
			<p>Classic PCA is a linear projection<a id="_idIndexMarker721"/> method that works well if the data is li<a id="_idTextAnchor338"/>nearly separable. However, in the case of linearly non-separable data, a non-linear approach is required. The basic idea of working with linearly inseparable data is <a id="_idIndexMarker722"/>to project it into a space with a larger number of dimensions, where it becomes linearly separable. We can choose a non-linear mapping function, <img alt="" role="presentation" src="image/B19849_Formula_139.png"/>, so that the sample mapping, <em class="italic">x</em>, can be written as <img alt="" role="presentation" src="image/B19849_Formula_140.png"/>. This is<a id="_idIndexMarker723"/> called the <strong class="bold">kernel function</strong>. The term <em class="italic">kernel</em> describes a function that calculates the scalar product of mapping (in a higher-order space) samples <em class="italic">x</em> with <img alt="" role="presentation" src="image/B19849_Formula_1411.png"/>. This scalar product can be interpreted as the distance measured in the new space. In other words, the <img alt="" role="presentation" src="image/B19849_Formula_1421.png"/> function maps the original <em class="italic">d</em>-dimensional elements into the <em class="italic">k</em>-dimensional feature space of a higher dimension by creating non-linear combinations of the original objects. For example, a function that displays 2D samples, <img alt="" role="presentation" src="image/B19849_Formula_1431.png"/>, in 3D space can look <span class="No-Break">like <img alt="" role="presentation" src="image/B19849_Formula_144.png"/>.</span></p>
			<p>In a linear PCA approach, we<a id="_idIndexMarker724"/> are interested in the principal components that maximize the variance in the dataset. We can maximize variance by calculating the eigenvectors (principal components) that correspond to the largest eigenvalues based on the covariance matrix of our data and project our data onto these eigenvectors. This approach can be generalized to data that is mapped into a higher dimension space using the kernel function. However, in practice, the covariance matrix in a multidimensional space is not explicitly calculated since we can use a method <a id="_idIndexMarker725"/>called the <strong class="bold">kernel trick</strong>. The kernel trick allows us to project data onto the principal components without explicitly calculating the projections, which is much<a id="_idIndexMarker726"/> more efficient. The general approach is <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Compute the <strong class="bold">kernel matrix</strong> equal <span class="No-Break">to <img alt="" role="presentation" src="image/B19849_Formula_145.png"/>.</span></li>
				<li>Make it so that it has a zero mean value, <img alt="" role="presentation" src="image/B19849_Formula_146.png"/>, where <img alt="" role="presentation" src="image/B19849_Formula_147.png"/> is a matrix of <em class="italic">N x N</em> size with <span class="No-Break"><em class="italic">1/N</em></span><span class="No-Break"> elements.</span></li>
				<li>Calculate the eigenvalues and eigenvectors <span class="No-Break">of </span><span class="No-Break"><img alt="" role="presentation" src="image/B19849_Formula_148.png"/>.</span></li>
				<li>Sort the eigenvectors in descending order, according to <span class="No-Break">their eigenvalues.</span></li>
				<li>Take <img alt="" role="presentation" src="image/B19849_Formula_149.png"/> eigenvectors that correspond to the largest eigenvalues, where <img alt="" role="presentation" src="image/B19849_Formula_150.png"/> is the number of dimensions of a new <span class="No-Break">feature space.</span></li>
			</ol>
			<p>These eigenvectors are projections of our data onto the corresponding main components. The main difficulty of this <a id="_idIndexMarker727"/>process is selecting the correct kernel and configuring its hyperparameters. Two frequently <a id="_idIndexMarker728"/>used kernels are the polynomial kernel <img alt="" role="presentation" src="image/B19849_Formula_1511.png"/>and the Gaussian (Radial Basis Function (RBF)) <img alt="" role="presentation" src="image/B19849_Formula_1521.png"/> <span class="No-Break">ones.</span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor339"/>Isomap</h2>
			<p>The <strong class="bold">Isomap</strong> algorithm is based on the manifold <a id="_idIndexMarker729"/>projection technique. In <a id="_idIndexMarker730"/>mathematics,<a id="_idTextAnchor340"/> the <strong class="bold">manifold</strong> is a topological space (which is, in general, a set of points with their neighbors) that locally resembles the Euclidian<a id="_idIndexMarker731"/> space near each point. For example, one-dimensional manifolds include lines and circles but not figures with self-intersections. 2D manifolds are called <strong class="bold">surfaces</strong>; for example, they<a id="_idIndexMarker732"/> can be a sphere, a plane, or a torus, but these surfaces can’t have self-intersection. For example, a circle is a one-dimensional manifold embedded into a 2D space. Here, each arc of the circle locally resembles a straight-line segment. A 3D curve can also be a manifold if it can be divided into straight-line segments that can be embedded in 3D space without self-intersections. A 3D shape can be a manifold if its surface can be divided into flat plane patches <span class="No-Break">without self-intersections.</span></p>
			<p>The basics of applying manifold projection techniques are to search for a manifold that is close to the data, project the data onto the manifold, and then unfold it. The most popular technique that’s used to find the manifold is to build a graph based on information about data points. Usually, these data points are placed into the graph nodes, and the edges simulate the relationships between the <span class="No-Break">data points.</span></p>
			<p>The Isomap algorithm depends on <span class="No-Break">two parameters:</span></p>
			<ul>
				<li>The number of neighbors, <img alt="" role="presentation" src="image/B19849_Formula_153.png"/>, used to search for <span class="No-Break">geodetic distances</span></li>
				<li>The dimension of the final <span class="No-Break">space, <img alt="" role="presentation" src="image/B19849_Formula_1321.png"/></span></li>
			</ul>
			<p>In brief, the Isomap algorithm follows <span class="No-Break">these steps:</span></p>
			<ol>
				<li>First, it constructs a graph representing geodesic distances. For each point, we search the <img alt="" role="presentation" src="image/B19849_Formula_155.png"/> nearest <a id="_idIndexMarker733"/>neighbors and construct a weighted, undirected graph from the distances to these nearest neighbors. The edge weight is the Euclidean distance to <span class="No-Break">the neighbor.</span></li>
				<li>Using an algorithm to find the shortest distance in the graph, for example, Dijkstra’s algorithm, we need to find the shortest distance between each pair of vertices. We can consider this distance as a geodesic distance on <span class="No-Break">a manifold.</span></li>
				<li>Based on the matrix of <a id="_idIndexMarker734"/>pairwise geodesic distances we obtained in the previous step, train the <span class="No-Break">MDS algorithm.</span></li>
				<li>The MDS algorithm associates a set of points in the <img alt="" role="presentation" src="image/B19849_Formula_027.png"/>-dimensional space with the initial set <span class="No-Break">of distances.</span></li>
			</ol>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor341"/>Sammon mapping</h2>
			<p><strong class="bold">Sammon mapping</strong> is one of the first non-linear dimensionality reduction algor<a id="_idTextAnchor342"/><a id="_idTextAnchor343"/>ithms. In contrast to traditional dimensionality reduction <a id="_idIndexMarker735"/>methods, such as PCA, Sammon mapping does not define a data conversion function directly. On the contrary, it only determines the measure of how <a id="_idIndexMarker736"/>well the conversion results (a specific dataset of a smaller dimension) correspond to the structure of the original dataset. In other words, it does not try to find the optimal transformation of the original data; instead, it searches for another dataset of lower dimensions with a structure that’s as close to the original one as possible. The algorithm can be described as follows. Let’s say we have <img alt="" role="presentation" src="image/B19849_Formula_157.png"/>-dimensional vectors, <img alt="" role="presentation" src="image/B19849_Formula_158.png"/>. Here, <img alt="" role="presentation" src="image/B19849_Formula_026.png"/> vectors are defined in the <img alt="" role="presentation" src="image/B19849_Formula_160.png"/>-dimensional space, <img alt="" role="presentation" src="image/B19849_Formula_1611.png"/>, which is denoted by <img alt="" role="presentation" src="image/B19849_Formula_1621.png"/>. The distances between the vectors in the <img alt="" role="presentation" src="image/B19849_Formula_163.png"/>-dimensional space will be denoted by <img alt="" role="presentation" src="image/B19849_Formula_164.png"/> and in the <img alt="" role="presentation" src="image/B19849_Formula_160.png"/>-dimensional space, <img alt="" role="presentation" src="image/B19849_Formula_166.png"/>. To determine the distance between the vectors, we can use any metric; in particular, the Euclidean distance. The goal of non-linear Sammon mapping is to search a selection of vectors, <img alt="" role="presentation" src="image/B19849_Formula_19.png"/>, in order to minimize the error function, <img alt="" role="presentation" src="image/B19849_Formula_168.png"/>, which is defined by the <span class="No-Break">following formula:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer400">
					<img alt="" role="presentation" src="image/B19849_Formula_169.jpg"/>
				</div>
			</div>
			<p>To minimize the error function, <img alt="" role="presentation" src="image/B19849_Formula_168.png"/>, Sammon used Newton’s minimization method, which can be simplified <span class="No-Break">as</span><span class="No-Break"><a id="_idIndexMarker737"/></span><span class="No-Break"> follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer402">
					<img alt="" role="presentation" src="image/B19849_Formula_171.jpg"/>
				</div>
			</div>
			<p>Here, <em class="italic">η</em> is the <span class="No-Break">learning</span><span class="No-Break"><a id="_idIndexMarker738"/></span><span class="No-Break"> rate.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor344"/>Distributed stochastic neighbor embedding</h2>
			<p>The SNE problem is formulated as follows: we have a dataset <a id="_idTextAnchor345"/><a id="_idTextAnchor346"/>with points described by a multidimensional variable with a dimension of space substantially higher than three. It is necessary to obtain a new variable that <a id="_idIndexMarker739"/>exists in a 2D or 3D space that would maximally preserve the structure and patterns in the original data. The difference between t-SNE and the classic<a id="_idIndexMarker740"/> SNE lies in the modifications that simplify the process of finding the global minima. The main modification is replacing the normal distribution with the Student’s t-distribution for low-dimensional data. SNE begins by converting the multidimensional Euclidean distance between points into conditional probabilities that reflect the similarity of points. Mathematically, it looks <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer403">
					<img alt="" role="presentation" src="image/B19849_Formula_172.jpg"/>
				</div>
			</div>
			<p>This formula shows how close the point <img alt="" role="presentation" src="image/B19849_Formula_173.png"/> lies to the point <img alt="" role="presentation" src="image/B19849_Formula_312.png"/> with a Gaussian distribution <span class="No-Break">around <img alt="" role="presentation" src="image/B19849_Formula_312.png"/>,</span>
 with a given deviation of <img alt="" role="presentation" src="image/B19849_Formula_176.png"/>. <img alt="" role="presentation" src="image/B19849_Formula_176.png"/> is different for each point. It is chosen so that the<a id="_idIndexMarker741"/> points in areas with higher density have less variance <span class="No-Break">than others.</span></p>
			<p>Let’s denote 2D or 3D mappings of the (<img alt="" role="presentation" src="image/B19849_Formula_312.png"/>, <img alt="" role="presentation" src="image/B19849_Formula_173.png"/>) pair as the (<img alt="" role="presentation" src="image/B19849_Formula_111.png"/>, <img alt="" role="presentation" src="image/B19849_Formula_1811.png"/>) pair. It is necessary to estimate the conditional probability using the same formula. The standard deviation <span class="No-Break">is <img alt="" role="presentation" src="image/B19849_Formula_1821.png"/>:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer414">
					<img alt="" role="presentation" src="image/B19849_Formula_183.jpg"/>
				</div>
			</div>
			<p>If the mapping points, <img alt="" role="presentation" src="image/B19849_Formula_111.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_1811.png"/>, correctly simulate the similarity between the original points of the higher dimension, <img alt="" role="presentation" src="image/B19849_Formula_312.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_187.png"/>, then the corresponding conditional probabilities, <img alt="" role="presentation" src="image/B19849_Formula_188.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_189.png"/>, will be equivalent. As an obvious assessment of the quality of how <img alt="" role="presentation" src="image/B19849_Formula_189.png"/> reflects <img alt="" role="presentation" src="image/B19849_Formula_188.png"/>, divergence, or <a id="_idIndexMarker742"/>the Kullback-Leibler distance is used. SNE minimizes the sum of such distances for all mapping points using gradient descent. The following formula determines the loss function for <span class="No-Break">this method:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer423">
					<img alt="" role="presentation" src="image/B19849_Formula_192.jpg"/>
				</div>
			</div>
			<p>It has the <span class="No-Break">following gradient:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer424">
					<img alt="" role="presentation" src="image/B19849_Formula_193.jpg"/>
				</div>
			</div>
			<p>The authors of this problem proposed the following physical analogy for the optimization process. Let’s imagine that springs connect all the mapping points. The stiffness of the spring connecting points <img alt="" role="presentation" src="image/B19849_Formula_194.png"/> and <img alt="" role="presentation" src="image/B19849_Formula_195.png"/> depends on the difference between the similarity of two points in a multidimensional space and two points in a mapping space. In this analogy, the gradient is the resultant force that acts on a point in the mapping space. If we let<a id="_idIndexMarker743"/> the system go, after some time, it results in balance, and this is the desired distribution. Algorithmically, it searches for balance while taking the following moments <span class="No-Break">into account:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer427">
					<img alt="" role="presentation" src="image/B19849_Formula_196.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_197.png"/> is the learning rate and <img alt="" role="presentation" src="image/B19849_Formula_198.png"/> is the coefficient of inertia. Classic SNE also allows us to get good results but can be associated with difficulties when optimizing the loss function and the crowding problem. t-SNE doesn’t solve these problems in general, but it makes them much <span class="No-Break">more manageable.</span></p>
			<p>The loss function in t-SNE has two<a id="_idIndexMarker744"/> principal differences from the loss function of classic SNE. The first one is that it has a symmetric form of similarity in a multidimensional space and a simpler gradient version. Secondly, instead of using a Gaussian distribution for points from the mapping space, the t-distribution (Student) <span class="No-Break">is used.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor347"/>Autoencoders</h2>
			<p><strong class="bold">Autoencoders</strong> represent a particular class of neural networks that are configured so that the output of the aut<a id="_idTextAnchor348"/><a id="_idTextAnchor349"/>oencoder is <a id="_idIndexMarker745"/>as close as possible to the input signal. In its most straightforward representation, the <a id="_idIndexMarker746"/>autoencoder can be modeled as a multilayer perceptron in which the number of neurons in the output layer is equal to the number of inputs. The following diagram shows that by choosing an intermediate hidden layer of a smaller dimension, we compress the source data into the lower dimension. Usually, values from this intermediate layer are a result of <span class="No-Break">an autoencoder:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer430">
					<img alt="Figure 6.1 – Autoencoder architecture" src="image/B19849_06_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Autoencoder architecture</p>
			<p>Now that we have learned about the linear and non-linear methods that can be used for dimension-reduction and <a id="_idIndexMarker747"/>explored the components of<a id="_idIndexMarker748"/> each of the methods in detail, we can enhance our implementation of dimension-reduction with the help of some <span class="No-Break">practical examples.</span></p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor350"/>Understanding dimension-reduction algorithms with various С++ libraries</h1>
			<p>Let’s look at how to use dimensionality reducti<a id="_idTextAnchor351"/>o<a id="_idTextAnchor352"/>n algorithms in practice. All of these examples use the same dataset, which <a id="_idIndexMarker749"/>contains four normally distributed 2D point sets that have been transformed with Swiss roll mapping, <img alt="" role="presentation" src="image/B19849_Formula_199.png"/>, into a 3D space. You can find the dataset and related details in the book's GitHub repository here: <a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition</a>. The following graph shows the result of <span class="No-Break">this mapping.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer432">
					<img alt="Figure 6.2 – Swiss roll dataset" src="image/B19849_06_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Swiss roll dataset</p>
			<p>This dataset is labeled. Each of the normally distributed parts has its own labels, and we can see these labels as a certain color on the result. We use these colors to show transformation results for <a id="_idIndexMarker750"/>each of the algorithms we’ll be using in the following samples. This gives us an idea of how the algorithm works. The following sections provide concrete examples of how to use the <strong class="source-inline">Dlib</strong>, <strong class="source-inline">Tapkee</strong>, <span class="No-Break">and </span><span class="No-Break">libraries.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor353"/>Using the Dlib library</h2>
			<p>There are three dimensionality<a id="_idIndexMarker751"/> reduction methods in<a id="_idIndexMarker752"/> the <strong class="source-inline">Dlib</strong> library—two linear ones, known <a id="_idTextAnchor354"/>a<a id="_idTextAnchor355"/>s PCA and LDA, and one non-linear one, known as <span class="No-Break">Sammon mapping.</span></p>
			<h3>PCA</h3>
			<p>PCA is one of the most popular<a id="_idIndexMarker753"/> dimensionality reduction algorithms and it has a couple of implementations in the<a id="_idTextAnchor356"/> <a id="_idTextAnchor357"/><strong class="source-inline">Dlib</strong> library. There is the <strong class="source-inline">Dlib::vector_normalizer_pca</strong> type, for which objects can be used to perform PCA on user data. This implementation also normalizes the data. In some cases, this automatic normalization is useful because we always have to perform PCA on normalized data. An object of this type should be parameterized with the input data sample type. After we’ve instantiated an object of this type, we use the <strong class="source-inline">train()</strong> method to fit the model to<a id="_idIndexMarker754"/> our data. The <strong class="source-inline">train()</strong> method takes <strong class="source-inline">std::vector</strong> as samples and the <strong class="source-inline">eps</strong> value as parameters. The <strong class="source-inline">eps</strong> value controls how many dimensions should be preserved after the PCA has been transformed. This can be seen in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
void PCAReduction(const std::vector&lt;Matrix&gt; &amp;data, double target_dim) {
  // instantiante the PCA algorithm object.
  Dlib::vector_normalizer_pca&lt;Matrix&gt; pca;
  // train the PCA algorithm
  pca.train(data, target_dim / data[0].nr());
  // apply trained algorithm to the new data
  std::vector&lt;Matrix&gt; new_data;
  new_data.reserve(data.size());
  for (size_t i = 0; i &lt; data.size(); ++i) {
    new_data.emplace_back(pca(data[i]));
  }
  // example how to get transformed values
  for (size_t r = 0; r &lt; new_data.size(); ++r) {
    Matrix vec = new_data[r];
    double x = vec(0, 0);
    double y = vec(1, 0);
  }</pre>			<p>After the algorithm has been trained, we use the object to transform individual samples. Take a look at the first loop in<a id="_idIndexMarker755"/> the code and notice how the <strong class="source-inline">pca([data[i]])</strong> call performs <span class="No-Break">this transformation.</span></p>
			<p>The following graph shows the<a id="_idIndexMarker756"/> result of the <span class="No-Break">PCA transformation:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer433">
					<img alt="Figure 6.3 – Dlib PCA transformation visualization" src="image/B19849_06_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Dlib PCA transformation visualization</p>
			<h4>Data compression with PCA</h4>
			<p>We can use dimensionality reduction <a id="_idIndexMarker757"/>a<a id="_idTextAnchor358"/>lgorithms for a slightly different task—data comp<a id="_idTextAnchor359"/>ression with information loss. This can be easily demonstrated when applying the PCA algorithm to images. Let’s implement PCA from scratch with the <strong class="source-inline">Dlib</strong> library using SVD decomposition. We can’t use an existing implementation because it performs normalization in a way we can’t <span class="No-Break">fully control.</span></p>
			<p>First, we need to load an image <a id="_idIndexMarker758"/>and transform it into <span class="No-Break">matrix form:</span></p>
			<pre class="source-code">
void PCACompression(const std::string&amp; image_file, long target_dim) {
  array2d&lt;Dlib::rgb_pixel&gt; img;
  load_image(img, image_file);
  array2d&lt;unsigned char&gt; img_gray;
  assign_image(img_gray, img);
  save_png(img_gray, "original.png");
  array2d&lt;DataType&gt; tmp;
  assign_image(tmp, img_gray);
  Matrix img_mat = Dlib::mat(tmp);
  img_mat /= 255.;  // scale
  std::cout &lt;&lt; "Original data size " &lt;&lt; img_mat.size() &lt;&lt;
  std::endl;</pre>			<p>After we’ve loaded the RGB image, we convert it into grayscale and transform its values into floating points. The next step is to transform the image matrix into samples that we can use for PCA training. This can be done by splitting the image into rectangular patches that are 8 x 8 in size with the <strong class="source-inline">Dlib::subm()</strong> function and then flattening them with the <span class="No-Break"><strong class="source-inline">Dlib::reshape_to_column_vector()</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
std::vector&lt;Matrix&gt; data;
int patch_size = 8;
for (long r = 0; r &lt; img_mat.nr(); r += patch_size) {
  for (long c = 0; c &lt; img_mat.nc(); c += patch_size) {
    auto sm =
      Dlib::subm(img_mat, r, c, patch_size, patch_size);
      data.emplace_back(Dlib::reshape_to_column_vector(sm));
  }
}</pre>			<p>When we have our samples, we can normalize them by subtracting the mean and dividing them by their standard <a id="_idIndexMarker759"/>deviation. We can make these operations vectorized by converting our vector of samples into the matrix type. We do this with the <span class="No-Break"><strong class="source-inline">Dlib::mat()</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
// normalize data
auto data_mat = mat(data);
Matrix m = mean(data_mat);
Matrix sd = reciprocal(sqrt(variance(data_mat)));
matrix&lt;decltype(data_mat)::type, 0, 1,
       decltype(data_mat)::mem_manager_type&gt;
    x(data_mat);
for (long r = 0; r &lt; x.size(); ++r)
    x(r) = pointwise_multiply(x(r) - m, sd);</pre>			<p>After we’ve prepared the data samples, we calculate the covariance matrix with <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">Dlib::</strong></span><strong class="source-inline">
covariance()</strong> function and perform SVD with the <strong class="source-inline">Dlib::svd()</strong> function. The SVD results are the eigenvalues matrix and the eigenvectors matrix. We sorted the eigenvectors according to the eigenvalues and left only a small number (in our case, 10 of them) of eigenvectors corresponding to the biggest eigenvalues. The number of eigenvectors we left is the number of dimensions in the new <span class="No-Break">feature space:</span></p>
			<pre class="source-code">
Matrix temp, eigen, pca;
// Compute the svd of the covariance matrix
Dlib::svd(covariance(x), temp, eigen, pca);
Matrix eigenvalues = diag(eigen);
rsort_columns(pca, eigenvalues);
// leave only required number of principal components
pca = trans(colm(pca, range(0, target_dim)));</pre>			<p>Our PCA transformation<a id="_idIndexMarker760"/> matrix is called <strong class="source-inline">pca</strong>. We used it to reduce the dimensions of each of our samples with simple matrix multiplication. Look at the following cycle and notice the <strong class="source-inline">pca * </strong><span class="No-Break"><strong class="source-inline">data[i]</strong></span><span class="No-Break"> operation:</span></p>
			<pre class="source-code">
// dimensionality reduction
std::vector&lt;Matrix&gt; new_data;
size_t new_size = 0;
new_data.reserve(data.size());
for (size_t i = 0; i &lt; data.size(); ++i) {
  new_data.emplace_back(pca * data[i]);
  new_size += static_cast&lt;size_t&gt;(new_data.back().size());
}
std::cout &lt;&lt; "New data size "
          &lt;&lt; new_size + static_cast&lt;size_t&gt;(pca.size())
          &lt;&lt; std::endl;</pre>			<p>Our data has been compressed and we can see its new size in the console output. Now, we can restore the original dimension of the data to be able to see the image. To do this, we need to use the transposed PCA matrix to multiply the reduced samples. Also, we need to denormalize the <a id="_idIndexMarker761"/>restored sample to get actual pixel values. This can be done by multiplying the standard deviation and adding the mean we got from the <span class="No-Break">previous steps:</span></p>
			<pre class="source-code">
auto pca_matrix_t = Dlib::trans(pca);
Matrix isd = Dlib::reciprocal(sd);
for (size_t i = 0; i &lt; new_data.size(); ++i) {
    Matrix sample = pca_matrix_t * new_data[i];
    new_data[i] = Dlib::pointwise_multiply(sample, isd) + m;
}</pre>			<p>After we’ve restored the pixel values, we reshape them and place them in their original location in <span class="No-Break">the image:</span></p>
			<pre class="source-code">
size_t i = 0;
    for (long r = 0; r &lt; img_mat.nr(); r += patch_size) {
        for (long c = 0; c &lt; img_mat.nc(); c += patch_size)
        {
            auto sm = Dlib::reshape(new_data[i],
                patch_size, patch_size);
                Dlib::set_subm(img_mat, r, c, patch_size,
                    patch_size) = sm;
                    ++i;
        }
    }
    img_mat *= 255.0;
    assign_image(img_gray, img_mat);
    equalize_histogram(img_gray);
    save_png(img_gray, "compressed.png");
}</pre>			<p>Let’s look at the result of <a id="_idIndexMarker762"/>compressing a standard test image that is widely used in image processing. The following is the Lena 512 x 512 <span class="No-Break">px image:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer434">
					<img alt="Figure 6.4 – Original image before compression" src="image/B19849_06_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Original image before compression</p>
			<p>Its original grayscale size is 262,144 bytes. After we perform PCA compression with only 10 principal components, its size becomes 45,760 bytes. We can see the result in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer435">
					<img alt="Figure 6.5 – Image after compression" src="image/B19849_06_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Image after compression</p>
			<p>Here, we can see that<a id="_idIndexMarker763"/> most of the essential visual information was preserved, despite the high <span class="No-Break">compression rate.</span></p>
			<h3>LDA</h3>
			<p>The <strong class="source-inline">Dlib</strong> library also has an<a id="_idIndexMarker764"/> implementation of the LDA algorithm, which can be used for dimensionality r<a id="_idTextAnchor360"/>e<a id="_idTextAnchor361"/>duction. It’s a supervised algorithm, so it needs labeled data. This algorithm is implemented with the <strong class="source-inline">Dlib::compute_lda_transform()</strong> function, which takes four parameters. The first one is the input/output parameter—as input, it is used to pass input training data (in matrix form) and as output, it<a id="_idIndexMarker765"/> receives the LDA transformation matrix. The second parameter is the output for the mean values. The third parameter is the labels for the input data, while the fourth one is the desired number of target dimensions. The following code shows an example of how to use LDA for dimension-reduction with the <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library:</span></p>
			<pre class="source-code">
void LDAReduction(const Matrix &amp;data,
                  const std::vector&lt;unsigned long&gt; &amp;labels,
                  unsigned long target_dim) {
  Dlib::matrix&lt;DataType, 0, 1&gt; mean;
  Matrix transform = data;
  // Apply LDA on input data,
  // result with will be in the "transform object"
  Dlib::compute_lda_transform(transform, mean, labels,
                              target_dim);
  // Apply LDA "transform" to the input "data"
  for (long r = 0; r &lt; data.nr(); ++r) {
    Matrix row =
        transform * Dlib::trans(Dlib::rowm(data, r)) - mean;
    double x = row(0, 0);
    double y = row(1, 0);
  }
}x`</pre>			<p>To perform an actual LDA transform <a id="_idIndexMarker766"/>after the algorithm has been trained, we multiply our samples with the LDA matrix. In our case, we also transposed them. The following code shows the <a id="_idIndexMarker767"/>essential part of <span class="No-Break">this example:</span></p>
			<pre class="source-code">
transform * Dlib::trans(Dlib::rowm(data, r))</pre>			<p>The following graph shows the result of using LDA reduction on <span class="No-Break">two components:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer436">
					<img alt="Figure 6.6 – The Dlib LDA transformation visualization" src="image/B19849_06_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – The Dlib LDA transformation visualization</p>
			<p>In the following block, we will <a id="_idIndexMarker768"/>see how to use Sammon mapping dimensionality reduction <a id="_idIndexMarker769"/>algorithm implementation from the <span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library.</span></p>
			<h3>Sammon mapping</h3>
			<p>In the <strong class="source-inline">Dlib</strong> library, Sammon mapping is implemented<a id="_idIndexMarker770"/> with the <strong class="source-inline">Dlib::sammon_projection</strong> type<a id="_idTextAnchor362"/>.<a id="_idTextAnchor363"/> We need to create an instance of this type and then use it as<a id="_idIndexMarker771"/> a functional object. Functional object call arguments are the data that we need to transform and the number of dimensions of the new feature space. The input data should be in the form of the <strong class="source-inline">std::vector</strong> of the single samples of the <strong class="source-inline">Dlib::matrix</strong> type. All samples should have the same number of dimensions. The result of using this functional object is a new vector of samples with a reduced number <span class="No-Break">of </span><span class="No-Break"><a id="_idIndexMarker772"/></span><span class="No-Break">dimensions:</span></p>
			<pre class="source-code">
void SammonReduction(const std::vector&lt;Matrix&gt; &amp;data, long target_dim) {
  Dlib::sammon_projection sp;
  auto new_data = sp(data, target_dim);
  for (size_t r = 0; r &lt; new_data.size(); ++r) {
    Matrix vec = new_data[r];
    double x = vec(0, 0);
    double y = vec(1, 0);
  }
}</pre>			<p>The following graph shows the <a id="_idIndexMarker773"/>result of using this dimensionality <span class="No-Break">reduction algorithm:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer437">
					<img alt="Figure 6.7 – The Dlib Sammon mapping transformation visualization" src="image/B19849_06_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – The Dlib Sammon mapping transformation visualization</p>
			<p>In the next section, we <a id="_idIndexMarker774"/>will learn how to use the Tapkee library for solving <a id="_idIndexMarker775"/>dimensionality <span class="No-Break">reduction tasks.</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor364"/>Using the Tapkee library</h2>
			<p>The Tapkee library contains numerous <a id="_idIndexMarker776"/>dimensionality reduction algorithms,<a id="_idTextAnchor365"/> <a id="_idTextAnchor366"/>both linear and non-linear ones. This is the<a id="_idIndexMarker777"/> headers-only C++ template library so it doesn’t require any compilation, and can be easily integrated into your application. It has a couple of dependencies: the <strong class="source-inline">fmt</strong> library for the formatted output and the Eigen3 as the <span class="No-Break">math backend.</span></p>
			<p>There are no special classes for algorithms in this library because it provides a uniform API based on parameters to build a dimensionality reduction object. So, using this approach, we can define a single function that will take a set of parameters and high-dimensional input data and<a id="_idIndexMarker778"/> perform dimensionality reduction. The result will be a 2D plot. The following code sample <a id="_idIndexMarker779"/>shows <span class="No-Break">its implementation:</span></p>
			<pre class="source-code">
void Reduction(tapkee::ParametersSet parameters,
               bool with_kernel,
               const tapkee::DenseMatrix&amp; features,
               const tapkee::DenseMatrix&amp; lables,
               const std::string&amp; img_file) {
  using namespace tapkee;
  // define the kernel callback object,
  // that will be applied to the input "features"
  gaussian_kernel_callback kcb(features, 2.0);
  // define distance callback object,
  // that will be applied to the input "features"
  eigen_distance_callback dcb(features);
  // define the feature access callback object
  eigen_features_callback fcb(features);
  // save the initial features indices order
  auto n = features.cols();
  std::vector&lt;int&gt; indices(n);
  for (int i = 0; i &lt; n; ++i) indices[i] = i;
  TapkeeOutput result;
  if (with_kernel) {
    // apply feature transformation with kernel function
    result =
        initialize()
            .withParameters(parameters)
            .withKernel(kcb)
            .withFeatures(fcb)
            .withDistance(dcb)
            .embedRange(indices.begin(), indices.end());
  } else {
    // apply features transformation without kernel
    // //function
    result =
        initialize()
            .withParameters(parameters)
            .withFeatures(fcb)
            .withDistance(dcb)
            .embedRange(indices.begin(), indices.end());
  }
  // create helper object for transformed data
  // //visualization result
  Clusters clusters;
  for (index_t i = 0; i &lt; result.embedding.rows(); ++i) {
    // get a transformed feature
    auto new_vector = result.embedding.row(i);
    // populate visualization helper structure
    auto label = static_cast&lt;int&gt;(lables(i));
    clusters[label].first.push_back(new_vector[0]);
    clusters[label].second.push_back(new_vector[1]);
  }
  // Visualize dimensionality reduction result
  PlotClusters(clusters,
               get_method_name(parameters[method]),
               img_ file);
}</pre>			<p>This is the single function that we will use to see several algorithms from the Tapkee library. <a id="_idTextAnchor367"/>The main parameter it takes is <strong class="source-inline">tapkee::ParametersSet</strong>; an object of this type can be initialized with the dimensionality reduction method type, the number of target dimensions, and some special parameters that configure the selected method. The <strong class="source-inline">with_kernel</strong> parameter specifies if this function will attach the kernel transform to the algorithm pipeline or not. The library API allows you to attach the kernel transform for any algorithm but it will be used only if the algorithm implementation uses it. In our case, we will <a id="_idIndexMarker780"/>use it only for the <strong class="source-inline">KernelPCA</strong> method. The last parameters for the Reduction function are the input data, labels for plotting, and the output <span class="No-Break">file name</span><span class="No-Break">.</span></p>
			<p>Let’s look into the implementation. At first, we define the callback functors for a method pipeline. There are the Gaussian kernel callback, the linear distance callback, and the feature callback. Notice that all of these callback objects were initialized with the input data. It was done to reduce the data coping in the pipeline. The library API requires callback objects to be able to produce some result for two data indices. These callback objects should implement functionality to get access to the original data. So, all our callbacks were constructed from library-defined classes and store references to the original data containers. <strong class="source-inline">tapkee::DenseMatrix</strong> is just a typedef for the Eigen3 dense matrix. Another important thing is the index map usage for the data access, for example, the <strong class="source-inline">indices</strong> variable; it allows you to use your data more flexibly. The following snippet shows the dimensionality reduction object creation <span class="No-Break">and application:</span></p>
			<pre class="source-code">
initialize().
withParameters(parameters).
withKernel(kcb).
withFeatures(fcb).
withDistance(dcb).
embedRange(indices.begin(), indices.end());</pre>			<p>You can see that the API is uniform and the builder functions that perform configurations start with the word <strong class="source-inline">with</strong>. At first, we passed the parameters to the build, then three callback functors, and finally called the <strong class="source-inline">embedRange</strong> method that does the actual dimensionality reduction. <strong class="source-inline">eigen_features_callback</strong> is needed to access particular data values with indices, and <strong class="source-inline">eigen_distance_callback</strong> is used in some algorithms to<a id="_idIndexMarker781"/> measure the distance between item vectors; in our case, it’s just the Euclidean distance, but you can define any <span class="No-Break">you need.</span></p>
			<p>In the last part of the function, the <strong class="source-inline">Clusters</strong> object is populated with new 2D coordinates and labels. Then, this object is used to plot the dimensionality reduction result. The plotting approach from the previous chapters <span class="No-Break">was applied.</span></p>
			<p>In the following subsections, we will learn how to use different dimensionality reduction methods with our general function. These methods will be <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">PCA</span></li>
				<li><span class="No-Break">Kernel PCA</span></li>
				<li><span class="No-Break">MDS</span></li>
				<li><span class="No-Break">Isomap</span></li>
				<li><span class="No-Break">Factor analysis</span></li>
				<li><span class="No-Break">t-distributed SNEs</span></li>
			</ul>
			<h3>PCA</h3>
			<p>Having the general function for<a id="_idIndexMarker782"/> the dimensionality reduction, we can use it for applying <a id="_idTextAnchor368"/>different methods. The following code<a id="_idIndexMarker783"/> shows how to create a parameter set to configure the <span class="No-Break">PCA method:</span></p>
			<pre class="source-code">
bool with_kernel = false;
Reduction((method = PCA, target_dimension = target_dim),
          with_kernel, input_data, labels_data,
          "pca-tapkee.png");</pre>			<p>The first argument is the initialization of the <strong class="source-inline">tapkee::ParametersSet</strong> type object. We used the <strong class="source-inline">tapkee::PCA</strong> enumeration value and specified the number of target dimensions. Also, we didn’t use a kernel. <strong class="source-inline">input_data</strong> and <strong class="source-inline">labels_data</strong> are input data loaded from the file, and these<a id="_idIndexMarker784"/> variables have <strong class="source-inline">tapkee::DenseMatrix</strong> type, which actually is the Eigen3 dense matrix. The last parameter is the name of the output file for the <span class="No-Break">plot image.</span></p>
			<p>The following graph shows the result of applying the Tapkee PCA implementation to <span class="No-Break">our data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer438">
					<img alt="Figure 6.8 – The Tapkee PCA transformation visualization" src="image/B19849_06_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – The Tapkee PCA transformation visualization</p>
			<p>You can see that this <a id="_idIndexMarker785"/>method was not able to spatially separate our 3D data in the <span class="No-Break">2D space.</span></p>
			<h3>Kernel PCA</h3>
			<p>The non-linear version of PCA is also implemented in the Tapkee library. To use<a id="_idTextAnchor369"/> <a id="_idTextAnchor370"/>this method, we define the kernel <a id="_idIndexMarker786"/>method and pass it as a callback to the <strong class="source-inline">withKernel</strong> builder method of library API. We already did it in our general function, so the only thing <a id="_idIndexMarker787"/>we have to do is pass the <strong class="source-inline">with_kernel</strong> parameter as <strong class="source-inline">true</strong>. We used the Gaussian kernel, which is defined <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer439">
					<img alt="" role="presentation" src="image/B19849_Formula_200.jpg"/>
				</div>
			</div>
			<p>Here, <em class="italic">ᵧ</em> is the sale coefficient that can be estimated as a median of the item difference, or just configured manually. The kernel callback function is defined in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
struct gaussian_kernel_callback {
  gaussian_kernel_callback(
      const tapkee::DenseMatrix&amp; matrix,
      tapkee::ScalarType gamma)
      : feature_ matrix(matrix), gamma(gamma){};
  inline tapkee::ScalarType kernel(
      tapkee::IndexType a, tapkee::IndexType b) const {
    auto distance =
        (feature_matrix.col(a) – feature_matrix.col(b))
            .norm();
    return exp(-(distance * distance) * gamma);
  }
  inline tapkee::ScalarType operator()(
      tapkee::IndexType a, tapkee::IndexType b) const {
    return kernel(a, b);
  }
  const tapkee::DenseMatrix&amp; feature_matrix;
  tapkee::ScalarType gamma{1};
}</pre>			<p><strong class="source-inline">Tapkee</strong> requires that <a id="_idIndexMarker788"/>you define the method named <strong class="source-inline">kernel</strong> and the function operator. Our implementation is very simple; the main feature here is that the <a id="_idIndexMarker789"/>reference to the particular data is stored as a member. It’s done in a such way because the library will use only indices to call the kernel functor. The actual call to our general function looks <span class="No-Break">like this:</span></p>
			<pre class="source-code">
bool with_kernel = true;
Reduction((method = KernelPCA, target_dimension = target_dim), 
                      with_kernel, input_data, labels_data,
                      "kernel-pca-tapkee.png");</pre>			<p>The following graph<a id="_idIndexMarker790"/> shows the result of applying the Tapkee kernel PCA<a id="_idIndexMarker791"/> implementation to <span class="No-Break">our data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer440">
					<img alt="Figure 6.9 – The Tapkee kernel PCA transformation visualization" src="image/B19849_06_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – The Tapkee kernel PCA transformation visualization</p>
			<p>We can see that this type of kernel makes some parts of the data separated, but that other ones were reduced <span class="No-Break">too much.</span></p>
			<h3>MDS</h3>
			<p>To use the MDS algorithm, we should just pass the method name to our general dimens<a id="_idTextAnchor371"/><a id="_idTextAnchor372"/>ionality reduction<a id="_idIndexMarker792"/> function. There are no other configurable parameters, especially for this algorithm. The<a id="_idIndexMarker793"/> following example shows how to use <span class="No-Break">this method:</span></p>
			<pre class="source-code">
Reduction((method = MultidimensionalScaling,
                    target_dimension = target_dim),
           false, input_data, labels_data, "mds-tapkee.png";</pre>			<p>The following graph shows the result of applying the Tapkee MDS algorithm to <span class="No-Break">our data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer441">
					<img alt="Figure 6.10 – The Tapkee MDS transformation visualization" src="image/B19849_06_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – The Tapkee MDS transformation visualization</p>
			<p>You can see that the result is very similar to the PCA algorithm, and there is the same problem that the method <a id="_idIndexMarker794"/>was not able to spatially separate our data in the <span class="No-Break">2D </span><span class="No-Break"><a id="_idIndexMarker795"/></span><span class="No-Break">space.</span></p>
			<h3>Isomap</h3>
			<p>The Isomap <a id="_idIndexMarker796"/>method can be applied to <a id="_idIndexMarker797"/>our data <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Reduction((method = Isomap, target_dimension = target_dim,
                            num_neighbors =100),
           false, input_data, labels_data, "isomap-tapkee.png");</pre>			<p>Apart from the method name and target dimensions, the <strong class="source-inline">num_neighbors</strong> parameter was passed. This is the number of nearest neighbor values that will be used by the <span class="No-Break"><strong class="source-inline">Isomap</strong></span><span class="No-Break"> algorithm.</span></p>
			<p>The following graph shows the result of applying the Tapkee Isomap implementation to <span class="No-Break">our data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer442">
					<img alt="Figure 6.11 – The Tapkee Isomap transformation visualization" src="image/B19849_06_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – The Tapkee Isomap transformation visualization</p>
			<p>You can see that this method <a id="_idIndexMarker798"/>can spatially separate our data in the 2D space, but the<a id="_idIndexMarker799"/> clusters are too close to each other. Also, you can play with the number of neighbor parameters to get <span class="No-Break">another separation.</span></p>
			<h3>Factor analysis</h3>
			<p>The factor analysis <a id="_idIndexMarker800"/>method can be <a id="_idIndexMarker801"/>applied to our data <span class="No-Break">as follo<a id="_idTextAnchor373"/><a id="_idTextAnchor374"/><a id="_idTextAnchor375"/>ws:</span></p>
			<pre class="source-code">
Reduction((method = FactorAnalysis,
           target_dimension = target_dim,
           fa_epsilon = 10e-5, max_iteration = 100),
          false, input_data, labels_data,
          "isomap-tapkee.png");</pre>			<p>Apart from the method name and target dimensions, the <strong class="source-inline">fa_epsilon</strong> and <strong class="source-inline">max_iteration</strong> parameters<a id="_idIndexMarker802"/> can be passed. <strong class="source-inline">fa_epsilon</strong> is used to check the <span class="No-Break">algorithm’s convergence.</span></p>
			<p>The following graph <a id="_idIndexMarker803"/>shows the result of applying the Tapkee factor analysis implementation to <span class="No-Break">our data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer443">
					<img alt="Figure 6.12 – The Tapkee factor analysis transformation visualization" src="image/B19849_06_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – The Tapkee factor analysis transformation visualization</p>
			<p>This method also fails to clearly separate our data in the <span class="No-Break">2D space.</span></p>
			<h3>t-SNE</h3>
			<p>The<a id="_idTextAnchor376"/> t-SNE <a id="_idIndexMarker804"/>method can be<a id="_idIndexMarker805"/> applied to our data <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Reduction((method = tDistributedStochasticNeighborEmbedding,
           target_dimension = target_dim,
           sne_perplexity = 30),
          false, input_data, labels_data,
          "tsne-tapkee.png");</pre>			<p>Apart from the method name and target dimensions, the <strong class="source-inline">sne_perplexity</strong> parameter was specified. This parameter regulates the algorithm convergence. Also, you can change the <strong class="source-inline">sne_theta</strong> value, which is the <span class="No-Break">learning rate.</span></p>
			<p>The following graph shows the result of applying the Tapkee t-SNE implementation to <span class="No-Break">our data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer444">
					<img alt="Figure 6.13 – The Tapkee t-SNE transformation visualization" src="image/B19849_06_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – The Tapkee t-SNE transformation visualization</p>
			<p>You can see that this <a id="_idIndexMarker806"/>method gave the most reasonable separation of our<a id="_idIndexMarker807"/> data in the 2D space; there are distinct bounds between <span class="No-Break">all clusters.</span></p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor377"/>Summary</h1>
			<p>In this chapter, we learned that dimensionality reduction is the p<a id="_idTextAnchor378"/>r<a id="_idTextAnchor379"/>ocess of transferring data that has a higher dimension into a new representation of data with a lower dimension. It is used to reduce the number of correlated features in a dataset and extract the most informative features. Such a transformation can help increase the performance of other algorithms, reduce computational complexity, and make <span class="No-Break">human-readable visualizations.</span></p>
			<p>We learned that there are two different approaches to solving this task. One is feature selection, which doesn’t create new features, while the second one is dimensionality reduction algorithms, which make new feature sets. We also learned that dimensionality reduction algorithms are linear and non-linear and that we should select either type, depending on our data. We saw that there are a lot of different algorithms with different properties and computational complexity and that it makes sense to try different ones to see which are the best solutions for particular tasks. Note that different libraries have different implementations for identical algorithms, so their results can differ, even for the <span class="No-Break">same data.</span></p>
			<p>The area of dimensionality reduction algorithms is a field that’s in continual development. There is, for example, a new algorithm called <strong class="bold">Uniform Manifold Approximation and Projection</strong> (<strong class="bold">UMAP</strong>) that’s based on Riemannian geometry and algebraic topology. It competes with the t-SNE algorithm in terms of visualization quality but also preserves more of the original data’s global structure after the transformation is complete. It is also much more computationally effective, which makes it suitable for large-scale datasets. However, at the moment, there is no C++ implementation <span class="No-Break">of it.</span></p>
			<p>In the next chapter, we will discuss classification tasks and how to solve them. Usually, when we have to solve a classification task, we have to divide a group of objects into several subgroups. Objects in such subgroups share some common properties that are distinct from the properties in <span class="No-Break">other subgroups.</span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor380"/>Further reading</h1>
			<ul>
				<li>A survey of dimensionality reduction <span class="No-Break">techniques: </span><a href="https://arxiv.org/pdf/1403.2877.pdf"><span class="No-Break">https://a<span id="_idTextAnchor381"/>r<span id="_idTextAnchor382"/>xiv.org/pdf/1403.2877.pdf</span></a></li>
				<li>A short tutorial for dimensionality <span class="No-Break">reduction: </span><a href="https://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf"><span class="No-Break">https://www.math.uwaterloo.ca/~aghodsib/courses/f06stat890/readings/tutorial_stat890.pdf</span></a></li>
				<li>Guide to 12 dimensionality reduction techniques (with Python <span class="No-Break">code): </span><a href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/"><span class="No-Break">https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/</span></a></li>
				<li>A geometric and intuitive explanation of the covariance matrix and its relationship with linear transformation, an essential building block for understanding and using PCA and <span class="No-Break">SVD: </span><a href="https://datascienceplus.com/understanding-the-covariance-matrix"><span class="No-Break">https://datascienceplus.com/understanding-the-covariance-matrix</span></a></li>
				<li>The kernel <span class="No-Break">trick: </span><a href="https://dscm.quora.com/The-Kernel-Trick"><span class="No-Break">https://dscm.quora.com/The-Kernel-Trick</span></a></li>
			</ul>
		</div>
	</body></html>