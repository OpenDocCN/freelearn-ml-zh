<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer250">
<h1 class="chapter-number" id="_idParaDest-110"><a id="_idTextAnchor143"/>7</h1>
<h1 id="_idParaDest-111"><a id="_idTextAnchor144"/>Working with Model Explainability </h1>
<p>The justification of model selection and performance is just as important as model training. You can have <em class="italic">N</em> trained models using different algorithms, and all of them will be able to make good enough predictions for real-world problems. So, how do you select one of them to be used in your production services, and how do you justify to your stakeholders that your chosen model is better than the others, even though all the other models were also able to make accurate predictions to some degree? One answer is performance metrics, but as we saw in the previous chapter, there are plenty of performance metrics and all of them measure different types of performance. Choosing the correct performance metric boils down to the context of your ML problem. What else can we use that will help us choose the right model and also further help us in justifying this selection?</p>
<p>The answer to that is visual graphs. Human beings are visual creatures and, as such, a picture speaks a thousand words. A good graph can explain more about a model than any metric number. The versatility of graphs can be very useful in explaining the model’s behavior and how it fits as a solution to our ML problem.</p>
<p>H2O’s explainability interface is a unique feature that wraps over various explainability features and visuals that H2O auto-computes for a model or list of models, including the H2O AutoML object.</p>
<p>In this chapter, we shall explore the H2O explainability interface and how it works with the H2O AutoML object. We shall also implement a practical example to understand how to use the explainability interface in Python and R. Finally, we shall go through and understand all the various explainability features that we get as outputs.</p>
<p>In this chapter, we are going to cover the following main topics:</p>
<ul>
<li><a id="_idTextAnchor145"/><a id="_idTextAnchor146"/>Working with the model explainability interface</li>
<li>Exploring the various explainability features</li>
</ul>
<p>By the end of this chapter, you should have a good idea of how to interpret model performance by looking at the various performance metrics described by the model explainability interface.</p>
<h1 id="_idParaDest-112"><a id="_idTextAnchor147"/>Technical requirements</h1>
<p>For this chapter, you will require the following:</p>
<ul>
<li>The latest version of your preferred web browser.</li>
<li>An <strong class="bold">Integrated Development Environment</strong> (<strong class="bold">IDE</strong>) of your choice or a Terminal.</li>
<li>All the experiments conducted in this chapter are performed on a Terminal. You are free to follow along using the same setup or you can perform the same experiments using any IDE of your choice.</li>
</ul>
<p>All the code examples for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207">https://github.com/PacktPublishing/Practical-Automated-Machine-Learning-on-H2O/tree/main/Chapter%207</a>.</p>
<p>So, let’s begin by understanding how the model explainability interface works.</p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor148"/>Working with the model explainability interface</h1>
<p>The <strong class="bold">model explainability interface</strong> is a simple function that incorporates various graphs and<a id="_idIndexMarker748"/> information about the model and its workings. There are two main functions for model explainability in H2O:</p>
<ul>
<li>The <strong class="source-inline">h2o.explain()</strong> function, which is<a id="_idIndexMarker749"/> used to explain the model’s behavior <a id="_idIndexMarker750"/>on the entire test dataset. This is also called <strong class="bold">global explanation</strong>.</li>
<li>The <strong class="source-inline">h2o.explain_row()</strong> function, which is<a id="_idIndexMarker751"/> used to explain the model’s behavior on an individual row in the test dataset. This is also<a id="_idIndexMarker752"/> called <strong class="bold">local explanation</strong>.</li>
</ul>
<p>Both these functions work on either a single H2O model object, a list of H2O model objects, or the H2O AutoML object. These functions generate a list of results that consists of various <a id="_idIndexMarker753"/>graphical plots such as<a id="_idIndexMarker754"/> a <strong class="bold">variable importance graph</strong>, <strong class="bold">partial dependency graph</strong>, and a <strong class="bold">leaderboard</strong> if <a id="_idIndexMarker755"/>used on multiple models.</p>
<p>For graphs<a id="_idIndexMarker756"/> and other visual results, the <strong class="source-inline">explain</strong> object relies on visualization engines to render the graphics:</p>
<ul>
<li>For the R interface, H2O uses the <strong class="source-inline">ggplot2</strong> package for rendering.</li>
<li>For the Python interface, H2O uses the <strong class="source-inline">matplotlib</strong> package for rendering.</li>
</ul>
<p>With this in mind, we need to make sure that whenever we are using the explainability interface to get visual graphs, we run it in an environment that supports graph rendering. This interface won’t be of much use in Terminals and other non-graphical command-line interfaces. The examples in this chapter have been run on <strong class="bold">Jupyter Notebook</strong>, but any <a id="_idIndexMarker757"/>environment that supports plot rendering should work fine.</p>
<p>The explainability function has the following parameters:</p>
<ul>
<li><strong class="source-inline">newdata</strong>/<strong class="source-inline">frame</strong>: This<a id="_idIndexMarker758"/> parameter is used to specify the H2O test DataFrame needed to compute <a id="_idIndexMarker759"/>some of the explainability features such as the <strong class="bold">SHAP summary</strong> and <strong class="bold">residual analysis</strong>. The <a id="_idIndexMarker760"/>parameter name that’s used in the R explainability interface is <strong class="source-inline">newdata</strong>, while the same in the Python explainability interface is <strong class="source-inline">frame</strong>.</li>
<li><strong class="source-inline">columns</strong>: This <a id="_idIndexMarker761"/>parameter is used to<a id="_idIndexMarker762"/> specify the columns to be considered in column-based explanations<a id="_idIndexMarker763"/> such as <strong class="bold">individual conditional expectation plots</strong> or <strong class="bold">partial dependency plots</strong>. </li>
<li><strong class="source-inline">top_n_features</strong>: This <a id="_idIndexMarker764"/>parameter is used to specify the number of columns to be considered based on the feature importance ranking for column-based explanations. The default value is <strong class="source-inline">5</strong>.</li>
</ul>
<p>Either the <strong class="source-inline">columns</strong> parameter or the <strong class="source-inline">top_n_features</strong> parameter will be considered by the explainability function. Preference is given to the <strong class="source-inline">columns</strong> parameter, so if both the parameters are passed with values, then <strong class="source-inline">top_n_features</strong> will be ignored.</p>
<ul>
<li><strong class="source-inline">include_explanations</strong>: This <a id="_idIndexMarker765"/>parameter is used to specify the explanations that you want from the explainability function’s output.</li>
<li><strong class="source-inline">exclude_explanations</strong>: This<a id="_idIndexMarker766"/> parameter is used to specify the explanations that you do not want from the explainability function’s output. <strong class="source-inline">include_explanations</strong> and <strong class="source-inline">exclude_explanations</strong> are mutually exclusive parameters. The available values for both parameters are as follows:<ul><li><strong class="source-inline">leaderboard</strong>: This value is only valid for the list of models or the AutoML object.</li><li><strong class="source-inline">residual_analysis</strong>: This value is only valid for regression models.</li><li><strong class="source-inline">confusion_matrix</strong>: This value is only valid for classification models.</li><li><strong class="source-inline">varimp</strong>: This value stands for variable importance and is only valid for base models, not for stacked ensemble models.</li><li><strong class="source-inline">varimp_heatmap</strong>: This value stands for heatmap of variable importance.</li><li><strong class="source-inline">model_correlation_heatmap</strong>: This value stands for heatmap of model correlation.</li><li><strong class="source-inline">shap_summary</strong>: This value stands for Shapley additive explanations.</li><li><strong class="source-inline">pdp</strong>: This value stands for partial dependency plots.</li><li><strong class="source-inline">ice</strong>: This value <a id="_idIndexMarker767"/>stands for individual conditional expectation plots.</li></ul></li>
<li><strong class="source-inline">plot_overrides</strong>: This<a id="_idIndexMarker768"/> parameter is used to override the values for individual explanation plots. This parameter is useful if you want the top 10 features to be considered for one plot but specific columns for another: <p class="source-code">list(pdp = list(top_n_features = 8))</p></li>
<li><strong class="source-inline">object</strong>: This <a id="_idIndexMarker769"/>parameter is used to specify the H2O models or the H2O AutoML object, which we will cover shortly. This parameter is specific to the R explainability interface.</li>
</ul>
<p>Now that we <a id="_idIndexMarker770"/>know how the explainability interface works and what its various parameters are, let’s understand it better with an implementation example.</p>
<p>We shall use <strong class="bold">Fisher’s Iris flower dataset</strong>, which we used in <a href="B17298_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>,<em class="italic"> Understanding H2O AutoML Basics</em>, to train models using AutoML. We will then use the explainability interface on the AutoML object to display all the explainability features it has to provide.</p>
<p>So, let’s start by implementing it in Python.</p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor149"/>Implementing the model explainability interface in Python</h2>
<p>To<a id="_idIndexMarker771"/> implement the model explainability<a id="_idIndexMarker772"/> function in Python, follow these steps:</p>
<ol>
<li>Import the <strong class="source-inline">h2o</strong> library and spin<a id="_idTextAnchor150"/> up a local H2O server: <p class="source-code">library(h2o)</p><p class="source-code">h2o.init(max_mem_size = "12g")</p></li>
</ol>
<p>The explainability interface performs heavy computations behind the scenes to calculate the data needed to plot the graphs. To speed up processing, it is recommended to initialize the H2O server with as much memory as you can allocate.</p>
<ol>
<li value="2">Import the dataset using <strong class="source-inline">h2o.importFile(“Dataset/iris.data”)</strong>:<p class="source-code">data = h2o.import_file("Dataset/iris.data")</p></li>
<li>Set which columns are the features and which columns are the labels:<p class="source-code">features = data.columns</p><p class="source-code">label = "C5"</p></li>
<li>Remove the label from among the features:<p class="source-code">features.remove(label)</p></li>
<li>Split the DataFrame into training and testing DataFrames:<p class="source-code">train_dataframe, test_dataframe = data.split_frame([0.8])</p></li>
<li>Initialize the H2O AutoML object:<p class="source-code">aml = h2o.automl.H2OAutoML(max_models=10, seed = 1)</p></li>
<li>Trigger<a id="_idIndexMarker773"/> the H2O AutoML <a id="_idIndexMarker774"/>object so that it starts auto-training the models:<p class="source-code">aml.train(x = features, y = label, training_frame = train_dataframe)</p></li>
<li>Once training has finished, we can use the H2O explainability interface, <strong class="source-inline">h2o.explain()</strong>, on the now-trained <strong class="source-inline">aml</strong> object:<p class="source-code">aml.explain(test_dataframe)</p></li>
</ol>
<p>The <strong class="source-inline">explain</strong> function will take some time to finish computing. Once it does, you should see a big output that lists all the explainability features. The output should look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer225">
<img alt="Figure 7.1 – Model explainability interface output " height="386" src="image/B17298_07_001.jpg" width="1160"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Model explainability interface output</p>
<ol>
<li value="9">You can also use the <strong class="source-inline">h2o.explain_row()</strong> interface to display the model explainability features for a single row of the dataset:<p class="source-code">aml.explain_row(test_dataframe, row_index=0)</p></li>
</ol>
<p>The output of this should give you a leaderboard of the models making predictions on the first row of the dataset.</p>
<ol>
<li value="10">To get additional information about the model from an explainability point of view, you can further extend the explainability interface by using the <strong class="source-inline">explain_row()</strong> function on the leader model, as follows:<p class="source-code">aml.leader.explain_row(test_dataframe, row_index=0)</p></li>
</ol>
<p>The<a id="_idIndexMarker775"/> output of this should <a id="_idIndexMarker776"/>give you all the applicable graphical model explainability features for that model based on its predictions on that row.</p>
<p>Now that we know how to use the model explainability interface in Python, let’s see how we can use this interface in the R Language.</p>
<h2 id="_idParaDest-115"><a id="_idTextAnchor151"/>Implementing the model explainability interface in R</h2>
<p>Similar to<a id="_idIndexMarker777"/> how we implemented the <a id="_idIndexMarker778"/>explainability interface in Python, H2O has provisions to use the explainability interface in the R programming language as well.</p>
<p>To implement the model explainability function in R, follow these steps:</p>
<ol>
<li value="1">Import the <strong class="source-inline">h2o</strong> library and spin up a local H2O server: <p class="source-code">library(h2o)</p><p class="source-code">h2o.init(max_mem_size = "12g")</p></li>
<li>Import the dataset using <strong class="source-inline">h2o.importFile(“Dataset/iris.data”)</strong>:<p class="source-code">data = h2o.import_file("Dataset/iris.data")</p></li>
<li>Set the <strong class="source-inline">C5</strong> column as the label:<p class="source-code">label &lt;- "C5"</p></li>
<li>Split the DataFrame into training and testing DataFrames and assign them to the appropriate variables:<p class="source-code">splits &lt;- h2o.splitFrame(data, ratios = 0.8, seed = 7)</p><p class="source-code">train_dataframe &lt;- splits[[1]]</p><p class="source-code">test_dataframe &lt;- splits[[2]]</p></li>
<li>Run the H2O AutoML training:<p class="source-code">aml &lt;- h2o.automl(y = label, training_frame = train_dataframe, max_models = 10)</p></li>
<li>Use the <a id="_idIndexMarker779"/>H2O explainability<a id="_idIndexMarker780"/> interface on the now-trained <strong class="source-inline">aml</strong> object:<p class="source-code">explanability_object &lt;- h2o.explain(aml, test_dataframe)</p></li>
</ol>
<p>Once the explainability object finishes its computation, you should see a big output that lists all the explainability features.</p>
<ol>
<li value="7">Just like Python, you can also extend the model explainability interface function so that it can be run on a single row using the <strong class="source-inline">h2o.explain_row()</strong> function, as follows:<p class="source-code">h2o.explain_row(aml, test, row_index = 1)</p></li>
</ol>
<p>This will give you the leaderboard of models making predictions on the first row of the dataset.</p>
<ol>
<li value="8">Similarly, you can expand this explainability interface by using the <strong class="source-inline">h2o.explain_row()</strong> function on the leader model to get more advanced information about the leader model:<p class="source-code">h2o.explain_row(aml@leader, test, row_index = 1)</p></li>
</ol>
<p>In these examples, we have used the Iris flower dataset to solve a multinomial classification problem. Similarly, we can use the explainability interface on trained regression models. Some of the explainability features are only available depending <a id="_idIndexMarker781"/>on whether the trained model <a id="_idIndexMarker782"/>is a regression model or a classification model.</p>
<p>Now that we know how to implement the model explainability interface in Python and R, let’s look deeper into the output of the interface and try to understand the various explainability features that H2O computed.</p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor152"/>Exploring the various explainability features</h1>
<p>The output of the <a id="_idIndexMarker783"/>explainability interface is an <strong class="source-inline">H2OExplanation</strong> object. The <strong class="source-inline">H2OExplanation</strong> object is nothing but a simple dictionary with the explainability features’ names as keys. You can retrieve individual explainability features by using a feature’s key name as a <strong class="source-inline">dict</strong> key on the explainability object.</p>
<p>If you scroll down the output of the explainability interface for the H2O AutoML object, you will notice that there are plenty of headings with explanations. Below these headings, there’s a brief description of what the explainability feature is. Some have graphical diagrams, while <a id="_idIndexMarker784"/>others may have tables.</p>
<p>The various explainability features are as follows:</p>
<ul>
<li><strong class="bold">Leaderboard</strong>: This feature is a leaderboard<a id="_idIndexMarker785"/> comprising all<a id="_idIndexMarker786"/> trained models and their basic metrics ranked from best performing to worst. This feature is computed only if the explainability interface is run on the H2O AutoML object or list of H2O models.</li>
<li><strong class="bold">Confusion Matrix</strong>: This feature is a performance metric that generates a matrix that keeps track of correct and incorrect predictions of a classification model. It is only <a id="_idIndexMarker787"/>available for classification models. For multiple models, the confusion matrix<a id="_idIndexMarker788"/> is only calculated for the leader model.</li>
<li><strong class="bold">Residual Analysis</strong>: This feature plots the predicted values against the residuals<a id="_idIndexMarker789"/> on the test dataset used in the explainability interface. It only analyzes the leader model based on the model ranking on the leaderboard. It is only available for regression models. For multiple models, residual analysis<a id="_idIndexMarker790"/> is performed on the leader model.</li>
<li><strong class="bold">Variable Importance</strong>: This feature plots t<a id="_idTextAnchor153"/>he importance of variable<a id="_idIndexMarker791"/>s in the dataset. It is available for all models except for stacked models. For <a id="_idIndexMarker792"/>multiple models, it is only performed on the leader model, which is not a stacked model.</li>
<li><strong class="bold">Variable Importance Heatmap</strong>: This feature plots a heatmap of variable importance across all the <a id="_idIndexMarker793"/>models. It is<a id="_idIndexMarker794"/> available for comparing all models except stacked models.</li>
<li><strong class="bold">Model Correlation Heatmap</strong>: This<a id="_idIndexMarker795"/> feature plots the correlation between the predicted values<a id="_idIndexMarker796"/> of different models. This helps group together models with similar performance. It is only available for multiple model explanations.</li>
<li><strong class="bold">SHAP Summary of Top Tree-Based Model</strong>: This feature plots the importance of <a id="_idIndexMarker797"/>variables in contributing to the <a id="_idIndexMarker798"/>decision-making that’s done by complex tree-based models such as Random Forest and neural networks. This feature computes this plot for the top-ranking tree-based model in the leaderboard.</li>
<li><strong class="bold">Partial Dependence Multi Plots</strong>: This <a id="_idIndexMarker799"/>feature <a id="_idIndexMarker800"/>plots the dependency between the target feature and a certain set of features in the dataset that we consider important.</li>
<li><strong class="bold">Individual Conditional Expectation</strong> (<strong class="bold">ICE</strong>) <strong class="bold">Plots</strong>: This feature plots the dependency <a id="_idIndexMarker801"/>between the target<a id="_idIndexMarker802"/> feature and a certain set of features in the dataset that we consider important for each instance separately.</li>
</ul>
<p>Comparing this to the output you got from the model explainability interface in the experiment we performed in the <em class="italic">Working with the model explainability interface</em> section, you will notice that some of the explainability features are missing from the output. This is because some of these features are only available to the type of model trained. For example, residual analysis is only available for regression models, while the experiment conducted in the <em class="italic">Working with the model explainability interface</em> section is a classification problem that trained a classification model. Hence, you won’t find residual analysis in the model’s explainability output.</p>
<p>You can perform the same experiment using a regression problem; the model explainability interface will output regression-supported explainability features.</p>
<p>Now that we know about the different explainability features that are available in the explanation interface, let’s dive deep into them one by one to get an in-depth understanding of what they mean. We shall go through the output we got from our implementation of the explainability interface in Python and R. </p>
<p>In the previous chapters, we understood what the leaderboard and confusion matrix are. So, let’s start with the next explanation feature: residual analysis.</p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor154"/>Understanding residual analysis</h2>
<p><strong class="bold">Residual analysis</strong> is <a id="_idIndexMarker803"/>performed<a id="_idIndexMarker804"/> for <strong class="bold">regression models</strong>. As described in <a href="B17298_05.xhtml#_idTextAnchor109"><em class="italic">Chapter 5</em></a>, <em class="italic">Understanding AutoML Algorithms</em>, in the <em class="italic">Understanding generalized linear models</em> and <em class="italic">Introduction to linear regression</em> sections, <strong class="bold">residuals</strong> are<a id="_idIndexMarker805"/> the<a id="_idIndexMarker806"/> difference between the values predicted by the regression model and the actual values for that same row of data. Analyzing these residual values is a great way of diagnosing any problems in your model.</p>
<p>A residual analysis plot<a id="_idIndexMarker807"/> is a graph<a id="_idIndexMarker808"/> where you plot the <strong class="bold">residual values</strong> against the <strong class="bold">predicted values</strong>. Another<a id="_idIndexMarker809"/> thing we learned in <a href="B17298_05.xhtml#_idTextAnchor109"><em class="italic">Chapter 5</em></a>, <em class="italic">Understanding AutoML Algorithms</em>, in the <em class="italic">Understanding generalized linear models</em> and <em class="italic">Understanding the assumptions of linear regression</em> sections, is that one of the <a id="_idIndexMarker810"/>primary <a id="_idIndexMarker811"/>assumptions in <strong class="bold">linear regression</strong> is that the distribution of residuals is <strong class="bold">normally distributed</strong>.</p>
<p>So, accordingly, we expect our residual plot to be an amorphous collection of points. There should not be any patterns between the residual values and the predicted values. </p>
<p>Residual analysis can highlight the<a id="_idIndexMarker812"/> presence of <strong class="bold">heteroscedasticity</strong> in a trained model. Heteroscedasticity is said to have occurred if the standard deviation of the predicted values changes over different values of the features.</p>
<p>Consider the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer226">
<img alt="Figure 7.2 – Regression graph for a homoscedastic dataset " height="532" src="image/B17298_07_002.jpg" width="1091"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Regression graph for a homoscedastic dataset</p>
<p>The <a id="_idIndexMarker813"/>preceding diagram shows a regression plot where we had some sample data that maps the relationship between <em class="italic">X</em> and <em class="italic">Y</em>. Let’s fit a straight line through this data, which represents our linear model. If we calculate the residuals for every point as we go from left to right on the <em class="italic">X</em>-axis, we will notice that the error rate remains fairly constant throughout all the values of <em class="italic">X</em>. This means that all the error values lie between the parallel blue<a id="_idIndexMarker814"/> lines. Such a situation where the distribution of errors or residuals is constant throughout the independent variables is called<a id="_idIndexMarker815"/> homoscedasticity.</p>
<p>The <a id="_idIndexMarker816"/>opposite of <a id="_idIndexMarker817"/>homoscedasticity is <strong class="bold">heteroscedasticity</strong>. This is where the error rate varies over the change in the value of <em class="italic">X</em>. Refer to the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer227">
<img alt="Figure 7.3 – Regression graph for a heteroscedastic dataset " height="499" src="image/B17298_07_003.jpg" width="1017"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Regression graph for a heteroscedastic dataset</p>
<p>As you can see, the magnitude of the errors made by the linear model increases with an increase in <em class="italic">X</em>. If you plot the blue error lines encompassing all the errors, then you will notice that they gradually fan out and are not parallel. This situation where the distribution of errors or residuals is not constant throughout the independent variables is called heteroscedasticity.</p>
<p>What <a id="_idIndexMarker818"/>heteroscedasticity <a id="_idIndexMarker819"/>tells us is that there <a id="_idIndexMarker820"/>is some sort of information that the model has not been able to capture and learn from. Heteroscedasticity also violates linear regressions’ basic assumption. Thus, it can help you identify that you may need to add the missing information to your dataset to correctly train your linear model or that you may need to implement some non-linear regression algorithm to get a better-performing model.</p>
<p>Since residual analysis is a regression-specific model explainability feature, we cannot use the Iris dataset classification experiment that we performed in the <em class="italic">Working with the model explainability interface</em> section. Instead, we need to train a regression model and then use the model explainability interface on that model to get the residual analysis output. So, let’s look at a regression problem using the Red Wine Quality dataset. You can find this <a id="_idIndexMarker821"/>dataset at <a href="https://archive.ics.uci.edu/ml/datasets/wine+quality">https://archive.ics.uci.edu/ml/datasets/wine+quality</a>.</p>
<p>This dataset consists of the following features:</p>
<ul>
<li><strong class="bold">fixed acidity</strong>: This<a id="_idIndexMarker822"/> feature explains the amount of acidity that is non-volatile, meaning it does not evaporate over some time.</li>
<li><strong class="bold">volatile acidity</strong>: This<a id="_idIndexMarker823"/> feature explains the amount of acidity that is volatile, meaning it will evaporate over some time.</li>
<li><strong class="bold">citric acid</strong>: This<a id="_idIndexMarker824"/> feature explains the amount of citric acid present in the wine.</li>
<li><strong class="bold">residual sugar</strong>: This <a id="_idIndexMarker825"/>feature explains the amount of residual sugar present in the wine.</li>
<li><strong class="bold">chlorides</strong>: This<a id="_idIndexMarker826"/> feature explains the number of chlorides present in the wine.</li>
<li><strong class="bold">free sulfur dioxide</strong>: This <a id="_idIndexMarker827"/>feature explains the amount of free sulfur dioxide present in the wine.</li>
<li><strong class="bold">total sulfur dioxide</strong>: This<a id="_idIndexMarker828"/> feature explains the amount of total sulfur dioxide present in the wine.</li>
<li><strong class="bold">density</strong>: This<a id="_idIndexMarker829"/> feature explains the density of the wine.</li>
<li><strong class="bold">pH</strong>: This <a id="_idIndexMarker830"/>feature explains the pH value of the wine, with 0 being the most acidic and 14 being the most basic. </li>
<li><strong class="bold">sulphates</strong>: This<a id="_idIndexMarker831"/> feature explains the number of sulfates present in the wine.</li>
<li><strong class="bold">alcohol</strong>: This <a id="_idIndexMarker832"/>feature explains the amount of alcohol present in the wine.</li>
<li><strong class="bold">quality</strong>: This is the<a id="_idIndexMarker833"/> response column that notes the quality of the wine. 0 indicates that the wine is very bad, while 10 indicates that the wine is excellent.</li>
</ul>
<p>We will run our basic H2O AutoML process of training the model and then use the model explainability interface on the trained AutoML object to get the residual analysis plot.</p>
<p>Now, let’s observe the <a id="_idIndexMarker834"/>residual analysis plot that we get from this implementation and then see how we can retrieve the required information from the graph. Refer to the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer228">
<img alt="Figure 7.4 – Residual analysis graph plot for the Red Wine Quality dataset " height="657" src="image/B17298_07_004.jpg" width="1319"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Residual analysis graph plot for the Red Wine Quality dataset</p>
<p>Here, you can see the residual analysis for the stacked ensemble model, which is the leader of the AutoML trained models. On the <em class="italic">X</em>-axis, you have <strong class="bold">Fitted</strong>, also called predicted values, while on the <em class="italic">Y</em>-axis, you have <strong class="bold">Residuals</strong>. </p>
<p>On the left border of the <em class="italic">Y</em>-axis and below the <em class="italic">X</em>-axis you will see a <strong class="bold">grayscale</strong> column and row, respectively. These help you observe the distribution of those residuals across the <em class="italic">X</em> and <em class="italic">Y</em> axes.</p>
<p>To ensure that the <a id="_idIndexMarker835"/>distribution of residuals is normal and that the data is not heteroskedastic, you need to observe this grayscale on the <em class="italic">Y</em>-axis. A normal distribution would ideally give you a grayscale that is the darkest at the center and lightens as it moves away.</p>
<p>Now that you understood how to interpret the residual analysis graph, let’s learn more about the next explainability feature: variable importance.</p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor155"/>Understanding variable importance</h2>
<p><strong class="bold">Variable importance</strong>, also called <strong class="bold">feature importance</strong>, as the<a id="_idIndexMarker836"/> name suggests, explains the importance <a id="_idIndexMarker837"/>of the different <a id="_idIndexMarker838"/>variables/features in the dataset in making predictions. In any ML problem, your dataset will often have multiple variables that contribute to the characteristics of your prediction column. However, in most cases, you will often have some features that contribute more compared to others.</p>
<p>This understanding can help scientists and engineers remove any unwanted features that introduce noise from the dataset. This can further improve the quality of the model.</p>
<p>H2O calculates variable importance differently for different types of algorithms. First, let’s understand how variable importance is calculated for <strong class="bold">tree-based algorithms</strong>. </p>
<p>Variable importance in tree-based algorithms is calculated based on two criteria:</p>
<ul>
<li>Selection of the variable for deciding on the decision tree</li>
<li>Improvement in the squared error over the whole tree because of the selection</li>
</ul>
<p>Whenever H2O is building a decision tree as a part of training a tree-based model, it will use one of the features as a node to further split the tree. As we studied in <a href="B17298_05.xhtml#_idTextAnchor109"><em class="italic">Chapter 5</em></a>, <em class="italic">Understanding AutoML Algorithms</em>, in the <em class="italic">Understanding the Distributed Random Forest algorithm</em> section, we know that every node split in the decision tree aims to reduce the overall squared error. This deducted value is nothing but the difference between the squared errors of the parent node against the children node.</p>
<p>H2O considers this reduction in squared error in calculating the feature importance. The squared error for every node in the tree-based model leads to the variance of the response value for that node being lowered. The squared error for every node in the tree-based model leads to the variance of the response value for that node being lowered. </p>
<p>Thus, accordingly, the<a id="_idIndexMarker839"/> equation for calculating the squared<a id="_idIndexMarker840"/> error of the tree is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer229">
<img alt="" height="71" src="image/Formula_B17298_07_001.jpg" width="1650"/>
</div>
</div>
<p>Here, we have the following:</p>
<ul>
<li><em class="italic">MSE</em> means mean squared error</li>
<li><em class="italic">N</em> indicates the total number of observations</li>
<li><em class="italic">VAR</em> means variance</li>
</ul>
<p>The equation for calculating variance is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer230">
<img alt="" height="136" src="image/Formula_B17298_07_002.jpg" width="1650"/>
</div>
</div>
<p>Here, we have the following:</p>
<ul>
<li><img alt="" height="46" src="image/Formula_B17298_07_003.png" width="38"/> indicates the value of the observation</li>
<li><img alt="" height="38" src="image/Formula_B17298_07_004.png" width="36"/> indicates the mean of all the observations</li>
<li><em class="italic">N</em> indicates the total number of observations</li>
</ul>
<p>For tree-based ensemble<a id="_idIndexMarker841"/> algorithms such as the <strong class="bold">Gradient Boosting Algorithm</strong> (<strong class="bold">GBM</strong>), the decision trees are trained sequentially. Every tree is built on top of the previous tree’s errors. So, the feature importance calculation is the same as how we do it for individual nodes in single decision trees.</p>
<p>For <strong class="bold">Distributed Random Forest</strong> (<strong class="bold">DRF</strong>), the decision<a id="_idIndexMarker842"/> trees are trained in parallel, so H2O just averages the results to calculate the feature importance.</p>
<p>For <strong class="bold">XGBoost</strong>, H2O calculates<a id="_idIndexMarker843"/> the feature importance from the gains of the loss function for individual features when building the tree. </p>
<p>For <strong class="bold">deep learning</strong>, H2O<a id="_idIndexMarker844"/> calculates the feature importance using a special method called <a id="_idIndexMarker845"/>the <strong class="bold">Gedeon method</strong>.</p>
<p>For <strong class="bold">Generalized Linear Models</strong> (<strong class="bold">GLMs</strong>), variable<a id="_idIndexMarker846"/> importance is the same as the predictor weights, also called the coefficient magnitudes. If, during training, you decide to standardize the data, then the standardized coefficients are returned. </p>
<p>The following <a id="_idIndexMarker847"/>diagram shows the feature importance that<a id="_idIndexMarker848"/> was calculated for our experiment on the Iris flower dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer233">
<img alt="Figure 7.5 – Variable importance graph for the Iris flower dataset " height="742" src="image/B17298_07_005.jpg" width="1308"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Variable importance graph for the Iris flower dataset</p>
<p>The preceding diagram shows the variable importance map for a deep learning model. If you compare it with your leaderboard, you will see that the variable importance graph is plotted for the most leading model, which is not a stacked ensemble model.</p>
<p>On the <em class="italic">Y</em>-axis <a id="_idIndexMarker849"/>of the graph, you have the feature names – in<a id="_idIndexMarker850"/> our case, the <strong class="bold">C1</strong>, <strong class="bold">C2</strong>, <strong class="bold">C3</strong>, and <strong class="bold">C4</strong> columns of the Iris flower dataset. On the <em class="italic">X</em>-axis, you have the importance of these variables. It is possible to get the raw metric value of feature importance, but H2O displays the importance values by scaling them down between <strong class="bold">0</strong> and <strong class="bold">1</strong>, where <strong class="bold">1</strong> indicates the most important variable while <strong class="bold">0</strong> indicates the least important variable.</p>
<p>Since variable importance is available for both classification and regression models, you will also get a variable importance graph as an explainability feature of the Red Wine Quality regression model. The graph should look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer234">
<img alt="Figure 7.6 – Variable importance graph for the Red Wine Quality dataset  " height="661" src="image/B17298_07_006.jpg" width="1187"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Variable importance graph for the Red Wine Quality dataset </p>
<p>Now that you know how to interpret a feature importance graph, let’s understand feature importance heatmaps.</p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor156"/>Understanding feature importance heatmaps</h2>
<p>When displaying feature<a id="_idIndexMarker851"/> importance for a specific model, it is fairly easy to represent it as a histogram or bar graph. However, we often need to compare the feature<a id="_idIndexMarker852"/> importance of various models so that we can understand which feature is deemed important by which model and how we can use this information to compare model performance. H2O AutoML will inherently train multiple models with different ML algorithms. Therefore, a comparative study of the model performance is a must and a graphical representation of feature importance can be of great help to scientists and engineers.</p>
<p>To represent the feature importance of all the models trained by H2O AutoML in a single graph, H2O generates a heatmap of feature importance.</p>
<p>A heatmap is a data visualization graph where the color of the graph is affected by the amount of density or magnitude of a specific value. </p>
<p>Some H2O models compute the variable importance on encoded versions of categorical columns. Different models also have different ways of encoding categorical values. So, comparing the variable importance of these categorical columns across all the models can be tricky. H2O does this comparison by summarizing the variable importance across all the features and returning a single variable importance value that represents the original categorical feature.</p>
<p>The following is the <a id="_idIndexMarker853"/>feature importance heatmap for the Iris flower dataset <a id="_idIndexMarker854"/>experiment:</p>
<div>
<div class="IMG---Figure" id="_idContainer235">
<img alt="Figure 7.7 – Variable importance heatmap " height="555" src="image/B17298_07_007.jpg" width="904"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Variable importance heatmap</p>
<p>Here, we can see the top 10 models on the leaderboard.</p>
<p>The heatmap has the <strong class="bold">C1</strong>, <strong class="bold">C2</strong>, <strong class="bold">C3</strong>, and <strong class="bold">C4</strong> features on the <em class="italic">Y</em>-axis and the model IDs on the <em class="italic">X</em>-axis. The color of the plots indicates how important the model considers the feature during its prediction. More importance equals more value, which, in turn, turns the respective <a id="_idIndexMarker855"/>plot red. The lower the importance, the lower the importance <a id="_idIndexMarker856"/>value of the feature will be; the color will become cooler and become blue.</p>
<p>Now that you know how to interpret feature importance heatmaps, let’s learn about model correlation heatmaps.</p>
<h2 id="_idParaDest-120"><a id="_idTextAnchor157"/>Understanding model correlation heatmaps</h2>
<p>Another important <a id="_idIndexMarker857"/>comparison between multiple models is <strong class="bold">model correlation</strong>. Model correlation can be interpreted as how similar the models <a id="_idIndexMarker858"/>are in terms of performance when you<a id="_idIndexMarker859"/> compare their prediction values.</p>
<p>Different models trained using the same or different ML algorithms are said to be highly correlated if the predictions made by one model are the same or similar to the predictions made by the other.</p>
<p>In a model correlation heatmap, H2O compares the prediction values of all the models that it trains and compares them to one another. </p>
<p>The following is the model correlation heatmap graph we got from our experiment on the Iris flower dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer236">
<img alt="Figure 7.8 – Model correlation heatmap for the Iris flower dataset " height="710" src="image/B17298_07_008.jpg" width="963"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Model correlation heatmap for the Iris flower dataset</p>
<p class="callout-heading">Tip</p>
<p class="callout">To understand this explainability feature graph, kindly refer to the <em class="italic">Model Correlation</em> section in the output you got after executing the <strong class="source-inline">explain()</strong> function in your code.</p>
<p>On the <em class="italic">X</em> and <em class="italic">Y</em> axes, we<a id="_idIndexMarker860"/> have the model IDs. Their cross-section on the graph indicates the correlation value between them. You will notice that <a id="_idIndexMarker861"/>the heat points on the graph within the <em class="italic">X</em> and <em class="italic">Y</em> axes have the same model ID, which will always be 1; therefore, the plot will always be red. This is correct as, technically, it’s the same model and when you compare the prediction values of a model with itself, there will be 100% correlation.</p>
<p>To get a better idea of the correlation between different models, you can refer to these heat values. Dark red points indicate high correlation, while those with cool blue values indicate low correlation. Models highlighted in red are interpretable models such as GLMs.</p>
<p>You may <a id="_idIndexMarker862"/>notice that since the model correlation heatmap supports <a id="_idIndexMarker863"/>stacked ensemble models and feature importance heatmaps don’t, if you ignore the stacked ensemble models in the model correlation heatmap (<em class="italic">Figure 7.8</em>), the rest of the models are the same as the ones in the feature importance heatmap (<em class="italic">Figure 7.7</em>).</p>
<p>Now that you know how to interpret model correlation heatmaps, let’s learn more about partial dependency plots.</p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor158"/>Understanding partial dependency plots</h2>
<p>A <strong class="bold">Partial Dependence Plot</strong> (<strong class="bold">PDP</strong>) is a <a id="_idIndexMarker864"/>graph diagram that shows you the dependency between the predicted values <a id="_idIndexMarker865"/>and the <a id="_idIndexMarker866"/>set of input features that we are interested in while marginalizing the values of features in which we are not interested.</p>
<p>Another way of understanding a PDP is that it represents a function of input features that we are interested in that gives us the expected predicted values as output.</p>
<p>A PDP is a very interesting graph that is useful in showing and explaining the model training results to members of the organization that are not so skilled in the domain of data science.</p>
<p>First, let’s understand how to interpret a DPD before learning how it is calculated. The following diagram shows the PDP we got for our experiment when using the Iris flower dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer237">
<img alt="Figure 7.9 – PDP for the C1 column with Iris-setosa as the target " height="661" src="image/B17298_07_009.jpg" width="1229"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – PDP for the C<a id="_idTextAnchor159"/>1 column with Iris-setosa as the target</p>
<p class="callout-heading">Tip</p>
<p class="callout">To understand this explainability feature graph, kindly refer to the <em class="italic">Partial Dependency Plots</em> section in the output you got after executing the <strong class="source-inline">explain()</strong> function in your code.</p>
<p>The PDP plot <a id="_idIndexMarker867"/>is a graph that shows you the marginal effects of a feature <a id="_idIndexMarker868"/>on the response values. On the <em class="italic">X</em>-axis of the graph, you have the selected feature and its range of values. On the <em class="italic">Y</em>-axis, you have the mean response values for the target value. The PDP plot aims to tell the viewer what the mean response value predicted by the model for a given value of the selected feature is.</p>
<p>In <em class="italic">Figure 7.9</em>, the PDP graph is plotted for the <strong class="bold">C1</strong> column for the target value, which is <strong class="bold">Iris-setosa</strong>. On the <em class="italic">X</em>-axis, we have the <strong class="bold">C1</strong> column, which stands for the sepal length of the flower in centimeters. The range of these values ranges from the minimum value present in the dataset to the maximum value. On the <em class="italic">Y</em>-axis, we have the mean response values. For this experiment, the mean response values are probabilities that the flower is an Iris-setosa, which is the selected target value of the plot. The colorful lines on the graph indicate the mean response values predicted by the different models trained by H2O AutoML for the range of <strong class="bold">C1</strong> values.</p>
<p>Looking at this graph gives us a good idea of how the response value is dependent on the single feature, <strong class="bold">C1</strong>, for every individual model. We can see that so long as the sepal length lies between 4.5 to 6.5 centimeters, most of the models show an approximate probability that there is a 35% chance that the flower is of the Iris-setosa class.</p>
<p>Similarly, in the <a id="_idIndexMarker869"/>following graph, we have plotted the PDP graph for the <strong class="bold">C1</strong> column, only <a id="_idIndexMarker870"/>this time the target response column is <strong class="bold">Iris-versicolor</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer238">
<img alt="Figure 7.10 – PDP for the C1 column with Iris-versicolor as the target " height="663" src="image/B17298_07_010.jpg" width="1246"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – PDP for the C1 column with Iris-versicolor as the target</p>
<p class="callout-heading">Tip</p>
<p class="callout">To understand this explainability feature graph, kindly refer to the <em class="italic">Partial Dependency Plots</em> section in the output you got after executing the <strong class="source-inline">explain()</strong> function in your code.</p>
<p>Here, we can see that so long the values of <strong class="bold">C1</strong> are between 4.5 to 6.5, there is around a 27% to 40% chance that the flower is of the Iris-versicolor class. Now, let’s look at the following PDP plot for <strong class="bold">C1</strong> for the third target value, <strong class="bold">Iris-virginica</strong>: </p>
<div>
<div class="IMG---Figure" id="_idContainer239">
<img alt="Figure 7.11 – PDP for the C1 column with Iris-virginica as the target " height="651" src="image/B17298_07_011.jpg" width="1117"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – PDP for the C1 column with Iris-virginica as the target</p>
<p class="callout-heading">Tip</p>
<p class="callout">To better understand this explainability feature graph, kindly refer to the <em class="italic">Partial Dependency Plots</em> section in the output you got after executing the <strong class="source-inline">explain()</strong> function in your code.</p>
<p>You will <a id="_idIndexMarker871"/>notice that, for <strong class="bold">Iris-virginica</strong>, all the models predict differently for<a id="_idIndexMarker872"/> the same values of <strong class="bold">C1</strong>. This could mean that the <strong class="bold">Iris-virginica</strong> class is not strongly dependent on the sepal length of the flower – that is, the <strong class="bold">C1</strong> value. </p>
<p>Another case where PDP might be useful is in model selection. Let’s assume you are certain that a specific feature in your dataset will be contributing greatly to the response value and you train multiple models on it. Then, you can choose the model that best suits this relationship as that model will make the most realistically accurate predictions.</p>
<p>Now, let’s try to<a id="_idIndexMarker873"/> understand how the PDP plot is generated and how <a id="_idIndexMarker874"/>H2O computes these plot values.</p>
<p>The PDP plot data can be calculated as follows:</p>
<ol>
<li value="1">Choose a feature and target value to plot the dependency.</li>
<li>Bootstrap a dataset from the validation dataset, where the value of the selected feature is set to the minimum value present in the validation dataset for all rows.</li>
<li>Pass this bootstrapped dataset to one of the models trained by H2O AutoML and calculate the mean of the prediction values it got for all rows.</li>
<li>Plot this value on the PDP graph for that model.</li>
<li>Repeat <em class="italic">steps 3</em> and <em class="italic">4</em> for the remaining models.</li>
<li>Repeat <em class="italic">step 2</em>, but this time, increment the value of the selected feature to the next value present in the validation dataset. Then, repeat the remaining steps.</li>
</ol>
<p>You will do this for all the feature values present in the validation dataset and plot them on the results for all the models on the same PDP graph.</p>
<p>Once finished, you will repeat the same process for different combinations of the feature and target response values.</p>
<p>H2O will make multiple PDP plots for all the combinations of features and response values. The following is a PDP plot where the selected feature is <strong class="bold">C2</strong> and the selected target value is <strong class="bold">Iris-setosa</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer240">
<img alt="Figure 7.12 – PDP for the C2 column with Iris-setosa as the target " height="677" src="image/B17298_07_012.jpg" width="1253"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – PDP for the C2 column with Iris-setosa as the target</p>
<p class="callout-heading">Tip</p>
<p class="callout">To better understand this explainability feature graph, kindly refer to the <em class="italic">Partial Dependency Plots</em> section in the output you got after executing the <strong class="source-inline">explain()</strong> function in your code.</p>
<p>Similarly, it <a id="_idIndexMarker875"/>created different combinations of the PDP plot for the <strong class="bold">C3</strong> and <strong class="bold">C4</strong> features. The <a id="_idIndexMarker876"/>following is a PDP plot where the selected feature is <strong class="bold">C3</strong> and the selected target value is <strong class="bold">Iris-versicolor</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer241">
<img alt="Figure 7.13 – PDP for the C3 column with Iris-versicolor as the target " height="660" src="image/B17298_07_013.jpg" width="1139"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – PDP for the C3 column with Iris-versicolor as the target</p>
<p class="callout-heading">Tip</p>
<p class="callout">To better understand this explainability feature graph, kindly refer to the <em class="italic">Partial Dependency Plots</em> section in the output you got after executing the <strong class="source-inline">explain()</strong> function in your code.</p>
<p>Now that you know how to interpret feature importance heatmaps, let’s learn about SHAP summary plots.</p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor160"/>Understanding SHAP summary plots</h2>
<p>For sophisticated problems, tree-based models can become difficult to understand. Complex tree models can be very large and complicated to understand. The <strong class="bold">SHAP summary plot</strong> is a <a id="_idIndexMarker877"/>simplified<a id="_idIndexMarker878"/> graph of the tree-based model that gives you a summarized view of the model’s complexity and how it behaves.</p>
<p><strong class="bold">SHAP</strong> stands for <strong class="bold">Shapley Additive Explanations</strong>. SHAP is a model explainability feature that takes an approach<a id="_idIndexMarker879"/> from game theory to explain the output of an ML model. The SHAP summary plot shows you the contribution of the features toward predicting values, similar to PDPs.</p>
<p>Let’s try to interpret a<a id="_idIndexMarker880"/> SHAP value from an example. The following is the SHAP <a id="_idIndexMarker881"/>summary we get from the Red Wine Quality dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer242">
<img alt="Figure 7.14 – SHAP summary plot for the Red Wine Quality dataset " height="658" src="image/B17298_07_014.jpg" width="1078"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – SHAP summary plot for the Red Wine Quality dataset</p>
<p class="callout-heading">Tip</p>
<p class="callout">To better understand this explainability feature graph, kindly refer to the <em class="italic">SHAP Summary</em> section in the output you get after executing the <strong class="source-inline">explain()</strong> function on your regression model.</p>
<p>On the right-hand side, you can see a bluish-red bar. This bar represents the normalized value of the wine quality in color. The redder the color, the better the quality; the bluer the color, the poorer the wine quality. In binomial problems, the color will be a stark contrast between red and blue. However, in regression problems, like in our example, we can have a whole spectrum of colors, indicating the range of possible numerical values.</p>
<p>On the <em class="italic">Y</em>-axis, you have the features from the dataset. They are in descending order from top to bottom based on the feature’s importance. In our example, the alcohol content is the most important feature in the dataset; it contributes more to the final prediction value.</p>
<p>On the <em class="italic">X</em>-axis, you <a id="_idIndexMarker882"/>have the <strong class="bold">SHAP value</strong>. The SHAP value denotes how the feature <a id="_idIndexMarker883"/>helps the model toward the expected outcome. The more positive the SHAP value, the more the feature contributes to the outcome.</p>
<p>Let’s take the example of alcohol from the SHAP summary. Based on this, we can see that alcohol has the highest SHAP value among the rest of the features. Thus, alcohol contributes greatly to the model’s prediction. Also, the points on the graph for alcohol with the highest SHAP value are in red. This also indicates that high alcohol content contributes to a positive outcome. Keeping this in mind, what we can extract from this graph is that the feature alcohol content plays an important part in the prediction of the quality of the wine and that the higher the content of the alcohol, the better the quality of the wine.</p>
<p>Similarly, you can interpret the same knowledge from the other features. This can help you compare and understand which features are important and how they contribute to the final prediction of the model.</p>
<p>One interesting question<a id="_idIndexMarker884"/> on the SHAP summary and the PDP is, what is the difference between them? Well, the prime difference between the two is that PDP explains the effect of replacing only one feature at a time on the output, while the SHAP summary considers the overall interaction of that feature with other features in the dataset. So, PDP works on the assumption that your features are independent of one another, while SHAP takes into account the combined contributions of different features and their combined effects on the overall prediction.</p>
<p>Calculating <strong class="bold">SHAP values</strong> is a complex process that is derived from game theory. If you are interested in expanding your knowledge of game theory and how SHAP values are calculated, feel free to explore them at your own pace. A good starting point for understanding SHAP is to follow the explanations<a id="_idIndexMarker885"/> at https://shap.readthedocs.io/en/latest/index.xhtml. At the time of writing, H2O acts as a wrapper for the SHAP library and internally uses this library to calculate the SHAP values.</p>
<p>Now that we know how to interpret a SHAP summary plot, let’s learn about explainability feature, <strong class="bold">Individual Conditional Expectation</strong> (<strong class="bold">ICE</strong>)<a id="_idTextAnchor161"/> plots.</p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor162"/>Understanding individual conditional expectation plots</h2>
<p>An <strong class="bold">ICE</strong> plot <a id="_idIndexMarker886"/>is a graph that displays a line for every instance of<a id="_idIndexMarker887"/> an observation that shows how the prediction for the given observation changes when the value of a feature changes.</p>
<p>ICE plots are similar to PDP graphs. PDP focuses on the overall average effect of a change in a feature on the prediction outcome, while ICE plots focus on the dependency of the outcome on individual instances of the feature value. If you average the ICE plot values, you should get a PDP.</p>
<p>The way to compute ICE plots is very simple, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer243">
<img alt="Figure 7.15 – Sample dataset for ICE graph plots highlighting Observation 1 " height="225" src="image/B17298_07_015.jpg" width="1026"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Sample dataset for ICE graph plots highlighting Observation 1</p>
<p>Once your model has been trained, you must perform the following steps to calculate the ICE plots:</p>
<ol>
<li value="1">Consider the first observation – in our example, <strong class="bold">Observation 1</strong> – and plot the relationship between <strong class="bold">Feature 1</strong> and the respective <strong class="bold">Target</strong> value.</li>
<li>Keeping the values in <strong class="bold">Feature 1</strong> constant, create a bootstrapped dataset while replacing all the other feature values with those seen in <strong class="bold">Observation 1</strong> in the original dataset; mark all other observations as <strong class="bold">Observation 1</strong>.</li>
<li>Calculate<a id="_idIndexMarker888"/> the <strong class="bold">Target</strong> value of the <a id="_idIndexMarker889"/>observations using your trained model. </li>
</ol>
<p>Refer to the following screenshot for the bootstrapped dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer244">
<img alt="Figure 7.16 – Bootstrapped dataset for Observation 1 for Feature 1 " height="218" src="image/B17298_07_016.jpg" width="1110"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Bootstrapped dataset for Observation 1 for Feature 1</p>
<ol>
<li value="4">Repeat the same for the next observation. Consider the second observation – in our example, <strong class="bold">Observation 2</strong> – and plot the relationship between <strong class="bold">Feature 1</strong> and the respective <strong class="bold">Target</strong> value:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer245">
<img alt="Figure 7.17 – Sample dataset for an ICE plot highlighting Observation 2 " height="222" src="image/B17298_07_017.jpg" width="1159"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – Sample dataset for an ICE plot highlighting Observation 2</p>
<ol>
<li value="5">Keep the values in <strong class="bold">Feature 1</strong> constant and create a bootstrapped dataset; then, calculate the <strong class="bold">Target</strong> values using the trained model. Refer to the following resultant bootstrapped dataset:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer246">
<img alt="Figure 7.18 – Bootstrapped dataset for Observation 2 for Feature 1 " height="220" src="image/B17298_07_018.jpg" width="1001"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – Bootstrapped dataset for Observation 2 for Feature 1</p>
<ol>
<li value="6">We<a id="_idIndexMarker890"/> repeat this process for all observations <a id="_idIndexMarker891"/>against all features.</li>
<li>The results that are observed from these bootstrapped datasets are plotted on the individual ICE plots per feature.</li>
</ol>
<p>Let’s see how we can interpret the ICE plot and extract observable information out of the graph. Refer to the following screenshot, which shows the ICE plot we get after running the model explainability interface on the AutoML object that was trained on the Red Wine Quality dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer247">
<img alt="Figure 7.19 – ICE plot for the Red Wine Quality dataset " height="653" src="image/B17298_07_019.jpg" width="1147"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – ICE plot for the Red Wine Quality dataset</p>
<p>As the heading<a id="_idIndexMarker892"/> states, this is an ICE plot on the alcohol<a id="_idIndexMarker893"/> feature column of the dataset for a stacked ensemble model trained by H2O AutoML. Keep in mind that this model is the leader of the list of models trained by AutoML. ICE plots are only plotted for the leader of the dataset. You can also observe the ICE plots of the other models by extracting the models using their model IDs and then running the <strong class="source-inline">ice_plot()</strong> function on them. Refer to the following code example:</p>
<pre class="source-code">
model = h2o.get_model("XRT_1_AutoML_2_20220516_64004")
model.ice_plot(test_dataframe, "alcohol")</pre>
<p>On the <em class="italic">X</em>-axis of the graph, you have the range of values for the alcohol feature. On the <em class="italic">Y</em>-axis, you have the range of values of the predicted outcomes – that is, the quality of the wine.</p>
<p>On the left-hand side of the graph, you can see the legends stating different types of lines and their percentiles. The ICE plot plots the effects for each decile. So, technically, when plotting the ICE plot, you compute a line for every observation. However, in a dataset that contains thousands if not millions of rows of data, you will end up with an equal number of lines on the plot. This will make the ICE plot messy. That is why to better observe this data, you must aggregate the lines together to the nearest decile and plot a single line for every percentile partition.</p>
<p>The dotted <a id="_idIndexMarker894"/>black line is the average of all these other <a id="_idIndexMarker895"/>percentile lines and is nothing but the PDP line for that feature.</p>
<p>Now that you know how to interpret an ICE plot, let’s look at learning curve plots.</p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor163"/>Understanding learning curve plots</h2>
<p>The <strong class="bold">learning curve plot</strong> is<a id="_idIndexMarker896"/> one of the most used plots by data scientists to observe the learning rate of a model. The <strong class="bold">learning curve</strong> shows how your model learns from the <a id="_idIndexMarker897"/>dataset and the efficiency with <a id="_idIndexMarker898"/>which it does the learning. </p>
<p>When working on an ML problem, an important question that often needs to be answered is, <em class="italic">how much data do we need to train the most accurate model?</em> A learning curve plot can help you understand how increasing the dataset affects your overall model performance.</p>
<p>Using this information, you can decide if increasing the size of the dataset can result in better model performance or if you need to work on your model training to improve your model’s performance.</p>
<p>Let’s observe the learning curve plot we got from our experiment on the Red Wine Quality dataset for the XRT model trained by AutoML:</p>
<div>
<div class="IMG---Figure" id="_idContainer248">
<img alt="Figure 7.20 – Learning curve plot for the XRT model on the Red Wine Quality dataset " height="572" src="image/B17298_07_020.jpg" width="996"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 – Learning curve plot for the XRT model on the Red Wine Quality dataset</p>
<p>On the <em class="italic">X</em>-axis of<a id="_idIndexMarker899"/> the graph, you have the number of trees created by the XRT<a id="_idIndexMarker900"/> algorithm. As you can see, the algorithm created around 40 to 50 trees in total. On the <em class="italic">Y</em>-axis, you have the performance metric, RMSE, which is calculated at every stage during the model training as the algorithm creates the trees.</p>
<p>As shown in the preceding screenshot, the RMSE metric decreases as the algorithm creates more trees. Eventually, the rate at which the RMSE lowers decreases over a certain number of trees created. Any trees created over this number do not contribute to the overall improvement in the model’s performance. Thus, the learning rate eventually decreases over the increase in several trees.</p>
<p>The lines on the graph depict the various datasets that were used by the algorithm during training and the respective RMSE during every instance of creating the trees.</p>
<p>At the time of writing, as of H2O version <em class="italic">3.36.1</em>, the learning curve plot is not part of the default model explainability interface. To plot the learning curve, you must plot it using the following function on the respective model object:</p>
<pre class="source-code">
model = h2o.get_model("GLM_1_AutoML_2_20220516_64004")
model.learning_curve_plot()</pre>
<p>The learning <a id="_idIndexMarker901"/>curve plot is different for different algorithms. The following <a id="_idIndexMarker902"/>screenshot shows a learning plot for a GLM model trained by AutoML on the same dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer249">
<img alt="Figure 7.21 – Learning curve plot for the GLM model on the Red Wine Quality dataset " height="571" src="image/B17298_07_021.jpg" width="997"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.21 – Learning curve plot for the GLM model on the Red Wine Quality dataset</p>
<p>As you can see, instead of the number of trees on the <em class="italic">X</em>-axis, we now have iterations. The number of trees is relevant for tree-based algorithms such as XRT and DRF, but linear models such as GLM running on the linear algorithm makes more sense to aid with learning. On the <em class="italic">Y</em>-axis, you have deviance instead of RMSE as deviance is more suitable for measuring the performance of a linear model.</p>
<p>The learning curve<a id="_idIndexMarker903"/> is different for different types of algorithms, including stacked ensemble models. Feel free to explore the different variations of the learning curve for different algorithms. H2O already takes care of selecting the appropriate performance metric and the steps in learning, depending on the algorithms, so you don’t have to worry about whether you chose the right metric to measure the learning rate or not.</p>
<h1 id="_idParaDest-125"><a id="_idTextAnchor164"/>Summary</h1>
<p>In this chapter, we focused on understanding the model explainability interface provided by H2O. First, we understood how the explainability interface provides different explainability features that help users get detailed information about the models trained. Then, we learned how to implement this functionality on models trained by H2O’s AutoML in both Python and R.</p>
<p>Once we were comfortable with its implementation, we started exploring and understanding the various explainability graphs displayed by the explainability interface’s output, starting with residual analysis. We observed how residual analysis helps highlight heteroscedasticity in the dataset and how it helps you identify if there is any missing information in your dataset.</p>
<p>Then, we explored variable importance and how it helps you identify important features in the dataset. Building on top of this, we learned how feature importance heatmaps can help you observe feature importance among all the models trained by AutoML.</p>
<p>Then, we discovered how model correlation heatmaps can be interpreted and how they help us identify models with similar prediction behavior from a list of models.</p>
<p>Later, we learned about PDP graphs and how they express the dependency of the overall outcome over the individual features of the dataset. With this knowledge in mind, we explored the SHAP summary and ICE plots, where we understood the two graphs and how each focuses on different aspects of outcome dependency on individual features.</p>
<p>Finally, we explored what a learning plot is and how it helps us understand how the model improves in performance, also called learning, over the number of observations, iterations, or trees, depending on the type of algorithms used to train the model. </p>
<p>In the next chapter, we shall use all the knowledge we’ve learned from the last few chapters and explore the other advanced parameters that are available when using H2O’s AutoML feature.</p>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer251">
<h1 id="_idParaDest-126"><a id="_idTextAnchor165"/>Part 3 H2O AutoML Advanced Implementation and Productization</h1>
<p>This part will help you understand H2O AutoML’s advanced features and parameters used to customize certain characteristics of AutoML to suit specialized needs. This will help you get the desired personalized results that generalized machine learning fails to provide. It will also explain the various ways that H2O AutoML can be used with different types of technologies, and you will understand how you can deploy your machine learning models into production, and commercially use them to meet business needs.</p>
<p>This section comprises the following chapters:</p>
<ul>
<li><a href="B17298_08.xhtml#_idTextAnchor169"><em class="italic">Chapter 8</em></a>, <a id="_idTextAnchor166"/><em class="italic">Exploring Optional Parameters for H2O AutoML</em></li>
<li><a href="B17298_09.xhtml#_idTextAnchor186"><em class="italic">Chapter 9</em></a>, <a id="_idTextAnchor167"/><a href="https://epic.packtpub.com/index.php?module=oss_Chapters&amp;action=DetailView&amp;record=3a065625-7e22-e0bf-231f-61a9d1f3e976"><em class="italic">Exploring Miscellaneous Features in H2O AutoML</em></a></li>
<li><em class="italic"><a id="_idTextAnchor168"/></em><a href="B17298_10.xhtml#_idTextAnchor196"><em class="italic">Chapter 10</em></a>, <a href="https://epic.packtpub.com/index.php?module=oss_Chapters&amp;action=DetailView&amp;record=bb77c8ca-d15c-48c5-2b00-61a9d1abce98"><em class="italic">Working with Plain Old Java Objects (POJOs)</em></a></li>
<li><a href="B17298_11.xhtml#_idTextAnchor210"><em class="italic">Chapter 11</em></a>, <em class="italic">Working with Model Object Optimized (MOJO)</em></li>
<li><a href="B17298_12.xhtml#_idTextAnchor225"><em class="italic">Chapter 12</em></a>, <a href="https://epic.packtpub.com/index.php?module=oss_Chapters&amp;action=DetailView&amp;record=854f151d-1690-0982-b488-61a9d16f9b67"><em class="italic">Working with H2O AutoML and Apache Spark</em></a></li>
<li><a href="B17298_13.xhtml#_idTextAnchor239"><em class="italic">Chapter 13</em></a>, <em class="italic">Using H2O AutoML with Other Technologies</em></li>
</ul>
</div>
</div></body></html>