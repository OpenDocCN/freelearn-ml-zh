- en: 03 Cassava Leaf Disease competition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Join our book community on Discord
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/file10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this chapter we will leave the domain of tabular data and focus on image
    processing. In order to demonstrate the steps necessary to do well in classification
    competitions, we will use the data from the Cassava Leaf Disease contest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/competitions/cassava-leaf-disease-classification](https://www.kaggle.com/competitions/cassava-leaf-disease-classification)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do upon starting a Kaggle competition is to read the descriptions
    properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“As the second-largest provider of carbohydrates in Africa, cassava is a key
    food security crop grown by smallholder farmers because it can withstand harsh
    conditions. At least 80% of household farms in Sub-Saharan Africa grow this starchy
    root, but viral diseases are major sources of poor yields. With the help of data
    science, it may be possible to identify common diseases so they can be treated.”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So this competition relates to an actually important real-life problem - your
    mileage might vary, but in general it is useful to know that.
  prefs: []
  type: TYPE_NORMAL
- en: '*“Existing methods of disease detection require farmers to solicit the help
    of government-funded agricultural experts to visually inspect and diagnose the
    plants. This suffers from being labor-intensive, low-supply and costly. As an
    added challenge, effective solutions for farmers must perform well under significant
    constraints, since African farmers may only have access to mobile-quality cameras
    with low-bandwidth.”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This paragraph - especially the last sentence - sets the expectations: since
    the data is coming from diverse sources, we are likely to have some challenges
    related to quality of the images and (possibly) distribution shift.'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Your task is to classify each cassava image into four disease categories
    or a fifth category indicating a healthy leaf. With your help, farmers may be
    able to quickly identify diseased plants, potentially saving their crops before
    they inflict irreparable damage.”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This bit is rather important: it specifies that this is a classification competition,
    and the number of classes is small (5).'
  prefs: []
  type: TYPE_NORMAL
- en: With the introductory footwork out of the way, let us have a look at the data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data and metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Upon entering the “Data” tab for this competition, we see the summary of the
    provided data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: description of the Cassava competition dataset](img/file11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: description of the Cassava competition dataset'
  prefs: []
  type: TYPE_NORMAL
- en: What can we make of that?
  prefs: []
  type: TYPE_NORMAL
- en: The data is in a fairly straightforward format, where the organisers even provided
    the mapping between disease names and numerical codes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have the data in tfrecord format, which is good news for anyone interested
    in using a TPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The provided test set is only a small subset and to be substituted with the
    full dataset at evaluation time. **This suggests that loading a previously trained
    model at evaluation time and using it for inference is a preferred strategy.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The evaluation metric was chosen to be categorization accuracy: [https://developers.google.com/machine-learning/crash-course/classification/accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy).
    This metric takes discrete values as inputs, which means potential ensembling
    strategies become somewhat more involved. Loss function is implemented during
    training to optimise a learning function and as long as we want to use methods
    based on gradient descent, this one needs to be continuous; evaluation metric,
    on the other hand, is used after training to measure overall performance and as
    such, can be discrete.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: without building a model, write a code to conduct a basic EDA'
  prefs: []
  type: TYPE_NORMAL
- en: Compare the cardinality of classes in our classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normally, this would also be the moment to check for distribution shift: if
    the images are very different in train and test set, it is something that most
    certainly needs to be taken into account. However, since we do not have access
    to the complete dataset in this case, the step is omitted - please check Chapter
    1 for a discussion of adversarial validation, which is a popular technique for
    detecting concept drift between datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Building a baseline model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start our approach by building a baseline solution. The notebook running
    an end-to-end solution is available at:'
  prefs: []
  type: TYPE_NORMAL
- en: While hopefully useful as a starting point for other competitions you might
    want to try, it is more educational to follow the flow described in this section,
    i.e. copying the code cell by cell, so that you can understand it better (and
    of course improve on it - it is called a baseline solution for a reason).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: the imports needed for our baseline solution](img/file12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: the imports needed for our baseline solution'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing the necessary packages - while personal differences in
    style are a natural thing, it is our opinion that gathering the imports in one
    place makes the code easier to maintain as the competition progresses and you
    move towards more elaborate solutions. In addition, we create a configuration
    class: a placeholder for all the parameters defining our learning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: configuration class for our baseline solution](img/file13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: configuration class for our baseline solution'
  prefs: []
  type: TYPE_NORMAL
- en: 'The components include:'
  prefs: []
  type: TYPE_NORMAL
- en: The data folder is mostly useful if you train models outside of Kaggle sometimes
    (e.g. in Google Colab or on your local machine)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BATCH_SIZE is a parameter that sometimes needs adjusting if you want to optimise
    your training process (or make it possible at all, for large images in constrained
    memory environment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Modifying EPOCHS is useful for debugging: start with small number of epochs
    to verify that your solution is running smoothly end-to-end and increase as you
    are moving towards a proper solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TARGET_SIZE defines the size to which we want to rescale our images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NCLASSES corresponds to the number of possible classes in our classification
    problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A good practice for coding a solution is to encapsulate the important bits
    in functions - and creating our trainable model certainly qualifies as important:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: function to create our model](img/file14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: function to create our model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Few remarks around this step:'
  prefs: []
  type: TYPE_NORMAL
- en: While more expressive options are available, it is practical to begin with a
    fast model that can quickly iterated upon; EfficientNet [https://paperswithcode.com/method/efficientnet](https://paperswithcode.com/method/efficientnet)
    architecture fits the bill quite well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We add a pooling layer for regularisation purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a classification head - a Dense layer with CFG.NCLASSES indicating the number
    of possible results for the classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we compile the model with loss and metric corresponding to the requirements
    for this competition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exercise**: Examine the possible choices for loss and metric - a useful guide
    is [https://keras.io/api/losses/](https://keras.io/api/losses/) What would the
    other reasonable options be?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next step is the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/file15.png)![Figure 3.5: setting up the data generator](img/file16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: setting up the data generator'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we setup the model - straightforward, thanks to the function we defined
    above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6: instantiating the model](img/file17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: instantiating the model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed with training the model, we should dedicate some attention
    to callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Model callbacks](img/file18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Model callbacks'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some points worth mentioning:'
  prefs: []
  type: TYPE_NORMAL
- en: ModelCheckpoint is used to ensure we keep the weights for the best model only,
    where the optimality is decided on the basis of the metric to monitor (validation
    loss in this instance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EarlyStopping helps us control the risk of overfitting by ensuring that the
    training is stopped if the validation loss does not improve (decrease) for a given
    number of epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReduceLROnPlateau is a schema for learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exercise**: what parameters would it make sense to modify in the above setup,
    and which ones can be left at their default values?'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this setup, we can fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Fitting the model](img/file19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Fitting the model'
  prefs: []
  type: TYPE_NORMAL
- en: Once the training is complete, we can use the model to build the prediction
    of image class for each image in the test set. Recall that in this competition
    the public (visible) test set consisted of a single image and the size of the
    full one was unknown - hence the need for a slightly convoluted manner of constructing
    the submission dataframe
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: Generating a submission](img/file20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Generating a submission'
  prefs: []
  type: TYPE_NORMAL
- en: In this section we have demonstrated how to start to compete in a competition
    focused on image classification - you can use this approach to move quickly from
    basic EDA to a functional submission. However, a rudimentary approach like this
    is unlikely to produce very competitive results. For this reason, in the next
    section we discuss more specialised techniques that were utilised in top scoring
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Learnings from top solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section we gather aspects from the top solutions that could allow us
    to rise above the level of the baseline solution. Keep in mind that the leaderboard
    (both public and private) in this competition were quite tight; this was a combination
    of a few factors:'
  prefs: []
  type: TYPE_NORMAL
- en: The noisy data - it was easy to get to .89 accuracy by correctly identifying
    large part of the train data, and then each new correct one allowed for a tiny
    move upward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metric - accuracy can be tricky to ensemble
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limited size of the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretraining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First and most obvious remedy to the issue of limited data size was pretraining:
    using more data. The Cassava competition was held a year before as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/competitions/cassava-disease/overview](https://www.kaggle.com/competitions/cassava-disease/overview)'
  prefs: []
  type: TYPE_NORMAL
- en: 'With minimal adjustments, the data from the 2019 edition could be leveraged
    in the context of the current one. Several competitors addressed the topic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Combined 2019 + 2020 dataset in TFRecords format was released in the forum:
    [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/199131](https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/199131)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Winning solution from the 2019 edition served as a useful starting point: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/216985](https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/216985)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating predictions on 2019 data and using the pseudo-labels to augment the
    dataset was reported to yield some (minor) improvements [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/203594](https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/203594)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TTA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea behind Test Time Augmentation (TTA) is to apply different transformations
    to the test image: rotations, flipping and translations. This creates a few different
    versions of the test image and we generate a prediction for each of them. The
    resulting class probabilities are then averaged to get a more confident answer.
    An excellent demonstration of this technique is given in a notebook by Andrew
    Khael: [https://www.kaggle.com/code/andrewkh/test-time-augmentation-tta-worth-it](https://www.kaggle.com/code/andrewkh/test-time-augmentation-tta-worth-it)'
  prefs: []
  type: TYPE_NORMAL
- en: 'TTA was used extensively by the top solutions in the Cassava competition, an
    excellent example being the top3 private LB result: [https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150](https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150)'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While more known architectures like ResNext and EfficientNet were used a lot
    in the course of the competition, it was the addition of more novel ones that
    provided the extra edge to many competitors yearning for progress in a tightly
    packed leaderboard. Transformers emerged in 2017 as a revolutionary architecture
    for NLP (if somehow you missed the paper that started it all, here it is: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))
    and were such a spectacular success that inevitably many people started wondering
    if they could be applied to other modalities as well - vision being an obvious
    candidate. Aptly named Vision Transformer made one of its first appearances in
    a Kaggle competition in the Cassava contest. An excellent tutorial for ViT has
    been made public:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline](https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Vision Transformer was an important component of the winning solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150](https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The core idea of ensembling is very popular on Kaggle (see Chapter 9 of the
    Kaggle Book for a more elaborate description) and Cassava competition was no exception.
    As it turned out, combining diverse architectures was very beneficial (by averaging
    the class probabilities): EfficientNet, ResNext and ViT are sufficiently different
    from each other that their predictions complement each other. Another important
    angle was stacking, i.e. using models in two stages: the 3rd place solution combined
    predictions from multiple models and then applied LightGBM as a blender'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/220751](https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/220751)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The winning solution involved a different approach (with fewer models in the
    final blend), but relying on the same core logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957](https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have described how to get started with a baseline solution
    for an image classification competition and discussed a diverse set of possible
    extensions for moving to a competitive (medal) zone.
  prefs: []
  type: TYPE_NORMAL
