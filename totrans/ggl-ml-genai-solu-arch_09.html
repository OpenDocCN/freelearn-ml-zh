<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer128" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-153"><a id="_idTextAnchor215" class="calibre6 pcalibre pcalibre1"/>7</h1>
<h1 id="_idParaDest-154" class="calibre5"><a id="_idTextAnchor216" class="calibre6 pcalibre pcalibre1"/>Feature Engineering and Dimensionality Reduction</h1>
<p class="calibre3">In this chapter, we will dive progressively deeper into the kinds of data processing steps that are common in many data science projects, and how to perform those steps using Vertex AI in Google Cloud. We’ll begin this chapter by taking a more detailed look at how features are used in machine learning workloads, and what kinds of challenges often arise concerning how features <span>are used.</span></p>
<p class="calibre3">We will then transition our discussion to focus on how to address those challenges, and how to use our machine learning features effectively in <span>Google Cloud.</span></p>
<p class="calibre3">This chapter covers the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">Fundamental concepts related to dimensions or features in <span>machine learning</span></li>
<li class="calibre8">An introduction to the curse <span>of dimensionality</span></li>
<li class="calibre8"><span>Dimensionality reduction</span></li>
<li class="calibre8"><span>Feature engineering</span></li>
<li class="calibre8">Vertex AI <span>Feature Store</span></li>
</ul>
<h1 id="_idParaDest-155" class="calibre5"><a id="_idTextAnchor217" class="calibre6 pcalibre pcalibre1"/>Fundamental concepts in this chapter</h1>
<p class="calibre3">In this <a id="_idIndexMarker755" class="calibre6 pcalibre pcalibre1"/>section, we’ll briefly cover concepts that provide additional context for this chapter’s <span>learning activities.</span></p>
<h2 id="_idParaDest-156" class="calibre9"><a id="_idTextAnchor218" class="calibre6 pcalibre pcalibre1"/>Dimensions and features</h2>
<p class="calibre3">We <a id="_idIndexMarker756" class="calibre6 pcalibre pcalibre1"/>introduced the concept of features in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a> and described examples of features using the King County housing sales dataset for illustration. To briefly recap, features are individual, measurable properties or characteristics of the observations in our dataset. They are the aspects of our dataset from which a machine learning algorithm learns to create a model. In other words, a model can be seen as a representation of patterns learned by the algorithm from the features in <span>our dataset.</span></p>
<p class="calibre3">The features of a house, for example, include information such as how many rooms it contains, the year it was constructed, where it is located, and other factors that describe the house, as depicted in <span><em class="italic">Table 7.1</em></span><span>:</span></p>
<p class="img---caption" lang="en-US" xml:lang="en-US"><a id="_idIndexMarker757" class="calibre6 pcalibre pcalibre1"/> </p>
<div class="calibre2">
<div class="img---figure" id="_idContainer118">
<img alt="Table 7.1: King County house sales features " src="image/B18143_01_9.jpg" class="calibre26"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.1: King County house sales features</p>
<p class="calibre3">When <a id="_idIndexMarker758" class="calibre6 pcalibre pcalibre1"/>we’re dealing with tabular data, features are generally represented as columns in our dataset, and each row represents an individual data point or observation, sometimes referred to as <span>an </span><span><strong class="bold">instance</strong></span><span>.</span></p>
<p class="calibre3">Features are also referred to as variables, attributes, or dimensions. So, when we talk about the dimensionality of a dataset, it relates to how many features or dimensions our dataset has, and how that affects our machine <span>learning workloads.</span></p>
<h2 id="_idParaDest-157" class="calibre9"><a id="_idTextAnchor219" class="calibre6 pcalibre pcalibre1"/>Overfitting, underfitting, and regularization</h2>
<p class="calibre3">We briefly discussed the concepts of overfitting and underfitting in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, and we will <a id="_idIndexMarker759" class="calibre6 pcalibre pcalibre1"/>continue to revisit these topics in <a id="_idIndexMarker760" class="calibre6 pcalibre pcalibre1"/>more detail throughout <a id="_idIndexMarker761" class="calibre6 pcalibre pcalibre1"/>this book since they <a id="_idIndexMarker762" class="calibre6 pcalibre pcalibre1"/>are so fundamentally important to the process of machine learning. In this <a id="_idIndexMarker763" class="calibre6 pcalibre pcalibre1"/>section, we’ll discuss how the number of features in our dataset can affect how our algorithms learn from our data. A key concept to keep in mind is that overfitting and underfitting can be strongly influenced by how many observations we have in our dataset, and how many features we have for <span>each observation.</span></p>
<p class="calibre3">We usually need to find the right balance between these two aspects of our dataset. For example, if we have very few observations and a lot of features for each observation, our model is likely to overfit the dataset because it learns very specific patterns for those <a id="_idIndexMarker764" class="calibre6 pcalibre pcalibre1"/>observations and their features, but it cannot generalize well to new observations. Conversely, if we have many <a id="_idIndexMarker765" class="calibre6 pcalibre pcalibre1"/>observations, but very few pieces of information (that is, features) for each observation, then our model may not be <a id="_idIndexMarker766" class="calibre6 pcalibre pcalibre1"/>able to learn any valuable patterns, meaning it will underfit our dataset. Because of this, reducing the number of features can help reduce overfitting, but only <a id="_idIndexMarker767" class="calibre6 pcalibre pcalibre1"/>to an extent – removing too many features may result in underfitting. Also, we don’t want to remove features that contain useful information for our models to learn, so another way that we can address overfitting while keeping many of our features is to use a mechanism called regularization. We also briefly mentioned this in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, and it’s something we will discuss in more <span>detail here.</span></p>
<h3 class="calibre11">Regularization</h3>
<p class="calibre3">To begin our <a id="_idIndexMarker768" class="calibre6 pcalibre pcalibre1"/>discussion on regularization, we need to bring up the concept <a id="_idIndexMarker769" class="calibre6 pcalibre pcalibre1"/>of the loss function in machine learning once more, something we introduced in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>. Remember that many machine learning algorithms work by trying to find the best coefficients (or weights) for each feature that result in the closest approximation of the target feature. So, overfitting is influenced by the mathematical relationship between the features and their coefficients. If we find that a model is overfitting to specific features, we can use regularization to reduce the influence of those features and their coefficients on <span>the model.</span></p>
<p class="calibre3">Because overfitting usually happens when a model is too complex, such as having too many features relative to the number of observations, regularization addresses this issue by adding a penalty to the loss function, which discourages the model from assigning too much importance to any feature. This helps improve the generalizability of <span>the model.</span></p>
<p class="calibre3">There are several ways to implement regularization in machine learning, but I’ll explain two of the most common types here – that is, <strong class="bold">L1</strong> and <strong class="bold">L2</strong> regularization – as well as a combination of both approaches, referred to as <span><strong class="bold">elastic net</strong></span><span>.</span></p>
<h4 class="calibre20">L1 regularization</h4>
<p class="calibre3">This type of <a id="_idIndexMarker770" class="calibre6 pcalibre pcalibre1"/>regularization is also known as <strong class="bold">Lasso</strong> regularization, and it <a id="_idIndexMarker771" class="calibre6 pcalibre pcalibre1"/>works by adding a penalty equivalent <a id="_idIndexMarker772" class="calibre6 pcalibre pcalibre1"/>to the L1 norm (or the absolute value) of the coefficients or weights in the cost function by using the <span>following formula:</span></p>
<p class="author-quote">Cost Function + λ * |weights|</p>
<p class="calibre3">Here, λ is the regularization parameter, which controls the strength of the penalty and can be considered a hyperparameter whose optimal value can vary depending on the problem. Note that if the penalty is too strong, it can result in underfitting, so it’s important to find the right balance in <span>this regard.</span></p>
<p class="calibre3">The effect of L1 regularization is to shrink some of the model’s coefficients to exactly zero, effectively excluding the corresponding feature from the model, which makes L1 regularization useful for feature selection (something we’ll cover in more detail shortly) when dealing with <span>high-dimensional data.</span></p>
<h4 class="calibre20">L2 regularization</h4>
<p class="calibre3">This type <a id="_idIndexMarker773" class="calibre6 pcalibre pcalibre1"/>of regularization <a id="_idIndexMarker774" class="calibre6 pcalibre pcalibre1"/>is also known as <strong class="bold">Ridge</strong> regularization. This <a id="_idIndexMarker775" class="calibre6 pcalibre pcalibre1"/>method adds a penalty equivalent to the L2 norm (or the square) of the coefficients in the cost function by using the <span>following formula:</span></p>
<p class="author-quote">Cost Function + λ * (weights^2)</p>
<p class="calibre3">Unlike L1 regularization, L2 regularization doesn’t result in the exclusion of features but rather pushes the coefficients close to zero, distributing the weights evenly among the features. This can be beneficial when we’re dealing with correlated features as it allows the model to keep all of them <span>under consideration.</span></p>
<h4 class="calibre20">Elastic net</h4>
<p class="calibre3">Elastic net, as a <a id="_idIndexMarker776" class="calibre6 pcalibre pcalibre1"/>combination of L1 and L2 regularization, was designed to provide <a id="_idIndexMarker777" class="calibre6 pcalibre pcalibre1"/>a compromise between these two methods, incorporating the strengths of both. Like L1 and L2 regularization, elastic net adds a penalty to the loss function, but instead of adding an L1 penalty or an L2 penalty, it adds a weighted sum of both by using the <span>following formula:</span></p>
<p class="author-quote">Cost Function + λ1 * |weights| + λ2 * (weights^2)</p>
<p class="calibre3">Here, λ1 and λ2 are hyperparameters that control the strength of the L1 and L2 penalties, respectively. If λ1 is zero, elastic net reduces to Ridge regression, and if λ2 is zero, it reduces to <span>Lasso regression.</span></p>
<p class="calibre3">Elastic net has the feature selection capability of L1 regularization (since it can shrink coefficients to zero) and the regularization strength of L2 regularization (since it can distribute <a id="_idIndexMarker778" class="calibre6 pcalibre pcalibre1"/>weights evenly among correlated features). The trade-off with elastic net is that it has two hyperparameters to tune, rather than just one in <a id="_idIndexMarker779" class="calibre6 pcalibre pcalibre1"/>the case of Lasso or Ridge, which can make the model more complex and computationally intensive <span>to train.</span></p>
<p class="calibre3">Now that we’ve covered the important topic of regularization in more detail, let’s dive into feature selection and <span>feature engineering.</span></p>
<h2 id="_idParaDest-158" class="calibre9"><a id="_idTextAnchor220" class="calibre6 pcalibre pcalibre1"/>Feature selection and feature engineering</h2>
<p class="calibre3">We briefly discussed feature engineering in previous chapters of this book, but we will explore these concepts in more detail here. In <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, we used the example of creating a <a id="_idIndexMarker780" class="calibre6 pcalibre pcalibre1"/>new feature named <strong class="source-inline">price-per-square-foot</strong> in our housing data by dividing the total cost of <a id="_idIndexMarker781" class="calibre6 pcalibre pcalibre1"/>each house by the total area of that house in square feet. We will explore many additional examples of feature engineering in <span>this chapter.</span></p>
<p class="calibre3">However, creating new <a id="_idIndexMarker782" class="calibre6 pcalibre pcalibre1"/>features from existing ones is not the only type <a id="_idIndexMarker783" class="calibre6 pcalibre pcalibre1"/>of activity we need to perform on our features when preparing our dataset for training a machine learning model. We also need to select which features we think could be most important for achieving the task that we want our model to achieve, such as predicting the price of a house. As we saw in <a href="B18143_06.xhtml#_idTextAnchor187" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 6</em></span></a>, we may also need to perform transformations on our features, such as ensuring that they are all represented on a common, <span>standardized scale.</span></p>
<p class="calibre3">The goal in selecting and engineering features is to provide our model with the most relevant information, in the most digestible format, so that it can most effectively learn from <span>the data.</span></p>
<h2 id="_idParaDest-159" class="calibre9"><a id="_idTextAnchor221" class="calibre6 pcalibre pcalibre1"/>The curse of dimensionality</h2>
<p class="calibre3">High-dimensional datasets contain many dimensions or features for each observation in the dataset. It would be possible to presume that, the more features we include for each data point, the more information our model will learn, and therefore, the more accurate <a id="_idIndexMarker784" class="calibre6 pcalibre pcalibre1"/>our model will be. However, note that not all features are equally useful. Some may contain little to no <a id="_idIndexMarker785" class="calibre6 pcalibre pcalibre1"/>useful information for our model, others may contain redundant information, and some may even be harmful to the model’s ability to learn. Part of the art and science of machine learning is figuring out which features to use and how to prepare them in a way that allows the model to perform <span>its best.</span></p>
<p class="calibre3">Also, bear in mind that, the more information we have in our dataset, the more information our model has to process. This directly translates to additional computing resources being required for our machine learning algorithm to process our dataset, which, in turn, directly translates to longer model training times, and increased monetary cost. Too much irrelevant data or <strong class="bold">noise</strong> in the dataset can also make it harder for the algorithm to identify (that is, to learn) patterns in the data. The ideal scenario, then, is to find the minimum number of features that provide the maximum amount of useful information to our model. If we can achieve the same results with three features or ten features, for example, it’s generally better to go with the option of using three features. The “maximum amount of useful information” can be measured in terms of <strong class="bold">variance</strong>, whereby features with high relative variance are those that influence our model’s outcomes most prominently, and features with little relative variance are often not as useful in training our model to <span>identify patterns.</span></p>
<p class="calibre3">The “curse of dimensionality” is a term that’s used in the data science industry to describe the challenges that arise when dealing with datasets that contain higher numbers of dimensions. Let’s take a look at what some of these challenges are. In subsequent sections, we will discuss mechanisms to address <span>these challenges.</span></p>
<h3 class="calibre11">Data exploration challenges</h3>
<p class="calibre3">This is an <a id="_idIndexMarker786" class="calibre6 pcalibre pcalibre1"/>important point at which we <a id="_idIndexMarker787" class="calibre6 pcalibre pcalibre1"/>can introduce the link between “dimensions” in our dataset, and the dimensions of physical space. As we all know, and as mentioned earlier in this book, humans can only perceive our physical world in up to a maximum of three dimensions (width, height, and depth), with “time” considered as the fourth dimension of our physical reality. For datasets that have two or three dimensions, we can easily create visualizations representing various aspects of those datasets, but we can’t create graphs or other visual representations of higher-dimensional <a id="_idIndexMarker788" class="calibre6 pcalibre pcalibre1"/>datasets. In such cases, it helps if we can try <a id="_idIndexMarker789" class="calibre6 pcalibre pcalibre1"/>to find other ways of visually interpreting those datasets, such as by projecting them down into lower-dimensional representations, something we will explore in more <span>detail shortly.</span></p>
<h3 class="calibre11">Feature sparsity</h3>
<p class="calibre3">In high-dimensional space, points (that is, instances or samples) in our dataset tend to be far away from each other, leading to sparsity. Generally, as the number of features increases <a id="_idIndexMarker790" class="calibre6 pcalibre pcalibre1"/>relative to the number of observations in our dataset, the feature space becomes increasingly sparse, and this sparsity makes <a id="_idIndexMarker791" class="calibre6 pcalibre pcalibre1"/>it more difficult for algorithms to learn from the data since they have fewer examples to learn from in the vicinity of any <span>given point.</span></p>
<p class="calibre3">As an example, let’s imagine that our dataset consists of information regarding a company’s customers, in which case each instance in the dataset represents a person, and each feature represents some characteristic of a person. If our dataset stores hundreds or even thousands of characteristics, then it’s unlikely that all characteristics for every person will be populated. As such, the overall feature space for our dataset will be sparsely populated. On the other hand, if we have fewer features, this is less likely <span>to occur.</span></p>
<h3 class="calibre11">Distance measurements</h3>
<p class="calibre3">In previous chapters, we talked about how Euclidean distance is used in many machine learning <a id="_idIndexMarker792" class="calibre6 pcalibre pcalibre1"/>algorithms to find potential relationships or differences between data points in our dataset. One of the most direct <a id="_idIndexMarker793" class="calibre6 pcalibre pcalibre1"/>examples of this concept that we’ve explored already is the K-means clustering algorithm. In high-dimensional spaces, distances can sometimes become less meaningful, because the difference between the maximum and minimum possible distances becomes increasingly smaller. This means that traditional distance metrics, such as Euclidean distance, also become less meaningful, which <a id="_idIndexMarker794" class="calibre6 pcalibre pcalibre1"/>can particularly affect algorithms that rely on distance, such as <strong class="bold">k-nearest neighbors</strong> (<strong class="bold">kNN</strong>) or <span>clustering algorithms.</span></p>
<h3 class="calibre11">Overfitting and increased data requirements</h3>
<p class="calibre3">Referring back to our discussion of how overfitting or underfitting are influenced by the ratio of <a id="_idIndexMarker795" class="calibre6 pcalibre pcalibre1"/>observations to features in our dataset, high-dimensional datasets are more prone to overfitting unless we have enormous <a id="_idIndexMarker796" class="calibre6 pcalibre pcalibre1"/>amounts of observations to help our model generalize. Bear in mind what we said earlier in this chapter, regarding the relationship between the amount of data our algorithms need to process and the cost of training our models. Datasets with lots of observations and lots of features will be more expensive to train <span>and manage.</span></p>
<h3 class="calibre11">Interpretability and explainability</h3>
<p class="calibre3">Interpretability and explainability refer to our ability to understand and explain how our machine learning models work. This is important for numerous reasons, all of which we will discuss briefly here. Firstly, a lack of understanding of how our models work inhibits our ability <a id="_idIndexMarker797" class="calibre6 pcalibre pcalibre1"/>to improve those models. However, more importantly, we need to ensure that our models are as fair and unbiased as possible, and this is where explainability plays a crucial role. If we cannot explain why our models are producing specific results, then we cannot adequately assess their fairness. Generally, the higher the dimensionality of our dataset, the more complex our models tend to be, which can directly affect (that is, reduce) interpretability <span>and explainability.</span></p>
<p class="calibre3">Now that we’ve reviewed some of the common challenges associated with high-dimensional datasets, let’s take a look at some mechanisms to address <span>those challenges.</span></p>
<h2 id="_idParaDest-160" class="calibre9"><a id="_idTextAnchor222" class="calibre6 pcalibre pcalibre1"/>Dimensionality reduction</h2>
<p class="calibre3">As you might imagine, one of the first ways to address the challenge of having too many dimensions <a id="_idIndexMarker798" class="calibre6 pcalibre pcalibre1"/>is to reduce the number of dimensions, and there are <a id="_idIndexMarker799" class="calibre6 pcalibre pcalibre1"/>two main types of techniques we can use for this purpose: feature selection and <span>feature projection.</span></p>
<h3 class="calibre11">Feature selection</h3>
<p class="calibre3">This <a id="_idIndexMarker800" class="calibre6 pcalibre pcalibre1"/>involves selecting a subset of the original <a id="_idIndexMarker801" class="calibre6 pcalibre pcalibre1"/>features. There are several strategies for feature selection, including <a id="_idIndexMarker802" class="calibre6 pcalibre pcalibre1"/><span>the following:</span></p>
<ul class="calibre16">
<li class="calibre8">Filter methods, which rank features based on statistical measures and select a subset of features with the <span>highest ranking</span></li>
<li class="calibre8">Wrapper methods, which evaluate multiple models using different subsets of input features and select the subset that results in the highest <span>model performance</span></li>
<li class="calibre8">Embedded methods, which use machine learning algorithms that have built-in feature selection methods (such as <span>Lasso regularization)</span></li>
</ul>
<p class="calibre3">It’s also important to understand that the feature projection methods we will discuss can be used to help select the most important subset of features from our dataset, so there is some overlap between <span>these concepts.</span></p>
<h3 class="calibre11">Feature projection</h3>
<p class="calibre3">With feature <a id="_idIndexMarker803" class="calibre6 pcalibre pcalibre1"/>projection, we use mathematical <a id="_idIndexMarker804" class="calibre6 pcalibre pcalibre1"/>transformations to project our features down into a <a id="_idIndexMarker805" class="calibre6 pcalibre pcalibre1"/>lower-dimensional space. In this section, we’ll <a id="_idIndexMarker806" class="calibre6 pcalibre pcalibre1"/>introduce three popular feature projection <a id="_idIndexMarker807" class="calibre6 pcalibre pcalibre1"/>techniques: <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>), <strong class="bold">Linear Discriminant Analysis</strong> (<strong class="bold">LDA</strong>), and <strong class="bold">t-distributed Stochastic Neighbor </strong><span><strong class="bold">Embedding</strong></span><span> (</span><span><strong class="bold">t-SNE</strong></span><span>).</span></p>
<h4 class="calibre20">PCA</h4>
<p class="calibre3">PCA is an <strong class="bold">unsupervised</strong> algorithm that aims to reduce the dimensionality of our feature <a id="_idIndexMarker808" class="calibre6 pcalibre pcalibre1"/>space while maintaining as much of the original data’s variance <a id="_idIndexMarker809" class="calibre6 pcalibre pcalibre1"/>as possible. In <a id="_idIndexMarker810" class="calibre6 pcalibre pcalibre1"/>the high-dimensional data space, PCA identifies the axes (principal components) along which the variation in the data is maximized. These principal components are orthogonal, meaning they’re at right angles to each <a id="_idIndexMarker811" class="calibre6 pcalibre pcalibre1"/>other in this multi-dimensional space. The <strong class="bold">first principal component</strong> (<strong class="bold">PC1</strong>) captures the direction of the greatest variance in the data. The <strong class="bold">second principal component</strong> (<strong class="bold">PC2</strong>) captures the maximum amount <a id="_idIndexMarker812" class="calibre6 pcalibre pcalibre1"/>of remaining variance while being orthogonal to PC1, and so on. The process of PCA generally involves the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Standardize the data if the features have different scales. This is important because PCA is sensitive to feature scales, whereby features with larger scales could mistakenly be perceived as <span>more dominant.</span></li>
<li class="calibre8">Calculate a covariance matrix to understand how different features vary together. The covariance matrix is a square matrix that contains the covariances between each pair of features. The covariance between two features measures how those features vary together: a positive covariance indicates that the features increase or decrease together, while a negative covariance indicates that <a id="_idIndexMarker813" class="calibre6 pcalibre pcalibre1"/>one feature increases while the <span>other decreases.</span></li>
<li class="calibre8">Calculate the <strong class="bold">eigenvalues</strong> and <strong class="bold">eigenvectors</strong> of the covariance matrix. The eigenvectors represent the directions or components of the new space, and the eigenvalues <a id="_idIndexMarker814" class="calibre6 pcalibre pcalibre1"/>represent the magnitude or explained variance for each component. The eigenvectors are often called the principal components of the data, and they form a basis for the new <span>feature space.</span></li>
<li class="calibre8">Sort the eigenvalues and their corresponding eigenvectors. After computing the eigenvalues and their associated eigenvectors, the next step is to sort the eigenvalues in descending order. The eigenvector with the highest corresponding eigenvalue is PC1. The eigenvector with the second highest corresponding eigenvalue is PC2, and so on. The reason for this ordering is that the significance of each eigenvector is given by the magnitude of <span>its eigenvalue.</span></li>
<li class="calibre8">Select a subset of the principal components. PCA creates as many principal components as there are variables in the original dataset. However, since the goal of PCA is dimensionality reduction, we usually select a subset of the principal components, referred to as the <strong class="bold">top k</strong> principal components, which capture the most variance in the data. This step is what reduces dimensionality because the smaller eigenvalues and their vectors <span>are dropped.</span></li>
<li class="calibre8">Transform the original data. The final step in PCA is to transform the original data into the reduced subspace defined by the selected principal components, which is done by multiplying the original data matrix by the matrix of the top <span><em class="italic">k</em></span><span> eigenvectors.</span></li>
</ol>
<p class="calibre3">The <a id="_idIndexMarker815" class="calibre6 pcalibre pcalibre1"/>transformed data is <a id="_idIndexMarker816" class="calibre6 pcalibre pcalibre1"/>now ready to be used for further analysis and visualization (as depicted in <span><em class="italic">Figure 7</em></span><em class="italic">.1</em>), or used as input to a machine learning algorithm. Importantly, the reduced dataset retains as much of the variance in the original data as possible (given the reduced number <span>of dimensions):</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer119">
<img alt="Figure 7.1: PCA visualization of European genetic structure (source: https://commons.wikimedia.org/wiki/File:PCA_plot_of_European_individuals.png)" src="image/B18143_07_1.jpg" class="calibre120"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.1: PCA visualization of European genetic structure (source: https://commons.wikimedia.org/wiki/File:PCA_plot_of_European_individuals.png)</p>
<p class="calibre3">PCA is a powerful technique with many uses, but it also has limitations. For example, it assumes that the principal components are a linear combination of the original features. If this is not the case (that is, if the underlying structure in the data is non-linear), then PCA may <a id="_idIndexMarker817" class="calibre6 pcalibre pcalibre1"/>not be the best <a id="_idIndexMarker818" class="calibre6 pcalibre pcalibre1"/>dimensionality reduction technique to use. It’s also worth noting that the principal components are less interpretable than the original features – they don’t have an intuitive meaning in terms of the <span>original features.</span></p>
<h4 class="calibre20">LDA</h4>
<p class="calibre3">LDA is a <strong class="bold">supervised</strong> algorithm that <a id="_idIndexMarker819" class="calibre6 pcalibre pcalibre1"/>aims to find a linear combination <a id="_idIndexMarker820" class="calibre6 pcalibre pcalibre1"/>of features that best separates classes <a id="_idIndexMarker821" class="calibre6 pcalibre pcalibre1"/>of objects. The resulting combination can then be used for dimensionality reduction. It involves the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Compute the class means. For each class in the dataset, calculate the mean vector, which is simply the average of all vectors in <span>that class.</span></li>
<li class="calibre8">Compute the within-class covariance matrix, which measures how the individual classes are dispersed around their <span>respective means.</span></li>
<li class="calibre8">Compute the between-class covariance matrix, which measures how the class means are dispersed around the overall mean of <span>the data.</span></li>
<li class="calibre8">Compute the linear discriminants, which are the directions in the feature space along which the classes are <span>best separated.</span></li>
<li class="calibre8">Sort the linear discriminants. Just like in PCA, the eigenvectors are sorted by their corresponding eigenvalues in descending order. The eigenvalues represent the amount of the data’s variance that is accounted for by each discriminant. The first few linear discriminants, corresponding to the largest eigenvalues, are the ones that account for the <span>most variance.</span></li>
<li class="calibre8">Finally, the data is projected onto the space spanned by the first few <span>linear discriminants.</span></li>
</ol>
<p class="calibre3">This results in a lower-dimensional representation of the data where the classes are maximally <a id="_idIndexMarker822" class="calibre6 pcalibre pcalibre1"/>separated. We can visualize this <span>as follows:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer120">
<img alt="Figure 7.2: LDA plots of wine varieties" src="image/B18143_07_02.jpg" class="calibre121"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.2: LDA plots of wine varieties</p>
<p class="calibre3">It’s important <a id="_idIndexMarker823" class="calibre6 pcalibre pcalibre1"/>to note that the main assumption of LDA is that the classes have identical covariance matrices. If this assumption is not met, LDA might not <span>perform well.</span></p>
<h2 id="_idParaDest-161" class="calibre9"><a id="_idTextAnchor223" class="calibre6 pcalibre pcalibre1"/>t-SNE</h2>
<p class="calibre3">This method has <a id="_idIndexMarker824" class="calibre6 pcalibre pcalibre1"/>perhaps the coolest name of them all. It’s an <strong class="bold">unsupervised</strong>, non-linear dimensionality reduction algorithm that is particularly well-suited <a id="_idIndexMarker825" class="calibre6 pcalibre pcalibre1"/>for embedding high-dimensional data into a space of two or three dimensions while aiming to keep similar instances <a id="_idIndexMarker826" class="calibre6 pcalibre pcalibre1"/>close and dissimilar instances apart. It does this by mapping the high-dimensional data to a lower-dimensional space in a way that retains much of the relative distances between points. It involves the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8"><strong class="bold">Compute similarities in the high-dimensional space</strong>: t-SNE begins by calculating the probability that pairs of data points in the high-dimensional space are similar. Points that are close to each other have a higher probability of being picked, while points that are far away have a <span>lower probability.</span></li>
<li class="calibre8"><strong class="bold">Computing similarities in low-dimensional space</strong>: t-SNE then calculates the probabilities of similarity for pairs of points in the <span>low-dimensional representation.</span></li>
<li class="calibre8"><strong class="bold">Optimization</strong>: Finally, t-SNE uses gradient descent to minimize the difference between the probabilities in the high-dimensional and low-dimensional spaces. The goal is to have similar objects modeled by nearby points and dissimilar objects modeled by distant points in the <span>low-dimensional space.</span></li>
</ol>
<p class="calibre3">The result of t-SNE is a map that reveals the structure of the high-dimensional data in a way that it’s easier for humans <span>to comprehend.</span></p>
<p class="calibre3">It should be noted that while t-SNE is excellent for visualization and can reveal clusters and structure in your data (as depicted in <span><em class="italic">Figure 7</em></span><em class="italic">.3</em>), it doesn’t provide explicit information about the importance or meaning of the features in your data like PCA does. It’s more of an exploratory tool that can help in dimensionality reduction, rather than a formal dimensionality <span>reduction technique:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer121">
<img alt="Figure 7.3: t-SNE visualization of digits dataset" src="image/B18143_07_03.jpg" class="calibre122"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.3: t-SNE visualization of digits dataset</p>
<p class="calibre3">Now that <a id="_idIndexMarker827" class="calibre6 pcalibre pcalibre1"/>we’ve covered some of the most popular feature projection techniques, let’s <a id="_idIndexMarker828" class="calibre6 pcalibre pcalibre1"/>discuss one more set of important concepts regarding how the features in our datasets influence <span>model training.</span></p>
<h1 id="_idParaDest-162" class="calibre5"><a id="_idTextAnchor224" class="calibre6 pcalibre pcalibre1"/>Using PCA and LDA for dimensionality reduction</h1>
<p class="calibre3">We’ll start our hands-on activities in this chapter with dimensionality reduction using PCA <a id="_idIndexMarker829" class="calibre6 pcalibre pcalibre1"/>and LDA. We <a id="_idIndexMarker830" class="calibre6 pcalibre pcalibre1"/>can use <a id="_idIndexMarker831" class="calibre6 pcalibre pcalibre1"/>the wine <a id="_idIndexMarker832" class="calibre6 pcalibre pcalibre1"/>dataset within scikit-learn as an example. I always wish I could impress my friends by being a wine expert, but I can barely tell a $10 bottle from a $500 bottle, so instead, I’ll use data science to develop <span>impressive knowledge.</span></p>
<p class="calibre3">The wine dataset is an example of a multivariate dataset that contains the results of a chemical analysis of wines grown in the same region in Italy but derived from three different types of grapes (referred to as <strong class="source-inline">cultivars</strong>). The analysis focused on quantifying 13 constituents found in each of the three types <span>of wines.</span></p>
<p class="calibre3">Using <a id="_idIndexMarker833" class="calibre6 pcalibre pcalibre1"/>PCA on <a id="_idIndexMarker834" class="calibre6 pcalibre pcalibre1"/>this dataset <a id="_idIndexMarker835" class="calibre6 pcalibre pcalibre1"/>will help <a id="_idIndexMarker836" class="calibre6 pcalibre pcalibre1"/>us to understand the important features. By looking at the weights of the original features in the principal components, we can see which features contribute most to the variability in the <span>wine dataset.</span></p>
<p class="calibre3">Again, we can use the same Vertex AI Workbench Notebook Instance that we created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a> for this purpose. Please open JupyterLab on that notebook instance and perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the directory explorer on the left-hand side of the screen, navigate to the <strong class="source-inline">Chapter-07</strong> directory and open the <span><strong class="source-inline">dimensionality-reduction.ipynb</strong></span><span> notebook.</span></li>
<li class="calibre8">Choose <strong class="bold">Python (Local</strong>) as <span>the kernel.</span></li>
<li class="calibre8">Run each cell in the notebook by selecting the cell and pressing <em class="italic">Shift</em> + <em class="italic">Enter</em> on <span>your keyboard.</span></li>
</ol>
<p class="calibre3">The code in the notebook performs the <span>following activities:</span></p>
<ol class="calibre7">
<li class="calibre8">First, it imports the <span>necessary libraries.</span></li>
<li class="calibre8">Then, it loads <span>the dataset.</span></li>
<li class="calibre8">Next, it standardizes the features of the wine dataset. It applies PCA to reduce the dimensionality to two dimensions (that is, the first two principal components). This is done using the <strong class="source-inline">fit_transform()</strong> method, which fits the PCA model to the data and then transforms <span>the data.</span></li>
<li class="calibre8">Finally, it visualizes the data in the space of those two principal components, coloring the points according to the type <span>of wine:</span><pre class="source-code">
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd
# Load dataset
data = load_wine()
df = pd.DataFrame(data.data, columns=data.feature_names)
# Standardize the features
scaler = StandardScaler()
df = scaler.fit_transform(df)
# Apply PCA
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(df)
principalDf = pd.DataFrame(data = principalComponents, 
    columns = ['principal component 1', 
        'principal component 2'])
# Visualize 2D Projection
plt.figure(figsize=(8,6))
plt.scatter(principalDf['principal component 1'], principalDf['principal component 2'], c=data.target)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()</pre><p class="calibre3">The <a id="_idIndexMarker837" class="calibre6 pcalibre pcalibre1"/>resulting <a id="_idIndexMarker838" class="calibre6 pcalibre pcalibre1"/>visualization should look similar to what’s shown in <span><em class="italic">Figure 7</em></span><span><em class="italic">.4</em></span><span>:</span></p></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer122">
<img alt="Figure 7.4: PCA scatter plot for the wine dataset" src="image/B18143_07_4.jpg" class="calibre123"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.4: PCA scatter plot for the wine dataset</p>
<p class="calibre3">The <a id="_idIndexMarker839" class="calibre6 pcalibre pcalibre1"/>scatter plot <a id="_idIndexMarker840" class="calibre6 pcalibre pcalibre1"/>should show a clear separation between the different types of wine, which suggests that the type of wine is closely related to its chemical constituents. Moreover, the PCA object (named <strong class="source-inline">pca</strong> in our code) stores the <strong class="source-inline">components_</strong> attribute, which contains the mappings of each feature concerning the principal components. By examining these, we can find out which features are the most important in distinguishing between the types <span>of wine.</span></p>
<p class="calibre3">Each data point in the visualization represents a single sample from our dataset, but instead of being plotted in the original, high-dimensional feature space, it’s plotted in the lower-dimensional space defined by the <span>principal components.</span></p>
<p class="calibre3">In the <a id="_idIndexMarker841" class="calibre6 pcalibre pcalibre1"/>context of the <a id="_idIndexMarker842" class="calibre6 pcalibre pcalibre1"/>wine dataset, each data point in the PCA visualization represents a single wine sample. Because <a id="_idIndexMarker843" class="calibre6 pcalibre pcalibre1"/>we have mapped our original 13 features to two PCA dimensions, the position of each point <a id="_idIndexMarker844" class="calibre6 pcalibre pcalibre1"/>on the <em class="italic">X</em> and <em class="italic">Y</em> axes corresponds to the values of the first and second principal components for that <span>wine sample.</span></p>
<p class="calibre3">The color of each point represents the true class of the wine sample (derived from three different cultivars). By coloring the points according to their true class, you can see how well the PCA transformation separates the different classes in the reduced <span>dimensional space.</span></p>
<p class="calibre3">Remember that each principal component is a linear combination of the original features, so the position of each point is still determined by the values of its original features. In this way, PCA enables us to visualize the high-dimensional data, and it highlights the dimensions of greatest variance, which are often the <span>most informative.</span></p>
<p class="calibre3">We can then access the <strong class="source-inline">components_</strong> attribute of the fitted PCA object to view the individual components. This attribute returns a matrix where each row corresponds to a principal component and each column corresponds to an <span>original feature.</span></p>
<p class="calibre3">So, the following code will print a table, where the values in the table represent the weights of each feature in <span>each component:</span></p>
<pre class="source-code">
components_df = pd.DataFrame(pca.components_, 
    columns=data.feature_names, index=['Component 1', 'Component 2'])
print(components_df)</pre> <p class="calibre3">The results should look similar to what’s shown in <span><em class="italic">Table 7.2</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer123">
<img alt="Table 7.2: PCA components and features for the wine dataset" src="image/B18143_07_Table2.jpg" class="calibre124"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.2: PCA components and features for the wine dataset</p>
<p class="calibre3">By looking at the absolute values of these weights, we can determine which features are most important for each principal component. Large absolute values correspond to features <a id="_idIndexMarker845" class="calibre6 pcalibre pcalibre1"/>that play a significant role in the variation captured by that principal component, and the <a id="_idIndexMarker846" class="calibre6 pcalibre pcalibre1"/>sign (positive or negative) of the weight can tell us about the direction of the relationship between the feature and the <span>principal component.</span></p>
<p class="calibre3">Next, let’s <a id="_idIndexMarker847" class="calibre6 pcalibre pcalibre1"/>see how we <a id="_idIndexMarker848" class="calibre6 pcalibre pcalibre1"/>could use LDA to identify the constituents that account for the most variance between the types of wine. Again, we’ll first import the necessary libraries and standardize the data. We’ll then apply LDA to the standardized features, specifying <strong class="source-inline">n_components=2</strong> to get a two-dimensional projection, and then fit the LDA model to the data and transform the data for the first two LDA components. Finally, we’ll visualize the <span>transformed data:</span></p>
<pre class="source-code">
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
# Apply LDA
lda = LDA(n_components=2)
lda_components = lda.fit_transform(df, data.target)
lda_df = pd.DataFrame(data = lda_components, 
    columns = ['LDA 1', 'LDA 2'])
# Visualize 2D Projection
plt.figure(figsize=(8,6))
plt.scatter(lda_df['LDA 1'], lda_df['LDA 2'], c=data.target)
plt.xlabel('LDA 1')
plt.ylabel('LDA 2')
plt.show()</pre> <p class="calibre3">The <a id="_idIndexMarker849" class="calibre6 pcalibre pcalibre1"/>resulting visualization <a id="_idIndexMarker850" class="calibre6 pcalibre pcalibre1"/>should <a id="_idIndexMarker851" class="calibre6 pcalibre pcalibre1"/>look <a id="_idIndexMarker852" class="calibre6 pcalibre pcalibre1"/>similar to what’s shown in <span><em class="italic">Figure 7</em></span><span><em class="italic">.5</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer124">
<img alt="Figure 7.5: LDA scatter plot for the wine dataset" src="image/B18143_07_5.jpg" class="calibre125"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 7.5: LDA scatter plot for the wine dataset</p>
<p class="calibre3">In <a id="_idIndexMarker853" class="calibre6 pcalibre pcalibre1"/>this case, the <a id="_idIndexMarker854" class="calibre6 pcalibre pcalibre1"/>scatter plot shows the data points in the space of the first two LDA components, and the points <a id="_idIndexMarker855" class="calibre6 pcalibre pcalibre1"/>are again <a id="_idIndexMarker856" class="calibre6 pcalibre pcalibre1"/>colored according to the type of wine. We should also see a clear separation between the different types of wine, indicating that they have different distributions of their various <span>chemical constituents.</span></p>
<p class="calibre3">As we did with the <strong class="source-inline">components_</strong> attribute of the fitted PCA object, we can inspect the <strong class="source-inline">coef_</strong> attribute of the fitted LDA object to view the most discriminative features, as shown in the following code and its <span>respective output:</span></p>
<pre class="source-code">
# Create and print a DataFrame with the LDA coefficients and feature names
coef_df = pd.DataFrame(lda.coef_, columns=data.feature_names, 
    index=['Class 1 vs Rest', 'Class 2 vs Rest', 'Class 3 vs Rest'])
print(coef_df)</pre> <p class="calibre3">The <a id="_idIndexMarker857" class="calibre6 pcalibre pcalibre1"/>results <a id="_idIndexMarker858" class="calibre6 pcalibre pcalibre1"/>should <a id="_idIndexMarker859" class="calibre6 pcalibre pcalibre1"/>look similar <a id="_idIndexMarker860" class="calibre6 pcalibre pcalibre1"/>to what’s shown in <span><em class="italic">Table 7.3</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer125">
<img alt="Table 7.3: LDA classes and features for the wine dataset" src="image/B18143_07_Table3.jpg" class="calibre126"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.3: LDA classes and features for the wine dataset</p>
<p class="calibre3">In the resulting table, each row corresponds to a class (compared to the rest), and each column corresponds to an original feature. So, the values in the table represent the coefficients of each feature in the context of the linear discriminants. Similar to our PCA assessment, large absolute values indicate features that contribute significantly to separating <span>the classes.</span></p>
<p class="calibre3">Now that we’ve looked at how to reduce the dimensionality of our datasets, let’s assume that we’ve identified our required features and begin to explore how we could further engineer features to ensure we have the best possible set of features to train <span>our models.</span></p>
<h1 id="_idParaDest-163" class="calibre5"><a id="_idTextAnchor225" class="calibre6 pcalibre pcalibre1"/>Feature engineering</h1>
<p class="calibre3">Feature engineering can constitute a large portion of a data scientist’s activities, and it can be <a id="_idIndexMarker861" class="calibre6 pcalibre pcalibre1"/>just as important to their success, or sometimes even more important, than choosing the right machine learning algorithm. In this section, we will dive deeper into feature engineering, which can be considered both an art and <span>a science.</span></p>
<p class="calibre3">We will use the <em class="italic">Titanic</em> dataset available on OpenML (<a href="https://www.openml.org/search?type=data&amp;sort=runs&amp;id=40945" class="calibre6 pcalibre pcalibre1">https://www.openml.org/search?type=data&amp;sort=runs&amp;id=40945</a>) for our examples in this section. This dataset contains information about passengers aboard the Titanic, including demographic data, ticket class, fare, and whether they survived the sinking of <span>the ship.</span></p>
<p class="calibre3">In the <strong class="source-inline">Chapter-07</strong> directory in JupyterLab on your Vertex AI Workbench Notebook Instance, open the <strong class="source-inline">feature-eng-titanic.ipynb</strong> notebook and choose <strong class="bold">Python (Local)</strong> as the kernel. Again, run each cell in the notebook by selecting the cell and pressing <em class="italic">Shift</em> + <em class="italic">Enter</em> on <span>your keyboard.</span></p>
<p class="calibre3">In this notebook, the code performs the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">First, it imports the <span>necessary libraries.</span></li>
<li class="calibre8">Then, it loads <span>the dataset.</span></li>
<li class="calibre8">After, it performs some initial exploration to see what our dataset <span>looks like.</span></li>
<li class="calibre8">Finally, it engineers <span>new features.</span></li>
</ol>
<p class="calibre3">Let’s take a look at each step in more detail, starting with importing the required libraries, and loading and exploring the dataset. We use the following code to perform <span>those tasks:</span></p>
<pre class="source-code">
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
# Load the data
titanic_raw = pd.read_csv('./data/titanic_train.csv')
titanic_raw.head()</pre> <p class="calibre3">The resulting output from the <strong class="source-inline">head()</strong> method should look similar to what’s shown in <span><em class="italic">Table 7.4</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer126">
<img alt="Table 7.4: Titanic dataset head() output" src="image/B18143_07_Table4.jpg" class="calibre127"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.4: Titanic dataset head() output</p>
<p class="calibre3">The fields <a id="_idIndexMarker862" class="calibre6 pcalibre pcalibre1"/>in the dataset are <span>as follows:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="source-inline">survived</strong>: Indicates if a passenger survived or not. It’s a binary feature where 1 stands for survived and 0 stands for <span>not survived.</span></li>
<li class="calibre8"><strong class="source-inline">pclass</strong> (passenger class): Indicates the class of the passenger’s ticket. It has three categories: 1 for first class, 2 for second class, and 3 for third class. This could also indicate the socio-economic status of <span>the passengers.</span></li>
<li class="calibre8"><strong class="source-inline">name</strong>: The name of <span>the passenger.</span></li>
<li class="calibre8"><strong class="source-inline">sex</strong>: The gender of the passenger; male <span>or female.</span></li>
<li class="calibre8"><strong class="source-inline">age</strong>: The age of the passenger. Some ages are fractional, and for passengers less than 1 year old, the age is estimated as <span>a fraction.</span></li>
<li class="calibre8"><strong class="source-inline">sibsp</strong>: The total number of the passengers’ siblings and spouses aboard <span>the Titanic.</span></li>
<li class="calibre8"><strong class="source-inline">parch</strong>: The total number of the passengers’ parents and children aboard <span>the Titanic.</span></li>
<li class="calibre8"><strong class="source-inline">ticket</strong>: The ticket number of <span>the passenger.</span></li>
<li class="calibre8"><strong class="source-inline">fare</strong>: The passenger fare – that is, how much the <span>ticket cost.</span></li>
<li class="calibre8"><strong class="source-inline">cabin</strong>: The cabin number where the passenger was staying. Some entries are NaN, indicating that the cabin number is missing from <span>the data.</span></li>
<li class="calibre8"><strong class="source-inline">embarked</strong>: The port where the passenger boarded the Titanic. C is for Cherbourg; Q is for Queenstown; S is <span>for Southampton.</span></li>
<li class="calibre8"><strong class="source-inline">boat</strong>: Which lifeboat the passenger was assigned to (if the <span>passenger survived).</span></li>
<li class="calibre8"><strong class="source-inline">body</strong>: Body number (if the passenger did not survive and their body <span>was recovered).</span></li>
<li class="calibre8"><strong class="source-inline">home.dest</strong>: The passenger’s home <span>and destination.</span></li>
</ul>
<p class="calibre3">Assuming that we want to use this dataset to build a model that could predict the likelihood of <a id="_idIndexMarker863" class="calibre6 pcalibre pcalibre1"/>a passenger surviving based on the information recorded about them, let’s see if we can use some domain knowledge to assess which features are likely to be the biggest contributors to the outcome of surviving or not surviving, and whether we could use any data manipulation techniques to engineer more <span>useful features.</span></p>
<p class="calibre3"><strong class="source-inline">Passenger Class</strong> stands out as a potentially important feature to begin with because the first-class and second-class passengers had cabins on higher levels of the ship, which were closer to <span>the lifeboats.</span></p>
<p class="calibre3">It’s unlikely that the passenger’s name would affect the outcome, nor their ticket number or port of embarkation. However, we could engineer a new feature named <strong class="source-inline">Title</strong> that’s extracted from the passengers’ names and could provide valuable information related to social status, occupation, marital status, and age, which might not be immediately apparent from the other features. We could also clean up this new feature by merging similar titles such as <strong class="source-inline">Miss</strong> and <strong class="source-inline">Ms</strong> and identifying elevated titles as <strong class="source-inline">Distinguished</strong>. The code to do that would be <span>as follows:</span></p>
<pre class="source-code">
# We first define a function to extract titles from passenger names
def get_title(name):
    if '.' in name:
        return name.split(',')[1].split('.')[0].strip()
    else:
        return 'Unknown'
# Create a new "Title" feature
titanic['Title'] = titanic['Name'].apply(get_title)
# Simplify the titles, merge less common titles into the same category
titanic['Title'] = titanic['Title'].replace(['Lady', 'Countess', 
    'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 
    'Dona'], 'Distinguished')
titanic['Title'] = titanic['Title'].replace('Mlle', 'Miss')
titanic['Title'] = titanic['Title'].replace('Ms', 'Miss')
titanic['Title'] = titanic['Title'].replace('Mme', 'Mrs')</pre> <p class="calibre3">Next, let’s consider the <strong class="source-inline">Fare</strong> and <strong class="source-inline">Cabin</strong> features. These could be somewhat correlated with class, but we will dive into these features in more detail. For the <strong class="source-inline">Cabin</strong> feature, we could <a id="_idIndexMarker864" class="calibre6 pcalibre pcalibre1"/>extract another feature named <strong class="source-inline">CabinClass</strong>, which more clearly represents the class associated with each entry. We could do this, for example, by extracting the first letter from the cabin number, using it to represent the cabin class (for example, A, B, C, and so on), and storing it in the new <strong class="source-inline">CabinClass</strong> feature. The code to do that would be <span>as follows:</span></p>
<pre class="source-code">
# Create "CabinClass" feature
titanic['CabinClass'] = titanic['Cabin'].apply(lambda x: x[0])</pre> <p class="calibre3">Let’s also ensure that we represent the fare as accurately as possible by considering that people may have purchased fares as families traveling together. To do this, we can create a new feature named <strong class="source-inline">FamilySize</strong> as a combination of the <strong class="source-inline">SibSp</strong> and <strong class="source-inline">Parch</strong> features (adding an additional “1” to account for the current passenger), and then compute <strong class="source-inline">FarePerPerson</strong> by dividing the <strong class="source-inline">Fare</strong> feature by the <strong class="source-inline">FamilySize</strong> feature by using the <span>following code:</span></p>
<pre class="source-code">
titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch'] + 1
# Create "FarePerPerson" feature
titanic['FarePerPerson'] = titanic['Fare'] / titanic['FamilySize']</pre> <p class="calibre3">Whether somebody is traveling alone or with their family could also affect their chances of survival. For example, family members could help each other when trying to get to the lifeboats. So, let’s create a feature from the <strong class="source-inline">FamilySize</strong> feature that identifies whether a passenger was <span>traveling alone:</span></p>
<pre class="source-code">
# Create new feature "IsAlone" from "FamilySize"
titanic['IsAlone'] = 0
titanic.loc[titanic['FamilySize'] == 1, 'IsAlone'] = 1</pre> <p class="calibre3">Next, let’s consider <a id="_idIndexMarker865" class="calibre6 pcalibre pcalibre1"/>how age affects the likelihood of survival. People who are very young, or elderly, may, unfortunately, have less likelihood of surviving unless they have people to help them. However, we may not need yearly and fractional-yearly granularity when considering age in this context, and perhaps grouping people into age groups may be more effective. In that case, we can use the following code to create a new feature named <strong class="source-inline">AgeGroup</strong> that will group passengers by decades such as 0-9, 10-19, 20-29, and <span>so on:</span></p>
<pre class="source-code">
# Create "AgeGroup" feature
bins = [0, 10, 20, 30, 40, 50, 60, 70, np.inf]
labels = ['0-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60-69', 
    '70+']
titanic['AgeGroup'] = pd.cut(titanic['Age'], bins=bins, labels=labels)</pre> <p class="calibre3">We also want to convert the categorical features into numerical values using one-hot encoding since machine learning models typically require numeric values. We could do this as follows (we need to do this for all of our <span>categorical features):</span></p>
<pre class="source-code">
# Convert "Title" into numerical values using one-hot encoding
one_hot = OneHotEncoder()
title_encoded = one_hot.fit_transform(titanic[['Title']]).toarray()
title_encoded_df = pd.DataFrame(title_encoded, 
    columns=one_hot.get_feature_names_out(['Title']))
titanic = pd.concat([titanic, title_encoded_df], axis=1)</pre> <p class="calibre3">Now, let’s drop <a id="_idIndexMarker866" class="calibre6 pcalibre pcalibre1"/>the features that we know will not be valuable for predicting the likelihood of survival (as well as the original features that we encoded, because only their encoded versions <span>are needed):</span></p>
<pre class="source-code">
titanic = titanic.drop(['name', 'ticket', 'Title', 'cabin', 'sex', 
    'embarked', 'AgeGroup', 'CabinClass', 'home.dest'], axis=1)</pre> <p class="calibre3">Then, we can take a quick peek at what our updated dataset <span>looks like:</span></p>
<pre class="source-code">
titanic.head()</pre> <p class="calibre3">The resulting output from the <strong class="source-inline">head()</strong> method should look similar to what’s shown in <span><em class="italic">Table 7.5</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer127">
<img alt="Table 7.5: Output from the head() method for the updated dataset" src="image/B18143_07_Table5.jpg" class="calibre128"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 7.5: Output from the head() method for the updated dataset</p>
<p class="calibre3">At this point, we have an augmented dataset with engineered features that can be used to train a model. It’s important to bear in mind that any feature engineering steps we perform on our source dataset also need to be taken into account when we use our model to make predictions. This common need in machine learning is what gave rise to the requirement for Google Cloud to develop a service named Feature Store. We’ll explore this in the <span>next section.</span></p>
<h1 id="_idParaDest-164" class="calibre5"><a id="_idTextAnchor226" class="calibre6 pcalibre pcalibre1"/>Vertex AI Feature Store</h1>
<p class="calibre3">We’ve done a lot of feature engineering work in this chapter. Bear in mind that we performed <a id="_idIndexMarker867" class="calibre6 pcalibre pcalibre1"/>data transformations and engineered new features because we had reason to believe that the raw data was insufficient to train a machine learning model to suit our business case. This means that the raw data our model will see in the real world would usually not contain the enhancements we performed on the data during training. After all of that work, we would generally want to save the updated features we’ve engineered so that our model can reference them when it needs to make predictions. Vertex AI Feature Store was created for this purpose. We briefly mentioned Vertex AI Feature Store in <a href="B18143_03.xhtml#_idTextAnchor059" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 3</em></span></a>, and in this section, we will dive into more detail regarding what it is and how we can use it to store and serve features for both training <span>and inference.</span></p>
<h2 id="_idParaDest-165" class="calibre9"><a id="_idTextAnchor227" class="calibre6 pcalibre pcalibre1"/>Introduction to Vertex AI Feature Store</h2>
<p class="calibre3">Here’s the official definition from the Google <span>Cloud documentation:</span></p>
<p class="author-quote">Vertex AI Feature Store is a managed, cloud-native feature store service that’s integral to Vertex AI. It streamlines your machine learning feature management and online serving processes by letting you manage your feature data in a BigQuery table or view. You can then serve features online directly from the BigQuery data source. Vertex AI Feature Store provisions resources that let you set up online serving by specifying your feature data sources. It then acts as a metadata layer interfacing with the BigQuery data sources and serves the latest feature values directly from BigQuery for online predictions at low latencies.</p>
<p class="calibre3">In addition to storing and serving our features, Vertex AI Feature Store also integrates with Google Cloud Dataplex to provide feature governance capabilities, including the ability to track feature metadata such as feature labels and versions. In <a href="B18143_13.xhtml#_idTextAnchor328" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 13</em></span></a>, we will dive into the importance of data governance and discuss how Dataplex can be used as an important component in building a robust <span>governance framework.</span></p>
<p class="calibre3">At this point, it’s important to highlight that an entirely new version of Vertex AI Feature Store was launched in 2023. As a result, there are now two different variants of the Feature Store service we can choose in Google Cloud, whereby the prior version is referred to as Vertex AI Feature Store (Legacy), and the new version is simply referred to as Vertex AI Feature Store. We will discuss both variants in this chapter, as well as some of the main distinctions between them. To provide context for the content in subsequent sections, I will briefly describe the topic of online versus offline <span>feature serving.</span></p>
<h2 id="_idParaDest-166" class="calibre9"><a id="_idTextAnchor228" class="calibre6 pcalibre pcalibre1"/>Online versus offline feature serving</h2>
<p class="calibre3">Simply put, <strong class="bold">online serving</strong> refers to <a id="_idIndexMarker868" class="calibre6 pcalibre pcalibre1"/>a scenario in which the interaction is happening in real time – that is, the requesting entity or client sends a request <a id="_idIndexMarker869" class="calibre6 pcalibre pcalibre1"/>and synchronously waits for a response. In this scenario, latency needs to be reduced as much as possible. On the <a id="_idIndexMarker870" class="calibre6 pcalibre pcalibre1"/>other side of the coin is <strong class="bold">offline serving</strong>, which refers to a scenario in which the requesting entity or client does not synchronously <a id="_idIndexMarker871" class="calibre6 pcalibre pcalibre1"/>wait for a response, and the operation <a id="_idIndexMarker872" class="calibre6 pcalibre pcalibre1"/>is allowed to happen over a longer period. In this case, latency is generally not a primary concern. This concept relates closely to the topic of online and offline inference, which we will cover in detail in <a href="B18143_10.xhtml#_idTextAnchor259" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 10</em></span></a><span>.</span></p>
<p class="calibre3">In the case of offline feature serving, Vertex AI Feature Store allows us to store and serve features directly in a Google Cloud BigQuery dataset. This is quite a convenient option as many Google Cloud customers already use BigQuery to store and analyze large amounts of <span>their data.</span></p>
<p class="calibre3">In the case of online feature serving, there are now two ways in which we can serve our features in Vertex AI Feature Store. The first option uses Google Cloud Bigtable to serve our features. Google Cloud Bigtable is a powerful service that is designed for serving large data volumes (terabytes <span>of data).</span></p>
<p class="calibre3">The second <a id="_idIndexMarker873" class="calibre6 pcalibre pcalibre1"/>option for online feature serving, which is referred to as <strong class="bold">optimized online serving</strong>, and was added as part of the new version of Vertex AI Feature Store, allows us to create an online store that is optimized specifically for serving feature data at <span>ultra-low latencies.</span></p>
<p class="calibre3">Choosing one option or the other depends on the needs of your use case, specifically whether you need to handle very large volumes of data or whether you need to serve your features with ultra-low latency. Cost is also a consideration in this decision, bearing in mind that the Bigtable solution generally costs less than the optimized online <span>serving solution.</span></p>
<p class="calibre3">We will focus primarily on the optimized online serving approach in this chapter and the accompanying Jupyter Notebook. The following section dives deeper into the process of setting up online feature serving in Vertex AI <span>Feature Store.</span></p>
<h3 class="calibre11">Online feature serving</h3>
<p class="calibre3">At a high level, the <a id="_idIndexMarker874" class="calibre6 pcalibre pcalibre1"/>following steps are required to set up online serving using Vertex AI Feature Store. We will elaborate on these steps in <span>subsequent sections:</span></p>
<ol class="calibre7">
<li class="calibre8">Prepare data sources <span>in BigQuery.</span><p class="calibre3">Optional: Register data sources in the feature registry by creating feature groups <span>and features.</span></p></li>
<li class="calibre8">Set up the online store and feature view resources to present the feature <span>data sources.</span></li>
<li class="calibre8">Serve the latest online feature values from a <span>feature view.</span></li>
</ol>
<p class="calibre3">Let’s take a look at these concepts in <span>more detail.</span></p>
<h4 class="calibre20">Feature registry</h4>
<p class="calibre3">When using the optimized online serving approach, we can perform an optional step to register our <a id="_idIndexMarker875" class="calibre6 pcalibre pcalibre1"/>features in the Vertex AI feature registry, which has also been added as a component of the new version of Vertex AI <span>Feature Store.</span></p>
<p class="calibre3">This involves the <a id="_idIndexMarker876" class="calibre6 pcalibre pcalibre1"/>process of creating resources referred to as <strong class="bold">feature groups</strong>, which represent logical groupings of feature columns and are associated with specific BigQuery source tables or views. In turn, feature groups contain resources <a id="_idIndexMarker877" class="calibre6 pcalibre pcalibre1"/>referred to as <strong class="bold">features</strong>, which represent specific columns containing feature values within the data source represented by the <span>feature group.</span></p>
<p class="calibre3">We can still serve features online even if we don’t add our BigQuery data sources to the feature registry, but note that the feature registry provides additional functionality, such as storing historical time series data associated with your features. As a result, we will use the feature registry in the practical exercises accompanying this chapter. Now, let’s take a look at the process of setting up online feature serving in <span>more detail.</span></p>
<h4 class="calibre20">Online feature store and feature views</h4>
<p class="calibre3">After we have set up our feature data in BigQuery, and optionally registered feature groups <a id="_idIndexMarker878" class="calibre6 pcalibre pcalibre1"/>and features in the feature registry, there are two main types of resources that we need to set up to enable online <span>feature serving:</span></p>
<ul class="calibre16">
<li class="calibre8">An <strong class="bold">online serving cluster instance</strong>, which is referred to as the <strong class="bold">online store</strong>. Remember <a id="_idIndexMarker879" class="calibre6 pcalibre pcalibre1"/>that we can either use Bigtable for online feature serving or the newly released optimized online feature <span>serving option.</span></li>
<li class="calibre8">One or <a id="_idIndexMarker880" class="calibre6 pcalibre pcalibre1"/>more <strong class="bold">feature view instances</strong>, where each feature view is associated with a feature data source, such as a feature group in our feature registry (if we have chosen the option of registering our features in the feature registry), or a BigQuery source table <span>or view.</span></li>
</ul>
<p class="calibre3">After we create a feature view, we can configure synchronization settings to ensure that our feature data in BigQuery is synchronized with our feature view. We can trigger a synchronization manually, but if our source data is expected to be updated over time, then we can also configure a schedule to periodically refresh the contents of our feature view from the <span>data source.</span></p>
<p class="calibre3">Now that we’ve covered many of the important concepts related to Vertex AI Feature Store, it’s time to dive in and build our very own <span>feature store!</span></p>
<h2 id="_idParaDest-167" class="calibre9"><a id="_idTextAnchor229" class="calibre6 pcalibre pcalibre1"/>Building our feature store</h2>
<p class="calibre3">In this section, we <a id="_idIndexMarker881" class="calibre6 pcalibre pcalibre1"/>will perform practical exercises that implement the concepts we learned about in the <span>previous sections.</span></p>
<h3 class="calibre11">Use our Vertex AI notebook to build the feature store</h3>
<p class="calibre3">In the Vertex AI Notebook Instance we created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a>, we can perform the following steps to build <a id="_idIndexMarker882" class="calibre6 pcalibre pcalibre1"/>the <span>feature store:</span></p>
<ol class="calibre7">
<li class="calibre8">Select <strong class="bold">Open JupyterLab</strong> for the Vertex AI Notebook Instance we created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a><span>.</span></li>
<li class="calibre8">When the JupyterLab opens, you should see a folder in your notebook <span>called </span><span><strong class="source-inline">Google-Machine-Learning-for-Solutions-Architects</strong></span><span>.</span></li>
<li class="calibre8">Double-click on that folder, then double-click on the <strong class="source-inline">Chapter-07</strong> folder within it, and then double-click on the <strong class="source-inline">feature-store.ipynb</strong> file to <span>open it.</span></li>
<li class="calibre8">On the <strong class="bold">Select Kernel</strong> screen that appears, select <span><strong class="bold">Python (Local).</strong></span></li>
<li class="calibre8">Press <em class="italic">Shift</em> + <em class="italic">Enter</em> to run each of the cells in the notebook and read the explanations in the Markdown and comments to understand what <span>we’re doing.</span></li>
</ol>
<p class="calibre3">Now that <a id="_idIndexMarker883" class="calibre6 pcalibre pcalibre1"/>you’ve followed the steps in the notebook to perform feature selection and engineering, have built a feature store, and used some of the features to train a model, let’s take a look at how those features could be used during inference time. In later chapters, you will learn how to deploy models for online inference and send inference requests to those models, but for now, I’ll explain the process at a <span>conceptual level.</span></p>
<h2 id="_idParaDest-168" class="calibre9"><a id="_idTextAnchor230" class="calibre6 pcalibre pcalibre1"/>How features are used during online inference</h2>
<p class="calibre3">In this <a id="_idIndexMarker884" class="calibre6 pcalibre pcalibre1"/>section, I’ll use the taxi fare prediction <a id="_idIndexMarker885" class="calibre6 pcalibre pcalibre1"/>model use case that we built in the accompanying Jupyter Notebook as an example to explain how we can use features from our feature store during inference. We’ll take a look at each step in <span>the process.</span></p>
<h3 class="calibre11">Combining real-time and precomputed features</h3>
<p class="calibre3">Features <a id="_idIndexMarker886" class="calibre6 pcalibre pcalibre1"/>such as the current pickup time (<strong class="source-inline">pickup_datetime</strong>), <strong class="source-inline">pickup_location</strong>, and <strong class="source-inline">passenger_count</strong> can be obtained in real time as each taxi <span>journey begins.</span></p>
<p class="calibre3">Our feature store also contains precomputed features, such as historical trip distances, fares per mile, pickup times, and locations. These features can be selected based on the current journey’s context from the available <span>real-time features.</span></p>
<p class="calibre3">To get <a id="_idIndexMarker887" class="calibre6 pcalibre pcalibre1"/>the precomputed features, the application handling the taxi journey can send a request to our feature store, passing identifiers such as the current time and location, after which the feature store can return the relevant feature values for <span>these identifiers.</span></p>
<h3 class="calibre11">Data assembly for prediction</h3>
<p class="calibre3">At this point, we can assemble the real-time data and fetched features into a feature vector <a id="_idIndexMarker888" class="calibre6 pcalibre pcalibre1"/>matching the format expected by the model, and then pass the assembled feature vector to the model. The model then processes this vector and outputs a fare prediction, which can then be displayed in <span>the app.</span></p>
<p class="calibre3">Well done! You have successfully built a feature store on Google Cloud. Let’s summarize everything we’ve discussed in <span>this chapter.</span></p>
<h1 id="_idParaDest-169" class="calibre5"><a id="_idTextAnchor231" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we discussed how the quality of our features, and the ratio of features to observations in our dataset, influences how our algorithms learn from our data. We discussed challenges that can occur when our dataset contains many features and how to address those challenges by using mechanisms such as dimensionality reduction. We dived into details on dimensionality reduction techniques such as feature selection and feature projection, including algorithms such as PCA, LDA, and t-SNE, and we looked at examples of how to use some of these algorithms using <span>hands-on activities.</span></p>
<p class="calibre3">Next, we dived into feature engineering techniques in which we augmented a source dataset to create new features that contained information that was not readily available in the original dataset. Finally, we dived into Vertex AI Feature Store to learn about how we can use that service to store and serve our engineered <span>feature sets.</span></p>
<p class="calibre3">In the next chapter, we will shift our focus away from the datasets and parameters that our models learn from and discuss different types of parameters that influence how our models learn. There, we’ll explore the concepts of hyperparameters and <span>hyperparameter optimization.</span></p>
</div>
</div></body></html>