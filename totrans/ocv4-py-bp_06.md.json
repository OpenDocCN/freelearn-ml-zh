["```py\nimport cv2\nfrom os import path\n\nfrom saliency import get_saliency_map, get_proto_objects_map\nfrom tracking import MultipleObjectsTracker\n\ndef main(video_file='soccer.avi', roi=((140, 100), (500, 600))):\n    if not path.isfile(video_file):\n        print(f'File \"{video_file}\" does not exist.')\n        raise SystemExit\n\n    # open video file\n    video = cv2.VideoCapture(video_file)\n\n    # initialize tracker\n    mot = MultipleObjectsTracker()\n```", "```py\n    while True: \n        success, img = video.read() \n        if success: \n            if roi: \n                # grab some meaningful ROI \n                img = img[roi[0][0]:roi[1][0], \n                     roi[0][1]:roi[1][1]] \n```", "```py\n        saliency = get_saliency_map(img, use_numpy_fft=False,\n                                    gauss_kernel=(3, 3))\n        objects = get_proto_objects_map(saliency, use_otsu=False)\n        cv2.imshow('tracker', mot.advance_frame(img, objects))\n```", "```py\nif cv2.waitKey(100) & 0xFF == ord('q'): \n    break \n```", "```py\n    def __init__(self, min_object_area: int = 400,\n                 min_speed_per_pix: float = 0.02):\n        self.object_boxes = []\n        self.min_object_area = min_object_area\n        self.min_speed_per_pix = min_speed_per_pix\n        self.num_frame_tracked = 0\n        # Setup the termination criteria, either 100 iteration or move by at\n        # least 1 pt\n        self.term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,\n                          5, 1)\n```", "```py\ndef calc_magnitude_spectrum(img: np.ndarray):\n    if len(img.shape) > 2:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n```", "```py\n    rows, cols = img.shape\n    nrows = cv2.getOptimalDFTSize(rows)\n    ncols = cv2.getOptimalDFTSize(cols)\n    frame = cv2.copyMakeBorder(img, 0, ncols-cols, 0, nrows-rows,\n                               cv2.BORDER_CONSTANT, value=0)\n```", "```py\nimg_dft = np.fft.fft2(img) \n```", "```py\nmagn = np.abs(img_dft) \n```", "```py\nlog_magn = np.log10(magn)\n```", "```py\nspectrum = np.fft.fftshift(log_magn) \n```", "```py\nreturn spectrum/np.max(spectrum)*255 \n```", "```py\ndef plot_power_spectrum(frame: np.ndarray, use_numpy_fft=True) -> None:\n    if len(frame.shape) > 2:\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n```", "```py\nrows, cols = frame.shape\nnrows = cv2.getOptimalDFTSize(rows) \nncols = cv2.getOptimalDFTSize(cols) \nframe = cv2.copyMakeBorder(frame, 0, ncols-cols, 0, \n     nrows-rows, cv2.BORDER_CONSTANT, value = 0)\n```", "```py\n    if use_numpy_fft:\n        img_dft = np.fft.fft2(frame)\n        spectrum = np.log10(np.real(np.abs(img_dft))**2)\n    else:\n        img_dft = cv2.dft(np.float32(frame), flags=cv2.DFT_COMPLEX_OUTPUT)\n        spectrum = np.log10(img_dft[:, :, 0]**2 + img_dft[:, :, 1]**2)\n```", "```py\nL = max(frame.shape) \nfreqs = np.fft.fftfreq(L)[:L/2] \ndists = np.sqrt(np.fft.fftfreq(frame.shape[0])\n     [:,np.newaxis]**2 + np.fft.fftfreq\n         (frame.shape[1])**2) \ndcount = np.histogram(dists.ravel(), bins=freqs)[0] \nhisto, bins = np.histogram(dists.ravel(), bins=freqs,\n     weights=spectrum.ravel())\n```", "```py\ncenters = (bins[:-1] + bins[1:]) / 2 \nplt.plot(centers, histo/dcount) \nplt.xlabel('frequency') \nplt.ylabel('log-spectrum') \nplt.show() \n```", "```py\ndef _calc_channel_sal_magn(channel: np.ndarray,\n                           use_numpy_fft: bool = True) -> np.ndarray:\n    if use_numpy_fft:\n        img_dft = np.fft.fft2(channel)\n        magnitude, angle = cv2.cartToPolar(np.real(img_dft),\n                                           np.imag(img_dft))\n    else:\n        img_dft = cv2.dft(np.float32(channel),\n                          flags=cv2.DFT_COMPLEX_OUTPUT)\n        magnitude, angle = cv2.cartToPolar(img_dft[:, :, 0],\n                                           img_dft[:, :, 1])\n```", "```py\nlog_ampl = np.log10(magnitude.clip(min=1e-9)) \n```", "```py\nlog_ampl_blur = cv2.blur(log_amlp, (3, 3)) \n```", "```py\nresidual = np.exp(log_ampl - log_ampl_blur)\n```", "```py\n    if use_numpy_fft:\n        real_part, imag_part = cv2.polarToCart(residual, angle)\n        img_combined = np.fft.ifft2(real_part + 1j * imag_part)\n        magnitude, _ = cv2.cartToPolar(np.real(img_combined),\n                                       np.imag(img_combined))\n    else:\n        img_dft[:, :, 0], img_dft[:, :, 1] =%MCEPASTEBIN% cv2.polarToCart(residual,\n                                                             angle)\n        img_combined = cv2.idft(img_dft)\n        magnitude, _ = cv2.cartToPolar(img_combined[:, :, 0],\n                                       img_combined[:, :, 1])\n\n    return magnitude\n```", "```py\ndef get_saliency_map(frame: np.ndarray,\n                     small_shape: Tuple[int] = (64, 64),\n                     gauss_kernel: Tuple[int] = (5, 5),\n                     use_numpy_fft: bool = True) -> np.ndarray:\n    frame_small = cv2.resize(frame, small_shape)\n    if len(frame.shape) == 2:\n        # single channelsmall_shape[1::-1]\n        sal = _calc_channel_sal_magn(frame, use_numpy_fft)\n\n```", "```py\n    else:\n        sal = np.zeros_like(frame_small).astype(np.float32)\n        for c in range(frame_small.shape[2]):\n            small = frame_small[:, :, c]\n            sal[:, :, c] = _calc_channel_sal_magn(small, use_numpy_fft)\n```", "```py\nsal = np.mean(sal, 2) \n```", "```py\n    if gauss_kernel is not None:\n        sal = cv2.GaussianBlur(sal, gauss_kernel, sigmaX=8, sigmaY=0)\n```", "```py\n        sal = sal**2 \n        sal = np.float32(sal)/np.max(sal) \n        sal = cv2.resize(sal, self.frame_orig.shape[1::-1]) \n    sal /= np.max(sal)\n    return cv2.resize(sal ** 2, frame.shape[1::-1])\n```", "```py\ndef get_proto_objects_map(saliency: np.ndarray, use_otsu=True) -> np.ndarray: \n```", "```py\n    saliency = np.uint8(saliency * 255)\n    if use_otsu:\n        thresh_type = cv2.THRESH_OTSU\n        # For threshold value, simply pass zero.\n        thresh_value = 0\n    else:\n        thresh_type = cv2.THRESH_BINARY\n        thresh_value = np.mean(saliency) * 3\n\n    _, img_objects = cv2.threshold(saliency,\n                                   thresh_value, 255, thresh_type)\n    return img_objects\n```", "```py\n    def advance_frame(self,\n                      frame: np.ndarray,\n                      proto_objects_map: np.ndarray,\n                      saliency: np.ndarray) -> np.ndarray:\n```", "```py\n        object_contours, _ = cv2.findContours(proto_objects_map, 1, 2)\n        object_boxes = [cv2.boundingRect(contour)\n                        for contour in object_contours\n                        if cv2.contourArea(contour) > self.min_object_area]\n```", "```py\n        if len(self.object_boxes) >= len(object_boxes):\n            # Continue tracking with meanshift if number of salient objects\n            # didn't increase\n            object_boxes = [cv2.meanShift(saliency, box, self.term_crit)[1]\n                            for box in self.object_boxes]\n            self.num_frame_tracked += 1\n```", "```py\n        else:\n            # Otherwise restart tracking\n            self.num_frame_tracked = 0\n            self.object_initial_centers = [\n                (x + w / 2, y + h / 2) for (x, y, w, h) in object_boxes]\n```", "```py\nself.object_boxes = object_boxes\nreturn self.draw_good_boxes(copy.deepcopy(frame))\n```", "```py\n    def draw_good_boxes(self, frame: np.ndarray) -> np.ndarray:\n        # Find total displacement length for each object\n        # and normalize by object size\n        displacements = [((x + w / 2 - cx)**2 + (y + w / 2 - cy)**2)**0.5 / w\n                         for (x, y, w, h), (cx, cy)\n                         in zip(self.object_boxes, self.object_initial_centers)]\n```", "```py\n        for (x, y, w, h), displacement, i in zip(\n                self.object_boxes, displacements, itertools.count()):\n            # Draw only those which have some avarage speed\n            if displacement / (self.num_frame_tracked + 0.01) > self.min_speed_per_pix:\n                cv2.rectangle(frame, (x, y), (x + w, y + h),\n                              (0, 255, 0), 2)\n                cv2.putText(frame, str(i), (x, y),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))\n        return frame\n```", "```py\nimport argparse\nimport time\n\nimport cv2\nimport numpy as np\n\n# Define Constants\nFONT = cv2.FONT_HERSHEY_SIMPLEX\nGREEN = (20, 200, 20)\nRED = (20, 20, 255)\n```", "```py\ntrackers = {\n    'BOOSTING': cv2.TrackerBoosting_create,\n    'MIL': cv2.TrackerMIL_create,\n    'KCF': cv2.TrackerKCF_create,\n    'TLD': cv2.TrackerTLD_create,\n    'MEDIANFLOW': cv2.TrackerMedianFlow_create,\n    'GOTURN': cv2.TrackerGOTURN_create,\n    'MOSSE': cv2.TrackerMOSSE_create,\n    'CSRT': cv2.TrackerCSRT_create\n}\n```", "```py\n# Parse arguments\nparser = argparse.ArgumentParser(description='Tracking API demo.')\nparser.add_argument(\n    '--tracker',\n    default=\"KCF\",\n    help=f\"One of {trackers.keys()}\")\nparser.add_argument(\n    '--video',\n    help=\"Video file to use\",\n    default=\"videos/test.mp4\")\nargs = parser.parse_args()\n```", "```py\ntracker_name = args.tracker.upper()\nassert tracker_name in trackers, f\"Tracker should be one of {trackers.keys()}\"\n```", "```py\nvideo = cv2.VideoCapture(args.video)\nassert video.isOpened(), \"Could not open video\"\nok, frame = video.read()\nassert ok, \"Video file is not readable\"\n```", "```py\nbbox = cv2.selectROI(frame, False)\n```", "```py\ntracker = trackers[tracker_name]()\ntracker.init(frame, bbox)\n```", "```py\nfor ok, frame in iter(video.read, (False, None)): \n    # Time in seconds\n    start_time = time.time()\n    # Update tracker\n    ok, bbox = tracker.update(frame)\n    # Calcurlate FPS\n    fps = 1 / (time.time() - start_time)\n```", "```py\n    if ok:\n        # Draw bounding box\n        x, y, w, h = np.array(bbox, dtype=np.int)\n        cv2.rectangle(frame, (x, y), (x + w, y + w), GREEN, 2, 1)\n    else:\n        # Tracking failure\n        cv2.putText(frame, \"Tracking failed\", (100, 80), FONT, 0.7, RED, 2)\n    cv2.putText(frame, f\"{tracker_name} Tracker\",\n                (100, 20), FONT, 0.7, GREEN, 2)\n    cv2.putText(frame, f\"FPS : {fps:.0f}\", (100, 50), FONT, 0.7, GREEN, 2)\n    cv2.imshow(\"Tracking\", frame)\n\n    # Exit if ESC pressed\n    if cv2.waitKey(1) & 0xff == 27:\n        break\n```", "```py\nmultiTracker = cv2.MultiTracker_create()\n```", "```py\nfor bbox in bboxes:\n    multiTracker.add(cv2.TrackerMIL_create(), frame, bbox)\n```", "```py\nsuccess, boxes = multiTracker.update(frame)\n```"]