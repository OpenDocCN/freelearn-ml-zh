# 第7章：监督机器学习

当你开始在数据科学领域提升你的职业和技能时，你将遇到许多不同类型的模型，这些模型可以分为监督学习或无监督学习两大类。回想一下，在无监督学习的应用中，模型通常被训练来聚类或转换数据，以便将数据分组或重塑以提取洞察，当给定数据集没有标签时。在本章中，我们将现在讨论监督学习的应用，这些应用适用于分类和回归领域，以开发强大的预测模型，对数据集的标签做出明智的猜测。

在本章的整个过程中，我们将讨论以下主题：

+   理解监督学习

+   衡量监督机器学习中的成功

+   理解监督机器学习中的分类

+   理解监督机器学习中的回归

带着我们的目标，现在让我们开始吧。

# 理解监督学习

当你开始探索数据科学，无论是自己还是在一个组织中，你经常会遇到这样的问题：“监督机器学习究竟是什么意思？”让我们继续并给出一个定义。我们可以将监督学习定义为机器学习的一个一般子集，其中数据（以及其关联的标签）被用来训练模型，这些模型可以从数据中学习或泛化以做出预测，最好是具有高度的确定性。回顾一下[*第5章*](B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082)，*机器学习简介*，我们可以回忆起我们完成的关于乳腺癌数据集的例子，其中我们将肿瘤分类为恶性或良性。这个例子，加上我们创建的定义，是学习和理解监督学习含义的极好方式。

在我们心中有了监督机器学习的定义后，让我们继续讨论它的不同子类型，即分类和回归。如果你还记得，在机器学习的范围内，**分类**是指为给定的一组数据预测一个**类别**，例如将肿瘤分类为恶性或良性，将电子邮件分类为垃圾邮件或非垃圾邮件，甚至将蛋白质分类为α或β。在这些案例中，模型将输出一个**离散**值。另一方面，**回归**是使用给定的一组数据预测一个**确切值**，例如小分子的亲脂性、单克隆抗体（**mAb**）的等电点或LCMS峰的LCAP。在这些案例中，模型将输出一个**连续**值。

在监督学习的两个类别中存在许多不同的模型。在本书的范围内，我们将重点关注这两个类别中的每个类别的主要四种模型。在分类方面，我们将讨论**K-最近邻**（**KNN**）、**支持向量机**（**SVMs**）、**决策树**和**随机森林**，以及XGBoost分类。在回归方面，我们将讨论**线性回归**、**逻辑回归**、**随机森林回归**，甚至**梯度提升回归**。我们可以在*图 7.1*中看到这些：

![图 7.1 – 监督机器学习的两个领域](img/B17761_07_001.jpg)

图 7.1 – 监督机器学习的两个领域

在这些模型中，我们的主要目标是针对特定数据集训练该模型的新实例。我们将用数据**拟合**我们的模型，并**调整**或调整参数以获得最佳结果。为了确定最佳结果应该是什么，我们需要知道如何在模型中衡量成功。我们将在下一节中了解这一点。

# 监督机器学习中的成功衡量

当我们开始训练监督分类器和回归器时，我们需要实施几种方法来确定哪些模型表现更好，从而有效地调整模型的参数并最大化其性能。实现这一目标的最有效方法是提前了解成功的外观，然后再深入到模型开发过程。根据情况，有许多不同的方法可以衡量成功。例如，准确率可以是一个好的分类器指标，但不适用于回归器。同样，一个分类器的业务案例可能不一定需要准确率成为主要关注的指标。这完全取决于具体情况。让我们看看**分类**和**回归**领域中常用的某些最常见指标。

![图 7.2 – 回归和分类的常见成功指标](img/B17761_07_002.jpg)

图 7.2 – 回归和分类的常见成功指标

虽然对于特定场景你可以使用许多其他指标，但*图 7.2*中列出的八种可能是你可能会遇到的最常见的一些。选择一个特定的指标可能很困难，因为它应该始终与给定的用例相一致。让我们在讨论分类时进一步探讨这一点。

## 使用分类器衡量成功

以我们迄今为止所使用的肿瘤数据集为例。我们定义我们的成功指标为准确率，因此最大化准确率是我们的模型的主要训练目标。然而，这并不总是如此，你选择的成功指标几乎总是取决于模型和手头的业务问题。让我们进一步仔细看看数据科学领域常用的某些指标，并定义它们：

+   **准确度**：一种与公认值非常一致的度量

+   **精确度**：一种与其他测量结果相似性的度量，即它们彼此相似

考虑准确度和精确度的更简单的方法是通过想象使用靶心表示法显示的结果。精确度和准确度的区别在于结果彼此之间有多接近，以及结果与它们的真实或实际值有多接近。我们可以在*图 7.3*中看到这种视觉表示：

![图 7.3 – 精确度和精确度差异的图形说明](img/B17761_07_003.jpg)

图 7.3 – 精确度和精确度差异的图形说明

除了视觉表示外，我们还可以将精确度视为一个计算，它将结果表示为相对于总体人群的子集。在这种情况下，我们还需要定义一个新的指标，称为**召回率**。我们可以在正负结果的上下文中通过所谓的**混淆矩阵**来数学地思考召回率和精确度。当比较预测结果与实际结果时，通过比较这些值中的几个，我们可以很好地了解模型的表现。我们可以在*图 7.4*中看到这种视觉表示：

![图 7.4 – 混淆矩阵的图形说明](img/B17761_07_004.jpg)

图 7.4 – 混淆矩阵的图形说明

考虑到这个表格，我们可以将**召回率**定义为给定模型识别的欺诈案例的比例，或者从数学角度来看，我们可以将其定义为如下：

![图片](img/B17761_Formula_07_001.jpg)

而我们可以在相同语境下定义**精确度**如下：

![图片](img/B17761_Formula_07_002.jpg)

我们可以用类似的方式在以下图表中可视化准确度、精确度和召回率，其中每个指标代表整体结果的具体计算：

![图 7.5 – 解释精确度、精确度和召回率的图形说明](img/B17761_07_005.jpg)

图 7.5 – 解释精确度、精确度和召回率的图形说明

最后，还有一个常用的指标，通常被认为是精确度和召回率的松散*组合*，称为**F1 分数**。我们可以将*F1 分数*定义为如下：

![图片](img/B17761_Formula_07_003.jpg)

那么，你如何确定使用哪个指标呢？没有一种*最佳*的指标是你应该始终使用的，因为它高度依赖于每种情况。在确定最佳指标时，你应该始终问自己，*模型以及业务的主要目标是什么？*在模型的眼中，精确度可能是最好的指标。另一方面，在业务的眼中，召回率可能是最好的指标。

最终，当被忽视的案例（如上所述为假阴性）更重要时，召回率可能更有用。例如，考虑一个预测患者诊断的模型 – 我们可能更关心假阴性而不是假阳性。另一方面，当假阳性对我们来说成本更高时，精确度可能更重要。这完全取决于给定的业务案例和需求。到目前为止，我们已经研究了与分类相关的成功，现在让我们研究这些想法与回归相关的内容。

## 使用回归器衡量成功

尽管我们还没有深入研究回归领域，但我们已经定义了主要思想，即开发一个输出为连续数值的模型。以包含许多连续浮点数值的分子毒性数据集为例。假设你可以使用这个数据集来预测 **总极性表面积**（**TPSA**）。在这种情况下，准确度、精确度和召回率的指标对我们最好理解模型性能并不是最有用的。相反，我们需要一些更适合连续值的指标。

在许多模型（不一定是机器学习）中定义成功最常用的指标之一是 **皮尔逊相关系数**，也称为 **R2**。这种计算是衡量数据线性度的常用方法，因为它代表了因变量变差的比例。我们可以如下定义 **R2**：

![公式 7.4](img/B17761_Formula_07_004.jpg)

在这个公式中，![公式 7.5](img/B17761_Formula_07_005.png) 是预测值，而![公式 7.6](img/B17761_Formula_07_006.png) 是平均值。

以一个已知实验（实际）值和预测值的示例数据集为例，我们可以绘制这些值相互对应的图表并测量相关性。理论上，一个完美的模型将会有一个理想的关联度（尽可能接近 1.00 的值）。我们可以在 *图 7.6* 中看到高关联度和低关联度的描述：

![图 7.6 – 散点图中高相关与低相关的差异](img/B17761_07_006.jpg)

图 7.6 – 散点图中高相关与低相关的差异

虽然这个指标可以给你一个关于模型性能的良好估计，但还有一些其他指标可以给你更好的模型误差感：**平均绝对误差**（**MAE**）、**平均平方误差**（**MSE**）和**均方根误差**（**RMSE**）。让我们继续定义这些：

+   **MAE**：给定数据集中实际值和预测值之间绝对差异的平均值。当处理含有异常值的数据集时，这个度量通常更稳健：![公式 7.7](img/B17761_Formula_07_007.jpg)

    其中 ![公式 7.8](img/B17761_Formula_07_008.png) 是预测值，而 ![公式 7.9](img/B17761_Formula_07_009.png) 是平均值。

+   **MSE**：给定数据集中实际值和预测值之间平方差的平均值：![公式 7.1](img/B17761_Formula_07_010.jpg)

+   **RMSE**：MSE的平方根，用于衡量值的标准差。这个指标通常用于比较回归模型：![公式 7.2](img/B17761_Formula_07_011.jpg)

当涉及到回归时，你可以根据给定情况使用许多不同的指标。在大多数回归模型中，RMSE通常用于比较多个模型的性能，因为它计算简单且可微分。另一方面，具有异常值的数据集通常使用MSE和MAE进行比较。现在我们已经更好地理解了通过各种指标来衡量成功，那么我们现在可以继续探索分类领域。

# 理解监督机器学习中的分类

在机器学习的背景下，分类模型是监督模型，其目标是根据先前学习到的例子对项目进行分类或归类。你将遇到许多形式的分类模型，因为它们往往是数据科学领域中使用的最常见模型之一。我们可以根据模型的输出开发三种主要的分类器。

![图 7.7 – 三种监督分类类型](img/B17761_07_007.jpg)

图 7.7 – 三种监督分类类型

第一种被称为**二元分类器**。正如其名所示，这是一种以二元方式预测的分类器，即输出是两个选项之一，例如电子邮件是否为垃圾邮件，或分子是否有毒。在这两种情况下都没有第三种选择，这使得模型成为二元分类器。

第二种分类器被称为**多类分类器**。这种分类器是在超过两个不同的输出上进行训练的。例如，许多类型的蛋白质可以根据结构和功能进行分类。这些例子包括结构蛋白、酶、激素、储存蛋白和毒素。开发一个基于蛋白质的一些特性的模型来预测蛋白质类型，将被视为多类分类器，因为在数据集中的每一行数据只能有一个可能的类别或输出。

最后，我们还有**多标签分类器**。这些分类器与它们的多元对应物不同，能够为给定行数据预测多个输出。例如，在筛选临床试验患者时，你可能希望使用许多不同类型的标签来构建患者档案，如性别、年龄、糖尿病状态和吸烟状态。在尝试预测某个患者群体可能的样子时，我们需要能够预测所有这些标签。

现在我们已经将分类分解为几种不同类型，你可能正在考虑你在项目中工作的许多不同领域，在这些领域中分类器可能非常有价值。好消息是，我们即将探索的许多标准或流行分类模型可以很容易地回收并使用新数据拟合。在我们开始探索下一节中的许多不同模型时，请考虑你正在工作的项目和你拥有的数据集，以及哪些模型最适合它们。

## 探索不同的分类模型

在探索多种机器学习模型的过程中，我们将测试这些模型在2016年由Nestorowa等人发布的关于*单细胞RNA序列*的新数据集上的性能。我们将专注于使用这个结构化数据集来开发多种不同的分类器。让我们继续导入数据并为分类模型做准备。首先，我们将使用`pandas`中的`read_csv()`函数导入我们感兴趣的数据集：

[PRE0]

接下来，我们将使用索引来隔离每行的标签（类别），使用每行的前四个字符：

[PRE1]

我们可以使用`head()`函数查看数据。我们会注意到有超过3,992列的数据。正如任何优秀的数据科学家都知道的，使用太多列开发模型会导致许多低效，因此最好使用无监督学习技术（如`sklearn`中的`StandardScaler`类）来减少这些列：

[PRE2]

接下来，我们可以应用PCA将数据集从3,992列减少到15列：

[PRE3]

现在数据状态大大减少，我们可以检查**解释方差比**以查看这与原始数据集相比如何。我们将看到所有列的总和为0.17，相对较低。我们希望达到大约0.8的值，因此让我们继续增加列的总数以提高方差百分比：

[PRE4]

应用PCA模型后，我们成功将列总数减少了大约77%。

完成此操作后，我们现在准备使用`train_test_split()`类来分割我们的数据集：

[PRE5]

现在数据集已分为训练集和测试集，我们已准备好开始分类模型开发过程！

### K-最近邻

经典、易于开发且最常讨论的分类模型之一被称为**KNN**模型，该模型最早由Evelyn Fix和Joseph Hodges于1951年开发。该模型背后的主要思想是基于最近邻的邻近度来确定类别成员资格。以一个二维**二值**数据集为例，其中项目被分类为A或B。当添加新的数据集时，该模型将根据其与其他项目（通常是**欧几里得**距离）的邻近度来确定其成员资格或类别。

![图7.8 – KNN模型的图形表示](img/B17761_07_008.jpg)

图7.8 – KNN模型的图形表示

由于其简单性和巧妙的设计，KNN被认为是最容易开发和实现的机器学习模型之一。虽然模型在应用上很简单，但为了完全有效，确实需要一些调整。让我们继续探索这个模型在单细胞RNA分类数据集上的应用：

1.  我们可以从导入`sklearn`中的`KNeighborsClassifier`模型开始：

    [PRE6]

1.  接下来，我们可以在Python中实例化这个模型的新实例，将邻居的数量设置为`5`，并将模型拟合到我们的训练数据：

    [PRE7]

1.  在模型拟合后，我们现在可以预测模型的输出，并将这些输出设置为我们称之为`y_pred`的变量，并最终使用`classification_report`函数查看结果：

    [PRE8]

    使用分类报告功能，我们可以了解每个类别的精确度、召回率和`F1`分数。我们可以看到，对于`LT.H`类，精确度相对较高，但其他两个类别的精确度略低。另一方面，对于`LT.H`类，召回率非常低，但对于`Prog`类却相当高。总的来说，这个模型的平均精确度为`0.63`：

    ![图7.9 – KNN模型的结果](img/B17761_07_009.jpg)

    图7.9 – KNN模型的结果

    考虑到这些结果，让我们继续调整一个参数，即`n_neighbours`参数，其范围在`1`到`10`之间。

1.  我们可以使用一个简单的`for`循环来完成这个任务：

    [PRE9]

    如果我们查看结果，我们可以看到`neighbors`的数量以及整体模型准确率。立即注意到，仅基于这个指标的最佳选项值是`n=2`，准确率大约为60%。

![图7.10 – 不同邻居数量的KNN模型结果](img/B17761_07_010.jpg)

图7.10 – 不同邻居数量的KNN模型结果

KNN是开发分类器中最简单和最快的模型之一；然而，对于像这样一个复杂的数据集，它并不总是最好的模型。你会注意到结果在各个类别之间差异很大，这表明模型无法仅根据它们与其他成员的邻近度来有效地区分它们。让我们继续探索另一种称为SVM的模型，它试图以稍微不同的方式对项目进行分类。

### 支持向量机

**支持向量机（SVMs**）是一类常用的监督机器学习模型，用于分类和回归，最早由AT&T Bell实验室在1992年开发。SVMs背后的主要思想是使用**超平面**来分离类别。在讨论或数据科学领域中，你可能会听到或遇到三种主要的SVM类型：线性SVMs、多项式SVMs和RBF SVMs。

![图7.11 – 不同SVMs的视觉解释](img/B17761_07_011.jpg)

图7.11 – 不同SVMs的视觉解释

三种模型背后的主要思想在于如何分离类别。例如，在**线性**模型中，超平面是一条将两个类别分开的线性线。或者，超平面可能由**多项式**组成，使模型能够考虑非线性特征。最后，也是最流行的方法，模型可以使用**径向基函数**（**RBF**）来确定数据点的成员资格，这基于两个参数，**gamma**和**C**，它们负责决策区域、其分布以及误分类的惩罚。考虑到这一点，我们现在更详细地看看超平面的概念。

**超平面**是一个函数，试图以线性或非线性方式清楚地定义并允许区分类别。超平面可以用以下方式数学描述：

![](img/B17761_Formula_07_012.jpg)![](img/B17761_Formula_07_013.jpg)![](img/B17761_Formula_07_014.jpg)![](img/B17761_Formula_07_015.jpg)

在其中![](img/B17761_Formula_07_016.png)是向量，![](img/B17761_Formula_07_017.png)是偏置项，![](img/B17761_Formula_07_018.png)是变量。

从RNA数据集短暂休息一下，让我们继续使用Python中的enrollment数据集来演示线性支持向量机（SVM）的使用——这是一个关于患者入组的数据集，其中受访者的数据通过`Likely`（可能）、`Very Likely`（非常可能）或`Unlikely`（不可能）入组的方式进行总结。**线性SVM**的主要目标是**绘制一条线**，根据类别清晰地分离数据。

在我们开始使用SVM之前，让我们先导入我们的数据集：

[PRE10]

为了简化，让我们消除`Likely`类别，保留`Very Likely`和`Unlikely`类别：

[PRE11]

让我们绘制一条分隔散点图中数据的线：

[PRE12]

执行此代码后，得到以下图表：

![图7.12 – 由初始SVM超平面分隔的两个簇](img/B17761_07_012.jpg)

图7.12 – 由初始SVM超平面分隔的两个簇

注意在图中，这条线性线可以用不同的斜率以多种方式绘制，但仍能成功地在数据集内分隔两个类别。然而，当新的数据点开始向两个簇之间的中间地带靠近时，超平面的斜率和位置将开始变得重要。解决这个问题的方法之一是根据最近的数据点定义平面的斜率和位置。如果线包含与最近数据点相关的宽度为*x*的边界，那么一个更改进的`fill_between`函数，如以下代码所示：

[PRE13]

执行此代码后，得到以下图表：

![图7.13 – 由指定边界的初始SVM超平面分隔的两个簇](img/B17761_07_013.jpg)

图7.13 – 由指定边界的初始SVM超平面分隔的两个簇

位于超平面边缘宽度内的来自两个类别的数据点被称为**支持向量**。主要直觉是，支持向量离超平面越远，正确识别新数据点的正确类别的概率就越高。

我们可以使用 `scikit-learn` 库中的 SVC 类来训练一个新的 SVM 分类器。我们首先导入类，分割数据，然后使用数据集训练模型：

[PRE14]

通过这样，我们现在已经将模型拟合到数据集上。作为最后一步，我们可以展示散点图，识别超平面，并指定哪些数据点是这个特定示例的支持向量：

[PRE15]

执行此代码后，得到以下图：

![图 7.14 – 由初始 SVM 超平面分隔的两个聚类，带有选定的支持向量](img/B17761_07_014.jpg)

图 7.14 – 由初始 SVM 超平面分隔的两个聚类，带有选定的支持向量

现在我们已经通过这个基本示例更好地理解了 SVM 如何相对于其超平面操作，让我们继续使用我们一直在工作的单细胞 RNA 分类数据集来测试这个模型。

按照与 KNN 模型大致相同的步骤，我们现在将实现 SVM 模型，首先导入库，用线性核实例化模型，拟合我们的训练数据，然后在测试数据上做出预测：

[PRE16]

打印报告后，得到以下结果：

![图 7.15 – SVM 模型的结果](img/B17761_07_015.jpg)

图 7.15 – SVM 模型的结果

我们可以看到，实际上模型非常稳健，我们的数据集产生了一些高指标，并给出了总平均精度为 88%。SVMs 是非常适合用于复杂数据集的模型，因为它们的主要目标是通过对数据进行超平面分离来实现这一点。现在让我们探索一个采用非常不同方法来得出最终结果的模型，即使用决策树。

### 决策树和随机森林

**决策树**是结构化数据集在分类和回归中最受欢迎和常用的机器学习模型之一。决策树由三个元素组成：**节点**、**边**和**叶节点**。

节点通常由一个问题组成，允许过程分割成任意数量的子节点，以下图中用橙色表示。根节点是整个树通过的第一个节点。边是节点之间的连接，用蓝色表示。当节点没有子节点时，这个最终目的地被称为叶节点，用绿色表示。在某些情况下，决策树将会有包含相同父节点的节点 – 这些被称为兄弟节点。树中的节点越多，树就越**深**。决策树的深度是**复杂度**的一个度量。

如果树不够复杂，将无法得到准确的结果，而过于复杂的树则会被过度训练。在训练过程中，找到一个良好的平衡是主要目标之一。使用这些元素，你可以构建一个决策树，使过程从上到下流动，从而到达特定的目的地或*决策*：

![图 7.16 – 节点、叶子和边在决策树中的表示](img/B17761_07_016.jpg)

图 7.16 – 节点、叶子和边在决策树中的表示

**决策树**以相当天才的方式运行。我们从一个初始数据集开始，其中所有数据点都已标记并准备就绪。第一个目标是使用最有信息量的决策边界来分割数据集 – 在这种情况下，是在 *y=m.* 这成功地隔离了一个类别与另外两个类别；然而，另外两个类别之间仍然没有完全隔离。然后算法再次在 *x = n* 处分割数据集，从而完全分离了三个簇。如果有更多的簇存在，这个过程将迭代和递归地继续，直到所有类别都得到最佳分离。我们可以在 *图 7.17* 中看到这个过程的视觉表示：

![图 7.17 – 决策树所采取过程的图形表示](img/B17761_07_017.jpg)

图 7.17 – 决策树所采取过程的图形表示

决策树通过使用各种称为属性选择度量的分割标准来确定数据在哪里以及如何分割。这些突出的属性选择度量包括：

+   信息增益

+   增益率

+   吉尼指数

让我们更仔细地看看这三项内容。

**信息增益**是一个关于进一步描述树所需信息量的属性。这个属性在利用最少随机性的分区的同时，最小化了数据分类所需的信息。将随机变量*A*的观察结果中确定的随机变量的信息增益视为一个函数，可以这样想：

![](img/B17761_Formula_07_019.jpg)

广义而言，信息增益是熵（信息熵）的变化 ![](img/B17761_Formula_07_020.png)such that:

![](img/B17761_Formula_07_021.jpg)

其中 ![](img/B17761_Formula_07_022.png) 表示在属性 ![](img/B17761_Formula_07_023.png) 给定的情况下 ![](img/B17761_Formula_07_024.png) 的条件熵。总之，信息增益回答了这样的问题，*从这个变量中获得多少信息，给定另一个变量？*

另一方面，**增益率**是相对于固有信息的相对信息增益。换句话说，这个度量是有偏的，偏向于导致许多结果测试，从而强制对这种性质的特征给予优先考虑。增益率可以表示为：

![](img/B17761_Formula_07_025.jpg)

其中 ![](img/B17761_Formula_07_026.png) 表示为：

![](img/B17761_Formula_07_027.jpg)

总结来说，增益率惩罚具有更多不同值的变量，这有助于决定下一级中的下一个分割。

最后，我们到达了**基尼指数**，这是一个属性选择度量，表示随机选择的一个元素被错误标记的频率。基尼指数可以通过减去每个类别的平方概率之和来计算：

![](img/B17761_Formula_07_028.jpg)

这种确定分割的方法自然倾向于较大的分区，而不是信息增益，后者倾向于较小的分区。任何数据科学家的目标都是探索不同的方法，并确定最佳前进路径。

现在我们已经对决策树及其模型的工作原理有了更详细的解释，接下来让我们使用之前的单细胞 RNA 分类数据集来实现这个模型：

[PRE17]

打印报告后，得到以下结果：

![图 7.18 – 决策树分类器的结果](img/B17761_07_018.jpg)

图 7.18 – 决策树分类器的结果

我们可以看到，模型在没有任何调整的情况下，使用`max_depth`值为`4`的情况下，能够提供 77% 的总精确度分数。使用与*KNN*模型相同的方法，我们可以遍历一系列`max_depth`值，以确定最佳值。这样做将导致理想的`max_depth`值为 3，总精确度达到 82%。

当我们开始训练许多模型时，我们将面临的最常见问题之一是以某种方式**过拟合**我们的数据。以一个决策树模型为例，该模型针对特定数据集进行了非常精细的调整，因为决策树是使用整个数据集的特征和感兴趣变量构建的。在这种情况下，模型可能容易过拟合。另一方面，另一个称为**随机森林**的模型使用多个不同的决策树构建，以帮助减轻任何潜在的过拟合。

随机森林是基于决策树的鲁棒集成模型。决策树通常设计为使用整个数据集来构建模型，而随机森林随机选择特征和行，然后构建多个决策树，并在它们的权重中平均。由于随机森林能够限制过拟合，同时避免由于**偏差**导致的错误显著增加，因此它们是强大的模型。我们可以在*图 7.19*中看到这种效果的视觉表示：

![图 7.19 – 随机森林模型的图形解释](img/B17761_07_019.jpg)

图 7.19 – 随机森林模型的图形解释

随机森林可以通过两种主要方式帮助减少**方差**。第一种方法是通过训练不同的数据样本。考虑先前的患者入组数据示例。如果模型是在不包含那些*介于*聚类之间的样本上训练的，那么在测试集上确定分数将导致显著降低的准确率。

第二种方法涉及使用随机特征子集进行训练，从而允许在模型中确定特征重要性的概念。让我们看一下使用单细胞RNA分类数据集的此模型：

[PRE18]

打印报告后，得到以下结果：

![图7.20 – 随机森林模型的结果](img/B17761_07_020.jpg)

图7.20 – 随机森林模型的结果

我们可以立即观察到模型的精确度大约为74%，略低于上面的决策树，这表明树可能略微过度拟合了数据。

**随机森林**模型在生物技术和生命科学行业中非常常用，因为它们有出色的避免过度拟合的方法，以及它们能够使用较小的数据集开发预测模型的能力。生物技术空间内机器学习的许多应用通常都存在一个称为**低N**问题的概念，即存在用例，但收集或组织的数据很少或没有，以开发模型。由于随机森林具有集成特性，因此它们通常用于这个空间的应用。现在让我们看看一个非常不同的模型，它不是基于决策而是基于统计概率来分割数据的。

### 极端梯度提升（XGBoost）

在过去几年中，许多稳健的机器学习模型开始进入数据科学领域，从而有效地改变了机器学习格局——其中之一就是**极端梯度提升**（**XGBoost**）模型。此模型背后的主要思想是它是**梯度提升模型**（**GBMs**）的实现，特别是*决策树*，其中速度和性能得到了高度优化。由于该模型的高效性和高效性，它开始主导数据科学的许多领域，并最终成为[Kaggle.com](http://Kaggle.com)上许多数据科学竞赛的首选算法。

有许多原因使得GBMs在结构化/表格数据集上非常有效。让我们探索这三个原因：

+   **并行化**：*XGBoost*模型实现了一种称为并行化的方法。这里的主要思想是它可以并行化构建每棵树的过程。本质上，单棵树的每个分支都是单独训练的。

+   `max_depth`参数，然后开始向后剪枝过程，最终在不再有正增益的分割后移除。

+   **正则化**：在基于树的算法的总体背景下，正则化是一种算法方法，用于定义一个最小增益，以促使树中的另一个分割。本质上，正则化会缩小分数，从而促使最终预测更加保守，这反过来有助于防止模型过拟合。

既然我们已经对XGBoost及其稳健性能背后的原因有了更好的理解，我们现在就可以在我们的RNA数据集上实现这个模型。我们将首先使用`pip`安装模型的库：

[PRE19]

在模型安装完成后，我们现在可以继续导入模型，并创建一个新的模型实例，在这个实例中我们指定`n_estimators`参数的值为`10000`：

[PRE20]

与之前的模型类似，我们现在可以使用训练数据集来拟合我们的模型，并打印出预测结果：

[PRE21]

打印报告后，得到以下结果：

![图7.21 – XGBoost模型的预测结果](img/B17761_07_021.jpg)

图7.21 – XGBoost模型的预测结果

有了这些，我们可以看到我们成功实现了`0.86`的精确度，这比我们测试的其他一些模型要高得多。这个模型高度优化的特性使其相对于大多数其他模型来说非常快且稳健。

在本节中，我们成功覆盖了相当广泛的**分类**模型。我们从简单的**KNN**模型开始，该模型试图根据新值与其最近邻的关系来预测其类别。接下来，我们介绍了**SVM**模型，这些模型试图根据支持向量绘制的指定边界来分配标签。然后，我们介绍了**决策树**和**随机森林**，它们基于节点、叶子和分割操作，最后我们看到了**XGBoost**的一个工作示例，这是一个高度优化的模型，实现了我们在其他模型中看到的大多数功能。

当你开始深入研究针对新数据集的许多不同模型时，你可能会考虑自动化模型选择过程。如果你这样考虑，我们上面采取的每个步骤都可以以某种方式自动化，以确定在特定的一组度量要求下哪个模型表现最佳。幸运的是，我们已经有一个库可以在这个领域帮助我们。在接下来的教程中，我们将研究这些模型与在**Google Cloud Platform**（**GCP**）上的一些自动化机器学习功能一起的使用。

## 教程：使用GCP对蛋白质进行分类

在本教程中，我们将研究多种分类模型，并随后实现一些自动化机器学习功能。我们的主要目标将是使用来自**结构生物信息学研究合作实验室**（**RCSB**）的**蛋白质数据银行**（**PDB**）的数据集自动开发一个用于蛋白质分类的模型。如果你还记得，我们在前一章中使用RCSB PDB的数据来绘制一个3D蛋白质结构。在本章中我们将使用的数据集由两部分组成——一个具有行和列的**结构化数据集**，其中一列是每个蛋白质指定的分类，以及每个蛋白质的一系列RNA序列。我们将把这一组基于序列的数据保存到后续章节中进行分析，现在我们专注于结构化数据集。

在本章中，我们将使用包含许多不同类型蛋白质的结构化数据集来尝试开发一个分类器。鉴于这个数据集的规模很大，我们将利用这个机会将我们的开发环境从本地安装的**Jupyter Notebook**迁移到GCP的在线笔记本。在我们这样做之前，我们需要创建一个新的GCP账户。让我们开始吧。

### 在GCP入门

在**GCP**（Google Cloud Platform）入门非常简单，只需几个简单的步骤即可完成：

1.  我们可以从导航到[https://cloud.google.com/](https://cloud.google.com/)并注册一个新账户开始。你需要提供一些详细信息，例如你的名字、电子邮件和一些其他项目。

1.  注册后，你可以通过点击页面右上角的**控制台**按钮来导航到控制台：![图7.22 – 控制台按钮的截图](img/B17761_07_022.jpg)

    图7.22 – 控制台按钮的截图

1.  在控制台页面内，你可以看到与你的当前项目相关的所有项目，例如一般信息、使用的资源、API使用情况，甚至账单：![图7.23 – 控制台页面的示例](img/B17761_07_023.jpg)

    图7.23 – 控制台页面的示例

1.  你可能还没有设置任何项目。为了创建一个新的项目，导航到页面左上角的下拉菜单，并选择**新建项目**选项。给你的项目起一个名字，然后点击**创建**。你可以使用相同的下拉菜单在不同的项目之间导航：

![图7.24 – GCP中项目名称和位置窗口的截图](img/B17761_07_024.jpg)

图7.24 – GCP中项目名称和位置窗口的截图

完成最后一步后，你现在已经准备好充分利用GCP平台了。在本教程中，我们将介绍一些GCP功能，以帮助我们开始数据科学领域的工作；然而，我强烈建议新用户探索并学习这里提供的许多工具和资源。

### 将数据上传到GCP BigQuery

在GCP中上传数据有多种不同的方式；然而，我们将专注于GCP特有的一个特定功能，称为**BigQuery**。BigQuery背后的主要思想是它是一个具有内置机器学习能力的无服务器数据仓库，支持使用**SQL**语言进行查询。如果你还记得，我们之前开发并部署了一个**AWS RDS**，使用**EC2**实例作为服务器来管理我们的数据，而另一方面，BigQuery使用的是无服务器架构。我们可以通过几个简单的步骤来设置BigQuery并开始上传我们的数据：

1.  使用页面左侧的导航菜单，滚动到**产品**部分，将鼠标悬停在**BigQuery**选项上，并选择**SQL工作区**。由于这是你第一次使用这个工具，你可能需要激活API。这将对所有你从未使用过的工具都适用：![图7.25 – GCP中BigQuery菜单的截图](img/B17761_07_025.jpg)

    图7.25 – GCP中BigQuery菜单的截图

1.  在这个列表中，你会找到你在上一节中创建的项目。点击右侧的选项按钮，选择`protein_structure_sequence`，保留所有其他选项为默认值。然后你可以点击**创建数据集**。

1.  在左侧菜单中，你将看到在项目名称下列出的新创建的数据集。如果你点击**选项**然后点击**打开**，你将被引导到数据集的主页。在这个页面上，你可以找到与该特定数据集相关的信息。现在让我们点击顶部的**创建表格**选项，在这里创建一个新的表格。将源更改为反映上传选项，并导航到与RCSB PDB中的蛋白质分类相关的CSV文件。给表格起一个新的名字，在保留所有其他选项为默认值的情况下，点击**创建表格**：![图7.26 – GCP中创建表格界面的截图](img/B17761_07_026.jpg)

    图7.26 – GCP中创建表格界面的截图

1.  如果你导航回资源管理器，你将看到新创建的表格列在你的项目下的数据集下：

![图7.27 – 数据集中创建的表格示例](img/B17761_07_027.jpg)

图7.27 – 数据集中创建的表格示例

如果你正确地完成了所有这些步骤，你现在应该有可用于在BigQuery中使用的数据。在下一节中，我们将准备一个新的笔记本，并开始解析这个数据集中的部分数据。

### 在GCP中创建笔记本

在本节中，我们将创建一个与我们在进行数据科学工作时使用的Jupyter笔记本等效的笔记本。我们可以通过几个简单的步骤来设置一个新的笔记本：

1.  在屏幕左侧的导航面板中，向下滚动到 **人工智能** 部分，悬停在 **AI 平台** 上，并选择 **笔记本** 选项。记住，如果你还没有这样做，你可能需要再次激活此 API：![图 7.28 – AI 平台菜单的视觉图](img/B17761_07_028.jpg)

    图 7.28 – AI 平台菜单的视觉图

1.  接下来，导航到屏幕顶部并选择 **新建实例** 选项。根据你的需求，有许多不同的选项可供选择。在本教程的目的上，我们可以选择第一个 **Python 3** 选项：![图 7.29 – 实例选项的截图](img/B17761_07_029.jpg)

    图 7.29 – 实例选项的截图

    如果你熟悉笔记本实例并且能够自定义它们，我建议创建一个定制的实例以满足你的具体需求。

1.  一旦创建了笔记本并实例在线，你将在主 **笔记本实例** 部分中看到它。点击 **打开 JupyterLab** 按钮。将打开一个新窗口，其中包含 Jupyter Lab：![图 7.30 – 实例菜单的截图](img/B17761_07_030.jpg)

    图 7.30 – 实例菜单的截图

1.  在 `home` 目录下，创建一个名为 `biotech-machine-learning` 的新目录，以便我们保存笔记本。打开目录，通过点击右侧的 **Python 3** 笔记本选项来创建一个新的笔记本：

![图 7.31 – GCP 上 Jupyter Lab 的截图](img/B17761_07_031.jpg)

图 7.31 – GCP 上 Jupyter Lab 的截图

现在实例已配置并创建了笔记本，你现在可以准备在 GCP 上运行所有你的数据科学模型了。现在让我们更仔细地查看数据，并开始训练几个机器学习模型。

### 在 GCP 笔记本中使用 auto-sklearn

如果你打开你刚刚创建的笔记本，你会看到我们一直使用的非常熟悉的 Jupyter Lab 环境。这里的主要好处是，我们现在能够在这个相同的环境中管理我们的数据集，并且可以分配更多的资源来处理我们的数据，相对于我们在本地机器上拥有的少量 **CPU** 和 **GPU**。

回想一下，我们达到这个状态的主要目标是能够开发一个分类模型，根据一些输入特征正确地分类蛋白质。我们正在处理的数据集被称为 `真实世界` 数据集，因为它组织得不好，有缺失值，可能包含过多的数据，并且在开发任何模型之前需要一些预处理。

让我们先导入一些必要的库：

[PRE22]

接下来，现在让我们从 BigQuery 导入我们的数据集。我们可以通过使用 Google Cloud 库中的 BigQuery 类来实例化一个客户端，直接在这里的笔记本中完成：

[PRE23]

接下来，我们可以使用SQL语言的`SELECT`命令查询我们的数据。我们可以简单地从查询我们数据集中的所有数据开始。在以下代码片段中，我们将使用SQL查询数据，并将结果转换为数据框：

[PRE24]

一旦转换为一个更易于管理的数据框，我们就可以看到我们正在处理的数据集相当庞大，有近140,000行和14列的数据。立刻，我们注意到其中一列被称作`classification`。让我们使用`n_unique()`函数查看这个数据集中独特的类别数量：

[PRE25]

我们注意到有5,050个不同的类别！对于一个这样的数据集来说，这相当多，表明我们可能需要在任何分析之前大幅度缩减。在继续之前，让我们先删除任何可能的重复项：

[PRE26]

让我们现在更仔细地查看数据集中按计数排序的前10个类别：

[PRE27]

以下图形是由代码生成的，展示了这个数据集中的前10个类别：

![图7.32 – 数据集中最频繁的10个标签](img/B17761_07_032.jpg)

图7.32 – 数据集中最频繁的10个标签

立刻，我们注意到有两到三个类别或蛋白质占用了大部分数据：水解酶、转移酶和氧化还原酶。这将会带来两个问题：

+   数据应该始终在意义上保持**平衡**，即每个类别应该有大致相等的行数。

+   作为一般规则，类别与观察值的比率应该大约是50:1，这意味着在5,050个类别的情况下，我们需要大约252,500个观察值，而我们目前并没有这么多。

在这两个约束条件下，我们可以通过专注于使用前三个类别来开发模型来同时解决这两个问题。目前，我们注意到无论手头上的类别如何，我们都有相当多的特征可用。我们可以继续使用`msno`库来更仔细地查看我们感兴趣的特征的完整性：

[PRE28]

以下截图，表示数据集的完整性，随后生成。请注意，`crystallizationTempK`特征的行数大量缺失：

![图7.33 – 一个图形表示，展示了数据集的完整性](img/B17761_07_033.jpg)

图7.33 – 一个图形表示，展示了数据集的完整性

到目前为止，在这个数据集中，我们已经注意到我们需要将类别数量缩减到前两个类别，以避免数据不平衡，同时我们还需要解决缺失的许多行数据。让我们继续准备我们的数据集，以便基于我们的观察开发几个分类模型。首先，我们可以使用简单的`groupby`函数来缩减数据集：

[PRE29]

如果我们使用`value_counts()`函数对数据框进行快速检查，我们会发现我们能够将其缩减到前两个标签。

或者，我们可以在 `SELECT` 中运行相同的命令，对分类进行 `COUNT`，并按 `GROUP BY` 分类。接下来，我们将该查询与原始表设置进行 `INNER JOIN`，分类对分类，但使用我们的 `WHERE` 子句进行过滤：

[PRE30]

接下来，我们可以使用 `dropna()` 函数删除含有缺失值的行数据：

[PRE31]

立即观察到数据集的大小已减少到 24,179 个观测值。这将是我们开发模型时的一个足够大的数据集。为了避免再次处理，我们可以将数据框的内容写入同一 BigQuery 数据集中新的表中：

[PRE32]

数据现在已准备就绪，让我们继续开发一个模型。我们可以继续分割输入和输出数据，使用 `StandardScaler` 类缩放数据，并将数据分割成测试集和训练集：

[PRE33]

在自动化部分，我们将使用一个名为 `autosklearn` 的库，该库可以通过命令行使用 `pip` 安装：

[PRE34]

安装了库之后，我们可以继续导入库并实例化该模型的新实例。然后我们将设置一些与我们要投入此过程的时间相关的参数，并为模型提供一个临时目录来操作：

[PRE35]

最后，我们可以继续将模型拟合到我们的数据上。这个过程将需要几分钟才能运行：

[PRE36]

当模型完成时，我们可以通过打印排行榜来查看结果：

[PRE37]

打印排行榜后，我们检索到以下结果：

![图 7.34 – auto-sklearn 模型的结果](img/B17761_07_034.jpg)

图 7.34 – auto-sklearn 模型的结果

我们还可以使用 `get_models_with_weights()` 函数查看表现最佳的 `random_forest` 模型：

[PRE38]

我们还可以通过使用模型和 `classification_report()` 函数进行一些预测来获取更多指标：

[PRE39]

打印报告后，得到以下结果：

![图 7.35 – 表现最佳的 auto-sklearn 模型的结果](img/B17761_07_035.jpg)

图 7.35 – 表现最佳的 auto-sklearn 模型的结果

通过这样，我们成功地为我们的数据集开发了一个成功且自动化的机器学习模型。然而，该模型尚未针对此任务进行微调或优化。我推荐你完成的一个挑战是调整模型中的各种参数，以尝试提高我们的指标。此外，另一个挑战将是尝试探索我们学到的其他一些模型，看看它们是否能够击败 `autosklearn`。提示：**XGBoost** 一直是一个针对结构化数据集的优秀模型。

### 使用 GCP 中的 AutoML 应用

在上一节中，我们使用了一个名为`auto-sklearn`的开源库来自动化使用`sklearn`库选择模型的过程。然而，正如我们通过`XGBoost`库所看到的，还有许多其他模型存在于`sklearn` API之外。GCP提供了一个强大的工具，类似于`auto-sklearn`，它会遍历大量模型和方法，以找到给定数据集的最优模型。让我们继续尝试一下：

1.  在GCP的导航菜单中，滚动到**人工智能**部分，将鼠标悬停在**表格**上，然后选择**数据集**：![图7.36 – 从人工智能菜单中选择数据集](img/B17761_07_036.jpg)

    图7.36 – 从人工智能菜单中选择数据集

1.  在页面顶部，选择**新建数据集**选项。在本书编写时，该模型的beta版本可用。一些步骤在未来实现中可能会发生变化：![图7.37 – 创建新数据集按钮的截图](img/B17761_07_037.jpg)

    图7.37 – 创建新数据集按钮的截图

1.  给数据集命名并选择区域，然后点击**创建数据集**。

1.  我们可以选择将我们感兴趣的数据库集以原始CSV文件或使用BigQuery的方式导入。请指定`projectID`、`datasetID`和表名，然后点击**导入**来导入我们清理过的蛋白质数据集版本。

1.  在**训练**部分，你将能够看到这个数据集内的表格。指定**分类**列为目标列，然后点击**训练模型**：![图7.38 – 训练菜单的示例](img/B17761_07_038.jpg)

    图7.38 – 训练菜单的示例

1.  模型选择过程需要一些时间来完成。完成后，你将在**评估**选项卡下看到模型的结果。在这里，你将了解我们一直在使用的分类指标，以及一些其他指标。

![图7.39 – 训练模型结果的截图，显示了指标](img/B17761_07_039.jpg)

图7.39 – 训练模型结果的截图，显示了指标

**GCP** **AutoML**是一个强大的工具，当处理困难的数据集时，你可以利用它。你会发现模型的实现相当稳健，相对于我们作为数据科学家可以探索的许多选项，通常比较全面。**AutoML**的一个缺点是最终模型不会与用户共享；然而，用户确实有测试新数据和稍后使用模型的能力。在下一节中，我们将探讨一个类似于**AutoML**的选项，在**AWS**中称为**AutoPilot**。现在我们已经探索了许多与分类相关的不同模型和方法，让我们去探索回归方面的相应对应物。

# 理解监督机器学习中的回归

**回归**是一般用于确定依赖变量和独立变量之间关系或**相关性**的模型。在机器学习的背景下，我们将回归定义为监督机器学习模型，允许识别两个或多个变量之间的相关性，以便**泛化**或从历史数据中学习，对新观测进行预测。

在生物技术领域的限制范围内，我们使用回归模型来预测许多不同领域的值。

+   提前预测化合物的LCAP

+   在上游预测滴度结果

+   预测单克隆抗体的等电点

+   预测化合物的分解百分比

通常在两列之间建立相关性。当观察到依赖关系时，数据集中的两列被认为具有强烈的**相关性**。可以使用线性回归模型更好地理解具体的关系，如下所示：

![](img/B17761_Formula_07_029.jpg)

在其中 ![](img/B17761_Formula_07_030.png) 是第一个特征，![](img/B17761_Formula_07_031.png) 是第二个特征，![](img/B17761_Formula_07_032.png) 是一个小的误差项，其中 ![](img/B17761_Formula_07_033.png) 和 ![](img/B17761_Formula_07_034.png) 是常数。使用这个简单的方程，我们可以更有效地理解我们的数据，并计算任何相关性。例如，回想一下我们之前在毒性数据集中观察到的相关性，特别是`MolWt`和`HeavyAtoms`特征之间的相关性。

任何给定的回归模型背后的主要思想，与它的分类对应物不同，是输出一个连续值而不是一个类别或类别。例如，我们可以在毒性数据集中使用多个列来尝试预测同一数据集中的其他列。

在数据科学领域，常用的回归模型有很多种。这些模型包括专注于优化一组变量之间线性关系的线性回归，更像是二元分类器而不是回归器的逻辑回归，以及结合多个基础估计器的预测能力的集成模型，等等。我们可以在*图7.40*中看到一些例子：

![图7.40 – 不同类型的回归模型](img/B17761_07_040.jpg)

图7.40 – 不同类型的回归模型

在本节的其余部分，我们将探讨这些模型中的一些，当我们调查使用毒性数据集应用一些回归模型时。让我们继续准备我们的数据。

我们可以从导入我们感兴趣的库开始：

[PRE40]

接下来，我们可以继续导入我们的数据集并删除缺失的行。为了练习，我建议你将此数据集上传到`SELECT`语句：

[PRE41]

接下来，我们可以将我们的数据分为输入值和输出值，并使用`MinMaxScaler()`类从`sklearn`中进行数据缩放：

[PRE42]

最后，我们可以将数据集分为训练数据和测试数据：

[PRE43]

在我们的数据集准备就绪后，我们现在可以继续前进并开始探索一些回归模型。

## 探索不同的回归模型

对于给定的数据集，可以使用许多类型的回归方法。我们可以将回归视为分为四个主要类别：线性回归、逻辑回归、集成回归，最后是提升回归。在下一节中，我们将探索每个这些类别中的示例，从线性回归开始。

### 单变量和多元线性回归

在你职业生涯中可能遇到的大多数数据集中，你通常会发现在某些特征之间存在某种类型的相关性。在本章早期，我们讨论了两个特征之间相关性的概念，即一个特征对另一个特征的依赖性，这可以通过称为**R2**的皮尔逊相关系数来计算。在过去的几章中，我们以不同的方式探讨了相关性的概念，包括热图和pairplot。

使用我们刚刚准备好的数据集，我们可以使用`seaborn`查看一些相关性：

[PRE44]

这产生了以下图示：

![图7.41 – 使用毒性数据集的pairplot函数的结果](img/B17761_07_041.jpg)

图7.41 – 使用毒性数据集的pairplot函数的结果

我们可以看到我们的数据集中已经存在一些相关性。使用**简单线性回归**，我们可以利用这种相关性，即我们可以使用一个变量来预测另一个变量最可能是什么。例如，如果X是自变量，Y是因变量，我们可以定义两者之间的线性关系为：

![图片](img/B17761_Formula_07_035.jpg)

其中*m*是斜率，*c*是*y*截距。根据本书前面的一些内容以及你在高中数学课上学到的知识，这个方程应该对你来说很熟悉。使用这种关系，我们的目标将是相对于我们的数据优化这条线，以确定*m*和*c*的值，使用的方法称为最小二乘法。

在我们讨论**最小二乘法**之前，让我们首先讨论**损失函数**的概念。在机器学习的背景下，损失函数是计算值和预期值之间差异的度量。例如，让我们考察**二次损失函数**，它通常用于回归模型中的损失计算，我们可以将其定义为：

![图片](img/B17761_Formula_07_036.jpg)

到现在为止，我希望你能从我们在*衡量成功*部分中的讨论中认出这个函数。你能告诉我我们上次在哪里使用了它吗？

既然我们已经讨论了损失函数的概念，让我们更深入地了解一下**最小二乘法**。这个数学方法背后的主要思想是通过最小化损失来确定一组数据的最佳拟合线，正如我们刚才看到的关联所展示的那样。为了完全解释这个方程背后的概念，我们需要讨论偏导数等。为了简化起见，我们将简单地将最小二乘法定义为一种**最小化**损失的过程，以便确定一组数据的最佳拟合线。

我们可以将线性回归分为两大类：**简单线性回归**和**多元线性回归**。这里的主要思想是我们将用多少个特征来训练模型。如果我们仅仅基于一个特征来训练模型，我们将开发一个简单线性回归。另一方面，如果我们使用多个特征来训练模型，我们将训练一个多元回归模型。

无论你是训练简单回归模型还是多元回归模型，过程和期望的结果通常是相同的。理想情况下，我们的模型在绘制实际值时应该显示出一条线性线，显示出数据中的强相关性，如图*图7.42*所示：

![图7.42 – 显示理想相关性的简单线性线](img/B17761_07_042.jpg)

图7.42 – 显示理想相关性的简单线性线

让我们继续探索多元线性回归模型的发展。在上一节导入的数据中，我们可以导入`sklearn`的`LinearRegression`类，用我们的训练数据对其进行拟合，并使用测试数据集进行预测：

[PRE45]

接下来，我们可以使用实际值和预测值来计算**R2**值，并在图上可视化。此外，我们还将捕捉**MSE**指标：

[PRE46]

代码将生成以下图：

![图7.43 – 线性回归模型的结果](img/B17761_07_043.jpg)

图7.43 – 线性回归模型的结果

立即，我们注意到这个简单的线性回归模型在预测我们的数据集方面非常有效。请注意，这个模型不仅使用了一个特征，而且还使用了所有特征来进行预测。从图中我们可以看出，我们的大部分数据都集中在左下角。这对于回归模型来说并不理想，因为我们更希望值在所有边界上均匀分布；然而，重要的是要记住，在生物技术领域，你几乎总是会遇到真实世界的数据，其中你会观察到这些项目。

如果你正在使用**Jupyter Notebooks**进行操作，请将数据集减少到只有一个输入特征，对数据进行缩放和拆分，训练一个简单线性回归，并将结果与多元线性回归进行比较。我们的预测值和实际值之间的相关性是增加还是减少？

### 逻辑回归

回想一下，在线性回归部分，我们讨论了使用单个线性线根据相关特征作为输入来预测值的方法。我们概述了以下线性方程：

![](img/B17761_Formula_07_037.jpg)

在某些情况下，数据与所需输出的关系可能不是由线性模型最好地表示，而是一个非线性模型：

![图 7.44 – 一个简单的 Sigmoid 曲线](img/B17761_07_044.jpg)

图 7.44 – 一个简单的 Sigmoid 曲线

虽然被称为**逻辑回归**，但这种回归主要用作**二元分类**算法。然而，这里的主要重点是，单词 *logistic* 指的是**逻辑函数**，也称为**Sigmoid 函数**，表示为：

![](img/B17761_Formula_07_038.jpg)

考虑到这一点，我们希望使用这个函数来预测我们的数据集中的结果。如果我们想确定一个化合物是否具有毒性，给定一个特定的输入值，我们可以计算输入的加权总和，使得：

![](img/B17761_Formula_07_039.jpg)

这将允许我们计算毒性的概率，使得：

![](img/B17761_Formula_07_040.jpg)

使用这个概率，可以做出最终预测，并分配一个输出值。请使用前一部分中的蛋白质分类数据集实现此模型，并将您找到的结果与其他分类器的结果进行比较。

### 决策树和随机森林回归

与其分类对应模型类似，**决策树回归（DTRs**）是常用的机器学习模型，它们实现了与决策树分类器几乎相同的内部机制。这两个模型之间的唯一区别是，回归器输出连续的数值，而分类器输出离散的类别。

类似地，还存在另一个称为**随机森林回归器（RFRs**）的模型，它的工作方式与其分类对应模型类似。这个模型也是一种集成方法，其中每个树被训练为一个单独的模型，然后平均。

让我们使用这个数据集来实现一个 RFR。就像我们之前做的那样，我们首先创建一个模型实例，将其拟合到我们的数据中，进行预测，并可视化结果：

[PRE47]

在模型开发完成后，我们可以使用以下图表来可视化结果：

![图 7.45 – 随机森林回归模型的结果](img/B17761_07_045.jpg)

图 7.45 – 随机森林回归模型的结果

注意，尽管 `max_depth` 参数：

[PRE48]

以下代码的输出如下，表明 `max_depth` 为 `8` 可能是最佳选择，因为它产生了 `0.967` 和 `248.133`：

![图 7.46 – 具有不同 max_depth 的随机森林回归模型的结果](img/B17761_07_046.jpg)

图 7.46 – 具有不同 max_depth 的随机森林回归模型的结果

与分类类似，回归中的决策树通常是避免过度拟合数据的同时开发模型的好方法。当使用 **sklearn** API 时，**DTR** 模型的另一个巨大好处是能够直接从模型中获得特征重要性的见解。让我们继续演示这一点：

[PRE49]

完成上述步骤后，得到以下图表：

![图 7.47 – 随机森林回归模型的特征重要性](img/B17761_07_047.jpg)

图 7.47 – 随机森林回归模型的特征重要性

观察这张图，我们可以看到对模型发展影响最大的前三个特征是 `HDonors`、`Heteroatoms` 和 `HAcceptors`。尽管这个特征重要性的例子是使用 RFR 模型开发的，但我们理论上可以使用它来与许多其他模型一起使用。在关于特征重要性的想法领域，有一个特别受到重视的库是 `SHAP` 库。强烈建议您浏览这个库及其提供的许多精彩特性（不是字面意义上的玩笑）。

### XGBoost 回归

与前一节中我们研究的 **XGBoost** 分类模型类似，我们也有回归实现，这使我们能够预测一个值而不是一个类别。我们可以像前一节那样轻松实现这一点：

[PRE50]

代码完成后，得到以下图示：

![图 7.48 – XGBoost 回归模型的结果](img/B17761_07_048.jpg)

图 7.48 – XGBoost 回归模型的结果

您会注意到，这个模型的实现给我们带来了一个非常令人尊重的 **R2** 值，在实际情况和预测值之间，并成功产生了 282.79 的 **MSE**，这略低于我们在本章尝试的一些其他模型。模型完成后，现在让我们继续看看我们如何在下一教程中使用 AWS 提供的一些自动化机器学习功能。

## 教程：属性预测的回归

在本章的整个过程中，我们回顾了一些最常见（且最受欢迎）的回归模型，这些模型与使用毒性数据集预测 `TPSA` 特征相关。在前一节关于分类的部分，我们创建了一个 GCP 实例，并使用 `auto-sklearn` 库自动识别给定数据集的最佳机器学习模型之一。在本教程中，我们将以类似的方式创建一个 `auto-sklearn` 库。此外，我们还将探索一种更强大的自动化机器学习方法，即使用 **AWS Autopilot**。在早期的一些章节中，我们使用 **AWS RDS** 启动一个关系型数据库来托管我们的毒性数据集。使用相同的 **AWS** 账户，我们现在将开始操作。

### 在 AWS 中创建 SageMaker 笔记本

与在**GCP**中创建笔记本类似，我们可以在**AWS**中通过几个简单的步骤创建一个**SageMaker**笔记本：

1.  在首页上导航到AWS管理控制台。点击**服务**下拉菜单，然后在**机器学习**部分下选择**Amazon SageMaker**：![图7.49 – AWS提供的服务列表](img/B17761_07_049.jpg)

    图7.49 – AWS提供的服务列表

1.  在页面左侧，点击**笔记本**下拉菜单，然后选择**笔记本实例**按钮：![图7.50 – 笔记本菜单截图](img/B17761_07_050.jpg)

    图7.50 – 笔记本菜单截图

1.  在笔记本实例菜单中，点击名为**创建笔记本实例**的橙色按钮：![图7.51 – 创建笔记本实例按钮截图](img/B17761_07_051.jpg)

    图7.51 – 创建笔记本实例按钮截图

1.  现在让我们给笔记本实例起一个名字，比如`biotech-machine-learning`。我们可以将实例类型保留为默认选择`ml.t2.medium`。这是一个中档实例，对于今天的演示来说已经足够了：![图7.52 – 笔记本实例设置截图](img/B17761_07_052.jpg)

    图7.52 – 笔记本实例设置截图

1.  在**权限和加密**部分下，为IAM角色部分选择**创建新角色**选项。IAM角色的主要思想是授予某些用户或角色与特定AWS资源交互的能力。例如，我们可以允许这个角色访问一些但不是所有的S3存储桶。为了本教程的目的，让我们继续授予这个角色访问任何S3存储桶的权限。点击**创建角色**：![图7.53 – 在AWS中创建IAM角色](img/B17761_07_053.jpg)

    图7.53 – 在AWS中创建IAM角色

1.  保持所有其他选项不变，点击**创建笔记本实例**。你将被重定向回**笔记本实例**菜单，在那里你会看到你的新创建的实例处于**待处理**状态。几分钟后，你会注意到状态变为**服务中**。点击状态右侧的**打开JupyterLab**按钮：![图7.54 – 在AWS中打开Jupyter笔记本或Jupyter lab的选项](img/B17761_07_054.jpg)

    图7.54 – 在AWS中打开Jupyter笔记本或Jupyter lab的选项

1.  再次，你将发现自己又回到了熟悉的Jupyter环境中，你一直在那里工作。

    **AWS SageMaker** 是一个成本极低的优质资源。在这个空间内，你可以运行本书中学到的所有 Python 命令和库。你可以创建目录来组织你的文件和脚本，并且可以在世界任何地方访问它们，而无需携带你的笔记本电脑。此外，你还将能够访问近 100 个 SageMaker 样本笔记本和启动代码供你使用。

![图 7.55 – SageMaker 启动笔记本的示例列表](img/B17761_07_055.jpg)

图 7.55 – SageMaker 启动笔记本的示例列表

完成这一最后步骤后，我们现在拥有了一个完全工作的笔记本实例。在接下来的部分，我们将使用 SageMaker 导入数据并开始运行我们的模型。

### 在 AWS 中创建笔记本并导入数据

由于我们再次在我们的熟悉的 Jupyter 空间中工作，我们可以通过选择右侧的许多选项之一轻松创建一个笔记本，并开始运行我们的代码。让我们开始吧：

1.  我们可以先选择右侧的 **conda_python3** 选项，在我们的当前目录中创建一个新的笔记本：![图 7.56 – conda_python3 的屏幕截图](img/B17761_07_056.jpg)

    图 7.56 – conda_python3 的屏幕截图

1.  笔记本准备就绪后，我们需要安装一些库以开始工作。请继续使用 `pip` 安装 `mysql-connector` 和 `pymysql`：

    [PRE51]

1.  接下来，我们需要导入一些东西：

    [PRE52]

1.  现在，我们可以定义一些我们将需要查询数据的项目，就像我们在 [*第 3 章*](B17761_03_Final_JM_ePub.xhtml#_idTextAnchor050)，*SQL 和关系型数据库入门* 中所做的那样：

    [PRE53]

1.  接下来，我们可以创建一个连接到我们的 **RDS** 实例：

    [PRE54]

1.  最后，我们可以使用基本的 `SELECT` 语句查询我们的数据：

    [PRE55]

完成这些后，我们现在可以直接从 **AWS RDS** 查询我们的数据。当你开始在数据科学领域探索新的模型时，你需要一个地方来存储和组织你的数据。选择一个平台，如 **AWS RDS**、**AWS S3** 或甚至 **GCP BigQuery**，将帮助你组织你的数据和研究成果。

### 使用毒性数据集运行 auto-sklearn

现在我们已经将数据导入到一个工作笔记本中，让我们继续使用 `auto-sklean` 库来识别最适合我们给定数据集的模型：

1.  我们可以在我们的 **SageMaker** 实例中安装 `auto-sklearn` 库：

    [PRE56]

1.  接下来，我们可以隔离我们的输入特征和输出值，并相应地进行缩放：

    [PRE57]

1.  数据缩放后，我们现在可以继续分离我们的训练集和测试集：

    [PRE58]

1.  最后，我们可以导入 `sklearn` 的回归实现，调整参数，并将模型拟合到我们的数据集：

    [PRE59]

1.  模型完成后，我们可以使用 `get_models_with_weights()` 函数查看表现最佳的候选模型：

    [PRE60]

1.  最后，我们可以使用**R2**和**MSE**指标来了解模型的表现，就像我们之前做的那样：

    [PRE61]

    在绘制输出后，得到以下结果：

![图7.57 – AutoML模型结果的关联性为0.97](img/B17761_07_057.jpg)

图7.57 – AutoML模型结果的关联性为0.97

从上面的图中我们可以看到实际结果和预测结果非常吻合，给出了大约0.97的R2值，显示出强烈的关联性。在下一节中，我们将探讨使用AWS Autopilot自动化模型开发过程部分的过程。

### 使用AWS Autopilot进行自动回归

在**AWS管理控制台**中可以找到许多不同的工具和应用，为开发者可能遇到的大量数据科学和计算机科学问题提供解决方案。特别是有一个工具在数据科学社区中脱颖而出，并开始受到欢迎，被称为**AWS Autopilot**。**AWS Autopilot**的目的是帮助自动化任何给定数据科学项目中的一些常规任务。我们可以在*图7.58*中看到这种视觉表示：

![图7.58 – Autopilot管道](img/B17761_07_058.jpg)

图7.58 – Autopilot管道

用户能够加载他们的数据集，识别几个参数，然后让模型从那里开始，因为它会识别表现最好的模型，优化一些参数，甚至为用户提供生成示例代码，以便进一步优化。让我们使用相同的数据集来演示这个模型的使用：

1.  我们可以从SageMaker控制台开始创建一个SageMaker Studio实例，导航到SageMaker控制台，并点击右侧的**打开SageMaker Studio**按钮。使用快速入门选项、默认设置和新的**IAM角色**，点击**提交**按钮。几分钟后，实例将配置。点击**打开工作室**按钮：![图7.59 – AWS中的打开工作室选项](img/B17761_07_059.jpg)

    图7.59 – AWS中的打开工作室选项

1.  在实例配置过程中，让我们将数据集上传到`biotech-machine-learning`，同时保持所有其他选项为默认值。![图7.60 – 在AWS中创建一个桶](img/B17761_07_060.jpg)

    图7.60 – 在AWS中创建一个桶

1.  创建完成后，打开桶并点击**上传**按钮。然后，上传经过减少和清理的蛋白质数据集的CSV文件。![图7.61 – 在AWS中上传文件](img/B17761_07_061.jpg)

    图7.61 – 在AWS中上传文件

1.  数据集上传后，现在让我们回到SageMaker。使用左侧的导航窗格，选择**SageMaker组件和注册表**标签。使用下拉菜单，选择**实验和试验**，然后点击**创建Autopilot实验**按钮：![图7.62 – 在AWS SageMaker Studio中创建SageMaker资源](img/B17761_07_062.jpg)

    图7.62 – 在AWS SageMaker Studio中创建SageMaker资源

1.  让我们继续，并为实验命名，例如`dataset-pdb-nodups-cleaned`。

1.  在**连接您的数据**部分，选择您之前创建的S3存储桶名称以及数据集文件名：![图7.63 – 将数据连接到实验](img/B17761_07_063.jpg)

    图7.63 – 将数据连接到实验

1.  接下来，选择目标列，在我们的案例中，是分类列：![图7.64 – 选择模型训练过程中的目标列](img/B17761_07_064.jpg)

    图7.64 – 选择模型训练过程中的目标列

1.  最后，你现在可以继续禁用**自动部署**选项，并点击**创建实验**。类似于GCP的**AutoML**，应用程序将识别出一组被认为最适合您给定数据集的模型。您可以选择**试点**或**完整**。

    一个完整的实验将在训练和调整模型的同时，允许用户实时查看详细信息和统计数据。它将经历各种阶段，如预处理、候选定义生成、特征工程、模型调整和报告生成。

1.  完成流程后，将展示一个包含所有训练模型及其相关指标的仪表板，如图7.65所示。用户可以探索这些模型，并通过几个简单的点击来部署它们。

![图7.65 – 自动驾驶模型的结果](img/B17761_07_065.jpg)

图7.65 – 自动驾驶模型的结果

AWS Autopilot是一个强大且有用的工具，每位数据科学家在面临困难的数据集时都可以使用。它不仅有助于识别给定数据集的最佳模型，还可以帮助预处理数据、调整模型，并为用户提供示例代码。

# 摘要

恭喜！我们终于到达了这一非常密集但非常有信息量的章节的结尾。在本章中，我们学到了很多不同的东西。在本章的前半部分，我们探索了分类领域，并展示了如何使用单细胞RNA数据集应用多个模型——这是生物技术和生命科学领域的经典应用。我们了解了许多不同的模型，包括KNNs、SVMs、决策树、随机森林和XGBoost。然后，我们将数据和代码移动到GCP，我们在BigQuery中存储了数据，并配置了一个笔记本实例来运行我们的代码。此外，我们还学习了如何使用auto-sklearn自动化与蛋白质分类数据集相关的模型开发过程中的某些手动和劳动密集型部分。最后，我们利用GCP的AutoML应用程序为我们的数据集开发了一个分类模型。

在本章的后半部分，我们探讨了与毒性数据集相关的回归领域。我们探讨了数据中的相关性概念，并学习了一些重要的回归模型。我们查看的一些模型包括简单和多元线性回归、逻辑回归、决策树回归器以及XGBoost回归器。然后，我们将代码迁移到AWS的SageMaker平台，并使用之前配置的RDS来查询我们的数据，并运行auto-sklearn进行回归。最后，我们为毒性数据集实现了AWS Autopilot的自动化机器学习模型。

到目前为止，我们的大部分时间都花在利用`sklearn`库开发机器学习模型上了。然而，并非所有数据集都可以通过机器学习进行分类或回归——有时，我们需要更强大的模型集。对于这类数据集，我们可以转向深度学习领域，这将是下一章的重点。
