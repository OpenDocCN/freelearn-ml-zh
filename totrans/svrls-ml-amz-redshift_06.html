<html><head></head><body>
		<div id="_idContainer119">
			<h1 id="_idParaDest-81" class="chapter-number"><a id="_idTextAnchor083"/>6</h1>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor084"/>Building Classification Models</h1>
			<p><a id="_idTextAnchor085"/>In this chapter, you will learn about classification algorithms <a id="_idIndexMarker257"/>used in <strong class="bold">machine</strong> <strong class="bold">learning</strong> (<strong class="bold">ML</strong>). You will learn about the various methods that Redshift offers when you create classification models. This chapter <a id="_idIndexMarker258"/>will provide detailed examples of both <strong class="bold">binary</strong> and <strong class="bold">multi-class classification models</strong> and <a id="_idIndexMarker259"/>show you how to solve business problems with these modeling techniques. By the end of this chapter, you will be in a position to identify whether a business problem is a classification or not, identify the right method that Redshift offers in training, and build <span class="No-Break">a model.</span></p>
			<p>In this chapter, we will go through the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>An introduction to <span class="No-Break">classification algorithms</span></li>
				<li>Creating a model syntax with <span class="No-Break">user guidance</span></li>
				<li>Training a binary classification model using the <span class="No-Break">XGBoost algorithm</span></li>
				<li>Training a multi-class classification model using the Linear Learner <span class="No-Break">model type</span></li>
			</ul>
			<h1 id="_idParaDest-83">Technic<a id="_idTextAnchor086"/>al requirements</h1>
			<p>This chapter requires a web browser and access to <span class="No-Break">the following:</span></p>
			<ul>
				<li>An <span class="No-Break">AWS account</span></li>
				<li>An Amazon Redshift <span class="No-Break">Serverless endpoint</span></li>
				<li>Amazon Redshift Query <span class="No-Break">Editor v2</span></li>
				<li>Completing the <em class="italic">Getting started with Amazon Redshift Serverless</em> section in <a href="B19071_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a></li>
			</ul>
			<p>You can find the code used in this chapter <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift<span id="_idTextAnchor087"/>/</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor088"/>An introduction to classification algorithms</h1>
			<p><strong class="bold">Classification</strong> is the <a id="_idIndexMarker260"/>process of categorizing any kind of entity or class so that it is better <a id="_idIndexMarker261"/>understood and analyzed. The classifying process usually happens as part of a pre-setup business process (for example, tagging a product as defective or good after observing it), or through a return process (for example, tagging a product as defective after the customer returned it as defective). In either event, the important point is classifying an entity – in this case, a product into a class (i.e., defective <span class="No-Break">or not).</span></p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em> shows data that has been classified into two classes using three input variables. The figure shows where a pair of <strong class="bold">Input</strong> and <strong class="bold">Output</strong> data points are categorized into two classes. When<a id="_idIndexMarker262"/> output labels consist of only two classes, it is <a id="_idIndexMarker263"/>called a <strong class="bold">binary </strong><span class="No-Break"><strong class="bold">classification</strong></span><span class="No-Break"> problem:</span></p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B19071_06_01.jpg" alt="Figure 6.1 – Binary classification"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Binary classification</p>
			<p>If the output variable consists of more than two classes – for example, predicting whether a fruit is an apple, an orange, or a pear – then it<a id="_idIndexMarker264"/> is called <strong class="bold">multi-class classification</strong>. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.2</em> shows data that has been classified into<a id="_idIndexMarker265"/> multiple classes based on a set of three input variables. The figure shows a multi-class classification chart, illustrating how input and output pairs are classified into <span class="No-Break">three classes:</span></p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B19071_06_02.jpg" alt="Figure 6.2 – Multi-class classification"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Multi-class classification</p>
			<p>The classification process can also happen on data that does not have classes defined yet. Let us continue to understand how this <span class="No-Break">is possible.</span></p>
			<p>It is not always the case that your entities are grouped or categorized in a certain way. For example, if you want to analyze your customers’ purchase history or clickstream activity, or if you want to group similar customers based on demographics or shopping behavior, then classification algorithms come in handy to analyze the data and group similar data points into clusters. This type of classification modeling is<a id="_idIndexMarker266"/> called <span class="No-Break"><strong class="bold">unsupervised learning</strong></span><span class="No-Break">.</span></p>
			<p>Establishing classes helps the analysis process – for example, once products are tagged to a class label, you can easily retrieve a list of defective products that are returned and then further study the characteristics, such as store location, the demographics of the customer who returned the product, and the season when a product was returned most. How and when classes are defined and established enables businesses to conduct a deep-dive analysis, not only answering questions such as where and what but also training an ML model on historical data and classes, and predicting which class an entity will <span class="No-Break">fall into.</span></p>
			<p>Common use cases <a id="_idIndexMarker267"/>where classification models are useful include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Customer <span class="No-Break">behavior prediction</span></li>
				<li>Document or <span class="No-Break">image classification</span></li>
				<li><span class="No-Break">Spam filtering</span></li>
			</ul>
			<p>In this chapter, we will show you how to<a id="_idIndexMarker268"/> create different classification models that <a id="_idIndexMarker269"/>Redshift offers you. Amazon Redshift provides <strong class="bold">XGBoost</strong>, <strong class="bold">multilayer perceptron</strong> (<strong class="bold">MLP</strong>), and <strong class="bold">Linear Learner algorithms</strong> to<a id="_idIndexMarker270"/> train and build a <span class="No-Break">classification model.</span></p>
			<p>In this chapter, you will begin the journey of learning about supervised classification models by building binary classification models, using XGBoost, and a multi-class classification model, using linear learner. MLP models will be covered in <a href="B19071_09.xhtml#_idTextAnchor157"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, whereas unsupervised classification modeling will be covered in <a href="B19071_08.xhtml#_idTextAnchor139"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><span class="No-Break">.</span></p>
			<p>Now, we will walk you through the detailed syntax of creating models with <span class="No-Break">Redshift ML.</span></p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor089"/>Diving into the Redshift CREATE MODEL syntax</h2>
			<p>In <a href="B19071_04.xhtml#_idTextAnchor057"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><em class="italic">,</em> we<a id="_idIndexMarker271"/> saw different variations of the Redshift <strong class="source-inline">CREATE MODEL</strong> command and how a data analyst, citizen data scientist, or data scientist can operate the <strong class="source-inline">CREATE MODEL</strong> command, with varying degrees of complexity. In this section, we will introduce you to a citizen data scientist persona, who is not fully aware of statistics but has good knowledge about identifying what algorithm to use and what problem type can be applied to a business problem. In the Redshift ML world, this type of model <a id="_idIndexMarker272"/>creation is known as <strong class="bold">CREATE MODEL with </strong><span class="No-Break"><strong class="bold">user guidance</strong></span><span class="No-Break">.</span></p>
			<p>We are going to explore the model type and problem type parameters of the <strong class="source-inline">CREATE MODEL</strong> statement. As part of <em class="italic">CREATE MODEL with user guidance</em>, you also have the option of setting a preprocessor, but we will leave that topic for <a href="B19071_10.xhtml#_idTextAnchor178"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
			<p>As an ML model creator, you will decide what algorithm to use and what problem type to address. Redshift ML still performs the feature engineering of independent variables behind the scenes. For example, out of 20 features, Redshift ML will automatically identify the categorical variables and numeric variables and create one-hot-encoded value or standardization of numerical variables where applicable, along with various other tasks required to complete the <span class="No-Break">model training.</span></p>
			<p>In<a id="_idIndexMarker273"/> summary, you let Redshift ML handle the bulk of data preparation tasks for ML. As a model creator, you come up with an algorithm to be used and a problem type to be solved. By preselecting an algorithm type and problem type, Redshift ML will reduce the training type, as it trains the model on other algorithms and problem types. Compared to the full <strong class="source-inline">AUTO</strong> CREATE MODEL statement that we created in <a href="B19071_05.xhtml#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">CREATE MODEL with user guidance</em> takes <span class="No-Break">less time.</span></p>
			<p>As mentioned in the previous section, we will use the XGBoost algorithm for binary classification and the linear learner algorithm for <span class="No-Break">multi-class classification.</span></p>
			<p>You can learn more<a id="_idIndexMarker274"/> about XGBoost <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/XGBoost.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/XGBoost.html</span></a><span class="No-Break">.</span></p>
			<p>And you can learn more about<a id="_idIndexMarker275"/> Linear Learner <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html</span></a><span class="No-Break">.</span></p>
			<p>Using a simple <strong class="source-inline">CREATE MODEL</strong> statement, Redshift ML will use SageMaker Autopilot to automatically determine the problem type, algorithm, and the best model type <span class="No-Break">to use.</span></p>
			<p>With Redshift ML, you can influence a model by providing user guidance. You can choose <strong class="source-inline">model_type</strong>, <strong class="source-inline">problem_type</strong>, and <strong class="source-inline">objective</strong> when you issue the <strong class="source-inline">CREATE MODEL</strong> statement. You can find more details on the syntax and options <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/dg/r_create_model_use_cases.html</span></a><span class="No-Break">.</span></p>
			<p>So far, we have discussed the basics of the Redshift ML <strong class="source-inline">CREATE MODEL</strong> syntax and how you can provide guidance, such as model type and objective, or choose to let Redshift ML automatically choose these <span class="No-Break">for you.</span></p>
			<p>Now, you will learn how to create a binary classification model and specify the <span class="No-Break">XG<a id="_idTextAnchor090"/>Boost algorithm.</span></p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor091"/>Training a binary classification model using the XGBoost algorithm</h1>
			<p>Binary classification models <a id="_idIndexMarker276"/>are used to solve the problem of predicting one class of two possible classes – for example, predicting whether it will rain or not. The goal is to learn about past data <a id="_idIndexMarker277"/>points and figure out which one of the target buckets a particular data point will fall into. The typical use cases of a binary classification model are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Predicting whether a patient suffers from <span class="No-Break">a disease</span></li>
				<li>Predicting whether a customer will churn <span class="No-Break">or not</span></li>
				<li>Predicting behavior – for example, whether a customer will file an appeal <span class="No-Break">or not</span></li>
			</ul>
			<p>In the next few sections, we will go through the following steps to achieve our goal of creating a binary classification model to be used to run <span class="No-Break">inference queries:</span></p>
			<ol>
				<li>Defining the <span class="No-Break">business problem</span></li>
				<li>Uploading and <span class="No-Break">analyzing data</span></li>
				<li>Creating <span class="No-Break">the model</span></li>
				<li>Running prediction queries<a id="_idTextAnchor092"/> against <span class="No-Break">the model</span></li>
			</ol>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor093"/>Establishing the business problem</h2>
			<p>To build our<a id="_idIndexMarker278"/> binary classification problem, we will take a look at a banking campaign issue. Banks spend a lot of money on marketing campaigns targeted toward their customers so that they will subscribe to their products. It is very important that banks build efficiency into their campaign, and this can be done by learning the last campaign dataset and predicting future campaign results. We will work on predicting whether a banking customer will subscribe to a banking product offer<a id="_idTextAnchor094"/> of a <span class="No-Break">term deposit.</span></p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor095"/>Uploading and analyzing the data</h2>
			<p>We are <a id="_idIndexMarker279"/>going to work on a bank marketing dataset in this section. The data is related to direct marketing <a id="_idIndexMarker280"/>campaigns of a Portuguese banking institution. Imagine you are a marketing analyst and your goal is to increase the amount of deposits by offering a term deposit to your customers. It is very important that marketing campaigns target customers appropriately. You will create a model using Redshift ML to predict whether a customer is likely to accept the term deposit offer. This dataset is sourced <span class="No-Break">from </span><a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing"><span class="No-Break">https://archive.ics.uci.edu/ml/datasets/bank+marketing</span></a><span class="No-Break">.</span></p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. <em class="italic">A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems</em>, Elsevier, 62:22–31, <span class="No-Break">June 2014</span></p>
			<p>The classification goal is to predict whether the client will subscribe (yes/no) to a term deposit (the <span class="No-Break"><em class="italic">y</em></span><span class="No-Break"> variable).</span></p>
			<p>The dataset has columns such as age, job, marital status, education level, and <span class="No-Break">employment status.</span></p>
			<p>Metadata about these columns can also be found at the UCI ML repository website <span class="No-Break">here: </span><a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing"><span class="No-Break">https://archive.ics.uci.edu/ml/datasets/bank+marketing</span></a><span class="No-Break">.</span></p>
			<p>As you can see from the preceding link, there are 20 independent variables and 1 dependent variable (<em class="italic">y</em>). We can use any or all of these independent variables as input to our <strong class="source-inline">CREATE MODEL</strong> statement to be able to predict the outcome, <em class="italic">y</em>, which indicates whether the customer is likely to accept <span class="No-Break">the offer.</span></p>
			<p>After successfully connecting to Redshift as an admin or database developer, create the schema and load data into Amazon Redshift using the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Navigate to Redshift <strong class="bold">query editor v2</strong>, and connect to the <strong class="bold">Serverless</strong> endpoint and the <span class="No-Break"><strong class="bold">dev</strong></span><span class="No-Break"> database.</span></li>
				<li>Rename the <strong class="bold">Untitled</strong> query editor by saving it <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">Chap6</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>The following screenshot shows the serverless connection, the database set to <strong class="bold">dev</strong>, and the query edito<a id="_idTextAnchor096"/>r <a id="_idTextAnchor097"/>page saved <span class="No-Break">as </span><span class="No-Break"><strong class="bold">Chap6</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B19071_06_03.jpg" alt="Figure 6.3 – Query Editor ﻿v2"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Query Editor v2</p>
			<ol>
				<li value="3">Now, using <a id="_idIndexMarker281"/>the <a id="_idIndexMarker282"/>following line of code, create the schema. This schema is where all the tables and data needed for this chapter will be created <span class="No-Break">and maintained:</span><pre class="source-code">
Create schema chapter6_supervisedclassification;</pre></li>
			</ol>
			<p>You will see output like this, indicating that your schema <span class="No-Break">is created:</span></p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B19071_06_04.jpg" alt="Figure 6.4 – Schema created"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Schema created</p>
			<p>The following code will create the <strong class="source-inline">bank_details_training</strong> table to store data to train the model, and the <strong class="source-inline">bank_details_inference</strong> table to store data to run the inference queries. Note that we have already split our input dataset into these two datasets for you. All of the SQL commands used in this chapter can be found <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter6/chapter6.sql"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter6/chapter6.sql</span></a><span class="No-Break">.</span></p>
			<ol>
				<li value="4">Run the<a id="_idIndexMarker283"/> following <a id="_idIndexMarker284"/>code from GitHub to create the training and inference tables in Query <span class="No-Break">Editor v2:</span><pre class="source-code">
CREATE TABLE chapter6_supervisedclassification.bank_details_training(</pre><pre class="source-code">
   age numeric, "job" varchar marital varchar, education varchar,</pre><pre class="source-code">
   "default" varchar, housing varchar, loan varchar,</pre><pre class="source-code">
   contact varchar, month varchar, day_of_week varchar,</pre><pre class="source-code">
   …,</pre><pre class="source-code">
   y boolean ) ;</pre><pre class="source-code">
--create table to store data for running predictions</pre><pre class="source-code">
CREATE TABLE chapter6_supervisedclassification.bank_details_inference(</pre><pre class="source-code">
   age numeric, "job" varchar marital varchar, education varchar,</pre><pre class="source-code">
   "default" varchar, housing varchar, loan varchar,</pre><pre class="source-code">
   contact varchar, month varchar, day_of_week varchar,</pre><pre class="source-code">
   …,</pre><pre class="source-code">
   y boolean ) ;</pre></li>
			</ol>
			<p>You will see <a id="_idIndexMarker285"/>output like this to verify that your tables have been <span class="No-Break">created successfully:</span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B19071_06_05.jpg" alt="Figure 6.5 – Tables created successfully"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Tables created successfully</p>
			<p>Now that <a id="_idIndexMarker286"/>you have created the tables, run the commands in <em class="italic">step 5</em> using Query Editor v2 to load the data, using the S3 <span class="No-Break">buckets provided.</span></p>
			<ol>
				<li value="5">Load the sample data into the tables created in <em class="italic">step 4</em> by using the following command, which can be found on GitHub. Note that we use the <strong class="source-inline">COPY</strong> command to load this data from <span class="No-Break">Amazon S3:</span><pre class="source-code">
--load data into  bank_details_inference</pre><pre class="source-code">
TRUNCATE chapter6_supervisedclassification.bank_details_inference;</pre><pre class="source-code">
 COPY chapter6_supervisedclassification.bank_details_inference from 's3://packt-serverless-ml-redshift/chapter06/bank-marketing-data/inference-data/inference.csv' REGION 'eu-west-1' IAM_ROLE default CSV IGNOREHEADER 1 delimiter ';';</pre><pre class="source-code">
-- load data into bank_details_training</pre><pre class="source-code">
TRUNCATE chapter6_supervisedclassification.bank_details_training;</pre><pre class="source-code">
 COPY chapter6_supervisedclassification.bank_details_training from 's3://packt-serverless-ml-redshift/chapter06/bank-marketing-data/training-data/training.csv' REGION 'eu-west-1' IAM_ROLE default CSV IGNOREHEADER 1 delimiter ';';</pre></li>
				<li>Analyze<a id="_idIndexMarker287"/> the <a id="_idIndexMarker288"/>customer term deposit subscription table by creating a histogram chart. First, run the following command again using Query <span class="No-Break">Editor v2:</span><pre class="source-code">
SELECT y, COUNT(*) Customer_Count FROM chapter6_supervisedclassification.bank_details_training</pre><pre class="source-code">
GROUP BY y</pre><pre class="source-code">
;</pre></li>
			</ol>
			<p>You can see in the result set that <strong class="bold">36548</strong> customers did not choose the bank’s offer and <strong class="bold">4640</strong> did accept. You can also use the chart feature in Query Editor v2 to create a bar chart. Click on the <strong class="bold">Chart</strong> option found on the right-hand side in the <span class="No-Break"><strong class="bold">Result</strong></span><span class="No-Break"> pane:</span></p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B19071_06_06.jpg" alt="Figure 6.6 – The subscription results and the Chart option"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – The subscription results and the Chart option</p>
			<p>You will <a id="_idIndexMarker289"/>get the following result after choosing <strong class="bold">Bar</strong> for <strong class="bold">Type</strong>, <strong class="bold">y</strong> for the <strong class="bold">X</strong> value, and <strong class="bold">customer_count</strong> for<a id="_idIndexMarker290"/> the <span class="No-Break"><strong class="bold">Y</strong></span><span class="No-Break"> value:</span></p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B19071_06_07.jpg" alt="Figure 6.7 – A chart of customer acceptance"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – A chart of customer acceptance</p>
			<p>Now that we have our data loa<a id="_idTextAnchor098"/>d<a id="_idTextAnchor099"/>ed, we can create <span class="No-Break">our model.</span></p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor100"/>Using XGBoost to train a binary classification model</h2>
			<p>In this <a id="_idIndexMarker291"/>section, you will specify <strong class="source-inline">MODEL_TYPE</strong> and <strong class="source-inline">PROBLEM_TYPE</strong> to create a binary classification model using the XGBoost algorithm. We will now address the banking campaign problem. The goal of this model is to predict whether a customer will subscribe to a term deposit <span class="No-Break">or not.</span></p>
			<p>We will set <strong class="source-inline">MODEL_TYPE</strong> as <strong class="source-inline">XGBoost</strong> and <strong class="source-inline">PROBLEM_TYPE</strong> as <strong class="source-inline">BINARY_CLASSIFICATION</strong>. We will use the default <strong class="source-inline">IAM_ROLE</strong>. We also need to specify the S3 bucket where the model artifacts will be stored and, additionally, set <strong class="source-inline">MAX_RUNTIME</strong> to <strong class="source-inline">3600</strong> (<span class="No-Break">in seconds).</span></p>
			<p>The following is the code to create the model. You will find the complete code along with all the SQL command<a id="_idIndexMarker292"/>s needed for the chapter <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/chapter6.sql"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/chapter6.sql</span></a><span class="No-Break">:</span></p>
			<pre class="source-code">
DROP MODEL chapter6_supervisedclassification.banking_termdeposit;
CREATE  MODEL chapter6_supervisedclassification.banking_termdeposit
FROM (
SELECT
   age ,
   "job" ,
   marital ,
   education ,
   "default" ,
   housing ,
   loan ,
   contact ,
   month ,
   day_of_week ,
   duration,
   campaign ,
   pdays ,
   previous ,
   poutcome ,
   emp_var_rate ,
   cons_price_idx ,
   cons_conf_idx ,
   euribor3m ,
   nr_employed ,
   y
FROM
    chapter6_supervisedclassification.bank_details_training )
    TARGET y
FUNCTION predict_term_deposit
IAM_ROLE default
MODEL_TYPE XGBoost
PROBLEM_TYPE BINARY_CLASSIFICATION
SETTINGS (
  S3_BUCKET '&lt;&lt;your-S3-Bucket',
  MAX_RUNTIME 9600
  )
;</pre>
			<p>By setting <strong class="source-inline">MODEL_TYPE</strong> to <strong class="source-inline">XGBoost</strong> and <strong class="source-inline">PROBLEM_TYPE</strong> to <strong class="source-inline">BINARY_CLASSIFICATION</strong>, we guide Redshift ML to only search for the best XGBoost model in this <a id="_idIndexMarker293"/>training run. If this is left as default, Redshift ML checks whether other classification models can be applied to <span class="No-Break">the dataset.</span></p>
			<p>Since<a id="_idIndexMarker294"/> the <strong class="bold">SageMaker AutoPilot algorithm</strong> does not have to test other model types or determine the problem type, the end result will be less training time. In this example, SageMaker Autopilot takes care of selecting the objective type, adjusting hyperparameters, and handling the data <span class="No-Break">preprocessing steps.</span></p>
			<p>To check the status of the model, run the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
SHOW MODEL chapter6_supervisedclassification.banking_termdeposit;</pre>
			<p>You will get the <span class="No-Break">following result:</span></p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B19071_06_08.jpg" alt="Figure 6.8 – Showing the model output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Showing the model output</p>
			<p>From the <a id="_idIndexMarker295"/>preceding screenshot, we can see that the model is still under training. Also, note that Redshift ML picks up the <strong class="bold">Model Type</strong> and <strong class="bold">Problem Type</strong> parameter values from our <strong class="source-inline">CREATE MODEL</strong> statement. Other parameters, such as the objective, hyperparameters, and preprocessing, are still auto-handled by <span class="No-Break">Redshift ML.</span></p>
			<p>The <strong class="bold">predict_term_deposit</strong> parameter under <strong class="bold">Function Name</strong> is used to generate predictions, which we will use in the <span class="No-Break">next section.</span></p>
			<p>Run the <strong class="source-inline">SHOW MODEL</strong> command again after some time to check whether model training is complete. From the following screenshot, you can see that <strong class="bold">Model State</strong> is <strong class="bold">READY</strong> and <strong class="bold">F1</strong> has been selected as the objective for model evaluation. The <strong class="bold">F1</strong> score is <strong class="bold">0.646200</strong>, or 64%. The <a id="_idIndexMarker296"/>closer this number is to <a id="_idTextAnchor101"/>1, the better the <span class="No-Break">model score:</span></p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B19071_06_09.jpg" alt="Figure 6.9 – Showing the model output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Showing the model output</p>
			<p>Let’s run the following query<a id="_idIndexMarker297"/> against our training data to validate the <span class="No-Break">F1 score:</span></p>
			<pre class="source-code">
WITH infer_data
 AS (
    SELECT  y as actual, chapter6_supervisedclassification.predict_term_deposit(
   age ,   "job" ,   marital ,   education ,   "default" ,   housing ,   loan ,   contact ,   month ,   day_of_week ,   duration ,   campaign ,   pdays ,   previous ,   poutcome ,   emp_var_rate ,   cons_price_idx ,        cons_conf_idx ,        euribor3m ,   nr_employed
) AS predicted,
     CASE WHEN actual = predicted THEN 1::INT
         ELSE 0::INT END AS correct
    FROM chapter6_supervisedclassification.bank_details_training
    ),
 aggr_data AS (
     SELECT SUM(correct) as num_correct, COUNT(*) as total FROM infer_data
 )
 SELECT (num_correct::float/total::float) AS accuracy FROM aggr_data;</pre>
			<p>You can see in the following output that our accuracy is very good at <span class="No-Break">almost 94%:</span></p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B19071_06_10.jpg" alt="Figure 6.10 – The accuracy results"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – The accuracy results</p>
			<p>Now that the model training is complete, we will use the function create<a id="_idTextAnchor102"/>d to run <span class="No-Break">prediction queries.</span></p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor103"/>Running predictions</h2>
			<p>Let us run <a id="_idIndexMarker298"/>some predictions on our inference dataset to see how many customers are predicted to subscribe to the term deposit. Run the following SQL statement in Query <span class="No-Break">Editor v2:</span></p>
			<pre class="source-code">
WITH term_data AS ( SELECT chapter6_supervisedclassification.predict_term_deposit( age,"job" ,marital,education,"default",housing,loan,contact,month,day_of_week,duration,campaign,pdays,previous,poutcome,emp_var_rate,cons_price_idx,cons_conf_idx,euribor3m,nr_employed) AS predicted
FROM chapter6_supervisedclassification.bank_details_inference )
SELECT
CASE WHEN predicted = 'Y'  THEN 'Yes-will-do-a-term-deposit'
     WHEN predicted = 'N'  THEN 'No-term-deposit'
     ELSE 'Neither' END as deposit_prediction,
COUNT(1) AS count
from term_data GROUP BY 1;</pre>
			<p>You should get the <span class="No-Break">following output:</span></p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B19071_06_11.jpg" alt="Figure 6.11 – Prediction results"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Prediction results</p>
			<p>We can see that <strong class="bold">642</strong> customers are predicted to accept the offer to subscribe to the term deposit, and <strong class="bold">3477</strong> are predicted to not accept <span class="No-Break">the offer.</span></p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor104"/>Prediction probabilities</h2>
			<p>Amazon Redshift ML<a id="_idIndexMarker299"/> now provides the capability to get the probability of a prediction for binary and multi-class classification problems. Note that in the output of the <strong class="source-inline">SHOW MODEL</strong> command in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em>, an additional function name has been created called <strong class="source-inline">predict_term_deposit_prob</strong>. Run the following query to check the probability that married customers with management jobs and between 35 and 45 years of age will accept the term <span class="No-Break">deposit offer:</span></p>
			<pre class="source-code">
SELECT
age,"job" ,marital ,
chapter6_supervisedclassification.predict_term_deposit_prob( age,"job" ,marital,education,"default",housing,loan,contact,month,day_of_week,duration,campaign,pdays,previous,poutcome,emp_var_rate,cons_price_idx,cons_conf_idx,euribor3m,nr_employed) AS predicted
FROM chapter6_supervisedclassification.bank_details_inference
where marital = 'married'
  and "job" = 'management'
  and age between 35 and 40;</pre>
			<p>You will see the <span class="No-Break">following results:</span></p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B19071_06_12.jpg" alt="Figure 6.12 – Probability results"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Probability results</p>
			<p>You can see in the <a id="_idIndexMarker300"/>first row a <strong class="bold">0.99985629</strong> probability of a <em class="italic">false</em> prediction and only a <strong class="bold">0.00014372</strong> probability of a <span class="No-Break"><em class="italic">true</em></span><span class="No-Break"> prediction.</span></p>
			<p>You can also modify the preceding query to see the probability of the customers that are predicted to accept the term deposit offer. Run the following SQL command in Query <span class="No-Break">Editor v2:</span></p>
			<pre class="source-code">
SELECT age, "job", marital, predicted.labels[0], predicted.probabilities[0]
from
 (select
age,"job" ,marital ,
chapter6_supervisedclassification.predict_term_deposit_prob( age,"job" ,marital,education,"default",housing,loan,contact,month,day_of_week,duration,campaign,pdays,previous,poutcome,emp_var_rate,cons_price_idx,cons_conf_idx,euribor3m,nr_employed) AS predicted
FROM chapter6_supervisedclassification.bank_details_inference
where marital = 'married'
  and "job" = 'management'
  and age between 35 and 40) t1
where predicted.labels[0] = 't';</pre>
			<p>You will <a id="_idIndexMarker301"/>see similar results <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B19071_06_13.jpg" alt="Figure 6.13 – The probability results for customers accepting the term offer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – The probability results for customers accepting the term offer</p>
			<p>In <a href="B19071_05.xhtml#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, you learned how to determine feature importance by running an explainability report. Run the following query to see which inputs contributed most to the <span class="No-Break">model prediction:</span></p>
			<pre class="source-code">
select json_table.report.explanations.kernel_shap.label0.global_shap_values from
 (select explain_model('chapter6_supervisedclassification.banking_termdeposit') as report) as json_table;</pre>
			<p>Take the result and copy it to the editor so that it is easier to read, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B19071_06_14.jpg" alt="Figure 6.14 – The explainability report"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – The explainability report</p>
			<p>This shows <a id="_idIndexMarker302"/>that <strong class="source-inline">pdays</strong> has the most importance and that <strong class="source-inline">poutcome</strong> has <span class="No-Break">the least.</span></p>
			<p>Now that you have built a binary classification model, let us move on and try building a multi-class <span class="No-Break">classification model.</span></p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor105"/>Training a multi-class classification model using the Linear Learner model type</h1>
			<p>In <a id="_idIndexMarker303"/>this section, you will <a id="_idIndexMarker304"/>learn how to build a multi-class classification model in Amazon Redshift ML using the linear <span class="No-Break">learner algorithm.</span></p>
			<p>To do this, we will use a customer segmentation dataset from <span class="No-Break">Kaggle: </span><a href="https://www.kaggle.com/datasets/vetrirah/customer"><span class="No-Break">https://www.kaggle.com/datasets/vetrirah/customer</span></a><span class="No-Break">.</span></p>
			<p>You will use this dataset to train a model to classify customers into one of four segments (<strong class="source-inline">A</strong>, <strong class="source-inline">B</strong>, <strong class="source-inline">C</strong>, or <strong class="source-inline">D</strong>). By segmenting customers, you can better understand the customer and do targeted marketing to customers, with product offerings that are relevant <span class="No-Break">to them.</span></p>
			<p>Our data has already been split into training and testing sets and is stored in the following <span class="No-Break">S3 locations:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">s3://packt-serverless-ml-redshift/chapter06/segmentation/train.csv</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">s3://packt-serverless-ml-redshift/chapter06/segmentation/test.csv</strong></span></li>
			</ul>
			<p>After successfully connecting to Redshift as an admin or database developer, load data into Amazon Redshift <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Navigate to Redshift <strong class="bold">query editor v2</strong>, and connect to the <strong class="bold">Serverless</strong> endpoint and the <span class="No-Break"><strong class="bold">dev</strong></span><span class="No-Break"> database.</span></li>
				<li>Use the same schema and query editor page you created for the binary classification exercise (see the <em class="italic">Uploading and analyzing the </em><span class="No-Break"><em class="italic">data</em></span><span class="No-Break"> section).</span></li>
			</ol>
			<p>Create the <a id="_idIndexMarker305"/>train <a id="_idIndexMarker306"/>and test tables and load the data using the following SQL commands in Query Editor v2. These SQL commands can be <span class="No-Break">found at</span></p>
			<p><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/CodeFiles/chapter6/chapter6.sql"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-</span><span class="No-Break">with-Amazon-Redshift/blob/main/CodeFiles/chapter6/chapter6.sql</span></a><span class="No-Break">:</span></p>
			<pre class="source-code">
CREATE TABLE chapter6_supervisedclassification.cust_segmentation_train (
    id numeric,
    gender varchar,
    ever_married  varchar,
    age numeric,
    graduated varchar,
    profession varchar,
    work_experience numeric,
    spending_score  varchar,
    family_size numeric,
    var_1 varchar,
    segmentation varchar
)
DISTSTYLE AUTO;
COPY chapter6_supervisedclassification.cust_segmentation_train FROM 's3://packt-serverless-ml-redshift/chapter06/Train.csv' IAM_ROLE DEFAULT FORMAT AS CSV DELIMITER ',' QUOTE '"' IGNOREHEADER 1 REGION AS 'eu-west-1';
CREATE TABLE chapter6_supervisedclassification.cust_segmentation_test (
    id numeric,
    gender varchar,
    ever_married  varchar,
    age numeric,
    graduated varchar,
    profession varchar,
    work_experience numeric,
    spending_score  varchar,
    family_size numeric,
    var_1 varchar
)
DISTSTYLE AUTO;
COPY chapter6_supervisedclassification.cust_segmentation_test FROM 's3://packt-serverless-ml-redshift/chapter06/Test.csv' IAM_ROLE DEFAULT FORMAT AS CSV DELIMITER ',' QUOTE '"' IGNOREHEADER 1 REGION AS 'eu-west-1';</pre>
			<p>Now that <a id="_idIndexMarker307"/>the data has loaded, let’s do some analysis of our <span class="No-Break">training data.</span></p>
			<ol>
				<li value="3">Analyze the <a id="_idIndexMarker308"/>training data by executing the following <span class="No-Break">SQL command:</span><pre class="source-code">
select segmentation, count(*)  from chapter6_supervisedclassification.cust_segmentation_train</pre><pre class="source-code">
group by 1;</pre></li>
			</ol>
			<p>You should get the <span class="No-Break">following output:</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B19071_06_15.jpg" alt="Figure 6.15 – Segmentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.15 – Segmentation</p>
			<p>Our training <a id="_idIndexMarker309"/>dataset has a total of 8,068 customer records. From this sample, we can see that segments <strong class="bold">C</strong>, <strong class="bold">B</strong>, and <strong class="bold">A</strong> are very similar and that more customers are in <span class="No-Break">segment </span><span class="No-Break"><strong class="bold">D</strong></span><span class="No-Break">.</span></p>
			<p>We will use the input from the training dataset to predict the customer segment, using the linear <span class="No-Break">learner algorithm.</span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor106"/>Using Linear Learner to predict the customer segment</h2>
			<p><strong class="bold">Linear learner</strong> is a <a id="_idIndexMarker310"/>supervised learning algorithm and one of the model types<a id="_idIndexMarker311"/> you can use to solve classification or <span class="No-Break">regression problems.</span></p>
			<p>For multi-class classification problems, we have more than two labels (or targets) that we will try to predict, compared to exactly two labels for binary classification problems. We will show you how to use linear learner to solve regression problems in <a href="B19071_07.xhtml#_idTextAnchor111"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
			<p>With <a id="_idIndexMarker312"/>linear learner, you can achieve a significant increase in speed compared to traditional hyperparameter optimization techniques, making it <span class="No-Break">very convenient.</span></p>
			<p>We will <a id="_idIndexMarker313"/>provide a training set with data that contains our input or observations about the data, and the label that represents the value we want to predict. We can optionally provide certain combinations of preprocessors to certain sets <span class="No-Break">of columns.</span></p>
			<p>In this section, you will apply user guidance techniques by providing <strong class="source-inline">MODEL_TYPE</strong>, <strong class="source-inline">PROBLEM_TYPE</strong>, and <strong class="source-inline">OBJECTIVE</strong> to create a multi-class classification model using the linear learner algorithm. The goal of this model is to predict the segment for <span class="No-Break">each customer.</span></p>
			<p>We will set <strong class="source-inline">MODEL_TYPE</strong> as <strong class="source-inline">LINEAR_LEARNER</strong> and <strong class="source-inline">PROBLEM_TYPE</strong> as <strong class="source-inline">MULTICLASS_CLASSIFICATION</strong>. We will leave other options <span class="No-Break">as default.</span></p>
			<p>Let us execute the following code in Query Editor v2 to train <span class="No-Break">the model:</span></p>
			<pre class="source-code">
CREATE  MODEL chapter6_supervisedclassification.cust_segmentation_model_ll
FROM (
SELECT
    id, gender, ever_married, age, graduated,profession,
    work_experience, spending_score,family_size,
    var_1,segmentation
FROM chapter6_supervisedclassification.cust_segmentation_train
)
TARGET segmentation
FUNCTION predict_cust_segment_ll   IAM_ROLE default
MODEL_TYPE LINEAR_LEARNER
PROBLEM_TYPE MULTICLASS_CLASSIFICATION
OBJECTIVE 'accuracy'
SETTINGS (
  S3_BUCKET '&lt;&lt;your-s3-bucket&gt;&gt;',
  S3_GARBAGE_COLLECT OFF,
  MAX_RUNTIME 9600
  );</pre>
			<p>To check<a id="_idIndexMarker314"/> the <a id="_idIndexMarker315"/>status of the model, run the following command in Query <span class="No-Break">Editor v2:</span></p>
			<pre class="source-code">
SHOW MODEL chapter6_supervisedclassification.cust_segmentation_model_ll;</pre>
			<p>You should get the <span class="No-Break">following output:</span></p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B19071_06_16.jpg" alt="Figure 6.16 – Showing the model output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.16 – Showing the model output</p>
			<p>You <a id="_idIndexMarker316"/>can see<a id="_idIndexMarker317"/> that the model is now in the <strong class="bold">READY</strong> state and that Redshift ML picks up <strong class="bold">Model Type</strong> and <strong class="bold">Problem Type</strong> parameter values from our <strong class="source-inline">CREATE </strong><span class="No-Break"><strong class="source-inline">MODEL</strong></span><span class="No-Break"> statement.</span></p>
			<p>Now that the model is trained, it is time to evaluate <span class="No-Break">its quality.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor107"/>Evaluating the model quality</h2>
			<p>When you<a id="_idIndexMarker318"/> issue the <strong class="source-inline">CREATE MODEL</strong> command, Amazon SageMaker will automatically divide your data into testing and training in the background so that it can determine the accuracy of the model. If you look at the <strong class="source-inline">validation:multiclass_accuracy</strong> key from the <strong class="source-inline">SHOW MODEL</strong> output, you will see a value of <strong class="bold">0.535028</strong>, which means our model can correctly pick the segment 53% of the time. Ideally, we prefer a value closer <span class="No-Break">to 1.</span></p>
			<p>We can also run a validation query to check our accuracy rates. In the following query, note that we select the actual segmentation, and then we use the function that was generated by our <strong class="source-inline">CREATE MODEL</strong> command to get the predicted segmentation<a id="_idIndexMarker319"/> to do <span class="No-Break">the comparison:</span></p>
			<pre class="source-code">
 select
 cast(sum(t1.match)as decimal(7,2)) as predicted_matches
,cast(sum(t1.nonmatch) as decimal(7,2)) as predicted_non_matches
,cast(sum(t1.match + t1.nonmatch) as decimal(7,2))  as total_predictions
,predicted_matches / total_predictions as pct_accuracy
from
(SELECT
    id,
    gender,
    ever_married,
    age,
    graduated,
    profession,
    work_experience,
    spending_score,
    family_size,
    var_1,
    segmentation as actual_segmentation,
    chapter6_supervisedclassification.predict_cust_segment_ll
(id,gender,ever_married,age,graduated,profession,work_experience,
spending_score,family_size,var_1) as predicted_segmentation,
    case when actual_segmentation = predicted_segmentation then 1
      else 0 end as match,
  case when actual_segmentation &lt;&gt; predicted_segmentation then 1
    else 0 end as nonmatch
    FROM chapter6_supervisedclassification.cust_segmentation_train
) t1;</pre>
			<p>We <a id="_idIndexMarker320"/>get the <span class="No-Break">following output:</span></p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B19071_06_17.jpg" alt="Figure 6.17 – The model accuracy"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.17 – The model accuracy</p>
			<p>This output shows that we are very close to the score of <strong class="bold">.535028</strong> when we compare the number of times the model correctly predicted the segment against the total number of <span class="No-Break">input records.</span></p>
			<p>Now that we have checked the model accuracy, we are ready to run prediction queries against the <span class="No-Break">test dataset.</span></p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor108"/>Running prediction queries</h2>
			<p>Now that we <a id="_idIndexMarker321"/>have our model and have done validation, we can run our prediction query against our test set so that we can segment our prospective customers, based on customer IDs. You can see that we now use our function against the test table to get the <span class="No-Break">predicted segment:</span></p>
			<pre class="source-code">
SELECT
id,
chapter6_supervisedclassification.predict_cust_segment_ll
(id,gender,ever_married,age,graduated,profession,work_experience,spending_score,family_size,var_1) as  segmentation
FROM chapter6_supervisedclassification.cust_segmentation_test;</pre>
			<p>The <a id="_idIndexMarker322"/>first 10 customers are <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B19071_06_18.jpg" alt="Figure 6.18 – The predicted segment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.18 – The predicted segment</p>
			<p>Let’s see how the new prospective customers are spread across the <span class="No-Break">various segments:</span></p>
			<pre class="source-code">
SELECT
    chapter6_supervisedclassification.predict_cust_segment_ll
    (id,gender,ever_married,age,graduated,profession,work_experience,spending_score,family_size,var_1) as  segmentation,
    count(*)
    FROM chapter6_supervisedclassification.cust_segmentation_test
   group by 1;</pre>
			<p>We can see <a id="_idIndexMarker323"/>here how many prospective customers are in <span class="No-Break">each segment:</span></p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B19071_06_19.jpg" alt="Figure 6.19 – The customer count by segment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.19 – The customer count by segment</p>
			<p>Now that you have this information, your marketing team is ready to target their efforts on these <span class="No-Break">prospective customers.</span></p>
			<p>Let’s now take a look at some other options you can use to solve this multi-class <span class="No-Break">classification problem.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor109"/>Exploring other CREATE MODEL options</h2>
			<p>We can also <a id="_idIndexMarker324"/>create this model in a couple of different ways, which we will explore in the following sections. It is important to understand the different options available so that you can experiment and choose the approach that gives you the <span class="No-Break">best model.</span></p>
			<p>In the first example, we will not provide any user guidance, such as specifying <strong class="source-inline">MODEL_TYPE</strong>, <strong class="source-inline">PROBLEM_TYPE</strong>, or <strong class="source-inline">OBJECTIVE</strong>. Use this approach if you are new to ML and want to let SageMaker Autopilot determine this <span class="No-Break">for you.</span></p>
			<p>Then, in <a id="_idIndexMarker325"/>the next example, you can see how you can provide <strong class="source-inline">PROBLEM_TYPE</strong> and <strong class="source-inline">OBJECTIVE</strong>. As a more experienced user of ML, you should now recognize which <strong class="source-inline">PROBLEM_TYPE</strong> and <strong class="source-inline">OBJECTIVE</strong> instances are best for your use case. When you provide these inputs, it will speed up the model training process, since SageMaker Autopilot will only train using the provided <span class="No-Break">user guidance.</span></p>
			<h3>Creating a model with no user guidance</h3>
			<p>In this <a id="_idIndexMarker326"/>approach, we let SageMaker Autopilot choose <strong class="source-inline">MODEL_TYPE</strong>, <strong class="source-inline">PROBLEM_TYPE</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">OBJECTIVE</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
CREATE MODEL chapter6_supervisedclassification.cust_segmentation_model
FROM (
SELECT
    id,
    gender,
    ever_married,
    age,
    graduated,
    profession,
    work_experience,
    spending_score,
    family_size,
    var_1,
    segmentation
FROM chapter6_supervisedclassification.cust_segmentation_train
)
TARGET segmentation
FUNCTION predict_cust_segment  IAM_ROLE default
SETTINGS (
  S3_BUCKET '&lt;&lt;your S3 Bucket&gt;&gt;',
  S3_GARBAGE_COLLECT OFF,
  MAX_RUNTIME 9600
);</pre>
			<p>Note that <a id="_idIndexMarker327"/>we have only provided the basic settings. We did not specify <strong class="source-inline">MODEL_TYPE</strong>, <strong class="source-inline">PROBLEM_TYPE</strong>, or <strong class="source-inline">OBJECTIVE</strong>. Amazon Redshift ML and SageMaker will automatically figure out that this is a multi-class classification problem and use the best model type. As an additional exercise, run this <strong class="source-inline">CREATE MODEL</strong> command, and then run the <strong class="source-inline">SHOW MODEL</strong> command. It will show you the <strong class="source-inline">MODEL_TYPE</strong> parameter that Amazon SageMaker used to train <span class="No-Break">the model.</span></p>
			<h3>Creating a model with some user guidance</h3>
			<p>In this example, we <a id="_idIndexMarker328"/>will provide <strong class="source-inline">PROBLEM_TYPE</strong> and <strong class="source-inline">OBJECTIVE</strong>, but we will let Amazon SageMaker <span class="No-Break">determine </span><span class="No-Break"><strong class="source-inline">MODEL_TYPE</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
CREATE MODEL chapter6_supervisedclassification.cust_segmentation_model_ug
FROM (
SELECT
    id,
    gender,
    ever_married,
    age,
    graduated,
    profession,
    work_experience,
    spending_score,
    family_size,
    var_1,
    segmentation
FROM chapter6_supervisedclassification.cust_segmentation_train
)
TARGET segmentation
FUNCTION predict_cust_segment_ug  IAM_ROLE default
PROBLEM_TYPE MULTICLASS_CLASSIFICATION
OBJECTIVE 'accuracy'
SETTINGS (
  S3_BUCKET '&lt;&lt;your S3 Bucket&gt;&gt;',
  S3_GARBAGE_COLLECT OFF,
  MAX_RUNTIME 9600
  );</pre>
			<p>In this <a id="_idIndexMarker329"/>example, we let Amazon Redshift ML and Amazon SageMaker determine <strong class="source-inline">MODEL_TYPE</strong>, and we pass in <strong class="source-inline">PROBLEM_TYPE</strong> and <strong class="source-inline">OBJECTIVE</strong>. When you have some free time, experiment with the different methods of creating the models, and note the differences you see in the time it takes to train the model, and <a id="_idIndexMarker330"/>also compare the accuracy and other outputs of the <strong class="source-inline">SHOW </strong><span class="No-Break"><strong class="source-inline">MODEL</strong></span><span class="No-Break"> command.</span></p>
			<p>You can also create multi-class classification models using XGBoost, which we will cover in <a href="B19071_10.xhtml#_idTextAnchor178"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor110"/>Summary</h1>
			<p>In this chapter, we discussed classification models in detail and looked at their common use cases. We also explained the <strong class="source-inline">CREATE MODEL</strong> syntax for classification models, where you provide guidance to train a model by supplying the model type <span class="No-Break">and objective.</span></p>
			<p>You learned how to do binary classification and multi-class classification with Amazon Redshift ML and how to use the XGBoost and linear learner algorithms. We also showed you how to check the status of your models, validate them for accuracy, and write SQL queries to run predictions on your <span class="No-Break">test dataset.</span></p>
			<p>In the next chapter, we will show you how to build regression models using Amazon <span class="No-Break">Redshift ML.</span></p>
		</div>
	</body></html>