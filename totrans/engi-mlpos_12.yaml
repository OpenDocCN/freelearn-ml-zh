- en: 'Chapter 10: Essentials of Production Release'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the **continuous integration** and **continuous
    delivery** (**CI/CD**) pipeline, the essentials of a production environment, and
    how to set up a production environment to serve your previously tested and approved
    **machine learning** (**ML**) models to end users. We will set up the required
    infrastructure for the CI/CD pipeline's production environment, configure processes
    for production deployments, configure pipeline execution triggers for complete
    automation, and learn how to manage production releases. This chapter will cover
    the essential fundamentals of the CI/CD pipeline and production environment since
    *the pipeline is the product, not the model*.
  prefs: []
  type: TYPE_NORMAL
- en: By learning about the fundamentals of CI/CD pipelines, you will be able to develop,
    test, and configure automated CI/CD pipelines for your use cases or business.
    We will cover an array of topics around production deployments and then delve
    into a primer on monitoring ML models in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the production infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up our production environment in the CI/CD pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing our production-ready pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring pipeline triggers for automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline release management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toward continuous monitoring the service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin by setting up the infrastructure that's required to build the CI/CD
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the production infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will set up the required infrastructure to serve our business
    use case (to predict weather conditions – raining or not raining at the port of
    Turku to plan and optimize resources at the port). We will set up an autoscaling
    Kubernetes cluster to deploy our ML model in the form of a web service. Kubernetes
    is an open source container orchestration system for automating software application
    deployment, scaling, and management. Many cloud service providers offer a Kubernetes-based
    infrastructure as a service. Similarly, Microsoft Azure provides a Kubernetes-based
    infrastructure as a service called **Azure Kubernetes Service** (**AKS**). We
    will use AKS to orchestrate our infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways to provision an autoscaling Kubernetes cluster on Azure.
    We will explore the following two ways to learn about the different perspectives
    of infrastructure provisioning:'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning workspace portal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure SDK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look into the easiest way first; that is, using the Azure Machine Learning
    workspace to provision an Azure Kubernetes cluster for production.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning workspace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will provision an Azure Kubernetes cluster using the Azure
    Machine Learning workspace. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Azure Machine Learning workspace and then go to the **Compute** section,
    which presents options for creating different types of computes. Select **Inference
    clusters** and click **Create**, as shown in the following screenshot:![Figure
    10.1 – Provisioning inference clusters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.1 – Provisioning inference clusters
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clicking the **Create** button will present various compute options you can
    use to create a Kubernetes service. You will be prompted to select a **Region**,
    which is where your compute will be provisioned, and some configuration so that
    you can provision in terms of cores, RAM, and storage. Select a suitable option
    (it is recommended that you select **Standard_D2_v4** as a cost-optimal choice
    for this experiment), as shown in the following screenshot:![Figure 10.2 – Selecting
    a suitable compute option
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.2 – Selecting a suitable compute option
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After selecting a suitable compute option, you will be prompted to `'prod-aks'`
    - meaning **production Azure Kubernetes Service**), set **Cluster purpose** to
    **Production** (as we are setting up for production), choose **Number of nodes**
    for the cluster, and select the **Basic** option for **Network configuration**.
    Omit **Enable SSL configuration** for simplicity. However, it is recommended to
    enable SSL connections for more security in production, as per your needs:![Figure
    10.3 - Configure Settings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.3 - Configure Settings
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click the **Create** button to provision the Kubernetes cluster for production.
    It will take around 15 minutes to create and provision the compute for production
    use:![Figure 10.4 – Provisioned Kubernetes cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.4 – Provisioned Kubernetes cluster
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once your AKS cluster has been provisioned, you will see a running Kubernetes
    cluster with the name you provided for the compute (for example, `prod-aks`),
    as shown in the preceding screenshot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Azure Machine Learning SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An alternative way of creating and provisioning a Kubernetes cluster on Azure
    is by using the Azure Machine Learning SDK. You can use a premade script named
    `create_aks_cluster.py`, which can be found in the `10_Production_Release` folder.
    The prerequisite to running the `create_aks_cluster.py` script is the `config.json`
    (it can be downloaded from the Azure Machine Learning workspace) file for your
    Azure Machine Learning workspace, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Fetching the config file from your Azure Machine Learning workspace'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_10_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Fetching the config file from your Azure Machine Learning workspace
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to your Azure Machine Learning workspace and click on `config.json` file
    into the same directory (`10_Production_Release`) that the `create_aks_cluster.py`
    file is in, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, you are now set to run the script (`create_aks_cluster.py`) to create
    AKS compute for production deployments. Let''s look at the `create_aks_cluster.py`
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary functions from the `azureml.core` SDK or library. Functions
    such as `Workspace`, `Model`, `ComputeTarget`, `AksCompute`, and so on will be
    used to provision your AKS cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By importing the necessary functions, you can start using them by connecting
    to your Azure Machine Learning workspace and creating the `ws` object. Do this
    by using the `Workspace` function and pointing it to your `config.json` file,
    like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By default, the `from_config()` function looks for the `config.json` file in
    same directory where you are executing the `create_aks.py` file. If your `config.json`
    file is in some other location, then point to the location of the file in the
    `from_config()` function. After successfully executing the `workspace.from_config()`
    function, you will see the workspace's name, resource group, and location printed
    out.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will create an AKS Kubernetes cluster for production deployments.
    Start by choose a name for your AKS cluster (reference it to the `aks_name` variable),
    such as `prod-aks`. The script will check if a cluster with the chosen name already
    exists. We can use the `try` statement to check whether the AKS target with the
    chosen name exists by using the `ComputeTarget()` function. It takes the `workspace`
    object and `aks_name` as parameters. If a cluster is found with the chosen name,
    it will print the cluster that was found and stop execution. Otherwise, a new
    cluster will be created using the `ComputeTarget.create()` function, which takes
    the `provisioning` config with the default configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After successfully executing the preceding code, a new cluster with the chosen
    name (that is, `prod-aks`) will be created. Usually, creating a new cluster takes
    around 15 minutes. Once the cluster has been created, it can be spotted in the
    Azure Machine Learning workspace, as we saw in *Figure 10.4*. Now that we have
    set up the prerequisites for enhancing the CI/CD pipeline for our production environment,
    let's start setting it up!
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our production environment in the CI/CD pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perform the following steps to set up a production environment in the CI/CD
    pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Azure DevOps project you worked on previously and revisit the **Pipelines**
    | **Releases** section to view your **Port Weather ML Pipeline**. We will enhance
    this pipeline by creating a production stage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Edit** button to get started and click on **Add** under the **DEV
    TEST** stage, as shown in the following screenshot:![Figure 10.6 – Adding a new
    stage
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.6 – Adding a new stage
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clicking the `production` or `PROD` and save it, as shown in the following screenshot:![Figure
    10.7 – Adding and saving the production stage (PROD)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.7 – Adding and saving the production stage (PROD)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A new production stage named **PROD** will be created. Now, you can configure
    jobs and processes at the production stage. To configure jobs for **PROD**, click
    on the 1 job, 0 task link (as shown in the preceding screenshot, in the **PROD**
    stage) in the **PROD** stage. You will be directed to the **Tasks** section, which
    is where you can add jobs to the **PROD** stage. In this stage, we will deploy
    models from our Azure Machine Learning workspace, so we will connect to it using
    the AzureML Model Deploy template we used previously in [*Chapter 7*](B16572_07_Final_JM_ePub.xhtml#_idTextAnchor143),
    *Building Robust CI and CD Pipelines*. Click on the **+** sign on the right-hand
    side of the **Agent job** section to add a task, as shown here:![Figure 10.8 –
    Adding an AzureML Model Deploy task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.8 – Adding an AzureML Model Deploy task
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Search for the `mlops_ws`) and point to **Model Artifact** as your model source.
    We are doing this because we will be using the Model Artifacts we trained in [*Chapter
    4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074), *Machine Learning Pipelines*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, point to your inference configuration file from the Azure DevOps repository,
    as shown in the following screenshot. The inference configuration file represents
    the configuration settings for a custom environment that's used for deployment.
    We will use the same inference `Config.yml` file that we used for the `inferenceConfig.yml`
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will configure `prod-aks`), name your deployment or web service (for
    example, `prod-webservice`), and select the **Deployment Configuration** file
    from the Azure DevOps repository that's connected to the pipeline, as shown here:![Figure
    10.10 – Configuring your deployment information
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Select the `AksDeploymentConfig.yml` file as our **Deployment Configuration**
    file. Now, hit the **Save** button to set up the **PROD** environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that, you have successfully set up the production environment and integrated
    it with your CI/CD pipeline for automation. Now, let's test the pipeline by executing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Testing our production-ready pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Congratulations on setting up the production pipeline! Next, we will test its
    robustness. One great way to do this is to create a new release and observe and
    study whether the production pipeline successfully deploys the model to production
    (in the production Kubernetes cluster setup containing the pipeline). Follow these
    steps to test the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: First, create a new release, go to the **Pipelines** | **Releases** section,
    select your previously created pipeline (for example, **Port Weather ML Pipeline**),
    and click on the **Create Release** button at the top right-hand side of the screen
    to initiate a new release, as shown here:![Figure 10.11 – Create a new release
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.11 – Create a new release
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the artifacts you would like to deploy in the pipeline (for example,
    `Learn_MLOps repo`, `_scaler`, and `support-vector-classifier` model and select
    their versions. Version 1 is recommended for testing PROD deployments for the
    first time), and click on the **Create** button at the top right-hand side of
    the screen, as shown in the preceding screenshot. Once you've done this, a new
    release is initiated, as shown in the following screenshot:![Figure 10.12 – New
    release’s execution
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.12 – New release's execution
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After executing the pipeline, both the **DEV TEST** and **PROD** stages will
    be deployed (for example, **Release-5**, as shown in the preceding screenshot).
    You can check each step in each stage by monitoring the logs of each step of any
    stage (DEV TEST or PROD) while the pipeline release is in progress, until the
    pipeline is deployed successfully. You can also check the logs of previous releases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon successfully working on a release, both the **DEV TEST** and **PROD** stages
    will be deployed using CI and CD. You must ensure that the pipeline is robust.
    Next, we can customize the pipeline further by adding custom triggers that will
    automate the pipeline without any human supervision. Automating CI/CD pipelines
    without any human supervision can be risky but may have advantages, such as real-time
    continuous learning (monitoring and retraining models) and faster deployments.
    It is good to know how to automate the CI/CD pipeline without any human supervision
    in the loop. Note that it is not recommended in many cases as there is a lot of
    room for error. In some cases, it may be useful – it really depends on your use
    case and ML system goals. Now, let's look at triggers for full automation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuring pipeline triggers for automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will configure three triggers based on artifacts that we
    have already connected to the pipeline. The triggers we will set up are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Git trigger**: For making code changes to the master branch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifactory trigger**: For when a new model or artifact is created or trained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schedule trigger**: A weekly periodic trigger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at each of these pipeline triggers in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Git trigger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In teams, it is common to set a trigger for deployment when code changes are
    made to a certain branch in the repository. For example, when code changes are
    made to the **master** branch or the **develop** branch, CI/CD pipelines are triggered
    to deploy the application to the PROD or DEV TEST environments, respectively.
    When a pull request is made to merge code in the **master** or **develop** branch,
    the QA expert or product manager accepts the pull request in order to merge with
    the respective branch. Upon making code changes to the master or develop branch,
    a trigger is generated to create a new release in the pipeline. Follow these steps
    to create a trigger for the master branch for the experiment, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the **Pipelines** | **Releases** section and select your pipeline (for
    example, **Port Weather ML pipeline**). Then, click **Edit**:![Figure 10.13 –
    Setting up continuous deployment triggers (git triggers)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.13 – Setting up continuous deployment triggers (git triggers)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will be directed to a portal where you can edit your pipeline (for example,
    **Port Weather ML pipeline**) so that you can configure continuous deployment
    triggers for your artifacts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To set up a Git trigger for the master branch (when changes are made to the
    master branch, a new release is triggered), click on the **Trigger** icon (thunder
    icon) and move the on/off switch button from disabled to enabled. This will enable
    the continuous deployment trigger.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, add a branch filter and point to the branch that you would like to set
    up a trigger for – in this case, the master branch – as shown in the preceding
    screenshot. Save your changes to set up the Git trigger.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By implementing these steps, you have set up a continuous deployment trigger
    to initiate a new release when changes are made to the master branch.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an Artifactory trigger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For ML applications, Artifactory triggers are quite useful. When new models
    or artifacts (files) have been trained by the Data Scientists in the team, it
    is useful to deploy those models to a test environment, and then eventually to
    production if they are promising or better than the previous models or trigger.
    Follow these steps to set up a continuous deployment trigger that will create
    a new release for the pipeline when a new model is trained, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the **Pipelines** | **Releases** section and select your pipeline (for
    example, **Port Weather ML pipeline**). Then, click **Edit**:![Figure 10.14 –
    Setting up CD for Artifact triggers (SVC model)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.14 – Setting up CD for Artifact triggers (SVC model)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upon clicking the **Edit** button, you will be directed to a portal where you
    can edit your pipeline, as shown in the preceding screenshot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To set up an Artifact trigger for your model, click on your choice of model,
    such as **Support Vector Classifier** (**SVC**), and enable **Continuous deployment
    trigger**. In the preceding screenshot , a trigger has been enabled for a model
    (SVC). Whenever a new SVC model is trained and registered to the model registry
    that's connected to your Azure Machine Learning workspace, a new release will
    be triggered to deploy the new model via the pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, save your changes to set up an Artifact trigger for the SVC model. You
    have a continuous deployment trigger set up to initiate a new release when a new
    SVC model is trained and registered on your Azure Machine Learning workspace.
    The pipeline will fetch the new model and deploy it to the DEV TEST and PROD environments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By implementing these steps, you have a continuous deployment trigger set up
    to initiate a new pipeline release when a new artifact is created or registered
    in your Azure Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Schedule trigger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will set up a time-specific Schedule trigger for the pipeline. This
    kind of trigger is useful for keeping the system healthy and updated via periodic
    new releases. Schedule triggers create new releases at set time intervals. We
    will set up a Schedule trigger for every week on Monday at 11:00 A.M. At this
    time, a new release is triggered to deploy the recent version of the SVC model
    to both the DEV TEST and PROD environments. Follow these steps to set up a Schedule
    trigger:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the **Pipelines** | **Releases** section and select your pipeline (for
    example, **Port Weather ML pipeline**). Then, click **Edit**, as shown in the
    following screenshot:![Figure 10.15 – Setting up a schedule trigger
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_10_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 10.15 – Setting up a schedule trigger
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upon clicking the **Edit** button, you will be directed to a portal where you
    can edit your pipeline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To set up a scheduled trigger for the pipeline, click on **Schedule Set** and
    enable **Scheduled release trigger**. Then, select times when you want to trigger
    a release. For example, in the preceding screenshot, a trigger has been enabled
    for every week on Monday at 11:00 A.M.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, save your changes to set up a set up Schedule trigger trigger for the
    pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By implementing these steps, you have a continuous deployment trigger set up
    to initiate a new pipeline release at a set time interval.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on setting up Git, Artifact, and Schedule triggers. These triggers
    enable full automation for the pipeline. The pipeline has been set up and can
    now successfully test and deploy models. You also have the option to semi-automate
    the pipeline by adding a human or **Quality Assurance** (**QA**) expert to approve
    each stage in the pipeline. For example, after the test stage, an approval can
    be made by the QA expert so that you can start production deployment if everything
    was successful in the test stage. As a QA expert, it is vital to monitor your
    CI/CD pipeline. In the next section, we'll look at some best practices when it
    comes to managing pipeline releases.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline release management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Releases in the CI/CD pipelines allow your team to automate fully and continuously
    deliver software to your customers faster and with lower risk. Releases allow
    you to test and deliver your software in multiple stages of production or set
    up semi-automated processes with approvals and on-demand deployments. It is vital
    to monitor and manage these releases. We can manage releases by accessing the
    pipeline from **Pipelines** | **Releases** and selecting our CI/CD pipeline (for
    example, **Port Weather ML Pipeline**), as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Pipeline Release Management'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_10_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.16 – Pipeline Release Management
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can keep track of all the releases and their history and perform
    operations for each release, such as redeploying, abandoning, checking logs, and
    so on. You can see the releases shown in the following screenshot. By clicking
    on individual releases (for example, **Release 4**), we can check which model
    and artifacts were deployed in the release and how the release was triggered (manual
    or using automatic triggers). It provides end-to-end traceability of the pipeline.
    This information is crucial for the governance and compliance of the ML system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17 – Inspecting a release'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_10_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.17 – Inspecting a release
  prefs: []
  type: TYPE_NORMAL
- en: 'Prevention is better than finding a cure. Just as we conduct incident reviews
    after a failure, it helps to prevent possible failures by conducting post-release
    reviews after deploying a new service or model. A thorough analysis of the release
    after deployment can enable us to understand answers to critical questions, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What works and what doesn't during a release?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Were there any roadblocks with the release?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any unclear processes that you could solve and make more explainable
    for the next release?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thoroughly understanding these questions post-release can help you improve and
    iterate on your strategy and develop better release management practices.
  prefs: []
  type: TYPE_NORMAL
- en: Toward continuous monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With that, we have set up a fully automated and robust pipeline. So far, we
    have successfully implemented the deployment part or module in the MLOps workflow
    (as we discussed in [*Chapter 1*](B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015),
    *Fundamentals of MLOps Workflow*). It is vital to monitor the deployed ML model
    and service in real time to understand the system's performance, as this helps
    maximize its business impact. One of the reasons ML projects are failing to bring
    value to businesses is because of the lack of trust and transparency in their
    decision making. Building trust into AI systems is vital these days, especially
    if we wish to adapt to the changing environment, regulatory frameworks, and dynamic
    customer needs. Continuous monitoring will enable us to monitor the ML system's
    performance and build trust into AIs to maximize our business value. In the next
    chapter, we will learn about the monitoring module in the MLOps workflow and how
    it facilitates continuous monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the essential fundamentals of the CI/CD pipeline
    and production environment. We did some hands-on implementation to set up the
    production infrastructure and then set up processes in the production environment
    of the pipeline for production deployments. We tested the production-ready pipeline
    to test its robustness. To take things to the next level, we fully automated the
    CI/CD pipeline using various triggers. Lastly, we looked at release management
    practices and capabilities and discussed the need to continuous monitor the ML
    system. A key takeaway is that *the pipeline is the product, not the model*. It
    is better to focus on building a robust and efficient pipeline more than building
    the best model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the MLOps workflow monitoring module and
    learn more about the game-changing explainable monitoring framework.
  prefs: []
  type: TYPE_NORMAL
