<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer036">
<h1 class="chapter-number" id="_idParaDest-66"><a id="_idTextAnchor067"/>4</h1>
<h1 id="_idParaDest-67"><a id="_idTextAnchor068"/>Comparing LightGBM, XGBoost, and Deep Learning</h1>
<p>The previous chapter introduced LightGBM for building <strong class="bold">gradient-boosted decision trees</strong> (<strong class="bold">GBDTs</strong>). In this chapter, we compare LightGBM against two other methods for modeling tabular data: XGBoost, another library for building gradient-boosted trees, and <strong class="bold">deep neural networks</strong> (<strong class="bold">DNNs</strong>), a state-of-the-art machine <span class="No-Break">learning technique.</span></p>
<p>We compare LightGBM, XGBoost, and DNNs on two datasets, focusing on complexity, dataset preparation, model performance, and <span class="No-Break">training time.</span></p>
<p>This chapter is aimed at advanced readers, and some understanding of deep learning is required. However, the primary purpose of the chapter is not to understand XGBoost or DNNs in detail (neither technique is used in subsequent chapters). Instead, by the end of the chapter, you should have some understanding of how competitive LightGBM is within the <span class="No-Break">machine-learning landscape.</span></p>
<p>The main topics are <span class="No-Break">as follows:</span></p>
<ul>
<li>An overview <span class="No-Break">of XGBoost</span></li>
<li>Deep learning <span class="No-Break">and TabTransformers</span></li>
<li>Comparing LightGBM, XGBoost, <span class="No-Break">and TabTransformers</span></li>
</ul>
<h1 id="_idParaDest-68"><a id="_idTextAnchor069"/>Technical requirements</h1>
<p>The chapter includes examples and code excerpts illustrating how to train LightGBM, XGBoost, and TabTransformer models in Python. Complete examples and instructions for setting up a suitable environment for this chapter are available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-4"><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-4</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-69"><a id="_idTextAnchor070"/>An overview of XGBoost</h1>
<p><strong class="bold">XGBoost</strong>, short for <a id="_idTextAnchor071"/><strong class="bold">eXtreme Gradient Boosting</strong>, is a widely popular open source gradient boosting<a id="_idIndexMarker213"/> library with similar goals and functionality to LightGBM. XGBoost is older than LightGBM and was developed by Tianqi Chen and initially released in <span class="No-Break">2014 </span><span class="No-Break"><em class="italic">[1]</em></span><span class="No-Break">.</span></p>
<p>At its core, XGBoost implements GBDTs and supports building them highly efficiently. Some of the main <a id="_idIndexMarker214"/>features of XGBoost are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Regularization</strong>: XGBoost incorporates both L1 and L2 regularization to <span class="No-Break">avoid overfitting</span></li>
<li><strong class="bold">Sparsity awareness</strong>: XGBoost efficiently handles sparse data and missing values, automatically learning the best imputation strategy <span class="No-Break">during training</span></li>
<li><strong class="bold">Parallelization</strong>: The library employs parallel and distributed computing techniques to train multiple trees simultaneously, significantly reducing <span class="No-Break">training time</span></li>
<li><strong class="bold">Early stopping</strong>: XGBoost provides an option to halt the training process if there is no significant improvement in the model’s performance, improving performance and <span class="No-Break">preventing overfitting</span></li>
<li><strong class="bold">Cross-platform compatibility</strong>: XGBoost is available for many programming languages, including Python, R, Java, and Scala, making it accessible to a diverse <span class="No-Break">user base</span></li>
</ul>
<p>Over the years, XGBoost has gained popularity in the machine learning community due to its support for various applications and the library’s ease of use <span class="No-Break">and efficiency.</span></p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor072"/>Comparing XGBoost and LightGBM</h2>
<p>There is <a id="_idIndexMarker215"/>considerable overlap in functionality between XGBoost and LightGBM. Both libraries implement GBDTs and DART and support building random forests. Both<a id="_idIndexMarker216"/> have similar techniques to avoid overfitting and handle missing values and sparse <span class="No-Break">data automatically.</span></p>
<p>However, some of the differences between XGBoost and LightGBM are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Tree-growing strategy</strong>: XGBoost employs a level-wise tree growth approach, where trees are built level by level, while LightGBM uses a leaf-wise tree growth strategy that focuses on growing the tree by choosing the leaf with the highest delta loss. This difference in growth strategy generally makes LightGBM faster, especially for <span class="No-Break">large datasets.</span></li>
<li><strong class="bold">Speed and scalability</strong>: LightGBM is designed to be more efficient regarding memory usage and computation time, making it a better choice for large-scale datasets or when training time is critical. However, this speed advantage can sometimes come at the cost of higher variance in <span class="No-Break">model predictions.</span></li>
<li><strong class="bold">Handling categorical features</strong>: LightGBM has built-in support for categorical features, meaning it can handle them directly without needing one-hot encoding or other preprocessing techniques. XGBoost, on the other hand, requires the user to preprocess categorical features before feeding them into <span class="No-Break">the model.</span></li>
<li><strong class="bold">Early stopping</strong>: XGBoost<a id="_idIndexMarker217"/> provides an option to halt the training process if there is no significant improvement in the model’s performance. LightGBM does not have this feature built in, although it can be implemented manually using callbacks, as seen in <span class="No-Break">earlier chapters.</span></li>
</ul>
<p>In summary, LightGBM <a id="_idIndexMarker218"/>and XGBoost provide similar functionality. LightGBM performs better on large datasets with many features, whereas XGBoost may provide more stable and accurate results on smaller or <span class="No-Break">medium-sized datasets.</span></p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor073"/>Python XGBoost example</h2>
<p>XGBoost <a id="_idIndexMarker219"/>provides a scikit-learn-based interface<a id="_idIndexMarker220"/> for building models. The following example shows how to use XGBoost on the Forest <span class="No-Break">Cover dataset:</span></p>
<pre class="source-code">
from xgboost import XGBClassifier
...
dataset = datasets.fetch_covtype()
X_train, X_test, y_train, y_test = train_test_split(
    dataset.data, dataset.target, random_state=179
    )
y_train = y_train - 1
y_test = y_test – 1
xgb = XGBClassifier(
    n_estimators=150, max_leaves=120, learning_rate=0.09
    )
xgb = xgb.fit(X_train, y_train)
f1_score(y_test, xgb.predict(X_test), average="macro")</pre>
<p>The<a id="_idIndexMarker221"/> scikit-learn<a id="_idIndexMarker222"/> interface should be familiar to you at this stage. The preceding code shows that XGBoost supports similar hyperparameters as we used to train LightGBM-based models. A full<a id="_idIndexMarker223"/> list of parameters is av<a href="https://xgboost.readthedocs.io/en/stable/parameter.xhtml">ailable <span class="No-Break">at </span><span class="No-Break">https://xgboost.readthedocs.io/en/stable/par</span></a><span class="No-Break">ameter.xhtml</span><span class="No-Break">.</span></p>
<p>XGBoost represents a direct alternative to LightGBM as another gradient-boosting library. In the next section, we look at deep learning, a wholly different but extremely popular learning technique, and how it compares to gradient boosting on tabular <span class="No-Break">learning problems.</span></p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor074"/>Deep learning and TabTransformers</h1>
<p>We now look at an approach to solving tabular-based data problems using deep learning. Deep learning has gained immense popularity in recent years due to the performance of deep-learning-based models. Deep-learning-based techniques such as AlphaZero, Stable Diffusion, and the GPT series of language models have achieved human or superhuman performance in gameplay, art generation, and <span class="No-Break">language-based reasoning.</span></p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor075"/>What is deep learning?</h2>
<p><strong class="bold">Deep learning</strong> is a<a id="_idIndexMarker224"/> subfield of the broader machine learning field of artificial neural networks. Artificial neural networks are mathematical mimics of the human brain and consist of interconnected layers of nodes (or “neurons” in biological parlance) that process and <span class="No-Break">transmit information.</span></p>
<p>Simple artificial neural networks <a id="_idIndexMarker225"/>consist of only a few layers. The term “deep” in deep learning refers to using neural networks of many more layers, each with potentially thousands of neurons. These layers are organized hierarchically, with <em class="italic">input</em> layers at the bottom, <em class="italic">output</em> at the top, and <em class="italic">hidden</em> between. Each layer extracts and refines features as data passes through the network, allowing the model to learn complex patterns <span class="No-Break">and representations.</span></p>
<p>The following diagram<a id="_idIndexMarker226"/> depicts a simple neural network <a id="_idIndexMarker227"/>called a multilayer perceptron with a single <span class="No-Break">hidden layer.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<img alt="Figure 4.1 – A multilayer perceptron with a single hidden layer and an output layer. The layers are fully connected" height="1001" src="image/B16690_04_01.jpg" width="1124"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – A multilayer perceptron with a single hidden layer and an output layer. The layers are fully connected</p>
<p>Each neuron receives input from other neurons, performs a mathematical operation, and then passes the result to the next layer <span class="No-Break">of neurons.</span></p>
<p>The <a id="_idIndexMarker228"/>mathematical operation involves two main steps – weighted sum and <span class="No-Break">activation function:</span></p>
<ol>
<li><strong class="bold">Weighted sum</strong>: The <a id="_idIndexMarker229"/>neuron takes the inputs (input data or outputs from previous neurons), multiplies each input by its corresponding weight, and then adds them together. A bias term is often added to the weighted sum for better control over the neuron’s output. Mathematically, this can be represented <span class="No-Break">as follows:</span><p class="list-inset"><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ij</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">b</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span></p><p class="list-inset">Here, <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> represents all inputs to the neuron, <span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ij</span> is the weight associated with the <span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">th</span> input, and <span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span> is the bias for <span class="No-Break">the neuron.</span></p></li>
<li><strong class="bold">Activation function</strong>: The <a id="_idIndexMarker230"/>weighted sum is then passed through an activation function, determining the neuron’s output. The purpose of the activation function is to introduce non-linearity into the mathematical operation. The non-linearity allows the neural network to model non-linear and, therefore, complex relationships<a id="_idIndexMarker231"/> between inputs and outputs. There are <a id="_idIndexMarker232"/>various activation functions, such as <strong class="bold">sigmoid</strong> (logistic function), <strong class="bold">hyperbolic tangent</strong> (<strong class="bold">tanh</strong>), and <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>), each <a id="_idIndexMarker233"/>with its own properties and use cases. This can be <span class="No-Break">represented as:</span><p class="list-inset"><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">z</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p><p class="list-inset">where <span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span> is the neuron output and <span class="_-----MathTools-_Math_Variable">σ</span> is the <span class="No-Break">activation function.</span></p></li>
</ol>
<p>Combining these two steps, a neuron in a neural network processes the input data, allowing the network to learn and model <span class="No-Break">complex patterns.</span></p>
<p>Neural networks <a id="_idIndexMarker234"/>are trained by adjusting the weights associated with the neurons. The algorithm can be summarized <span class="No-Break">as follows:</span></p>
<ol>
<li>Weights are initialized to small, <span class="No-Break">random values.</span></li>
<li>A <strong class="bold">forward pass</strong> is <a id="_idIndexMarker235"/>performed: for each example in a batch, the input features are passed through the entire network (calculating the sum and activation at each neuron) to produce a prediction at the <span class="No-Break">output layer.</span></li>
<li>The loss is then calculated by comparing the output against the actual output for each example in a batch. Like GBDTs, the loss function has to be differentiable, and standard loss functions include the MSE and <span class="No-Break">cross-entropy loss.</span></li>
<li><strong class="bold">Backpropagation</strong> is <a id="_idIndexMarker236"/>performed: the gradient of the loss function with respect to the weights is calculated using the calculus chain rule. The process is started at the output layer and works backward through <span class="No-Break">the network.</span></li>
<li>The <a id="_idIndexMarker237"/>weights are then updated using gradient descent or one of the modern variants, such as Adam, based on the <span class="No-Break">backpropagated gradients.</span></li>
<li>The process is repeated for a set number of epochs (each epoch running through the entire dataset) to minimize the <span class="No-Break">loss function.</span></li>
</ol>
<p>A unique property of neural networks is<a id="_idIndexMarker238"/> that neural networks have been proven to be <strong class="bold">universal function approximators</strong>. DNNs have the theoretical capability to approximate any continuous function to a desired level of accuracy, given a sufficient number of hidden neurons and an appropriate activation function. This property is based on the <strong class="bold">Universal Approximation Theorem</strong>, which <a id="_idIndexMarker239"/>has been proven for various types of <span class="No-Break">neural networks.</span></p>
<p>This means that a <a id="_idIndexMarker240"/>neural network can learn to represent complex relationships between input and output data, no matter how intricate or non-linear these relationships might be. This capability is one of the reasons why neural networks, especially DNNs, have successfully solved a wide range of problems across different domains. However, this guarantee is theoretical. In practice, finding the correct network architecture, hyperparameters, and training techniques to achieve the desired level of approximation can be challenging. The process often requires experimentation, expertise, and prohibitive <span class="No-Break">computational resources.</span></p>
<h3>Advantages and disadvantages of deep learning</h3>
<p>Given the capabilities of DNNs, we might believe that they should be our first port of call for all machine learning problems. The primary advantage of using DNNs is their high accuracy in very complex domains: the current state-of-the-art performance on a wide range of complex tasks, natural language processing, generative AI, image recognition, and speech recognition are all achieved by DNNs due to their ability to learn complex and hidden patterns in <span class="No-Break">large datasets.</span></p>
<p>Another advantage is <a id="_idIndexMarker241"/>automatic feature extraction. With the correct architecture, a DNN can extract complex or higher-order features automatically, alleviating the need for a data scientist to perform <span class="No-Break">feature engineering.</span></p>
<p>Finally, DNNs can also transfer learning: pre-trained deep learning models can be fine-tuned on a smaller dataset for a specific task, leveraging the knowledge acquired during the initial training. Transfer learning can significantly reduce training time and data requirements for <span class="No-Break">new tasks.</span></p>
<p>However, deep learning is not a panacea to all machine learning problems. Some of the disadvantages of<a id="_idIndexMarker242"/> using DNNs include <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Computational resources</strong>: Deep learning models often require significant computing power and memory for training, especially when dealing with large datasets and <span class="No-Break">complex architectures.</span></li>
<li><strong class="bold">Large datasets</strong>: DNNs usually perform well when trained on large datasets, but their performance can degrade when trained on smaller datasets. When a dataset is too small, the DNN overfits the training data and cannot generalize to <span class="No-Break">unseen data.</span></li>
<li><strong class="bold">Interpretability</strong>: DNNs are often considered “black boxes” due to their complex architectures and the large number of parameters involved. The complexity can make understanding how the model makes decisions difficult, which may be a concern for applications requiring transparency or <span class="No-Break">regulatory compliance.</span></li>
<li><strong class="bold">Hyperparameter tuning</strong>: DNNs involve numerous hyperparameters, such as network architecture, learning rate, and activation functions. Coupled with longer training<a id="_idIndexMarker243"/> times and resource needs, finding the optimal combination of these hyperparameters can be expensive <span class="No-Break">and time-consuming.</span></li>
</ul>
<h2 id="_idParaDest-74"><a id="_idTextAnchor076"/>Introducing TabTransformers</h2>
<p>We want to apply<a id="_idIndexMarker244"/> deep learning to tabular data, as most practical machine learning problems have tabular data. To this end, we use a new deep learning architecture called <strong class="bold">TabTransformer</strong>: a deep neural network model designed to handle tabular <span class="No-Break">data specifically.</span></p>
<p>Like the GPT family of DNNs, TabTransformer is based on the transformer architecture originally introduced by Vaswani et al. <em class="italic">[2]</em>. TabTransformer adapts the transformer architecture to work effectively with tabular data, providing an alternative to other machine learning models such as decision trees and gradient boosting machines for such <span class="No-Break">data </span><span class="No-Break"><em class="italic">[3]</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<img alt="Figure 4.2 – The TabTransformer architecture as implemented in Keras [3]" height="1582" src="image/B16690_04_02.jpg" width="802"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – The TabTransformer architecture as implemented in Keras [3]</p>
<p>The model architecture<a id="_idIndexMarker245"/> for TabTransformer is shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em>. With TabTransformer, each feature in the tabular data is treated as a token, similar to how words are treated as tokens in natural language processing. The model applies self-attention mechanisms to learn complex interactions and dependencies between features in the input data. The token embedding and attention mechanism allows the model to capture global and local relationships <span class="No-Break">between features.</span></p>
<p>The TabTransformer model has several key components: token embedding, positional encoding, transformer layers, pooling, and output layers. The token embedding converts each feature value into a continuous vector representation, combined with positional information through <span class="No-Break">positional encoding.</span></p>
<p>Moving through the layers, shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em>, we can see that the categorical and numerical features <span class="No-Break">are split.</span></p>
<p>The categorical features first go through an embedding layer, as implemented by <strong class="source-inline">layers.Embedding</strong> in Keras, and are then passed along to the transformer blocks. A variable number of transformer blocks can be implemented (as set using a hyperparameter), but each consists of a <strong class="source-inline">layers.MultiHeadAttention</strong> layer and a <strong class="source-inline">layers.Dense</strong> layer with <strong class="source-inline">Dropout</strong>. The output values are added and normalized after passing through the attention and <span class="No-Break">dense layers.</span></p>
<p>Due to their performance, the transformer feedforward layer uses the <strong class="bold">Gaussian error linear unit</strong> (<strong class="bold">GELU</strong>) activation<a id="_idIndexMarker246"/> function in our implementation. However, other activation functions may be <span class="No-Break">used </span><span class="No-Break"><em class="italic">[5]</em></span><span class="No-Break">.</span></p>
<p>The numerical features are passed through a normalization layer (normalizing the numerical input ranges) and then concatenated with output from <span class="No-Break">the transformers.</span></p>
<p>The concatenated results are passed <a id="_idIndexMarker247"/>through a <strong class="bold">multilayer perceptron</strong> (<strong class="bold">MLP</strong>) block consisting of a variable number of dense layers, each with <strong class="source-inline">Dropout</strong>. Our implementation uses <strong class="bold">scaled exponential linear unit</strong> (<strong class="bold">SELU</strong>), which <a id="_idIndexMarker248"/>causes the activations to <span class="No-Break">self-normalize </span><span class="No-Break"><em class="italic">[6]</em></span><span class="No-Break">.</span></p>
<p>Finally, the output from the MLP block is passed to the loss function, the implementation of which depends on the learning problem (classification <span class="No-Break">or regression).</span></p>
<p>TabTransformers are much more complex to implement and train than gradient-boosted trees. Like other DNNs, TabTransformers require more data preparation and computational power than <span class="No-Break">gradient-boosted trees.</span></p>
<p>In addition to TabTransformers, this section introduced deep learning alongside its advantages and disadvantages. In the next section, we use a practical example to compare the different approaches, including the complexity of working <span class="No-Break">with TabTransformers.</span></p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor077"/>Comparing LightGBM, XGBoost, and TabTransformers</h1>
<p>In this<a id="_idIndexMarker249"/> section, we <a id="_idIndexMarker250"/>compare <a id="_idIndexMarker251"/>the performance of LightGBM, XGBoost, and TabTransformers on two different datasets. We<a id="_idIndexMarker252"/> also look at more data<a id="_idIndexMarker253"/> preparation techniques for unbalanced classes, missing <a id="_idIndexMarker254"/>values, and <span class="No-Break">categorical data.</span></p>
<h2 id="_idParaDest-76"><a id="_idTextAnchor078"/>Predicting census income</h2>
<p>The first <a id="_idIndexMarker255"/>dataset we use is the Census Income dataset, which predicts whether personal income will exceed $50,000 based on attributes such as education, marital status, occupation, and others <em class="italic">[4]</em>. The dataset has 48,842 instances, and as we’ll see, some missing values and <span class="No-Break">unbalanced classes.</span></p>
<p>The dataset is available from the following URL: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data">https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data</a>. The data has already been split into a training set and a test set. Once loaded, we can sample <span class="No-Break">the data:</span></p>
<pre class="source-code">
train_data.sample(5)[["age", "education", "marital_status", "hours_per_week", "income_bracket"]]</pre>
<p>The data sample for the selected columns is shown in <span class="No-Break"><em class="italic">Table 4.1</em></span><span class="No-Break">.</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">age</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">education</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">marital_status</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">hours_per_week</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">income_bracket</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">12390</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">34</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Some-college</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Never-married</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">40</span></p>
</td>
<td class="No-Table-Style">
<p>&lt;=<span class="No-Break">50K</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">20169</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">41</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Assoc-acdm</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Married-civ-spouse</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">45</span></p>
</td>
<td class="No-Table-Style">
<p>&gt;<span class="No-Break">50K</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">17134</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">35</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Doctorate</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Never-married</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">60</span></p>
</td>
<td class="No-Table-Style">
<p>&gt;<span class="No-Break">50K</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">23452</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">49</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">HS-grad</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Married-civ-spouse</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">40</span></p>
</td>
<td class="No-Table-Style">
<p>&gt;<span class="No-Break">50K</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">22372</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">31</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">HS-grad</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Separated</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">45</span></p>
</td>
<td class="No-Table-Style">
<p>&lt;=<span class="No-Break">50K</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1 – Sample data from the Census Income dataset</p>
<p><em class="italic">Table 4.1</em> shows<a id="_idIndexMarker256"/> that we have mixed data types: some features are numeric, and others are text. Notably, some columns in the dataset are <strong class="bold">categorical features</strong>: string-based features with a fixed set of values. Next, we look at encoding these features for use in <span class="No-Break">machine learning.</span></p>
<h3>Encoding categorical features</h3>
<p>Most <a id="_idIndexMarker257"/>machine learning algorithms need string-based features to be encoded to numbers; in some cases, this can be done automatically. We discuss automatic encoding for LightGBM in <a href="B16690_06.xhtml#_idTextAnchor094"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Solving Real-World Data Science Problems with LightGBM</em>. In this example, we encode the features to understand what <span class="No-Break">this entails.</span></p>
<p>We need to map each categorical value to a unique number; therefore, we first build a vocabulary of all values for <span class="No-Break">each feature:</span></p>
<pre class="source-code">
CATEGORICAL_FEATURES_WITH_VOCABULARY = {
    "workclass": sort_none_last(list(train_data["workclass"].unique())),
    "education": sort_none_last(list(train_data["education"].unique())),
    "marital_status": sort_none_last(list(train_data["marital_status"].unique())),
    "occupation": sort_none_last(list(train_data["occupation"].unique())),
    "relationship": sort_none_last(list(train_data["relationship"].unique())),
    "race": sort_none_last(list(train_data["race"].unique())),
    "gender": sort_none_last(list(train_data["gender"].unique())),
    "native_country": sort_none_last(list(train_data["native_country"].unique())),
    "income_bracket": sort_none_last(list(train_data["income_bracket"].unique())),
}</pre>
<p>The <a id="_idIndexMarker258"/>preceding code extracts unique values for each column into a list and sorts the list, putting <strong class="source-inline">null</strong> values last. When working with pandas DataFrames, it’s also useful to explicitly set the data type for categorical columns <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">category</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
for c in CATEGORICAL_FEATURES_WITH_VOCABULARY.keys():
    for dataset in [train_data, test_data]:
        dataset[c] = dataset[c].astype('category')
        dataset[c] = dataset[c].astype('category')</pre>
<p>Using our vocabulary, we can now update the values in each column to numbers representing their category (using the index in the vocabulary list as the <span class="No-Break">numeric value):</span></p>
<pre class="source-code">
def map_to_index(val, vocab):
    if val is None:
        return None
    return vocab.index(val)
for dataset in (train_data, test_data):
    for feature, vocab in CATEGORICAL_FEATURES_WITH_VOCABULARY.items():
        dataset[feature] = dataset[feature].map(lambda val: map_to_index(val, vocab))</pre>
<p>The <a id="_idIndexMarker259"/>result is a DataFrame where all features are <span class="No-Break">now numeric:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">age</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">education</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">marital_status</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">hours_per_week</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">income_bracket</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">18545</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">37</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">11</span></p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">40</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">26110</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">51</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">14</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">60</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">21905</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">36</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">11</span></p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">32</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">1496</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">32</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">43</span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">3148</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">47</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">15</span></p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">40</span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.2 – Encoded categorical data from the Census Income dataset</p>
<p>Our categorical features are now encoded, and we can proceed with further <span class="No-Break">data cleaning.</span></p>
<h3>Missing values and duplicates</h3>
<p>We need to<a id="_idIndexMarker260"/> check for missing values, duplicates, and outliers. We<a id="_idIndexMarker261"/> can use the <span class="No-Break">following code:</span></p>
<pre class="source-code">
train_data.isnull().sum()
train_data.drop_duplicates(inplace=True)
train_data.describe()</pre>
<p>We drop the duplicate data, and reviewing the output of <strong class="source-inline">describe</strong> shows us there are no significant outliers. However, there are <strong class="bold">missing values</strong> in the dataset. LightGBM and XGBoost can deal with missing values automatically, a significant advantage of tree-based algorithms. However, for TabTransformers, we need to implement particular logic to deal with the missing values, as we’ll <span class="No-Break">see next.</span></p>
<h3>Unbalanced data</h3>
<p>The <a id="_idIndexMarker262"/>dataset is also skewed: the number of examples of each class is not balanced. We can calculate the skew using the <span class="No-Break">following code:</span></p>
<pre class="source-code">
counts = np.bincount(train_data["income_bracket"])
class_weight = {
    0: counts[0] / train_data.shape[0],
    1: counts[1] / train_data.shape[0]
}</pre>
<p>The output shows a roughly 75%/25% skew toward the negative (<strong class="source-inline">0</strong>) class. One of the simplest ways of dealing with unbalanced classes (if we have binary classes) is to weigh the positive class more strongly than the negative class. Therefore, when calculating the loss, a prediction that misses the positive class has a more <span class="No-Break">significant impact.</span></p>
<p>LightGBM and XGBoost both support this through the <strong class="source-inline">scale_pos_weight</strong> parameter, which<a id="_idIndexMarker263"/> can be calculated <span class="No-Break">as follows:</span></p>
<pre class="source-code">
scale_pos_weight = class_weight[0]/class_weight[1]</pre>
<h3>Training LightGBM and XGBoost models</h3>
<p>With <a id="_idIndexMarker264"/>the data cleaned and prepared, we<a id="_idIndexMarker265"/> can now train our models. Training the LightGBM and XGBoost models is straightforward. For LightGBM, we have <span class="No-Break">the following:</span></p>
<pre class="source-code">
model = lgb.LGBMClassifier(force_row_wise=True, boosting_type="gbdt", scale_pos_weight=scale_pos_weight)
model = model.fit(X_train, y_train)</pre>
<p>And for XGBoost, we can run the <span class="No-Break">following code:</span></p>
<pre class="source-code">
model = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight)
model = model.fit(X_train, y_train)</pre>
<p>The preceding code highlights the simplicity of working with <span class="No-Break">both libraries.</span></p>
<h3>Training a TabTransformer model</h3>
<p>We’ll now <a id="_idIndexMarker266"/>build a TabTransformer model. We’ll use <strong class="bold">TensorFlow’s Keras</strong> to define the model based on the<a id="_idIndexMarker267"/> example <span class="No-Break">code: </span><a href="https://keras.io/examples/structured_data/tabtransformer/"><span class="No-Break">https://keras.io/examples/structured_data/tabtransformer/</span></a><span class="No-Break">.</span></p>
<p>Our dataset preparation remains mostly the same, with two key differences: we don’t encode the categorical features and must handle the missing <span class="No-Break">values explicitly.</span></p>
<p>We don’t encode the categorical features because Keras provides a special layer to perform string lookup and conversion to a numerical value. However, we must still supply the vocabulary. The following code illustrates creating a <span class="No-Break">lookup layer:</span></p>
<pre class="source-code">
lookup = layers.StringLookup(
                vocabulary=vocabulary,
                mask_token=None,
                num_oov_indices=0,
                output_mode="int",
            )</pre>
<p>The <strong class="source-inline">num_oov_indices</strong>  parameter is set to <strong class="source-inline">0</strong>, meaning no indices are used if an <strong class="bold">out-of-vocabulary</strong> (<strong class="bold">OOV</strong>) value is<a id="_idIndexMarker268"/> encountered. Since our vocabulary is exhaustive, this is not needed. The <strong class="source-inline">mask_token</strong> parameter is also set to <strong class="source-inline">None,</strong> as we aren’t masking any <span class="No-Break">string inputs.</span></p>
<p>We need <a id="_idIndexMarker269"/>to supply a default value for each column in the dataset to handle missing values. Our strategy is to replace string values with a default string value, <strong class="source-inline">NA</strong>. We use the statistical mean for the numeric columns to fill in the missing value. The following code creates a list of the <span class="No-Break">default values:</span></p>
<pre class="source-code">
train_data_description = train_data.describe()
COLUMN_DEFAULTS = [
    train_data_description[feature_name]["mean"] if feature_name in NUMERIC_FEATURE_NAMES else ["NA"]
    for feature_name in HEADERS
]</pre>
<p>The Keras code for implementing a TabTransformer model is roughly 100 lines long and is available in our GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/blob/main/chapter-4/tabtransformer-census-income.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/blob/main/chapter-4/tabtransformer-census-income.ipynb</span></a><span class="No-Break">.</span></p>
<p>The following code sets up the gradient optimizer and data that we can use with the <span class="No-Break">TabTransformer model:</span></p>
<pre class="source-code">
optimizer = tfa.optimizers.AdamW(
    learning_rate=learning_rate,
    weight_decay=weight_decay
)
model.compile(
    optimizer=optimizer,
    loss=keras.losses.BinaryCrossentropy(),
    metrics=[keras.metrics.BinaryAccuracy(name="accuracy"),
             f1_metric,
             precision_metric,
             recall_metric],
)
train_dataset = get_dataset_from_csv(
    train_data_file, batch_size, shuffle=True
)
validation_dataset = get_dataset_from_csv(
    test_data_file, batch_size
)</pre>
<p>We use<a id="_idIndexMarker270"/> an <strong class="source-inline">AdamW</strong> optimizer with weight decay <em class="italic">[7]</em> with a binary cross-entropy loss function to fit the binary classification problem. We can then train and evaluate our model with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
callback = keras.callbacks.EarlyStopping(
    monitor='loss', patience=3
)
history = model.fit(
    train_dataset,
    epochs=num_epochs,
    validation_data=validation_dataset,
    class_weight=class_weight,
    callbacks=[callback]
)
model.evaluate(validation_dataset, verbose=0)</pre>
<p>We also add early stopping via the Keras callback with a patience of <strong class="source-inline">3</strong> epochs. During training and for validation, we track the accuracy and <span class="No-Break">F1 score.</span></p>
<p>Training takes <a id="_idIndexMarker271"/>significantly longer than either gradient-boosting framework and requires a GPU (training on a CPU is technically possible but takes an inordinate amount <span class="No-Break">of time).</span></p>
<p>We can now look at the results of the three algorithms on the Census <span class="No-Break">Income dataset.</span></p>
<h3>Results</h3>
<p>Parameter optimization<a id="_idIndexMarker272"/> was performed for all three algorithms using the grid search technique discussed in the previous chapter. The learning rate, bin size, and number of trees were optimized for the two boosting algorithms. For the TabTransformer, both parameters and aspects of the architecture must be optimized. In terms of parameters, the learning rate, weight decay, and dropout rate were optimized, while for the architecture, the number of transformer blocks and hidden layers (in the MLP) had to be chosen. The optimized parameters are available in the <span class="No-Break">source code.</span></p>
<p>The following table shows the results of the validation set for <span class="No-Break">the algorithms.</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table003-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Model</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Training Time</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Accuracy</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">F1 score</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">LightGBM GBDT</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">1.05s</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">84.46%</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.71</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">XGBoost GBDT</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">5.5s</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">84.44%</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.72</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">TabTransformer</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">113.63s</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">77.00%</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.64</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.3 – Results from training the three models on the Census Income dataset</p>
<p>XGBoost and LightGBM performed similarly on the dataset, reaching an accuracy of 84% and an F1 score of 0.7. The TabTransformer model performed worse, with a lower accuracy and <span class="No-Break">F1 score.</span></p>
<p>Regarding <a id="_idIndexMarker273"/>training time, LightGBM was much faster than the other approaches. The LightGBM model was trained 5.23 times faster than XGBoost and 108.22 times faster than the TabTransformer. The TabTransformer was trained for 15 epochs on an 8-core <span class="No-Break">P4000 GPU.</span></p>
<p>For another point of comparison and to illustrate how the TabTransformer architecture can be adapted when categorical features aren’t present, we look at solving a second problem using the <span class="No-Break">three algorithms.</span></p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor079"/>Detecting credit card fraud</h2>
<p>Our second <a id="_idIndexMarker274"/>task is detecting fraudulent transactions in a credit card transaction dataset<em class="italic"> [8]</em>. The dataset is available at <a href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud">https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud</a>. The task is a binary classification problem, with the training data transactions labeled non-fraudulent (<strong class="source-inline">0</strong>) and fraudulent (<strong class="source-inline">1</strong>). The dataset consists only of numerical features that have been anonymized for confidentiality. Notably, the dataset is highly unbalanced, with fraudulent transactions making up only 0.17% of <span class="No-Break">the data.</span></p>
<h3>Training LightGBM and XGBoost models</h3>
<p>Since<a id="_idIndexMarker275"/> the values are all<a id="_idIndexMarker276"/> numeric, very little data preparation is required for the gradient-boosting models. To counteract the imbalance in the dataset, we again calculate <strong class="source-inline">scale_pos_weight</strong> and pass it to the model as a parameter. We perform a grid search with cross-validation to find good hyperparameters for both the LightGBM and XGBoost models. For LightGBM, both DART and a GBDT model were tried, with DART performing better. Unlike the Census Income dataset, the credit cards dataset is not pre-split into a training and test set. We, therefore, apply five-fold cross-validation to measure performance on unseen data. The following code trains the LightGBM model, with the XGBoost code being <span class="No-Break">very similar:</span></p>
<pre class="source-code">
model = lgb.LGBMClassifier(force_row_wise=True, boosting_type="dart", learning_rate=0.0023, max_bin=384, n_estimators=300, scale_pos_weight=scale_pos_weight, verbose=-1)
scores = cross_val_score(model, X, y, scoring="f1_macro")
print(f"Mean F1-score: {scores.mean()}")</pre>
<p>The <a id="_idIndexMarker277"/>results for both LightGBM and XGBoost <a id="_idIndexMarker278"/>are shown in <em class="italic">Table 4.4</em> alongside the <span class="No-Break">TabTransformer results.</span></p>
<h3>Training a TabTransformer model</h3>
<p>Without <a id="_idIndexMarker279"/>categorical features, the TabTransformer architecture can be significantly simplified. Let’s look at the architecture as shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em>. We can see that the embedding and attention layers are no longer required. Indeed, <em class="italic">the model simplifies to a regular MLP</em> (it is disingenuous to still call the model a transformer as the attention layers are not used <span class="No-Break">at all).</span></p>
<p>Besides removing the unneeded layers, the rest of the architecture and process remain the same as for the Census Income problem. <strong class="source-inline">AdamW</strong> is <a id="_idIndexMarker280"/>again used as the optimizer, and we perform grid search optimization of the hyperparameters and the number of hidden layers in the model. As with the gradient-boosting models, five-fold cross-validation is performed to measure <span class="No-Break">the performance.</span></p>
<h3>Results</h3>
<p>Although the<a id="_idIndexMarker281"/> accuracy is also reported next, it is essential to note that it is not a good performance indicator with unbalanced data. In the dataset, 99.82% of the samples are of one class, and a model that predicts only that class will have a 99.82% accuracy and be completely pointless. The F1 score is unaffected by the class imbalance and remains a good performance indicator for classification performance in unbalanced datasets. The following table shows the results for all three algorithms with <span class="No-Break">five-fold cross-validation.</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table004-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Model</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Training Time</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Accuracy</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">F1 score</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">LightGBM GBDT</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">113s</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">99.88%</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.80</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">XGBoost GBDT</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">351s</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">98.41%</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.82</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">TabTransformer</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">528.59s</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">93.37%</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.05</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.4 – Results from training the three models on the Credit Card Fraud dataset. Training time includes five-fold cross-validation</p>
<p>XGBoost and<a id="_idIndexMarker282"/> LightGBM performed very similarly on the dataset, obtaining F1 scores of 0.82 and 0.80, respectively. The DNN struggles significantly with the problem, obtaining an F1 score of only 0.05 even using class weights to compensate for the <span class="No-Break">imbalanced dataset.</span></p>
<p>Debugging performance issues in DNNs are notoriously tricky. Due to the complexity and opaqueness of building and training DNN models, small changes can have a <span class="No-Break">significant effect.</span></p>
<p>Possible reasons for the poor performance include <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Inadequate model architecture</strong>: This is the most likely cause. The architecture is not well suited to the problem. Further experimentation with the architecture, the size of layers, or even the type of neural network <span class="No-Break">is needed.</span></li>
<li><strong class="bold">Insufficient training</strong>: The model might not be trained long enough. Increasing the training epochs can improve performance. However, in our experiments, the loss stagnated after 10 epochs (while training continued <span class="No-Break">to 15).</span></li>
<li><strong class="bold">Inappropriate loss function</strong>: We applied a <strong class="source-inline">BinaryCrossentropy</strong> loss function with class weights. However, a more advanced loss function, such as a focal loss, may be <span class="No-Break">tried </span><span class="No-Break"><em class="italic">[9]</em></span><span class="No-Break">.</span></li>
</ul>
<p>In terms of the training and validation time, it is a similar story as with the Census Income dataset. The LightGBM model was trained and validated significantly faster than the other approaches: 3.1 times faster than the XGBoost and 4.62 times faster than <span class="No-Break">the DNN.</span></p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor080"/>Summary</h1>
<p>In this chapter, we discussed two additional algorithms that may be used to solve tabular learning problems: XGBoost, another gradient-boosting framework, and TabTransformer, a deep <span class="No-Break">learning approach.</span></p>
<p>We showed how to set up and train both XGBoost models and TabTransformer on two datasets. We also showed how to encode categorical features for tree-based and neural network models. Both datasets also had imbalanced classes, which we had to compensate for <span class="No-Break">during training.</span></p>
<p>We found that LightGBM and XGBoost produced similarly accurate models but that LightGBM trained models much faster and more efficiently. We also saw the complexity of training DNNs and the lackluster performance on these problems. Deep learning is an extremely powerful technique, but tree-based approaches are often more applicable when working with <span class="No-Break">tabular datasets.</span></p>
<p>In the next chapter, we focus on more effective parameter optimization with LightGBM using a framework <span class="No-Break">called </span><span class="No-Break"><strong class="bold">Optuna</strong></span><span class="No-Break">.</span></p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor081"/>References</h1>
<table class="No-Table-Style" id="table005-1">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">1]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">T. Chen and C. Guestrin, “XGBoost,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data </em><span class="No-Break"><em class="italic">Mining, 2016.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">2]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention Is All You </em><span class="No-Break"><em class="italic">Need, 2017.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">3]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">X. Huang, A. Khetan, M. Cvitkovic, and Z. Karnin, TabTransformer: Tabular Data Modeling Using Contextual </em><span class="No-Break"><em class="italic">Embeddings, 2020.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">4]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">R. Becker, Adult, UCI Machine Learning </em><span class="No-Break"><em class="italic">Repository, 1996.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">5]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">D. Hendrycks and K. Gimpel, Gaussian Error Linear Units (</em><span class="No-Break"><em class="italic">GELUs), 2020.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">6]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">G. Klambauer, T. Unterthiner, A. Mayr and S. Hochreiter, Self-Normalizing Neural </em><span class="No-Break"><em class="italic">Networks, 2017.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">7]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">I. Loshchilov and F. Hutter, Decoupled Weight Decay </em><span class="No-Break"><em class="italic">Regularization, 2019.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">8]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">A. Dal Pozzolo, O. Caelen, R. Johnson and G. Bontempi, “Calibrating Probability with Undersampling for Unbalanced </em><span class="No-Break"><em class="italic">Classification,” 2015.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">9]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">T.-Y. Lin, P. Goyal, R. Girshick, K. He and P. Dollár, Focal Loss for Dense Object </em><span class="No-Break"><em class="italic">Detection, 2018.</em></span></p>
</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer037">
<h1 id="_idParaDest-80" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor082"/>Part 2: Practical Machine Learning with LightGBM</h1>
<p>Part 2 delves into the intricate processes that underpin practical machine learning engineering, starting with a look at efficient hyperparameter optimization via a framework called <strong class="bold">Optuna</strong>. We will then transition into a comprehensive exploration of the data science lifecycle, illustrating the rigorous steps from problem definition and data handling to practical data science modeling applications. Concluding this part, the focus will shift to automated machine learning, spotlighting the FLAML library, which aims to simplify and streamline model selection and tuning. Throughout this part, a blend of case studies and hands-on examples will provide a clear roadmap to harnessing the full potential of these advanced tools, underscoring the themes of efficiency <span class="No-Break">and optimization.</span></p>
<p>This part will include the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B16690_05.xhtml#_idTextAnchor083"><em class="italic">Chapter 5</em></a><em class="italic">, LightGBM Parameter Optimization with Optuna</em></li>
<li><a href="B16690_06.xhtml#_idTextAnchor094"><em class="italic">Chapter 6</em></a><em class="italic">, </em><em class="italic">Solving Real-World Data Science Problems with LightGBM </em></li>
<li><a href="B16690_07.xhtml#_idTextAnchor116"><em class="italic">Chapter 7</em></a><em class="italic">, AutoML with LightGBM and FLAML</em></li>
</ul>
</div>
<div>
<div id="_idContainer038">
</div>
</div>
<div>
<div id="_idContainer039">
</div>
</div>
<div>
<div id="_idContainer040">
</div>
</div>
<div>
<div id="_idContainer041">
</div>
</div>
<div>
<div id="_idContainer042">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer043">
</div>
</div>
<div>
<div id="_idContainer044">
</div>
</div>
<div>
<div id="_idContainer045">
</div>
</div>
</div></body></html>