<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Tuning Clusters for Machine Learning</h1>
                </header>
            
            <article>
                
<p>Many data scientists and machine learning practitioners face the problem of scale when attempting to run ML data pipelines over big data. In this chapter, we will focus primarily on <span><strong>Elastic MapReduce</strong> </span>(<strong><span>EMR</span></strong>), which is a very powerful tool for running very large machine learning jobs. There are many ways to configure EMR and not every setup works for every scenario. In this chapter, we will outline the main configurations of EMR and how each configuration works for different objectives. Additionally, we will present AWS Glue as a tool to catalog the results of our big data pipelines.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Introduction to the EMR architecture</li>
<li>Tuning EMR for different applications</li>
<li>Managing data pipelines with Glue</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to the EMR architecture</h1>
                </header>
            
            <article>
                
<p>In <a href="af506fc8-f482-453e-8162-93a676b2e737.xhtml">Chapter 4</a>, <em>Predicting User Behavior with Tree-Based Methods</em>, we introduced EMR, which is an AWS service that allows us to run and scale Apache Spark, Hadoop, HBase, Presto, Hive, and other big data frameworks. These big data frameworks typically require a cluster of machines running specific software that are correctly configured so that the machines are able to communicate with each other. Let's look at the most commonly used products within EMR.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Hadoop</h1>
                </header>
            
            <article>
                
<p>Many applications, such as Spark and HBase, require Hadoop. The basic installation of Hadoop comes with two main services:</p>
<ul>
<li style="font-weight: 400"><strong>Hadoop Distributed Filesystem</strong> (<strong>HDFS</strong>): This is a service that allows us to store large amounts of data (for example, files that cannot be stored on a single machine) across many servers. A NameNode server is responsible for indexing which blocks of which file are stored in which server. The blocks of each file are replicated across the cluster so that if a machine goes down, we don't lose any information. DataNode servers are responsible for keeping and serving the data on each machine. Many other EMR services, such as Apache HBase, Presto, and Apache Spark, are able to use HDFS to read and write data. HDFS works well when you are using long-running clusters. For clusters that are launched just for the purpose of a single job (<span>such as</span> a training job), you should consider having the data storage in S3 instead.</li>
<li style="font-weight: 400"><strong>MapReduce</strong>: This framework was the basis for big data crunching for many years. By allowing users to specify two functions (a <kbd>map</kbd> function and a <kbd>reduce</kbd> function), many big data workloads were made possible. The map function is responsible for taking chunks of data and transforming them in a one-to-one fashion (for <span>example,</span> take the price of every transaction). The reduce function takes the output of the map function and aggregates it in some way (<span>such as</span> finding the average transaction price per region). MapReduce was designed so that the processing was done on the same machines that we store the HDFS file blocks on, to avoid sending large amounts of data over the network. This data locality principle proved to be very effective for running big data jobs on commodity hardware and with limited network speeds.</li>
</ul>
<p>EMR allows you to create clusters with three types of nodes:</p>
<ul>
<li style="font-weight: 400"><strong>Master node</strong>: This is unique in a cluster and is typically responsible for orchestrating work throughout other nodes in the cluster.</li>
<li style="font-weight: 400"><strong>Core nodes</strong>: These kinds of nodes will host HDFS blocks and run a DataNode server, hence job tasks running on these nodes may take advantage of data locality.</li>
<li style="font-weight: 400"><strong>Task nodes</strong>: These nodes do not host HDFS blocks but can run arbitrary job tasks. Tasks running on these nodes will need to read data from filesystems hosted on other machines (for <span>example,</span> core nodes or S3 servers).</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Spark</h1>
                </header>
            
            <article>
                
<p>Apache Spark is one of the most popular big data frameworks. It extends the idea of MapReduce by allowing the user to specify additional high-level functions on top of the data. It can perform map and reduce functions but also supports filter, group, join, window functions, and many other operations. Additionally, as we have seen throughout this book, <span><span>we</span></span> can use SQL operations to perform ETL and analytics. Apache Spark was designed to cache large amounts of data in-memory to speed up algorithms that require several passes of the data. For example, algorithms that require several iterations of <strong>gradient descent</strong> can run orders of magnitude faster if the datasets are cached in-memory.</p>
<p>Apache Spark also comes with a number of very useful libraries for streaming, graph manipulation, and the machine learning ones that we have used throughout this book. We encourage you to explore these additional libraries as they are extremely high-quality and useful. Spark is unique in that it seamlessly integrates many well-developed libraries together, such as TensorFlow and <kbd>scikit-learn</kbd>. You can build excellent models with both of these tools, but they do not currently allow us to read and prepare data by parallelizing the work in a cluster like Spark does. In other words, Apache Spark provides the full stack of packages, from data ingestion to model generation. Some people refer to Spark as the operating system for big data. Often, data scientists and engineers use Spark to perform data preparation at scale and then use other tools, such as TensorFlow and SageMaker to build and deploy specialized models. In <a href="ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml">Chapter 5</a> <span>,</span> <em>Customer Segmentation Using Clustering Algorithms,</em> we saw <span>how we can smoothly integrate Apache Spark and SageMaker through the use of SageMaker Spark estimators.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Hive</h1>
                </header>
            
            <article>
                
<p>Apache Hive was born as a translator from SQL to MapReduce jobs. You can specify <strong>Data Definition Language</strong> (<strong><span>DDL</span></strong>) and <strong>Data Manipulation Language</strong> (<strong>DML</strong>) statements and work with SQL as if you were working on a standard database management system using Apache Hive. Many non-technical users that knew SQL could perform analytics at scale when Hive first appeared, which was one of the reasons for its popularity. What happens under the hood with Hive (and with Spark SQL) is that the SQL statement is parsed and a series of MapReduce jobs are constructed on the fly and run on the cluster to perform the declarative operation described by the SQL statement.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Presto</h1>
                </header>
            
            <article>
                
<p>Presto is a product developed by Facebook that also translates SQL into big data workloads but is tailored for interactive analytics. It is extremely fast and is specially optimized for when you have a large fact table and several smaller-dimension tables (such as a transaction and other joined tables, such as a product and clients). AWS provides a serverless alternative based on Presto, called Athena, which is great when your data is on S3. Athena queries are charged based on how much data is scanned. For this reason, it has become extremely popular for big data analytics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache HBase</h1>
                </header>
            
            <article>
                
<p>HBase is a product similar to Google's Bigtable. Conceptually, it can be seen as a huge distributed key-value store. HBase is not as popular anymore due to the appearance of technologies such as AWS DynamoDB, which is serverless and, in our experience, more reliable. However, it can be a cost-effective way to store data when you need to access it through keys. For example, you could use HBase to store a custom model for each user (on the assumption that you have billions of users to justify it).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Yet Another Resource Negotiator</h1>
                </header>
            
            <article>
                
<p>Apache Hadoop also developed <strong>Yet Another Resource Negotiator</strong> (<strong>YARN</strong>), which is the fundamental tool with which EMR schedules and coordinates different applications. YARN is effectively the cluster manager behind EMR and is responsible for launching the necessary daemons on different machines. When you configure a cluster through EMR, you can specify the different applications that you want to run. Examples of such applications are Spark, HBase, and Presto. YARN is responsible for launching the necessary processes. In the case of Spark, YARN will launch Spark executors and drivers as needed. Each of these processes reports the necessary memory and CPU code consumption to YARN. That way, YARN can make sure that the cluster load is properly managed and not overloaded.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning EMR for different applications</h1>
                </header>
            
            <article>
                
<p>in this section we will consider the aspects involved in tuning the clusters we use for machine learning. When you launch an EMR cluster, you can specify the different applications you want to run.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following screenshot shows the applications available in EMR version 5.23.0:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-669 image-border" src="assets/559d2782-2eb6-49c0-8f69-06dc67d60f14.png" style="width:154.50em;height:91.50em;"/></p>
<p>Upon launching an EMR cluster, these are the most relevant items that need to be configured:</p>
<ul>
<li style="font-weight: 400"><strong>Applications</strong>: Applications such as Spark).</li>
<li style="font-weight: 400"><strong>Hardware</strong>: We covered this in <a href="06270fa5-1364-4ad2-b4a0-3522c1ef7bcd.xhtml">Chapter 10</a>, <em>Creating Clusters on AWS</em>.</li>
<li style="font-weight: 400"><strong>Use of the Glue Data Catalog</strong>: We'll cover this in the last section of this chapter, <em>Managing data pipelines with Glue</em>).</li>
<li style="font-weight: 400"><strong>Software configuration</strong>: These are properties that we can specify to configure application-specific properties. In the next section, <em>Configuring application properties,</em> we'll show how to customize the behavior of Spark through specific properties.</li>
<li style="font-weight: 400"><strong>Bootstrap actions</strong>: These are user-specific scripts (typically located in S3) that will run on every node of the cluster as it boots up. Bootstrap actions are useful, for example, when you want to install a specific package on all the machines of the cluster upon startup.</li>
<li style="font-weight: 400"><strong>Steps</strong>: These are the different jobs that the user wants to run once the applications are up. For example, if we want to launch a cluster that runs a training job in Spark and then we want to shut down the cluster, we would specify a Spark job step and check the <span class="packt_screen">auto-terminate cluster after the last step is complete</span> option. Such a use case is pertinent when we are launching a cluster programmatically (via the AWS API). Scheduled or event-driven AWS Lambda functions can use libraries such as <kbd>boto3</kbd> to launch clusters programmatically upon the occurrence of an event, or on a regular schedule. More information about AWS Lambda can be found at <a href="https://docs.aws.amazon.com/lambda/">https://docs.aws.amazon.com/lambda/</a>.<a href="https://docs.aws.amazon.com/lambda/"/></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring application properties</h1>
                </header>
            
            <article>
                
<p>In the preceding screenshot, you might have noticed that there is a space called <strong>Software Settings</strong> for customizing the configuration of different applications. There are different categories of configurations, named <kbd>classifications</kbd>, that allow you to override the default configuration of the different applications by changing the values for a chosen set of properties.</p>
<p>In the following code block, we provide a very useful set of properties to configure Spark for two things: maximizing resource allocation and enabling the AWS Glue metastore:</p>
<pre>classification=spark,properties=[maximizeResourceAllocation=true]<br/>classification=spark-defaults,properties=[spark.sql.catalogImplementation=hive]<br/>classification=spark-hive-site,properties=[hive.metastore.connect.retries=50,hive.metastore.client.factory.class=com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory]</pre>
<p>Let's look at the effect of each of these configurations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maximize Resource Allocation</h1>
                </header>
            
            <article>
                
<p>When you enable <kbd>maximizeResourceAllocation</kbd>, EMR and Spark will figure out how to configure Spark so as to use all of the available resources (for example, memory and CPU). The alternative is to manually configure properties such as the number of executors, Java heap space for each executor, and the number of cores (that is, threads) for each executor. If you choose to do this manually, you need to take great care not to exceed the available resources of the cluster (and also not to underuse the available hardware). We recommend having this setting always set by default.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The AWS Glue Catalog</h1>
                </header>
            
            <article>
                
<p>AWS Glue provides a service that is known as a Hive metastore. The purpose of this service is to keep track of all the data in our data lake by defining tables that describe the data. A data lake is typically hosted on S3 or HDFS. Any data that lies on these distributed filesystems, and has a tabular format, such as Parquet or CSV, can be added to the metastore. This does not copy or move the data; it is just a way of keeping a catalog of all our data. By configuring the <kbd>hive.metastore.client.factory.class</kbd> property in the cluster configuration, we allow Spark to use all the tables registered in the Glue Catalog. Additionally, Spark can also create a new table or modify the catalog through Spark SQL statements. In the next section, we will show a concrete example of how Glue is useful.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing data pipelines with Glue</h1>
                </header>
            
            <article>
                
<p class="mce-root">Data scientists and data engineers run different jobs to transform, extract, and load data into systems such as S3. For example, we might have a daily job that processes text data and stores a table with the bag-of-words table representation that we saw in <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>, <em>Classifying Twitter Feeds with Naive Bayes</em>. We might want to update the table each day to point to the latest available data. Upstream processes can then only rely on the table name to find and process the latest version of the data. If we do not catalog this data properly, it will be very hard to combine the different data sources or even to know where the data is located, which is where AWS Glue metastore comes in. Tables in Glue are grouped into databases. However, tables in different databases can be joined and referenced.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating tables with Glue</h1>
                </header>
            
            <article>
                
<p class="mce-root">You can access the Glue console on AWS by going to <a href="https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases">https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=databases</a>.</p>
<p>In the console, create a new database, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-670 image-border" src="assets/03081b05-181c-4c95-a244-db0732623893.png" style="width:19.67em;height:9.92em;"/></p>
<p>Once the database is created, you can switch to the Athena AWS service and start creating tables from our data in S3 to run queries for analytics. The AWS Athena console can be accessed at <a href="https://console.aws.amazon.com/athena/home">https://console.aws.amazon.com/athena/home</a>.</p>
<p>Let's create a table in S3 for the Boston house prices dataset that we worked on in <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms</em>.</p>
<p>In the following screenshot we can see how the create table SQL statement will specify the name, format, and fields of the table from our CSV data located in S3:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-671 image-border" src="assets/c2127dbe-4e33-4297-8098-b8f5b623f53c.png" style="width:93.17em;height:38.17em;"/></p>
<p>Note, that the location specifies a folder (not a file). In our case, we have a single <kbd>CSV</kbd> folder at <kbd>s3://mastering-ml-aws/chapter3/linearmodels/train/training-housing.csv</kbd>. However, we could have many CSVs on the same folder and all would be linked to the <kbd>house_prices</kbd> table we just created. Once we create the table, since the data is in S3, we can start querying our table as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-672 image-border" src="assets/3bb5d075-8139-44d4-b3a9-1cb5dd505239.png" style="width:78.92em;height:48.83em;"/></p>
<p>Note how the data is tabulated correctly. This is because we have told Glue the right format and location of our data. Now we can run ultra-fast analytics using SQL with Presto-as-a-service through Athena. </p>
<div class="packt_infobox">We just performed a create table operation; however, often, we want to perform alter table commands to switch the underlying data behind a table to a more recent version. It's also very common to perform add-partition operations to incrementally add data to a table (such as new batches or dates). Partitions also help the query engine to filter the data more effectively.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accessing Glue tables in Spark</h1>
                </header>
            
            <article>
                
<p>Once the table created is in Glue, it will also become available on every EMR Spark cluster (as long as we configure the <kbd>hive.metastore.client.factory.class</kbd> described in the previous section,  <em>Tuning EMR for different applications</em>). Let's launch an EMR cluster with the JupyterHub application enabled. The JupyterHub application is an alternative to the EMR notebooks feature we used throughout <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a><span>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Classifying Twitter Feeds with Naive Bayes</span></span></em><span>, to </span><a href="c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml">Chapter 6</a><span>, </span><em>Analyzing Visitor Patterns to Make Recommendations</em>. Consider using <kbd>JupyterHub</kbd> when you have a team of data scientists reusing the same cluster and running different notebooks. You can learn more on JupyterHub at <a href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub.html">https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-jupyterhub.html</a>.</p>
<p>The following screenshot shows our cluster created with the Glue metastore enabled and <kbd>JupyterHub</kbd> as the application:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-673 image-border" src="assets/086ac49e-8153-4744-9384-b694ffa4ac74.png" style="width:53.25em;height:23.83em;"/></p>
<p>If you click on the <kbd>JupyterHub</kbd> link, it will take you to an authentication page, such as the following:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-674 image-border" src="assets/47db6713-f8da-4a3d-b2e2-8e5da74444ec.png" style="width:16.92em;height:12.08em;"/></p>
<p>The default configuration of <kbd>JupyterHub</kbd> has a default user account with a username of <kbd>jovyan</kbd> and a password of <kbd>jupyter</kbd> available. The authentication can be customized through the EMR configuration if needed.</p>
<p>Once we authenticate, we can start creating notebooks exactly as we did with EMR notebooks. In this case, we will create a <kbd>PySpark3</kbd> notebook:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-676 image-border" src="assets/f6931e08-4e0a-466a-90ec-8d8b017b3e58.png" style="width:13.33em;height:16.25em;"/></p>
<p>Now, notebooks can use SparkMagic to interleave paragraphs in Python and SQL. Let's look at <span>the following notebook </span>example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-675 image-border" src="assets/71792244-9d3d-44bc-b87d-c627d4f7ab15.png" style="width:79.75em;height:68.33em;"/></p>
<p>The first paragraph runs a SQL on the table we just created through Glue/Athena through SparkMagic's <kbd>%%sql</kbd> magic (more on SparkMagic can be found at <a href="https://github.com/jupyter-incubator/sparkmagic">https://github.com/jupyter-incubator/sparkmagic</a>). The second paragraph constructs a Spark DataFrame through a simple SQL statement that selects two fields from our table. The third paragraph runs a Spark job (that is, the describe command) over the Spark DataFrame we constructed. You will appreciate how easy it is to handle, integrate, and process data once we have properly cataloged it in our Glue metastore.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at the main configuration parameters of EMR and how they can help us run many big data frameworks, such as Spark, Hive, and Presto. We also explored the AWS services of Athena and Glue as a way to catalog the data on our data lake so that we can properly synchronize our data pipelines. Finally, we demonstrated how Glue can also be used in EMR, with smooth integration for <kbd>JupyterHub</kbd> with SparkMagic.</p>
<p>In the next chapter,<em> Deploying Models Built in AWS</em>, we will cover how to deploy machine learning models in different environments.</p>


            </article>

            
        </section>
    </body></html>