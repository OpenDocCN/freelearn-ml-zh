<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Using AWS Rekognition</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have studied deep learning algorithms and how to implement them using SageMaker in <a href="c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml">Chapter 7</a>, <em>Implementing Deep Learning Algorithms</em>, and <a href="37b99b21-96c2-4857-b8e0-686179d109cf.xhtml">Chapter 9</a>, <em>Image Classification and Detection with SageMaker</em>. You must have realized that training a good <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>) takes a lot of expertise and resources. Moreover, it also requires a large number of labeled images with objects. Amazon has an out-of-box solution for image recognition, called <strong>Amazon Rekognition</strong>, that offers various tools for image recognition using pretrained image recognition models. </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Introducing Amazon Rekognition</li>
<li>Implementing object and scene detection</li>
<li>Implementing facial analysis</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Amazon Rekognition</h1>
                </header>
            
            <article>
                
<p>Building image recognition models using deep learning is very challenging. Firstly, you need a large, labeled dataset in order to train the deep learning model to perform specific tasks. Secondly, you need knowledge of how to design a network and tune the parameters to get the best accuracy. Finally, training such deep learning models at scale requires expensive GPU-based clusters to train these models. </p>
<p>Amazon Rekognition (<a href="https://aws.amazon.com/rekognition/">https://aws.amazon.com/rekognition/</a>) is a tool offered by AWS featuring image recognition models that are already pretrained for use in your applications. Amazon Rekognition models are based on an analysis of billions of videos and images. Similar to how <strong>Amazon Comprehend</strong> offers NLP models as a service, Rekognition offers various image recognition models that can perform specific tasks. The advantage of using Amazon Rekognition is that you can simply use dashboards and APIs to perform image recognition tasks at high accuracy, without the high-level expertise required to train such machine learning models. </p>
<p class="mce-root"/>
<p>Amazon Rekognition only offers a limited number of models that perform specific tasks. In this section, we'll look at the various tools available in the Amazon Rekognition dashboard. We'll also look at how we can access these features using AWS APIs <span><span>in </span></span>Python. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing object and scene detection</h1>
                </header>
            
            <article>
                
<p>Object and scene detection algorithms can recognize various objects in the image and assign confidence to each prediction. This algorithm uses a hierarchy of labels to label objects and returns all the nodes of the leaf when it detects an object. Object detection is a classic application of image recognition. It allows us to identify what is inside an image and label it. For example, consider a newsroom where photographers are submitting hundreds of images and videos every day. You need people to label such images so that if you wish to access an image of a celebrity who was pictured during a car crash, these image libraries can be searchable. </p>
<p>Object detection allows you to automatically label these images so that they can be stored, organized, and retrieved efficiently. One of the key features of an object detection algorithm is that they have to be comprehensive and should be able to detect a large array of objects. Moreover, such algorithms also detect the edges of the object and should be able to return the bounding box for an object. Amazon Rekognition performs both these tasks effectively. </p>
<p>You can access the Amazon Rekognition dashboard <span><span>using </span></span>the AWS Console. Just search for Rekognition in the search bar and you will be able to access the demo for Amazon Rekognition. The demo shows you how the tools work, but you would need to use the API if you want to analyze multiple images. </p>
<p>Once, you are on the demo screen, select <span class="packt_screen">Object and Scene detection</span> to access a demo where you can select a single image and detect the images in the object.</p>
<p>For the purpose of this demo, I have used a <span><span>screenshot </span></span>of the Chicago river with ferry boats on the river:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-647 image-border" src="assets/d75ffa44-7ade-4576-85d5-e96b4420801a.png" style="width:122.42em;height:56.42em;"/></p>
<p>As you can see from the preceding screenshot, the object detection tool returns a ranked list of objects in the image along with the confidence of detection. As we mentioned previously, since the tool uses a hierarchy of categories, it may detect similar categories at the top. For example, it was able to detect the boat in the image. However, it also returned <span class="packt_screen">Vehicle</span> and <span class="packt_screen">Transportation</span> categories with the same confidence score. We can also see that the demo shows bounding boxes for each of the objects that it detected in the image. </p>
<p>However, using this tool to perform object detection may be tedious as it only handles one image at a time. So, we can also use an API to access the Amazon Rekognition tool. You need to upload your images to a folder in S3 bucket in order to use object detection on them.</p>
<p>The following Python code can be used to perform the same operation on the image: </p>
<pre>import boto3<br/>import json<br/><span class="n"><br/>client</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s1">'rekognition'</span><span class="p">)<br/></span><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">detect_labels</span><span class="p">(</span> <br/>                <span class="n">Image</span><span class="o">=</span><span class="p">{<br/>                    </span><span class="s1">'S3Object'</span><span class="p">:</span> <br/>                            <span class="p">{<br/>                                </span> <span class="s1">'Bucket'</span><span class="p">:</span> <span class="s1">'masteringmlsagemaker'</span><span class="p">,<br/></span>                                 <span class="s1">'Name'</span><span class="p">:</span> <span class="s1">'ImageRecognition/chicago_boats.JPG'<br/>                            </span><span class="p">}<br/>                    </span><span class="p">},<br/>                </span><span class="n">MaxLabels</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <br/>                <span class="n">MinConfidence</span><span class="o">=90</span> <br/><span class="p">)<br/><br/>print(json.dumps(response, sort_keys=True, indent=4))<br/></span></pre>
<p><span><span>T</span></span>he image has to be in the S3 bucket and you have to specify the bucket name and image name as a parameter of the request function. The response is long, so we only show the format of the first prediction in the response JSON:</p>
<pre>{<br/>  "LabelModelVersion": "2.0",<br/>  "Labels": [<br/>    {<br/>      "Confidence": 99.86528778076172,<br/>      "Instances": [<br/>        {<br/>          "BoundingBox": {<br/>            "Height": 0.29408860206604004,<br/>            "Left": 0.5391838550567627,<br/>            "Top": 0.6836633682250977,<br/>            "Width": 0.25161588191986084<br/>          },<br/>          "Confidence": 99.86528778076172<br/>        },<br/>        {<br/>          "BoundingBox": {<br/>            "Height": 0.11046414822340012,<br/>            "Left": 0.23703880608081818,<br/>            "Top": 0.6440696120262146,<br/>            "Width": 0.07676628232002258<br/>          },<br/>          "Confidence": 99.5784912109375<br/>        },<br/>        {<br/>          "BoundingBox": {<br/>            "Height": 0.040305182337760925,<br/>            "Left": 0.5480409860610962,<br/>            "Top": 0.5758911967277527,<br/>            "Width": 0.04315359890460968<br/>          },<br/>          "Confidence": 77.51519012451172<br/>        }<br/>      ],<br/>      "Name": "Boat",<br/>      "Parents": [<br/>        {<br/>          "Name": "Vehicle"<br/>        },<br/>        {<br/>          "Name": "Transportation"<br/>        }<br/>      ]<br/>    },<br/>...<br/>  ]<br/>}</pre>
<p>As you can observe from the response, we found three instances of the <strong><kbd>Boat</kbd> </strong>object in the image. The response provides the bounding box for each of the objects found in the image. Moreover, you can observe that the boat in the far right is small, so the confidence in detecting it is much lower than the other two boats in the image. The response also returned the parents of the object in the hierarchy. So, if you have hundreds of images to categorize, you can add them all to an S3 bucket and use this code to iterate through them and detect labels for those objects. Because of tools such as Amazon Rekognition, data scientists now have access to world-class deep learning models that they can apply in the tools that they are building. However, such an object detection algorithm only works for a limited number of objects. For example, we tried the algorithm on x-ray images of cancer in this tool and it was not able to return any results. If you are working on a very specialized product where you are trying to detect medical images of tumors or images from a space telescope, you would need to train your own models based on a large number of labeled images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing facial analysis</h1>
                </header>
            
            <article>
                
<p>Amazon Rekognition also offers a powerful tool for performing facial analysis on images. It can predict interesting attributes such as age and gender based on looking at the image. It can also detect features such as a smile or whether the person is wearing glasses from this model. Such models would be trained by analyzing a lot of labeled facial images and training an image recognition model to recognize these features. The CNN models that we studied in <a href="c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml">Chapter 7</a>, <em>Implementing Deep Learning Algorithms</em>, would be a good fit for such applications as it can automatically generate feature maps using local receptive fields methodology from the image and detect boxes that would contain evidence of these facial features. </p>
<p>The facial analysis demo can be accessed in the same way as the object detection demo. In order to test the model, we picked the picture of Mona Lisa by Leonardo Da Vinci. One of the long-standing mysteries about the image is whether the lady in the image is smiling or not.</p>
<p class="mce-root"/>
<p>In the following screenshot, we can see how the facial analysis demo provides features of the face from the image:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-648 image-border" src="assets/23e3776b-36a6-499c-98fa-e0f75919e4ae.png" style="width:125.00em;height:82.75em;"/></p>
<p>The facial analysis model does predict that there is a face in the image and creates a correct box around it. It correctly predicted that the image is female and predicts an age range for that person. It predicted that the person in the image is not smiling. It also correctly predicted that the person is not wearing any glasses. </p>
<p>You can also access this same information using an API call.</p>
<p>With the following Python code, you can perform the same task of facial analysis as in the preceding demo:</p>
<pre>import boto3<br/>import json<br/><br/>client = boto3.client('rekognition')<br/>response = client.detect_faces(<br/>    Image={<br/>        'S3Object': {<br/>            'Bucket': 'masteringmlsagemaker',<br/>            'Name': 'ImageRecognition/monalisa.jpg'<br/>        }<br/>    },<br/>    Attributes=['ALL']<br/>)<br/><br/>print(json.dumps(response, sort_keys=True, indent=4))</pre>
<p>You have to store your image on an S3 bucket and provide the bucket and image name to the API call. You can also specify what attributes you need to be returned, or specify <kbd>All</kbd> in case you need all the attributes.</p>
<p>The response of this call is in JSON format and looks as follows:</p>
<pre>{<br/>    "FaceDetails": [<br/>        {<br/>            "BoundingBox": {<br/>                "Width": 0.22473210096359253,<br/>                "Height": 0.21790461242198944,<br/>                "Left": 0.35767847299575806,<br/>                "Top": 0.13709242641925812<br/>            },<br/>            "AgeRange": {<br/>                "Low": 26,<br/>                "High": 43<br/>            },<br/>            "Smile": {<br/>                "Value": false,<br/>                "Confidence": 96.82086944580078<br/>            },<br/>            "Gender": {<br/>                "Value": "Female",<br/>                "Confidence": 96.50946044921875<br/>            },<br/>          "Emotions": [<br/>                {<br/>                    "Type": "CALM",<br/>                    "Confidence": 34.63209533691406<br/>                },<br/>                {<br/>                    "Type": "SAD",<br/>                    "Confidence": 40.639801025390625<br/>                }<br/>            ],<br/>            "Landmarks": [<br/>                {<br/>                    "Type": "eyeLeft",<br/>                    "X": 0.39933907985687256,<br/>                    "Y": 0.23376932740211487<br/>                },<br/>                {<br/>                    "Type": "eyeRight",<br/>                    "X": 0.49918869137763977,<br/>                    "Y": 0.23316724598407745<br/>                },<br/>            "Confidence": 99.99974060058594<br/>        }<br/>    ]<br/>}</pre>
<p>We have edited this response to maintain brevity. However, you can observe that you can see information about <kbd>Age</kbd>, <kbd>Gender</kbd>, and <kbd>Smile</kbd> as we saw in the demo. However, it also identifies emotions on the face such as sadness and calm. It also locates landmarks on the face such as the eyes, nose, and lips. </p>
<p>Such tools are used in current smartphones where a smile can trigger a photo. It is used in consumer surveys in restaurants to gauge the demographics of people in the restaurant and whether they are happy with the service. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other Rekognition services</h1>
                </header>
            
            <article>
                
<p>Amazon Rekognition also offers other image recognition services. You can use the API as in the examples in this chapter to access these services. We will list some of the services and their applications here. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image moderation</h1>
                </header>
            
            <article>
                
<p>We can use Rekognition to monitor images and check whether the content is suggestive or unsafe. Such techniques are used to moderate live video services, such as Twitch or Facebook Live, where <strong>Artificial Intelligence</strong> (<strong>AI</strong>) can automatically detect unsafe content. As services such as YouTube or Instagram see an unimaginable amount of data being uploaded on them every day, using such AI techniques can help to lower the cost of moderating the platform. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following screenshot shows how the image moderation tool can detect suggestive themes in the image and automatically label them:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-649 image-border" src="assets/4cb081b8-a312-4e7a-90d7-dc6cfcdbb7b5.png" style="width:154.92em;height:70.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Celebrity recognition</h1>
                </header>
            
            <article>
                
<p>Recognition can also be used to detect celebrities in pictures or videos automatically. This can be done by an image recognition model learning from labeled images and videos. Deep learning algorithms can automatically extract facial features and then compare them to predict who the celebrity may be. For example, many of the movies and TV shows on services such as Amazon Prime can show the names of actors on the screen using this technique. Manually labeling these scenes with the names of actors may be a very tedious task; however, deep learning algorithms can do this automatically.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the following example, Amazon Rekognition detects an image of Jeff Bezos and labels it correctly:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-650 image-border" src="assets/8b99b830-cdb2-410e-8cf0-9f009f3c3536.png" style="width:158.17em;height:60.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face comparison</h1>
                </header>
            
            <article>
                
<p>Celebrity recognition technology can be further extended to do facial comparisons and detect faces that are similar. For example, your Facebook account automatically matches the faces in an image you upload with your friends and tags the images automatically. They use such image recognition algorithms to train models for each face and run those models on your uploaded images to detect whether your friends are in that picture. Amazon Rekognition also offers a feature called <strong>face comparison</strong> that compares faces between two images and detects whether the same people appear in both pictures. </p>
<p><span><span>In the following screenshot, we can observe that the face comparison algorithm can automatically match the faces in two images and detect which faces are similar to each other:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-651 image-border" src="assets/c306bb9d-5e78-4949-8151-8dcbe8e1c4f4.png" style="width:155.33em;height:65.00em;"/></p>
<p>Amazon Rekognition also offers another tool that can detect text in a picture. This model is similar to what we built in <a href="a05fc52e-bb4c-4200-b0c5-154dccaad739.xhtml">Chapter 8</a>, <em>Implementing Deep Learning with TensorFlow on AWS</em>, where our model was able to detect numbers. This tool is also very useful for reading text in the real world. Applications such as Google Translate can analyze camera images and can translate them to your native language. Self-driving cars can also use this technology to read road signs and react accordingly.</p>
<p>The following screenshot shows how Amazon Rekognition can detect text inside an image:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-652 image-border" src="assets/1002b21c-e520-47b4-b1ea-677ecda4dcd4.png" style="width:158.33em;height:61.17em;"/></p>
<p>Recognition does not do an accurate job with this image, but is able to box and recreate the text in this image. </p>
<p class="mce-root"/>
<p>We have not given code examples for these services in this section. The API calls are similar to what we discussed in the first two tools presented in this section. We encourage you to try the API calls for these services and test how they work. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Amazon Rekognition allows data scientists to access high-quality image recognition algorithms using API calls. One of the biggest obstacles in using deep learning is generating large datasets and running expensive GPU-based clusters to train the models. AWS Rekognition makes it easier for users to access these features without the prerequisite expertise required to train such models. The application developers can concentrate on building functionality without having to spend a lot of time on deep learning tasks. In this chapter, we studied various tools that are available in Amazon Rekognition and also learned how to make API calls and read the response JSON. Moreover, we also studied various applications where these tools can be useful. </p>
<p>In the next chapter, we will demonstrate how you can build automated chat bots using a service called Amazon Lex.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercise</h1>
                </header>
            
            <article>
                
<ol>
<li>Create an app using Python where you can pass a photo of a group and detect what the mood of the room was at that time. Provide details on what your code detected based on the facial analysis tools and how you summarized the results to find the mood in the photo. </li>
<li>Create a tool that would recognize the actors in a movie clip. Provide the time at which the actors appeared on the screen. </li>
</ol>


            </article>

            
        </section>
    </body></html>