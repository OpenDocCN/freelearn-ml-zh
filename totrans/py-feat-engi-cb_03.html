<html><head></head><body>
		<div><h1 id="_idParaDest-95" class="chapter-number"><a id="_idTextAnchor351"/><st c="0">3</st></h1>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor352"/><a id="_idTextAnchor353"/><st c="2">Transforming Numerical Variables</st></h1>
			<p><st c="34">The statistical methods that are used in data analysis make certain assumptions about the data. </st><st c="131">For example, in the general linear model, it is assumed that the values of the dependent variable (the target) are independent, that there is a linear relationship between the target and the independent (predictor) variables, and that the residuals – that is, the difference between the predictions and the real values of the target – are normally distributed and centered at </st><code><st c="507">0</st></code><st c="508">. When these assumptions are not met, the resulting probabilistic statements might not be accurate. </st><st c="608">To correct for failure in the assumptions and thus improve the performance of the models, we can transform variables before </st><st c="732">the analysis.</st></p>
			<p><st c="745">When we transform a variable, we replace its original values with a function of that variable. </st><st c="841">Transforming variables with mathematical functions helps reduce variable skewness, improves the value spread, and sometimes unmasks linear and additive relationships between predictors and the target. </st><st c="1042">Commonly used mathematical transformations include the logarithm, reciprocal, power, and square and cube root transformations, as well as the Box-Cox and Yeo-Johnson transformations. </st><st c="1225">This set of transformations </st><a id="_idTextAnchor354"/><st c="1253">is commonly referred</st><a id="_idIndexMarker200"/><st c="1273"> to as </st><strong class="bold"><st c="1280">variance stabilizing transformations</st></strong><st c="1316">. Variance stabilizing transformations intend to bring the distribution of the variable to a more symmetric – that is, Gaussian – shape. </st><st c="1453">In this chapter, we will discuss when to use each transformation and then implement them using NumPy, scikit-learn, </st><st c="1569">and Feature-engine.</st></p>
			<p><st c="1588">This chapter contains the </st><st c="1615">following recipes:</st></p>
			<ul>
				<li><st c="1633">Transforming variables with the </st><st c="1666">logarithm function</st></li>
				<li><st c="1684">Transforming variables with the </st><st c="1717">reciprocal function</st></li>
				<li><st c="1736">Using the square root to </st><st c="1762">transform variables</st></li>
				<li><st c="1781">Using </st><st c="1788">power transformations</st></li>
				<li><st c="1809">Performing </st><st c="1821">Box-Cox transformatio</st><a id="_idTextAnchor355"/><st c="1842">ns</st></li>
				<li><st c="1845">Performing </st><st c="1857">Yeo-Johnson transformatio</st><a id="_idTextAnchor356"/><st c="1882">ns</st><a id="_idTextAnchor357"/><a id="_idTextAnchor358"/></li>
			</ul>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor359"/><st c="1885">Transforming variables with the logarithm function</st></h1>
			<p><st c="1936">The</st><a id="_idIndexMarker201"/><st c="1940"> logarithm function is a</st><a id="_idIndexMarker202"/><st c="1964"> powerful transformation for dealing with positive data wi</st><a id="_idTextAnchor360"/><st c="2022">th a right-skewed distribution (observations accumulate at lower values of the variable). </st><st c="2113">A comm</st><a id="_idTextAnchor361"/><st c="2119">on example is the </st><code><st c="2138">income</st></code><st c="2144"> variable, with a heavy accumulation of values toward lower salaries. </st><st c="2214">The logarithm transformation has a strong effect on the shape of the </st><st c="2283">variable distribution.</st></p>
			<p><st c="2305">In this </st><a id="_idIndexMarker203"/><st c="2314">recipe, we will perform logarithmic transformation using NumPy, scikit-learn, and Feature-engine. </st><st c="2412">We will also create a diagnostic plot function to evaluate the effect of the transformation on the </st><st c="2511">variable distributi</st><a id="_idTextAnchor362"/><a id="_idTextAnchor363"/><st c="2530">on.</st></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor364"/><st c="2534">Getting ready</st></h2>
			<p><st c="2548">To evaluate the variable distribution and understand whether a transformation improves value spread and stabilizes the variance, we can visually inspect the data w</st><a id="_idTextAnchor365"/><st c="2712">ith histograms and </st><strong class="bold"><st c="2732">Quantile-Quantile</st></strong><st c="2749"> (</st><strong class="bold"><st c="2751">Q-Q</st></strong><st c="2754">) plots. </st><st c="2764">A Q-Q plot helps us determine whether two</st><a id="_idIndexMarker204"/><st c="2805"> variables show a similar distribution. </st><st c="2845">In a Q-Q plot, we plot the quantiles of one variable against the quantiles of the second variable. </st><st c="2944">If we plot the quantiles of the variable of interest against the expected quantiles of the normal distribution, then we can determine whether our variable is also normally distributed. </st><st c="3129">If the variable is normally distributed, the points in the Q-Q plot will fall along a </st><st c="3215">45-degree diagonal.</st></p>
			<p class="callout-heading"><st c="3234">Note</st></p>
			<p class="callout"><st c="3239">A quantile is the value below which there is a certain fraction of data points in the distribution. </st><st c="3340">Thus, the 20th quantile is the point in the distribution at which 20% of the observations fall below and 80% above </st><st c="3455">that va</st><a id="_idTextAnchor366"/><a id="_idTextAnchor367"/><st c="3462">lue.</st></p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor368"/><st c="3467">How to do it...</st></h2>
			<p><st c="3483">Let’s b</st><a id="_idTextAnchor369"/><st c="3491">egin</st><a id="_idIndexMarker205"/><st c="3496"> by importing the libr</st><a id="_idTextAnchor370"/><st c="3518">aries and getting the </st><st c="3541">dataset ready:</st></p>
			<ol>
				<li><st c="3555">Import the required Python libraries </st><st c="3593">and dataset:</st><pre class="source-code"><st c="3605">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from sklearn.datasets import fetch_california_housing</st></pre></li>				<li><st c="3758">Let’s</st><a id="_idIndexMarker206"/><st c="3764"> load the California housing dataset into a </st><st c="3808">pandas DataFrame:</st><pre class="source-code"><st c="3825">
X, y = fetch_california_housing(return_X_y=True,
    as_frame=True)</st></pre></li>				<li><st c="3889">Let’s explore the distributions of all the variables in the dataset by plotting histograms </st><st c="3981">with pandas:</st><pre class="source-code"><st c="3993">
X.hist(bins=30, figsize=(12, 12))
plt.show()</st></pre><p class="list-inset"><st c="4038">In the following output, we can see that the </st><code><st c="4084">MedInc</st></code><st c="4090"> variable shows </st><a id="_idTextAnchor371"/><st c="4106">a mild right-skewed distribution, var</st><a id="_idTextAnchor372"/><st c="4143">iables such as </st><code><st c="4159">AveRooms</st></code><st c="4167"> and </st><code><st c="4172">Population</st></code><st c="4182"> are heavily right-skewed, and the </st><code><st c="4217">HouseAge</st></code><st c="4225"> variable shows an even spread of values across </st><st c="4273">its </st><a id="_idTextAnchor373"/><st c="4277">range:</st></p></li>			</ol>
			<p class="IMG---Figure"><st c="4283">figure 3</st></p>
			<div><div><img src="img/B22396_03_01.jpg" alt="Figure 3.1 – Histograms with the distribution of the numerical variables"/><st c="4292"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="4684">Figure 3.1 – Histograms with the distribution of the numerical variables</st></p>
			<ol>
				<li value="4"><st c="4756">To </st><a id="_idIndexMarker207"/><st c="4760">evaluate</st><a id="_idIndexMarker208"/><st c="4768"> the effect of the transformation on the variable distribution</st><a id="_idTextAnchor374"/><st c="4830">, we’ll create a f</st><a id="_idTextAnchor375"/><st c="4848">unction that takes a DataFrame and a variable name as inputs and plots a histogram next to a </st><st c="4942">Q-Q plot:</st><pre class="source-code"><st c="4951">
def diagnostic_plots(df, variable):
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    df[variable].hist(bins=30)
    plt.title(f"Histogram of {variable}")
    plt.subplot(1, 2, 2)
    stats.probplot(df[variable], dist="norm",
        plot=plt)
    plt.title(f"Q-Q plot of {variable}")
    plt.show()</st></pre></li>				<li><st c="5221">Let’s </st><a id="_idIndexMarker209"/><st c="5228">plot the</st><a id="_idIndexMarker210"/><st c="5236"> distribution of the </st><code><st c="5257">MedInc</st></code><st c="5263"> variable with the function from </st><em class="italic"><st c="5296">step 4</st></em><st c="5302">:</st><pre class="source-code"><st c="5304">
diagnostic_plots(X, "MedInc")</st></pre><p class="list-inset"><st c="5334">The following output shows that </st><code><st c="5367">MedInc</st></code><st c="5373"> has a </st><st c="5380">right-skewed distr</st><a id="_idTextAnchor376"/><st c="5398">ibution:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_02.jpg" alt="Figure 3.2 – A histogram and Q-Q plot of the MedInc variable"/><st c="5407"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="5547">Figure 3.2 – A histogram and Q-Q plot of the MedInc variable</st></p>
			<p class="list-inset"><st c="5607">Now, let’s transform the data with </st><st c="5643">the logarithm:</st></p>
			<ol>
				<li value="6"><st c="5657">Firs</st><a id="_idTextAnchor377"/><st c="5662">t, let’s mak</st><a id="_idTextAnchor378"/><st c="5675">e a copy of the </st><st c="5692">original DataFrame:</st><pre class="source-code"><st c="5711">
X_tf = X.copy()</st></pre><p class="list-inset"><st c="5727">We’ve </st><a id="_idIndexMarker211"/><st c="5734">created a copy so that we can modify the values in the copy and not in the original DataFrame, which we need for the rest of </st><st c="5859">this recipe.</st></p></li>			</ol>
			<p class="callout-heading"><st c="5871">Note</st></p>
			<p class="callout"><st c="5876">If we execute </st><code><st c="5891">X_tf = X</st></code><st c="5899"> instead of using pandas’ </st><code><st c="5925">copy()</st></code><st c="5931">function, </st><code><st c="5942">X_tf</st></code><st c="5946"> will not be a copy of the DataFrame; instead, it will be another view of the same data. </st><st c="6035">Therefore, changes made in </st><code><st c="6062">X_tf</st></code><st c="6066"> will be reflected in </st><code><st c="6088">X</st></code> <st c="6089">as well.</st></p>
			<ol>
				<li value="7"><st c="6097">Let’s </st><a id="_idIndexMarker212"/><st c="6104">make a list with the variables that we want </st><st c="6148">to transform:</st><pre class="source-code"><st c="6161">
vars = ["MedInc", "AveRooms", "AveBedrms",
    "Populati</st><a id="_idTextAnchor379"/><st c="6214">on"]</st></pre></li>				<li><st c="6219">Let’s apply the logarithmic transformation with NumPy to the variables from </st><em class="italic"><st c="6296">step 7</st></em><st c="6302"> a</st><a id="_idTextAnchor380"/><st c="6304">nd capture the transformed variables in the </st><st c="6348">new DataFrame:</st><pre class="source-code"><st c="6362">
X_tf[vars] = np.log(X[vars])</st></pre></li>			</ol>
			<p class="callout-heading"><st c="6391">Note</st></p>
			<p class="callout"><st c="6396">Remember that the logarithm transformation can only be applied to strictly positive variables. </st><st c="6492">If the variables have zero or negative values, sometimes, it is useful to add a constant to make those values positive. </st><st c="6612">We could add a constant value of </st><code><st c="6645">1 </st></code><st c="6647">using </st><code><st c="6653">X_tf[vars] = np.log(X[vars] + </st></code><code><st c="6683">1)</st></code><st c="6685">.</st></p>
			<ol>
				<li value="9"><st c="6686">Let’s check the distribution of </st><code><st c="6719">MedInc</st></code><st c="6725"> after the transformation with the diagnostic function from </st><em class="italic"><st c="6785">step 4</st></em><st c="6791">:</st><pre class="source-code"><st c="6793">
diagnostic_plots(X_tf, "MedInc")</st></pre><p class="list-inset"><st c="6826">In the following output, we can see that the logarithmic transformation returned a more evenly distributed variable that better approximates the theoretical normal distribution in the </st><a id="_idTextAnchor381"/><st c="7011">Q-Q plot:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_03.jpg" alt="Figure 3.3 – A histogram and Q-Q plot of the MedInc variable after the logarithm transformation﻿"/><st c="7020"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="7187">Figure 3.3 – A histogram and Q-Q plot of the MedInc variable after the logarithm transformation</st><a id="_idTextAnchor382"/></p>
			<p class="list-inset"><st c="7282">Go ahead </st><a id="_idIndexMarker213"/><st c="7292">an</st><a id="_idTextAnchor383"/><st c="7294">d plot the other transformed variables to familiarize yourself with the effect of the logarithm transformation </st><st c="7406">on distributions.</st></p>
			<p class="list-inset"><st c="7423">Now, let’s </st><a id="_idIndexMarker214"/><st c="7435">apply the logarithmic transformation </st><st c="7472">with </st><code><st c="7477">scikit-learn</st></code><st c="7489">.</st></p>
			<ol>
				<li value="10"><st c="7490">Let’s </st><st c="7497">import </st><code><st c="7504">FunctionTransformer()</st></code><st c="7525">:</st><pre class="source-code"><st c="7527">
from sklearn.preprocessing import FunctionTransformer</st></pre><p class="list-inset"><st c="7581">Before we proceed, we need to take a copy of the original dataset, as we did in </st><em class="italic"><st c="7662">step 6</st></em><st c="7668">.</st></p></li>				<li><st c="7669">We’ll set up the transformer to apply the logarithm and to be able to revert the transformed variable to its </st><st c="7779">original representation:</st><pre class="source-code"><st c="7803">
transformer = FunctionTransformer(np.log,
    inverse_func=np.exp)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="7866">Note</st></p>
			<p class="callout"><st c="7871">If we set up </st><code><st c="7885">FunctionTransformer()</st></code><st c="7906"> with the default parameter, </st><code><st c="7935">validate=False</st></code><st c="7949">, we don’t need to fit the transformer before transforming the data. </st><st c="8018">If we set </st><code><st c="8028">validate</st></code><st c="8036"> to </st><code><st c="8040">True</st></code><st c="8044">, the transformer will check the data input to the </st><code><st c="8095">fit</st></code><st c="8098"> method. </st><st c="8107">The latter is useful when fitting the transformer with a DataFrame so that it learns and stores the </st><st c="8207">variable n</st><a id="_idTextAnchor384"/><st c="8217">ames.</st></p>
			<ol>
				<li value="12"><st c="8223">Let’s tr</st><a id="_idTextAnchor385"/><st c="8232">ansform the positive variables from </st><em class="italic"><st c="8269">step 7</st></em><st c="8275">:</st><pre class="source-code"><st c="8277">
X_tf[vars] = transformer.transform(X[vars])</st></pre></li>			</ol>
			<p class="callout-heading"><st c="8321">Note</st></p>
			<p class="callout"><st c="8326">Scikit-learn transformers return NumPy arrays and transform the entire DataFrame by default. </st><st c="8420">In this case, we assigned the results of the array directly to our existing DataFrame. </st><st c="8507">We can change the returned format through the </st><code><st c="8553">set_output</st></code><st c="8563"> method and we can restrict the variables to transform </st><st c="8618">with </st><code><st c="8623">ColumnTransformer()</st></code><st c="8642">.</st></p>
			<p class="list-inset"><st c="8643">Check </st><a id="_idIndexMarker215"/><st c="8650">the results of the transformation</st><a id="_idIndexMarker216"/><st c="8683"> with the diagnostic function from </st><em class="italic"><st c="8718">step 4</st></em><st c="8724">.</st></p>
			<ol>
				<li value="13"><st c="8725">Let’s now revert the transformation to the original </st><st c="8778">variable representation:</st><pre class="source-code"><st c="8802">
X_tf[vars] = transformer.inverse_transform(X_tf[vars])</st></pre><p class="list-inset"><st c="8857">If you check the distribution by executing </st><code><st c="8901">diagnostic_plots(X_tf, "MedInc")</st></code><st c="8933">, you should see a plot that is identical to that returned by </st><em class="italic"><st c="8995">step 5</st></em><st c="9001">.</st></p></li>			</ol>
			<p class="callout-heading"><st c="9002">Note</st></p>
			<p class="callout"><st c="9007">To add a constant value to the variables, in case they are not strictly positive, use </st><code><st c="9094">transformer = FunctionTransformer(lambda x: np.log(x + </st></code><code><st c="9149">1)</st><a id="_idTextAnchor386"/><st c="9151">)</st></code><st c="9153">.</st></p>
			<p class="list-inset"><st c="9154">Now, let’s apply the logarithm transformation </st><st c="9201">with Feature-</st><a id="_idTextAnchor387"/><st c="9214">engine.</st></p>
			<ol>
				<li value="14"><st c="9222">Let’s import </st><st c="9236">the </st><code><st c="9240">LogTransformer()</st></code><st c="9256">:</st><pre class="source-code"><st c="9258">
from feature_engine.transformation import LogTransformer</st></pre></li>				<li><st c="9315">We’ll set up the transformer to transform the variables from </st><em class="italic"><st c="9377">step 7</st></em><st c="9383"> and then fit the transformer to </st><st c="9416">the dataset:</st><pre class="source-code"><st c="9428">
lt = LogTransformer(variables = vars)
lt.fit(X)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="9476">Note</st></p>
			<p class="callout"><st c="9481">If the </st><code><st c="9489">variables</st></code><st c="9498"> argument is left as </st><code><st c="9519">None</st></code><st c="9523">, </st><code><st c="9525">LogTransformer()</st></code><st c="9541"> applies the logarithm to all the numerical variables found during </st><code><st c="9608">fit()</st></code><st c="9613">. Alternatively, we can indicate which variables to modify, as we did in </st><em class="italic"><st c="9686">step 15</st></em><st c="9693">.</st></p>
			<ol>
				<li value="16"><st c="9694">Finally, let’s</st><a id="_idIndexMarker217"/><st c="9709"> transform</st><a id="_idIndexMarker218"/> <st c="9719">the data:</st><pre class="source-code"><st c="9729">
X_tf = lt.transform(X)</st></pre><p class="list-inset"><code><st c="9752">X_tf</st></code><st c="9757"> is a copy of the </st><code><st c="9775">X</st></code><st c="9776"> DataFrame, where the variables from </st><em class="italic"><st c="9813">step 7</st></em><st c="9819"> are transformed with </st><st c="9841">the logarithm.</st></p></li>				<li><st c="9855">We can also revert </st><a id="_idTextAnchor388"/><st c="9875">the transformed variables to their </st><st c="9910">original representation:</st><pre class="source-code"><st c="9934">
X_tf = lt.inverse_transfo</st><a id="_idTextAnchor389"/><st c="9960">rm(X_tf)</st></pre><p class="list-inset"><st c="9969">If you check the distribution of the variables after </st><em class="italic"><st c="10023">step 17</st></em><st c="10030">, they should be identical to those of the </st><st c="10073">original data.</st></p></li>			</ol>
			<p class="callout-heading"><st c="10087">Note</st></p>
			<p class="callout"><st c="10092">Feature-engine has a dedicated transformer that adds constant values to the variables before the applying the logarithm transformation. </st><st c="10229">Check the </st><em class="italic"><st c="10239">There’s more…</st></em><st c="10252"> section later in this recipe fo</st><a id="_idTextAnchor390"/><a id="_idTextAnchor391"/><st c="10284">r </st><st c="10287">more details.</st></p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor392"/><st c="10300">How it works...</st></h2>
			<p><st c="10316">In this recipe, we applied the logarithm transformation to a subset of positive variables using NumPy, scikit-learn, </st><st c="10434">and Feature-engine.</st></p>
			<p><st c="10453">To compare</st><a id="_idIndexMarker219"/><st c="10464"> the effect of the transformation on the variable distribution, we created a diagnostic function to plot a histogram next to a Q-Q plot. </st><st c="10601">To create the Q-Q plot, we used </st><code><st c="10633">scipy.stats.probplot()</st></code><st c="10655">, which plotted the quantiles of the variable of interest in the </st><em class="italic"><st c="10720">y</st></em><st c="10721"> axis versus the quantiles of a theoretical normal distribution, which we indicated by setting the </st><code><st c="10820">dist</st></code><st c="10824"> parameter to </st><code><st c="10838">norm</st></code><st c="10842"> in the </st><em class="italic"><st c="10850">x</st></em><st c="10851"> axis. </st><st c="10858">We used </st><code><st c="10866">matplotlib</st></code><st c="10876"> to display the plot by setting the </st><code><st c="10912">plot</st></code><st c="10916"> parameter </st><st c="10927">to </st><code><st c="10930">plt</st></code><st c="10933">.</st></p>
			<p><st c="10934">With </st><code><st c="10940">plt.figure()</st></code><st c="10952"> and </st><code><st c="10957">figsize</st></code><st c="10964">, we adjusted the size of the figure and, with </st><code><st c="11011">plt.subplot()</st></code><st c="11024">, we organized the two plots in </st><code><st c="11056">one</st></code><st c="11059"> row with </st><code><st c="11069">two</st></code><st c="11072"> columns – that is, one plot next to the other. </st><st c="11120">The numbers within </st><code><st c="11139">plt.subplot()</st></code><st c="11152"> indicated the number of rows, the number of columns, and the place of the plot in the figure, respectively. </st><st c="11261">We placed the histogram in position 1 and the Q-Q plot in position 2 – that is, left and </st><st c="11350">right, respe</st><a id="_idTextAnchor393"/><st c="11362">ctively.</st></p>
			<p><st c="11371">To test the </st><a id="_idIndexMarker220"/><st c="11384">function, we plotted a histogram and a Q-Q plot for the </st><code><st c="11440">MedInc</st></code><st c="11446"> variable before t</st><a id="_idTextAnchor394"/><st c="11464">he transformation and observed that </st><code><st c="11501">MedInc</st></code><st c="11507"> was not normally distributed. </st><st c="11538">Most observations were at the left of the histogram and the values deviated from the 45-degree line in the Q-Q plot at both ends of </st><st c="11670">the distribution.</st></p>
			<p><st c="11687">Next, using </st><code><st c="11700">np.log()</st></code><st c="11708">, we applied the logarithm to a slice of the DataFrame with four positive variables. </st><st c="11793">To evaluate the effect of the transformation, we plotted a histogram and Q-Q plot of the transformed </st><code><st c="11894">MedInc</st></code><st c="11900"> variable. </st><st c="11911">We observed that, after the logarithm transformation, the values were more centered in the histogram and that, in the Q-Q plot, they only deviated from the 45-degree line toward the ends of </st><st c="12101">the distribution.</st></p>
			<p><st c="12118">Next, we used the </st><code><st c="12137">FunctionTransformer()</st></code><st c="12158"> from scikit-learn, which applies any user-defined function to a dataset. </st><st c="12232">We passed </st><code><st c="12242">np.log()</st></code><st c="12250"> as an argument to apply the logarithm transformation and NumPy’s </st><code><st c="12316">exp()</st></code><st c="12321"> for the inverse transformation to </st><code><st c="12356">FunctionTransfomer()</st></code><st c="12376">. With the </st><code><st c="12387">transform()</st></code><st c="12398"> method, we transformed a slice of the DataFrame with the positive variables by using the logarithm. </st><st c="12499">With </st><code><st c="12504">inverse_transform()</st></code><st c="12523">, we reverted the variable values to their </st><st c="12566">original representation.</st></p>
			<p><st c="12590">Finally, we used Feature-engine’s </st><code><st c="12625">LogTransformer()</st></code><st c="12641"> and specified the variables to transform in a list using the </st><code><st c="12703">variables</st></code><st c="12712"> argument. </st><st c="12723">With </st><code><st c="12728">fit()</st></code><st c="12733">, the transformer checked that the variables were numerical and positive, and with </st><code><st c="12816">transform()</st></code><st c="12827">, it applied </st><code><st c="12840">np.log()</st></code><st c="12848"> under the hood to transform the selected variables. </st><st c="12901">With </st><code><st c="12906">inverse_transform()</st></code><st c="12925">, we reverted the transformed variables to their </st><st c="12974">original</st><a id="_idTextAnchor395"/><a id="_idTextAnchor396"/><st c="12982"> representations.</st></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor397"/><st c="12999">There’s more…</st></h2>
			<p><st c="13013">Feature-engine has a</st><a id="_idIndexMarker221"/><st c="13034"> dedicated</st><a id="_idIndexMarker222"/><st c="13044"> transformer for adding a constant</st><a id="_idTextAnchor398"/><st c="13078"> value to variables that are not strictly positive, before applying the logarithm: </st><code><st c="13161">LogCpTransformer()</st></code><st c="13179">. </st><code><st c="13181">LogCpTransformer()</st></code><st c="13199"> can:</st></p>
			<ul>
				<li><st c="13204">Add the same constant to </st><st c="13230">all variables</st></li>
				<li><st c="13243">Automatically identify and add the minimum value required to make the </st><st c="13314">variables positive</st></li>
				<li><st c="13332">Add different values defined by the user to </st><st c="13377">different variables.</st></li>
			</ul>
			<p><st c="13397">You can find a code implementation of </st><code><st c="13436">LogCpTransformer()</st></code><st c="13454"> in this book’s GitHub </st><st c="13477">repository: </st><a href="https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch03-variable-transformation/Recipe-1-logarithmic-transformation.ipynb"><st c="13489">https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch03-variable-transformation/Recipe-1-logarithmic-t</st><st c="13635">ransformation.ipynb</st></a></p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor401"/><st c="13655">Transforming variables with the reciprocal function</st></h1>
			<p><st c="13707">The reciprocal function</st><a id="_idIndexMarker223"/><st c="13731"> is defined as 1/x</st><a id="_idTextAnchor402"/><st c="13749">. It is often useful when we have rati</st><a id="_idTextAnchor403"/><st c="13787">os – that is, values resulting from the division of two variables. </st><st c="13855">Examples of </st><a id="_idIndexMarker224"/><st c="13867">this are </st><strong class="bold"><st c="13876">population density</st></strong><st c="13894"> – that is, people per area – and, as we will see in this recipe, </st><strong class="bold"><st c="13960">house occupancy</st></strong><st c="13975"> – that is, the</st><a id="_idIndexMarker225"/><st c="13990"> number of occupants </st><st c="14011">per house.</st></p>
			<p><st c="14021">The reciprocal</st><a id="_idIndexMarker226"/><st c="14036"> transformation is not defined for the </st><code><st c="14075">0</st></code><st c="14076"> value, and although it is defined for negative values, it is mainly useful for transforming </st><st c="14169">positive variables.</st></p>
			<p><st c="14188">In this recipe, we will implement the reciprocal transformation using </st><code><st c="14259">NumPy</st></code><st c="14264">, </st><code><st c="14266">scikit-learn</st></code><st c="14278">, and </st><code><st c="14284">Feature-engine</st></code><st c="14298">, and compare its effect on variable distribution using his</st><a id="_idTextAnchor404"/><a id="_idTextAnchor405"/><st c="14357">tograms and a </st><st c="14372">Q-Q plot</st><a id="_idTextAnchor406"/><st c="14380">.</st></p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor407"/><st c="14381">How to do it...</st></h2>
			<p><st c="14397">Let’s</st><a id="_idTextAnchor408"/><st c="14403"> begin by </st><a id="_idIndexMarker227"/><st c="14413">importing the libraries and getting the </st><st c="14453">dataset ready:</st></p>
			<ol>
				<li><st c="14467">Import the required Python libraries </st><st c="14505">and data:</st><pre class="source-code"><st c="14514">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
from sklearn.datasets import fetch_california_housing</st></pre></li>				<li><st c="14667">Let’s load</st><a id="_idIndexMarker228"/><st c="14678"> the California </st><st c="14694">housing dataset:</st><pre class="source-code"><st c="14710">
X, y = fetch_california_housing(return_X_y=True,
    as_frame=True)</st></pre></li>				<li><st c="14774">To evaluate variable distributions, we’ll create a function that takes a DataFrame and a variable name as inputs and plots a histogram next to a </st><st c="14920">Q-Q plot:</st><pre class="source-code"><st c="14929">
def diagnostic_plots(df, variable):
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    df[variable].hist(bins=30)
    plt.title(f"Histogram of {variable}")
    plt.subplot(1, 2, 2)
    stats.probplot(df[variable], dist="norm",
        plot=plt)
    plt.title(f"Q-Q plot of {variable}")
</st><a id="_idTextAnchor409"/><st c="15189">    plt.show()</st></pre></li>				<li><st c="15199">Now, le</st><a id="_idTextAnchor410"/><st c="15207">t’s plot the distribution of the</st><a id="_idTextAnchor411"/> <code><st c="15240">AveOccup</st></code><st c="15249"> variable, which specifies the average occupancy of </st><st c="15301">the house:</st><pre class="source-code"><st c="15311">
diagnostic_plots(X, "AveOccup")</st></pre><p class="list-inset"><st c="15343">The </st><code><st c="15348">AveOccup</st></code><st c="15356"> variable </st><a id="_idIndexMarker229"/><st c="15366">shows a very strong</st><a id="_idIndexMarker230"/><st c="15385"> right-skewed distribution, as show</st><a id="_idTextAnchor412"/><st c="15420">n in the </st><st c="15430">following output:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_04.jpg" alt="Figure 3.4 – A histogram and Q-Q plot of the AveOccup variable"/><st c="15447"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="15699">Figure 3.4 – A histogram and Q-Q plot of the AveOccup variable</st></p>
			<p class="callout-heading"><st c="15761">Note</st></p>
			<p class="callout"><st c="15766">Th</st><a id="_idTextAnchor413"/><st c="15769">e </st><code><st c="15772">AveOccup</st></code><st c="15780"> variable</st><a id="_idIndexMarker231"/><st c="15789"> refers to the average number of household members – that is, the ratio between t</st><a id="_idTextAnchor414"/><st c="15870">he number of people and the nu</st><a id="_idTextAnchor415"/><st c="15901">mber of houses in a certain area. </st><st c="15936">This is a promising variable for a reciprocal transformation. </st><st c="15998">You can find more details about the variables and the dataset by executing </st><code><st c="16073">data = fetch_california_housing()</st></code><st c="16106"> followed </st><st c="16116">by </st><code><st c="16119">print(data.DESCR)</st></code><st c="16136">.</st></p>
			<p class="list-inset"><st c="16137">Now, let’s apply the reciprocal transformation </st><st c="16185">with NumPy.</st></p>
			<ol>
				<li value="5"><st c="16196">First, let’s make a copy of the original DataFrame so that we can modify the values in</st><a id="_idIndexMarker232"/><st c="16283"> the copy and not in the original one, which we will need for the rest of </st><st c="16357">this recipe:</st><pre class="source-code"><st c="16369">
X_tf = X.copy()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="16385">Note</st></p>
			<p class="callout"><st c="16390">Remember that executing </st><code><st c="16415">X_tf = X</st></code><st c="16423"> instead of using pandas’ </st><code><st c="16449">copy()</st></code><st c="16455"> creates an additional view of the same data. </st><st c="16501">Therefore, changes that are made in </st><code><st c="16537">X_tf</st></code><st c="16541"> will be r</st><a id="_idTextAnchor416"/><st c="16551">eflected in </st><code><st c="16564">X</st></code> <st c="16565">as well</st><a id="_idTextAnchor417"/><st c="16572">.</st></p>
			<ol>
				<li value="6"><st c="16573">Let’s</st><a id="_idIndexMarker233"/><st c="16579"> apply the reciprocal transformation to the </st><code><st c="16623">AveOccup</st></code><st c="16631"> variable:</st><pre class="source-code"><st c="16641">
X_tf["AveOccup"] = np.reciprocal(X_tf["AveOccup"])</st></pre></li>				<li><st c="16692">Let’s check the distribution of the </st><code><st c="16729">AveOccup</st></code><st c="16737"> variable after the transformation with the diagnostic function we created in </st><em class="italic"><st c="16815">step 3</st></em><st c="16821">:</st><pre class="source-code"><st c="16823">
diagnostic_plots(X_tf, "AveOccup")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="16858">Note</st></p>
			<p class="callout"><st c="16863">After the transformation, </st><code><st c="16890">AveOccup</st></code><st c="16898"> is now the ratio of the number of houses and the number of people in a certain area – in other words, houses </st><st c="17008">per citizen.</st></p>
			<p class="list-inset"><st c="17020">Here, we can see a dramatic change in the distribution of the </st><code><st c="17083">AveOccup</st></code><st c="17091"> variable after th</st><a id="_idTextAnchor418"/><st c="17109">e </st><st c="17112">reciprocal transformation:</st></p>
			<div><div><img src="img/B22396_03_05.jpg" alt="Figure 3.5 – A histogram and Q-Q plot of the AveOccup variable after the reciproc﻿al transfo﻿rmation"/><st c="17138"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="17309">Figure 3.5 – A histogram and Q-Q plot of the AveOccup variable after the reciproc</st><a id="_idTextAnchor419"/><st c="17390">al transfo</st><a id="_idTextAnchor420"/><st c="17401">rmation</st></p>
			<p class="list-inset"><st c="17409">Now, let’s apply </st><a id="_idIndexMarker234"/><st c="17427">the reciprocal transformation </st><st c="17457">with </st><code><st c="17462">scikit-learn</st></code><st c="17474">.</st></p>
			<ol>
				<li value="8"><st c="17475">Let’s import </st><st c="17489">the </st><code><st c="17493">FunctionTransformer()</st></code><st c="17514">:</st><pre class="source-code"><st c="17516">
from sklearn.preprocessing import FunctionTransformer</st></pre></li>				<li><st c="17570">Let’s set up the transformer by passing </st><code><st c="17611">np.reciprocal</st></code><st c="17624"> as </st><st c="17628">an argument:</st><pre class="source-code"><st c="17640">
transformer = FunctionTransformer(np.reciprocal)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="17689">Note</st></p>
			<p class="callout"><st c="17694">By default, </st><code><st c="17707">FunctionTransformer()</st></code><st c="17728"> does not need to be fit before tran</st><a id="_idTextAnchor421"/><st c="17764">sforming </st><st c="17774">the data.</st></p>
			<ol>
				<li value="10"><st c="17783">Now, let’s </st><a id="_idIndexMarker235"/><st c="17795">make a copy of the orig</st><a id="_idTextAnchor422"/><st c="17818">inal dataset and transform </st><st c="17846">the variable:</st><pre class="source-code"><st c="17859">
X_tf = X.copy()
X_tf["AveOccup"] = transformer.transform(
    X["AveOccup"])</st></pre><p class="list-inset"><st c="17932">You can check the effect of the transformation using the function from </st><em class="italic"><st c="18004">step 3</st></em><st c="18010">.</st></p></li>			</ol>
			<p class="callout-heading"><st c="18011">Note</st></p>
			<p class="callout"><st c="18016">The inverse transformation of the reciprocal function is also the reciprocal function. </st><st c="18104">Hence, if you re-apply </st><code><st c="18127">transform()</st></code><st c="18138"> to the transformed data, you will revert it to its original representation. </st><st c="18215">A better practice would be to set the </st><code><st c="18253">inverse_transform</st></code><st c="18270"> parameter of the </st><code><st c="18288">FunctionTransformer()</st></code><st c="18309"> to </st><code><st c="18313">np.reciprocal</st></code> <st c="18326">as well.</st></p>
			<p class="list-inset"><st c="18335">Now, let’s apply</st><a id="_idIndexMarker236"/><st c="18352"> the reciprocal transformation </st><st c="18383">with </st><code><st c="18388">feature-engine</st></code><st c="18402">.</st></p>
			<ol>
				<li value="11"><st c="18403">Let’s import </st><st c="18417">the </st><code><st c="18421">ReciprocalTransformer()</st></code><st c="18444">:</st><pre class="source-code"><st c="18446">
from feature_engine.transformation import ReciprocalTransformer</st></pre></li>				<li><st c="18510">Let’s set up the transformer to modify the </st><code><st c="18554">AveOccup</st></code><st c="18562"> variable and then fit it to </st><st c="18591">the dataset:</st><pre class="source-code"><st c="18603">
rt = ReciprocalTransformer(variables=»AveOccup»)
rt.fit(X)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="18662">Note</st></p>
			<p class="callout"><st c="18667">If the </st><code><st c="18675">variables</st></code><st c="18684"> argument is set to </st><code><st c="18704">None</st></code><st c="18708">, the transformer applies the reciprocal function to </st><em class="italic"><st c="18761">all the numerical variables</st></em><st c="18788"> in the dataset. </st><st c="18805">If some of the variables contain a </st><code><st c="18840">0</st></code><st c="18841"> value, the transfo</st><a id="_idTextAnchor423"/><st c="18860">rmer will rais</st><a id="_idTextAnchor424"/><st c="18875">e </st><st c="18878">an error.</st></p>
			<ol>
				<li value="13"><st c="18887">Let’s</st><a id="_idIndexMarker237"/><st c="18893"> transform the selected variable in </st><st c="18929">our dataset:</st><pre class="source-code"><st c="18941">
X_tf = rt.transform(X)</st></pre><p class="list-inset"><code><st c="18964">ReciprocalTransformer()</st></code><st c="18988"> will return a new pandas DataFrame containing the original variables, where the variable indicated in </st><em class="italic"><st c="19091">step 12</st></em><st c="19098"> is transform</st><a id="_idTextAnchor425"/><a id="_idTextAnchor426"/><st c="19111">ed with the </st><st c="19124">reciprocal function.</st></p></li>			</ol>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor427"/><st c="19144">How it works...</st></h2>
			<p><st c="19160">In this recipe, we applied the reciprocal transformation using NumPy, scikit-learn, </st><st c="19245">and Feature-engine.</st></p>
			<p><st c="19264">To evaluate the variable distribution, we used the function to plot a histogram next to a Q-Q plot that we described in the </st><em class="italic"><st c="19389">How it works…</st></em><st c="19402"> section of the </st><em class="italic"><st c="19418">Transforming variables with the logarithm function</st></em><st c="19468"> recipe earlier in </st><st c="19487">this chapter.</st></p>
			<p><st c="19500">We plotted the histogram and Q-Q plot of the </st><code><st c="19546">AveOccup</st></code><st c="19554"> variable, which showed a heavy right-skewed distribution; most of its values were at the left of the histogram and they deviated from the 45-degree line toward the right end of the distribution in the </st><st c="19756">Q-Q plot.</st></p>
			<p><st c="19765">To carry out the </st><a id="_idIndexMarker238"/><st c="19783">reciprocal transformation, we applied </st><code><st c="19821">np.reciprocal()</st></code><st c="19836"> to the variable. </st><st c="19854">After the transformation, </st><code><st c="19880">AveOccup</st></code><st c="19888">’s va</st><a id="_idTextAnchor428"/><st c="19894">lues were more evenly d</st><a id="_idTextAnchor429"/><st c="19918">istributed across the value range and followed the theoretical quantiles of the normal distribution in the Q-Q plot </st><st c="20035">more closely.</st></p>
			<p><st c="20048">Next, we used </st><code><st c="20063">np.reciprocal()</st></code><st c="20078"> with scikit-learn’s </st><code><st c="20099">FunctionTransformer()</st></code><st c="20120">. The </st><code><st c="20126">transform()</st></code><st c="20137"> method applied </st><code><st c="20153">np.reciprocal()</st></code><st c="20168"> to </st><st c="20172">the dataset.</st></p>
			<p class="callout-heading"><st c="20184">Note</st></p>
			<p class="callout"><st c="20189">To restrict the effect of </st><code><st c="20216">FunctionTransformer()</st></code><st c="20237"> to a group of variables, use the </st><code><st c="20271">ColumnTransformer()</st></code><st c="20290">. To change the output to a pandas DataFrame, set the transform output </st><st c="20361">to pandas.</st></p>
			<p><st c="20371">Finally, we</st><a id="_idIndexMarker239"/><st c="20383"> used Feature-engine’s </st><code><st c="20406">ReciprocalTransformer()</st></code><st c="20429"> to specifically modify one variable. </st><st c="20467">With </st><code><st c="20472">fit()</st></code><st c="20477">, the transformer checked that the variable was numerical. </st><st c="20536">With </st><code><st c="20541">transform()</st></code><st c="20552">, the transformer applied </st><code><st c="20578">np.reciprocal()</st></code><st c="20593"> under the hood to transform </st><st c="20622">the variable.</st></p>
			<p><st c="20635">Feature-engine’s </st><code><st c="20653">ReciprocalTransformer()</st></code><st c="20676"> provides functionality to revert the variable to its original representation out of the box via the </st><code><st c="20777">inverse_transform()</st></code><st c="20796"> method.</st></p>
			<p class="callout-heading"><st c="20804">Note</st></p>
			<p class="callout"><st c="20809">Using the transformers from scikit-learn or Feature-engine, instead of NumPy’s </st><code><st c="20889">reciprocal()</st></code><st c="20901"> function, allows us to apply the reciprocal function as an additional step of a feature engineering pipeline within the </st><code><st c="21022">Pipeline</st></code><st c="21030"> object </st><st c="21038">from scikit-learn.</st></p>
			<p><st c="21056">The difference </st><a id="_idIndexMarker240"/><st c="21072">between </st><code><st c="21080">FunctionTransformer()</st></code><st c="21101"> and </st><code><st c="21106">ReciprocalTransformer()</st></code><st c="21129"> is that the first one can apply any user-specified transformation, whereas the latter only applies the reciprocal function. </st><st c="21254">scikit-learn returns NumPy arrays by default and transforms all variables in the dataset. </st><st c="21344">Feature-engine’s transformer, on the other hand, returns pandas DataFrames and can modif</st><a id="_idTextAnchor430"/><st c="21432">y subsets of variables within the data</st><a id="_idTextAnchor431"/><a id="_idTextAnchor432"/><st c="21471"> without using </st><st c="21486">additional classes.</st></p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor433"/><st c="21505">Using the square root to transform variables</st></h1>
			<p><st c="21550">The</st><a id="_idIndexMarker241"/><st c="21554"> square root transformation, </st><strong class="bold"><st c="21583">√x</st></strong><st c="21585">, as well as its variations, the Anscombe transformation, </st><strong class="bold"><st c="21643">√(x+3/8)</st></strong><st c="21651">, and the Freeman-Tukey transf</st><a id="_idTextAnchor434"/><st c="21681">ormation,  </st><strong class="bold"><st c="21692">√x + √(x+1)</st></strong><st c="21703">, are variance stabilizing transformations that transform a variable with a P</st><a id="_idTextAnchor435"/><st c="21780">oisson distribution into one with an approximately standard Gaussian distribution. </st><st c="21864">The square root transformation is a form of power transformation where the exponent is </st><strong class="bold"><st c="21951">1/2</st></strong><st c="21954"> and is only defined for </st><st c="21979">positive values.</st></p>
			<p><st c="21995">The</st><a id="_idIndexMarker242"/><st c="21999"> Poisson distribution is a probability distribution that indicates the number of times an event is likely to occur. </st><st c="22115">In other words, it is a count distribution. </st><st c="22159">It is right-skewed and its variance equals its mean. </st><st c="22212">Examples of variables that could follow a Poisson distribution are the number of financial items of a customer, such as the number of current accounts or credit cards, the number of passengers in a vehicle, and the number of occupants in </st><st c="22450">a household.</st></p>
			<p><st c="22462">In this recipe, we will implement square root transformations using Num</st><a id="_idTextAnchor436"/><a id="_idTextAnchor437"/><st c="22534">Py, scikit-learn, </st><st c="22553">and Feature-engine.</st></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor438"/><st c="22572">How to do it...</st></h2>
			<p><st c="22588">We’ll start </st><a id="_idIndexMarker243"/><st c="22601">by creating a dataset with two variables whose values are drawn from a </st><st c="22672">Poisson distribution:</st></p>
			<ol>
				<li><st c="22693">Let’s begin by importing the </st><st c="22723">necessary libraries:</st><pre class="source-code"><st c="22743">
import numpy as np import pandas as pd
import scipy.stats as stats</st></pre></li>				<li><st c="22810">Let’s </st><a id="_idIndexMarker244"/><st c="22817">create a DataFrame with two variables drawn from a Poisson distribution with mean values of </st><code><st c="22909">2</st></code><st c="22910"> and </st><code><st c="22915">3</st></code><st c="22916">, respectively, and </st><code><st c="22936">10000</st></code><st c="22941"> observations:</st><pre class="source-code"><st c="22955">
df = pd.DataFrame()
df["counts1"] = stats.poisson.rvs(mu=3, size=10000)
df["counts2"] = stats.poisson.rvs(mu=2, size=10000)</st></pre></li>				<li><st c="23079">Let’s create a function that takes a </st><a id="_idTextAnchor439"/><st c="23117">DataFrame and a variab</st><a id="_idTextAnchor440"/><st c="23139">le name as inputs and plots a bar graph with the number of observations per value next to a </st><st c="23232">Q-Q plot:</st><pre class="source-code"><st c="23241">
def diagnostic_plots(df, variable):
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    df[variable].value_counts().sort_index(). </st><st c="23368">plot.bar()
    plt.title(f"Histogram of {variable}")
    plt.subplot(1, 2, 2)
    stats.probplot(df[variable], dist="norm",
        plot=plt)
    plt.title(f"Q-Q plot of {variable}")
    plt.show()</st></pre></li>				<li><st c="23537">Let’s create a bar plot and a Q-Q plot for one of the variables in the data using the function from </st><em class="italic"><st c="23638">step 3</st></em><st c="23644">:</st><pre class="source-code"><st c="23646">
diagnostic_plots(df, "counts1")</st></pre><p class="list-inset"><st c="23678">Here, we can see t</st><a id="_idTextAnchor441"/><st c="23697">he Poisson distribution in </st><st c="23725">the output:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_06.jpg" alt="Figure 3.6 – The bar and Q-Q pl﻿ots of the counts1 variable"/><st c="23736"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="23866">Figure 3.6 – The bar and Q-Q pl</st><a id="_idTextAnchor442"/><st c="23897">ots of the counts1 variable</st></p>
			<ol>
				<li value="5"><st c="23925">Now, let’s </st><a id="_idIndexMarker245"/><st c="23937">make a copy of </st><st c="23952">the</st><a id="_idTextAnchor443"/><st c="23955"> dataset:</st><pre class="source-code"><st c="23964">
df_tf = df.copy()</st></pre></li>				<li><st c="23982">Let’s apply the</st><a id="_idIndexMarker246"/><st c="23998"> square root transformation to </st><st c="24029">both variables:</st><pre class="source-code"><st c="24044">
df_tf[["counts1", "counts2"]] = np.sqrt(
    df[["counts1","counts2"]])</st></pre></li>				<li><st c="24112">Let’s round the values to two decimals for a </st><st c="24158">nicer visualization:</st><pre class="source-code"><st c="24178">
df_tf[["counts1", "counts2"]] = np.round(
    df_tf[["counts1", "counts2"]], 2)</st></pre></li>				<li><st c="24254">Let’s plot the distribution of </st><code><st c="24286">counts1</st></code><st c="24293"> after </st><st c="24300">the transformation:</st><pre class="source-code"><st c="24319">
diagnostic_plots(df_tf, "counts1")</st></pre><p class="list-inset"><st c="24354">We see a more </st><strong class="bold"><st c="24369">stabilized</st></strong><st c="24379"> variance, as the dots in the Q-Q plot foll</st><a id="_idTextAnchor444"/><st c="24422">ow the 45-degree diagonal </st><st c="24449">more closely:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_07.jpg" alt="Figure 3.7 – The bar and Q-Q plots of the counts1 variable after th﻿e square root transformation"/><st c="24462"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="24667">Figure 3.7 – The bar and Q-Q plots of the counts1 variable after th</st><a id="_idTextAnchor445"/><st c="24734">e square root transformation</st></p>
			<p class="list-inset"><st c="24763">Now, let’s </st><a id="_idIndexMarker247"/><st c="24775">apply the square root</st><a id="_idTextAnchor446"/><a id="_idIndexMarker248"/><st c="24796"> transformation </st><st c="24812">with </st><code><st c="24817">scikit-learn</st></code><st c="24829">.</st></p>
			<ol>
				<li value="9"><st c="24830">Let’s import </st><code><st c="24844">FunctionTransformer()</st></code><st c="24865"> and set up it to perform a square </st><st c="24900">root transformation:</st><pre class="source-code"><st c="24920">
from sklearn.preprocessing import FunctionTransformer
transformer = FunctionTransformer(
    np.sqrt).set_output(transform="pandas")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="25049">Note</st></p>
			<p class="callout"><st c="25054">If we wanted to round the values as we did in </st><em class="italic"><st c="25101">step 7</st></em><st c="25107">, we can set up the transformer using </st><code><st c="25145">transformer = FunctionTransformer(func=lambda x: </st></code><code><st c="25194">np.round(np.sqrt(x), 2))</st></code><st c="25218">.</st></p>
			<ol>
				<li value="10"><st c="25219">Let’s make a copy of the data and transform </st><st c="25264">the variables:</st><pre class="source-code"><st c="25278">
df_tf = df.copy()
df_tf = transformer.transform(df)</st></pre><p class="list-inset"><st c="25330">Go ahead and check the result of the transformation as we did in </st><em class="italic"><st c="25396">step 8</st></em><st c="25402">.</st></p><p class="list-inset"><st c="25403">To apply</st><a id="_idIndexMarker249"/><st c="25412"> the square root with Feature-engine instead, we use the </st><code><st c="25469">PowerTransformer()</st></code><st c="25487"> with an exponent </st><st c="25505">of 0.5:</st></p><pre class="source-code"><st c="25512">from feature_engine.transformation import PowerTransformer
r</st><a id="_idTextAnchor447"/><st c="25573">oot_t = PowerTransformer(exp=1/2)</st></pre></li>				<li><st c="25607">Next, we </st><a id="_idIndexMarker250"/><st c="25617">fit the transformer to </st><st c="25640">the data:</st><pre class="source-code"><st c="25649">
root_t.fit(df)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="25664">Note</st></p>
			<p class="callout"><st c="25669">The transformer automatically identifies the numerical variables, which we can explore by </st><st c="25760">executing </st><code><st c="25770">root_t.variables_</st></code><st c="25787">.</st></p>
			<ol>
				<li value="12"><st c="25788">Finally, let’s transform </st><st c="25814">the data:</st><pre class="source-code"><st c="25823">
df_tf = root_t.transform(df)</st></pre><p class="list-inset"><code><st c="25852">PowerTransformer()</st></code><st c="25871"> returns a panda</st><a id="_idTextAnchor448"/><a id="_idTextAnchor449"/><st c="25887">s DataFrame with the </st><st c="25909">transformed variables.</st></p></li>			</ol>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor450"/><st c="25931">How it works…</st></h2>
			<p><st c="25945">In this recipe, we applied the square root transformation using NumPy, scikit-learn, </st><st c="26031">and Feature-engine.</st></p>
			<p><st c="26050">We used NumPy’s </st><code><st c="26067">sqrt()</st></code><st c="26073"> function either directly or within scikit-learn’s </st><code><st c="26124">FunctionTrasnformer()</st></code><st c="26145"> to determine the variables’ square root. </st><st c="26187">Alternatively, we used Feature-engine’s </st><code><st c="26227">PowerTransformer()</st></code><st c="26245">, setting the exponent to 0.5, that of the square root function. </st><st c="26310">NumPy modified the variables directly. </st><st c="26349">The transformers of scikit-learn and Feature-engine modified the var</st><a id="_idTextAnchor451"/><a id="_idTextAnchor452"/><st c="26417">iables when calling the </st><code><st c="26442">transform()</st></code><st c="26453"> method.</st></p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor453"/><st c="26461">Using power transformations</st></h1>
			<p><st c="26489">Power functions </st><a id="_idIndexMarker251"/><st c="26506">are mathematical transformations that follow the </st><img src="img/15.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;l&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.340em;height:1.088em;width:4.058em"/><st c="26555"/><st c="26562"> format, where lambda can take any value. </st><st c="26603">The square and</st><a id="_idTextAnchor454"/><st c="26617"> cube root transformations are special cases of power transformations where lambda is 1/2 or 1/3, respectively. </st><st c="26729">The challenge resides in finding the value for the lambda parameter. </st><st c="26798">The Box-Cox transformation, which</st><a id="_idIndexMarker252"/><st c="26831"> is a generalization of the power transformations, finds the optimal lambda  value via maximum likelihood. </st><st c="26937">We will discuss the Box-Cox transformation in the following recipe. </st><st c="27005">In practice, we will try different lambda values and visually inspect the variable distribution to determine which one offers the best transformation. </st><st c="27156">In general, if the data is right-skewed – that is, if observations accumulate toward lower values – we use a lambda value that is smaller than 1, while if the data is left-skewed – that is, there are more observations around higher values – then we use a lambda value that is greater </st><st c="27440">than 1.</st></p>
			<p><st c="27447">In this recipe, we will carry out power transformations u</st><a id="_idTextAnchor455"/><a id="_idTextAnchor456"/><st c="27505">sing NumPy, scikit-learn, </st><st c="27532">an</st><a id="_idTextAnchor457"/><st c="27534">d Feature-engine.</st></p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor458"/><st c="27552">How to do it...</st></h2>
			<p><st c="27568">Let’s begin by importing </st><a id="_idIndexMarker253"/><st c="27594">the libraries and getting the </st><st c="27624">dataset ready:</st></p>
			<ol>
				<li><st c="27638">Import the required Python libraries </st><st c="27676">and classes:</st><pre class="source-code"><st c="27688">
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import FunctionTransformer
from feature_engine.transformation import PowerTransformer</st></pre></li>				<li><st c="27894">Let’s load the California housing dataset into a </st><st c="27944">pandas DataFrame:</st><pre class="source-code"><st c="27961">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)</st></pre></li>				<li><st c="28026">To evaluate variable distributions, we’ll create a function that takes a DataFrame and a</st><a id="_idIndexMarker254"/><st c="28115"> variable name as inputs and plots a histogram next to a </st><st c="28172">Q-Q plot:</st><pre class="source-code"><st c="28181">
def diagnostic_plots(df, variable):
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    df[variable].hist(bins=30)
    plt.title(f"Histogram of {variable}")
    plt.subplot(1, 2, 2)
    stats.probplot(df[variable], dist="norm",
        plot=plt)
    plt.title(f"Q-Q</st><a id="_idTextAnchor459"/><st c="28419"> plot of {variable}")
    plt.show()</st></pre></li>				<li><st c="28451">Let’s plot the distribution of the </st><code><st c="28487">Population</st></code><st c="28497"> variable with the </st><st c="28516">previous function:</st><pre class="source-code"><st c="28534">
diagnostic_plots(X, "Population")</st></pre><p class="list-inset"><st c="28568">In the plots returned by the previous command, we can see t</st><a id="_idTextAnchor460"/><st c="28628">hat </st><code><st c="28633">Population</st></code><st c="28643"> is heavily skewed to </st><st c="28665">the right:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_08.jpg" alt="Figure 3.8 – A histogram and ﻿Q-Q plot of the Population variable"/><st c="28675"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="28903">Figure 3.8 – A histogram and </st><a id="_idTextAnchor461"/><st c="28932">Q-Q plot of the Population variable</st></p>
			<p class="list-inset"><st c="28967">Now, let’s </st><a id="_idIndexMarker255"/><st c="28979">apply a power transformation to the </st><code><st c="29015">MedInc</st></code><st c="29021"> and </st><code><st c="29026">Population</st></code><st c="29036"> variables. </st><st c="29048">As both are skewed to the right, an exponent smaller than </st><em class="italic"><st c="29106">1</st></em><st c="29107"> might return a better spread of the </st><st c="29144">variable values.</st></p>
			<ol>
				<li value="5"><st c="29160">Let’s capture the variables to transform in </st><st c="29205">a list:</st><pre class="source-code"><st c="29212">
variables = ["MedInc", "Population"]</st></pre></li>				<li><st c="29249">Let’s make a copy of the DataFrame and then apply a power transformation to the variables from </st><em class="italic"><st c="29345">step 5,</st></em><st c="29352"> where the exponent </st><st c="29372">is </st><code><st c="29375">0.3</st></code><st c="29378">:</st><pre class="source-code"><st c="29380">
X_tf = X.copy()
X_tf[variables] = np.power(X[variables], 0.3)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="29442">Note</st></p>
			<p class="callout"><st c="29447">With </st><code><st c="29453">np.power()</st></code><st c="29463">, we can apply any power transformation by changing the value of the exponent in the second position of </st><st c="29567">the function.</st></p>
			<ol>
				<li value="7"><st c="29580">Let’s examine the change in the distribution </st><st c="29626">of </st><code><st c="29629">Population</st></code><st c="29639">:</st><pre class="source-code"><st c="29641">
diagnostic_plots(X_tf, "Population")</st></pre><p class="list-inset"><st c="29678">As shown in the plots returned by the previous command, </st><code><st c="29735">Population</st></code><st c="29745"> is now more evenly distributed across the value range and follows the quan</st><a id="_idTextAnchor462"/><st c="29820">tiles of the normal distribution </st><st c="29854">more closely:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_09.jpg" alt="Figure 3.9 – A histogram and Q-Q plot of the Populatio﻿n variable after the transformation"/><st c="29867"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="30015">Figure 3.9 – A histogram and Q-Q plot of the Populatio</st><a id="_idTextAnchor463"/><st c="30069">n variable after the transformation</st></p>
			<p class="list-inset"><st c="30105">Now, let’s apply a</st><a id="_idIndexMarker256"/><st c="30124"> power transformation </st><st c="30146">with scikit-learn.</st></p>
			<ol>
				<li value="8"><st c="30164">Let’s set up </st><code><st c="30178">FunctionTransformer()</st></code><st c="30199"> with a power transformation with an exponent </st><st c="30245">of </st><code><st c="30248">0.3</st></code><st c="30251">:</st><pre class="source-code"><st c="30253">
transformer = FunctionTransformer(
    lambda x: np.power(x,0.3))</st></pre></li>				<li><st c="30315">Let’s make a copy of the DataFrame and transform the variables from </st><em class="italic"><st c="30384">step 5</st></em><st c="30390">:</st><pre class="source-code"><st c="30392">
X_tf = X.copy()
X_tf[variables] = transformer.transform(X[variables])</st></pre><p class="list-inset"><st c="30462">That’s it – we can now examine the variable distribution. </st><st c="30521">Finally, let’s perform an exponential transformation </st><st c="30574">with Feature-engine.</st></p></li>				<li><st c="30594">Let’s set up </st><code><st c="30608">PowerTransformer()</st></code><st c="30626"> with an exponent of </st><code><st c="30647">0.3</st></code><st c="30650"> to transform the variables from </st><em class="italic"><st c="30683">step 5</st></em><st c="30689">. Then, we’ll fit it to </st><st c="30713">the data:</st><pre class="source-code"><st c="30722">
power_t = PowerTransformer(variables=variables,
    exp=0.3)
power_t.fit(X)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="30794">Note</st></p>
			<p class="callout"><st c="30799">If we don’t define the variables to transform, </st><code><st c="30847">PowerTransformer()</st></code><st c="30865"> will select and transform all of the numerical variables in </st><st c="30926">the DataFrame.</st></p>
			<ol>
				<li value="11"><st c="30940">Finally, let’s transform those </st><st c="30972">two variables:</st><pre class="source-code"><st c="30986">
X_tf = power_t.transform(X)</st></pre></li>			</ol>
			<p><st c="31014">The transformer </st><a id="_idIndexMarker257"/><st c="31031">returns a DataFrame containing the original variables, where the two variables specified in</st><a id="_idTextAnchor464"/><a id="_idTextAnchor465"/> <em class="italic"><st c="31122">step 5</st></em><st c="31129"> are transformed with the </st><st c="31155">po</st><a id="_idTextAnchor466"/><st c="31157">wer function.</st></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor467"/><st c="31171">How it works...</st></h2>
			<p><st c="31187">In this recipe, we applied power transformations using NumPy, scikit-learn, </st><st c="31264">and Feature-engine.</st></p>
			<p><st c="31283">To apply power functions with NumPy, we applied the </st><code><st c="31336">power()</st></code><st c="31343"> method to the slice of the dataset containing the variables to transform. </st><st c="31418">To apply this transformation with scikit-learn, we set up the </st><code><st c="31480">FunctionTransformer()</st></code><st c="31501">with </st><code><st c="31507">np.power()</st></code><st c="31517"> within a </st><code><st c="31527">lambda</st></code><st c="31533"> function, using </st><code><st c="31550">0.</st><a id="_idTextAnchor468"/><st c="31552">3</st></code><st c="31554"> as the exponent. </st><st c="31572">To apply power functions with Feature-engine, we set up the </st><code><st c="31632">PowerTransformer()</st></code><st c="31650"> with a list of the variables to transform and an exponent </st><st c="31709">of </st><code><st c="31712">0.3</st></code><st c="31715">.</st></p>
			<p><st c="31716">scikit-learn and Feature-engine transformers applied the transformation when we called the </st><code><st c="31808">transform()</st></code><st c="31819"> method. </st><st c="31828">scikit-learn’s </st><code><st c="31843">FunctionTransformer()</st></code><st c="31864"> modifies the entire dataset and returns NumPy arrays by default. </st><st c="31930">To return pandas DataFrames, we need to set the transform output to pandas, and to apply the transformation to specific variables, we can use </st><code><st c="32072">ColumnTransformer()</st></code><st c="32091">. Feature-engine’s </st><code><st c="32110">PowerTransformer()</st></code><st c="32128">, on the other hand, can apply the transformation to a subset of variables out of </st><a id="_idTextAnchor469"/><a id="_idTextAnchor470"/><st c="32210">the box, returning pandas DataFrames </st><st c="32247">by default</st><a id="_idTextAnchor471"/><st c="32257">.</st></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor472"/><st c="32258">Performing Box-Cox transformations</st></h1>
			<p><st c="32293">The Box-Cox transformation</st><a id="_idIndexMarker258"/><st c="32320"> is a generalization of the power family of transformations and is defined </st><st c="32395">as follows:</st></p>
			<p><img src="img/16.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mfrac&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mo&gt;≠&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.513em;height:1.807em;width:8.583em"/><st c="32406"/></p>
			<p><img src="img/17.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/mfenced&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.307em;height:1.055em;width:8.136em"/><st c="32432"/></p>
			<p><st c="32450">Here, </st><em class="italic"><st c="32456">y</st></em><st c="32457"> is the </st><a id="_idIndexMarker259"/><st c="32465">variable and </st><em class="italic"><st c="32478">λ</st></em><st c="32479"> is the transformation parameter. </st><st c="32513">It includes important special cases of transformations, such as untransformed </st><em class="italic"><st c="32591">(λ = 1)</st></em><st c="32598">, the logarithm </st><em class="italic"><st c="32614">(λ = 0)</st></em><st c="32621">, the reciprocal </st><em class="italic"><st c="32638">(λ = - 1)</st></em><st c="32647">, the square root (when </st><em class="italic"><st c="32671">λ</st></em> <em class="italic"><st c="32672">= 0.5</st></em><st c="32677">, it applies a scaled and shifted version of the square root function), and the </st><st c="32757">cube root.</st></p>
			<p><st c="32767">The Box-Cox transformation evaluates several values of </st><em class="italic"><st c="32823">λ</st></em><st c="32824"> using the maximum likelihood and selects the </st><em class="italic"><st c="32870">λ</st></em><st c="32871"> parameter that returns the </st><st c="32899">best transformation.</st></p>
			<p><st c="32919">In this recipe, we will perform the Box-Cox transformation using scikit-learn </st><st c="32998">a</st><a id="_idTextAnchor473"/><st c="32999">nd Feature-engine.</st></p>
			<p class="callout-heading"><st c="33017">Note</st></p>
			<p class="callout"><st c="33022">The Box-Cox transformation can only be used on positive variables. </st><st c="33090">If your variables have negative values, try the Yeo-Johnson transformation, which is described in the next recipe, </st><em class="italic"><st c="33205">Performing Yeo-Johnson transformation</st></em><st c="33242">. Alternatively, you can shift the variable distributi</st><a id="_idTextAnchor474"/><a id="_idTextAnchor475"/><st c="33296">on by adding a constant befo</st><a id="_idTextAnchor476"/><st c="33325">re </st><st c="33329">the transformation.</st></p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor477"/><st c="33348">How to do it...</st></h2>
			<p><st c="33364">Let’s begin by importing the necessary libraries and getting the </st><st c="33430">dataset ready:</st></p>
			<ol>
				<li><st c="33444">Import the required Python libraries </st><st c="33482">and classes:</st><pre class="source-code"><st c="33494">
import numpy as np
import pandas as pd
import scipy.stats as stats
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import PowerTransformer
from feature_engine.transformation import BoxCoxTransformer</st></pre></li>				<li><st c="33726">Let’s load the California housing dataset into a </st><st c="33776">pandas DataFrame:</st><pre class="source-code"><st c="33793">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)</st></pre></li>				<li><st c="33858">Let’s</st><a id="_idIndexMarker260"/><st c="33864"> drop the </st><code><st c="33874">Latitude</st></code><st c="33882"> and </st><code><st c="33887">Longitude</st></code><st c="33896"> variables:</st><pre class="source-code"><st c="33907">
X.drop(labels=["Latitude", "Longitude"], axis=1,
    inplace=True)</st></pre></li>				<li><st c="33970">Let’s inspect the variable distributions </st><st c="34012">with histograms:</st><pre class="source-code"><st c="34028">
X.hist(bins=30,</st><a id="_idTextAnchor478"/><st c="34044"> figsize=(12, 12), layout=(3, 3))
plt.show()</st></pre><p class="list-inset"><st c="34088">In the following output, we can see that the </st><code><st c="34134">MedInc</st></code><st c="34140"> variable shows a mild right-skewed distribution, variables such as </st><code><st c="34208">AveRooms</st></code><st c="34216"> and </st><code><st c="34221">Population</st></code><st c="34231"> are heavily right-skewed, and the </st><code><st c="34266">HouseAge</st></code><st c="34274"> variabl</st><a id="_idTextAnchor479"/><st c="34282">e shows an even spread of values across </st><st c="34323">its range:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_10.jpg" alt="Figure 3.10 – Histograms of the numerical variables"/><st c="34333"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="34643">Figure 3.10 – Histograms of the numerical variables</st></p>
			<ol>
				<li value="5"><st c="34694">Let’s</st><a id="_idIndexMarker261"/><st c="34700"> capture the variable names in a list since we will use these in the </st><st c="34769">follo</st><a id="_idTextAnchor480"/><st c="34774">wing step:</st><pre class="source-code"><st c="34785">
variables = list(X.columns)</st></pre></li>				<li><st c="34813">Let’s create a function that will plot Q-Q plots for all the variables in the data in two rows with three </st><st c="34920">plots each:</st><pre class="source-code"><st c="34931">
def make_qqplot(df):
    plt.figure(figsize=(10, 6),
        constrained_layout=True)
    for i in range(6):
        # location in figure
        ax = plt.subplot(2, 3, i + 1)
        # variable to plot
        var = variables[i]
        # q-q plot
        stats.probplot((df[var]), dist="norm",
            plot=plt)
        # add variable name as title
        ax.set_title(var)
    plt.show()</st></pre></li>				<li><st c="35231">Now, let’s display the Q-Q plots using the </st><st c="35275">preceding function:</st><pre class="source-code"><st c="35294">
m</st><a id="_idTextAnchor481"/><st c="35296">ake_qqplot(X)</st></pre><p class="list-inset"><st c="35309">By looking </st><a id="_idIndexMarker262"/><st c="35321">at the following plots, we can corrobora</st><a id="_idTextAnchor482"/><st c="35361">te that the variables are not </st><st c="35392">normally distributed:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_11.jpg" alt="Figure 3.11 – Q-Q plots of the numerical variables"/><st c="35413"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="35950">Figure 3.11 – Q-Q plots of the numerical variables</st></p>
			<p class="list-inset"><st c="36000">Next, let’s carry</st><a id="_idIndexMarker263"/><st c="36018"> out the Box-Cox transformation </st><st c="36050">using scikit-learn.</st></p>
			<ol>
				<li value="8"><st c="36069">Let’s set up </st><code><st c="36083">PowerTransformer()</st></code><st c="36101"> to apply the Box-Cox transformation and fit it to the data so that it finds the optimal </st><em class="italic"><st c="36190">λ</st></em><st c="36191"> parameter:</st><pre class="source-code"><st c="36202">
transformer = PowerTransformer(
    method="box-cox", standardize=False,
).set_output(transform="pandas")
transformer.fit(X)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="36323">Note</st></p>
			<p class="callout"><st c="36328">To avoid data leakage, the </st><em class="italic"><st c="36356">λ</st></em><st c="36357"> parameter should be learned from the train set and then used to transform the train and test sets. </st><st c="36457">Thus, remember to split your data into train and tes</st><a id="_idTextAnchor483"/><st c="36509">t sets before </st><st c="36524">fitting </st><code><st c="36532">PowerTransformer()</st></code><st c="36550">.</st></p>
			<ol>
				<li value="9"><st c="36551">Now, let’s transform </st><st c="36573">the dataset:</st><pre class="source-code"><st c="36585">
X_tf = transformer.transform(X)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="36617">Note</st></p>
			<p class="callout"><st c="36622">scikit-learn’s </st><code><st c="36638">PowerTransformer()</st></code><st c="36656"> stores the learned lambdas in its </st><code><st c="36691">lambdas_</st></code><st c="36699"> attribute, which you can display by </st><st c="36736">executing </st><code><st c="36746">transformer.lambdas_</st></code><st c="36766">.</st></p>
			<ol>
				<li value="10"><st c="36767">Let’s inspect the distributions of the transformed data </st><st c="36824">with histograms:</st><pre class="source-code"><st c="36840">
X_tf.hist(bins=30</st><a id="_idTextAnchor484"/><st c="36858">, figsize=(12, 12), layout=(3, 3))
plt.show()</st></pre><p class="list-inset"><st c="36903">In the following output, we can see that the variables’</st><a id="_idTextAnchor485"/><st c="36959"> values are more evenly spread across </st><st c="36997">their ranges:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_12.jpg" alt="Figure 3.12 – Histograms of the variables after the transformation"/><st c="37010"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="37302">Figure 3.12 – Histograms of the variables after the transformation</st></p>
			<ol>
				<li value="11"><st c="37368">Now, let’s return </st><a id="_idIndexMarker264"/><st c="37387">Q-Q plots of the </st><st c="37404">transformed variables:</st><pre class="source-code"><st c="37426">
make_qqplot(X_tf)</st></pre><p class="list-inset"><st c="37444">In the following output, we can see that, after the transformation, the variables follo</st><a id="_idTextAnchor486"/><st c="37532">w the theoretical normal distribution </st><st c="37571">more closely:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_13.jpg" alt="Figure 3.13 – Q-Q plots o﻿f the variables after the transformation"/><st c="37584"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="38093">Figure 3.13 – Q-Q plots o</st><a id="_idTextAnchor487"/><st c="38118">f the variables after the transformation</st></p>
			<p class="list-inset"><st c="38159">Now, let’s</st><a id="_idIndexMarker265"/><st c="38170"> implement the Box-Cox transformation </st><st c="38208">with Feature-engine.</st></p>
			<ol>
				<li value="12"><st c="38228">Let’s set up </st><code><st c="38242">BoxCoxTransformer()</st></code><st c="38261"> to transform all the variables in the dataset and then fit it to </st><st c="38327">the data:</st><pre class="source-code"><st c="38336">
bct = BoxCoxTransformer()
bct.fit(X)</st></pre></li>				<li><st c="38373">Now, let’s go ahead and transform </st><st c="38408">the variables:</st><pre class="source-code"><st c="38422">
X_tf = bct.transform(X)</st></pre><p class="list-inset"><st c="38446">The transformation returns a pandas DataFrame containing </st><a id="_idTextAnchor488"/><st c="38504">the </st><st c="38508">modified variables.</st></p></li>			</ol>
			<p class="callout-heading"><st c="38527">Note</st><a id="_idTextAnchor489"/></p>
			<p class="callout"><code><st c="38532">PowerTransformer()</st></code><st c="38551"> from scikit-learn will transform the entire dataset. </st><st c="38605">On the other hand, </st><code><st c="38624">BoxCoxTransformer()</st></code><st c="38643"> from Feature-engine can modify a subset of the variables, if we pass their names in a list to the </st><code><st c="38742">variables</st></code><st c="38751"> parameter when setting up the transformer. </st><st c="38795">If the </st><code><st c="38802">variables</st></code><st c="38811"> parameter is set to </st><code><st c="38832">None</st></code><st c="38836">, the transformer will transform all numerical variables seen </st><st c="38898">during </st><code><st c="38905">fit()</st></code><st c="38910">.</st></p>
			<ol>
				<li value="14"><st c="38911">The </st><a id="_idIndexMarker266"/><st c="38916">optimal lambdas for the Box-Cox transformation are stored in the </st><code><st c="38981">lambda_dict_</st></code><st c="38993"> attribute. </st><st c="39005">Let’s </st><st c="39011">inspect them:</st><pre class="source-code"><st c="39024">
bct.lambda_dict_</st></pre><p class="list-inset"><st c="39041">The output of the previous command is </st><st c="39080">the following:</st></p><pre class="source-code"><strong class="bold"><st c="39094">{'MedInc': 0.09085449361507383,</st></strong>
<strong class="bold"><st c="39126">'HouseAge': 0.8093980940712507,</st></strong>
<strong class="bold"><st c="39158">'AveRooms': -0.2980048976549959,</st></strong>
<strong class="bold"><st c="39191">'AveBedrms': -1.6290002625859639,</st></strong>
<strong class="bold"><st c="39225">'Population': 0.235767</st><a id="_idTextAnchor490"/><st c="39248">57812051324,</st></strong>
<strong class="bold"><st c="39261">'AveOccup': -0.4763032278973292}</st></strong></pre></li>			</ol>
			<p><st c="39294">Now you know how to implement the Box-Co</st><a id="_idTextAnchor491"/><a id="_idTextAnchor492"/><st c="39335">x transformation with two different </st><st c="39372">Python libraries.</st></p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor493"/><st c="39389">How it works...</st></h2>
			<p><st c="39405">scikit-learn’s </st><code><st c="39421">PowerTransformer()</st></code><st c="39439"> can apply both Box-Cox and Yeo-Johnson transformations, so we specified the transformation when setting up the transformer by passing the b</st><code><st c="39579">ox-cox</st></code><st c="39586"> string. </st><st c="39595">Next, we fit the transformer to the data so that the transformer learned the optimal lambdas for each variable. </st><st c="39707">The learned lambdas were stored in the </st><code><st c="39746">lambdas_</st></code><st c="39754"> attribute. </st><st c="39766">Finally, we used the </st><code><st c="39787">transform()</st></code><st c="39798"> method to transform </st><st c="39819">the variables.</st></p>
			<p class="callout-heading"><st c="39833">Note</st></p>
			<p class="callout"><st c="39838">Remember that to return DataFrames instead of arrays, you need to specify the transform output through the </st><code><st c="39946">set_output()</st></code><st c="39958"> method. </st><st c="39967">You can apply the transformation to a subset of values by using </st><st c="40031">the </st><code><st c="40035">ColumnTransformer()</st></code><st c="40054">.</st></p>
			<p><st c="40055">Finally, we applied the Box-Cox transformation using Feature-engine. </st><st c="40125">We initialized </st><code><st c="40140">BoxCoxTransformer()</st></code><st c="40159">, leaving</st><a id="_idIndexMarker267"/><st c="40168"> the parameter </st><code><st c="40183">variables</st></code><st c="40192"> set the </st><code><st c="40201">None</st></code><st c="40205">. Due to this, the transformer automatically found the numerical variables in the data during </st><code><st c="40299">fit()</st></code><st c="40304">. We fit the transformer to the data so that it learned the optimal lambdas per variable, which were stored in </st><code><st c="40415">lambda_dict_</st></code><st c="40427">, and transformed the variables using the </st><code><st c="40469">transform()</st></code><st c="40480"> method. </st><st c="40489">Feature-engine’s </st><code><st c="40506">BoxCoxTransformer()</st></code><st c="40525"> can take the entire DataFrame a</st><a id="_idTextAnchor494"/><a id="_idTextAnchor495"/><st c="40557">s input and it yet mo</st><a id="_idTextAnchor496"/><st c="40579">dify only the </st><st c="40594">selected variables.</st></p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor497"/><st c="40613">There’s more…</st></h2>
			<p><st c="40627">We can apply the Box-Cox transformation with the SciPy library. </st><st c="40692">For a code implementation, visit this book’s GitHub </st><st c="40744">repository: </st><a href="https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch03-variable-transformation/Recipe-5-Box-Cox-transformation.ipynb"><st c="40756">https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch03-variabl</st><st c="40863">e-transformation/Recipe-5-Box-Cox-transformation.ipynb</st></a></p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor500"/><st c="40918">Performing Yeo-Johnson transformations</st></h1>
			<p><st c="40957">The </st><a id="_idIndexMarker268"/><st c="40962">Yeo-Johnson transformation is an extension of the Box-Cox transformation that is no longer constrained to positive values. </st><st c="41085">In other words, the Yeo-Johnson transformation can be used on variables with zero and negative values, as well as positive values. </st><st c="41216">These transformations are defined </st><st c="41250">as follows:</st></p>
			<ul>
				<li><img src="img/18.png" alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" style="vertical-align:-0.218em;height:1.059em;width:2.791em"/><st c="41261"/><st c="41274">; if λ ≠ 0 and X &gt;= 0</st></li>
				<li><st c="41295">ln(X + 1 ); if λ = 0 and X &gt;= 0</st></li>
				<li><img src="img/19.png" alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" style="vertical-align:-0.218em;height:1.059em;width:4.268em"/><st c="41327"/><st c="41329">; if λ ≠ 2 and X &lt; 0</st></li>
				<li><st c="41349">-ln(-X + 1); if λ = 2 and X &lt; 0</st></li>
			</ul>
			<p><st c="41381">When the </st><a id="_idIndexMarker269"/><st c="41391">variable has only positive values, then the Yeo-Johnson transformation is like the Box-Cox transformation of the variable plus one. </st><st c="41523">If the variable has only negative values, then the Yeo-Johnson transformation is like the Box-Cox transformation of the negative of the variable plus one, at the power of </st><em class="italic"><st c="41694">2- λ</st></em><st c="41698">. If the variable has a mix of positive and negative values, the Yeo-Johnson transformation applies different powers to the positive and </st><st c="41835">negative values.</st></p>
			<p><st c="41851">In this recipe, we will perform the Yeo-Johns</st><a id="_idTextAnchor501"/><st c="41897">on transfo</st><a id="_idTextAnchor502"/><st c="41908">rmation using scik</st><a id="_idTextAnchor503"/><st c="41927">it-learn </st><st c="41937">and Feature-engine.</st></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor504"/><st c="41956">How to do it...</st></h2>
			<p><st c="41972">Let’s begin</st><a id="_idIndexMarker270"/><st c="41984"> by importing the necessary libraries and getting the </st><st c="42038">dataset ready:</st></p>
			<ol>
				<li><st c="42052">Import the required Python libraries </st><st c="42090">and classes:</st><pre class="source-code"><st c="42102">
import numpy as np import pandas as pd
import scipy.stats as stats
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import PowerTransformer
from feature_engine.transformation import YeoJohnsonTransformer</st></pre></li>				<li><st c="42338">Let’s load the California housing dataset into a pandas DataFrame and then drop the </st><code><st c="42423">Latitude</st></code><st c="42431"> and </st><code><st c="42436">Longitude</st></code><st c="42445"> variables:</st><pre class="source-code"><st c="42456">
X, y = fetch_california_housing(
    return_X_y=True, as_frame=True)
X.drop(labels=[«Latitude», «Longitude»], axis=1,
    inplace=True)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="42584">Note</st></p>
			<p class="callout"><st c="42589">We can evaluate the variable distribution with histograms and Q-Q plots, as we did in </st><em class="italic"><st c="42676">steps 4</st></em><st c="42683"> to </st><em class="italic"><st c="42687">7</st></em><st c="42688"> of the </st><em class="italic"><st c="42696">Performing Box-Cox </st></em><em class="italic"><st c="42715">transformations</st></em><st c="42730"> recipe.</st></p>
			<p class="list-inset"><st c="42738">Now, let’s </st><a id="_idIndexMarker271"/><st c="42750">apply the Yeo-Johnson transformation </st><st c="42787">with scikit-learn.</st></p>
			<ol>
				<li value="3"><st c="42805">Let’s set up </st><code><st c="42819">PowerTransformer()</st></code><st c="42837"> with the </st><code><st c="42847">yeo-johnson</st></code><st c="42858"> transformation:</st><pre class="source-code"><st c="42874">
transformer = PowerTransformer(
    method="yeo-johnson", sta</st><a id="_idTextAnchor505"/><st c="42932">ndardize=False,
).set_output(transform="pandas")</st></pre></li>				<li><st c="42981">Let’s </st><a id="_idIndexMarker272"/><st c="42988">fit the transformer to </st><st c="43011">the data:</st><pre class="source-code"><st c="43020">
transformer.fit(X)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="43039">Note</st></p>
			<p class="callout"><st c="43044">The </st><em class="italic"><st c="43049">λ</st></em><st c="43050"> parameter should be learned from the train set and then used to transform the train and test sets. </st><st c="43150">Thus, remember to separate your data into train and test sets before </st><st c="43219">fitting </st><code><st c="43227">PowerTransformer()</st></code><st c="43245">.</st></p>
			<ol>
				<li value="5"><st c="43246">Now, let’s transform </st><st c="43268">the dataset:</st><pre class="source-code"><st c="43280">
X_tf = transformer.transform(X)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="43312">Note</st></p>
			<p class="callout"><code><st c="43317">PowerTransformer()</st></code><st c="43336"> stores the learned parameters in its </st><code><st c="43374">lambda_</st></code><st c="43381"> attribute, which you can return by </st><st c="43417">executing </st><code><st c="43427">transformer.lambdas_</st></code><st c="43447">.</st></p>
			<ol>
				<li value="6"><st c="43448">Let’s inspect the distributions of the transformed data </st><st c="43505">with histograms:</st><pre class="source-code"><st c="43521">
X_tf.hist(bins=30, figsize=(12, 12), layout=(3, 3))
plt.show()</st></pre><p class="list-inset"><st c="43584">In the following output, we can see that the variables’ values are more evenly spread across </st><st c="43678">their ranges:</st></p></li>			</ol>
			<div><div><img src="img/B22396_03_14.jpg" alt="Figure 3.14 – Histograms of the variables after the yeo-Johnson transformation"/><st c="43691"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="43883">Figure 3.14 – Histograms of the variables after the yeo-Johnson transformation</st></p>
			<p class="list-inset"><st c="43961">Finally, let’s implement</st><a id="_idIndexMarker273"/><st c="43986"> the Yeo-Johnson transformation </st><st c="44018">with Feature-engine.</st></p>
			<ol>
				<li value="7"><st c="44038">Let’</st><a id="_idTextAnchor506"/><st c="44043">s set up </st><code><st c="44053">YeoJohnsonTransformer()</st></code><st c="44076"> to transform all numerical variables and then fit it to </st><st c="44133">the data:</st><pre class="source-code"><st c="44142">
yjt = YeoJohnsonTransformer()
yjt.fit(X)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="44183">Note</st></p>
			<p class="callout"><st c="44188">If the </st><code><st c="44196">variables</st></code><st c="44205"> argument is left set to </st><code><st c="44230">None</st></code><st c="44234">, the transformer selects and transforms all the numerical variables in the dataset. </st><st c="44319">Alternatively, we can pass the names of the variables to modify in </st><st c="44386">a list.</st></p>
			<p class="callout"><st c="44393">Compared to </st><code><st c="44406">PowerTransformer()</st></code><st c="44424"> from scikit-learn, Feature-engine’s transformer can take the entire DataFrame as an argument of the </st><code><st c="44525">fit()</st></code><st c="44530"> and </st><code><st c="44535">transform()</st></code><st c="44546"> methods, and yet it will only modify the </st><st c="44588">selected variables.</st></p>
			<ol>
				<li value="8"><st c="44607">Let’s transform</st><a id="_idIndexMarker274"/> <st c="44623">the variables:</st><pre class="source-code"><st c="44638">
X_tf = yjt.transform(X)</st></pre></li>				<li><code><st c="44662">YeoJohnsonTransformer()</st></code><st c="44686"> stores the best parameters per variable in its </st><code><st c="44734">lambda_dict_</st></code><st c="44746"> attribute, which we can display </st><st c="44779">as follows:</st><pre class="source-code"><st c="44790">
yjt.lambda_dict_</st></pre><p class="list-inset"><st c="44807">The previous command returns the </st><st c="44841">following dictionary:</st></p><pre class="source-code"><strong class="bold"><st c="44862">{'MedInc': -0.1985098937827175,</st></strong>
<strong class="bold"><st c="44894">'HouseAge': 0.8081480895997063,</st></strong>
<strong class="bold"><st c="44926">'AveRooms': -0.5536698033957893,</st></strong>
<strong class="bold"><st c="44959">'AveBedrms': -4.3940822236920365,</st></strong>
<strong class="bold"><st c="44993">'Population': 0.2335</st><a id="_idTextAnchor507"/><st c="45014">2363517075606,</st></strong>
<strong class="bold"><st c="45029">'AveOccup': -0.9013456270549428}</st></strong></pre></li>			</ol>
			<p><st c="45062">Now you know</st><a id="_idTextAnchor508"/><st c="45075"> how to implement the Yeo-John</st><a id="_idTextAnchor509"/><a id="_idTextAnchor510"/><st c="45105">son transformation with two different open </st><st c="45149">source libraries.</st></p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor511"/><st c="45166">How it works...</st></h2>
			<p><st c="45182">In this recipe, we applied the Yeo-Johnson transformation using </st><code><st c="45247">scikit-learn</st></code> <st c="45259">and </st><code><st c="45264">Feature-engine</st></code><st c="45278">.</st></p>
			<p><code><st c="45279">scikit-learn</st></code><st c="45292">’s </st><code><st c="45296">PowerTransformer()</st></code><st c="45314"> can apply both Box-Cox and Yeo-Johnson transformations, so we specified the transformation with the </st><code><st c="45415">yeo-johnson</st></code><st c="45426"> string. </st><st c="45435">The </st><code><st c="45439">standardize</st></code><st c="45450"> argument allowed us to determine whether we wanted to standardize (scale) the transformed values. </st><st c="45549">Next, we fit the transformer to the DataFrame so that it learned the optimal lambdas for each variable. </st><code><st c="45653">PowerTransformer()</st></code><st c="45671"> stored the learned lambdas in its </st><code><st c="45706">lambdas_</st></code><st c="45714"> attribute. </st><st c="45726">Finally, we used the </st><code><st c="45747">transform()</st></code><st c="45758"> method to return the transformed variables. </st><st c="45803">We set the transform output to </st><code><st c="45834">pandas</st></code><st c="45840"> to return DataFrames after </st><st c="45868">the transformation.</st></p>
			<p><st c="45887">After that, we </st><a id="_idIndexMarker275"/><st c="45903">applied the Yeo-Johnson transformation using Feature-engine. </st><st c="45964">We set up </st><code><st c="45974">YeoJohnsonTransformer()</st></code><st c="45997"> so that it transforms all numerical variables seen during </st><code><st c="46056">fit()</st></code><st c="46061">. We fitted the transformer to the data so that it learned the optimal lambdas per variable, which were stored in </st><code><st c="46175">lambda_dict_</st></code><st c="46187">, and finally transformed the variables using the </st><code><st c="46237">transform()</st></code><st c="46248"> method. </st><st c="46257">Feature-engine’s </st><code><st c="46274">YeoJohnnsonTransformer()</st></code><st c="46298"> can take the entire DataFrame</st><a id="_idTextAnchor512"/><a id="_idTextAnchor513"/><st c="46328"> as input, yet it wil</st><a id="_idTextAnchor514"/><st c="46349">l only transform the </st><st c="46371">selected variables.</st></p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor515"/><st c="46390">There’s more…</st></h2>
			<p><st c="46404">We can apply the Yeo-Johnson transformation with the SciPy library. </st><st c="46473">For a code implementation, visit this book’s GitHub </st><st c="46525">repository: </st><a href="https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Second-Edition/blob/main/ch03-variable-transformation/Recipe-6-Yeo-Johnson-transformation.ipynb"><st c="46537">https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Second-Edition/blob/main/ch03-variable-transformation/Recipe-6-Yeo-Johnson-transformation.ipynb</st></a></p>
		</div>
	<div></body></html>