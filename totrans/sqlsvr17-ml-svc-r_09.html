<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Machine Learning Services with R for DBAs</h1>
                
            
            <article>
                
<p class="calibre2">R integration (along with Python integration in SQL Server 2017) offered a wide range of possibilities that one can use. And the targeted group of people has just increased in terms of people (job roles or departments) using R Services. DBAs (and also SysAdmins) will for sure gain a lot from this. Not only do R and statistics give them some additional impetus for discovering and gaining insights on their captured data, but also they might help them to find some hidden nuggets that they might have missed before. The mixture of different languages-and I am not solely talking about R, but also other languages-for sure bring new abilities to track, capture, and analyze <span>captured</span> data.</p>
<p class="calibre2">One thing is clear, if you have R (any Python) so close to the database, several people can switch from monitoring tasks to predicting tasks. This literally means that instead of taking actions when something has already happened, people can now diagnose and predict what might happen. I'm not saying this is an easy task, since we all know that the complexity, for example, of one query all of a sudden running slow, can have one or more hidden reasons, that might not be seen immediately, R in-database will for sure help find this hidden reason in near real time. Contrary to data-mining in SSAS, which in my personal opinion is still a very strong and good tool, there might be more  latency in comparison to sending and analyzing data through R Engine.</p>
<p class="calibre2">This chapter will capture the important steps on how to help DBAs (or other roles to tackle similar issues) to get the advantages of R:</p>
<ul class="calibre7">
<li class="calibre8">Gathering data relevant for DBAs </li>
<li class="calibre8">Exploring and analyzing data</li>
<li class="calibre8">Creating predictions with R Services</li>
<li class="calibre8">Improving monitoring with predictions</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Gathering relevant data</h1>
                
            
            <article>
                
<p class="calibre2">Gathering data - simple as it might be - is a task that needs to be well crafted. There are a few reasons for that. The first and most important is that we want to gather data in a way that will have minimum or zero impact on the production environment. This means that the process of collecting and storing data should not disturb any on-going process. The second important thing is storage. Where and how do you want to store the data and the retention policy of the stored data? At the beginning, this might seem a very trivial case, but over time, storage itself will play an important role. The third and also utterly important thing is which data you want to gather. Of course, we all want to have smart data present, that is, having all the data relevant for solving or improving our business processes. But in reality, gathering smart is neither that difficult nor that easy. First of all, one must understand the concept of the database feature and furthermore, how to capture the relevant indicators and how this particular feature will work.</p>
<p class="calibre2">Let's see where and how one can see performance improvements, if you know where and how to look for them.</p>
<p class="calibre2">For example, delayed durability is a feature that has been in SQL Server since SQL Server 2014, but can in some scenarios help improve performance for compromising the durability (durability is part of the ACID acronym-atomic, consistent, isolation, and durability-and prevents, in the case of a failure or system restart, committed data from not being saved or saved in an incorrect state). <strong class="calibre1">Write-ahead log</strong> (<strong class="calibre1">WAL</strong>) is a system that SQL Server uses, which means that all the changes are written to the log first, before they are allowed to be committed to the database table.</p>
<p class="calibre2">For this quick demo, we will create an empty database with <kbd class="calibre11">DELAYED_DURABILITY</kbd> set to allowed with <kbd class="calibre11">NO_WAIT</kbd>. An additional and important step to this test is to set the backup of the database to <kbd class="calibre11">NUL</kbd>, which is similar to the command <kbd class="calibre11">with truncate_only</kbd>. This statement discards any inactive logs (when the database is in full or bulk-logged recovery mode; for simple recovery mode, this does not hold water) and from the point when a full backup of the database is done, any inactive log records get discarded (deleted). This could be simulated. When a checkpoint runs, the attempt to back up the log will result in an error message. In other words, a database could be running in simple recovery mode. Essentially, the <kbd class="calibre11">NUL</kbd> command simply stores and discards the log:</p>
<pre class="calibre19"><strong class="calibre1">USE [master];</strong>
<strong class="calibre1">GO</strong>
<strong class="calibre1">CREATE DATABASE [DelayedDurability];</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">ALTER DATABASE [DelayedDurability] SET DELAYED_DURABILITY = ALLOWED    <br class="title-page-name"/>WITH NO_WAIT;</strong>
<strong class="calibre1">GO</strong>
<strong class="calibre1">BACKUP DATABASE [DelayedDurability] TO DISK = N'nul'</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">I will create a sample table to do the inserts:</p>
<pre class="calibre19"><strong class="calibre1">USE [DelayedDurability];</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">DROP TABLE IF EXISTS TestDDTable;</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">CREATE TABLE TestDDTable</strong>
<strong class="calibre1">(ID INT IDENTITY(1,1) PRIMARY KEY</strong>
<strong class="calibre1">,R_num INT</strong>
<strong class="calibre1">,Class CHAR(10)  </strong>
<strong class="calibre1">,InsertTime DATETIME DEFAULT(GETDATE())</strong>
<strong class="calibre1">);</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">Having created a table, we can now test two types of insert, with and without delayed durability:</p>
<pre class="calibre19"><strong class="calibre1">EXECUTE sys.sp_flush_log;</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">DECLARE @count INT = 0</strong>
<strong class="calibre1">DECLARE @start1 DATETIME = GETDATE()</strong>
<strong class="calibre1">WHILE (@count &lt;= 250000)</strong>
    <strong class="calibre1">      BEGIN</strong>
    <strong class="calibre1">            BEGIN TRAN</strong>
    <strong class="calibre1">                  INSERT INTO TestDDTable(R_num, class) VALUES(@count, 'WITHOUT_DD')</strong>
    <strong class="calibre1">                  SET @count += 1</strong>
    <strong class="calibre1">            COMMIT</strong>
    <strong class="calibre1">      END</strong>
    
<strong class="calibre1">SET @count = 0</strong>
<strong class="calibre1">DECLARE @start2 DATETIME = GETDATE()</strong>
<strong class="calibre1">WHILE (@count &lt;= 250000)</strong>
    <strong class="calibre1">      BEGIN</strong>
    <strong class="calibre1">            BEGIN TRAN</strong>
    <strong class="calibre1">                  INSERT INTO TestDDTable(R_num, class) VALUES(@count, 'WITH_DD')</strong>
    <strong class="calibre1">                  SET @count += 1</strong>
    <strong class="calibre1">            COMMIT WITH (DELAYED_DURABILITY = ON)</strong>
    <strong class="calibre1">      END</strong>
    
<strong class="calibre1">SELECT </strong>
<strong class="calibre1">DATEDIFF(SECOND, @start1, GETDATE()) AS With_DD_OFF</strong>
<strong class="calibre1">,DATEDIFF(SECOND, @start2, GETDATE()) AS With_DD_ON</strong></pre>
<p class="calibre2">And the result is obvious: with the delayed durability set to on, one can gain in performance when doing so many inserts:</p>
<div class="packt_figure"><img class="aligncenter89" src="../images/00154.gif"/></div>
<p class="calibre2">I could have also used the query stress to simulate several threads, each doing the same number of inserts:</p>
<div class="packt_figure"><img class="aligncenter90" src="../images/00155.jpeg"/></div>
<p class="calibre2">And with such heavy stress tool testing, the question is, How can we monitor and track the behavior of the delayed durability? One can test the performance with the performance monitor:</p>
<div class="packt_figure"><img src="../images/00156.jpeg" class="calibre85"/></div>
<p class="calibre2">Alternatively, you can test the performance with the activity monitor:</p>
<div class="packt_figure"><img class="aligncenter91" src="../images/00157.jpeg"/></div>
<p class="calibre2">But soon you will realize that either you need to store this information for later analysis or you need to get some additional know-how as to, which performance pointers or extended events are worth monitoring in this case.</p>
<p class="calibre2">So, in this case, you will need to check the wait resources on logging in:</p>
<pre class="calibre19"><strong class="calibre1">SELECT * FROM sys.dm_os_wait_stats</strong>
<strong class="calibre1">WHERE wait_type IN ('WRITELOG');</strong>
<strong class="calibre1">GO</strong> </pre>
<p class="calibre2">You will also need to either add some mechanism to capture those wait statistics in a table for later analysis or capture the performance monitor or use profiler, XE, and others. Querying data in runtime and capturing statistics is rather tedious job; imagine merging statistics from <kbd class="calibre11">sys.dm_os_wait_stats</kbd> and combining them with <kbd class="calibre11">sys.dm_io_virtual_file_stats</kbd>. All in all the, more data you try to gather, the more complicated querying these statistics might becomes.</p>
<p class="calibre2">Capturing with performance monitor of both queries from earlier, the picture looks as shown in the following screenshot:</p>
<div class="packt_figure"><img src="../images/00158.jpeg" class="calibre86"/></div>
<p class="calibre2">The above screenshot shows, on the left-hand side (1), how delayed durability is working and how log flushing is happening in a sequential period of time. Comparing this to the right-hand side (2), we can see how delayed durability is turned off and log flushing is not active.</p>
<p class="calibre2">Extracting raw data from the performance monitor may not be the right approach, but storing the same set of data through extended events will be much more lightweight on the system, as well as the users, for later analysis.</p>
<p class="calibre2">Setting up the extended event you will need for your analysis can be done fast and easily. But rather than choosing too many events, focus on the ones that are really needed, because, the log file might get big very quickly:</p>
<pre class="calibre19"><strong class="calibre1">-- creating extended event</strong>
    
<strong class="calibre1">IF EXISTS(SELECT * FROM sys.server_event_sessions WHERE name='DelayDurab_Log_flush')  </strong>
<strong class="calibre1">DROP EVENT session DelayDurab_Log_flush ON SERVER; </strong>
    
<strong class="calibre1">-- Get DelayedDurability database ID</strong>
<strong class="calibre1">SELECT db_id()</strong>

<strong class="calibre1">CREATE EVENT SESSION DelayDurab_Log_flush ON SERVER</strong>
<strong class="calibre1">ADD EVENT sqlserver.log_flush_start</strong>
<strong class="calibre1">      (WHERE  (database_id=40)),</strong>
<strong class="calibre1">ADD EVENT sqlserver.databases_log_flush </strong>
<strong class="calibre1">      (WHERE (database_id =40)),</strong>
<strong class="calibre1">ADD EVENT sqlserver.transaction_log</strong>
<strong class="calibre1">      (WHERE (database_id =40))</strong>
<strong class="calibre1">-- maybe add batchrequests/second</strong>
    
 <strong class="calibre1">ADD TARGET package0.event_file</strong>
<strong class="calibre1">(</strong>
<strong class="calibre1">     SET filename     ='C:\CH09\MonitorDelayDurability.xel'</strong>
<strong class="calibre1">          ,metadatafile ='C:\CH09\MonitorDelayDurability.xem'</strong>
<strong class="calibre1">)</strong>
<strong class="calibre1">WITH (MAX_MEMORY=4096KB</strong>
<strong class="calibre1">            ,EVENT_RETENTION_MODE=ALLOW_SINGLE_EVENT_LOSS</strong>
<strong class="calibre1">            ,MAX_DISPATCH_LATENCY=30 SECONDS</strong>
<strong class="calibre1">            ,MEMORY_PARTITION_MODE=NONE</strong>
<strong class="calibre1">            ,STARTUP_STATE=ON);</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">After starting the event, read the content of the file by breaking down the XML structure:</p>
<pre class="calibre19"><strong class="calibre1">SELECT </strong>
 <strong class="calibre1">     CAST(event_data AS XML) AS event_data</strong>
<strong class="calibre1">FROM sys.fn_xe_file_target_read_file('C:\CH09\MonitorDelayDurability*.xel', 'C:\CH09\MonitorDelayDurability*.xem', null, null)</strong> </pre>
<p class="calibre2">Also, getting the information out of XML is another important task to tackle extended events correctly:</p>
<div class="packt_figure"><img src="../images/00159.jpeg" class="calibre87"/></div>
<p class="calibre2">Coming back to the starting point, gathering data for DBAs and further analysis is of the utmost importance. One thing can also be seen from this example: if we also add to log file growth, one of the logs needs to grow additionally by adding new VLF files. Adding delayed durability gives faster inserts as compared to transactions with delayed durability turned off. Sometimes adding new <kbd class="calibre11">XE</kbd> or measures can dramatically increase the logging file, where the data is being gathered. Using statistical analysis, we can optimize measure selection or later find that they give us additional insight information. Working on exploring and later analyzing data can give you a huge pay-off, in terms of workloads and in terms of different data gathered.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Exploring and analyzing data</h1>
                
            
            <article>
                
<p class="calibre2">In a similar way, gathering data using event features can give you a rich way to a lot of system information data. Deriving from the previous sample, with the following demo, we will see how measures of a server can be used for advanced statistical analyses and how to help reduce the amount of different information, and pin-point the relevant measures. A specific database and a stage table will be created:</p>
<pre class="calibre19"><strong class="calibre1">CREATE DATABASE ServerInfo;</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">USE [ServerInfo]</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">DROP TABLE IF EXISTS server_info;</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">CREATE TABLE [dbo].[server_info]([XE01] [tinyint] NULL, [XE02] [tinyint] NULL,</strong>
<strong class="calibre1">      [XE03] [tinyint] NULL, [XE04] [tinyint] NULL, [XE05] [tinyint] NULL,</strong>
<strong class="calibre1">      [XE06] [tinyint] NULL, [XE07] [tinyint] NULL, [XE08] [tinyint] NULL,</strong>
 <strong class="calibre1">     [XE09] [tinyint] NULL, [XE10] [tinyint] NULL, [XE11] [tinyint] NULL,</strong>
<strong class="calibre1">      [XE12] [tinyint] NULL, [XE13] [tinyint] NULL, [XE14] [tinyint] NULL,</strong>
<strong class="calibre1">      [XE15] [tinyint] NULL, [XE16] [tinyint] NULL, [XE17] [tinyint] NULL,</strong>
 <strong class="calibre1">     [XE18] [tinyint] NULL, [XE19] [tinyint] NULL, [XE20] [tinyint] NULL,</strong>
<strong class="calibre1">      [XE21] [tinyint] NULL, [XE22] [tinyint] NULL, [XE23] [tinyint] NULL,</strong>
<strong class="calibre1">      [XE24] [tinyint] NULL, [XE25] [tinyint] NULL, [XE26] [tinyint] NULL,</strong>
<strong class="calibre1">      [XE27] [tinyint] NULL, [XE28] [tinyint] NULL, [XE29] [tinyint] NULL,</strong>
<strong class="calibre1">      [XE30] [tinyint] NULL, [XE31] [tinyint] NULL, [XE32] [tinyint] NULL</strong>
<strong class="calibre1">) ON [PRIMARY];</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">Then, import the measure that can be found in the accompanying code file. There are 433 measuring points from 32 different extended events for the purpose of understanding the server and its environment settings.</p>
<p class="calibre2">After the initial load, the table will be populated with measures of different extended events that have also been discretized and cleaned for further data analysis:</p>
<div class="packt_figure"><img class="aligncenter92" src="../images/00160.gif"/></div>
<p class="calibre2">The <kbd class="calibre11">boxplot</kbd> function enables users to explore the distribution of each of the measures and find potential outliers. Use only R code to explore the data:</p>
<pre class="calibre19"><strong class="calibre1">dbConn &lt;- odbcDriverConnect('driver={SQL Server};server=TOMAZK\\MSSQLSERVER2017;database=ServerInfo;trusted_connection=true') 
server.feature &lt;- sqlQuery(dbConn, 'SELECT * FROM Server_info') 
close(dbConn) 
boxplot(server.feature)</strong> </pre>
<p class="calibre2">The following graph gives a quick overview:</p>
<div class="packt_figure"><img src="../images/00161.gif" class="calibre88"/></div>
<p class="calibre2">Boxplot shows that there are four events, where the value is vastly exceeding the average and third quartile. Cleaning these outliers will make the data easier to read and not result in abnormal distribution and skewed results. Note that there are particular analyses that deal with outliers and searching for such values. For this demo, we will recode these values into N/A.</p>
<p class="calibre2">After cleaning, adding summary statistics and a correlation is a relevant way to see how all of the events are correlating with one another:</p>
<pre class="calibre19"><strong class="calibre1"># replace value 25 with N/A</strong>
<strong class="calibre1">server.feature$XE12[server.feature$XE12=="25"]&lt;-NA</strong>
<strong class="calibre1">server.feature$XE18[server.feature$XE18=="25"]&lt;-NA</strong>
<strong class="calibre1">server.feature$XE24[server.feature$XE24=="25"]&lt;-NA</strong>
<strong class="calibre1">server.feature$XE27[server.feature$XE27=="25"]&lt;-NA</strong>
    
<strong class="calibre1">cor.plot(server.feature,numbers=TRUE,main="Server Features")</strong>  </pre>
<p class="calibre2">A correlation matrix of server features is a nice way to represent which events are correlating and which are not, and how:</p>
<div class="packt_figure"><img src="../images/00162.jpeg" class="calibre89"/></div>
<p class="calibre2">Going a step further, let's reduce these extended event measures, because it is obvious that not all are playing an important role and some might be just overhead. From the preceding heat map, it is hard to see which correlations for certain measures are not working out; therefore, we will use factor analysis. This observes correlations among the variables in such a way that it reflects the lower number of underlying variables. A factor is an underlying variable, that is, a latent variable that gets structured based on the observed and correlated variables. Structure is created by each of the factors being loaded with the response of correlated variable. This means that factor 1 can be, for example, loaded 25% with variable <kbd class="calibre11">A</kbd>, 65% with variable <kbd class="calibre11">B</kbd>, and 10% with variable <kbd class="calibre11">C</kbd>. So factor 1 will take the majority (65%) of the features from variable <kbd class="calibre11">A</kbd> and so on.</p>
<p class="calibre2">In this manner, factor analysis will try to reduce the number of original correlated variables (our measures of extended events) and try to create new structured variables.</p>
<p class="calibre2">Exploring data with R code, a simple exploratory factor analysis can reveal a number of factors:</p>
<pre class="calibre19"><strong class="calibre1">fa.parallel(server.feature, fa="fa")</strong>  </pre>
<p class="calibre2">The following screen plot reveals that there are seven factors available for extraction:</p>
<div class="packt_figure"><img src="../images/00163.gif" class="calibre90"/></div>
<p class="calibre2">In the following way, this will also reveal the loadings for the factors themselves; simply call the R function:</p>
<pre class="calibre19"><strong class="calibre1">fa.model &lt;- fa(server.feature,7,n.obs = 459,fm="pa",scores="regression", use="pairwise",rotate="varimax") #can use WLS - weighted least squares<br class="title-page-name"/>fa.model.r &lt;- target.rot(fa.model)<br class="title-page-name"/>fa.diagram(fa.model.r)<br class="title-page-name"/></strong></pre>
<p class="calibre2">The following diagram shows how loading construct each of the factor.:</p>
<div class="packt_figure"><img src="../images/00164.gif" class="calibre91"/></div>
<p class="calibre2">Storing loadings back into the database for further analysis and factor naming are a common practice and given the ability to incorporate factors into any further analysis (for example: classification or clustering methods) . Now that we know the number of factors, we can store the loadings into the database:</p>
<pre class="calibre19"><strong class="calibre1">-- Factor Analysis</strong>
<strong class="calibre1">-- extract factor loadings</strong>
    
<strong class="calibre1">DECLARE @Rcode NVARCHAR(MAX)</strong>
<strong class="calibre1">SET @Rcode = N'</strong>
<strong class="calibre1">      ## with actual FA funcitons</strong>
<strong class="calibre1">      library(psych)</strong>
<strong class="calibre1">      library(Hmisc)</strong>
<strong class="calibre1">      ## for data munching and visualization</strong>
<strong class="calibre1">      library(ggplot2)</strong>
<strong class="calibre1">      library(plyr)</strong>
<strong class="calibre1">      library(pastecs)</strong>
  
<strong class="calibre1">      server.feature &lt;- InputDataSet</strong>
    
<strong class="calibre1">      server.feature$XE12[server.feature$XE12=="25"]&lt;-NA</strong>
<strong class="calibre1">      server.feature$XE18[server.feature$XE18=="25"]&lt;-NA</strong>
<strong class="calibre1">      server.feature$XE24[server.feature$XE24=="25"]&lt;-NA</strong>
<strong class="calibre1">      server.feature$XE27[server.feature$XE27=="25"]&lt;-NA</strong>
    
<strong class="calibre1">      fa.model &lt;- fa(server.feature</strong>
<strong class="calibre1">               ,7</strong>
<strong class="calibre1">               ,fm="pa"</strong>
 <strong class="calibre1">              ,scores="regression"</strong>
<strong class="calibre1">               ,use="pairwise"</strong>
<strong class="calibre1">               ,rotate="varimax") #can use WLS - weighted least squares</strong>
    
<strong class="calibre1">      fa.loadings &lt;- as.list.data.frame(fa.model$loadings)</strong>
    <strong class="calibre1">  OutputDataSet &lt;- data.frame(fa.loadings)'</strong>
    
 <strong class="calibre1">EXEC sp_execute_external_script</strong>
<strong class="calibre1">       @language = N'R'</strong>
<strong class="calibre1">      ,@script = @Rcode</strong>
  <strong class="calibre1">    ,@input_data_1 = N'SELECT * FROM server_info'</strong>
<strong class="calibre1">WITH RESULT SETS</strong>
<strong class="calibre1">((</strong>
<strong class="calibre1">       PA1 NUMERIC(16,3)</strong>
<strong class="calibre1">      ,PA2 NUMERIC(16,3)</strong>
<strong class="calibre1">      ,PA3 NUMERIC(16,3)</strong>
<strong class="calibre1">      ,PA4 NUMERIC(16,3)</strong>
<strong class="calibre1">      ,PA5 NUMERIC(16,3)</strong>
<strong class="calibre1">      ,PA6 NUMERIC(16,3)</strong>
<strong class="calibre1">      ,PA7 NUMERIC(16,3)</strong>
 <strong class="calibre1">))</strong>  </pre>
<p class="calibre2">The results can be interpreted as: the higher the value (positive or negative), the more loaded is a particular measure is with the accompanying factor:</p>
<div class="packt_figure"><img class="aligncenter93" src="../images/00165.gif"/></div>
<p class="calibre2">Factor 1, <kbd class="calibre11">PA1</kbd>, is mostly loaded with XE03 (<kbd class="calibre11">0.680</kbd>), XE13 (<kbd class="calibre11">0.640</kbd>), XE18 (<kbd class="calibre11">-0.578</kbd>), and XE28 (<kbd class="calibre11">0.652</kbd>). All four are measuring the transactions of query and are as shown in the following screenshot:</p>
<div class="packt_figure"><img class="aligncenter94" src="../images/00166.jpeg"/></div>
<p class="calibre2">Here, the negative value is the free space in <span>tempdb (KB)</span> that is negatively loaded and it only means the relationship to the factor. But having the ability to reduce the number of extended events, and to have them combined through the advanced statistical analysis, is a very neat approach to a potentially complex problem.</p>
<p class="calibre2">With loadings in the database, I can additionally represent how these factors are being dispersed on a scatterplot. I have exported the results of the preceding query to Power BI and used the clustering visual. Furthermore, you can see clusters of these factor loadings and the ones that are similar. The red group (to the left) is again something that DBAs and data scientists should look at together for further examination:</p>
<div class="packt_figure"><img src="../images/00167.jpeg" class="calibre92"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating a baseline and workloads, and replaying</h1>
                
            
            <article>
                
<p class="calibre2">Given the ability to reduce and create new measures that are tailored and adapted to your particular server or environment, now we want to understand how the system is behaving with all the other parameters unchanged (in Latin, <em class="calibre12">ceteris paribus</em>). This is the baseline. And with the baseline, we establish what is normal, or in other words, what the performance is under normal conditions. A baseline is used for comparing what might be or seem abnormal or out of the ordinary. It can also serve as a control group for any future tests (this works well especially when new patches are rolled out an upgrade of a particular environment/server needs to be performed).</p>
<p class="calibre2">A typical corporate baseline would be described as follows over a period of one day (24 hours) in the form of the number of database requests from users or machines:</p>
<div class="packt_figure"><img class="aligncenter95" src="../images/00168.gif"/></div>
<p class="calibre2">When all requests are represented as a breakdown for each of the corporate processes, immediate patterns can be seen.</p>
<p class="calibre2">The ERP system usually peak when people are at their workplace-on a normal day between 8.00 AM and 5.00 PM with two distinct peaks and a very obvious lunch break from 11.00 AM until 1.00 PM:</p>
<div class="packt_figure"><img class="aligncenter96" src="../images/00169.gif"/></div>
<p class="calibre2">Adding ETL jobs for daily ERP system maintenance, it is obvious where and when DBAs and SysAdmins usually try to squeeze these important jobs and how this is also limited and narrated by daily ERP workloads:</p>
<div class="packt_figure"><img class="aligncenter97" src="../images/00170.gif"/></div>
<p class="calibre2">A warehouse has a completely different behavior pattern, which means that it is usually the highest request in the morning hours 4.00 AM and 5.00 AM and somehow steady until evening hours:</p>
<div class="packt_figure"><img class="aligncenter98" src="../images/00171.gif"/></div>
<p class="calibre2">A bakery, on the contrary, has a reverse request to the database, since the majority of their activities are done starting from 9.00 PM until 3.00 AM, so that customers get fresh bread in the morning:</p>
<div class="packt_figure"><img class="aligncenter99" src="../images/00172.gif"/></div>
<p class="calibre2">Lastly, websites can be understood as a constant database request resource with relatively slight daily changes:</p>
<div class="packt_figure"><img class="aligncenter100" src="../images/00173.gif"/></div>
<p class="calibre2">All these can be understood as a daily baseline and of course, things get even more complicated when this is projected on a monthly basis. Also, patterns emerge immediately. During the weekends (day 7, 14, and 21) the requests are diminished and toward the end of the month, the financial cycle needs to be closed; hence, there is additional load on the database:</p>
<div class="packt_figure"><img class="aligncenter101" src="../images/00174.gif"/></div>
<p class="calibre2">Showing all these is crucial to understand how the baseline of the system (or environment) must be understood. This data can be collected using performance counters, many of the DMV, using the query store, and other tools. What we usually gather is what we will be later monitoring and predicting. So choosing wisely is the most important task, since through these measures, counters, and values you will be defining when your system is healthy and when it is not. But usually general information on the system and database is crucial. Also, SQL Server information, and many configurable parameters, query-related information, and database-I/O, and RAM-related information need to be stored as well.</p>
<p class="calibre2">After having a baseline, we need to have the workload created. Usually, the workload is captured against the baseline in the production server, and a restore of capture statistics is replayed on the test server/environment. Database tuning or changes in configuration can be alternated by replaying the captured workload from production on the test environment, by alternatively changing the values of a particular parameter. The next demo is a representation of the workload expressed through two parameters that have been changed when the same workload has been replied:</p>
<pre class="calibre19"><strong class="calibre1">USE [master];</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">CREATE DATABASE Workloads;</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">USE Workloads;</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">Querying the table <kbd class="calibre11">[dbo].[WLD]</kbd> is essentially just repeating the same workloads with changes on one or another parameter:</p>
<div class="packt_figure"><img src="../images/00175.gif" class="calibre93"/></div>
<p class="calibre2">First, we need to have the outlier analysis against the workload each time something has changed. T-SQL code with R can deliver a <em class="calibre12">Mahalanobis</em> graph, which clearly shows where the outliers are:</p>
<pre class="calibre19"><strong class="calibre1">EXEC sp_execute_external_script 
 @language = N'R' 
,@script = N' 
               library(car) 
               library(ggplot2) 
               dataset &lt;- InputDataSet 
               dataset$WL_ID &lt;- as.numeric(recode(dataset$WL_ID, "''WL1''=1; ''WL2''=2;''WL3''=3")) 
               dataset$Param1 &lt;- as.numeric(dataset$Param1) 
               dataset$Param2 &lt;- as.numeric(dataset$Param2) 
 
               m.dist &lt;- mahalanobis(dataset, colMeans(dataset), cov(dataset)) 
               dataset$maha_dist &lt;- round(m.dist) 
 
               # Mahalanobis Outliers - Threshold set to 7 
               dataset$outlier_mah &lt;- "No" 
               dataset$outlier_mah[dataset$maha_dist &gt; 7] &lt;- "Yes" 
 
                image_file = tempfile();   
               jpeg(filename = image_file);   
 
               # Scatterplot for checking outliers using Mahalanobis  
               ggplot(dataset, aes(x = Param1, y = Param2, color = outlier_mah)) + 
                 geom_point(size = 5, alpha = 0.6) + 
                 labs(title = "Mahalanobis distances for multivariate regression outliers", 
                        subtitle = "Comparison on 1 parameter for three synthetic Workloads") + 
                 xlab("Parameter 1") + 
                 ylab("Parameter 2") + 
                 scale_x_continuous(breaks = seq(5, 55, 5)) + 
                 scale_y_continuous(breaks = seq(0, 70, 5))    + geom_abline(aes(intercept = 12.5607 , slope = 0.5727)) 
 
                 dev.off();  
               OutputDataSet &lt;- data.frame(data=readBin(file(image_file, "rb"), what=raw(), n=1e6))' 
,@input_data_1 = N'SELECT * FROM WLD'</strong> </pre>
<p class="calibre2">The graph was inserted into Power BI, where the workloads can be changed against both of the parameters. So DBAs cannot only change the workloads, they can also see which outliers have caused and needed extra attention when performing a replay on the restored workload:</p>
<div class="packt_figure"><img class="aligncenter102" src="../images/00176.jpeg"/></div>
<p class="calibre2">ANOVA or MANOVA can also be performed to see specific changes among the workloads. R code can do just that:</p>
<pre class="calibre19"><strong class="calibre1">dataset$Param1 &lt;- as.numeric(dataset$Param1)</strong>
<strong class="calibre1">dataset$Param2 &lt;- as.numeric(dataset$Param2)</strong>
<strong class="calibre1">dataset$WL_ID &lt;- as.numeric(recode(dataset$WL_ID, "'WL1'=1; 'WL2'=2;'WL3'=3"))</strong>
    
<strong class="calibre1">LM.man &lt;- Anova(lm(cbind(Param1, Param2) ~ WL_ID, data=dataset))</strong>
<strong class="calibre1">summary(LM.man)</strong>    </pre>
<p class="calibre2">And the ANOVA statistics show the differences among the workloads and their changes in parameter settings:</p>
<div class="packt_figure"><img class="aligncenter103" src="../images/00177.gif"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating predictions with R - disk usage</h1>
                
            
            <article>
                
<p class="calibre2">Predictions involve spotting any unplanned and unwanted activities or unusual system behavior, especially when compared it to the baseline. In this manner, raising a red flag would result in fewer false positive states.</p>
<p class="calibre2">In addition, we always come across disk-size problems. Based on this problem, we will demo database growth, store the data, and then run predictions against the collected data to be able at the end to predict when a DBA can expect disk space problems.</p>
<p class="calibre2">To illustrate this scenario, I will create a small database of 8 MB and no possibility of growth. I will create two tables. One will serve as a baseline, <kbd class="calibre11">DataPack_Info_SMALL</kbd>, and the other will serve as a so-called everyday log, where everything will be stored for unexpected cases or undesired behavior. This will persist in the <kbd class="calibre11">DataPack_Info_LARGE</kbd> table.</p>
<p class="calibre2">First, create a database:</p>
<pre class="calibre19"><strong class="calibre1">USE [master];</strong>
<strong class="calibre1">GO</strong>
      
<strong class="calibre1">CREATE DATABASE FixSizeDB</strong>
<strong class="calibre1">CONTAINMENT = NONE</strong>
<strong class="calibre1">ON  PRIMARY</strong>
<strong class="calibre1">( NAME = N'FixSizeDB', FILENAME = N'C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER2017\MSSQL\DATA\FixSizeDB_2.mdf' , </strong>
<strong class="calibre1">SIZE = 8192KB , FILEGROWTH = 0)</strong>
<strong class="calibre1">LOG ON</strong>
<strong class="calibre1">( NAME = N'FixSizeDB_log', FILENAME = N'C:\Program Files\Microsoft SQL Server\MSSQL14.MSSQLSERVER2017\MSSQL\DATA\FixSizeDB_2_log.ldf',</strong>
<strong class="calibre1">SIZE = 8192KB , FILEGROWTH = 0)</strong>
<strong class="calibre1">GO</strong>
<strong class="calibre1">ALTER DATABASE [FixSizeDB] SET COMPATIBILITY_LEVEL = 140</strong>
<strong class="calibre1">GO</strong>
<strong class="calibre1">ALTER DATABASE [FixSizeDB] SET RECOVERY SIMPLE</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">The <kbd class="calibre11">DataPack</kbd> table will serve as a storage place for all the generated inserts and later deletes:</p>
<pre class="calibre19"><strong class="calibre1">CREATE TABLE DataPack</strong>
<strong class="calibre1">       (</strong>
<strong class="calibre1">         DataPackID BIGINT IDENTITY NOT NULL</strong>
<strong class="calibre1">        ,col1 VARCHAR(1000) NOT NULL</strong>
<strong class="calibre1">        ,col2 VARCHAR(1000) NOT NULL</strong>
<strong class="calibre1">       )</strong>  </pre>
<p class="calibre2">Populating the <kbd class="calibre11">DataPack</kbd> table will be done with the following simple <kbd class="calibre11">WHILE</kbd> loop:</p>
<pre class="calibre19"><strong class="calibre1">DECLARE @i INT = 1;</strong>
<strong class="calibre1">BEGIN TRAN</strong>
<strong class="calibre1">       WHILE @i &lt;= 1000</strong>
<strong class="calibre1">              BEGIN</strong>
<strong class="calibre1">                     INSERT dbo.DataPack(col1, col2)</strong>
<strong class="calibre1">                           SELECT</strong>
<strong class="calibre1">                                    REPLICATE('A',200)</strong>
<strong class="calibre1">                                   ,REPLICATE('B',300);</strong>
<strong class="calibre1">                     SET @i = @i + 1;</strong>
<strong class="calibre1">              END</strong>
<strong class="calibre1">COMMIT;</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">Capturing disk space changes with the following query will be very important for the task:</p>
<pre class="calibre19"><strong class="calibre1">SELECT</strong>
<strong class="calibre1">    t.NAME AS TableName</strong>
<strong class="calibre1">    ,s.Name AS SchemaName</strong>
<strong class="calibre1">    ,p.rows AS RowCounts</strong>
<strong class="calibre1">    ,SUM(a.total_pages) * 8 AS TotalSpaceKB</strong>
<strong class="calibre1">    ,SUM(a.used_pages) * 8 AS UsedSpaceKB</strong>
<strong class="calibre1">    ,(SUM(a.total_pages) - SUM(a.used_pages)) * 8 AS UnusedSpaceKB</strong>
<strong class="calibre1">FROM</strong>
<strong class="calibre1">    sys.tables t</strong>
<strong class="calibre1">INNER JOIN sys.indexes AS i</strong>
<strong class="calibre1">       ON t.OBJECT_ID = i.object_id</strong>
<strong class="calibre1">INNER JOIN sys.partitions AS p</strong>
<strong class="calibre1">       ON i.object_id = p.OBJECT_ID</strong>
<strong class="calibre1">       AND i.index_id = p.index_id</strong>
<strong class="calibre1">INNER JOIN sys.allocation_units AS a</strong>
<strong class="calibre1">       ON p.partition_id = a.container_id</strong>
<strong class="calibre1">LEFT OUTER JOIN sys.schemas AS s</strong>
<strong class="calibre1">       ON t.schema_id = s.schema_id</strong>
<strong class="calibre1">WHERE</strong>
<strong class="calibre1">        t.NAME NOT LIKE 'dt%'</strong>
<strong class="calibre1">    AND t.is_ms_shipped = 0</strong>
<strong class="calibre1">    AND i.OBJECT_ID &gt; 255</strong>
<strong class="calibre1">    AND t.Name = 'DataPack'</strong>
<strong class="calibre1">GROUP BY t.Name, s.Name, p.Rows</strong>  </pre>
<p class="calibre2">The <kbd class="calibre11">Log</kbd> table will be filled along with the <kbd class="calibre11">DataPack</kbd> table in order to gather immediate changes in disk space:</p>
<pre class="calibre19"><strong class="calibre1">DECLARE @nof_steps INT = 0</strong>
<strong class="calibre1">WHILE @nof_steps &lt; 15</strong>
<strong class="calibre1">BEGIN</strong>
<strong class="calibre1">       BEGIN TRAN</strong>
<strong class="calibre1">              -- insert some data</strong>
<strong class="calibre1">              DECLARE @i INT = 1;</strong>
 <strong class="calibre1">             WHILE @i &lt;= 1000 -- step is 100 rows</strong>
<strong class="calibre1">                                  BEGIN</strong>
   <strong class="calibre1">                                      INSERT dbo.DataPack(col1, col2)</strong>
<strong class="calibre1">                                                SELECT</strong>
    <strong class="calibre1">                                                         REPLICATE('A',FLOOR(RAND()*200))</strong>
    <strong class="calibre1">                                                        ,REPLICATE('B',FLOOR(RAND()*300));</strong>
<strong class="calibre1">                                         SET @i = @i + 1;</strong>
<strong class="calibre1">                                  END</strong>
<strong class="calibre1">              -- run statistics on table</strong>
<strong class="calibre1">              INSERT INTO dbo.DataPack</strong>
<strong class="calibre1">              SELECT</strong>
    <strong class="calibre1">                 t.NAME AS TableName</strong>
   <strong class="calibre1">                  ,s.Name AS SchemaName</strong>
    <strong class="calibre1">                 ,p.rows AS RowCounts</strong>
    <strong class="calibre1">                 ,SUM(a.total_pages) * 8 AS TotalSpaceKB</strong>
    <strong class="calibre1">                 ,SUM(a.used_pages) * 8 AS UsedSpaceKB</strong>
    <strong class="calibre1">                 ,(SUM(a.total_pages) - SUM(a.used_pages)) * 8 AS UnusedSpaceKB</strong>
<strong class="calibre1">                     ,GETDATE() AS TimeMeasure</strong>
 <strong class="calibre1">             FROM </strong>
<strong class="calibre1">                           sys.tables AS t</strong>
    <strong class="calibre1">                 INNER JOIN sys.indexes AS i</strong>
    <strong class="calibre1">                 ON t.OBJECT_ID = i.object_id</strong>
    <strong class="calibre1">                 INNER JOIN sys.partitions AS p</strong>
    <strong class="calibre1">                 ON i.object_id = p.OBJECT_ID</strong>
    <strong class="calibre1">                 AND i.index_id = p.index_id</strong>
    <strong class="calibre1">                 INNER JOIN sys.allocation_units AS a</strong>
    <strong class="calibre1">                 ON p.partition_id = a.container_id</strong>
    <strong class="calibre1">                 LEFT OUTER JOIN sys.schemas AS s</strong>
    <strong class="calibre1">                 ON t.schema_id = s.schema_id</strong>
    <strong class="calibre1">          WHERE</strong>
    <strong class="calibre1">                        t.NAME NOT LIKE 'dt%'</strong>
    <strong class="calibre1">                 AND t.is_ms_shipped = 0</strong>
    <strong class="calibre1">                 AND t.name = 'DataPack'</strong>
    <strong class="calibre1">                 AND i.OBJECT_ID &gt; 255</strong>
    <strong class="calibre1">          GROUP BY t.Name, s.Name, p.Rows</strong>
    <strong class="calibre1">          WAITFOR DELAY '00:00:02'</strong>
    <strong class="calibre1">   COMMIT;</strong>
<strong class="calibre1">END</strong></pre>
<p class="calibre2">This will serve as our baseline when comparing the results. When we query the <kbd class="calibre11">DataPack_Log_Small</kbd> table, the results are as follows:</p>
<pre class="calibre19"><strong class="calibre1">DECLARE @RScript nvarchar(max)</strong>
<strong class="calibre1">SET @RScript = N'</strong>
<strong class="calibre1">                            library(Hmisc)     </strong>
<strong class="calibre1">                            mydata &lt;- InputDataSet</strong>
<strong class="calibre1">                            all_sub &lt;- mydata[2:3]</strong>
<strong class="calibre1">                            c &lt;- cor(all_sub, use="complete.obs", method="pearson")</strong>
<strong class="calibre1">                            t &lt;- rcorr(as.matrix(all_sub), type="pearson")</strong>
<strong class="calibre1">                            c &lt;- cor(all_sub, use="complete.obs", method="pearson")</strong>
<strong class="calibre1">                            c &lt;- data.frame(c)</strong>
<strong class="calibre1">                            OutputDataSet &lt;- c'</strong>
<strong class="calibre1">DECLARE @SQLScript nvarchar(max)</strong>
<strong class="calibre1">SET @SQLScript = N'SELECT</strong>
<strong class="calibre1">                                          TableName</strong>
<strong class="calibre1">                                         ,RowCounts</strong>
<strong class="calibre1">                                         ,UsedSpaceKB</strong>
<strong class="calibre1">                                         ,TimeMeasure</strong>
<strong class="calibre1">                                         FROM DataPack_Info_SMALL'</strong>
<strong class="calibre1">EXECUTE sp_execute_external_script</strong>
<strong class="calibre1">        @language = N'R'</strong>
<strong class="calibre1">       ,@script = @RScript</strong>
<strong class="calibre1">       ,@input_data_1 = @SQLScript</strong>
<strong class="calibre1">       WITH result SETS ((RowCounts VARCHAR(100)</strong>
<strong class="calibre1">                         ,UsedSpaceKB  VARCHAR(100)));</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">As a result, we get a strong and positive correlation between the <kbd class="calibre11">RowCounts</kbd> and <kbd class="calibre11">UsedSpaceKB</kbd> columns. This can easily be interpreted as: when the value for <kbd class="calibre11">RowCounts</kbd> goes up, the value for <kbd class="calibre11">UsedSpaceKB</kbd> also goes up. This is the only logical explanation. It would be somehow strange to have a negative correlation. Now, we will try to simulate random deletes and inserts and observe a similar behavior with the following code:</p>
<pre class="calibre19"><strong class="calibre1">DECLARE @nof_steps INT = 0</strong>
<strong class="calibre1">WHILE @nof_steps &lt; 15</strong>
<strong class="calibre1">BEGIN</strong>
<strong class="calibre1">       BEGIN TRAN</strong>
<strong class="calibre1">              -- insert some data</strong>
<strong class="calibre1">              DECLARE @i INT = 1;</strong>
<strong class="calibre1">              DECLARE @insertedRows INT = 0;</strong>
   <strong class="calibre1">           DECLARE @deletedRows INT = 0;</strong>
<strong class="calibre1">              DECLARE @Rand DECIMAL(10,2) = RAND()*10</strong>
<strong class="calibre1">              IF @Rand &lt; 5</strong>
<strong class="calibre1">                BEGIN</strong>
<strong class="calibre1">                                  WHILE @i &lt;= 1000 -- step is 100 rows</strong>
<strong class="calibre1">                                                       BEGIN</strong>
    <strong class="calibre1">                                                              INSERT dbo.DataPack(col1, col2)</strong>
    <strong class="calibre1">                                                                     SELECT</strong>
    <strong class="calibre1">                                                                             REPLICATE('A',FLOOR(RAND()*200))  -- pages are filling up differently</strong>
    <strong class="calibre1">                                                                            ,REPLICATE('B',FLOOR(RAND()*300));</strong>
    <strong class="calibre1">                                                           SET @i = @i + 1;</strong>
    <strong class="calibre1">                                                       END</strong>
 <strong class="calibre1">                                 SET @insertedRows = 1000                                </strong>
 <strong class="calibre1">                    END</strong>
 
 <strong class="calibre1">              IF @Rand  &gt;= 5</strong>
 <strong class="calibre1">                    BEGIN                                             </strong>
 <strong class="calibre1">                                 SET @deletedRows = (SELECT COUNT(*) FROM dbo.DataPack WHERE DataPackID % 3 = 0)</strong>
 <strong class="calibre1">                                 DELETE FROM dbo.DataPack</strong>
    <strong class="calibre1">                                                WHERE</strong>
 <strong class="calibre1">                                 DataPackID % 3 = 0 OR DataPackID % 5 = 0</strong>
    
 <strong class="calibre1">                    END</strong>
 <strong class="calibre1">             -- run statistics on table</strong>
 <strong class="calibre1">             INSERT INTO dbo.DataPack_Info_LARGE</strong>
 <strong class="calibre1">             SELECT</strong>
 <strong class="calibre1">                    t.NAME AS TableName</strong>
 <strong class="calibre1">                    ,s.Name AS SchemaName</strong>
 <strong class="calibre1">                    ,p.rows AS RowCounts</strong>
 <strong class="calibre1">                    ,SUM(a.total_pages) * 8 AS TotalSpaceKB</strong>
 <strong class="calibre1">                    ,SUM(a.used_pages) * 8 AS UsedSpaceKB</strong>
 <strong class="calibre1">                    ,(SUM(a.total_pages) - SUM(a.used_pages)) * 8 AS UnusedSpaceKB</strong>
 <strong class="calibre1">                    ,GETDATE() AS TimeMeasure</strong>
 <strong class="calibre1">                    ,CASE WHEN @Rand &lt; 5 THEN 'Insert'</strong>
 <strong class="calibre1">                            WHEN @Rand &gt;= 5 THEN 'Delete'</strong>
 <strong class="calibre1">                            ELSE 'meeeh' END AS Operation</strong>
 <strong class="calibre1">                    ,CASE WHEN @Rand &lt; 5 THEN @insertedRows</strong>
 <strong class="calibre1">                            WHEN @Rand &gt;= 5 THEN @deletedRows</strong>
 <strong class="calibre1">                            ELSE 0 END AS NofRowsOperation</strong>
 <strong class="calibre1">             FROM </strong>
 <strong class="calibre1">                          sys.tables AS t</strong>
 <strong class="calibre1">                    INNER JOIN sys.indexes AS i</strong>
 <strong class="calibre1">                    ON t.OBJECT_ID = i.object_id</strong>
 <strong class="calibre1">                    INNER JOIN sys.partitions AS p</strong>
 <strong class="calibre1">                    ON i.object_id = p.OBJECT_ID</strong>
 <strong class="calibre1">                    AND i.index_id = p.index_id</strong>
 <strong class="calibre1">                    INNER JOIN sys.allocation_units AS a</strong>
 <strong class="calibre1">                    ON p.partition_id = a.container_id</strong>
 <strong class="calibre1">                    LEFT OUTER JOIN sys.schemas AS s</strong>
 <strong class="calibre1">                    ON t.schema_id = s.schema_id</strong>
    
 <strong class="calibre1">             WHERE</strong>
 <strong class="calibre1">                           t.NAME NOT LIKE 'dt%'</strong>
 <strong class="calibre1">                    AND t.is_ms_shipped = 0</strong>
 <strong class="calibre1">                    AND t.name = 'DataPack'</strong>
 <strong class="calibre1">                    AND i.OBJECT_ID &gt; 255</strong>
 <strong class="calibre1">             GROUP BY t.Name, s.Name, p.Rows</strong>
 <strong class="calibre1">             WAITFOR DELAY '00:00:01'</strong>
 <strong class="calibre1">      COMMIT;</strong>
<strong class="calibre1">END</strong>  </pre>
<p class="calibre2">We have added a <kbd class="calibre11">DELETE</kbd> statement, as well as <kbd class="calibre11">RowCounts</kbd>, so that the demo will not be so straightforward. By calculating the correlation coefficient, it is obvious, that we again get a very strong and positive correlation.</p>
<p class="calibre2">We will now compare our <kbd class="calibre11">LARGE</kbd> test with the baseline by running the same correlation coefficient on different datasets. The first is on our baseline (<kbd class="calibre11">DataPack_Info_SMALL</kbd>) and the second one is from our test table (<kbd class="calibre11">DataPack_Info_LARGE</kbd>):</p>
<pre class="calibre19"><strong class="calibre1">DECLARE @RScript1 nvarchar(max)</strong>
<strong class="calibre1">SET @RScript1 = N'</strong>
    <strong class="calibre1">                            library(Hmisc)     </strong>
    <strong class="calibre1">                            mydata &lt;- InputDataSet</strong>
    <strong class="calibre1">                            all_sub &lt;- mydata[4:5]</strong>
    <strong class="calibre1">                            c &lt;- cor(all_sub, use="complete.obs", method="pearson")</strong>
    <strong class="calibre1">                            c &lt;- data.frame(c)</strong>
    <strong class="calibre1">                            OutputDataSet &lt;- c'</strong>
    
<strong class="calibre1">DECLARE @SQLScript1 nvarchar(max)</strong>
<strong class="calibre1">SET @SQLScript1 = N'SELECT</strong>
    
    <strong class="calibre1">                                          TableName</strong>
    <strong class="calibre1">                                         ,RowCounts</strong>
    <strong class="calibre1">                                         ,TimeMeasure</strong>
    <strong class="calibre1">                                         ,UsedSpaceKB </strong>
    <strong class="calibre1">                                         ,UnusedSpaceKB</strong>
    <strong class="calibre1">                                         FROM DataPack_Info_SMALL</strong>
    <strong class="calibre1">                                         WHERE RowCounts &lt;&gt; 0'</strong>
<strong class="calibre1">EXECUTE sp_execute_external_script</strong>
<strong class="calibre1">        @language = N'R'</strong>
<strong class="calibre1">       ,@script = @RScript1</strong>
<strong class="calibre1">       ,@input_data_1 = @SQLScript1</strong>
 <strong class="calibre1">      WITH result SETS ( (</strong>
    <strong class="calibre1">                                      RowCounts VARCHAR(100)</strong>
    <strong class="calibre1">                                     ,UsedSpaceKB  VARCHAR(100)</strong>
    <strong class="calibre1">                                     ));</strong>
    
    
<strong class="calibre1">DECLARE @RScript2 nvarchar(max)</strong>
<strong class="calibre1">SET @RScript2 = N'</strong>
    <strong class="calibre1">                        library(Hmisc)     </strong>
    <strong class="calibre1">                        mydata &lt;- InputDataSet</strong>
    <strong class="calibre1">                        all_sub &lt;- mydata[4:5]</strong>
    <strong class="calibre1">                        c &lt;- cor(all_sub, use="complete.obs", method="pearson")</strong>
    <strong class="calibre1">                        c &lt;- data.frame(c)</strong>
    <strong class="calibre1">                        OutputDataSet &lt;- c'</strong>
<strong class="calibre1">DECLARE @SQLScript2 nvarchar(max)</strong>
<strong class="calibre1">SET @SQLScript2 = N'SELECT</strong>
    <strong class="calibre1">                                      TableName</strong>
    <strong class="calibre1">                                     ,RowCounts</strong>
    <strong class="calibre1">                                     ,TimeMeasure</strong>
    <strong class="calibre1">                                     ,UsedSpaceKB </strong>
    <strong class="calibre1">                                     ,UnusedSpaceKB</strong>
    <strong class="calibre1">                                     FROM DataPack_Info_LARGE</strong>
    <strong class="calibre1">                                     WHERE NofRowsOperation &lt;&gt; 0</strong>
    <strong class="calibre1">                                     AND RowCounts &lt;&gt; 0'</strong>
    
<strong class="calibre1">EXECUTE sp_execute_external_script</strong>
<strong class="calibre1">        @language = N'R'</strong>
<strong class="calibre1">       ,@script = @RScript2</strong>
<strong class="calibre1">       ,@input_data_1 = @SQLScript2</strong>
<strong class="calibre1">       WITH result SETS ( (</strong>
<strong class="calibre1">                                          RowCounts VARCHAR(100)</strong>
<strong class="calibre1">                                         ,UsedSpaceKB  VARCHAR(100)</strong>
   <strong class="calibre1">                                      )</strong>
   <strong class="calibre1">                                );</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">The results are very interesting. The baseline shows no correlation between <kbd class="calibre11">UsedSpaceKB</kbd> and <kbd class="calibre11">UnusedSpaceKB</kbd> (it is <kbd class="calibre11">-0.049</kbd>), whereas our test shows an almost 3x stronger negative correlation (it is <kbd class="calibre11">-0.109</kbd>). A couple of words on this correlation: this shows that <kbd class="calibre11">UsedSpaceKB</kbd> is negatively correlated with <kbd class="calibre11">UnUsedSpaceKB</kbd>; this which is still too small to draw any concrete conclusions, but it shows how a slight change can cause a difference in a simple correlation.</p>
<p class="calibre2">You can gather disk space usage information with T-SQL, by using PowerShell, by implementing .NET assembly, or creating a SQL Server job, or any other way. The important part and the biggest advantage is that, using R and the data collected, now you will not only be monitoring and reacting to the past data, but you will also be able to predict what will happen.</p>
<p class="calibre2">Let's go a step further and assume the following query and dataset taken from our sample created:</p>
<pre class="calibre19"><strong class="calibre1">SELECT</strong>
<strong class="calibre1">  TableName</strong>
<strong class="calibre1"> ,Operation</strong>
<strong class="calibre1"> ,NofRowsOperation</strong>
<strong class="calibre1"> ,UsedSpaceKB</strong>
<strong class="calibre1"> ,UnusedSpaceKB</strong>
<strong class="calibre1">FROM dbo.DataPack_Info_LARGE</strong></pre>
<p class="calibre2">We will give a prediction on the size of the <kbd class="calibre11">usedSpaceKB</kbd> based on historical data. Our input will be <kbd class="calibre11">TableName</kbd>, <kbd class="calibre11">Operation</kbd>, and <kbd class="calibre11">NofRowsOperation</kbd> for a given number to predict on. I will be using a general linear model (the GLM algorithm) for predicting <kbd class="calibre11">usedDiskSpace</kbd>! Before you all start saying this is absurd, this cannot be done due to DBCC caching, page brakes, indexes, stall statistics, and many other parameters, I would like to point out that all this information can be added into the algorithm and would make the prediction even better. Since my queries are very simple <kbd class="calibre11">INSERT</kbd> and <kbd class="calibre11">DELETE</kbd> statements, you should also know what kinds of queries are you predicting. In addition, such an approach can be good for code testing, unit testing, and stress testing before deployment.</p>
<p class="calibre2">With the following R code, we can start creating predictions:</p>
<pre class="calibre19"><strong class="calibre1">-- GLM prediction</strong>
<strong class="calibre1">DECLARE @SQL_input AS NVARCHAR(MAX)</strong>
<strong class="calibre1">SET @SQL_input = N'SELECT</strong>
<strong class="calibre1">                                  TableName</strong>
  <strong class="calibre1">                                ,CASE WHEN Operation = ''Insert'' THEN 1 ELSE 0 END AS Operation</strong>
<strong class="calibre1">                                  ,NofRowsOperation</strong>
<strong class="calibre1">                                  ,UsedSpaceKB</strong>
<strong class="calibre1">                                  ,UnusedSpaceKB</strong>
<strong class="calibre1">                                   FROM dbo.DataPack_Info_LARGE</strong>
<strong class="calibre1">                                   WHERE</strong>
<strong class="calibre1">                                         NofRowsOperation &lt;&gt; 0';</strong>
    
<strong class="calibre1">DECLARE @R_code AS NVARCHAR(MAX)</strong>
<strong class="calibre1">SET @R_code = N'library(RevoScaleR)</strong>
<strong class="calibre1">                library(dplyr)</strong>
    <strong class="calibre1">            DPLogR &lt;- rxGlm(UsedSpaceKB ~ Operation + NofRowsOperation + UnusedSpaceKB, data = DataPack_info, family = Gamma)</strong>
 <strong class="calibre1">               df_predict &lt;- data.frame(TableName=("DataPack"), Operation=(1), NofRowsOperation=(451), UnusedSpaceKB=(20))</strong>
<strong class="calibre1">                predictions &lt;- rxPredict(modelObject = DPLogR, data = df_predict, outData = NULL,  </strong>
  <strong class="calibre1">                              predVarNames = "UsedSpaceKB", type = "response",checkFactorLevels=FALSE);</strong>
  <strong class="calibre1">              OutputDataSet &lt;- predictions'</strong>
    
<strong class="calibre1">EXEC sys.sp_execute_external_script</strong>
<strong class="calibre1">     @language = N'R'</strong>
  <strong class="calibre1">  ,@script = @R_code</strong>
  <strong class="calibre1">  ,@input_data_1 = @SQL_input</strong>
<strong class="calibre1">    ,@input_data_1_name = N'DataPack_info'</strong>
   <strong class="calibre1">    WITH RESULT SETS ((</strong>
    <strong class="calibre1">                     UsedSpaceKB_predict INT</strong>
    <strong class="calibre1">                     ));</strong>
<strong class="calibre1">GO</strong> </pre>
<p class="calibre2">Now we can predict the size of <kbd class="calibre11">UsedSpaceKB</kbd> based on the following data:</p>
<pre class="calibre19"><strong class="calibre1">df_predict &lt;- data.frame(TableName=("DataPack"), Operation=(1), NofRowsOperation=(451), UnusedSpaceKB=(20))</strong></pre>
<p class="calibre2">We have a few things to clear out first. The following R code with the <kbd class="calibre11">xp_execute_external_script</kbd> would work much better as a stored procedure with input parameters for these columns: <kbd class="calibre11">TableName</kbd>, <kbd class="calibre11">Operation</kbd>, <kbd class="calibre11">NofRowsOperation</kbd>, and <kbd class="calibre11">UnusedSpaceKB</kbd>. Furthermore, to avoid unnecessary computational time for model building, it is usually the practice to store a serialized model in a SQL table and just deserialize it when running predictions. At last, since this was just a demo, make sure that the numbers used in predictions make sense, As we saw in our example, the <kbd class="calibre11">UsedSpaceKB</kbd> would be predicted much better if absolutely calculated, rather than using the cumulative values. Only later is the cumulative value calculated.</p>
<p class="calibre2">To sum up this rather long demo, let's create a procedure and run some predictions to see how efficient this is. The stored procedure is as follows:</p>
<pre class="calibre19"><strong class="calibre1">CREATE PROCEDURE Predict_UsedSpace</strong>
    <strong class="calibre1">(</strong>
    <strong class="calibre1"> @TableName NVARCHAR(100)</strong>
    <strong class="calibre1">,@Operation CHAR(1)  -- 1  = Insert; 0 = Delete</strong>
    <strong class="calibre1">,@NofRowsOperation NVARCHAR(10)</strong>
    <strong class="calibre1">,@UnusedSpaceKB NVARCHAR(10)</strong>
    <strong class="calibre1">)</strong>
    <strong class="calibre1">AS</strong>
    <strong class="calibre1">DECLARE @SQL_input AS NVARCHAR(MAX)</strong>
    <strong class="calibre1">SET @SQL_input = N'SELECT</strong>
    <strong class="calibre1">                                  TableName</strong>
    <strong class="calibre1">                                  ,CASE WHEN Operation = ''Insert'' THEN 1 ELSE 0 END AS Operation</strong>
    <strong class="calibre1">                                  ,NofRowsOperation</strong>
    <strong class="calibre1">                                  ,UsedSpaceKB</strong>
    <strong class="calibre1">                                  ,UnusedSpaceKB</strong>
    <strong class="calibre1">                                   FROM dbo.DataPack_Info_LARGE</strong>
    <strong class="calibre1">                                   WHERE</strong>
    <strong class="calibre1">                                         NofRowsOperation &lt;&gt; 0';</strong>
    <strong class="calibre1">DECLARE @R_code AS NVARCHAR(MAX)</strong>
    <strong class="calibre1">SET @R_code = N'library(RevoScaleR)</strong>
    <strong class="calibre1">                DPLogR &lt;- rxGlm(UsedSpaceKB ~ Operation + NofRowsOperation + UnusedSpaceKB, data = DataPack_info, family = Gamma)</strong>
    <strong class="calibre1">                df_predict &lt;- data.frame(TableName=("'+@TableName+'"), Operation=('+@Operation+'), </strong>
    <strong class="calibre1">                          NofRowsOperation=('+@NofRowsOperation+'), UnusedSpaceKB=('+@UnusedSpaceKB+'))</strong>
    <strong class="calibre1">                predictions &lt;- rxPredict(modelObject = DPLogR, data = df_predict, outData = NULL,  predVarNames = "UsedSpaceKB", type = "response",checkFactorLevels=FALSE);</strong>
    <strong class="calibre1">                OutputDataSet &lt;- predictions'</strong>
    
    <strong class="calibre1">EXEC sys.sp_execute_external_script</strong>
    <strong class="calibre1">     @language = N'R'</strong>
    <strong class="calibre1">    ,@script = @R_code</strong>
    <strong class="calibre1">    ,@input_data_1 = @SQL_input</strong>
    <strong class="calibre1">    ,@input_data_1_name = N'DataPack_info'</strong>
    
    <strong class="calibre1">WITH RESULT SETS ((</strong>
    <strong class="calibre1">                                    UsedSpaceKB_predict INT</strong>
    <strong class="calibre1">                                   ));</strong>
    <strong class="calibre1">GO</strong>
  </pre>
<p class="calibre2">Now we need to run the procedure two times in a row:</p>
<pre class="calibre19"><strong class="calibre1">EXECUTE Predict_UsedSpace</strong>
<strong class="calibre1">                     @TableName = 'DataPack'</strong>
 <strong class="calibre1">                    ,@Operation = 1</strong>
  <strong class="calibre1">                   ,@NofRowsOperation = 120</strong>
  <strong class="calibre1">                   ,@UnusedSpaceKB = 2;</strong>
<strong class="calibre1">GO</strong>
    
<strong class="calibre1">EXECUTE Predict_UsedSpace</strong>
    <strong class="calibre1">                 @TableName = 'DataPack'</strong>
    <strong class="calibre1">                 ,@Operation = 1</strong>
    <strong class="calibre1">                 ,@NofRowsOperation = 500</strong>
    <strong class="calibre1">                 ,@UnusedSpaceKB = 12;</strong>
<strong class="calibre1">GO</strong>  </pre>
<p class="calibre2">Both predictions on used space disk are based on our demo data but can be used on a larger scale and for predictions as well. Of course, for even better predictions, some baseline statistics could also be included. With every model, we also need to test the predictions to see how good they are.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">Using SQL Server R for any kind of DBA task, as we have seen here, it is not always hardcore statistics or predictive analytics; we might also be some simple statistical understanding underlying the connection and relationships between the attribute's queries, gathered statistics, and indexes. Prognosing and predicting, for example, information from execution plans in order to prepare a better understanding of the query of cover missing index, is a crucial point. Parameter sniffing or a cardinality estimator would also be a great task to tackle along the usual statistics.</p>
<p class="calibre2">But we have seen that predicting events that are usually only monitored can be a huge advantage for a DBA and a very welcome feature for core systems.</p>
<p class="calibre2">With R integration into SQL Server, such daily, weekly, or monthly tasks can be automated to different, before not uses yet, extent. And as such, it can help give different insight to DBAs and also people responsible for system maintenance.</p>
<p class="calibre2">In next chapter, we will be covering extending features beyond R external procedure and how to use them.</p>


            </article>

            
        </section>
    </body></html>