- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anomaly Detection in MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the life cycle of a physical system, the occurrence of failures or
    malfunctions poses a potential threat to its normal functioning. To safeguard
    against critical interruptions, it becomes imperative to implement an anomaly
    detection system within the facility. Termed as a **fault diagnosis system**,
    this mechanism is designed to identify potential malfunctions within the monitored
    system. The pursuit of fault detection stands as a pivotal and defining phase
    in maintenance interventions, demanding a systematic and deterministic approach
    to comprehensively analyze all conceivable causes that might have led to the malfunction.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn the basic concepts of anomaly detection systems
    and how to implement an anomaly detection system in MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing anomaly detection and fault diagnosis systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **machine learning** (**ML**) to identify anomalous functioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a fault diagnosis system using MATLAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding advanced regularization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic ML concepts. To understand these topics,
    a basic knowledge of algebra and mathematical modeling is needed. A working knowledge
    of the MATLAB environment is also required.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with the MATLAB code in this chapter, you need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`GearboxAccData.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AnomalyDetectGearBox.m`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DroneFaultDiagnosis.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UAVFaultDiagnosis.m`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing anomaly detection and fault diagnosis systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anomaly detection and fault diagnosis systems are crucial components of various
    industries, particularly in areas where safety, reliability, and efficiency are
    of utmost importance, such as manufacturing, healthcare, finance, and cybersecurity.
    These systems aim to identify unusual or unexpected patterns, behaviors, or conditions
    in data, processes, or systems that may indicate the presence of faults, defects,
    or anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Delving into the realm of anomaly detection, this section provides a comprehensive
    overview, unraveling the key principles and methodologies employed in identifying
    deviations from the norm within diverse systems and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anomaly detection is a technique used in data analysis and ML to identify data
    points or patterns that deviate significantly from the expected or normal behavior
    within a dataset. Anomalies, also known as outliers, are data points that do not
    conform to most of the data and may indicate errors, fraud, unusual events, or
    other important information. Anomaly detection has various applications across
    different domains, such as cybersecurity, industrial **quality control** (**QC**),
    finance, healthcare, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start to get an overview of different types of anomalies to understand
    what is intended with this term, we will list some types of anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Point anomalies**: These are individual data points that are considered anomalies,
    such as a single fraudulent transaction in a credit card dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual anomalies**: These are anomalies that are context-dependent. A
    data point might not be an anomaly on its own but is unusual in a particular context
    or time, such as a sudden spike in web traffic during a holiday sale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collective anomalies**: These are anomalies that are identified by examining
    a group of data points collectively. These anomalies involve patterns or relationships
    between data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several methods for addressing anomaly detection problems, ranging
    from simple statistical techniques to complex ML algorithms. The choice of method
    depends on the nature of the data and the specific problem you are trying to solve.
    Here, we are listing the most used ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical methods**: Statistical techniques such as z-scores, percentiles,
    and boxplots can be used to identify anomalies based on deviations from the mean
    or median of the data distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML**: Supervised, unsupervised, and semi-supervised ML algorithms can be
    used for anomaly detection. Some popular methods include Isolation Forest, **One-Class
    Support Vector Machine** (**One-Class SVM**), **autoencoders** (**AEs**), and
    k-means clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series analysis**: Specialized techniques are used for detecting anomalies
    in time series data, such as **autoregressive** (**AR**) models, exponential smoothing,
    and **moving** **averages** (**MAs**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density estimation**: Methods such as **kernel density estimation** (**KDE**)
    and **Gaussian Mixture Models** (**GMMs**) are used to estimate the probability
    density function of the data and identify anomalies as low-density regions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning (DL)**: **Neural networks** (**NNs**), especially **deep AEs**
    (**DAEs**) and **recurrent NNs** (**RNNs**), are used for anomaly detection in
    high-dimensional data or sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Combining multiple anomaly detection models can improve
    overall performance and robustness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addressing anomaly detection problems, we have to face some challenges. For
    example, determining an appropriate threshold for defining anomalies can be challenging.
    Imbalanced datasets, where anomalies are rare, can make model training and evaluation
    tricky. Handling high-dimensional data and noisy datasets can also be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection is a valuable tool for identifying rare but potentially important
    events or patterns in large datasets. The choice of method depends on the specific
    domain, data characteristics, and the nature of anomalies that need to be detected.
  prefs: []
  type: TYPE_NORMAL
- en: Fault diagnosis systems explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Diagnostics** refers to a procedure that translates information obtained
    from measuring parameters and collecting data about a machine into insights about
    its current or potential failures. This process encompasses a combination of analytical
    and synthetic activities, utilizing physical measurements and machine-specific
    characteristics to derive valuable information regarding the machine’s condition
    and its future trends. This information is crucial for assessing both short-term
    and long-term reliability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The adoption of fault diagnosis techniques is becoming increasingly important
    to ensure high levels of safety and dependability in automated and autonomous
    systems. Indeed, over recent years, the global scientific community has invested
    significant efforts in developing systematic approaches for diagnosing failures
    in a wide array of systems. The primary objective of a fault diagnosis system
    is to continuously monitor a system during its operation to achieve three key
    goals: detecting the presence of faults (fault detection), pinpointing the specific
    location of these faults (fault isolation), and understanding how these faults
    evolve over time (fault identification).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, the output of a fault diagnosis system provides a group of sensitive
    variables that are influenced by the type of fault, with some modifications when
    the system experiences a failure. Subsequently, the system extracts and processes
    information embedded within these fault occurrences to carry out the tasks of
    detecting, isolating, and identifying faults. The techniques employed for fault
    diagnostics can be categorized into three fundamental groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model-based**: This approach relies on precise mathematical models, enabling
    efficient fault detection and diagnosis. These models are constructed to depict
    the actual degradation processes of the components under scrutiny. This involves
    describing, in terms of the laws of physics, how operational conditions influence
    the performance and lifespan of assets. Key variables encompass various thermal,
    mechanical, chemical, and electrical parameters. Expressing their impact on machinery’s
    health is a challenging endeavor, demanding a high degree of domain expertise
    and modeling skills from those creating such solutions. Once the model is established,
    it becomes essential to have sensors that can provide data regarding the relevant
    quantities identified during the analysis and modeling phase. This data serves
    as input for the model. The primary advantage of this approach lies in its descriptiveness.
    It allows for a detailed analysis of the causes behind each output it generates
    because it’s rooted in a physical representation of the process. This, in turn,
    enables validation and certification. However, its accuracy heavily relies on
    the quality of analysis and modeling conducted by domain experts. Conversely,
    drawbacks include its complexity and high implementation costs, along with its
    high specificity to the system, limiting possibilities for reuse and expansion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge-based**: This approach also leans on domain experts, aiming to
    model the expertise and behavior of these experts directly. The objective is to
    formalize the knowledge they possess, enabling its automated replication and application.
    Expert systems are essentially software programs that leverage knowledge repositories
    collected from proficient individuals in various fields. These systems then employ
    inference and reasoning mechanisms to emulate human thought processes, offering
    support and solutions for practical issues. Two prevalent techniques for implementing
    this type of model are rule-based mechanisms and fuzzy logic. Rule-based approaches
    are valued for their simplicity of implementation and interpretability. However,
    they may fall short in expressing complex conditions and may suffer from a combinatorial
    explosion when dealing with many rules. On the other hand, the use of fuzzy logic
    permits describing the system’s state through more imprecise and less rigid inputs,
    simplifying the formalization and model description process and making it more
    intuitive. As with model-based methods, the effectiveness of expert systems heavily
    hinges on the quality and level of detail achieved by the model, resulting in
    highly specific solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data-based methods**: The data-based approach applies statistical and ML
    techniques to the data gathered from machines, with the goal of discerning the
    condition of components. The aim is to acquire comprehensive real-time information
    about the machinery, typically through sensors and production and maintenance
    activity logs, and to correlate this information with the individual components’
    degradation levels or the system’s performance. Presently, this approach is most
    prevalent in practical applications. This is attributed to several advantages
    it offers over the other two methods. Data-driven methods require substantial
    data volumes for effectiveness. However, given the accessibility of modern interconnected
    sensors, this requirement is generally easy to fulfill. In contrast to the other
    approaches, they have the significant advantage of not mandating in-depth domain-specific
    expertise, thereby reducing experts’ impact on the model’s final performance.
    While expert insights can facilitate the selection of input variables, their influence
    is less pronounced compared to other methods. Additionally, ML and data mining
    techniques may uncover relationships between input parameters and system states
    that even experts might not know beforehand. ML-based algorithms can be harnessed
    to build predictive models that yield domain-specific knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embarking on the intersection of machinery and intelligence, this section explores
    a cutting-edge approach to fault diagnosis by harnessing the power of ML. Unraveling
    the potential of algorithms and models, we delve into how these technological
    advancements enable a more precise and efficient identification of faults within
    complex systems.
  prefs: []
  type: TYPE_NORMAL
- en: Approaching fault diagnosis using ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML algorithms autonomously derive knowledge from data through the inputs they
    receive, eliminating the need for explicit developer instructions. In such models,
    the machine independently identifies the patterns required to achieve the desired
    outcome, a hallmark of **artificial intelligence** (**AI**). During the learning
    process that characterizes these algorithms, the system is supplied with a training
    dataset, enabling it to estimate the relationships between input and output data.
    These relationships constitute the parameters of the model that the system infers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The selection of a particular ML model hinges heavily on the intended goal
    of the system. The nature of the problem is modeled differently based on the objective,
    and two primary approaches can be identified: diagnostics and prognostics. A diagnostic
    system is designed to pinpoint and identify faults when they occur. This involves
    continuous monitoring of a system, alerting when deviations from the expected
    behavior are detected, specifying which component is affected by the anomaly,
    and categorizing the type of anomaly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, prognostics seeks to predict whether a fault is imminent or to
    estimate the probability of its occurrence. As a proactive analysis, prognostics
    can contribute significantly to cost reduction in terms of interventions, but
    it represents a more intricate objective to accomplish. Another viable option
    is the simultaneous application of diagnostic and prognostic solutions to the
    same system. This combination offers two valuable advantages: diagnostics can
    provide support in cases where prognostics falls short. This situation is inevitable
    because some failures do not follow predictable patterns, and even when certain
    failures are predictable with a high degree of accuracy, they may not be identifiable
    in every instance. The information gleaned from diagnostic applications can serve
    as supplementary input for predictive systems, enabling the creation of more sophisticated
    and precise models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fault diagnosis problems can be addressed using the following two types of
    approaches depending on the type of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression**: Regression can serve as a modeling technique for prognostic
    challenges. This entails estimating the remaining useful life of a component in
    continuous time units. In this scenario, the training dataset should exclusively
    encompass data from components that have experienced failures, facilitating the
    labeling of inputs starting from the failure event backward in time. This approach
    to the problem still adheres to the **supervised learning** (**SL**) framework,
    where input data is linked to continuous output values in this context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binary classification**: The simplest approach is to frame fault detection
    as a binary classification problem, where each input representing the system’s
    state must be categorized into one of two exclusive values. In the context of
    diagnostic problem-solving, this entails determining whether the machine is operating
    correctly or incorrectly, categorizing all possible states into these two classes.
    This process aligns with SL, as the input data is paired with labels that represent
    the model’s output. In such cases, the system learns to discern relationships
    between the input data and the corresponding labels. In the case of prognostics,
    the task shifts toward assessing whether the machine could potentially fail within
    a specified time frame. The distinction between these two meanings arises solely
    from the differing interpretations of the labels. This implies that the same model
    can be employed to address both issues. The differentiation lies in how the dataset
    is labeled during the model training phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiclass classification**: The multiclass variant represents an extension
    of binary classification, introducing a broader range of available labels for
    selection. Nevertheless, each input is still associated with only one label. In
    the diagnostic scenario, the extension follows a straightforward approach, involving
    the determination of whether the machine is operating correctly or not. In the
    latter case, it’s about pinpointing the specific anomaly state from various possibilities.
    When considering prognostics applications, the problem shifts toward determining
    the time interval before the machine’s potential failure. Here, the labels denote
    different intervals representing proximity to a failure event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a precise diagnosis of faults in intricate machinery, it is imperative to
    gather information by collecting data, employing advanced signal processing algorithms
    for data analysis, and subsequently extracting the appropriate features to efficiently
    identify and classify faults. Data can be acquired through various methods, primarily
    via measurements of physical attributes that describe the machine’s state during
    operation. These measurements are obtained by specialized sensors that convert
    physical properties into electrical values, often referred to as sensor data.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that the choice of parameters depends greatly on the specific
    system under scrutiny, with examples including noise, vibrations, pressure, temperature,
    and humidity. Typically, in modern automated industrial systems, the necessary
    diagnostic data is readily available. In cases where it’s not, adding extra sensors
    becomes the initial step in establishing an effective fault identification strategy.
    Additionally, data can also be gathered by linking the machine’s static operating
    conditions to specific time points, incorporating factors such as material codes,
    machine production speed, or the type of product being manufactured. This type
    of data is categorized as statistical data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, data may encompass the history of significant events and actions related
    to a machine and its components, which is often referred to as log data. This
    can include records of faults, repairs, replacements, and other pertinent activities.
  prefs: []
  type: TYPE_NORMAL
- en: With the foundational concepts of fault diagnosis explored, it’s now time to
    practically address a simulation problem.
  prefs: []
  type: TYPE_NORMAL
- en: Using ML to identify anomalous functioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A gearbox, also known as a gear mechanism or transmission, is a mechanical device
    designed to transmit mechanical power from one component to another while altering
    the speed, torque, and direction of rotation. Gearboxes consist of a set of gears
    with different sizes and configurations that mesh to provide specific mechanical
    advantages based on the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: In an automotive context, a gearbox, often referred to as a transmission or
    gearshift, is a critical component that plays a central role in controlling the
    power output of the engine and the vehicle’s speed. Anomaly detection in a gearbox
    involves identifying irregularities or deviations from the normal behavior of
    the gearbox components and their associated systems. Detecting anomalies in a
    car’s gearbox is crucial for ensuring the vehicle’s safety, reliability, and overall
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: To promptly identify anomalies in a gearbox, we can use sensors such as those
    measuring temperature, pressure, speed, and fluid levels to continuously collect
    data. These sensors provide valuable information about the gearbox’s condition.
    Raw sensor data often contains noise and outliers. Data preprocessing techniques,
    such as filtering and data cleaning, are applied to ensure the quality and reliability
    of the data. Relevant features are extracted from the preprocessed data. These
    features may include gear engagement patterns, rotational speeds, and temperature
    fluctuations. Anomaly detection typically begins by establishing a baseline model
    of the normal behavior of the gearbox. This is done by analyzing historical data
    to understand typical operating conditions and performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Various anomaly detection techniques are applied to identify deviations from
    the established normal behavior. Some common techniques include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical methods**: These methods flag data points that significantly
    deviate from the mean or follow a different distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML algorithms**: These models can learn to recognize patterns in the data
    and detect anomalies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series analysis**: For gearbox data collected over time, time series
    methods such as **Autoregressive Integrated Moving Average** (**ARIMA**) or **long
    short-term memory** (**LSTM**) NNs can be used to detect anomalies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomalies are detected based on predefined thresholds or through statistical
    analysis of the data. These thresholds are determined during the model training
    phase and may be adjusted over time to optimize detection. When an anomaly is
    detected, the system can generate alerts for the vehicle operator or service technician.
    These alerts may include information about the specific anomaly, its potential
    impact, and recommended actions for maintenance or repair.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection is an ongoing process. The gearbox’s condition is continuously
    monitored, and data is analyzed in real time to ensure prompt detection of any
    emerging issues. Anomaly detection in car gearboxes is essential for predictive
    maintenance, preventing unexpected failures, and optimizing the lifespan and performance
    of the vehicle. It is a critical component of modern automotive diagnostics and
    maintenance systems, especially in commercial and industrial settings where vehicle
    reliability is of utmost importance.
  prefs: []
  type: TYPE_NORMAL
- en: Navigating the landscape of anomaly detection, this section focuses on the application
    of logistic regression as a powerful tool. By delving into the intricacies of
    this statistical model, we uncover its efficacy in identifying anomalies within
    datasets, shedding light on its nuances with a practical implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection using logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this instance, we will employ data captured by accelerometers that recorded
    vibrations from a gearbox under two distinct operating conditions: one representing
    a healthy state and the other indicating a broken state. To ensure comprehensive
    coverage, the sensors were positioned in opposing directions to capture any operational
    variances. These datasets will serve as the foundation for training various algorithms
    aimed at classifying the gearbox’s operational conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go in deep to describe the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the dataset into the MATLAB environment. The dataset
    provides measurements of vibrations recorded by two sensors, along with the associated
    classification of engine operation: `0` denotes a broken state, and `1` indicates
    a healthy state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A table with `20000` rows and `3` columns was imported.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To perform a first visual examination of the data, let’s utilize graphs for
    assistance. For instance, we can create boxplots illustrating the data distribution
    as detected by the two sensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The chart seen in *Figure 11**.1* will be drawn:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Boxplot of the data](img/B21156_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Boxplot of the data
  prefs: []
  type: TYPE_NORMAL
- en: A boxplot serves as a visual representation employed to characterize the distribution
    of a sample through basic dispersion and positional measures. Upon scrutinizing
    *Figure 11**.1*, it becomes evident that the data distributions from the two sensors
    vary. Sensor `Vib1` appears to exhibit a greater divergence in vibration values
    between malfunctioning and normal conditions. This implies that faults are likely
    manifesting at the location of this sensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'After visually analyzing the data at our disposal, we need to split the data
    so that we can use it to train an anomaly detection model. Data splitting is a
    fundamental step in the process of building and evaluating ML models. It involves
    dividing your dataset into multiple subsets, typically including a training set,
    a validation set, and a test set. Each of these subsets serves a specific purpose
    in the ML workflow. We will divide the data into two subsets: a training set and
    a test set. The training set is the largest portion of your data and is used to
    train the ML model. It’s the dataset the model learns from and uses to update
    its parameters. The test set is used to evaluate the final, trained model’s performance.
    It provides an unbiased estimate of how well the model will perform on unseen
    data. The test set should not be used during model development or training to
    avoid data leakage. Let’s see how to make a data split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We first extracted the number of observations present in our dataset using
    the `length()` function: this function provides the size of the most extensive
    dimension in the *X* array. When dealing with vectors, this size corresponds to
    the total number of elements. After that, we used the `cvpartition()` function
    to establish a random partition for a dataset. We can utilize this partition to
    create training and test subsets, which are crucial for validating a statistical
    model. We used the two object functions, `training` and `test`, to extract the
    training data index and the test data index from the initial dataset. After that,
    we applied these indices to extract the data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can build the logistic regression model. Logistic regression is a statistical
    method used for binary classification, but it can be extended for multiclass classification
    too. It’s a type of regression analysis that models the probability of a binary
    outcome, often denoted as `0` or `1`. Logistic regression is widely used in various
    fields, including medicine, social sciences, economics, and ML. Logistic regression
    is used to predict the probability of an observation belonging to one of two classes
    or categories. It’s not suitable for problems with more than two classes; for
    such cases, multinomial logistic regression or other techniques are used. The
    logistic regression model uses a `sigmoid` *(logistic)* function to map input
    features to a probability between `0` and `1`. Logistic regression is a powerful
    and interpretable method for binary classification tasks. It serves as a foundational
    technique in ML and statistics and is especially useful when you need to understand
    the relationship between input features and the probability of a binary outcome.
    Let’s go to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We used the `fitglm()` function for creating a logistic regression model: this
    function provides a generalized linear model fitted to the variables in the table
    or dataset array. By default, it designates the last variable as the response
    variable. We set a binomial distribution of the response variable and `logit`
    as a link function. The link function *f(μ) = log(μ/(1–μ))* is associated with
    logistic regression. It transforms the linear combination of predictors into a
    probability range (`0` to `1`). In this formula, *\(\mu\)* represents the probability
    of the dependent variable being `1`. The logarithmic transformation maps the odds
    of an event occurring in a linear space, facilitating modeling in the context
    of binary outcomes. As the logistic function’s range is (`0`, `1`), the log-odds
    ratio spans the entire real number line. This link function is crucial in logistic
    regression, aiding in the estimation of probabilities and prediction of binary
    outcomes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The distribution refers to the probability distribution that describes the variability
    of the response variable (dependent variable) in the model. In other words, it
    specifies the form of the likelihood function that the model assumes for the response
    variable. The choice of distribution is based on the nature of the response variable
    and the assumptions of the model. The link function defines the relationship between
    the linear predictor and the expected value of the response variable. We model
    the linear predictor as a linear combination of the predictors’ coefficients.
    The link function transforms this linear predictor into the expected value of
    the response variable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following model was returned from the training process:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After training the model, we can use it to identify anomalies, starting from
    data that the algorithm has never seen before. To do this, we will use the test
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We used the `predict()` function; this function provides the forecasted response
    values generated by the generalized linear regression model. After that, we have
    to also obtain binary predictions based on a threshold:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this way, all the `>=` values of the threshold are transformed into 1 and
    all the `>` values of the threshold are transformed into 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We now can calculate the accuracy of the model. This term refers to the degree
    to which a predictive model’s predictions align with the actual or observed values.
    It is a measure of how well the model performs in making correct predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Certainly not a high value of accuracy, which tells us that half of the predictions
    are wrong. A non-linear model to handle non-linearity in the data is needed. Logistic
    regression being a linear model fails to capture the non-linear relationship between
    dependent and independent variables. Surely, we can increase the accuracy by adopting
    another algorithm, as we will see in the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s move on to improving accuracy using the Random Forest algorithm next.
  prefs: []
  type: TYPE_NORMAL
- en: Improving accuracy using the Random Forest algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random Forest is a powerful ensemble learning algorithm used for both classification
    and regression tasks. It is an extension of the **Bootstrap Aggregating** (**Bagging**)
    algorithm that builds multiple decision trees during training and combines their
    predictions to improve accuracy and reduce overfitting. Random Forest begins by
    creating multiple random samples (with replacements) from the original dataset.
    Each of these samples is called a **bootstrap sample**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each bootstrap sample, a decision tree is constructed. These trees are
    often referred to as **base learners** or **weak learners**. Decision trees are
    grown with the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random feature selection**: At each node of the tree, a random subset of
    features is considered for splitting, reducing the correlation between trees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited depth**: Trees are often grown to a certain depth to prevent overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No pruning**: The Random Forest algorithm typically does not prune trees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For classification tasks, each tree in the Random Forest algorithmvotes for
    the class it predicts. The class that receives the most votes is the final prediction.
    For regression tasks, each tree in the Random Forest predicts a numeric value.
    The final prediction is the average of these individual tree predictions. The
    majority vote (for classification) or averaging (for regression) results in the
    ensemble decision.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest includes a mechanism to estimate the model’s accuracy without
    the need for a separate test set. This is known as the **out-of-bag** (**OOB**)
    error. For each data point, it estimates the error rate using only the trees that
    did not use this point during training. Random Forest can provide an estimate
    of the importance of each feature in the dataset. This is done by measuring the
    decrease in prediction accuracy when the values of a particular feature are randomly
    shuffled.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list provides key characteristics and advantages of using Random
    Forest:'
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest is known for its strong generalization performance and resistance
    to overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can handle both categorical and numerical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The random feature selection and ensemble nature make it robust to noisy data
    and outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest can be used for feature selection, as it provides a feature importance
    ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It works well “out of the box” without much tuning of hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used for both classification and regression tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, Random Forest is a popular and effective ML algorithm used in a wide
    range of applications, from image classification and text analysis to financial
    modeling and healthcare.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement an automated system to detect anomalies in gearbox data, we will
    use the same data already used in the *Anomaly detection using logistic regression*
    section. Refer to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Random Forest involves bootstrapping, where we create multiple random samples
    (with replacement) from the dataset. We can do this to create different subsets
    of your data. We start by defining some parameters as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We defined the number of trees in the forest (`numTrees`), the number of observations
    that we will use in the training (`numSamples`), and the sample size (`sampleSize`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will pass `forest` to initialize the forest, setting the data as a `cell`
    structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will use a `for` loop to repeatedly execute a block of code a specific
    number of times or iterate over a sequence. This will be necessary to create a
    random sample with a replacement first and to then train a decision tree on the
    sample:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `fitctree()` function trains a binary decision tree for multiclass classification.
    It returns a trained binary classification decision tree using the input variables
    found in the table and the output responses. The binary tree forms a split at
    branching nodes using values from a column in the table.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The process involves randomly selecting samples to simulate bootstrap sampling,
    yet the Random Forest classifier typically chooses `\( \sqrt{p} \)` features for
    each decision tree. In the context of two features, every decision tree would
    ideally operate with a single feature when introduced into the `for` loop. Consequently,
    substantial performance enhancement isn’t anticipated in this scenario.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can use the model to test the model performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We calculated the number of observations in the test data and we initialized
    the prediction using a matrix of zeros.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can now make the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once again, we used the `predict()` function to make predictions on the dataset
    never seen before by the algorithm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we will calculate the performance of the model using the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We used the `mode()` function, which evaluates the most frequent values in an
    array. This function provides the mode of elements along a specified dimension.
    In our case, this function returns results in a column vector that holds the most
    common value for each row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following accuracy is returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We obtained the improvement requested.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next section, we will see how to implement a fault diagnosis system using
    the acoustic emission data of an **unmanned aerial** **vehicle** (**UAV**).
  prefs: []
  type: TYPE_NORMAL
- en: Building a fault diagnosis system using MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technological advancements have given rise to highly capable aircraft that can
    autonomously manage flight operations. These aircraft belong to the category known
    as UAVs, meaning they can fly without human pilots. UAVs offer numerous advantages,
    including significantly reduced operating costs compared to traditional piloted
    aircraft, the ability to operate in environments unsuitable for human presence,
    and the capacity for timely aerial surveillance, such as during natural disasters.
  prefs: []
  type: TYPE_NORMAL
- en: Initially employed primarily for military purposes, UAVs were used in dull missions
    involving monotonous and lengthy surveillance and reconnaissance, as well as in
    dirty missions that posed risks to human pilots’ safety. They also played a crucial
    role in dangerous missions where human lives were at risk. However, today, they
    are considered the future of modern aeronautics. The vast potential of UAV technology,
    its successes in military operations, and advancements in micro- and nanotechnologies
    have spurred both industries and universities to develop increasingly modern and
    reliable UAV systems. These systems can be utilized across a wide spectrum of
    civilian and military missions.
  prefs: []
  type: TYPE_NORMAL
- en: The widespread proliferation of UAVs worldwide has brought to light uncharted
    issues. While the convenience of using these devices is evident, they also introduce
    a host of challenges and potential hazards associated with this technology. Among
    concerns that have arisen is the safety of UAV flights—what would be the consequences
    if a UAV were to crash to the ground? An automatic system for identifying a defect
    on the propeller of a UAV, therefore, represents a tool that can guarantee us
    an acceptable level of safety.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will develop an algorithm for fault diagnosis of a UAV
    propeller from acoustic emission measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start importing the dataset into the MATLAB workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are 847 observations, 32 features, 31 predictors, and 1 binary response.
    We can explore the data using a boxplot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plot will display:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Boxplot of the predictors](img/B21156_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Boxplot of the predictors
  prefs: []
  type: TYPE_NORMAL
- en: We can see that more variability in the data is shown for the low frequencies
    on the left of the graph (*Figure 11**.2*). This tells us that it is the low frequencies
    that will allow us to identify damage to the UAV’s propeller. Low frequencies
    are valuable for identifying damage to a UAV’s propeller due to their unique acoustic
    signatures. When a propeller is damaged, it often produces distinctive low-frequency
    vibrations or sounds that are not easily discernible in higher frequency ranges.
    These low-frequency signals can be indicative of structural issues, such as cracks,
    imbalances, or deformities, which might not be as prominent in higher frequency
    ranges. By focusing on low frequencies, you enhance sensitivity to subtle but
    crucial vibrations associated with propeller damage, providing a more effective
    means of early detection and preventive maintenance for UAVs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to partition data for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We first extracted the number of observations of the data using the `height()`
    function, which returns the number of table rows. Then, we used `randperm()`;
    this function generates a row vector that represents a random permutation of integers
    from 1 to *n*, ensuring that no elements are repeated. After that, we decided
    to divide the data into 70% for the training set and the remaining 30% for the
    test set. Finally, we used the indices obtained to split the dataset into two
    subsets to train and test the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To detect faults in the UAV propeller, we will use SVMs for *binary classification*.
    SVMs represent a category of **supervised ML** (**SML**) models applied in tasks
    such as classification and regression analysis. They are powerful and versatile
    algorithms that excel in various applications, including image classification,
    text classification, and outlier detection. SVMs are primarily designed for binary
    classification problems, where the goal is to separate data points into two distinct
    classes. The classifier finds a hyperplane that best separates the data, maximizing
    the margin between the classes. The margin is the distance between the hyperplane
    and the nearest data points from each class. SVMs aim to maximize this margin
    because a larger margin implies better generalization of unseen data and improved
    robustness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Support Vector Classification** (**SVC**) and **Support Vector Regression**
    (**SVR**) are both types of SVMs. SVC is employed for classification tasks, aiming
    to separate data into distinct categories, while SVR is used for regression, predicting
    continuous outcomes. In both cases, the SVMs work by finding the optimal hyperplane
    that maximizes the margin between different classes or fits the data points for
    regression. They are effective in high-dimensional spaces and versatile in handling
    complex relationships in data, making them valuable tools in various ML applications.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Support vectors are data points closest to the decision boundary (hyperplane).
    These points are critical in defining the margin and the position of the hyperplane.
    SVMs derive their name from these support vectors. In a two-dimensional space,
    the hyperplane is a straight line that separates the data into two classes. In
    higher dimensions, it’s a hyperplane. The goal is to find the hyperplane that
    maximizes the margin while correctly classifying as many data points as possible.
    SVMs can be applied to non-linear problems by transforming the input data into
    a higher-dimensional space, where a linear separation is possible. Various kernel
    functions, such as the polynomial kernel, **radial basis function** (**RBF**)
    kernel, and sigmoid kernel, are used to achieve this. SVMs are known for their
    ability to provide robust models with strong generalization capabilities, particularly
    when dealing with high-dimensional data. SVMs are widely used in ML and have shown
    effectiveness in a variety of applications. While they excel in many scenarios,
    selecting the appropriate kernel and tuning hyperparameters can be critical for
    their success. Let's apply SVM model to identify a fault in UAV blades.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `fitcsvm()` function trains an SVM model for classification, which can be
    used for one-class or two-class (binary) problems, on a predictor dataset with
    low or moderate dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following parameters are passed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`TrainData{:,1:31}`: The training data for the model is represented as a table,
    where each row represents an observation, and each column represents a predictor
    variable. Multicolumn variables and non-character cell arrays are not supported.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TrainData{:,32}`: The name of the response variable should be provided as
    a character vector or string scalar.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KernelFunction`, `linear`: The choice of kernel function used to calculate
    the elements of the `Gram` matrix is specified as a pair using `KernelFunction`
    followed by the name of the kernel function, provided as a comma-separated pair.
    The following kernel functions are available: `gaussian, linear,` `and polynomial`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BoxConstraint`, `1`: This parameter controls the regularization strength.
    It determines how much error the SVM model is willing to tolerate to achieve a
    wider margin between the classes. A larger `BoxConstraint` value allows more tolerance
    for misclassifications, while a smaller value enforces a stricter fit to the training
    data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can use the SVM model to perform a fault diagnosis on the acoustic
    emission of a UAV. We will use the test dataset, which contains data that has
    not yet been seen by the algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, we used the `predict()` function to detect a fault in the UAV propeller,
    and then we calculated the accuracy of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following result is returned:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 91% accuracy is a great result. This is evidence that an SVM is a good choice
    to predict a possible fault in the UAV propeller.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next part of this chapter, we will introduce some methods to prevent
    overfitting and improve the performance of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding advanced regularization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced regularization techniques are methods used in ML and statistical modeling
    to prevent overfitting and improve the generalization performance of models. Overfitting
    occurs when a model fits the training data too closely, capturing noise and irrelevant
    patterns, which leads to poor performance on unseen data. Regularization techniques
    introduce constraints or penalties to the model’s parameters during training to
    encourage simpler, more generalized models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dropout is a regularization technique used in NNs, particularly **deep NNs**
    (**DNNs**), to prevent overfitting. Overfitting occurs when an NN learns to fit
    the training data too closely, capturing noise and memorizing specific examples
    rather than generalizing from the data. Dropout is a simple yet effective method
    for improving a model’s generalization performance.
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase, at each forward and backward pass, dropout randomly
    “drops out” (deactivates) a portion of the neurons (units) in the NN. This means
    that during the forward pass, the output of some neurons is set to zero with a
    certain probability, effectively excluding them from the computation. The dropout
    probability is a hyperparameter and determines the probability that a neuron will
    be dropped out. A common value is around 0.5, which means that each neuron has
    a 50% chance of being dropped out during training.
  prefs: []
  type: TYPE_NORMAL
- en: The dropout process introduces variability during training because, for each
    forward and backward pass, a different set of neurons is active, and the network
    effectively trains on different subnetworks. This variability forces the network
    to learn more robust features and prevents it from relying too heavily on any
    neuron or feature. During the inference or testing phase (when you use the trained
    model for predictions), dropout is typically turned off. However, the output of
    each neuron is scaled by a factor of *1/(1 - p)*, where *p* is the dropout probability
    used during training. This scaling helps maintain the expected values of neuron
    activations, ensuring that the model’s behavior is consistent with what it learned
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key benefits of dropout are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization**: Dropout acts as a regularization technique by preventing
    the NN from overfitting the training data. Training on various subnetworks encourages
    the network to generalize better to unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing co-adaptation**: Dropout discourages neurons from relying too heavily
    on their neighboring neurons. This helps prevent the co-adaptation of neurons,
    where certain neurons become specialized to predict the output of other neurons.
    As a result, dropout can lead to more independent and informative features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensembling effect**: The dropout process is akin to training multiple models
    with different architectures, which is similar to an ensemble of models. This
    ensembling effect can lead to improved model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, dropout is widely used in DL and is especially effective when dealing
    with large and complex NNs. It’s a simple yet powerful technique for improving
    model generalization and reducing overfitting, making it an essential tool for
    training robust DL models.
  prefs: []
  type: TYPE_NORMAL
- en: Venturing into the intricacies of model regularization, the next section delves
    into the distinct realms of L1 and L2 regularization. Unraveling the nuances of
    these techniques, we explore how they contribute to enhancing the robustness and
    generalization capabilities of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring L1 and L2 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: L1 and L2 regularization are two common techniques used in ML and statistical
    modeling to prevent overfitting by adding penalty terms to the loss function.
    They encourage the model to have smaller weights and, in the case of L1, can lead
    to feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s introduce two typical methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**L1 regularization (Lasso)**: L1 regularization, also known as Lasso regularization,
    introduces a penalty term to the loss function, proportionate to the absolute
    values of the model’s coefficients (weights). Mathematically, it can be represented
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss with L1 regularization = Original Loss + λ * Σ|w _ i|
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, *λ* (lambda) is a hyperparameter that controls the strength of regularization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*L1* regularization has the effect of shrinking some of the model’s weights
    to exactly zero. As a result, it performs feature selection by effectively removing
    less important features from the model. *L1* regularization is especially useful
    when you suspect that only a subset of your features is relevant for making predictions.
    It helps you create simpler and more interpretable models by eliminating irrelevant
    features.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**L2 regularization (Ridge)**: L2 regularization, also known as Ridge regularization,
    adds a penalty term to the loss function that is proportional to the square of
    the model’s coefficients (weights). Mathematically, it can be represented as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss with L2 regularization = Original Loss + λ * Σ(w _ i ^ 2)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Again, *λ* is the hyperparameter controlling the strength of regularization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*L2* regularization encourages all the model’s weights to be small but rarely
    forces them to be exactly zero. It prevents any one feature from having an overwhelmingly
    large weight and has the effect of making the model more stable and numerically
    well conditioned. *L2* regularization is often used when you want to prevent multicollinearity
    (high correlation between features) and reduce the impact of outliers. It’s suitable
    when you believe that all features are relevant but should not be overly influential.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the following list, we summarize the key differences between *L1* and *L2*
    regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: '*L1* encourages sparsity by setting some weights to exactly zero, leading to
    feature selection, while *L2* mainly shrinks weights but rarely eliminates them
    entirely'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L1* is suitable for situations where you suspect that only a subset of features
    is relevant, whereas *L2* is more appropriate when you want to prevent multicollinearity
    and ensure all features contribute to the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A combination of both *L1* and *L2* regularization is known as elastic net,
    which balances **feature selection** (**L1**) and **feature** **grouping** (**L2**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice between *L1* and *L2* regularization depends on the specific problem,
    the nature of the data, and your understanding of the underlying relationships
    between features. In some cases, a combination of both *L1* and *L2* regularization
    may provide the best results by offering a balance between sparsity and stability.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing early stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Early stopping is a regularization technique commonly used in ML, particularly
    in training DNNs, to prevent overfitting and optimize model performance. It involves
    monitoring the model’s performance on a validation dataset during the training
    process and stopping the training when a certain criterion is met.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the training process, your dataset is typically divided into two parts:
    the training dataset and the validation dataset. The training dataset is used
    to update the model’s parameters, while the validation dataset is used to monitor
    the model’s performance. To implement early stopping, you need to define a performance
    criterion. Common choices for the criterion include validation loss or validation
    accuracy, depending on the specific ML task (for example, regression or classification).
    You aim to minimize the validation loss or maximize the validation accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: As the model trains, you periodically evaluate its performance on the validation
    dataset by calculating the validation loss or accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: You typically monitor the validation performance at regular intervals or after
    a fixed number of training epochs. You set a threshold, or a rule based on the
    validation performance, to decide when to stop the training. This could involve
    tracking whether the validation loss is decreasing or if the validation accuracy
    has reached a plateau. The most common condition is to stop training when validation
    performance starts to degrade or stagnate. In other words, you look for the point
    where the validation loss begins to increase or the validation accuracy starts
    to decrease. When the early stopping condition is met, training is halted. The
    model’s parameters at the point when training stops are typically considered the
    final model.
  prefs: []
  type: TYPE_NORMAL
- en: 'What are the benefits of using early stopping? There are a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prevents overfitting**: Early stopping prevents the model from overfitting
    the training data because it terminates the training process when the model starts
    to fit the noise in the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saves training time**: It can significantly reduce training time because
    the model doesn’t continue to train unnecessarily once optimal or near-optimal
    performance is achieved'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enables better generalization**: The model’s performance is more likely to
    generalize well to new, unseen data since it’s not overfitting the training data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, it’s essential to be cautious when using early stopping: choosing
    the right hyperparameters, such as the early stopping threshold or the frequency
    of validation checks, is critical. The wrong choices can lead to premature stopping
    or excessive training. It’s also important to use a separate validation dataset
    that the model has not seen during training. Data leakage can occur if the same
    data is used both for training and validation.'
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping is a valuable tool for training ML models, but it should be used
    with careful consideration of the problem and the specifics of the data. It helps
    to achieve better model generalization and reduces the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to implement an automatic fault diagnosis system
    in MATLAB. We started by introducing the essential concepts of anomaly detection
    and fault diagnosis. Then, we saw how to implement a system for identifying anomalous
    operations in MATLAB. We used vibrational data from a gearbox to train a model
    based on logistic regression. Subsequently, we used the same data, but this time
    using a model based on Random Forest to improve the performance of the predictive
    model.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we implemented a model for identifying a fault in UAV propellers
    based on acoustic emission. We used a classification model based on an SVM.
  prefs: []
  type: TYPE_NORMAL
- en: In the final section, we introduced the most popular methods for regularizing
    algorithms to improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, this book serves as a comprehensive guide and invaluable resource
    for both beginners and seasoned practitioners navigating the dynamic landscape
    of machine learning. The book not only equips readers with a solid foundation
    in key concepts and methodologies but also empowers them with practical skills
    to implement and deploy machine learning models using MATLAB. As we embark on
    the ever-evolving journey of technological advancement, this book stands as a
    beacon, illuminating the path to understanding and mastering the intricate realm
    of machine learning. Whether you are a student, researcher, or industry professional,
    the knowledge gained from these pages will undoubtedly propel you towards harnessing
    the full potential of machine learning in your endeavors. May this book be a catalyst
    for innovation and discovery, inspiring readers to contribute to the exciting
    and transformative field of machine learning.
  prefs: []
  type: TYPE_NORMAL
