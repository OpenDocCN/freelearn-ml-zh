<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Bayesian Models for Unsupervised Learning"><div class="titlepage" id="aid-1KEEU2"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Bayesian Models for Unsupervised Learning</h1></div></div></div><p>The machine learning models that we have discussed so far in the previous two chapters share one common characteristic: they require training data containing ground truth. This implies a dataset containing true values of the predicate or dependent variable that is often manually labeled. Such machine learning where the algorithm is trained using labeled data is called <a id="id315" class="indexterm"/>
<span class="strong"><strong>supervised learning</strong></span>. This type of machine learning gives <a id="id316" class="indexterm"/>a very good performance in terms of accuracy of prediction. It is, in fact, the de facto method used in most industrial systems using machine learning. However, the drawback of this method is that, when one wants to train a model with large datasets, it would be difficult to get the labeled data. This is particularly relevant in the era of Big Data as a lot of data is available for organizations from various logs, transactions, and interactions with consumers; organizations want to gain insight from this data and make predictions about their consumers' interests.</p><p>In unsupervised methods, no labeled data is required for learning. The process of learning happens through identifying dominant patterns and correlations present in the dataset. Some common examples of unsupervised learning are clustering, association rule mining, density estimation, and dimensional reduction. In clustering, naturally occurring groups in data are identified using a suitable algorithm that makes use of some distance measure between data points. In association rule mining, items that frequently occur together in a transaction are identified from a transaction dataset. In dimensional reduction techniques such as principal component analysis, the original dataset containing a large number of variables (dimensions) is projected down to a lower dimensional space where the maximum information in the data is present. Though unsupervised learning doesn't require labeled training data, one would need a large amount of data to learn all the patterns of interest and often the learning is more computationally intensive.</p><p>In many practical cases, it would be feasible to create a small amount of labeled data. The third type of learning, semi-supervised learning, is a method that makes use of this small labeled dataset and propagates labels to the rest of the unlabeled training data using suitable algorithms. In this chapter, we will cover Bayesian approaches for unsupervised learnings. We will discuss in detail two important models: Gaussian mixture models for clustering and Latent Dirichlet allocation for topic modeling.</p><div class="section" title="Bayesian mixture models"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec49"/>Bayesian mixture models</h1></div></div></div><p>In general, a <a id="id317" class="indexterm"/>mixture model corresponds to representing data using a mixture of probability distributions. The most common mixture model is of the following type:</p><div class="mediaobject"><img src="../Images/image00486.jpeg" alt="Bayesian mixture models"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00487.jpeg" alt="Bayesian mixture models"/></span> is a probability distribution of <span class="emphasis"><em>X</em></span> with parameters <span class="inlinemediaobject"><img src="../Images/image00488.jpeg" alt="Bayesian mixture models"/></span>, and <span class="inlinemediaobject"><img src="../Images/image00489.jpeg" alt="Bayesian mixture models"/></span> represents the weight for the <span class="emphasis"><em>k</em></span><sup><span class="emphasis"><em>th</em></span></sup> component in the mixture, such that <span class="inlinemediaobject"><img src="../Images/image00490.jpeg" alt="Bayesian mixture models"/></span>. If the underlying probability distribution is a normal (Gaussian) distribution, then the mixture model is called a <a id="id318" class="indexterm"/>
<span class="strong"><strong>Gaussian mixture model</strong></span> (<span class="strong"><strong>GMM</strong></span>). The mathematical representation of GMM, therefore, is given by:</p><div class="mediaobject"><img src="../Images/image00491.jpeg" alt="Bayesian mixture models"/></div><p style="clear:both; height: 1em;"> </p><p>Here, we have used the same notation, as in previous chapters, where <span class="emphasis"><em>X</em></span> stands for an <span class="emphasis"><em>N</em></span>-dimensional data vector <span class="inlinemediaobject"><img src="../Images/image00492.jpeg" alt="Bayesian mixture models"/></span> representing each observation and there are <span class="emphasis"><em>M</em></span> such observations in the dataset.</p><p>A mixture model such as <a id="id319" class="indexterm"/>this is suitable for clustering when the clusters have overlaps. One of the applications of GMM is in computer vision. If one wants to track moving objects in a video, it is useful to subtract the background image. This is called background subtraction or foreground detection. GMMs are used for this purpose where the intensity of each pixel is modeled using a mixture of Gaussian distributions (reference 1 in the <span class="emphasis"><em>References</em></span> section of this chapter).</p><p>The task of learning GMMs corresponds to learning the model parameters <span class="inlinemediaobject"><img src="../Images/image00493.jpeg" alt="Bayesian mixture models"/></span> and mixture weights <span class="inlinemediaobject"><img src="../Images/image00489.jpeg" alt="Bayesian mixture models"/></span> for all the components <span class="inlinemediaobject"><img src="../Images/image00494.jpeg" alt="Bayesian mixture models"/></span>. The standard approach for learning GMMs is by using the <a id="id320" class="indexterm"/>
<span class="strong"><strong>maximum likelihood</strong></span> method. For a dataset consisting of <span class="emphasis"><em>M</em></span> observations, the logarithm of the likelihood function is given by:</p><div class="mediaobject"><img src="../Images/image00495.jpeg" alt="Bayesian mixture models"/></div><p style="clear:both; height: 1em;"> </p><p>Unlike a single Gaussian model, maximizing the log-likelihood with respect to parameters <span class="inlinemediaobject"><img src="../Images/image00496.jpeg" alt="Bayesian mixture models"/></span> cannot be done in a straightforward manner in GMM. This is because there is no closed-form expression for the derivative in this case, since it is difficult to compute the logarithm of a sum. Therefore, one uses what is called an <a id="id321" class="indexterm"/>
<span class="strong"><strong>expectation-maximization</strong></span> (<span class="strong"><strong>EM</strong></span>) algorithm to maximize the log-likelihood function. The EM algorithm is an iterative algorithm, where each iteration consists of two computations: expectation and maximization. The EM algorithm proceeds as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize parameters <span class="inlinemediaobject"><img src="../Images/image00497.jpeg" alt="Bayesian mixture models"/></span>, <span class="inlinemediaobject"><img src="../Images/image00498.jpeg" alt="Bayesian mixture models"/></span>, and <span class="inlinemediaobject"><img src="../Images/image00489.jpeg" alt="Bayesian mixture models"/></span> and evaluate the initial value of log-likelihood.
</li><li class="listitem">
In the expectation step, evaluate mixture components <span class="inlinemediaobject"><img src="../Images/image00489.jpeg" alt="Bayesian mixture models"/></span> from log-likelihood using the current parameter values <span class="inlinemediaobject"><img src="../Images/image00497.jpeg" alt="Bayesian mixture models"/></span> and <span class="inlinemediaobject"><img src="../Images/image00498.jpeg" alt="Bayesian mixture models"/></span>.
</li><li class="listitem">
In the maximization<a id="id322" class="indexterm"/> step, using the values of <span class="inlinemediaobject"><img src="../Images/image00489.jpeg" alt="Bayesian mixture models"/></span> computed in step 2, estimate new parameter values <span class="inlinemediaobject"><img src="../Images/image00497.jpeg" alt="Bayesian mixture models"/></span> and <span class="inlinemediaobject"><img src="../Images/image00498.jpeg" alt="Bayesian mixture models"/></span> by the maximization of log-likelihood.
</li><li class="listitem">
Compute a new value of the log-likelihood function using the estimated values of <span class="inlinemediaobject"><img src="../Images/image00489.jpeg" alt="Bayesian mixture models"/></span>, <span class="inlinemediaobject"><img src="../Images/image00497.jpeg" alt="Bayesian mixture models"/></span>, and <span class="inlinemediaobject"><img src="../Images/image00498.jpeg" alt="Bayesian mixture models"/></span> from steps 2 and 3.
</li><li class="listitem">Repeat steps 2-4 until the log-likelihood function is converged.</li></ol><div style="height:10px; width: 1px"/></div><p>In the Bayesian treatment of GMM, the maximization of log-likelihood is simplified by introducing a latent variable <span class="emphasis"><em>Z</em></span>. Let <span class="emphasis"><em>Z</em></span> be a <span class="emphasis"><em>K</em></span>-dimensional binary random variable having only one element <span class="emphasis"><em>1</em></span>, and the rest of the <span class="emphasis"><em>K – 1</em></span> elements are <span class="emphasis"><em>0</em></span>. Using <span class="emphasis"><em>Z</em></span>, one can write the joint distribution of <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Z</em></span> as follows:</p><div class="mediaobject"><img src="../Images/image00499.jpeg" alt="Bayesian mixture models"/></div><p style="clear:both; height: 1em;"> </p><p>Here:</p><div class="mediaobject"><img src="../Images/image00500.jpeg" alt="Bayesian mixture models"/></div><p style="clear:both; height: 1em;"> </p><p>And:</p><div class="mediaobject"><img src="../Images/image00501.jpeg" alt="Bayesian mixture models"/></div><p style="clear:both; height: 1em;"> </p><p>Therefore:</p><div class="mediaobject"><img src="../Images/image00502.jpeg" alt="Bayesian mixture models"/></div><p style="clear:both; height: 1em;"> </p><p>And:</p><div class="mediaobject"><img src="../Images/image00503.jpeg" alt="Bayesian mixture models"/></div><p style="clear:both; height: 1em;"> </p><p>The advantage <a id="id323" class="indexterm"/>of introducing a latent variable <span class="emphasis"><em>Z</em></span> in the problem is that the expression for log-likelihood is simplified, where the logarithm directly acts on the normal distribution as in the case of a single Gaussian model. Therefore, it is straightforward to maximize <span class="emphasis"><em>P(X, Z)</em></span>. However, the problem that still remains is that we don't know the value of <span class="emphasis"><em>Z!</em></span> So, the trick is to use an EM-like iterative algorithm where, in the E-step, the expectation value of <span class="emphasis"><em>Z</em></span> is estimated and in the M-step, using the last estimated value of <span class="emphasis"><em>Z</em></span>, we find the parameter values of the Gaussian distribution. The <a id="id324" class="indexterm"/>Bayesian version of the EM algorithm for GMM proceeds as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize parameters <span class="inlinemediaobject"><img src="../Images/image00497.jpeg" alt="Bayesian mixture models"/></span>, <span class="inlinemediaobject"><img src="../Images/image00498.jpeg" alt="Bayesian mixture models"/></span>, and <span class="inlinemediaobject"><img src="../Images/image00489.jpeg" alt="Bayesian mixture models"/></span> and evaluate the initial value of log-likelihood.
</li><li class="listitem">
In the expectation step, use these values to compute the expectation value <span class="inlinemediaobject"><img src="../Images/image00504.jpeg" alt="Bayesian mixture models"/></span>.
</li><li class="listitem">
In the maximization step, using <span class="inlinemediaobject"><img src="../Images/image00504.jpeg" alt="Bayesian mixture models"/></span> fixed, estimate <span class="inlinemediaobject"><img src="../Images/image00497.jpeg" alt="Bayesian mixture models"/></span> and <span class="inlinemediaobject"><img src="../Images/image00498.jpeg" alt="Bayesian mixture models"/></span> by maximizing <span class="inlinemediaobject"><img src="../Images/image00505.jpeg" alt="Bayesian mixture models"/></span>.
</li><li class="listitem">Compute the new likelihood function.</li><li class="listitem">Repeat steps 2-4 until convergence.</li></ol><div style="height:10px; width: 1px"/></div><p>A more detailed treatment of the Bayesian version of the EM algorithm and GMM can be found in the book by Christopher M. Bishop (reference 2 in the <span class="emphasis"><em>References</em></span> section of this chapter). Here, we leave the theoretical treatment of the Bayesian GMM and proceed to look at its R implementation in the <a id="id325" class="indexterm"/>
<span class="strong"><strong>bgmm</strong></span> package.</p><div class="section" title="The bgmm package for Bayesian mixture models"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec41"/>The bgmm package for Bayesian mixture models</h2></div></div></div><p>The <a id="id326" class="indexterm"/>bgmm package was developed by Przemyslaw Biecek and Ewa Szczurek for modeling gene expressions data (reference 3 in the <span class="emphasis"><em>References</em></span> section of this chapter). It can be downloaded from the CRAN website at <a class="ulink" href="http://cran.r-project.org/web/packages/bgmm/index.html">http://cran.r-project.org/web/packages/bgmm/index.html</a>. The package contains not only an unsupervised version of GMM but fully supervised and semi-supervised<a id="id327" class="indexterm"/> implementations as well. The following are the different models available in the bgmm package:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Fully supervised GMM</strong></span>: This <a id="id328" class="indexterm"/>is the labeled data available for all records in a training set. This includes the following:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The <code class="literal">supervised( )</code> function</li></ul></div></li><li class="listitem"><span class="strong"><strong>Semi-supervised GMM</strong></span>: This<a id="id329" class="indexterm"/> is the labeled data available for a small subset of all records in a training set. This includes the following<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The <code class="literal">semisupervised( )</code> function</li></ul></div></li><li class="listitem"><span class="strong"><strong>Partially supervised GMM</strong></span>: This <a id="id330" class="indexterm"/>is the labeled data available for a small subset of all records, but these labels are uncertain. The values of labels are given with some probability. There are two functions in the package for partially supervised GMM. This includes the following::<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
The <code class="literal">belief( )</code> function: The <a id="id331" class="indexterm"/>uncertainty of labels is expressed as a probability distribution over its components. For the first <span class="emphasis"><em>m</em></span> observations, a belief matrix <span class="emphasis"><em>B</em></span> of dimensions <span class="emphasis"><em>m x k</em></span> is given as input where the matrix entry <span class="inlinemediaobject"><img src="../Images/image00506.jpeg" alt="The bgmm package for Bayesian mixture models"/></span> denotes the probability that the <span class="emphasis"><em>i</em></span><sup>th</sup> record has the <span class="emphasis"><em>j</em></span><sup>th</sup> label.
</li><li class="listitem">
The <code class="literal">soft( )</code> function: In <a id="id332" class="indexterm"/>this approach, a plausibility matrix of dimension <span class="emphasis"><em>M x k</em></span> is defined across all records in the training set of size <span class="emphasis"><em>M</em></span>. The matrix element <span class="inlinemediaobject"><img src="../Images/image00507.jpeg" alt="The bgmm package for Bayesian mixture models"/></span> is interpreted as the weight of the prior probability that the <span class="emphasis"><em>i</em></span><sup>th</sup> record has the <span class="emphasis"><em>j</em></span><sup>th</sup> label. If there is no particular information about labels of any records, they can be given equal weights. For the purpose of implementation, a constraint is imposed on the matrix elements: <span class="inlinemediaobject"><img src="../Images/image00508.jpeg" alt="The bgmm package for Bayesian mixture models"/></span>.
</li></ul></div></li><li class="listitem"><span class="strong"><strong>Unsupervised GMM</strong></span>: This <a id="id333" class="indexterm"/>labeled data is not available for any records. This includes the following:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The <a id="id334" class="indexterm"/><code class="literal">unsupervised( )</code> function</li></ul></div></li></ul></div><p>The typical parameters <a id="id335" class="indexterm"/>that are passed to these functions are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="emphasis"><em>X</em></span>: This is a data.frame with the unlabelled <span class="emphasis"><em>X</em></span> data.</li><li class="listitem"><span class="emphasis"><em>knowns</em></span>: This is a data.frame with the labeled <span class="emphasis"><em>X</em></span> data.</li><li class="listitem"><span class="emphasis"><em>B</em></span>: This is a belief matrix that specifies the distribution of beliefs for the labeled records. The number of rows of <span class="emphasis"><em>B</em></span> should be the same as that of <span class="emphasis"><em>knowns</em></span>.</li><li class="listitem"><span class="emphasis"><em>P</em></span>: This is a matrix of weights of prior probabilities (plausibilities).</li><li class="listitem"><span class="emphasis"><em>class</em></span>: This is a vector of classes or labels for the labeled records.</li><li class="listitem"><span class="emphasis"><em>k</em></span>: This is the number of components or columns of the <span class="emphasis"><em>B</em></span> matrix.</li><li class="listitem"><span class="emphasis"><em>init.params</em></span>: These are the initial values for the estimates of model parameters.</li></ul></div><p>The difference <a id="id336" class="indexterm"/>between the <code class="literal">belief( )</code> and <code class="literal">soft( )</code> functions is that, in the first case, the input is a matrix containing prior probability values for each possible label, whereas in the second case, the input is a matrix containing weights for each of the priors and not the prior probability itself. For more details, readers are requested to read the paper by Przemyslaw Biecek et.al (reference 3 in the <span class="emphasis"><em>References</em></span> section of this chapter).</p><p>Now, let's do a small illustrative example of using bgmm. We will use the ADL dataset from the UCI Machine Learning repository. This dataset contains acceleration data from wrist-worn accelerometers from 16 volunteers. The dataset and metadata details can be found at <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer">https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer</a>. The research work on ADL monitoring systems, where this dataset was generated, is published in the two papers by Bruno B. et.al. (reference 4 and reference 5 in the <span class="emphasis"><em>References</em></span> section of this chapter).</p><p>For the example of bgmm, we will only use one folder in the dataset directory, namely <code class="literal">Brush_teeth</code>. Firstly, we will do a small amount of preprocessing to combine data from the different volunteers into a single file. The following R script does this job:</p><div class="informalexample"><pre class="programlisting">&gt;#Set working directory to folder containing files (provide the correct path)
&gt;setwd("C:/…/ADL_Dataset/HMP_Dataset/Brush_teeth")
&gt;flist &lt;- list.files(path = "C:/../ADL_Dataset/HMP_Dataset/Brush_teeth",pattern = "*.txt")
&gt;all.data &lt;- lapply(flist,read.table,sep = " ",header = FALSE)
&gt;combined.data &lt;- as.data.frame(do.call(rbind,all.data))
&gt;combined.data.XZ &lt;- combined.data[,c(1,3)]</pre></div><p>The last step is to select the <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Z</em></span> components of acceleration to create a two-dimensional dataset.</p><p>The following R script calls the <code class="literal">bgmm</code> function and performs clustering. A simple scatter plot of the data suggests that there could be four clusters in the dataset and choosing <span class="emphasis"><em>k = 4</em></span> would be sufficient:</p><div class="informalexample"><pre class="programlisting">&gt;modelbgmm &lt;- unsupervised(combined.data.XZ,k=4)
&gt;summary(modelbgmm)
&gt;plot.mModel(modelbgmm)</pre></div><p>The clusters <a id="id337" class="indexterm"/>generated by bgmm can be seen in the following figure; there are four clusters whose centers are represented by the four color dots and their respective Gaussian densities are represented by the ellipses:</p><div class="mediaobject"><img src="../Images/image00509.jpeg" alt="The bgmm package for Bayesian mixture models"/></div><p style="clear:both; height: 1em;"> </p></div></div></div>
<div class="section" title="Topic modeling using Bayesian inference" id="aid-1LCVG1"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec50"/>Topic modeling using Bayesian inference</h1></div></div></div><p>We<a id="id338" class="indexterm"/> have seen the supervised learning (classification) of text documents in <a class="link" title="Chapter 6. Bayesian Classification Models" href="part0049.xhtml#aid-1ENBI1">Chapter 6</a>, <span class="emphasis"><em>Bayesian Classification Models</em></span>, using the Naïve Bayes model. Often, a large text document, such as a news article or a short story, can contain different topics as subsections. It is useful to model such intra-document statistical correlations for the purpose of classification, summarization, compression, and so on. The Gaussian mixture model learned in the previous section is more applicable for numerical data, such as images, and not for documents. This is because words in documents seldom follow normal distribution. A more appropriate choice would be multinomial distribution.</p><p>A powerful extension of mixture models to documents is the work of T. Hofmann on Probabilistic <a id="id339" class="indexterm"/>Semantic Indexing (reference 6 in the <span class="emphasis"><em>References</em></span> section of this chapter) and that of David Blei, et. al. on Latent Dirichlet allocation (reference 7 in the <span class="emphasis"><em>References</em></span> section of this chapter). In these works, a document is described as a mixture of topics and each topic is described by a distribution of words. LDA is a generative unsupervised model for text documents. The task of LDA is to learn the parameters of the topic distribution, word distributions, and mixture coefficients from data. A brief overview of LDA is presented in the next section. Readers are strongly advised to read the paper by David Blei, et al. to comprehend their approach.</p><div class="section" title="Latent Dirichlet allocation"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec42"/>Latent Dirichlet allocation</h2></div></div></div><p>In LDA, it is<a id="id340" class="indexterm"/> assumed that words are the basic units of documents. A word is one element of a set known as <a id="id341" class="indexterm"/>vocabulary, indexed by <span class="inlinemediaobject"><img src="../Images/image00510.jpeg" alt="Latent Dirichlet allocation"/></span>. Here, <span class="emphasis"><em>V</em></span> denotes the size of the vocabulary. A word can be represented by a unit-basis vector, whose all components are zero except the one corresponding to the word that has a value 1. For example, the <span class="emphasis"><em>n</em></span><sup>th</sup> word in a vocabulary is described by a vector of size <span class="emphasis"><em>V</em></span>, whose <span class="emphasis"><em>n</em></span><sup>th</sup> component <span class="inlinemediaobject"><img src="../Images/image00511.jpeg" alt="Latent Dirichlet allocation"/></span> and all other components <span class="inlinemediaobject"><img src="../Images/image00512.jpeg" alt="Latent Dirichlet allocation"/></span> for <span class="inlinemediaobject"><img src="../Images/image00513.jpeg" alt="Latent Dirichlet allocation"/></span>. Similarly, a document is a collection of <span class="emphasis"><em>N</em></span> words denoted by <span class="inlinemediaobject"><img src="../Images/image00514.jpeg" alt="Latent Dirichlet allocation"/></span> and a corpus is a collection of <span class="emphasis"><em>M</em></span> documents denoted by <span class="inlinemediaobject"><img src="../Images/image00515.jpeg" alt="Latent Dirichlet allocation"/></span> (note that documents are represented here by a bold face <span class="strong"><strong>w</strong></span>, whereas words are without bold face w).</p><p>As mentioned earlier, LDA is a generative probabilistic model of a corpus where documents are represented as random mixtures over latent topics and each topic is characterized by a distribution over words. To generate each document <span class="strong"><strong>w</strong></span> in a corpus in an LDA model, the following steps are performed:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Choose the value of <span class="emphasis"><em>N</em></span> corresponding to the size of the document, according to a Poisson distribution characterized by parameter <span class="inlinemediaobject"><img src="../Images/image00516.jpeg" alt="Latent Dirichlet allocation"/></span>:
<div class="mediaobject"><img src="../Images/image00517.jpeg" alt="Latent Dirichlet allocation"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">
Choose<a id="id342" class="indexterm"/> the <a id="id343" class="indexterm"/>value of parameter <span class="inlinemediaobject"><img src="../Images/image00518.jpeg" alt="Latent Dirichlet allocation"/></span> that characterizes the topic distribution from a Dirichlet distribution characterized by parameter <span class="inlinemediaobject"><img src="../Images/image00519.jpeg" alt="Latent Dirichlet allocation"/></span>:
<div class="mediaobject"><img src="../Images/image00520.jpeg" alt="Latent Dirichlet allocation"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">
For each of the <span class="emphasis"><em>N</em></span> words <span class="inlinemediaobject"><img src="../Images/image00521.jpeg" alt="Latent Dirichlet allocation"/></span><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Choose a topic <span class="inlinemediaobject"><img src="../Images/image00522.jpeg" alt="Latent Dirichlet allocation"/></span> according to the multinomial distribution characterized by the parameter <span class="inlinemediaobject"><img src="../Images/image00518.jpeg" alt="Latent Dirichlet allocation"/></span> drawn in step 2:
<div class="mediaobject"><img src="../Images/image00523.jpeg" alt="Latent Dirichlet allocation"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">
Choose a word <span class="inlinemediaobject"><img src="../Images/image00521.jpeg" alt="Latent Dirichlet allocation"/></span> from the multinomial probability distribution characterized by <span class="inlinemediaobject"><img src="../Images/image00524.jpeg" alt="Latent Dirichlet allocation"/></span> and conditioned on <span class="inlinemediaobject"><img src="../Images/image00522.jpeg" alt="Latent Dirichlet allocation"/></span>:
</li></ol><div style="height:10px; width: 1px"/></div><div class="mediaobject"><img src="../Images/image00525.jpeg" alt="Latent Dirichlet allocation"/></div><p style="clear:both; height: 1em;"> </p></li></ol><div style="height:10px; width: 1px"/></div><p>Given <a id="id344" class="indexterm"/>values of <span class="emphasis"><em>N</em></span>, <span class="inlinemediaobject"><img src="../Images/image00519.jpeg" alt="Latent Dirichlet allocation"/></span>, and <span class="inlinemediaobject"><img src="../Images/image00524.jpeg" alt="Latent Dirichlet allocation"/></span>, the joint<a id="id345" class="indexterm"/> distribution of a topic mixture <span class="inlinemediaobject"><img src="../Images/image00518.jpeg" alt="Latent Dirichlet allocation"/></span>, set of topics <span class="strong"><strong>z</strong></span>, and set of words <span class="strong"><strong>w</strong></span>, is given by:</p><div class="mediaobject"><img src="../Images/image00526.jpeg" alt="Latent Dirichlet allocation"/></div><p style="clear:both; height: 1em;"> </p><p>Note that, in this case, only <span class="emphasis"><em>w</em></span> is observed (the documents) and both <span class="inlinemediaobject"><img src="../Images/image00518.jpeg" alt="Latent Dirichlet allocation"/></span> and <span class="strong"><strong>z</strong></span> are treated as latent (hidden) variables.</p><p>The Bayesian inference problem in LDA is the estimation of the posterior density of latent variables <span class="inlinemediaobject"><img src="../Images/image00518.jpeg" alt="Latent Dirichlet allocation"/></span> and <span class="strong"><strong>z</strong></span>, given a document given by:</p><div class="mediaobject"><img src="../Images/image00527.jpeg" alt="Latent Dirichlet allocation"/></div><p style="clear:both; height: 1em;"> </p><p>As usual, with many Bayesian models, this is intractable analytically and one has to use approximate techniques, such as MCMC or variational Bayes, to estimate the posterior.</p></div></div>
<div class="section" title="R packages for LDA" id="aid-1MBG21"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec51"/>R packages for LDA</h1></div></div></div><p>There are <a id="id346" class="indexterm"/>mainly two packages in R that can be <a id="id347" class="indexterm"/>used for performing LDA on documents. One is the <span class="strong"><strong>topicmodels</strong></span> package developed by Bettina Grün and Kurt Hornik and the second one is <span class="strong"><strong>lda</strong></span> developed by Jonathan Chang. Here, we describe both these packages.</p><div class="section" title="The topicmodels package"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec43"/>The topicmodels package</h2></div></div></div><p>The<a id="id348" class="indexterm"/> topicmodels package is <a id="id349" class="indexterm"/>an interface to the C and C++ codes developed by the authors of the papers on LDA and <a id="id350" class="indexterm"/>
<span class="strong"><strong>Correlated Topic Models</strong></span> (<span class="strong"><strong>CTM</strong></span>) (references 7, 8, and 9 in the <span class="emphasis"><em>References</em></span> section of this chapter). The main function <code class="literal">LDA</code> in this package is used to fit LDA models. It can be called by:</p><div class="informalexample"><pre class="programlisting">&gt;LDA(X,K,method = "Gibbs",control = NULL,model = NULL,...)</pre></div><p>Here, <span class="emphasis"><em>X</em></span> is a document-term matrix that can be generated using the <span class="strong"><strong>tm</strong></span> package and <span class="emphasis"><em>K</em></span> is the number of topics. The <code class="literal">method</code> is the method to be used for fitting. There are two methods that are supported: <code class="literal">Gibbs</code> and <code class="literal">VEM</code>.</p><p>Let's do a small example of building LDA models using this package. The dataset used is the <a id="id351" class="indexterm"/>
<span class="strong"><strong>Reuter_50_50</strong></span> dataset from the UCI Machine Learning repository (references 10 and 11 in the <span class="emphasis"><em>References</em></span> section of this chapter). The dataset can be downloaded from <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Reuter_50_50">https://archive.ics.uci.edu/ml/datasets/Reuter_50_50</a>. For this exercise, we will only use documents from one directory, namely <code class="literal">AlanCrosby</code> in the <code class="literal">C50train</code> directory. The required preprocessing can be done using the following R script; readers should have installed the tm and topicmodels packages before trying this exercise:</p><div class="informalexample"><pre class="programlisting">&gt;library(topicmodels)
&gt;library(tm)
&gt;#creation of training corpus from reuters dataset
&gt;dirsourcetrain &lt;- DirSource(directory = "C:/…/C50/C50train/AaronPressman")
&gt;xtrain &lt;- VCorpus(dirsourcetrain)
&gt;#remove extra white space
&gt;xtrain &lt;- tm_map(xtrain,stripWhitespace)
&gt;#changing to lower case
&gt;xtrain &lt;- tm_map(xtrain,content_transformer(tolower))
&gt;#removing stop words
&gt;xtrain &lt;- tm_map(xtrain,removeWords,stopwords("english"))
&gt;#stemming the document
&gt;xtrain &lt;- tm_map(xtrain,stemDocument)
&gt;#creating Document-Term Matrix
&gt;xtrain &lt;-  as.data.frame.matrix(DocumentTermMatrix(xtrain))</pre></div><p>The same <a id="id352" class="indexterm"/>set of steps can be used to create the test dataset from the <code class="literal">/…/C50/C50test/</code> directory.</p><p>Once we have the document-term matrices <code class="literal">xtrain</code> and <code class="literal">xtest</code>, the LDA model can be built and tested using the following R script:</p><div class="informalexample"><pre class="programlisting">&gt;#training lda model
&gt;ldamodel &lt;- LDA(xtrain,10,method = "VEM")
&gt;#computation of perplexity, on training data (only with VEM method)
&gt;perp &lt;- perplexity(ldamodel)
&gt;perp
[1] 407.3006</pre></div><p>A value of<a id="id353" class="indexterm"/> perplexity around 100 indicates a good fit. In this case, we need to add more training data or change the value of <span class="emphasis"><em>K</em></span> to improve perplexity.</p><p>Now let's use the trained LDA model to predict the topics on the test dataset:</p><div class="informalexample"><pre class="programlisting">&gt;#extracting topics from test data)
&gt;postprob &lt;- posterior(ldamodel,xtest)
&gt;postprob$topics</pre></div><div class="mediaobject"><img src="../Images/image00528.jpeg" alt="The topicmodels package"/></div><p style="clear:both; height: 1em;"> </p><p>Here, the test set contains only one file, namely <code class="literal">42764newsML.txt</code>. The distribution of its topic among the 10 topics produced by the LDA model is shown.</p></div><div class="section" title="The lda package"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec44"/>The lda package</h2></div></div></div><p>The <a id="id354" class="indexterm"/>lda package<a id="id355" class="indexterm"/> was developed by Jonathan Chang and he implemented a collapsed Gibbs sampling method for the estimation of posterior. The package can be downloaded from the CRAN website at <a class="ulink" href="http://cran.r-project.org/web/packages/lda/index.html">http://cran.r-project.org/web/packages/lda/index.html</a>.</p><p>The main function in the package, <code class="literal">lda.collapsed.gibbs.sampler</code>, uses a collapsed Gibbs sampler to fit three different models. These are <a id="id356" class="indexterm"/>
<span class="strong"><strong>Latent Dirichlet allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>), <span class="strong"><strong>supervised LDA</strong></span> (<span class="strong"><strong>sLDA</strong></span>), and the <span class="strong"><strong>mixed membership stochastic blockmodel</strong></span> (<span class="strong"><strong>MMSB</strong></span>). These functions take input documents and <a id="id357" class="indexterm"/>return point <a id="id358" class="indexterm"/>estimates of latent parameters. These functions can be used in R as follows:</p><div class="informalexample"><pre class="programlisting">&gt;lda.collapsed.gibbs.sampler(documents,K,vocab,num.iterations,alpha,eta,initial = NULL,burnin = NULL,compute.log.likelihood = FALSE,trace = 0L,freeze.topics = FALSE)</pre></div><p>Here, <code class="literal">documents</code><a id="id359" class="indexterm"/> represents<a id="id360" class="indexterm"/> a list containing documents, the length of the list is equal to <code class="literal">D</code>, and <code class="literal">K</code> is the number of topics; <code class="literal">vocab</code> is a character vector specifying the vocabulary of words; <code class="literal">alpha</code> and <code class="literal">eta</code> are the values of hyperparameters.</p></div></div>
<div class="section" title="Exercises" id="aid-1NA0K1"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec52"/>Exercises</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">For <a id="id361" class="indexterm"/>the Reuter_50_50 dataset, fit the LDA model using the <code class="literal">lda.collapsed.gibbs.sampler</code> function in the lda package and compare performance with that of the topicmodels package. Note that you need to convert the document-term matrix to lda format using the <code class="literal">dtm2ldaformat( )</code> function in the topicmodels package in order to use the lda package.</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="References" id="aid-1O8H61"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec53"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Bouwmans, T., El Baf F., and "Vachon B. Background Modeling Using Mixture of Gaussians for Foreground Detection – A Survey" (PDF). Recent Patents on Computer Science 1: 219-237. 2008</li><li class="listitem">Bishop C.M. <span class="emphasis"><em>Pattern Recognition and Machine Learning</em></span>. Springer. 2006</li><li class="listitem">Biecek P., Szczurek E., Tiuryn J., and Vingron M. "The R Package bgmm: Mixture Modeling with Uncertain Knowledge". Journal of Statistical Software. Volume 47, Issue 3. 2012</li><li class="listitem">Bruno B., Mastrogiovanni F., Sgorbissa A., Vernazza T., and Zaccaria R. "Analysis of human behavior recognition algorithms based on acceleration data". In: IEEE Int Conf on Robotics and Automation (ICRA), pp. 1602-1607. 2013</li><li class="listitem">Bruno B., Mastrogiovanni F., Sgorbissa A., Vernazza T., and Zaccaria R. "Human Motion Modeling and Recognition: A computational approach". In: IEEE International Conference on Automation Science and Engineering (CASE). pp 156-161. 2012</li><li class="listitem">Hofmann T. "Probabilistic Latent Semantic Indexing". In: Twenty-Second Annual International SIGIR Conference. 1999</li><li class="listitem">Blei D.M., Jordan M.I., and Ng A.Y. "Latent Dirichlet Allocation". Journal of Machine Learning Research 3. 993-1022. 2003</li><li class="listitem">Blei D.M., and Lafferty J.D. "A Correlated Topic Model of Science". The Annals of Applied Statistics. 1(1), 17-35. 2007</li><li class="listitem">Phan X.H., Nguyen L.M., and Horguchi S. "Learning to Classify Short and Sparse Text &amp; Web with Hidden Topics from Large-scale Data Collections". In: 17th International World Wide Web Conference (WWW 2008). pages 91-100. Beijing, China. 2008</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="Summary" id="aid-1P71O1"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec54"/>Summary</h1></div></div></div><p>In this chapter, we discussed the concepts behind unsupervised and semi-supervised machine learning, and their Bayesian treatment. We learned two important Bayesian unsupervised models: the Bayesian mixture model and LDA. We discussed in detail the bgmm package for the Bayesian mixture model, and the topicmodels and lda packages for topic modeling. Since the subject of unsupervised learning is vast, we could only cover a few Bayesian methods in this chapter, just to give a flavor of the subject. We have not covered semi-supervised methods using both item labeling and feature labeling. Interested readers should refer to more specialized books in this subject. In the next chapter, we will learn another important class of models, namely neural networks.</p></div></body></html>